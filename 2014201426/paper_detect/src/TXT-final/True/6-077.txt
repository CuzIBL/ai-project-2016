 
predictive accuracy has been used as the main and often only evaluation criterion for the predictive performance of classification learning algorithms. in recent years  the area under the roc  receiver operating characteristics  curve  or simply auc  has been proposed as an alternative single-number measure for evaluating learning algorithms. in this paper  we prove that auc is a better measure than accuracy. more specifically  we present rigourous definitions on consistency and discriminancy in comparing two evaluation measures for learning algorithms. we then present empirical evaluations and a formal proof to establish that auc is indeed statistically consistent and more discriminating than accuracy. our result is quite significant since we formally prove that  for the first time  auc is a better measure than accuracy in the evaluation of learning algorithms. 
1 	introduction 
the predictive ability of the classification algorithm is typically measured by its predictive accuracy  or error rate  which is 1 minus the accuracy  on the testing examples. however  most classifiers  including c1 and naive bayes  can also produce probability estimations or  confidence  of the class prediction. unfortunately  this information is completely ignored in accuracy. this is often taken for granted since the true probability is unknown for the testing examples anyway. 
　in many applications  however  accuracy is not enough. for example  in direct marketing  we often need to promote the top x% of customers during gradual roll-out  or we often deploy different promotion strategies to customers with different likelihood of buying some products. to accomplish these tasks  we need more than a mere classification of buyers and non-buyers. we need  at least  a ranking of customers in terms of their likelihoods of buying. if we want to achieve a more accurate ranking from a classifier  one might naturally expect that we must need the true ranking in the training examples  cohen et a/.  1 . in most scenarios  however  that is not possible. instead  what we are given is a dataset of examples with class labels only. thus  given only classification labels 
harry zhang 
faculty of computer science 
university of new brunswick 
fredericton  nb  canada e1b 1 hzhang unb.ca 
in training and testing sets  are there better methods than accuracy to evaluate classifiers that also produce rankings  
　the roc  receiver operating characteristics  curve has been recently introduced to evaluate machine learning algorithms  provost and fawcett  1; provost et al  1 . it compares the classifiers' performance across the entire range of class distributions and error costs. however  often there is no clear dominating relation between two roc curves in the entire range; in those situations  the area under the roc curve  or simply auc  provides a single-number  summary  for the performance of the learning algorithms. bradley  bradley  1  has compared popular machine learning algorithms using auc  and found that auc exhibits several desirable properties compared to accuracy. for example  auc has increased sensitivity in analysis of variance  anova  tests  is independent to the decision threshold  and is invariant to a priori class probability distributions  bradley  1 . however  no formal arguments or criteria have been established. how can we compare two evaluation measures for learning algorithms  how can we establish that one measure is better than another  in this paper  we give formal definitions on the consistency and discriminancy for comparing two measures. we show  empirically and formally  that auc is indeed a statistically consistent and more discriminating measure than accuracy. 
　one might ask why we need to care about anything more than accuracy  since by definition  classifiers only classify examples  and do not care about ranking and probability . we answer this question from two aspects. first  as we discussed earlier  even with labelled training and testing examples  most classifiers do produce probability estimations that can rank training/testing examples. ranking is very important in most real-world applications. as we establish that auc is a better measure than accuracy  we can choose classifiers with better auc  thus producing better ranking. second  and more importantly  if we build classifiers that optimize auc  instead of accuracy   it has been shown  ling and zhang  1  that such classifiers produce not only better auc  a natural consequence   but also better accuracy  a surprising result   compared to classifiers that optimize the accuracy. to make an analogy  when we train workers on a more complex task  they will do better on a simple task than workers who are trained only on the simple task. 
our work is quite significant for several reasons. first  

learning 	1 

 we establish rigourously  for the first time  that even given only labelled examples  auc is a better measure  defined in section 1  than accuracy. second  our result suggests that auc should replace accuracy in comparing learning algorithms in the future. third  our results prompt and allow us to re-evaluate well-established results in machine learning. for example  extensive experiments have been conducted and published on comparing  in terms of accuracy  decision tree classifiers to naive bayes classifiers. a well-established and accepted conclusion in the machine learning community is that those learning algorithms are very similar as measured by accuracy  kononenko  1;langley et al  1;domingos and pazzani  1 . since we have established that auc is a better measure  are those learning algorithms still very similar as measured by auc  we have shown  ling et a/.  1  that  surprisingly  this is not true: naive bayes is significantly better than decision tree in terms of auc. these kinds of new conclusions are very useful to the machine learning community  as well as to machine learning applications  e.g.  data mining . fourth  as a new measure  such as auc  is discovered and proved to be better than a previous measure  such as accuracy   we can re-design most learning algorithms to optimize the new measure  ferri et al  1; ling and zhang  1 . this would produce classifiers that not only perform well in the new measure  but also in the previous measure  compared to the classifiers that optimize the previous measure  as shown in  ling and zhang  1 . this would further improve the performance of our learning algorithms. 
1 criteria for comparing evaluation measures for classifiers 
we start with some intuitions in comparing auc and accuracy  and then we present formal definitions in comparing evaluation measures for learning algorithms. 
1 	auc vs accuracy 
hand and till  hand and till  1  present a simple approach to calculating the auc of a classifier g below. 
		 1  
where n1 and n1 are the numbers of positive and negative examples respectively  and  is the rank of ith positive example in the ranked list. table 1 shows an example of how to calculate auc from a ranked list with 1 positive examples and 1 negative examples. the auc of the ranked list in table 1 is which is 1. 
it is clear that auc obtained by equation 1 is a measure for the quality of ranking  as the more positive examples are ranked higher  to the right of the list   the larger the term . auc is shown to be equivalent to the wilcoxon statistic rank test  bradley  1 . 
　intuitively  we can see why auc is a better measure than accuracy from the following example. let us consider two classifiers  classifier 1 and classifier 1  both producing probability estimates for a set of 1 testing examples. assume that both classifiers classify 1 of the 1 examples as positive  and 

table 1: an example for calculating auc with rz 

table 1: an example in which two classifiers have the same classification accuracy  but different auc values 
the other 1 as negative. if we rank the testing examples according to increasing probability of being +  positive   we get the two ranked lists as in table 1. 
　clearly  both classifiers produce an error rate of 1%  one false positive and one false negative   and thus the two classifiers are equivalent in terms of error rate. however  intuition tells us that classifier 1 is better than classifier 1  since overall positive examples are ranked higher in classifier 1 than 1. if we calculate auc according to equation 1  we obtain that the auc of classifier 1 is  as seen in table 1   and the auc of classifier 1 is . clearly  auc does tell us that classifier 1 is indeed better than classifier 1. 
　unfortunately   counter examples  do exist  as shown in table 1 on two other classifiers: classifier 1 and classifier 1. 
it is easy to obtain that the auc of classifier 1 is   and the auc of classifier 1 is . however  the error rate of classifier 1 is 1%  while the error rate of classifier 1 is only 1%  again we assume that the threshold for accuracy is set at the middle so that 1 examples are predicted as positive and 1 as negative . therefore  a larger auc does not always imply a lower error rate. 

table 1: a counter example in which one classifier has higher 
auc but lower classification accuracy 
　another intuitive argument for why auc is better than accuracy is that auc is more discriminating than accuracy since it has more possible values. more specifically  given a dataset with n examples  there is a total of only n + 1 different classification accuracies . on the other hand  
assuming there are no positive examples and n1 negative exdifferent auc val  generally more than 
n + 1. however  counterexamples also exist in this regard. table 1 illustrates that classifiers with the same auc can have different accuracies. here  we see that classifier 1 and classifier 1 have the same  but different accuracies  1% and 1% respectively . in general  a measure with more values is not necessarily more discriminating. for example  the weight of a person  having infinitely many possible values  has nothing to do with the number of siblings  having only a small number of integer values  he or she has. 

1 	learning 

table 1: a counter example in which two classifiers have same auc but different classification accuracies 
   how do we compare different evaluation measures for learning algorithms  some general criteria must be established. 
1 	 strict  consistency and discriminaney of two measures 
intuitively speaking  when we discuss two different measures 
/ and g on evaluating two learning algorithms a and b  we 	figure 1: illustrations of two measures / and g. a link bewant at least that / and g be consistent with each other. that 	tween two points indicates that the function values are the is  when / stipulates that algorithm a is  strictly  better than 	same on domain . in  a   / is strictly consistent and more b  then g will not say b is better than a. further  i f / is more 	discriminating than g. in  b   / is not strictly consistent or discriminating than g  we would expect to see cases where / 	more discriminating than g. counter examples on consistency can tell the difference between algorithms a and b but g can-	 denoted by x in the figure  and discriminaney  denoted by y  not. 	exist here 
　this intuitive meaning of consistency and discriminaney can be made precise as the following definitions. 
definition 1  consistency  for two measures f  g on domain   /  g are  strictly  consistent if there exist no a  b such that f  a    f b  and g a    g b . 
definition 1  discriminaney  for two measures f  g on domain   f is  strictly  more discriminating than g if there exist a  b such that f  a    f b  and g a  - g b   and there exist no a  b such that g a    g b  and f a  - f b . 
　as an example  let us think about numerical marks and letter marks that evaluate university students. a numerical mark gives 1  1  1  or 1 to students  while a letter mark-
gives a  b  c  d  or f to students. obviously  we regard a   there are clear and important implications of these definib   c   d   f. clearly  numerical marks are consistent with tions of measures / and g in evaluating two machine learning letter marks  and vice versa . in addition  numerical marks algorithms  say a and b. if / and g are consistent to degree c  are more discriminating than letter marks  since two students then when / stipulates that a is better than b  there is a probwho receive 1 and 1 respectively receive different numer- ability c that g will agree  stipulating a is better than b . if ical marks but the same letter mark  but it is not possible to / is d times more discriminating than g  then it is d times have students with different letter marks  such as a and b  but more likely that / can tell the difference between a and b but with the same numerical marks. this ideal example of a mea- g cannot than that g can tell the difference between a and b sure /  numerical marks  being strictly consistent and more but / cannot. clearly  we require that c   1 and d   1 if discriminating than another g  letter marks  can be shown in we want to conclude a measure / is  better  than a measure the figure 1 a . g. this leads to the following definition: 
1 	statistical consistency and discriminaney of 	definition 1 	the measure f 	is statistically consistent and 
	two measures 	more discriminating than g if and only if c   1 and d   1. 
as we have already seen in section 1  counter examples on in this case  we say  intuitively  that f is a better measure than consistency and discriminaney do exist for auc and accu- 1  racy. therefore  it is impossible to prove the consistency and the statistical consistency and discriminaney is a special 
discriminaney on auc and accuracy based on definitions 1 case of the strict consistency and more discriminaney. for the and 1. figure 1 b  illustrates a situation where one measure example of numerical and letter marks in the student evalua/ is not completely consistent with g  and is not strictly more tion discussed in section 1  we can obtain that c = 1 and discriminating than g. in this case  we must consider the prob- d - oc  as the former is strictly consistent and more discrimability of being consistent and degree of being more discrimi- inating than the latter. 
nating. what we will define and prove is the probabilistic version of the two definitions on strict consistency and discrim- 'it is easy to prove that this definition is symmetric; that is  the inaney. that is  we extend the previous definitions to degree degree of consistency of/ and g is same as the degree of consistency 
of consistency and degree of discriminaney  as follows: 	of g and /. 
learning 	1 

before we present a formal proof that auc is statistically consistent and more discriminating than accuracy  we first present an empirical verification. to simplify our notation  we will use auc to represent auc values  and ace for accuracy. 
　we conduct experiments with  artificial  testing sets to verify the statistical consistency and discriminancy between auc and accuracy. the datasets are balanced with equal numbers of positive and negative examples. we test datasets with 1  1  1  1  1  1  and 1 testing examples. for each number of examples  we enumerate all possible ranked lists of positive and negative examples. for the dataset with 1n examples  there are  1n  such ranked lists. 
　we exhaustively compare all pairs of ranked lists to see how they satisfy the consistency and discriminating propositions probabilistically. to obtain degree of consistency  we count the number of pairs which satisfy  auc  a    auc b  and acc a    acc b  and the number of pairs which satisfy  auc a    auc b  and ace a    acc b  . we then calculate the percentage of those cases; that is  the degree of consistency. to obtain degree of discriminancy  we count the number of pairs which satisfy  auc a    auc b  and acc a  = acc by  and the number of pairs which satisfy  auc a  = auc b  and aec{a    orc 1  . 
　tables 1 and 1 show the experiment results for the balanced dataset. for consistency  we can see  table 1  that for various numbers of balanced testing examples  given auc  a    auc b   the number  and percentage  of cases that satisfy aee a    ace b  is much greater than those that satisfy ace a    aee b . when n increases  the degree of consistency  c  seems to approach 1  much larger than the required 1. for discriminancy  we can see clearly from table 1 that the number of cases that satisfy. and 
acc a  = acc b  is much more  from 1 to 1 times more  than the number of cases that satisfy ace a    acc b  and auc a  = auc b . when n increases  the degree of discriminancy  d  seems to approach 1  much larger than the required threshold 1. 
　these empirical experiments verify empirically that auc is indeed a statistically consistent and more discriminating table 1: experimental results for verifying auc is statistically more discriminating than accuracy for the balanced dataset 
measure than accuracy for the balanced datasets  as a measure for learning algorithms. 
1 	formal proof 
in our following discussion  we assume that the domain   is the set of all the ranked lists with no positive and n1 negative examples. in this paper  we only study the cases where a ranked list contains an equal number of positive and negative examples  and we assume that the cutoff for classification is at the exact middle of the ranked list  each classifier classifies exactly half examples into the positive class and the other half into the negative class . that is  n1 = n1  and we use n instead of n1 and n1. 
　lemma 1 gives the maximum and minimum auc values for a ranked list with a fixed accuracy rate. 

proof: assume that there are np positive examples correctly classified. then there are n-np positive examples in negative section. according to equation 1  auc value is determined by 1o  which is the sum of indexes of all positive examples. so when all n-np positive examples are put on the highest positions in negative section  their indexes are n  n - 1          np + 1   and np positive examples are put on the highest positions in positive section  their indexes are 

1 	learning 


learning 	1 

therefore  from inequality 1 and equation 1  we will get that auc is a better measure than accuracy in evaluating and comparing classification learning algorithms. this conclusion has many important implications in evaluating  comparing  and designing learning algorithms. in our future research  we will extend the results in this paper to cases with unbalanced class distribution and multiple classes. 
