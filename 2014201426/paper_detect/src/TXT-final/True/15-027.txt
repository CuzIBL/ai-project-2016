 
we discuss commercial expert system development  using our experience with the dipmeter advisor system as a case study. while the data is too sparse for definitive results  several ideas have emerged as important and suggestive as guidelines for subsequent commercial expert system undertakings. 
during the past four years  the dipmeter advisor system has migrated from an initial experiment in application of expert system techniques in well-log interpretation to a candidate commercial interpretation system. the system has undergone substantial change: it has been implemented in different configurations  in different languages  and on different computer systems. our ability to experiment with the system has been greatly enhanced by the tools and ideas of rapid prototyping. we have also observed an oscillation in thrust over time between  i  expansion and change in the domain knowledge  and 1 selection and design of appropriate expert system tools. finally  we have found several of the maxims of expert system development to be valid  but question a number of others. 
1. introduction 
the past decade has seen the development of a number of expert systems  mostly by ai researchers for use in research environments. to date  few have been utilized for industrial applications. as a result  we have little experience with which to characterize either the nature of commercial expert systems or their development process. 
the dipmeter advisor system is the result of a four year effort by schlumberger to apply expert systems technology to problems of well-log interpretation. we have observed during this effort that the development of a commercial expert system imposes a substantially different set of constraints and requirements in terms of characteristics and methods of development than those seen in the research environment. 
this paper is intended as a case study. we briefly describe the dipmeter interpretation problem and the evolution of the dipmeter advisor system. during its development a number of ideas have surfaced which we believe to be characteristic of this type of effort  given the current state of the technology. while the data is too sparse for definitive results  these ideas are thought to be important and suggestive as guidelines for subsequent commercial expert system undertakings. 
1. the problem 
oil-well logs are made by lowering tools into the borehole and recording measurements made by the tools as they are raised to the surface. the resulting logs are sequences of values indexed by depth. logging tools measure a variety of petrophysical properties. the dipmeter tool in particular measures the conductivity of rock in a number of directions around the borehole. variations in conductivity can be correlated and combined with measurements of the inclination and orientation of the tool to estimate the magnitude and azimuth of the dip or tilt of various formation layers penetrated by the borehole  figure 1. . 
w 	a 	e 

because the dipmeter tool has high resolution in the vertical direction  1-1 in.   it provides the petroleum geologist with detailed information on relatively fine-structured sedimentary beds. this type of information is invaluable in defining hydrocarbon reservoir structure and designing methods to drain such reservoirs. 
knowledge of the dip variations as a function of depth in the vicinity of the borehole does not in itself identify geologic features. however  when combined with knowledge of local geology and rock properties measured by other logs  e.g.  lithology  sand  shale  ...    the characteristic dip patterns  signatures  of geologic events in the depositional sequence can be interpreted. 
the right channel of figure 1 is an interval of a dipmeter log. dip estimates are shown as tadpoles. dip magnitude increases to the right of the graph  and the down dip direction is indicated by the tail on each tadpole. the vertical axis is depth.  hollow tadpoles indicate lower confidence dip estimates than solid tadpoles.  the left channel is a gamma ray log.  it measures natural gamma radiation in the formation-a rudimentary lithology indicator.  


sequences of tadpoles can be grouped together in patterns. 
three of the characteristic dip patterns are described below  schlumberger  1 . 
  green pattern: an interval  zone  of constant dip magnitude and azimuth. this pattern is characteristic of structural dip-caused by large-scale tectonic disturbance that occurs long after deposition and compaction of sediment. 
  red pattern: a zone of increasing dip magnitude with constant azimuth over depth. this pattern is indicative of down dip thickening  which may be associated with distortions near structural features  e.g.  faults   differential compaction of sediment over buried topographic features  e.g.  reefs   or channel filling. 
  blue pattern: a zone of decreasing dip magnitude with constant azimuth over depth. this pattern is indicative of down dip thinning  which may be associated with distortions near structural features  differential compaction beneath denser overlying deposits  e.g.  sand lenses   or sediment transport by water or wind. 
r. smith and j. baker 1 
from this localized data  a skilled interpreter is often able to make comprehensive deductions about the geological history of deposition  the composition and structure of the beds  and the optimum locations for future wells. 
1. system overview 
the dipmeter advisor system attempts to emulate human expert performance in dipmeter interpretation. it utilizes dipmeter patterns together with local geological knowledge and measurements from other logs. it is characteristic of the class of programs that deal with what has come to be known as signal to symbol transformation  nii  1 .1 the program is written in 1nterl1sp and operates on the xerox 1 scientific information processor  dolphin . 
the system is made up of four central components:  i  a number of production rules partitioned into several distinct sets according to function  e.g.  structural rules vs stratigraphic rules ;  ii  an inference engine that applies rules in a forward-chained manner  resolving conflicts by rule order;  iii  a set of feature detection algorithms that examines both dipmeter and open hole data  e.g.  to detect tadpole patterns and identify lithological zones ; and  iv  a 
menu-driven graphical user interface that provides smooth scrolling of log data. 
conclusions are stored as instances of one of 1 token types  with approximately 1 features/token  on a blackboard that is partitioned into 1 layers of abstraction  e.g.  patterns  lithology  stratigraphic features . there are 1 rules and the rule language uses approximately 1 predicates and functions. the rules have the familiar empirical association flavor. a sample is shown below. 
if there exists a delta-dominated  continental-she f marine zone  and there exists a sand zone intersecting the marine zone  and 
there exists a blue pattern within the intersection 
then 
assert a distributary fan zone 
top - top of blue pattern bottom  - bottom of blue pattern 
flow - azimuth of blue pattern 
the system divides the task of dipmeter interpretation into 
1 successive phases as shown below. after the system completes its analysis for a phase  it engages the human interpreter in an interactive dialogue. he can examine  delete  or modify conclusions reached by the system. he can also add his own conclusions. in addition  he can revert to earlier phases of the analysis to refer to the conclusions  or to rerun the computation. 
1. initial examination: the human interpreter can peruse the available data and select logs for display. 
1. validity check: the system examines the logs for evidence of tool malfunction or incorrect processing. 
1. early versions of the program are described in  davis  1   and  gershman  1. 
1. this sample is similar to the actual interpretation rule  but has been simplified somewhat for presentation. 

1 r. smith and j. baker 
1. green pattern detection: the system identifies zones in which the tadpoles have similar magnitude and azimuth. 
1. structural dip analysis: the system merges and filters green patterns to determine zones of constant structural dip. 
+ 1. preliminary structural analysis: the system applies a set of rules to identify structural features  e.g.  faults . 
1. structural pattern detection: the system examines the dipmeter data for red and blue 
patterns in the vicinity of structural features.1 
+ 1. final structural analysis: the system applies a 
set of rules that combines information from previous phases to refine its conclusions about structural features  e.g.  strike of faults . 
1. lithology analysis: the system examines the open hole data  e.g.  gamma ray  to determine zones of constant lithology  e.g.  sand and shale . 
+ 1. depositional environment analysis: the system applies a set of rules that draws conclusions about the depositional environment. for example  if told by the human interpreter that the depositional environment is marine  the system attempts to infer the water depth at the time of deposition. 
1. the algorithms used by the system to detect dip patterns are beyond the scope of this paper. it is worth noting  however  that the textbook definitions given earlier do not provide sufficient specification. the problem is complicated by local dip variations and occasional gaps in the data. 
1. stratigraphic pattern detection: the system examines the dipmeter data for red  blue  and green patterns in zones of known depositional environment. 
+ 1. stratigraphic analysis: the system applies a set of rules that uses information from previous phases to draw conclusions about stratigraphic features  e.g. channels  fans  bars . 
for the phases shown above    +   indicates that the phase uses production rules written on the basis of interactions with an expert interpreter. the remaining phases do not use 
rules.1 
figure 1 shows a sample xerox 1 screen following the stratigraphic analysis phase. on the extreme right the system displays a summary log of dip magnitude for the entire well. the black box indicates the region of the well that is expanded in the second window from the right. this window shows the dipmeter data together with the deviation of the borehole itself. the next window displays two other logs  gr  gamma ray  and ild  a resistivity log . 
1. the rules obtained to date are due to j. a. gilreath of schlumberger offshore services  new orleans  la. the feature detectors and signal processing algorithms were written independently by project members. 
the 	system 	summarizes 	relevant 	conclusions 	in 	the 
 scrolling  windows in the lower left hand part of the screen. the user  a dipmeter interpreter  has selected a number of conclusions to be examined in greater detail and shown as annotations on the dipmeter log. also shown is the dip azimuth trend before and after structural dip removal.1 
1. on commercial expert systems 
in this section we summarize some of our observations on the development and evolution of the dipmeter advisor system. we discuss the nature of commercial expert systems as we see them in the coming decade  characteristics of the evolution process  development methods  and finally we compare our experience with some of the traditional wisdom of expert system development. 
1 embedded systems 
domain practitioners don't care which methods are used to help them solve their problems. what matters is utility and performance. indeed it is unlikely that traditional ai methods alone will solve real problems. they are likely to be augmented by techniques from signal processing and pattern recognition  to name but two possibilities. this implies that the knowledge engineer involved in commercial expert system development must be prepared to solve problems that involve a variety of disciplines and techniques. 
thus it is our view that the expert system kernel is likely to be only a  perhaps even relatively small  component embedded in a larger system. the particular suite of problems common to signal understanding problems may  of course  bias our outlook  but we believe that it is difficult to avoid the conclusion that acceptance and real use of expert systems depend on far more than a knowledge base and 
inference engine.1 
indeed our experience has been that these traditional parts of an expert system are not the predominant parts of the overall system either in terms of the amount of code or the resources required for system development. it is instructive in this regard to examine the relative amounts of code devoted to various functions in the dipmeter advisor system: 

this breakdown cannot be used  of course  as a direct measure of programming effort or as an indicator of where the system gets its power. it does  however  demonstrate the importance of a good programming language  given the relative amount of code that is devoted to the user interface. it also indicates that traditional programming skills will still be required for the development of commercial expert systems. 
1. the scrolling graphics code was written by paul barth. extensions to the interlisp-d menu package were written by eric schoen. 
1. oaschnig has made a similar observation in the context of the prospector system  oaschnig  1 . 
r. smith and j. baker 1 
1 system evolution 
based on our experience  we hypothesize an oscillating focus of attention in commercial expert system development projects. initially  the focus is demonstration of feasibility; acquiring the knowledge for a constrained problem and finding the appropriate set of expert system tools with which to encode and apply the knowledge. this phase could be relatively short. it is followed by a phase of expansion of the domain knowledge-during which the expert system tools remain relatively constant. there will likely come a point at which the intial tools do not provide sufficient power to allow continued expansion of the system's expertise. at that point  the focus will move away from domain problems and towards selection-more likely development-of new expert system tools. once a new set of more powerful tools has been constructed  then the focus will again return to the domain problems at hand. 
naturally any particular system may not pass through very many of these oscillations. the focus in the rl project  for example  didn't appear to oscillate at all  mcdermott  1. we believe this is due to the nature of the task. there was little of the uncertainty about the nature of the problem that is evident in the the signal understanding or diagnosis tasks. consequently the initial tools were in fact sufficiently powerful to handle the problem. 
in the mycin project we seem to be observing the beginnings of an oscillation. the initial system was constructed. then the rule base was expanded  leaving the initial expert system tools intact. more recently a new design  
neomycin  	has 	appeared--a 	new 	set 	of 	tools 
 clancey  1 . 
along with the oscillating focus  we hypothesize a performance vs time curve. we expect this curve to show periods of high positive slope-corresponding to implementation of new expert system tools  followed by periods of lower slope-corresponding to expansion of domain knowledge  followed by periods of level or even decreasing slope-corresponding to reaching  or surpassing  the amount of domain knowledge and generality that can be supported by the tools. 
at any given point in time  then  an expert system will suffer from two kinds of weakness  due to  i  insufficient domain knowledge; and  ii  inadequate expert system tools. just as the focus of the project will vary  depending on which of the two types of weakness is most troublesome  the type of person required to improve the system will also vary. 
improvements in the first area can be made to a large extent by people primarily knowledgeable in the domain  but not necessarily knowledgeable in the design of expert systems.1 for example  the dipmeter advisor system is familiar with a 
relatively small number of different lithologies. the performance of the system could be improved in this area without redesign. similarly  the coverage of the rules could be extended to handle more environments  or specialized to handle local anomalies. 
1. we have already noted  however  the likelihood that traditional programming skills will continue to be required. 

1 r. smith and j. baker 
weaknesses due to inadequate expert system tools cannot be corrected without redesign. this type of effort requires a person who can actually build expert systems  as opposed to one who can use the framework to expand capabilities. for example  the dipmeter advisor system deals with uncertainty in a rudimentary way. it uses rule order to help circumvent potential multiple interpretations for the same interval in the well  or simply draws multiple conclusions for the same zone  leaving the problem to be sorted out by the human interpreter. similarly  the system has a very local view of consistency in the vertical sequence. this is directly attributable to the fact that it is reasoning from sets of empirical rules and has no model of the underlying geological processes that lead to the rules. these deficiencies cannot be overcome without redesign. 
1 system development 
we have attempted a critical review of the development side of the dipmeter advisor system. although we are as yet unable to abstract a development methodology  several observations stand out. almost every major issue and decision in the evolution of the dipmeter advisor system addressed one or more of the following. 
1. demonstration of feasibility 
1. demonstration of utility and performance 
1. evaluation of utility and performance 
demonstration of feasibility: the problem of dipmeter interpretation was initially selected as a vehicle for investigating the applicability of expert systems techniques to well-log interpretation. until feasibility could be demonstrated  other questions were somewhat secondary. 
as a first step  a substantial effort was expended on acquisition of dipmeter interpretation knowledge. this effort was carried out over a 1 to 1 month period using standard techniques  protocols  video tape  discussion  representative examples  and so on . a single expert was studied in detail  again adhering to standard practice. 
the implementation of a prototype system followed data acquisition and was carried out in approximately four months  completed in december 1 . the rule dase and inference engine were written in interlisp  1 kbytes of source code  and ran on a dec 1. the user interface was graphical  written in fortran  1 kbytes of source code   and ran on a ramtek 1 connected to a vax 
1. the vax and 1 were linked via a chaosnet. the rule base was made up of aproximately 1 rules. there were also several feature detectors and signal processing algorithms. 
demonstration of utility and performance: the prototype system demonstrated to the expert that significant analyses were possible. to determine commercial viability  other issues must be addressed. does the system solve enough of the problem to be interesting and useful  can the system perform with the efficiency and interactivity necessary in a field environment without overutilizing available computing resources. 
two examples demonstrate the problem. the initial prototype  had no means of actually detecting the red and blue patterns and the lithology zones that are required to perform an unaided interpretation. it did not solve enough of the problem to be useful. this resulted in implementation of algorithms for simple detection of tadpole patterns and lithologic zones. 
second  the detection of green patterns and determination of structural dip took approximately 1 minutes in the first test well. this was unacceptable for actual use-later effort reduced the time to under 1 minutes. 
evaluation of utility and performance: this is the area of field evaluation  and several concerns exist here. first  is the rule base sufficiently complete to correctly solve a wide variety of problems in the geological environments for which it was developed. second  what changes and effort would be required when working in other geological environments  and third  does the rule base sufficiently capture the thinking of enough dipmeter interpreters to be useful  
to date  this has been the most difficult area. the above questions need to be answered by people in the engineering and field groups. to accomplish this  the prototype system must be capable of operating in their existing environmentpossibly upgraded with modest investment. 
one of the difficulties with the initial prototype was the unusual architecture of linked computers  which was not a standard company configuration. in an effort to facilitate testing  the system was reimplemented in franzlisp 
 except for the graphical interface   totally on the vax 
1. unfortunately this did not solve the problem. the vax/ramtek configuration  as a shared resource in a generally overloaded situation  required an excessively long time to complete a case. under worst conditions  it took several hours.  in an unloaded vax environment  it could be completed in one-half hour or less.  
at this point  new technology came to the rescue  and the system was re-implemented on the xerox 1  which has both a dedicated processor and sophisticated graphics. in this implementation the graphical interface code was integrated into the remainder of the system. the result was approximately 1 kbytes of interlisp source code. this implementation was robust enough and fast enough to allow transfer to a shlumberger interpretation engineering group for testing in a non-research environment. 
1.1 rapid prototyping and technology transfer 
in the beginning of a commercial expert system development project  it is important to demonstrate the feasibility of the system. rapid prototyping seems to be an appropriate strategy-especially given the usual vagueness of the understanding of what can be accomplished. 
1. somewhat surprising  our field organization seems to be prepared to believe that with enough effort  the system can be made sufficiently intelligent  but that system efficiency must be closely monitored. 

the main concern in such an approach is a flexible and powerful development environment. traditionally  such an environment is not even closely related to the commercial computational environment. this leads to the problems noted above. with the advent of inexpensive personal workstations  however  there is real hope that the situation may be changing  as has been our experience with the dipmeter advisor system . 
significant questions still remain. one of the problems of rapid prototyping is that it provides a good start toward system development  but does not offer clear guidance on how to produce a well-engineered commercial product  see  for example  sheil  1  . traditionally this is viewed as a problem in technology transfer. 
our experience with the dipmeter advisor system may suggest a different strategy. we have seen an evolution through successive refinement. through the different stages  functionality has changed  as well as the target systems  development environments  and personnel. basically  what we have seen at each stage is the introduction of new features  the solving of old problems  and the consolidation of existing code which does not require substantial change. 
is it possible that the traditional transfer from research to engineering may involve successive releases  corresponding to successive prototypes  if true  then we must somehow convey to our engineering organizations a more accurate perception of the expected lifetimes of our prototypes. in addition  this methodology suggests early transfer to engineering rather than late  with the expectation that several such transfers will be made. furthermore it forces the prototype designers to pay even more attention to user interfaces than our earlier figures would suggest. if the systems are going to be changing rapidly then they must have especially convenient and easy-to-learn interfaces. 
expert systems technology by its nature will be difficult to transfer. such systems require skills that are possessed by a very small number of individuals. furthermore  the rapid prototyping development methodology makes traditional transfer even more difficult-the systems are in a constant state of flux. as a result it is fair to say that for the foreseeable future  greater than normal responsibility will lie with the research organizations to ensure successful transfer. 
1 some observations on the traditional wisdom 
for the remainder of this section we consider a number of maxims of expert system development in the light of our experience in the commercial environment.  see  barstow  1    buchanan  1   or  davis  1  for good summaries of the traditional wisdom of expert systems development.  
a common maxim of expert system development is that we should throw away the code for the mark-i version of the system as soon as it demonstrates feasibility and get started on mark-il in the commercial environment  there is great reluctance to throw away code. as a result  a more likely scenario involves a series of progressive releases of the 
r. smith and j. baker 1 
system to the expert and possibly to the engineering organization for development and use. the fact is that even though the knowledge engineer knows all too well the limitations of mark-i  and even has ideas on how to overcome them  mark-1 may still provide some useful service. we do not yet know how to manage this type of progressive and evolutionary technology transfer. 
it is well accepted that expert system development is an incremental process. usually we understand this to mean that the performance of the system improves incrementally. there is  however  another kind of change that may occur; namely  our experts are themselves moving targets-partially as a result of the perspective gained through experience in expert system development! this has been apparent during the dipmeter advisor project. for example  we have seen an increasing geological awareness in our expert dipmeter interpreter. this has led to a series of changes in the way stratigraphic analysis is handled in the system. not all of these changes have proved useful-the expert appeared to be using the program at times as a test bed for his own evolving ideas. 
it is traditional wisdom that the task should be very carefully defined before the system is designed. our experience has been that this is quite difficult. in consonance with our comments on the rapid prototyping development strategy  it is not clear that task definition can be done in a rigorous fashion. we suggest a contingent definition-one that is clear for a time  but can be easily changed. we should note that the evolving performance of the system itself at least partially fuels changes in the task definition. 
it is generally accepted that construction of the mark-1 system should be commenced as soon as one example of the intended behavior is understood. we now believe that we spent too much time in knowledge acquisition before actually starting to build a system. this had the effect of slowing our rate of progress. we could not move forward in formalizing the knowledge that had been gained  because we could not demonstrate in concrete terms our understanding of it. 
some of the development team also deemed themselves to have acquired more expertise than was warranted. this is a natural tendency. it was partially due to infrequent interactions with the expert. more responsibility fell on the shoulders of the knowedge engineers to organize the domain knowledge than appears prudent. this infrequency also led to a problem of validation-how to be sure that we were on the right track. on a related note  we can testify to the necessity of an adequate set of generic examples with which to test the system as it evolves. 
one piece of traditional wisdom might be questioned. it is common to deal with a single expert during the development of an expert system. the perceived danger is that it is difficult enough to capture what a single expert is doing  let 
1. this is a good illustration of a conflict that can arise as a result of somewhat different goals of research and of development in expert systems. the former is concerned with continued exposition and machine implementation of human expert reasoning methods  while the latter is concerned with construction of products that utilize already understood and implemented methods. 

1 r. smith and j. baker 
alone a number of experts. in the particular context of dipmeter interpretation  however  it might have been useful to involve a number of different experts from the outset. we now understand that there are many schools of thought on the problem. there is also a variety of perspectives that can be brought to bear on it--dipmeter interpretation expertise and geological expertise are not necessarily co-located in the same person. while the rules for a first approach are most appropriately phrased by a dipmeter interpreter  we might have been well-advised to obtain the necessary geological vocabulary and structure from a geologist. in future systems  we will attempt to synthesize these overlapping points of view. 
in a similar vein  we have noted a difficulty that can arise when a single expert is used and when he provides all examples with which to test the system. when working with familiar examples our expert does indeed appear to apply forward-chained empirical rules-kind of compiled inferences. recently  however  we have participated in experiments with a number of interpreters  and examples  from around the world. during these experiments we noted that our expert resorted to a different mode of operation when faced with completely unfamiliar examples. he appeared to reason from underlying geological and geometric models-abandoning the rules. in some sense  this is of course to be expected. it was instructive  however  to actually document the change. we believe that dealing with multiple experts would have provided concrete evidence of this phenomenon much sooner in the life of the project.1 
we have also noted a lurking danger in dealing with experts. 
it appears to be possible to give an expert a false sense of comfort with a particular formalism  e.g.  rules . at times we had a sense that the expert was trying to make us happy by expressing what he was doing in terms of the rule framework we had offered-perhaps at the cost of accuracy. we would be well-advised to avoid over-reliance on the rule  or any other presently known  framework. we don't want to convince the expert that this simple idea covers everything he does  or that system failures are necessarily the result of incorrect or missing rules. 
with regard to acceptance of the expert systems approach  our experience has been somewhat different from that of the rl designers  mcdermott  1 ; that is  for rl there was general relatively rapid acceptance of the ideas within the organization. from early in the project concerns revolved almost totally around performance and utility in the problem domain. 
we have seen a substantial increase in the size of the rule base  approximately tripled  and the functionality required of the system before we could consider field evaluation. mcdermott has described a similar experience with rl. the size of its rule base tripled during the development phase  mcdermott  1 . 
the traditional wisdom notes the importance of early construction of a flexible user interface. for the dipmeter advisor system the interface is graphical. it has proved 
1. actually seeing the change in reasoning was further complicated by the fact that our expert has extremely broad experience. hence  rinding a completely unfamiliar example was quite difficult. 
invaluable in testing and user acceptance. furthermore  as has been noted elsewhere  buchanan  1   expert systems that are actually used by people trying to solve problems in their own domains of interest  as opposed to being used by researchers as vehicles for experimentation with ai techniques  must pay particular attention to human interface issues. for the dipmeter advisor system  it was only after we constructed a personal workstation implementation that was flexible  robust  and fast that it became possible to seriously consider testing by the schlumberger engineering organization. 
one final observation worth noting relates to the impact of an expert system on the domain experts. as has been found in other applications of expert systems 
 feigenbaum  1   the existence of an expert system is helping to identify the real knowledge used in the field-the kind of knowledge that is rarely found in textbooks. a program that captures some of it at least gives a concrete basis for comparing the methods of different experts. as gaschnig has noted  gaschnig  1   it can also help a group to reach some form of consensus. the dipmeter advisor system has stimulated .an examination of current dipmeter interpretation methods that promises to improve quality. 
1. conclusions 
the current dipmeter advisor system has provided substantial demonstration of the feasibility of using expert system techniques in commercial well-log interpretation. additional analysis and evaluation of the system will certainly further define the the strengths and weaknesses of its approach. the experience gained to date has also helped to suggest characteristics of commercial expert system development as well as properties of a development methodology. 
acknowledgements: the latest implementation is a product of the efforts of a number of people. they include: dave barstow  paul barth  jim callan  roger duffey  tony passera  paul 
pruchnik  and eric schoen. a number of helpful suggestions were made by stan vestal  and by bob langley and bob young  both with houston interpretation engineering . barstow  vestal  j. a. gilreath  and tom mitchell also made helpful suggestions on this paper. 
