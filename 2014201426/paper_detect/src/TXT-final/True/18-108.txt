 
     about fifteen years ago  the hand eye system was one of the exciting research topics in a r t i f i c i a l intelligence. several ai groups attempted to develop i n t e l l i g e n t robots by combining computer vision with computer controlled manipulator. however  after the success of early prototypes  research efforts have been splitted into general intelligence research and real world oriented robotics research. currently  the author feels there exists a significant gap between ai and robotics in spite of the necessity of communication and integration. thus  without building a bridge over the gap  ai w i l l lose a fertile research field that needs real-time realworld intelligence  and robotics w i l l never acquire intelligence even if it works skillfully. in this paper  i would like to encourage ai community to promote more efforts on real world robotics  with the discussions about key points for the study. this paper also introduces current steps of our robotics research that attempt to connect perception with action as intelligently as 
possible. 
i. introduction 
     about fifteen years ago  hand eye system was one of the exciting research topics in artificial intelligence. several ai groups concentrated large efforts to create their prototype intelligent robots by combining computer vision with computer controlled manipulator. the performance of early hand eye system was quite limited. vision system could recognize only simple block world. control algorithm of manipulator was also simple and sensory interaction was very primitive. many researchers who succeeded in building up the prototype concluded that individual subsystem should be pursued further before designing new system. and  research efforts have been devoted to explore general aspects of vision  planning  and control. since then  the attempt of system integration has been left behind in ai community. 
     although early hand eye systems could carry out only simple jobs in very simple block world  its success presented promising perspectives to manufacturing industries. during the last decade  industries have concentrated their efforts to advance industrial robots  and they succeeded in introducing robots into various production lines. the progress of hardware implementation including microelectronics is remarkable. and now  industries are keen to implement intelligent capability into their robots. 
     robotics is the study of i n t e l l i g e n t connection of perception to action. l  although artificial intelligence involves many disciplines  it is the study of intelligence from the standpoints of computation. thus  both robotics and ai can share common interests. however currently  the author feels there exists significant gap between them. that is  ai deals mainly abstracted world  while robotics concerns real world  and the interests of those approaches seem to pass each other. as robotics invol ves many difficult problems which would not be solved without ai approach  it is very important to build a bridge that connects ai to robotics. in this paper  the author will discuss about key problems for integrating several disciplines into an intelligent robot system. and  current steps of our robotics research w i l l be introduced as an example for building a bridge that connects robotics to ai. 
i i . key problems 
     robot is a versatile intelligent system that can interact with real world through sensors and effectors. generally  it is consisted of four major subsystems; perception subsystem  action subsystem  thinking subsystem  and user friendly interface. perception subsystem recognizes situations of real environment by means of vision  force sensing  and touch sensing. action subsystem changes environment situation by moving objects or walking around. thinking subsystem makes a plan of robot behavior  monitors its execution  and evaluates the results. user interface provides communication channel between human operator and robot. during last decade  a large number of researches on v i s i o n   manipulation  and planning have been done  and the performance of each subsystem has progressed very much. however  research on system integration has been left behind. we must remind that more studies of system architecture of robot are required if we wish to make robots smarter. 
     i would like to stress the importance of system oriented research of robotics  because 

1 h.lnoue 
intelligent interaction of perception with action in real world is the main concern of robotics. it opens very interesting and promising research field for not only robotics but also ai. general priciples of intelligence gained from ai research would help us to synthesize total robot system. however  the problems which have been dealt with in theoretical ai research are oversimplified when we consider its application to dealing with complex constraints in real world. on the other hand  researches on robot mechanisms  control and sensors have been focussed to advance physical performance  and lack semantic aspects of robot behavior. although there exists such a gap between ai and robotics  attempts to connect them together will be needed to explore fertile research field where intelligent machines can behave in real world. 
     there are so many d i f f i c u l t problems to be studied for creating smarter robot system. before entering into each discussion  we have to consider the level of complexity of real environment to be dealt by several disciplines. considering current performance of each related field  i propose a world of simple electro-mechanical assembly. object shapes are supposed to be generated from general blocks and cylinders. objects may have holes and threaded holes. screws  gears and wires are also included in the repertoire of objects. 
     major problems to be studied are listed below with short comments and discussions. 
a. abstracted definition of manipurator system 
     so far  research interests on manipulator system have been focussed onto the theory of advanced dynamic control for fast and precise motion. on the other hand  ai program assumes manipulator as very simple static object mover that always succeed in precise motion. control discipline prefers aspects of dynamics  and ignores semantic aspect. ai approach pays its attention on semantic aspects of action rather than dynamic control. thus  the interests of the two approach pass each other. in order to bridge over the gap  it seems necessary to provide ai people with an abstracted framework of manipulator control system so as to encourage them to construct smarter control structure. for this purpose  the author proposes a reasonable sets of abstracted functions for manipulator system. the seven basic functions are: put-arm-reference  position  orientation  get-arrn-coordinate  position  orientation  put-hand-opening  opening-width  put-grasp-force  grasping-force  get-hand-opening  opening-width  get-wrist-force  force  moment  get-touch-sensor  sensor-state  above seven functions are defined in abstracted coordinates system  and are free from actual mechanical configuration. they work as if they were the instruction sets for computer. the cycle time is supposed to be very short  equivalent to the sampling period of servomechanism. each function completes its operation in a few mili second. for instance  if we trigger put-arm-reference with the arguments about reference position and orientation in 1d world coordinates  then this function calculates corresponding 1 joint angles and sends the data to joint servo within single sampling period. 
     it is also necessary to provide a flexible means to combine the functions together. for such framework  i propose a fast real time operating system with concurrent process execution capability. if we connect above mentioned function sets by means of the concurrent real time os commands such as start-process  stop-process  dofunction  signal  wait  delay  and so on  we can describe any sensor based operations of manipulator.   1   i would like to encourage ai community to describe semantic aspects of manipulation on the above mentioned abstraction. 
b. theory of manipulation 
     sensor based robot programs are very d i f f i c u l t to write in general form. so far  several basic robot operations such as pin-intohole  block-in-corner and rope-into-ring have been demonstrated. however  most of those program synthesis are based on ad hoc strategies  where geometric structures of parts relationship are assumed a priori  and lack generality. lozanoperez et al pointed out that small changes in the geometry of parts can have significant impact on fine-motion strategies  and they proposed a formal approach to the automatic synthesis of a class of compliant fine-motion strategies. their approach uses geometric descriptions of parts and estimates of measurement and motion errors to produce finemotion strategies.  mason presented a theoretical explanation of the mechanics of pushing and demonstrated application of the theory to the analysis and synthesis of manipulator operations.  those approach open a quite important novel f i e l d for future robotics research. more efforts should be devoted to the general study of manipulation strategy  in order to explore theoretical foundations for the program synthesis of complex sensor based manipulation. 
c. control of visual attention. 
     although we are not conscious in every instance  we employ so many kinds of visual attentions during task execution  to find objects  to monitor situation changes of environment  to guide the motion of object  and to verify preconditions as well as postconditions for every piece of action. if we can provide a robot with fast and flexible capability of such visual attentions  the level of intelligent interaction of robot with real world will evolve dramatically. however  neither ai oriented vision research nor sensor based robotics research can t e l l very l i t t l e about this problem. can we formulate basic repertoire of visual attentions  what is the good framework to relate visual attentions to actions  
how should we synthesize the control strategies of visual attentions  those questions seem to overlap the previous discussions on theory of manipulation. general study on visual attentions and their control strategy would provide one of the unexplored rich research field for connecting perception with action in intelligent way. 
d. automated planning in robot language. 
     the main purpose of high level robot language is to simplify the programming of complex robot tasks. we can classify robot language into three levels such as manipulation oriented  parts o r i e n t e d   and goal oriented. like al  manipulation oriented language requires to describe  robot tasks in explicit manipulation procedures. in parts oriented language  the task is described as the state transition sequence of geometrical relationship between parts. rapt is an example of this kind of language. tt does not require explicit description of coordinate data of object motion in source code  instead  it attempts to solve those data from the descriptions about the logical constraint expressions among surfaces and axes of objects. goal oriented robot language requires only the goal description in source code  and generates action sequence by means of automated planning technique. in order to discriminate parts oriented language from goal oriented language  we assume the former takes complete state transition sequence that covers a l l subgoals so as to avoid the use of problem solver. from the viewpoint of programming simplicity  the goal oriented language is the ultimate goal of robot language development. but  the performance of problem solving today is too primitive to cope with complex robot environment. tt deals very simple state description like  on a 
b   while actual robot environment needs more complex state descriptions as rapt shows. moreover  object structures such as arch or tower which are dealt by current problem solver are far from actual assembly environment in its complexity. the author hopes at w i l l promote researches on powerful problem solver that can 
deal with complex real world constraints. 
e. sensor based environment model management. 
     presumably  the environment model management system would be a core of an integrated intelligent robot system. it manages accumulated data base about the description of current environment situation  conceptual definitions of various structures and objects to be handled  and general strategies for primitive operations. studies on robot language t e l l us the environment model management plays very important role in compiling manipulation procedures. every piece of manipulation changes the environment situation in some extent. therefore  when language processor compiles source codes of robot tasks  it must simulate situation changes in order to generate right codes with right data. the higher the performance of model management is  the easier the 
h.lnoue 1 
programming becomes. tf no model management is provided  we are forced to write complicated task program keeping a l l the details of situation changes in mind. 
     if the i n i t i a l state of environment is not known  the scene analysis program would be invoked to recognize the real world and the results are returned to the model. during the execution phase of planned action sequence  the execution monitor can know the expected situation changes in advance  and compares it with actual environment changes. doing the motion under inconsistent circumstance between real world and its computer model sometimes causes a serious damage. in order to avoid such disaster  the consistency between the two should be checked a l l the time by means of sensors. generally  the capacity to recover from action errors reflects the level of intelligence of robots. suppose a situation where a robot drops object during the move and the object hits a tower under construction. recovery from this accident needs the following procedure. first  robot must aware the happening by vision. next  the damaged situation is recognized by scene analyzer  which is considered as the i n i t i a l state. the goal state can be obtained from the model as the state description before that accident. problem solver plans an action sequence that converts the initial state to the goal state. it is really difficult to implement such a robot system even for simple block world  however this example covers most of the robot component which should be equipped in future robots with high intelligence. model management system is related to many aspects of robotics such as scene analysis  robot language  automated planning  motion execution  and control of visual attention. finding a common rich framework that can be employed compatibly in those aspects would be the most important problem. 
i i i . cosmos: an example of integrated system 
     this chapter introduces cosmos  an example of interactive programming environment for the study of intelligent connection of perception to action in real world. 1 cosmos is  in another word  a lisp system that can interact with real environment by means of vision and manipulation. 
figure 1 shows hardware organization of 
cosmos. we have two arms. one is a small universal arm driven by seven dc motors. single board microcomputer which includes servo routine  communication routine  and diagnostic routine controls this arm. trajectory data and control parameters are supplied from host minicomputer through gpib interface. another arm is a conventional 1 axes industrial robot of dc motor drive. as this arm can handle large pay load  it is used to move heavy experimental attachments such as tv camera or multiple axes wrist mechanism. simple touch sensors and 1 axes force sensor are also interfaced to the system. as visual input device  we employed precision tv camera. video 

1 h.lnoue 
signal is digitized and stored into image frame memory device consisting of 1 pixels with 1 bits intensity scale. host computer system consists of a 1 bits minicomputer  data general eclipse mv/1 with 1 mb memory  and a 1 bit minicomputer  eclipse s/1. 
     figure 1 shows software configuration of cosmos. currently  nine software modules are imbedded into top level lisp programming environment. brief summary of each module is described below. 
model management part of al/l is written in frl to explore ai oriented study of robot language. we have also attempted to extend robot language into parts oriented and task oriented level. in order to describe stractural constraints of objects  we adopted rapt like representation. providing conceptual description of goal structure such as arch or tower in rapt like notation  and employing a simple problem solver  our task oriented language plans action sequence to perform a given goal like  build arch   and generates the al/l source code for the task.  

top level lisp is the core of cosmos system. 
until 1  we used eclisp interpreter which was implemented on eclipse s/1. eclisp had a serious address space limitation caused by the 1 bit architecture of the s/1 processor. in order to solve this limitation  we decomposed a large program into several processes and connected them together by means of the i n t e r process communication facility of aos   advanced operating system of eclipse . in 1  we updated our host computer to a 1 bit virtual address machine eclipse mv/1  and lisp system is also updated to lisp/mv and its successor etalisp  both of which are originally implemented in c on vax at electrotechnical laboratory.  1   eclisp  lisp/mv and etalisp are designed upward compatible  so  a l l the early programs implemented on eclisp s t i l l run on our current lisp system. 
     manipulation system has been built up as a hierachy of task level robot language  al/l language  trajectory calculator  arm control system  and servo controller  from top to bottom. 
al/l is a manipulation oriented language. the syntax of al/l is similar to al which is developed at stanford  but it is implemented in lisp. the 

     vision system is conventionally devided into two modules; vision primitives  and 1d vision. the former includes a set of primitive image processing functions such as data aquisition  image operators  feature extracters  planer logic operators  and display functions. the latter covers recognition level programs. robot vision requires three dimensional recognition. we are mainly studying on stereo vision that analyses binocular images. as ai oriented vision research  we are exploring two competitive approaches for scene analysis. one is a frl based line finder  in which frames control visual recognition process and finds right line drawings for simple blocks. another attempt employs production system for simple scene analysis. 
     cosmos is an open purpose project. we are just climbing up step by step towards smarter intelligent robot system. when i initiated this project in 1  i intend to create general purpose research tool for exploring intelligent connection of perception to action. at f i r s t   we implemented lisp interpreter. then  we developped al/l robot language onto the lisp. next  we developped general purpose robot arm and interfaced it to al/l. almost at same time  we added primitive vision f a c i l i t y to our lisp system. thus  we succeeded in constructing simple hand eye system within lisp environment. it actually provided us very convenient research environment for intelligent robotics. after a system was integrated  we enjoyed cosmos very 
much  updated it  and accumulated new features of robotics  year by year. we do not think that cosmos configuration is the best solution. rather  we consider that cosmos reflects our research history and future direction in computer executable form. 
	iv. 	hand eye experiment on cosmos 
     this chapter introduces one example of hand eye experiment which is performed on c1sm1s. the experiment is on rope handling  which is a d i f f i c u l t task to do without visual feedback  because a flexible rope cannot hold a well defined shape. 
     as explained earlier  f u l l cosmos is a toolbox which covers a l l our robotics software. it is really convenient  but sometimes it is heavy for a particular experiment. in order to improve 

h.lnoue 1 


run time efficiency  cosmos software is rearranged and tuned for the rope handling experiment. the tuned system consists of top level lisp  vision subsystem  arm subsystem  and hand eye calibration. vision subsystem involves three modules; eye  watch  and stereo. 
     top level lisp: top level lisp allows us interactive hand eye programming. command sets are tuned for this experiment. robot language al/l is not employed. instead  adequate arm commands which directly manages arm control system are built in. all the following vision modules can be invoked by lisp commands. 
     eye: this module provides low level vision primitives  such as image operators  feature extracters  logic operators  and display functions. usually  vision functions are applied on the window area of 1 pixels. this module is used for rough  global analysis of a scene. 
     watch: in order to perform fine local analysis efficiently  scan-line based image processing is employed. when we use linear scan  the position  direction  and length of the scanline are controlled. when we use circular scan  the center and diameter of scan c i r c l e are controlled. zero crossing operator along those scan extracts precise edge point on the scan line. this module provides a simple and efficient visual recognition for feedback. 
     stereo: in order to obtain stereo image  we attached mirror adapter onto tv camera. it composes binocular image onto single tv frame. left half of the frame corresponds to left eye image  and right half of the frame corresponds to right eye image. both images are analysed separatedly  and the coordinates of corresponding points on both images are calculated. then  three dimensional position of the point is calculated by the principle of triangulation. 
     hand eye calibration: in order to mate manipulator system with vision system together  we must know precise numerical relationship between the two coordinate systems. it is called hand-eye calibration. this module calculates precise trnsformation matrix between the two coordinates by means of vision facility automatically. 
     the experiment includes a task of rope-intoring and tying a knot around the ring. the scenario is as follows. at f i r s t   robot finds a rope by vision and grasps it. next  robot finds a ring and puts the rope into ring by visual feedback. then  robot release the rope  regrasps the rope from opposite side of the ring  and pull the rope out. finally robot ties a knot around the ring. figure 1 shows flow diagram of the experiment. the block number in circle indicates vision processing  while the block number in square indicates manipulation. the experiment itself will be shown by movie. 

1 h.lnoue 
	v. 	vision system with multiple attentions 
     the use of visual monitoring  visual feedback  and visual verification is necessary in execution phase of action plan. they accomplish various interaction between vision and action in real time. in most cases  above mentioned vision program can be constructed so as to focus its attention onto local regions in which the existence of key features are expected. usually  points of attention should be multiple  although processing for each attention is simple. as the processing time of such visual interaction is very c r i t i c a l for real time use  those multiple attentions must run in parallel. in order to realize such kind of visual interaction in real time  we designed and implemented new versatile multi window vision processor. a rectangular local region to be processed is referred to as a window. in our system  the location  shape  and resolution of each window can be independently controlled by hardware. 
     figure 1 shows a block diagram of multi window vision system. it consists of one digitizer/transmitter unit and a large number of window processing units. digitizer/transmitter unit converts ntsc composite color signal into digital video signals and broadcasts them onto the video bus. the signals on the video bus are r  g  b  and y  each of which has 1 bits intensity scale. during the video scan  pixel address increments in video scanning rate within 1 pixel area. in order to inform the current scanning position  the horizontal and vertical pixel address are also broadcasted onto the video bus. by observing the current pixel address  each window unit picks up only the necessary data inside the designated window area  and stores them 

in its window memory  which is processed by a microcomputer independently. 
     the signal on the video bus is one way  and read only. so  a large number of window unit can aquire the window data independently and simultaneously. in our current implementation  window memory has only 1 kb. it corresponds to 1 pixels if it is used as square window. memory size is small  however its usage is very flexible. firstly  the location of each window is controllable. so  we can locate each window anywhere inside tv screen. secondly  the aspect ratio of each window is programmable. so  we can change window shape into square  rectangular  and linear. thirdly  spatial data resolution  in another word  pixel sampling rate is variable. we can read every pixel data  one pixel data from every 1 neighbors  from every 1 neighbors  or from every 1 neighbors. if we use low resolution mode  we can obtain rough data over large window area. if we need precise data  we use high resolution mode for small window area. finally  we have four kinds of input signals such as red  green  blue  and brightness. we can choose one signal from the four. 
the prototype model has sixteen windows. 
four window memories are processed by single 
motorola 1. and four such processor units are connected to host computer. image processing programs which run on window processor are written in c. they are compatible to vision primitives module of cosmos. host machine is also motorola 1  on which lisp interpreter runs. 
thus  recognition level program written in lisp controls individual window processing  following the same philosophy of cosmos design. 
     when we designed multi window vision system  we expected to operate one window as one visual demon. therefore  multi window vision system provides very flexible hardware for pandemonium models for visual recognition. we plan to control a large number of visual demons in knowledge invoked way in order to explore sensor based environment model management system for robot. in such application  we need a large number of window hardwares. therefore we attempted to develop special lsi. we have already succeeded in developping lsi chip for window data acquisition. we are now developping large scale multi window vision system by using that lsi. 
	vi. 	concluding remarks 
     research on intelligent robot was born in a r t i f i c i a l intelligence more than twenty years ago. however  after the development of early hand eye systems  very few efforts have been continued on system oriented research of i n t e l l i g e n t machines that behave in real world. the author believes that many key problems for intelligent robot could not be solved without merging ai approach with robotics. therefore  in order to encourage ai community to pay more attentions to 
h.lnoue 1 
intelligent robotics  several key points are discussed from the viewpoint of system oriented approach. then  an attempt to build a bridge that connects ai to robotics is presented with an example of cosmos experimental system. so far  we have concentrated our efforts onto building a lisp programming environment that has manipulators  vision systems  robot language and other sensors. using this total system  we are going to explore a method to connect perception with action as intelligently as possible. 
acknowledgement: 	cosmos is developped at 
information systems laboratory  in the department of mechanical engineering  the university of tokyo. the author wishes to express deep appreciations to the past and present students who contributed to implement cosmos: t.ogasawara  o.naito  o.shiroshi ta  t.matsui  h.mizoguchi  
s.kawagoe  	m.inaba  	h.matsubara  	m.fujita  
a.okano  y.tsusaka  y.murata  t.fukuizumi  k.kado. 
