 
ensemble methods such as bagging and boosting have been successfully applied to classification problems. two important issues associated with an ensemble approach are: how to generate models to construct an ensemble  and how to combine them for classification. in this paper  we focus on the problem of model generation for heterogeneous data classification. if we could partition heterogeneous data into a number of homogeneous partitions  we will likely generate reliable and accurate classification models over the homogeneous partitions. we examine different ways of forming homogeneous subsets and propose a novel method that allows a data point to be assigned multiple times in order to generate homogeneous partitions for ensemble learning. we present the details of the new algorithm and empirical studies over the uci benchmark datasets and datasets of image classification  and show that the proposed approach is effective for heterogeneous data classification. 
1 introduction 
　ensemble approaches such as bagging and boosting have been successfully applied to many classification problems  dietterich  1; bauer and kohavi  1 . the basic idea of ensemble methods is to construct a number of classifiers over training data and then classify new data points by taking a  weighted  vote of their predictions. thus  two important issues associated with an ensemble approach are: 1  how to generate accurate yet diverse classification models  and 1  how to combine the models for ensemble classification. diverse classifiers ensure good ensembles  quinlan  1 . in this paper  we focus on the first issue with an emphasis on heterogeneous data classification. heterogeneous data classification refers to the problem when input data of a single class are widely distributed into multiple modes. it arises when training data are collected under different environments or through different sources. an example of heterogeneous data classification is image classification  in which labeled images are acquired from multiple resources and exhibit disparate characteristics. for instance  some images are black and white  and others are colorful.  
　a widely used approach for constructing an ensemble of models is to sample different subsets from the training data and create a classification model for each subset. bagging  briemann  1  and adaboost  schapire and singer  1  are two representative methods in this category. bagging randomly draws samples from the training data with replacement and adaboost samples training data according to a dynamically changed distribution  which is updated by putting more weight on the misclassified examples and smaller weights on the correctly classified examples. clearly  both methods do not treat homogeneous data and heterogeneous data differently.  
　for ensemble methods to work effectively on heterogeneous data  one intuitive solution is to first divide the heterogeneous data into a set of homogeneous partitions and then to create a model for each partition of data. member classifiers built with different homogeneous partitions will likely result in good diversity of an ensemble. one way to realize this homogeneity-based partition is to employ standard clustering algorithms  such as k-means  hartigan and wong  1  and the em clustering algorithm  celeux and govaert  1 . an example is the gaussian mixture model  gmm . but  in general  there are two problems with this simple clustering approach:  
　　  single cluster membership. most clustering algorithms assume that cluster membership is mutually exclusive and each data point can only belong to a single cluster. even though the em clustering algorithm allows soft membership for a data point  in the resulting clusters  each data point still only belongs to a single cluster  witten and frank  1 . therefore  when we use these clustering algorithms to partition data  if the number of clusters is large and the subsets of training data formed by a clustering algorithm are mutually disjoint  some clusters may have a very small number of data points  which can lead to unreliable classification models. this is similar to the data fragmentation problem occurred in decision tree induction  quinlan  1 . in contrast  the subsets of training data produced by bagging and adaboost are not mutually disjoint. for example  in bootstrap sampling  each subset contains around 1% of the original training data.  
　　  unbalanced cluster sizes. since most clustering algorithms do not have control over cluster sizes  unbalanced cluster sizes resulting from clustering cannot be easily corrected. when the resulting clusters have very different sizes  a classifier built over a small cluster can be unreliable and thus degrade the performance of the ensemble in forming final ensemble classification. on the contrary  both bagging and adaboost have data samples of similar sizes when learning different models. note that there have been previous efforts on balancing the sizes of different clusters  particularly for spectral clustering algorithms  e.g.  the normalized cut algorithm  melta & shi  1  . but  since the control of cluster size comes indirectly from the objective function  the resulting clusters can still have unbalanced sizes.  
　in sum  a clustering approach may produce homogeneous data partitions  but cannot ensure similar sizes of different partitions; methods like bagging can produce equally sized partitions  but partitions are not homogeneous. therefore  we need a novel approach to partitioning data into homogeneous subsets of similar sizes in ensemble learning for heterogeneous data classification. 
　the goal of this work is to divide heterogeneous data into homogeneous subsets of similar sizes in order to generate reliable and accurate classification models. by focusing on homogeneous subsets  we do not require that each data point belong to one subset; by ensuring similar sizes of data subsets  each classification model can be built with a similar number of data points. in this paper  we propose a hiss  homogeneous data in similar size  algorithm specially designed for the above purposes for heterogeneous data classification. specifically  hiss allows the user to specify the size of a subset. for example  the user can ask the algorithm to create 1 subsets with each containing 1% of the original data. this algorithm is similar to the bootstrap sampling procedure in that both the number of subsets and the percentage of training data covered by each cluster can be specified and varied. however  it differs from the simple bootstrap sampling procedure in that it puts the similar data points into a single subset while bootstrap sampling randomly selects data to form a subset. this property is important in ensemble learning for classifying heterogeneous data. we will use strata for the homogeneous data partitions  and subsets for data partitions resulting from random sampling. 
1 	related work 
there have been many previous studies on how to create an ensemble of models. the methods for constructing an ensemble of models can be categorized into five groups  dietterich  1 : 1  bayesian methods  which creates an ensemble of model by sampling them from a estimated posterior model distribution; 1  sampling training examples  which creates multiple subsets of training examples and trains a classifier for each of the subsets; 1  sampling input features  which creates a number of subsets of the input features and a classifier is built for each subset of input features;  1  error correct output code  ecoc   which convert a multiple class problem into a set of binary class problems; 1  injecting randomness  that generates ensembles of classifiers by injecting randomness into the learning algorithm.  
　among the five categories  our work is closely related to the second one  which creates multiple classifiers by sampling training examples. important methods in this group include bagging  brieman  1  and adaboost  schapire and singer  1 . although these methods have been shown to be effective for classification  they are not designed to take into account characteristics of heterogeneous data. in this paper  we propose hiss -an algorithm that constructs homogeneous strata from heterogeneous data while maintains the nice property of boostrap sampling procedure - each stratum contains a similar number of data points.  
　another line of research closely related to this work is the study of clustering algorithms. in general  clustering algorithms can be categorized into parametric approaches and non-parametric approaches. the parametric approach is to find a parametric model that minimizes a cost function associated with instance-cluster assignments. such methods include the mixture model  celeux and govaert  1  and k-means algorithm. for the non-parametric approaches  a cost function is minimized by either merging two separate clusters into a larger one or dividing a cluster into two smaller ones. the representative examples of this category are the agglomerative approach and the divisive approach.  
　most clustering approaches assume that each data point only belongs to a single cluster. this assumption may not be appropriate since the ultimate goal of clustering is to group similar data points together. when it is uncertain to assign a 
　data point to a single cluster  it is better off assigning it to multiple clusters. although the traditional probabilistic model and the fuzzy clustering algorithm allow for multi- or soft-memberships  the uncertainty of cluster membership is only exploited during the process of estimation. in the resulting clusters  each data point is assigned to only a single cluster. furthermore  most clustering algorithms do not have any control over the size of clusters. hence  the resulting clusters can be very unbalanced in size and the clusters of too small sizes could be useless in learning.  
1 	the hiss algorithm for model generation  
1 from probabilistic clustering to hiss 
　we first describe the traditional probabilistic clustering algorithm  and then introduce algorithm hiss. 
　the general idea of probabilistic clustering is to describe data with a mixture of generative models. optimal parameters are usually obtained by maximizing the likelihood of data using the mixture model. let n be the number of input data points  k be the number clusters  {x1  x1 ...  xn} be the input data  and {m m1  1 ... mk } be the underlying models that generate the data. by assuming that each data point is generated from a mixture of models{m m1  1 ... mk }  we have the likelihood of the data written as: 
l {mi}kj=1   τ =‘ ‘n log   k	j p x  i | mj     	 1  τi
	i=1	  j=1	 
where p x  i | mj   is the likelihood of generating xi from the model mj   and τij is the likelihood for data point xi to be in the j-th cluster. based on the assumption that each data point can only belong to a single cluster  we have constraint 
. an example of probabilistic clustering is the 
gaussian mixture model  gmm   in which both τij and p x  i | mj   are parameterized as: 
τ θij = j  and
	1	   xi  μj  t Σ j1 xi  μj    	 1  
p x m  i |	j   = 1π d /1/1 exp    	1	   
|Σj |
where θj denotes the prior for the j-th cluster  and μj and Σ j are the mean and variance matrix for the j-th cluster  respectively. expectation and maximization algorithm  em   dempster et al  1  can be used to search for the optimal parameters.  
　by removing the constraint   we allow each data point to belong to multiple homogeneous clusters  or in short  strata. hence  the optimization problem becomes n	  k	 
	max  {j l	mi}kj=1   τ =‘ ‘log  τij p x  i | mj   
	mi  τi	i=1	  j=1	 
	subject to 	 	 1  
	1 ＋τij ＋1  for i =1 ...    and  n	j =1 ... k
where all τij are constrained to between 1 and 1 to maintain the probability interpretation. it is easy to see that the optimal solution is to set all τij to be 1  which means that each data point is included in every stratum. 
　to avoid the trivial solution for τij   we choose to enforce the percentage of training data that are covered by each cluster to be a predefined constantγ  i.e.  
		  1  for j	1 ... k 	 1  
n
with the above constraint  we guarantee that the number of data points that support each stratum is aroundγn .  
　compared to the single membership constraint  this new constraint has the following two advantages: 1  it does not assume that each data point has to belong to one stratum. for this new stratifying method  on average each data point can belong to γk number of strata. therefore  when γk is larger than one  each data point is allowed to be in more than one stratum simultaneously. 1  it ensures that different strata have balanced numbers of data points. in contrast to most clustering algorithms  the new algorithm ensures almost the same size for each stratum. this is particularly important to the research goal of this paper - generating a reliable and accurate ensemble for heterogeneous data. by setting γ to be a reasonably large value  1 in this work   we ensure that each stratum has a sufficiently large number of examples for building a statistical learning model. for later reference  we refer this new clustering approach as  hiss   which stands for homogeneous data in similar size.  
1 optimization for hiss 
putting equations  1  and  1  together  we have: 
	max  {j l	mi}kj=1   τ =‘ ‘n log  k τj p x  i | mj    
　　　　　　　　　　 	i mi  τi i=1   j=1  
	subject to	 	 1  
		  1  for j	1 ... k 
n
	1 ＋τij ＋1  for i =1 ...    and  n	j =1 ... k
let us assume the gaussian distribution for p x  i | mj     i.e.  p x m  |	j   ~ n μj  σj   . following the idea of the em algo-
rithm  the difference in the likelihood of data between two consecutive iterations is bound by: 
	l	m t	t	l	m t	t
	−    τiτj  jt   +t 1     +	 	 1  
i
  p x  i | m tj   +1   
	 	 
   p x  i | m tj        
where υij is defined as 
	 	 1  
thus  the optimal solutions for the mean and variance of gaussian distribution can be obtained as follows: 
n υj    t xi	n υij    t xi1 i
μj  t	1 	 σj  t	1 	μj  t	1 
however  the optimal solution for τij is rather difficult to obtain because of the inequality constraints 1 ＋τij ＋1 . directly optimizing the equation  1  with only the equality constraint will result in the following solution for τij  t +1  : 
	τj  t + =1  υij    t γn 	 1  

apparently  the above solution will always be nonnegative if  τij    t is nonnegative. however  it does not guarantee that  τij  t +1  is not greater than 1.  
finding optimal τij  t +1  
 
inputs: υij    t for i =1 ... n and j =1 ... k 
outputs: τij  t +1  that maximizes equation  1 .  initialization:  
	τij  t + =1 	1 for i =1 ... n and j =1 ... k 
 
for each cluster j    do 
        for all examples i   
	set τij  t + =1 	1 if τij  t +  1 	1 
        compute the probability mass 
	s =  γ τn	{i | ij  t + =1 	1} 
       re-compute  
 
	while   j s.t. τij  t +  1 	1  
end 
 
figure 1: algorithm for finding optimal τij  t +1  
　in order to satisfy the inequality constraints 1 ＋τij ＋1   we use the kkt conditions  fletcher  1  to efficiently adjust the value of τij  t +1  . the basic idea is to reset τij to be 1 whenever the output from equation  1  violates the constraint 1 ＋τij ＋1 . after the adjustment  we will recompute τij  t +1  that are less than 1 using equation  1 . the procedure of adjusting and recomputing τij  t +1  will continue until no τij  t +1  violates the constraint. figure 1 shows the detailed steps for finding the optimal solution for τij  t +1  . due to the space limit  the proof for the optimality of the algorithm in figure 1 is not provided here.  
1 classifying heterogeneous data 
　for classification problems  heterogeneous data can be found in many applications and in experiments:  
1  data acquired from multiple sources. in many cases  training data are acquired from multiple sources. because each source has its own data distribution that may be different from others  the data merged from multiple sources are therefore heterogeneous. for example  consider building a classification model for outdoor scenes. the training images are collected from several different types of videos. some of the videos are news stories and some of them are of advertisement. some of them are of high quality and some of them are not. thus  the widely disparate characteristics in videos cause the merged data to be heterogeneous. 
1  data by converting a multiple class problem into a set of binary class problems. in order to apply the binary class classification algorithm to multiple class case  we need to 
data set # examples #class # featuresecoli 1 1 1 pendigit 1 1 1 glass 1 1 1 yeast 1 1 1 vehicle 1 1 1 image/indoor1 1 1 image/outdoor1 1 1 table 1: description of datasets for the experiment for heterogeneous data classification. 
convert the classification problem of multiple classes into a set of binary class problems. the representative examples include the one-against-all approach and error correct output coding  ecoc  method  dietterich  1 . during this process  multiple classes are grouped into two subsets of classes. data points from one subset of classes are used as positive examples and the remaining are used as negative examples. because both the positive and negative pools can be comprised of examples from multiple classes  it will create data heterogeneity for each of the binary classes. 
　as discussed  an intuitive solution to classifying heterogeneous data is to create a set of classification models with each classifier built on a homogeneous partition  stratum  of the data  and then combine classifiers for the final prediction.  the traditional clustering algorithms are not designed for this task because of the potential unbalanced clustersizes and the data fragmentation problem. with the proposed algorithm hiss  we can avoid these two problems by setting the parameter to be large  1 in the experiment .  
　in sum  to classify heterogeneous data  we first apply hiss to obtain homogeneous strata and then create a classification model for each stratum to form an ensemble. we will refer to this model generation method as 'hiss-based model generation' in our empirical study next. finally  a stacking approach  wolpert  1  is used to combine models that are generated by the hiss-based model generation method for the final prediction of the ensemble. 
1. experimental study 
the experimental study is designed to answer the following questions: 
1  is the proposed model generation method effective for classifying heterogeneous data  to this end  we compare the proposed model generation method to bagging and adaboost in classifying heterogeneous datasets.  
1  is the proposed hiss algorithm effective for generating reliable models  to address this question  we will apply both the proposed hiss algorithm and the probabilistic clustering algorithm to partition the training data and build a classification model for each partition.  
1 experimental design 
data set baseline adaboost     standard   	bagging      standard  hiss-based ensemble  bagging   stacking  adaboost   stacking  ecoli 1  1  1  1  1  1  1  1  1  1  1  1  pendigit 1  1  1  1  1  1  1  1  1  1  1  1  glass 1  1  1  1  1  1  1  1  1  1  1  1  yeast 1  1  1  1  1  1  1  1  1  1  1  1  vehicle 1  1  1  1  1  1  1  1  1  1  1  1  image/indoor 1  1  1  1  1  1  1  1  1  1  1  1  image/outdoor 1  1  1  1  1  1  1  1  1  1  1  1  table 1: classification errors for the baseline model  svm   adaboost  bagging and the propose model generation method  'hiss-based ensemble' . the column 'bagging  stacking ' refers to the case when the ensemble of models is created by the bagging algorithm but combined through the stacking approach using an svm. the same is for the column 'adaboost  stacking '.  the variance of classification error is listed in parenthesis. seven different datasets are used in the experiments: five multiple class datasets from the uci machine learning repository  blake and merz  1  and two binary class datasets for image classification. the characteristics of these seven datasets are listed in table 1. 
  for the multiple class datasets  we introduce the heterogeneity into the data by converting the original multipleclass problem into a binary one. similar to the one-againstall approach  examples from the most popular class are used as the positive instances and examples from the remaining classes are assigned to the negative class. because data of the negative class are from multiple classes  we would expect some degree of heterogeneity inside the negative class. for the two datasets of image classification  they both are binary classification problems. the heterogeneity of data is due to the fact that images are from seven different video clips and each video clip provides 1 images. since each video clip is of different type  e.g.  varied quality in images   we would expect certain amount of heterogeneity within the data.   
　the baseline algorithm used in this experiment is support vector machine  burger  1 . in all the experiments  each ensemble method generates 1 different svms; a stacking approach  wolpert  1  that also uses a svm is employed to combine the outputs from all 1 models to form the final prediction of the ensemble. for each experiment  we randomly select 1% of the data as training and the remaining 1% as testing. the experiment is repeated 1 times and the average classification error of the ten runs is used as the final result with the variance of classification errors.  
1 heterogeneous data classification 
table 1 shows classification errors for the baseline support 
data set hiss em 
 1 clusters  em 
 1 clusters ecoli 1 1  1  1  1  1 pendigit 1 1  1  1  1  1 glass 1 1  1  1  1  1 yeast 1  1  1  1  1  1 vehicle 1  1  1  1  1  1 image/indoor 1 1  1  1  1  1 image/outdoor 1 1  1  1  1  1 table 1: classification error for using different clustering algorithms for model generation. 'em' refers to using expectation-maximization algorithm to cluster data.  vector machine  the proposed hiss-based ensemble learning approach  standard bagging and standard adaboost. first  we can see that the baseline model performs well comparing with both standard bagging and adaboost. this observation indicates that these seven heterogeneous datasets are rather difficult for the standard ensemble approaches to learn. in contrast  the proposed hiss-based ensemble method performs better than the baseline model and the two standard ensemble methods. for the datasets 'glass'  'vehicle'  and 'image/outdoor'  the improvement is substantial  from 1% to 1% for 'galss'  1% to 1% for 'vehicle'  and from 1% to 1% for 'image/outdoor'.  
　since the hiss-based ensemble method uses the stacking approach for combining different models  it is different from the combination method that is used by adaboost and bagging. to address this difference  we conduct the experiments that apply a stacking method to combine the models generated by both bagging and adaboost. the results are listed in table 1 on the right side of the hiss-based approach  titled as 'bagging  stacking ' and 'adaboost  stacking '  respectively. compared these results to the results of 'bagging  standard ' and 'adaboost  standard '  we see that there is no substantial change in classification errors when using a stacking approach to combine models in ensemble learning. for all the seven datasets  the ensemble of models generated by hiss performs the best. the reason why a stacking approach is useful for the hiss-based model generation method but not to the other two is that models generated by the hiss-based algorithm are much more diverse than the ones generated by both bagging and adaboost. as a result  applying another layer of classification model to combine the outputs from the distinguishable models  or stacking  will be able to take full advantage of all the models and obtain the best performance. 
　based on the above discussion  we conclude that the hiss-based ensemble model is more effective for classifying heterogeneous data than existing ensemble approaches. 
1 comparison with other clustering-based ensemble methods 
the advantage of hiss versus the traditional clustering algorithms is that hiss allows each data point to be in multiple different strata. thus it can ensure that the number of data points distributed over each stratum is of similar size and sufficiently large. 
　in this experiment  we use both the traditional clustering algorithm and the proposed hiss algorithm for model generation and see how different they are in classifying the heterogeneous datasets. to observe the effect due to the tradeoff between the number of strata and the number of data points in each stratum  we consider two different numbers of strata  or clusters  for the traditional clustering algorithm: 1 and 1  「1/γ . we did not use 1 clusters in the comparison because for some datasets the traditional clustering algorithm is unable to produce the full twenty clusters. the traditional clustering algorithm used in the experiment is the probabilistic em clustering algorithm. similar to the hissbased ensemble approach  a stacking method is used to combine models generated by the em clustering algorithm. the results for using em clustering algorithms for model construction are listed in table 1  titled 'em  1 clusters ' and 'em  1 clusters '. as suggested by table 1  the increasing number of clusters can lead to degraded performance. this is because a large number of clusters will form clusters with a small number of data points  which can be insufficient for building a reliable classification model. on the other hand  as already indicated in the previous study  ditterich  1   being able to generate a relatively large number of models is critical to the success of the ensemble approach. the proposed hiss algorithm can satisfy both needs by introducing the substantial overlapping between different clusters. as shown in table 1  the hiss-based method outperforms the em clustering-based ensemble approaches substantially for almost all datasets except for 'yeast'  similar . the most noticeable cases are 'ecoli' and 'pendigit'  for which the classification errors of em-based clustering approaches are one order more than that of the hiss-based ensemble algorithm. 
　based on the above experiments and analysis  we conclude that the hiss-based model generation is an effective method for model generation in ensemble learning for heterogeneous data classification. 
1. conclusion and future work 
　in this paper  we propose and examine a new method for generating an ensemble of models  which is to first partition data into homogeneous subsets and then create a model for each subset. a traditional clustering algorithm like em is not suitable for the task of partitioning data due to potential size-unbalanced clusters and the data fragmentation problem. to address these two problems  we propose a novel algorithm hiss  which allows for data overlapping between different clusters  strata  and promises size-balanced clusters. empirical studies over seven different heterogeneous datasets have shown that this new hiss-based model generation method performs very well for heterogeneous data classification. currently  the proposed hiss algorithm assumes equal size for each stratum  cluster . one possible extension is to examine alternatives to balance sizes of clusters. for example  instead of enforcing all the clusters to have one size  we can constrain the sizes of the clusters into a specified range to allow some flexibility in maintaining high homogeneity of clusters. 
