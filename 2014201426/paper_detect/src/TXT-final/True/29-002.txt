 
many of the challenges faced by the field of computational intelligence in building intelligent agents  involve determining mappings between numerous and varied sensor inputs and complex and flexible action sequences. in applying nonparametric learning techniques to such problems we must therefore ask:  is nonparametric learning practical in very high dimensional spaces   contemporary wisdom states that variable selection and a  greedy  choice of appropriate functional structures are essential ingredients for nonparametric learning algorithms. however  neither of these strategies is practical when learning problems have thousands of input variables  and tens of thousands of learning examples. we conclude that such nonparametric learning is practical by using a methodology which does not use either of these techniques. we propose a simple nonparametric learning algorithm to support our conclusion. the algorithm is evaluated first on 1 well known regression data sets  where it is shown to produce regression functions which are as good or better than published results on 1 of these data sets. the algorithm is further evaluated on 1 large  very high dimensional data sets  1 learning examples of 1  1  1  1 and 1 dimensional data  and is shown to construct effective regression functions despite the presence of noise in both inputs and outputs. 
1 	introduction 
nonparametric learning algorithms are designed for learning problems where relatively little information is available a priori about the what type of model structure is required  or which set of variable inputs are important. such problems are common in many domains of computational intelligence. as a simple example  consider the problem of autonomous navigation in a cluttered environment. as researchers we must decide which features  inputs  in the environment are important  and 
1 	learning 
peter d. lawrence 
dept. of elect  and comp. eng. 
university of british columbia 
vancouver  b.c.  v1t 1  canada 
peterl ee.ubc.ca 
http://www.ee.ubc.ca/~peterl 
what kind of computational models are required. nonparametric learning techniques are of potential value to us because they are designed to define their own model structure  and to determine which set of inputs are most relevant to effectively predict the task output. there are a number of effective non-parametric learning techniques in the literature  including c1  quinlan  1   cart  breiman et a/.  1   and mars  friedman  1   see  michie et a/.  1  for a more complete list . 
　almost without exception  contemporary implementations of nonparametric learning algorithms have two common properties. first  some form of variable selection is done to determine which of all possible inputs best predict the output. and second  they utilize some type of  greedy  search procedure which attempts to pick the best model structure  from some set of possible structures. such techniques are feasible when there are relatively few input variables    1 . however  neither of these two algorithmic properties are practical when large numbers of potential inputs are available. consider the learning example where there are 1 potential inputs  and assume that not all of these inputs are necessary to predict the output. in order to pick the best 1 of these 1 inputs  one would need to construct 
 models  and then evaluate them all 
to see which set of 1 inputs best predict the output. such an exhaustive search is currently not feasible  and is unlikely to be in the near future. similarly  if we use some greedy search procedure to determine which of all possible model structures is the best  we would further increase our search space by the total number of possible model structures. 
　certainly  the flexibility of nonparametric learning recommends it as a useful tool for constructing intelligent agents. this has been clearly demonstrated in numerous publications  michie et a/.  1 . however  in almost all applications of nonparametric learning  the number of inputs have been relatively low    1   and the number of learning examples have been relatively few     1 . thus  given that many challenges faced by the field of computational intelligence in building intelligent agents  involve determining mappings between numerous  thousands or more  and varied sensor inputs 

and complex and flexible action sequences requiring tens of thousands of training examples  we must ask whether nonparametric learning can be usefully applied to such problems  given our argument in the previous paragraph  the initial response to this question is no: the computational complexity associated with applying existing algorithms to such large problems makes them impractical. in fact  all currently published nonparametric learning algorithms  utilize search strategies which are impractical in high dimensional spaces. even algorithms such as projection pursuit  friedman and stuetzie  1   which do not perform traditional exhaustive searches  build models using all input variables simultaneously  making them impractical for very high dimensional problems. 
　in an effort to develop methods to address the high dimensional mappings required for intelligent agents  we challenge the need for variable selection and greedy function search in nonparametric learning. instead  we propose an algorithm which has two basic characteristics: first  very little computational effort is spent on variable selection; and second  the model selection strategy does not explode in computational complexity  as the dimension of the problem increases. our intent is to show that a model having these properties is capable of building practical  effective nonparametric models  for both small low dimensional learning problems  as well as larger  very high dimensional problems. 
　the basic premise behind the proposed algorithm is that very high dimensional regression can be done using a finite number of low dimensional structural units  which are added one at a time to the regression function. these structural units are by necessity low dimensional  because high dimensional structures suffer from the curse of dimensionality  see  friedman  1  for a detailed discussion . the inputs to new structural units can be the outputs of previously added units as well as input variables. however  both the choice of which structural unit to add next  and which inputs will act on these structural units  must be done using minimal computational effort for the algorithm to be practical in high dimensional spaces. 
　in the current paper  we implement and evaluate the simplest algorithmic structure which is consistent with the ideas outlined above. the proposed algorithm has a 
　very simple structure  using only one type of structural unit  and variables are added to the model in a random order. the surprising conclusion of this paper is that this simple  in essence random structure  produces highly effective nonparametric models. 
　much of the work in computational learning has been directed towards the classification problem. however  many real problems in az- involve controlling agents which manipulate the world using continuous values control signals. hence  in this paper we have chosen to directly address the continuous valued regression problem  instead of the discrete valued classification problem. however  as discussed in  friedman  1   classification problems can be solved using regression techniques in a number of ways. therefore the high dimensional regression techniques studied in this paper are  at least in theory  applicable to classification problems. 
　in the remainder of this paper we give a detailed description of the proposed algorithm  followed by an extensive numerical evaluation. our evaluation is done in 1 parts. first we compare the performance of our algorithm to published results on 1 well known regression problems from the literature. this gives us a good measure of the performance of the proposed algorithm on some standard regression problems. next we test the algorithm on 1 large very high dimensional  highly nonlinear regression problems. this allows us to test the efficacy of the algorithm on the type of problems we are most interested in addressing. our final evaluation of the algorithm is on the 1-bit parity problem  or equivalently the 1 input xor problem . this problem was chosen to show that the proposed algorithm is capable of solving problems which have an intrinsic dimension  1 inputs  greater than that of its highest dimensional functional unit  1 dimensional . a common criticism of algorithms which build functions using low dimensional structural units is that they cannot model high dimensional xor functions. in this paper  we demonstrate that this is not true for the proposed algorithm. 

	grudic & lawrence 	1 


1 	learning 

table 1: low dimensional data sets 


s 	results and discussion 
1 	low dimensional data sets 
in this section we evaluate the efficacy of the proposed algorithm by applying it to 1 well known regression problems from the literature  table 1 . 
　for the data sets found in  breiman  1  and  rasmussen  1   the regression functions were constructed using 1 fold cross-validation: the learning data set was divided into 1 approximately equally distributed sets  and then 1 regression functions were constructed using  in turn  1 of these sets as training sets  and the remaining set as a validation set. for each test data point  test data was not used during learning   the outputs of the 1 regression functions were averaged to produce the final approximation output  for which error results are reported. to test reproducibility  1 independent approximations  independent with respect to random sequences of input variables and bootstrap samples as defined in section 1  were generated using the learning data: table 1 reports the best test set error  along with the average and standard deviation  s.d.  over the 1 independent runs. 
　for the data sets found in  breiman  1   we followed the reported experimental setup. for each of the 1 data sets  1 learning and 1 test sets were created randomly according to the guidelines given in  breiman  1 . for each of the 1 learning sets a regression function was constructed  using 1 fold cross-validation as described above  and evaluated on the corresponding test set. experimental results for the 1 experiments are given in table 1. the previously published error for the  breiman  1  data refers to the average bagged error reported. 
　for the data sets found in  rasmussen  1   we report results for the largest learning sets only  we used 1 learning examples of auto price data  1 of cpu data  1 of housing data  1 of mpg data  and 1 of servo data . for each of these 1 fixed data sets  the regression functions were constructed using 1 fold cross validation as described above. we generated 1 independent approximations in order to determine what effect the stochastic aspect of the proposed algorithm has on its performance. as reported in section 1  the ordering of the independent variables is random  and the construction of the 1 dimensional functions gi   is done using a  random  bootstrap sample of the training data. the best and average relative mean squared test set error  and its standard deviation  are reported in table 1. from table 1  it is evident that although there is some variation in error performance from run to run  the stochastic effect of the algorithm is mostly negligible. the previously published error given in table 1 under the  rasmussen  1  data sets is the best reported error  indicated by brackets  of the 1 algorithms evaluated  and was obtained from the graphs presented in the paper. for both the  breiman  1  and  rasmussen  1  data sets  the average learning time ranged from about 1 to 1 minutes per approximation  all learning times reported in this paper are for proposed algorithm running on a pentium pro 1 using linux . the average size of a single cascade was about 1 k-bytes. 
　finally  for the forward dynamics data reported in  jordan and jacobs  1   our evaluation was done using the experimental setup described by jordan and jacobs for the on-line back-propagation algorithm: learning was done using 1 training examples  and learning stopped when the error could no longer be reduced on 1 validation examples. the regression functions for this data consisted of a single cascade per output: due to the large data size  1 fold cross validation was not necessary. in order to estimate the reproducibility of proposed algorithm on this data  we constructed 1 independent approximations. the best and average relative error on the validation set  in accordance with  jordan and jacobs  1    and its standard deviation over these 1 independent experiments  is reported in table 1. the previously published error  shown in table 1  is the best relative error  on the validation data  of the 1 algorithms studied in  jordan and jacobs  1 . the forward dynamics data consists of 1 inputs and 1 outputs  which requires 1 of our regression functions  1 for each output . the algorithm required about 1 hours of computation to build all 1 cascades  and the average 
	grudic & lawrence 	1 

size of each cascade was about 1 m-byte. 

figure 1: no noise data: increase in approximation 
size with dimension 
　from table 1  it is evident that the proposed algorithm demonstrated as good or better error results on all but 1  the servo data  of the data sets. however  this result should be interpreted with caution. the specific goal of the proposed algorithm is to build a regression function which best represents the learning data in the mean squared error sense. the regression function continues to grow in parameter size until the mean squared error can no longer be reduced: this is beneficial if one wants the  best  approximation  but detrimental if one 
wants a representation of fixed size. most algorithms referred to in table 1 are parametric and therefore of fixed size. two exceptions are mars and cart. however  even comparing these to the proposed algorithm should be done with caution: unlike our algorithm  both mars and cart can be used to analyze the significance of various inputs and how they interact with one another. 
1 	high dimensional data 
　in this section  our goal is to study the proposed algorithm when applied to very high dimensional regression data  under various noise conditions. since no large  high dimensional regression data examples were found in the literature  we applied the proposed algorithm to 1 artificial data sets ranging from 1 to 1 input dimensions. for 1 of these data sets no noise was present  while the remaining 1 data sets had noise. for the first 1 of these  noisy  data sets  noise was present only in the output  while the remaining 1 data sets had both input and output noise. input noise was generated by 
1 	learning 

figure 1: output noise data: increase in approximation size with dimension 
simply allowing only half of all inputs to contribute to the output  thus half of the inputs used in constructing the regression function had no effect on output . output noise was generated using a normal distribution with the standard deviation selected to give a signal to noise ratio of 1 to 1  thus implying that the true underlying function accounts for 1%. of the variance in learning and test data. 

1 1 1 1 1 
data dimension 


tion  1  allows us to generate highly nonlinear data of any dimension  while at the same time controlling the complexity of the data via the appropriate selection of functions ri .  and si   . by data complexity  we are referring to the number of random sample points required to build sufficient non-parametric models of the data  with respect to least squared error. the more complex the generating function  the more data points are required for non-parametric modeling. for the purposes of this paper  we have chosen functions ri .  and si    such that 1 random sample points are sufficient in order to effectively model the data. for specific details on how ri .  and s    are constructed see http://www.ee.ubc.ca/~gregg for c code and documentation. 
　the simulation results for the 1 high dimensional data sets are presented in table 1. for each data set there are 1 learning examples and 1 testing examples. each learning set is further divided into 1 training examples and 1 validation examples: these are used to construct one cascade which forms the approximation for that data set. the 1 right most columns of table 1 contain the test set average relative mean squared error and corresponding standard deviations over 1 independent runs. relative mean squared error is defined  in the usual sense  as the mean squared error on the test data  divided by the variance of the test data. from table 1 it is evident that the relative error is small when no noise is present. with the addition of noise the relative error approaches the theoretical limit  due to the 1 to 1 signal to noise ratio  of 1. learning time for each data set with no noise was approximately 1 hours and produced cascades which were between 1 and 1 m-bytes in size. in contrast  learning time for each data set containing noise was about 1 hour and produced approximations of between 1 and 1 k-bytes in size. it is interesting to observe that the dimension of the data did not affect either learning time or the size of the approximation. this is clearly demonstrated in figures 1 through 1  which show an increase in approximation size  as a function of the number of input variables. note that the no noise data generated approximations which where much larger than those generated by data which had noise. this is because the proposed algorithm stops adding levels to the regression function when error can no longer be reduced on the validation set. this condition occurs much earlier when there is noise in the training data  hence generating much smaller regression functions. 
1 	1 input x o r problem 
because the proposed algorithm constructs approximations 1 dimensions at a time  one may be lead to conclude that it is not capable of solving xor problems of 1 or more inputs  however  this is not the case. the proposed algorithm was able to solve the 1 input xor problem in 1 minutes of learning time  the size of the resulting regression function was 1 k-bytes . the theoretical basis for this surprising fact is the subject of ongoing theoretical study  however  we postulate that it is the direct result of our use of bootstrap samples during learning. 
1 	conclusion 
we have demonstrated that nonparametric learning is practical for very high dimensional learning problems. we did this by proposing a simple nonparametric learning methodology  which neither requires computationally expensive variable selection  nor an expensive search procedure to find the best model representation. an algorithm based on this methodology was successfully applied to several high dimensional learning problems  which no other currently published algorithm could effectively address. the size of the regression functions produced by the proposed algorithm depended on the learning data's complexity  and not on its dimension. our current ongoing efforts are being directed to a more complete theoretical analysis of nonparametric algorithms having these properties  as well as the application of our methodology to real world agents. in addition  we are interested in exploring the efficacy of the proposed methodology as an analytical tool for exploring the relative importance of input variables  and for defining confidence intervals for model predictions. 
