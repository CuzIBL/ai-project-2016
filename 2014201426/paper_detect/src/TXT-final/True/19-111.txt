 
while one can characterize deep and shallow models at a high level of abstraction and contrast their relative merits in a general way  this provides little direction for knowledge engineering. in particular  the field lacks a clear definition of 'knowledge depth' and lacks guidelines regarding the appropriate depth of models for a given application  in this paper we provide a very simple operational definition of knowledge depth' and use it to examine the opportunities for 
varying depth in intelligent safety systems. the paper illustrates a domain-independent mode of analysis for examining progressively deeper models of expertise  and sketches some domain-specific guidelines for constructing intelligent safety systems. we draw upon examples from the domains of nuclear reactor management  chemical plant control  and management of computer installation operations. 
1. introduction 
several authors have noted the distinction between 'deep' and 
'shallow' models of expertise in expert systems  e.g.  hart 1  chandrasekaran 1  fink 1 . by shallow models we usually mean that conclusions are drawn directly from observed facts that characterize a situation. an advantage of shallow models is that they directly encode the heuristics that experts use in performing their reasoning tasks  and are thus relatively easy to build. in addition  shallow models tend to be relatively efficient because they select rather than construct their solutions. one disadvantage of shallow models  however  is that explicitly stating all the preconditions under which a solution should be selected is an error prone process. another weakness of shallow models is that they are inflexible  unable to deal with circumstances even slightly different from those explicitly anticipated  de kleer & brown 1 . in addition  shallow models may be difficult to maintain  since what is conceptually a single piece of knowledge may be unsystematically distributed across several objects in a knowledge base. finally  explanations generated from shallow models tend to be limited to traces of the chains of inference that lead to conclusions. 
in contrast  deep models of expertise correspond more closely to the notion of reasoning from first principles. they tend to be more robust than shallow models  handling problems not explicitly anticipated and exhibiting higher performance at the periphery of their knowledge. in addition  it can be easier to verify the completeness of deep models. for example  in device-centered models of physical systems  e.g.  de kleer & brown 1  davis 1  each physical device maps directly into a structured object in the representation. deep models of expertise are also more useful for generating explanations in that reasoning steps which are usually implicit in shallow models can be elucidated. deep reasoning is  however  bound to be slower and more complex than shallow reasoning in that a more sophisticated controistructure is required  koton 1 . 
abstractly characterizing deep and shallow models contrast their relative merits in a general way provides little direction for knowledge engineering  in particular  the field lacks a definition of exactly what makes a model 'deep' and lacks guidelines 
1  this research was supported in part by a grants from the army 
research office and the nasa graduate student researchers program. 
regarding the appropriate depth of models for a given application. we believe that such guidelines should be developed by abstracting from a large set of examples. in particular  we advocate the approach ot  i  adopting an informal definition of 'knowledge depth'   ii  isolating high-level reasoning tasks  e.g.  diagnosis  simulation  for analysis  and  iii  for each such reasoning task  contrasting the merits of models inspired by various domains which vary in their relative depth. 
in this paper we sketch such an analysis for a reasoning task called provide a safety function'  which is defined in section 1  in order to achieve two objectives:  i  to illustrate a domainindependent mode of analysis for examining progressively deeper models  and  ii  to make the analysis available to those interested in building systems which provide safety functions. the paper is organized as follows. section 1 provides a simple operational definition of 'depth' that is used in the ensuing analysis. in section 1 we define the reasoning task 'provide a safety function' and identify two subtasks which present opportunities for varying depth of reasoning  which are analyzed in sections 1 and 1. our conclusions are presented in section 1. 
1. an operational definition of knowledge depth we need a simple relation that will distinguish the depth of models of expertise for a given reasoning task. our focus is on the explicit representation of knowledge in models of expertise  although other notions of depth which highlight  for example  multiple perspectives on a domain  e.g.  davis 1  or notions of causality  e.g.  rieger 1  are also potentially valuable for this purpose. the following is intended only as an informal  operational definition of the deeper-than relation which leaves terms such as 'knowledge' and 'model' to intuition. 
definition: consider two models of expertise m and m'. 
we will say that m' is deeper-than m if there exists some implicit knowledge in m which is explicitly represented or computed in m  
the deeper-than relation is defined over an infinite space of models of expertise for a given reasoning task. in cases where a reasoning task is decomposed into isolated subtasks which present opportunities for varying depth  the relation is applied to subtasks rather than to the composite task. for example  consider a task t which may be naturally decomposed 'into subtasks t1 and t1. we address the relative depth of models for these subtasks rather than for t  for if we build a model x and a deeper model x' for t1  and build a model y and a deeper model y' for t1  then the composite models for t consistina of {x y'} and {x' y} are not strictly ordered by deeper-than. this occurs in intelligent safety systems as described in the next section. 
1. intelligent safety systems and knowledge depth there has recently been great interest in intelligent systems that represent and reason about physical devices  bobrow 1 . one line of research concerns the development of facilities which provide advice or take direct action in response to system disturbances  e.g.  underwood 1  nelson 1  ennis 1 . of these  we focus on systems which provide safety functions in physical systems. providing a safety function involves executing plans which circumvent potential crises in physical system environments. 
	klein and finln 	1 
in nuclear power operations  this encompasses executing 'a group of actions that prevent melting of the reactor core or minimize radiation releases to the public'  corcoran 1 . while the term 'safety function' originated in the context of nuclear facility management  we can identify applications of the same idea in other domains  including preventing a chemical reactor in a process plant from catching fire  and preventing the depletion of operating system queue space in a large computer installation. providing safety functions in such process environments may be considered an expert-level task  and we will refer to systems which employ models of expertise for providing safety functions as intelligent safety systems  iss . 
generally speaking  an iss receives a description of the state of the system being controlled  the target system  as input and provides a plan of action for circumventing a crisis as output  te work of an iss may be naturally decomposed as follows: 
    monitor target system state variables to detect potential crisis conditions and to ascertain the status of plant components   ii  determine  possibly several  alternative plans for preventing a crisis  plan determination    iii  evaluate these alternatives to select the best one  plan evaluation   and  iv  execute or display the chosen plan. 
our first task in examining the relative merits of models which vary in depth for a given reasoning task is to identify opportunities for varying depth  in the sense of section 1  that rovide some potential advantages  in the sense of section 1 . or iss's  monitoring and execution are relatively straightforward operations  but plan determination and plan evaluation may be accomplished in a number of ways that vary in their relative depth of reasoning. the next step in the analysts involves defining and evaluating progressively deeper models for performing each of these subtasks. 
1. reasoning depth in plan determination we examine four progressively deeper models that may be employed to determine plans to prevent a crisis: invoking hardcoded plans  determining plans based on hard-coded paths of components  generating plans based on system structure  and generating plans based on system structure and component behavior. 
1. invoking hard-coded plans 
the shallowest model of expertise we consider involves hardcoding plans for preventing a potential crisis under various conditions. this model is conveniently implemented in formalisms that encode situation-action pairs such as production rules. as an example  consider jesq  klein 1   one of several rule-based systems that comprise yes/mvs  ennis 1   an expert system for managing large computer installations. jesq's task is to maintain a comfortable' level of unused space on an operating system queue  i.e.  to provide the safety function 'prevent queue space depletion'. the antecedents of jesq s rules describe the states under which the hard-coded plans in their consequents should be performed. for example  the rule in figure 1 encodes the plan to enable a printer to print large jobs so that queue space may be freed. in effect it enables a path of data flow from the queue to the printer. 

these plans  which specify the movement of data from the queue to other components  e.g.  tape drives  printers   are based on the structure of the underlying computer system being modelled  but this structure is only implicitly represented in jesq. as such  jesq suffers from some of the disadvantages of shallow models. for example  the configuration of the computer system may be changed  requiring modifications to this and other rules  but there is no 
1 	knowledge representation 
systematic way of identifying such modifications. in addition  jesq's rules may omit reference to conditions in the computer system which do not usually occur  but which occasionally render encoded plans unsuccessful. another limitation of the system is that explanations can offer little more than a presentation of the conditions under which plans are applicable. finally  jesq can handle only precisely those state conditions that have been anticipated. 
according to our definition  a deeper model would explicitly represent the potential paths of data flow  although deeper does not necessarily imply better. for example  if configuration changes are unlikely or if the encoded preconditions are appropriate most of the time and are of manageable volume  the benefits of a deeper model of plan determination expertise might not justify the cost of its construction or the overhead of its execution. 
1. determining plans from hard-coded paths the next model to be considered involves explicitly representing sets of components which may be employed to provide a safety function. this approach is taken in reactor  nelson 1   which provides safety functions in a nuclear reactor facility. 
potential paths of components that can be used to cool the reactor core are encoded in a response tree as shown in figure 1. each path in the tree contains an instance of each of the functional components required to provide the safety function  e.g. water source  heat sink . part of reactor's mission is to select  in real time  a path composed of components that are correctly operating. 

figure 1: response tree  nelson 1  
the primary advantage of this approach is its robustness. not all potential combinations of component failures need be explicitly anticipated  since these are coordinated by the response tree structure and associated logic. another advantage is that a change to the configuration is more easily mapped into the representation  since paths of components are explicitly represented. still implicit however  is the configuration description from which response trees are constructed. since the response tree is hard-coded  only those potential paths of components explicitly identified in advance are candidates for selection  and configuration changes require that the resulting new paths be identified by a knowledge engineer. a still deeper model would reason directly from a schematic to generate the potential paths. however  for applications in which the number of potential paths is manageable and the structure of the target system is relatively stable  we might not be inclined to consider a deeper model. 
1. generating plans from system structure 
the next model that we consider involves explicitly representing the configuration of the target system and using this description to generate plans for circumventing a potential 

crisis. as an example  we again refer to jesq's domain  section 1  as cast in the structural representation of figure 1. using this model  plan determination adopts the form of searching a graph in which nodes represent components and edges represent their interconnections. the search always begins at node queue and terminates at node user  and each such path through the graph represents a candidate path through which datasets may flow in order to clear the queue. 

figure 1: structural model of computer installation 
this representation has the advantage that a change in the configuration may be directly mapped into a change in the representation  and the knowledge about computing plans for moving datasets  searching the graph  remains unchanged. it is also more portable than the other representations  requiring only a configuration description for any particular installation. this model is deeper than the rules of section 1 which only implicitly represent the queue-clearing paths. this model is also deeper than the response trees of section 1 because we have hard-coded the mechanism by which paths are generated rather than the paths themselves. 
the queue space domain permits the convenience of uniformly treating each represented device in the system  because each device is capable of accepting and storing data. in domains that encompass devices whicn do not exhibit this behavioral homogeneity  however  we would require a still deeper model in order to generate plans. specifically  we would need to explicitly reason about the behavior of components  since not all components would play the same functional role in providing a safety function. a similar point is made by ginsberg  1  regarding diagnostic systems. 
1. generating plans from system structure and behavior 
the deepest model of plan determination we consider is based on an explicit representation of target system structure and component behavior. this approach is taken in several systems that perform other reasoning tasks such as simulation  de kleer & brown 1   troubleshooting  davis 1   and verification  barrow 1 . 
as an example of reasoning from structure and behavior to provide a safety function  consider the following hypothetical iss. using a spatial representation as in figure 1 
 stephanopoulous 1  and descriptions of component behaviors  the iss generates alternative plans for keeping the reactor  rxr1  from catching fire when components fail. for example  if the valve  v1  that regulates the coolant flowing to the reactor jacket becomes stuck closed  the iss searches for a compensating action to lower the temperature of the reactor. the search proceeds both forward and backward from the reactor to yield the following alternative plans: close v1. close v1  request a to stop feed stream  request a to lower feed stream temperature  request cool temperature material from the heating system  ana request the heating system to stop. for example  the iss identifies the action close v1' as follows. the feedstream input to rxr1 must be such that the temperature of rxr1 is normal. since v1 is stuck closed  the iss must reduce either the temperature or the flow rate of the feedstream. searching backward from rxr1  the iss examines the behavioral model of valve v1  noting that in state closed the flow rate from the valve is zero. the iss searches a list of potential actions to find that action 'close valve' causes v1 to enter state closed. thus  the plan 'close v1' is a candidate for execution to be assessed by the plan evaluation process. 

figure 1: chemical reactor subsystem  stephanopoulos 1  
these components  heat exchangers  reactor  valves  are not behaviorally homogeneous  so reasoning about how to maintain a safe temperature in the reactor must encompass consideration of the individual behaviors of the components that impact it  as well as their position in the system structure. this model is deeper than the solely structural model of 1 in that the behavior of components is explicitly represented and reasoned about. 
1. varying depth of reasoning in plan evaluation given a set of alternative plans for providing a given safety function in a particular situation  plan evaluation involves selecting the 'best' one. we discuss three significant levels of depth for evaluating alternative plans: hard-coded evaluation  explicit priorities   evaluation using utility theory and hardcoded decision attributes  computation of priorities   and evaluation using utility theory and decision attributes which are themselves computed from a structural and behavioral model of the target system  computation of priorities and of underlying attributes . 
1. evaluations encoded as priorities 
the shallowest representation of plan evaluation that we consider is the hard-coded numeric priority. this is a commonly employed approach; for example  a priority is associated with each rule in jesq and with each path in the response tree in reactor to provide for choosing the best when more than one are applicable. 
in rule-based systems like jesq  priorities specify relative preferences to conflict resolution. the antecedent of a rule determines the eligibility of the plan in its consequent and the rule's associated priority indicates its desirability relative to other plans. note that all objectives underlying the desirability of a plan  e.g.  minimizing cost  maximizing convenience  maximizing the satisfaction of system users  maximizing speed  are implicitly represented in the priority  and this gives rise to 
	klein and finln 	1 
several difficulties. 
first  as the rule base grows it becomes difficult to predict the consequences of adding new rules to the knowledge base. in effect  the knowledge engineer must understand tne basis for the priorities of all existing rules in order to assign a new priority to a new rule. second  the meaning of a priority is completely opaque  so there exists no basis for justifying a priority in an explanation. finally  a system that uses this shallow model of choice will lack robustness. because great importance is placed on a single heuristically justified symbol  the result of changing the priority of a single rule can significantly alter the overall behavior of an iss. 
this model of plan evaluation suffices when the number of plans is small and the relative desirability of each plan is obvious. for applications not sharing these characteristics  deeper models which explicitly represent the factors underlying priority selection may be useful for automatically justifying a choice among competing plans in an explanation and for facilitating the incremental modification of the knowledge base. 
1. utility theory with encoded objective values the next model to be examined involves explicitly representing the attributes and objectives underlying priority selection in the framework of utility theory. multiattribute utility theory  keeney & raiffa 1. hansen 1  is of particular interest in the domains of intelligent safety systems  where multiple  often mutually competitive objectives drive choices between competing plans. under this approach  we regard each plan's charactenstics with respect to each of the objectives which underlie plan evaluation as arguments to a utility function which computes the priority for each plan. the utility function itself abstractly captures tne relationships between objectives which underlie the choice of plan  and can be thought of as encoding the plant's operational policy. 
 employing an explicit model of choice based on utility theory has several advantages over implicit models such as hardcoded priorities. first  since the underlying objectives of plan evaluation are explicitly represented  a basis is provided for generating explanations regarding choices among competing plans. in addition  adding plans to the knowledge base is simplified in that the knowledge engineer need only score a new plan with respect to the defined objectives. wnere such scores are reasonably easy to formulate  this second level of depth suffices for plan evaluation. however  in applications where scores for objectives encompass consideration of the behaviors of large sets of components  formulating scores may be difficult  encouraging a greater level of depth. specifically  we may wish to reason about  or compute  the scores for objectives rather than assign them. in applications which employ a model of plan determination which actually generates plans  e.g.. those of sections 1 and 1   a deeper model of plan evaluation will be required  for there will be no way to assign scores in advance to plans which are constructed during problem solving. 
1. utility theory with computed objective values the deepest model of plan evaluation that we consider for 
 iss's involves computing the scores for underlying objectives that are input to the utility function. this may be accomplished by formulating another set of  hard-coded  data from which objective values may be computed for each plan. in applications which employ a model of plan determination which actually generates plans  we would use the same representation of the target system to support both plan determination and evaluation. for example  consider augmenting the spatial description of the computer system of figure 1 with component descriptions  e.g.. processing time per line of data  pertaining to the objectives mentioned in section 1  e.g.  maximize user satisfaction . a plan  path of devices  can be evaluated with respect to turnaround time  one aspect of user satisfaction  by summing the processing times of the processors that lie along the generated path. other objectives  e.g.  speed of action  work for the operator  would be similarly computed. 
1 	knowledge representation 
this model of evaluation is deeper than that of 1 because rather than encoding the values for objectives  we encode functions for computing them. one advantage of this method is that we need only supply local device-dependent data for each represented device in order to compute the desirability of actions  rather than making subjective judgements about the desirability of predefined paths. 
1. summery and conclusions 
we have characterized the depth of models of expertise in terms of the knowledge they explicitly represent and reason about. for the reasoning task 'provide a safety function'  we identified two subtasks which provide opportunities for building progressively deeper models of knowledge  described some particular models for performing each subtask  and reviewed their relative merits for some particular domains. 
if knowledge engineering is to become more of a discipline than an art. we will need to develop some guidelines which more precisely characterize 'depth of knowledge' and its implications for intelligent system construction  performance  and maintenance. ultimately  the guidelines would provide a basis for selecting among models of varying depth based on general domain characteristics. we believe that such guidelines should be developed by circumscribing isolated reasoning tasks and analyzing the relative merits of models of varying depth inspired 1y various domains  and we have sketched one such analysis in this paper. future work will involve employing alternative definitions of depth in the analysis and analyzing other reasoning tasks to provide the data upon which the mentioned guidelines may be based. 
1. references 
1 barrow  h.  'verify: a program for proving correctness of digital hardware designs'  artificial intelligence 1. 
1. bobrow  d   ed.  qualitative reasoning about physical systems  elsevier. 1. 
1 chandrasekaran  b   mittal. s   deep verses compiled knowledge approaches to diagnostic problem solving'  int. j. man-machine studies  1. 
1. corcoran  w.  finnicum  d.  hubbard iii. r.  musick  c. walzer  p.  
'nuclear power-plant safety functions'  nuclear safety vol 1  no 1  1. 
1. davis  r.  'diagnostic reasoning based on structure and 
behavior'  artificial intelligence 1. 
1. de kleer  j.  brown  j. s  a qualitative physics based on confluences'  artificial intelligence 1. 
1. ennis  r.  griesmer  j.  hong  s.  karnaugh  m.  kastner  j.  klein  d.  miiliken  k.  schor  m.  van woerkom  h. 'a continuous realtime expert system for computer operations'  ibm journal of research and development  january 1. 
1. fink  p.  'control and integration of diverse knowledge in a diagnostic expert system'  proc ucai  1. 
1. ginsberg  a  localization problems and expert systems'  cbmtr 1  rutgers university  1. 
1. hansen  p.. ed.  essays and surveys on multiple criteria decision making  springer-vertag  1. 
1. hart. p.. 'directions for al in the eighties'  sigart newsletter no 1  january 1. 
1. keeney  r  raiffa. h  decisions with multiple objectives: p