
in this paper  we consider the generation of features for automatic speech recognition  asr  that are robust to speaker-variations. one of the major causes for the degradation in the performance of asr systems is due to inter-speaker variations. these variations are commonly modeled by a pure scaling relation between spectra of speakers enunciating the same sound. therefore  current state-of-the art asr systems overcome this problem of speakervariability by doing a brute-force search for the optimal scaling parameter. this procedure known as vocal-tract length normalization  vtln  is computationally intensive. we have recently used scaletransform  a variation of mellin transform  to generate features which are robust to speaker variations without the need to search for the scaling parameter. however  these features have poorer performance due to loss of phase information. in this paper  we propose to use the magnitude of scaletransform and a pre-computed  phase -vector for each phoneme to generate speaker-invariant features. we comparethe performanceof the proposed features with conventional vtln on a phoneme recognition task.
1 introduction
inter-speaker variation is a major cause for the degradation in the performance of automatic speech recognition  asr . this is especially important in speaker-independent si  asr systems which are typically built to handle speech from any arbitrary unknown speaker  e.g. in applications such as directory assistance. as a rule of thumb speaker-dependent  sd  asr systems have half the error rates when compared to si systems for the same task. this difference in performance suggests that the performance of si systems can be significantly improved if we can account for inter-speaker variations. note that in practice  we cannot directly build sd systems for large vocabulary speech recognition tasks  since the speech data required from that speaker will be too enormous even for a co-operative user. instead  we build si systems and then  adapt  the system for a particular speaker to make it speaker-dependent. hence  irrespective of the application 

figure 1: spectra of 1 speakers for one  frame  of /aa/
there is a lot of interest in improving the performance of si systems by accounting for inter-speaker variability.
¡¡in general  two speakers enunciating the same sound have very different pressure waveforms and the resulting spectra are very different. fig. 1 shows the smoothed spectra  after smoothing out pitch  for two speakers enunciating the vowel /aa/. as seen from the figure the two spectra are very different even though it is the same vowel spoken by two speakers. since asr systems use features extracted from the spectra  the features are themselves different for the same sound enunciated by two speakers. therefore for an automated system to recognize these different features as belonging to the same sound is difficult leading to a degradation in performance. note that there will be a certain amount of variability for the same sound spoken by the same person  i.e. intraspeaker variability  which is handled by the statistical model. however  in general inter-speaker variability is substantially larger than intra-speaker variability  leading to  coarse  statistical models and hence increased confusion between the sound classes.
¡¡there is a lot of research that is being done in trying to understand the mathematical relationship between spectra of two speakers enunciating the same sound. this has important implications in other areas apart from speech recognition  such as in vowel perception  hearing-aid design and speech pathology. one of the major causes for inter-speaker variations is attributed to the physiological differences in the vocal-tracts of the speakers. based on this idea  in asr a commonly used model to describe the relation between spectra of two speakers enunciating the same sound is given by
                 sa f  = sb ¦Áf   1  where sa f  and sb f  are the spectra of speakers a and b respectively; and ¦Á denotes the uniform constant  frequencywarp factor. we refer to the above model as linear-scaling model  since the two spectra are just scaled versions of each other. the motivation for linear scaling comes from the fact that to a first-order approximation  the vocal tract shape can be assumed to be a tube of uniform cross-section and for this simplifying approximation  the resonant frequencies  which characterize different sounds  are inversely proportional to vocal-tract length   wakita  1  . therefore  differences in vocal-tractlengths manifest in scaling of resonantfrequencies with the scaling being inversely proportionalto the vocal-tract length.
¡¡in practical asr systems  we have access only to the acoustic data from the speaker and therefore do not have any idea of the speaker's vocal-tract length. therefore  we do a brute-force search for the optimal scaling-factor  ¦Á by trying out different values of ¦Á. the optimality criterion is based on maximizing the likelihood with respect to the si hidden markov model  hmm . since in asr we use features derived from the spectra  for each value of ¦Á we scale the frequency-spectra appropriately and then compute the features. therefore for each value of ¦Á  we recompute the feature of the ith utterance  x¦Ái   for that scale-factor. we then find the optimal scale-factor by
¦Á i = argmaxpr x¦Ái |¦Ë wi 	 1  ¦Á
where ¦Á represents scale factor. ¦Ë is the hmm model and wi is the transcription of the utterance. due to physiological constraints of the human vocal-tract apparatus  ¦Á lies in the range of 1 to 1. therefore  in state-of-the-art asr systems  we compute the features for different values of ¦Á from 1 to 1 in steps of 1 and find the optimal ¦Á using eq. 1. this is a computationally expensive process since for every frame the features have to be computed 1 times corresponding to different ¦Á  before finding the optimal feature for that frame after a maximum-likelihood  ml  search. hence  a lot of research is being done to have alternate normalization schemes that are computationally efficient.
¡¡recently  an alternate method for generating features that are invariant to speaker-variations due to the linear scaling  see eq. 1  was proposed  umesh et al.  1 . in this method  the features are insensitive to the scale-factor ¦Á and hence there is no need to search for the optimal ¦Á. we describe the method in the next section.
1 review of scale-invariant features using scale-transform
in the linear-scaling model of eq. 1  the speaker-dependent warp-factor  ¦Á is a multiplicative constant and is independent

figure 1: block diagram for the computation of stcc
of frequency. for such a model  log-warping the frequencyaxis of the spectra of speakers separates the linear scaling factor as a translation factor in log-warped spectral domain  i.e. 
sa ¦Ë  = sa f = e¦Ë  = sb ¦Á e¦Ë 
	= sb e¦Ë+ln¦Á  = sb ¦Ë + ln¦Á .	 1 
hence  in the log-warped domain  ¦Ë  the speaker-dependent scale-factor separates out as a translation factor ln¦Á for the linear scaling model of eq. 1.
¡¡let the fourier-transform of the log-warped spectra sa ¦Ë  be denoted as da c  and that of sb ¦Ë  be denoted as db c  
i.e.
	sa ¦Ë  dft   da c 	and	sb ¦Ë  dft   db c 	 1 
using the property of the fourier-transform that a translation in one domain appears only as linear-phase in the other domain we have 
	da c  = db c e jcln¦Á	 1 
therefore  the speaker-dependent term ¦Á appears only in the phase term. hence the magnitudes of da c  and db c  are identical and independent of the speaker-variations i.e.
	|da c | = |db c |.	 1 
we can therefore use the magnitude as speaker-invariant features for asr.
¡¡we can show that the above procedure is equivalent to taking the scale-transform  cohen  1   a special case of mellin transform  of sa f  and sb f  and then using the magnitude of scale-transform as features in asr. in this paper  we refer to these features as scale-transform cepstral

figure 1: figure shows that linear-scaling of original spectra  results in their being shifted versions in ¦Ë = log f  domain. therefore  the magnitude of the subsequent fouriertransform  stcc  are identical.
coefficients  stcc . the steps to compute stcc are shown in fig. 1.
¡¡fig. 1 illustrates the idea of using scale-transform to obtain speaker-invariant features for the linear-scaling model of eq.1 using a synthetic example. fig. 1 a  shows synthetic spectra of two speakers enunciating the same sound that are exactly scaled versions of each other corresponding to the linear-scaling model of eq.1. in fig. 1 b  we show the same spectra in the frequency-warped i.e. log-warped  domain. as seen from the figure  the frequency-warped spectra are exactly shifted versions of each other as expected from eq. 1. finally  we take the fourier-transform of these frequencywarped spectra and then take the magnitude of the fouriertransform to get the stcc features. these are shown in fig. 1 c . as seen from the figure  the stcc features are identical  and are therefore invariant to speaker variations that are present in the original spectra as seen in fig. 1 a .
¡¡since the linear scaling model of eq. 1 is only a crude approximation  recently  umesh et al.  1  have proposed the use of mel-scale for frequency-warping and not the log-scale. with this mel-scale frequency-warping they claim better results  sinha and umesh  1 . therefore  for the purposes of this paper  we will use mel-scale warping to compute the stcc features.
¡¡we now compare the performance of conventional mfcc features with stcc features. timit database was used for the experiments and we considered the classification of 1 monophones. the train and test set consist of 1 and 1 monophone occurrences for adult male and female speakers respectively. the hmm models consisted of 1 emitting and 1 non-emitting states  with left-to-right and with-
stccmfccacc%11table 1: accuracy of the phoneme recognizer using stcc and mfcc
out skips over states. two mixture gaussians with full covariance matrices per state were used. the features vectors are of 1 dimensions comprising normalized log-energy  c1...c1 excludingc1  and their first and second order derivatives. conventional mfcc features are computed as described in  lee and rose  1  while the stcc features are computed as described in  umesh et al.  1  and a block diagram is shown in fig. 1.
¡¡the classification performance between stcc and the mfcc features are shown in table 1. as seen from the table  stcc features have a lower classification performance when compared to mfcc features. since stcc provides speakerinvariant features  it should have provided improved normalization performance when compared to mfcc. the reason for the degradation is the complete loss of phase-information. we discuss about the loss of phase information in section 1 and proposenew features to overcomethis loss of information by introducing an average phase vector which is discussed in detail in section 1.
1 drawback of stcc - loss of phase information
theoretically  stcc provides speaker-normalization by exploiting the fact that the magnitude of stcc are identical as seen in eq. 1. however  note that da c  and db c  are complex quantities with associated magnitude and phase. let
	da c  = |da c |ej¦Õ c 	and	 1 
	db c  = |da c |ej¦Õ c e j ln¦Áabc.	 1 
therefore |da c | = |db c |. however  we have completely lost the phase information ej¦Õ c  which is also important for discrimination of phonemes. this loss in phase information degrades the performance of the stcc and even though it is insensitive to speaker-variations  the final performance is inferior to the mfcc. in the next section  we propose a method to overcome this problem and help improve the performance of the stcc.
1 proposed improvement over stcc features
as seen in the previous section  the loss in phase information ej¦Õ c  leads to degradation in performance when compared to mfcc features even though stcc features are invariant to speakers as seen in eq. 1. as seen in eq. 1  da c   db c  contain both the magnitude and phase information and the speaker-specific factor  ¦Á  appears only in the phase term. hence retaining the complete phase will result in no normalization since the ln¦Á will also be present. on the other hand  taking only the magnitude will result in the loss of ej¦Õ c  information which provides additional information for discrimination between vowels.
¡¡in this section  we propose a method to incorporate the phase information ej¦Õ c  but not the speaker-specific ln¦Á in the stcc features. the basic idea of this approach is to estimate an  average phase vectorfor each phonemeusing training data from all speakers.
¡¡as previously discussed  for the linear-scaling model  the speaker differences manifest themselves as speaker-specific shifts in the log-warped domain as seen in fig. 1. the stcc exploits this fact by considering the magnitude of the subsequent fourier-transform but in the process loses the phase information completely. our approach is to estimate the average phase for each phoneme from the training data and use this same phase-vector for every occurrence of that phoneme irrespective of the speaker. for example  for the phoneme /ae/  our average-phase stcc features with acronym apstcc will be of the type |da c |ej¦Õavgae c . we will first illustrate this idea through a synthetic example that is discussed below.
¡¡consider that a particular phoneme is enunciated by three different speakers and the corresponding spectra after frequency-warping are shifted versions of one another as shown in fig. 1 a . the three spectra in the fourier-domain will differ only in a linear-phase term and this can be seen mathematically as:
sa ¦Ë dft   da c  = |da c |ej¦Õ c 	 1 sa ¦Ë   ¦Ó1 dft   da c e j¦Ó1c = |da c |ej¦Õ c e j¦Ó1csa ¦Ë + ¦Ó1 dft   da c e+j¦Ó1c = |da c |ej¦Õ c e+j¦Ó1c¡¡if we now compute the average phase of all the three phases  i.e.
¦Õavg c =	1
  1 
1= ¦Ó
¦Õ c  +	1   ¦Ó1 c ¦Õ c  +  ¦Õ c    ¦Ó c  +  ¦Õ c  + ¦Ó c 
1
then the average-phase ¦Õavg c  is same as the phase of the phoneme  ¦Õ c   with an additional linear-phase term that corresponds to the average of all the three shifts. therefore  the average phase ¦Õavg c  preserves the phase of the phoneme with an additional term corresponding to the average shift of the spectra of the training speakers. if we now use this average phase for all speakers along with the magnitude of the stcc feature  we get speaker-normalized features. this can be easily seen by recalling the fact that |da c | is constant for all speakers; and when it is multiplied by the same phase term ej¦Õavg c  for all speakers  then the resulting features are same for all speakers. this is shown in fig. 1.
¡¡in practice  since the processing is done in the discretedomain  the phases are 1¦Ð wrapped  and hence we first unwrap the phases before averaging. the other importantpractical limitation at this point is the fact that we need to know the phoneme a priori so that we can add the appropriate averagephase of that phone to the magnitude vector |da c |. we are now working on practical algorithms to find the appropriate phase for a given frame without having the knowledge of which phoneme the frame came from. however  as a first

figure 1: figure shows our proposed method of normalization using average phase. the top figure shows the shifted spectra from different speakers in the frequency-warped domain. using the average phase and the magnitude of the dft we then reconstruct the spectra  shown in solid line .
step  we will assume knowledge of appropriate phase vector and test the efficacy of this method of normalization and compare it with the conventional method of normalization.
1 performance of the proposed normalization method
in this section  we will compare the performance of the proposed normalization scheme with conventionalnormalization scheme.
¡¡here we considered the classification of 1  most confusable  vowels in contrast to the classification of 1 monophones used for mfcc and stcc feature vectors discussed in section.1. the main reason was to study the effect of our proposed phase-estimation procedure in more detail. the vowels were extracted from timit database  where the train and test set consisted of 1 and 1 utterances respectively. the hmm models consisted of 1 emitting and 1 non-emitting states  with left-to-right and without skips over states. we used single mixture gaussian with diagonal variance for each state. the feature vectors are of 1 dimensions comprising normalized log-energy  c1...c1 excluding c1  and their first order derivatives.
¡¡the conventional normalization scheme is based on eq. 1  which involves ml estimation of the warp-factor ¦Á. this method is also popularly known as vocal-tract length normalization  vtln . as discussed in the introduction  we do a brute-force search for the optimal ¦Á by computing the mfcc feature for each ¦Á by appropriately scaling the mel filter-bank
conditionno norm.norm.norm.baselinerecog. trans.true trans.vtln-mfcc111ap-stcc111table 1: % of accuracy for vtln-mfcc and ap-stcc speaker normalization methods for classification of 1 vowels in timit test set. first column is without normalization. in the second column we have used recognition output of first column as transcription for the normalization  while the third column shows the normalization performance when the true transcription is known.
and choosing the feature that maximizes the likelihood with respect to the statistical hmm model and the given transcription wi. note that although the method needs transcription  there is graceful degradation when there are errors in the transcription.
¡¡in our proposed method  we are still working on methods to find the optimal phase-vector to multiply a given frame of speech. as seen from table 1 if we are given exact transcription  then we can multiply by the correct average-phase vectors and the performance is exceptional. on the other hand  even with exact transcription the vtln method is far inferior to the proposed method of using ap-stcc features. but  if we use the recognition output of the baseline recognizer  i.e. without normalization  as the transcription then the vtln normalization method degrades gracefully while the ap-stcc completely falls apart - it is worse than even baseline. note that we do not need to know the transcription for ap-stcc  but a method of finding  optimal  phase-average vector for each utterance. we are working on various distance measures to obtain an  optimal  solution. however  it is important to note that at least under ideal conditions  when true transcription is known  our method shows that it is possible to remove almost all the speaker-variability which conventional vtln-mfcc is never able to achieve.
¡¡another approach to measure the normalization performance is to measure the separability of the vowel models using the various normalization schemes. one good measure of separability is the f-ratio between models considered pair-wise. tables 1 1 shows the f-ratio for the unnormalized features  the vtln features and proposed apstcc features. these tables show f-ratio of models that are built using training data for which true transcription is always known. the f-ratio shows the separability between models and the higher the number the better. note that since mfcc and stcc are computed slightly differently the features are slightly different and hence there are small differences in performance in the un-normalized case. from the table  it can be seen that the separability between the vowels models are excellent for the ap-stcc features.
1 conclusion & discussion
in this paper  we have proposed a method of obtaining speaker-invariant features for automatic speech recognition. our method is motivated by the fact that conventional methods reduce inter-speaker variability by doing a brute-force search for optimal features and are therefore computationally very expensive. recently a method has been proposed that uses a special transform called scaletransform to obtain speaker-invariant features. since the scale-transform uses only the magnitude of the features to obtain speaker-invariance  there is a loss of discriminability between phonemes due to the loss of phase information. in our proposed method  we use the average-phase of the corresponding phoneme in place of the lost phase information of stcc. we are currently working on various distance measures to find the appropriate average-phase for normalization. using vowel classification experiments and f-ratio measures we show that if we have optimal average-phase information then the proposed method provides excellent normalization performance when compared to the conventional vtln method.
1 acknowledgments
this work was supported in part by funding from the dept. of science & technology  government of india under project no. sr/s1/eece/1-serc-engg.
