
learning real-time search  which interleaves planning and acting  allows agents to learn from multiple trials and respond quickly. such algorithms require no prior knowledge of the environment and can be deployed without pre-processing. we introduce prioritized-lrta*  p-lrta*   a learning real-time search algorithm based on prioritized sweeping. p-lrta* focuses learning on important areas of the search space  where the importance of a state is determined by the magnitude of the updates made to neighboring states. empirical tests on path-planning in commercial game maps show a substantial learning speed-up over state-of-the-art real-time search algorithms.
1	introduction
we introduce prioritized learning real-time a*  p-lrta* . p-lrta* prioritizes learning to achieve significant speed-ups over competing real-time search algorithms.
　real-time search algorithms interleave planning and acting to generate actions in user-bounded time. these algorithms are also agent-centered  koenig  1 : they do not require a priori knowledge of the map  they learn the transition model by interacting with the environment  and they improve their solutions by refining a heuristic function over multiple trials. algorithms with these three properties can be used for pathplanning in computer games  virtual reality trainers  dini et al.  1   and robotics  koenig and simmons  1 . in each of these domains  agents plan and act in potentially unknown environments; when identical tasks must be solved in succession  the agent has an opportunity to improve its performance by learning. examples include commute-type tasks  such as resource collection and patrolling.
　our focus is on the learning process. since learning takes place on-line and on-site  it is critical to minimize learning time by converging rapidly to high-quality solutions.
　we build on existing research and present a learning realtime search algorithm that makes learning more experienceefficient by prioritizing updates  moore and atkeson  1 . states that are deemed important are updated before other states  where importance is determined by the magnitude of the updates made to a neighboring state. prioritizing heuristic updates for efficient learning is motivated by two observations about asynchronous dynamic programming  barto et al.  1 . first  since states are updated one at a time  new state values depend upon the order in which the updates occur; a judicious update ordering can make individual updates more effective. second  a significant update to one state often affects its neighbors  and giving priority to these neighbors can focus computation on the most worthwhile updates.
　the rest of the paper is organized as follows: first  we formally describe the problem. next  we examine and identify the shortcomings of related algorithms  and motivate our approach. we then provide a comprehensive description of our novel algorithm  and its general properties. next we justify our performance metrics and conduct an empirical evaluation. we conclude by considering possible extensions to p-lrta*.
1	problem formulation
we	focus	on	problems	defined	by	the	tuple
 s a c s1 sg h1 : s is a finite set of states  a is a set of deterministic actions that cause state transitions  and c s a  is the cost of performing action a （ a in state s （ s; if executing action a in state s returns the agent to state s  that action is said to be blocked. blocked actions are discovered when they are within the agent's visibility radius  the distance at which the agent can sense the environment and update its transition model.
　the agent begins in a particular start state s1 （ s and aims to reach the goal state sg. upon reaching a goal state  the agent is teleported back to the start state and commences a new trial. a learning agent has converged when it completes a trial without updating the heuristic value of any state.
　for every action executed  the agent incurs a cost associated with that action. in general  any real-valued costs lowerbounded by a positive constant can be used. we assume that the state space is safely explorable insomuch as a goal state is reachable from any state.
　the agent deals with its initial uncertainty about whether an action is blocked in a particular state by assuming that all actions from all states are unblocked; this belief is known as the freespace assumption  koenig et al.  1 . as the agent explores  it can learn and remember which actions are possible in which states  and plan accordingly.
1	related work
a real-time agent is situated at any given time in a single state known as its current state. the current state can be changed by taking actions and incurring an execution cost. an agent can reach the goal from the current state in one of two ways: the agent can plan a sequence of actions leading to the goal and then act  or the agent can plan incompletely  execute this partial plan  and then repeat until it reaches the goal. we review search algorithms for both techniques.
　an agent using local repair a*  lra*   silver  1  plans a complete path from its current state to the goal  and then executes this plan. this path is optimal given the agent's
current knowledge. the agent may discover that its planned path is blocked  and at this point will stop and re-plan with updated world knowledge. each plan is generated with an a* search  hart et al.  1   so the time needed for the first move and for re-planning is o nlogn   where n is the number of states. algorithms such as dynamic a*  d*   stentz  1  and d* lite  koenig and likhachev  1  efficiently correct the current plan rather than reproduce it  but they do not reduce the computation needed before the first move of each trial can be executed. a large first-move delay negatively affects an agent's responsiveness in interactive environments such as computer games and high-speed robotics.
　in its simplest form  korf's learning real-time a*  lrta*   updates the current state only with respect to its immediate neighbors. we refer to this version of lrta* as lrta* d=1 . if the initial heuristic is non-overestimating then lrta* converges optimally  korf  1 . the amount of travel on each trial during learning can oscillate unpredictably  causing seemingly irrational behavior. heuristic weighting and learning a separate upper bound reduces the path length instability  shimbo and ishida  1; bulitko and lee  1   but can result in suboptimal solutions.
　we define the local search space  lss  as those states whose neighbors are generated when planning a move. lrta* d=1  only looks at the current state's immediate neighbors  but a deeper lookahead gives an agent more information to decide on the next action  korf  1 . for instance  lrts  bulitko and lee  1  considers all of the states within a radius d of the current state. alternatively  koenig uses an lss defined by a partial a* search from the current state to the goal  1; 1 .
　updating more than one state value in each planning stage can result in more efficient learning. one example of this is physical backtracking  which helps to keep state updates in close physical proximity to the agent: sla*  sla*t  shue and zamani  1a; 1b; shue et al.  1   γ-trap  bulitko  1   and lrts  bulitko and lee  1  all physically return to previously visited states  potentially reducing the number of trials needed for convergence with the possibility of increasing travel cost. alternatively  lrta* k   hernandez and meseguer  1b;＞ 1a  uses mental backups to decrease the number of convergence trials. these two techniques are difficult to combine  and a recent study demonstrates the fragile and highly context-specific effects of combining physical and mental backups  sigmundarson and bjornsson  1：  .
prioritized-lrta* s 
whiledo
stateupdate s  repeat
p = queue.pop   ifthen
　stateupdate p  end if
until n states are updated or queue =   s   neighbor s' with lowest
end while

figure 1: the p-lrta* agent updates the current state  s  and as many as n states taken from the queue  where n is an input parameter defined by the user. the agent takes a single greedy move  and then repeats this process until reaching the goal state  sg.
　koenig's lrta*  koenig  1  updates the heuristic values of all lss states on each move to accelerate the convergence process. while the dijkstra-style relaxation procedure it uses produces highly informed heuristics  the process is expensive  and limits the size of lss that can be used in realtime domains  koenig and likhachev  1 .
　in an attempt to reduce convergence execution cost  prlrts  bulitko et al.  1  builds a hierarchy of levels of abstraction and runs search algorithms on each level. the search at the higher levels of abstraction constrains lowerlevel searches to promising sections of the map  reducing exploration in lower levels. as a result  convergence travel and first move delay are improved at the cost of a complex implementation and the additional computation required to maintain the abstraction hierarchy during exploration.
　a different line of research considers sophisticated update schemes for real-time dynamic programming algorithms. as barto  bradtke  and singh note    the  subset of states whose costs are backed up changes from stage to stage  and the choice of these subsets determines the precise nature of the algorithm  . prioritized sweeping  a reinforcement learning algorithm  performs updates in order of priority  moore and atkeson  1 . a state has high priority if it has a large potential change in its value function. only states with a potential update greater than are added to the queue. prioritized sweeping is shown to be more experience efficient than q-learning and dyna-pi  moore and atkeson  1   and is a core influence for our algorithm.
1	novel algorithm
prioritized-lrta*  p-lrta*  combines the ranked updates of prioritized sweeping with lrta*'s real-time search assumptions of a deterministic environment and a non-trivial initial heuristic. in this section we describe p-lrta*'s algorithmic details  then comment on the nature of its execution.
　a p-lrta* agent has a planning phase and an acting phase which are interleaved until the agent reaches the goal  figure 1 . during its planning phase  the agent gains knowlstateupdate s  find neighbor s with the lowest

for all neighbors n of s do addtoqueue n Δ  end for
end if

figure 1: the value of state s is set to the lowest available
  where is the cost of traveling to s  and  estimates the distance from s to the goal. if the value of
s changes  its neighbors are enqueued.

addtoqueue s Δs 
if s /（ queue then
if queuefull   then find state r （ queue with smallest Δr if Δr   Δs then
queueremove r  queueinsert s Δs 
　end if else
　queueinsert s Δs  end if
end if

figure 1: state s is inserted into the queue if there is room in the queue or if its priority is greater than that of some previously enqueued state r. if a state has already been enqueued  it will not be inserted a second time into the queue.
edge about how to navigate the world by updating the distance heuristics at select states. during its acting phase  the agent simply uses greedy action-selection to choose it next move. we describe these two phases in detail.
　at the beginning of the planning phase  the agent updates the current state s by considering its immediate neighbors  figure 1 . the amount by which the current state's value changes is stored in a variable  Δ. next  each of the current state's neighbors are slated for entry into the queue with priority Δ. if the queue is full  the entry with lowest priority is removed  figure 1 . p-lrta* requires no initial world model; updates that spread to unseen states use the freespace assumption.
　once the current state's heuristic is updated  a series of prioritized updates begins. at most n states  where n is specified by the user  are taken from the top of the queue and updated using the procedure in figure 1. if there are still states in the queue after the n updates  they remain in the queue to be used in the next planning phase.
　having completed the planning phase  the agent moves to the acting phase. the agent takes a single action  moving to the neighboring state s with the minimum-valued
  where is the cost of traveling to s and is the current estimated cost of traveling from s to the closest goal state  figure 1 .
1	algorithm properties
in this section we make general observations of the key features that make p-lrta* different from other learning realtime search algorithms. next  we discuss p-lrta*'s convergence  as well as its space and time complexity.
key features and design
unlike prioritized sweeping's  parameter  which restricts potentially small updates from entering the queue  p-lrta* specifies a maximum queue size. this guarantees a strict limit on the memory and computational time used while enabling the algorithm to process arbitrarily small updates.
　because p-lrta* specifies that the current state always be updated  p-lrta* degenerates to korf's lrta* d=1   korf  1  when the size of the queue is 1.
　a p-lrta* agent disallows duplicate entries in its queue  and retains the contents of its queue between acting phases. this design is a key difference between p-lrta* and realtime search algorithms such as lrta* k   hernandez and＞ meseguer  1b; 1a  and koenig's lrta*  koenig  1  since a p-lrta* agent's local search space is not necessarily contiguous or dependent on the agent's current state. this property is beneficial  as a change to a single state's heuristic value can indeed affect the heuristic values of many  potentially remote  states. empirically  limiting how far updates can propagate into unexplored regions of the state space does not significantly impact p-lrta*'s performance.
　the p-lrta* algorithm we present specifies only a prioritized learning process and greedy action selection. but p-lrta* can be modified to select actions in a way that incorporates techniques from other algorithms  such as increased lookahead within a specified radius  bulitko and lee  1   heuristic weighting  shimbo and ishida  1; bulitko and lee  1   or preferentially taking actions that take the agent through recently updated states  koenig  1 .
theoretical evaluation
similar to korf's lrta*  korf  1   p-lrta* can be viewed as a special case of barto et al.'s trial-based realtime dynamic programming  rtdp . in the following theorems  we outline why the theoretical guarantees of trialbased rtdp still hold for p-lrta*  and explain p-lrta*'s time and space complexity.
theorem 1 p-lrta* converges to an optimal solution from a start state s1 （ s to a goal state sg （ s when the initial heuristic is non-overestimating.
　our algorithm meets the criteria set out for convergence by barto et al. in  barto et al.  1 . it is shown that trialbased rtdp  and by extension p-lrta*  converges to an optimal policy in undiscounted shortest path problems for any start state s1 and any set of goal states sg when there exists a policy that takes the agent to the goal with probability 1.
theorem 1 p-lrta*'s per-move time complexity is in o m ， log q    where m is the maximum number of updates per time step and q is the size of the queue.
　the primary source of p-lrta*'s per-move time complexity comes from the m updates the algorithm performs  where m can be specified. each of these m updates potentially results in b queue insertions  where b is the branching factor at the current state. each of these insertions requires that we check whether that state is already queued  o 1  using a hash table  and whether or not it has high enough priority to be queued  o log q   using an ordered balanced tree .
theorem 1 p-lrta*'s memory requirements are of space complexity o |s|   where |s| is the size of the state space.
　p-lrta* requires memory for both its environmental model and the states on the queue. learning a model of an a priori unknown environment necessitates keeping track of a constant number of connections between up to |s| states. also  because duplicate states are disallowed  the maximum number of states in the queue  q  cannot exceed |s|.
1	performance metrics
real-time search algorithms are often used when system response time and user experience are important  and so we measure the performance of search algorithms using the following metrics.
　first-move lag is the time before the agent can make its first move. we measure an algorithm's first-move lag in terms of the number of states touched on the first move of the last trial; that is  after the agent has converged on a solution. this is in line with previous research in real-time search and enables us to compare new results to old results.
　suboptimality of the final solution is the percentage-wise amount that the length of the final path is longer than the length of the optimal path.
　planning time per unit of distance traveled indicates the amount of planning per step. in real-time multi-agent applications  this measure is upper bounded by the application constraints  e.g.  producing an action at each time step in a video game . an algorithm's planning time is measured in terms of the number of states touched per unit of distance traveled.
　the memory consumed is the number of heuristic values stored in the heuristic table. this measure does not include the memory used to store the observed map  as the algorithms we run use the same map representation.
　learning heuristic search algorithms typically learn over multiple trials. we use convergence execution cost to measure the total distance physically traveled by an agent during the learning process. we measure this in terms of the distance traveled by the agent until its path converges  where distance is measured in terms of the cost c of traveling from one state to the next.
　note that to keep the measures platform independent  we report planning time as the number of states touched by an algorithm. a state is considered touched when its heuristic value is accessed by the algorithm. there is a linear correlation between the number of states touched and the wall time taken for our implementation.
1	experimental results
we ran search algorithms on path-planning problems on five different maps taken from a best-selling commercial computer game. the use of this particular testbed enabled comparison against existing  published data. the number of states in each map is 1  1  1  1  and 1. we generated 1 problems for each map  resulting in a total of 1 unique path planning problems to be solved by each of the 1 competing parameterizations of learning real-time search algorithms  table 1 .
the agent incurs unit costs for moving between states in
a cardinal direction  and costs of 〔1 for moving diagonally. all the algorithms experimented with use octile distance  bulitko and lee  1  as the initial heuristic h1  which  for every state s  is the precise execution cost that would be incurred by the agent if there were no obstacles between s and the goal  sg. the maps are initially unknown to each of the agents  and the uncertainty about the map is handled using the aforementioned freespace assumption. the visibility radius of each algorithm is set to 1  which limits each agent in what it knows about the world before it begins planning.
　we compare p-lrta* to five algorithms: lra*  lrta* d=1   lrts d=1 γ=1 t=1   pr-lrts with lra* on the base level and lrts d=1 γ=1 t=1  on the first abstract level. the lrts parameters were hand-tuned for low convergence execution cost. since the size of koenig's a*-defined strict search space is a similar parameter to plrta*'s queue size  we compare against koenig's lrta* using local search spaces of size 1  1  1  and 1.

figure 1: convergence execution cost in semi-log averaged over 1 problems plotted against optimal solution length. lrta* d=1  has a large convergence execution cost  while plrta* queuesize=1 updates=1  has convergence execution cost comparable to lra*.
　each algorithm's performance is tabulated for comparison in table 1. lra* and lrta* d=1  demonstrate extremes: lra* has the smallest convergence execution cost and stores no heuristic values  but its first-move lag is the greatest of all algorithms because it plans all the way to the goal. lrta* d=1  has the largest convergence execution cost as a result of its simplistic update procedure  but its first-
algorithmexecutionplanninglagheuristic memorysuboptimality  % lra*
lrta* d=1 
lrts d=1  γ=1  t=1 
pr-lrts d=1 γ =1 t=1 
koenig's lrta* lss=1 
koenig's lrta* lss=1 
koenig's lrta* lss=1 
koenig's lrta* lss=1 
p-lrta* queuesize=1  updates=1 
p-lrta* queuesize=1  updates=1 
p-lrta* queuesize=1  updates=1 
p-lrta* queuesize=1  updates=1 1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1	11 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1	11 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1 1 ＼ 1 1 ＼ 1
1	11
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
1 ＼ 1
	1	11
1
1 ＼ 1
1 ＼ 1 1
1
1
1
1
1
1
1	＼	＼	＼	＼
table 1: results on 1 problems with visibility radius of 1. the results for lra*  lrta*  lrts  and pr-lrts are takenfrom  bulitko et al.  1 .
move lag and planning costs are extremely low. for each parameterization  p-lrta* has a first-move lag comparable to lrta* d=1   and its convergence execution cost is even lower than algorithms that use sophisticated abstraction routines  such as pr-lrts.
　figure 1 shows the average convergence execution cost of the algorithms on 1 problems of various lengths: plrta*'s convergence execution cost is similar to lra*'s. this indicates that heuristic learning in p-lrta* is efficient enough that the expense of learning the heuristic function scales similarly with the expense of learning the a priori unknown map itself.

figure 1: the impact of queue size and maximum number of updates on convergence execution cost.
　using the same set of 1 problems used to generate table 1  we explore p-lrta*'s parameter space. this experiment reveals that the queue size and the maximum number of updates per move exhibit some independence in how they affect performance. that is  when we increase the maximum number of updates with a fixed queue size  the convergence execution cost decreases; the same happens when we increase the queue size with a fixed maximum number of updates  figure 1 . this demonstrates that p-lrta*'s parameters provide two effective ways of changing the algorithm to meet specific time and/or space requirements.
1	future work
prioritized lrta* is a simple  effective algorithm that invites further analysis. further experimentation with randomized start-locations in the problems we explore could provide additional insight. we would also like to extend p-lrta* to include dynamic parameterization  as in some settings it may be beneficial to use extra memory as it becomes available. plrta* provides a convenient way to take advantage of free memory: the queue size can be dynamically adjusted.
　p-lrta* may benefit from a more sophisticated actionselection scheme than the purely greedy decision-making process we present. p-lrta*'s simplicity makes it easy to experiment with a number of recent real-time search techniques that improve convergence time.
　state abstraction has been successfully applied to real-time heuristic search  bulitko et al.  1 . a hybrid approach that combines prioritized updates with state abstraction is likely to produce a search routine that is more powerful than either method alone. another interesting direction is to extend p-lrta* to handle dynamic environments by including focused exploration and allowing heuristic values to decrease.
1	conclusions
incremental search algorithms and real-time search algorithms meet different requirements. incremental search algorithms such as lra* converge quickly  but can suffer from arbitrarily long delays before responding. real-time search works under a strict per-move computational limit  but its long on-line interactive learning process reduces its applicability when good solutions are needed from the start.
　p-lrta* has a response time on par with the fastest known real-time search algorithms. meanwhile  its learning process converges in a time that scales with that of mere map discovery in lra*. a p-lrta* agent can learn twice as fast on the actual map as a state-of-the-art pr-lrts agent learns on a smaller  abstract map.
　as a step toward rapid-response algorithms that learn quickly  we presented a real-time heuristic algorithm that combines lrta*'s aggressive initial heuristics and prioritized sweeping's ranking of state updates. p-lrta* does not require the map a priori making it well suited to virtual reality trainers and computer games in which believable agents can only see limited portions of the world.
acknowledgments
we would like to thank nathan sturtevant for developing hog  hierarchical open graph   the simulation framework we used to generate our empirical results. we would also like to thank mitja lustrek  david thue  and several anony-  mous reviewers for their helpful comments. this work was supported by nserc and icore.
