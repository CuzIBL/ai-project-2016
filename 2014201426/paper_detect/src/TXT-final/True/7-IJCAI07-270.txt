
an efficient parsing technique for hpsg is presented. recent research has shown that supertagging is a key technology to improve both the speed and accuracy of lexicalized grammar parsing. we show that further speed-up is possible by eliminating non-parsable lexical entry sequences from the output of the supertagger. the parsability of the lexical entry sequences is tested by a technique called cfg-filtering  where a cfg that approximates the hpsg is used to test it. those lexical entry sequences that passed through the cfg-filter are combined into parse trees by using a simple shift-reduce parsing algorithm  in which structural ambiguities are resolved using a classifier and all the syntactic constraints represented in the original grammar are checked. experimental results show that our system gives comparable accuracy with a speed-up by a factor of six  1 msec/sentence  compared with the best published result using the same grammar.
1 introduction
deep syntactic analysis by lexicalized grammar parsing offers a solid basis for intelligent text processing  such as question-answering  text mining  and machine translation. in those tasks  however  a large amount of text has to be processed to build a useful system. one of the main difficulties of using lexicalized grammars for such large-scale applications is the inefficiencyof parsing caused by complicated data structures used in such grammars.
　recent research showed the importance of supertagging for the speed  as well as the accuracy  of lexicalized grammar parsing  clark and curran  1; ninomiya et al.  1 . supertagging is a tagging process where lexical entries are assigned to the words in the input sentence  bangalore and joshi  1 . in lexicalized grammars  a lexical entry encodes many constraints on how a word is combined with surrounding words or phrases. the parsing efficiency is therefore increased by supertagging because it makes the search space explored by the parser much narrower.
　the current accuracy of the supertagger is  in general  not sufficient to use it as a single-tagger  which assigns only one lexical entry for each word  because the number of supertags is generally large  more than 1 in our case  and only one or two tagging errors in a sentence will cause a parse failure in many cases. in previous research  clark and curran  1; ninomiya et al.  1   the problem is overcome by assigning several supertags to each word in a sentence  i.e.  multisupertagging; the supertagger initially assigns only a small number of lexical entries to each word and the number of lexical entries is gradually increased until the parser finds a successful parse. in short  the parser 'takes over' a part of the supertagger's task and resolves a certain amount of the lexical ambiguity by itself to avoid parse failures.
　in this paper  we show that  by making the supertagger more powerful  the workload of the parser can be further reduced and the overall system becomes more efficient. specifically  we combine the supertagger with a cfg that approximates the original lexicalized grammar  to enumerate maybeparsable supertag assignments in the order of their scores given by the supertagger. we say a supertag sequence is maybe-parsable if the sequence is parsable by the approximating cfg. the most time-consuming part of the enumeration algorithm is parsing of the input sentence with the approximating cfg. however  we can do this cfg parsing efficiently because the cfg is generally sparse  i.e.  the combination of symbols that appear in a cf-rule is highly restricted.
　the enumerated supertag sequences are parsed by an hpsg parser one by one  until a successful parse is obtained. though the enumerated supertag sequences are not necessarily parsable by the original hpsg  we observed in the experiments that the parser finds a successful parse within only a few maybe-parsable supertag sequences for most sentences.
　the biggest advantage of our approach is that the hpsg parser used in the last stage of the system  which is the most computationally demanding part in the previous approach  can now be replaced by a simpler and more efficient parsing algorithm. our hpsg parser is implemented as a classifierbased  deterministic shift-reduce parser that is guided by a cfg-forest. the cfg-forest is created by the approximating cfg  and it approximates the hpsg-forest that would be obtained if the input supertag sequence were fully parsed with the hpsg. we do not need to keep many hypothetical subanalyses represented by complex feature structures  as in the chart parsers used in the previous approach  since the input to the hpsg parser is single-supertagged and also we can know almost surely which sub-analysis grows into a well-formed parse tree by referring to the cfg-forest.
1 background
1 head-driven phrase structure grammar
hpsg  pollard and sag  1  is a linguistic theory based on the lexicalized grammar formalism. an instance of hpsg grammar mainly consists of two parts: a small number of rule schemata and a large number of lexical entries. the rule schemata represent general grammatical constraints  while the lexical entries in the lexicon express word-specific characteristics. in hpsg  both lexical entries and phrasal constituents are represented by typed feature structures called signs and applications of the rule schemata are implemented by unification of signs and schemata.
　figure 1 presents an example of hpsg parsing for the sentence  i like it. 1 first  three lexical entries are selected from ones associated with each word in the lexicon. then  the lexical entries of  like  and  it  are combined by applying the head-complement schema to them and then the resultant phrasal sign of the verb phrase  like it  is combined with the lexical entry of  i  by the subject-head schema.
　variations of syntactic constructionsallowed for a word are represented by different lexical entries associated to the word in the lexicon. these variations include not only ones with different subcategorization frames  e.g.  transitive and intransitive  but also 'transformational' variations such as passivization and wh-extraction. the form of a parse tree constructed upon a word sequence is hence highly constrained once a lexical entry is selected for each word.
1 supertagging
supertagging is a process where the words in the input sentence are tagged with 'supertags.' in our case  a supertag is a lexical template  which is a common structure shared among lexical entries for different words.
　supertaggingcan be formulated as a sequence labeling task and several types of techniques have been applied to it. we selected a simple approach proposed by clark   which uses a maximum entropy classifier. the conditional probability of a supertag sequence t = t1 ...tn given a  pos-tagged  sentence s is calculated as:

where w is a vector of feature weights  Φ s ti i  is the feature vector  and zi is a normalization constant.
　as is clear from the equation above  the supertagger neglects the parsability of the supertag sequence. we empirically show that  by combining a cfg-filter with the supertagger  it is possible to enumerate supertag sequences which are most of the case parsable  in the order of the probability defined in eq. 1   with only a small additional processing time.
n1

	like	it
figure 1: analysis by the approximating cfg
1 cfg-filtering
cfg-filtering  kiefer and krieger  1; torisawa et al.  1  is a parsing technique for unification-based grammars. in hpsg parsing based on cfg-filtering  an input sentence is first parsed by a cfg that approximates the original hpsg and then the cfg-parsing is 'replayed' using the hpsg. the cfg parsing in the first stage is much faster than normal hpsg parsing since the unification operations are replaced by identity checking of atomic symbols of cfg  which is a much more faster operation than unification.
　the non-terminal symbols n  the terminal symbols t  and the set of cf-rules r in the approximating cfg respectively represent  abstracted  phrasal signs   abstracted  lexical entries  and instantiations of the rule schemata. for example  the cfg tree in figure 1  which is an analysis of the sentence  i like it.  by an approximating cfg  corresponds to the hpsg parse tree in figure 1. in figure 1  n1  n1  t1  t1  and t1 are non-terminal and terminal symbols of the approximating cfg which represent feature structures at the corresponding positions in the hpsg parse tree.
　the number of possible phrasal signs in an hpsg is infinite in general. to approximate the hpsg by a cfg  we thus need to abstract away several parts of the phrasal signs to keep the number of abstracted signs finite. the abstraction of the feature structures is specified by means of a restrictor  shieber  1  on the feature structures; a restrictor takes a feature structure and overwrites several feature values in it to the most general values allowed for the features.
　we use one of the algorithms proposed by kiefer and krieger  to approximate the hpsg. the algorithm finds all possible abstracted phrasal signs by iteratively applying the schemata to abstracted signs that have been found so far. a sketch of the algorithm is as follows:
1: restrict all the lexical entries and obtain the set of terminal symbols t
1: i ○ 1; ni ○ t
1:do
1:
1:for all n1 n1 （ ni do1:for all schema s do1:apply s to n1 and n1 as daughters and obtain themother phrasal sign m
1:	
1:	

figure 1: hpsg parsing　the restrictor defines a many-to-one mapping from the set of supertags  i.e.  lexical entries  to the set of terminal symbols t. we however pretend there is a one-to-one mapping between these two sets by allowing multiple occurrences of the same terminal symbols in t  if necessary  and remembering which supertag was mapped to which terminal symbol. in the rest of the paper  we assume such a one-to-one relation and treat terminal symbols and supertags interchangeably.
　an important property of the cfg approximation is that the language of the obtained cfg is a superset of the set of parsable supertag sequences. in other words  if a supertag sequence is not parsable by the cfg  it is also not parsable by the hpsg. we use this property to eliminate non-parsable supertag sequences from the output of the supertagger.
1 an efficient multi-stage parsing algorithm for hpsg
our parsing system takes as input a pos-tagged sentence   where wi is a word and pi is a pos
tag  and outputs an hpsg parse tree. the algorithm is as follows:
1: for all in the input sentence do
1: assign scores by the supertagger to supertags associated to wi in the lexicon
1: j ○ 1
1: repeat
1:	j ○ j + 1
1:	t ○ j-th best maybe-parsable supertag sequence
1:	parse t with the approximating cfg
1: select an hpsg parse tree by the disambiguator  using the cfg forest made in the above step
1: until a well-formed hpsg parse tree is obtained
1: return the selected hpsg parse tree
　the approximating cfg is generally not equivalent to the hpsg  and therefore  a maybe-parsable supertag sequence might not be parsable by the hpsg. in such cases  the disambiguator at line 1 fails to find a well-formed hpsg parse tree and another supertag sequence is tried in the next iteration. the disambiguator might fail even when the maybeparsable sequence is truly parsable by the hpsg because the cfg-forest created at line 1 may contain a tree not licensed by the hpsg and the disambiguator might mistakenly try to reproduce such a tree with the hpsg. however  we found such cases are empirically rare and thus decided to simply try the next supertag sequence in such cases. the rest of this section describes the algorithm of enumerating maybe-parsable supertag sequences  line 1  and the disambiguator  line 1 .
1 enumeration of maybe-parsable supertag sequences
the algorithm is based on jime＞nez and marzal's algorithm  jime＞nez and marzal  1   which enumerates  given a weighted cfg and an sentence  the n-best parse trees for the sentence in order of their weights. in their algorithm  the input sentence is first parsed with the weighted cfg using the cky algorithm and then the n-best parse trees are obtained by enumerating the top n derivations of the root edge  i.e.  the edge which spans the whole input and whose label is the start symbol. the n-best derivations of an edge e are recursively obtained by combining the n-best sub-derivations of the sub-edges of e and selecting the top n derivations from the combinations. in reality  it is not necessary to calculate all the n-best derivations for every edge in the chart because the n-best sub-derivationscan be enumerated in a 'lazy' manner. see their paper for further details on their algorithm.
　to obtain the n-best parsable supertag sequences  instead of the n-best parse trees  we modify jime＞nez and marzal's algorithm as follows. our algorithm takes as input a sentence w =  w1 w1 ...  and a list of scored supertags for each
  where tij is the j-th scored supertag for wi and sij is the log-probability of tij. given the input and the approximating cfg  ga  we make a weighted cfg such that each cf-rule in ga has a weight zero and each 'leaf rule' tij ★ wi has a weight sij. note that the score of a derivation defined by the weighted cfg equals the log-probability of the supertag sequence on the fringe of the derivation. the fringe of a derivation is the sequence of pre-terminals  i.e.  tijs  of the tree of the derivation. in the first phase of the algorithm  we parse the input sentence with the weighted cfg by using the cky algorithm. in the second phase of the algorithm  we enumerate the n-best fringes of the derivations of the root edge. just as in jime＞nez and marzal's algorithm  we can recursively obtain the n-best fringes for an edge e by concatenating the n-best sub-fringes for the sub-edges of e and selecting the top n fringes from them in a lazy manner.
　we made several modifications to the basic algorithm described above in order to make it more efficient. the first is that we replace the cky algorithm used in the first step of the algorithm with an agenda-based best-first parsing algorithm. we further split the agenda-based parsing into two stages. the first stage of the best-first parsing stops when a root edge is found. in most cases  the first maybe-parsablesupertag sequence is truly parsable with the original hpsg. in such cases  we need only the best-scored parse tree since the first maybe-parsable supertag sequence is its fringe. when the second supertag sequence is requested  we start the second stage  in which the best-first parsing is continued until all the edges scored greater than  α   θ  are added to the chart  where α is the score of the best-scored parse tree and θ is a user-defined threshold. at the end of the second stage  any edge used in a parse tree with a score greater than  α   θ  is stored in the chart and therefore we can find all the maybeparsable sequences scored greater than  α   θ  without fully parsing the input sentence.
　another modification is that we set the weight of the leaf rule tij ★ wi to  sij  si1  instead of sij. note that this modification does not alter the supertag sequences enumerated by the algorithm or their orders. by this modification  generation of edges with small scores is suppressed for the same reason as in a -parsing  klein and manning  1 .
　the last modification is that  before the agenda-based parsing starts  we discard all the supertags for word wi whose
scores are less than si1  β  where β is a user-defined thresh-
old. by this modification  we might lose some maybeparsable sequences in which those discarded supertags appear. in the experiments  however  we found that the accuracy of parse trees created upon such supertag sequences with low scores is generally low. we therefore decided to use this thresholding with β for the efficiency of the enumeration.
1 disambiguation by shift-reduce parsing with a guiding forest
we use an algorithm based on deterministic shift-reduce parsing to select an hpsg parse tree from those that can be constructed upon a given supertag sequence. although we could use other disambiguation algorithms  the deterministicparser-like algorithm is well suited to our framework because there is almost no need to keep multiple hypothetical subanalyses  as in bottom-up chart parsing algorithms  to avoid search failure  since the supertag sequence given to the parser is maybe-parsable and we also have a cfg-forest on the supertag sequence  which guides the shift-reduceparserjust like an lr-table with infinite look-ahead.
　deterministic  classifier-based approaches to disambiguation have been applied to dependency parsing  e.g.   yamada and matsumoto  1; nivre and scholz  1   and cfg parsing  e.g.   sagae and lavie  1  . note that we cannot apply these algorithms to hpsg parsing in a straightforward manner since they cause many parse failures for highly constrained grammars such as hpsg; to avoid parse failures  we need the approximating cfg for filtering the supertag sequences and for making the guiding cfg forest.
　the disambiguator has two components  a stack of phrasal and lexical signs  s  and a queue of lexical signs  i.e.  supertags   q. the input to the disambiguator is a postagged sentence  a maybe-parsable supertag assignment t =  t1 t1 ...   and a forest  f  created by parsing t with the approximating cfg  using the cky algorithm. the disambiguation algorithm is as follows:
1: s ○ empty stack; q ○  t1 t1 ... 
1: while q is not empty or length s    1 do
1:if Γ s f  = φ then1:return fail1:a ○ argmaxw ， Φ s q a 
a（Γ s f 1:	apply a 
1: return r
in the algorithm  a denotes a parser action  Γ s f  is the set of possible parser actions given the stack s and the forest f  w is the vector of feature weights  and Φ s q a  is the feature vector. there are two types of parser action: shift: it pops a lexical sign from q and pushes it into s.
applyschemax: it applies an n-ary schema x to the top n elements of s and replaces them with the resulting mother phrasal sign.
for example  the hpsg parse tree in figure 1 is constructed by the following sequence of actions: shift  shift  shift  applyschema head complement 
applyschema subject head. the weight vector w is obtained with the averaged-perceptronalgorithm  collins and duffy  1  with the polynomial kernel of degree 1.
　the set of possible actions Γ s f  is determined so that the parser never 'goes out of the forest.'; Γ s f  is created by first mapping each element in s  i.e.  signs of subtrees  to its corresponding node in f  then selecting actions which can lead to at least one complete cfg parse tree in f that includes all the nodes to which some stack elements are mapped  and finally eliminating from them any such apply schemaxs whose schema x is not applicable to the stack elements on the top of s.
1 experiments
in this section  we first give a brief summary of the specific hpsg grammar used in the experiments and also describe the test/training data we used. we then give implementation details of the supertagger the cfg-filter  and the disambiguator. finally the experiments and their results are discussed.
1 enju grammar
we used enju version 1  an hpsg grammar for english  miyao et al.  1 . the design of the grammar basically follows the definition of  pollard and sag  1 . twelve schemata are defined in the grammar  1 binary schemata and 1 unary schemata . the lexicon of the grammar was extracted from sections 1 of the penn treebank  marcus et al.  1   1 sentences . the grammar consists of 1 lexical entries for 1 words.
　a program that converts the penn treebank into an hpsg treebank is also distributed with the grammar. we used it to make the training and test data for the experiment. a standard training/development/test split of the data is used; i.e.  we
descriptionfeature templatessurrounding wordsw 1  w1  w1surrounding pos tagsp 1 p 1 p1 p1 p1 p1w 1  w1  p 1  p1  p1 combinationsp1p1  p 1p 1  p 1p1  p1p1  p 1p 1  p 1  p1  p1table 1: features used in the supertagger
used the hpsg treebanks created from section 1/1 of the penn treebank as training/development/test data.
1 implementation details
hpsg supertagger
table 1 lists the features used in our supertagger. in the table  px and wx respectively denote a pos tag and a word at relative position x from the target word. we used the same feature set as one used in ninomiya et al.'s hpsg supertagger so that the comparison of our system and theirs becomes more meaningful. the number of supertags is 1  i.e.  the number of lexical entries . 1
cfg-filter
we created an approximatingcfg from the enju grammarby kiefer and krieger's algorithm. examples of the features we restricted in the approximation are phon  phonology feature   synsem:local:cont  semantic structures of the phrase   and synsem:local:cat:head:agr  agreement feature . the cfg contains 1 terminal symbols  1 non-terminal symbols  and 1 rules.
disambiguator
table 1 lists features used in the disambiguator.1 features 1 are adaptations of the features used in sagae and lavie's cfg shift-reduce parser  sagae and lavie  1   and features 1 are adaptations of ones used in miyao and tsujii's cky-style hpsg parser  miyao and tsujii  1 . feature 1 includes several types of valency constraint read off from the phrasal/lexical signs. for example  from a lexical sign for a ditransitive usage of  give   we extract two features   subject=np  and  complement=np np .
1 experimental results
we evaluated the speed and the accuracy of the proposed method on sentences in the test data of ＋ 1 words  1 sentences  and ＋ 1 words  1 sentences . we measured the accuracy of the parse results by the precision  lp  and recall  lr  of the  labeled  predicate-argumentrelations output by the parser. see  miyao and tsujii  1  for details of the definition of the predicate-argument relations. the f1 score
1. surface form of the head word of x
1. pos tag of the head word of x
1. lexical template of the head word of x
1. phrasal category of x  e.g.  s  np  nx 
	1 s	 ... s	 q	 ... q
1 are for.right
: s 1 .leftdep s 1 .rightdep ;
1. distance between head words of s 1  and s 1 
1. whether a comma exists between s 1  and s 1 
1. whether a comma exists inside s 1  or s 1 
1. pair of s 1 .rmost pos and s 1 .lmost pos
1. number of words dominated by s 1  and s 1 
1. valence features of s 1  s 1  q 1   and q 1 

table 1: features used in the parser
is the harmonic mean of lp and lr. all the timing information was collected on an amd opteron server with a 1-ghz cpu. the two parameters  β and θ  were manually tuned using the development set; β = log 1  and θ = log 1 .
　table 1 lists the results of the parsing experiments on the test set. the table also lists several reported results on the same test set by other hpsg parsers with the same grammar: ninomiya et al.'s parser is the model iii in  ninomiya et al.  1   which is the supertagger-based hpsg parser briefly explained in section 1. miyao and tsujii's parser is a ckystyle hpsg parser  miyao and tsujii  1. both parsers use maximumentropy models for the disambiguation and the main difference between them is the inclusion of a supertagger by ninomiya et al.'s parser. the higher efficiency of our approach is clear from the table: out system runs around six times faster than ninomiya et al.'s parser with comparable accuracy.
　on the test set of the sentences ＋ 1 words  our method found a well-formed parse for 1% of the sentences.1 in preliminary experiments on the development set  we found the rate of successfully parsed sentences reached nearly 1% when we chose larger values for β. however  for such settings  average parse time significantly increased and the f1 score did not improve because while the recall slightly improved  the precision slightly deteriorated. for example  with β = log 1   1% of the sentences in the development set got a parse with an average parse time of 1 ms. this fact means that when the first maybe-parsable supertag sequence is assigned very low probability by the supertagger  the enumeration algorithm needs to generate many edges until it finds the sequence  but the hpsg parse created on such a sequence is not so accurate.
　table 1 shows the cumulative percentages of the sentences on which the parser found a well-formed parse within a certain number of maybe-parsable supertag sequences. this experiment was done on the development set and with the same parameter values as the above experiment. for about 1% of the sentences  the first maybe-parsable supertag sequence
# of invocations1＋1＋1＋1＋1cumulative%11111sentences ＋ 1 wordssentences ＋ 1 wordsparserslplrf1avg. timelplrf1avg. timethis paper1111 ms1111 msninomiya et al. 1111 ms1111 msmiyao and tsujii 1111 ms1111 mstable 1: results of parsing on section 1table 1: number of enumerated supertag sequences until a successful parse
sub-moduleavg. time  % pos tagging1 ms  1% supertagging1 ms  1% enumeration of supertag sequences1 ms  1% cky parsing of a supertag sequence1 ms  1% disambiguation by the hpsg parser1 ms  1% other1 ms	total	1 ms  1% 
table 1: breakdown of the average processing time
was truly parsable with the hpsg and more than five invocations of the hpsg parser were needed for only 1% of the sentences.1 for any failed sentences  the enumerator returned no maybe-parsable supertag sequences  i.e  the cfg-parser failed on those sentences. these observations mean that the cfg approximation was a fairly tight one  and hence  the number of 'unfruitful'invocations of the hpsg parser  which do not give a well-formed parse tree  was kept small.
　table 1 shows a breakdown of the average processing time on the development set. the table suggests that our approach significantly reduced the 'workload' of the hpsg parser by its small overhead for the enumeration of maybe-parsable supertag sequences; the processing time used in the hpsg parser is now very close to that for pos tagging.
