
this paper presents a mechanism for acquiring a case base for a cbr system that has to deal with a limited perception of the environment. the construction of case bases in these domains is very complex and requires mechanisms for autonomously adjusting the scope of the existing cases and for acquiring new cases. the work presented in this paper addresses these two goals: to find out the  right  scope of existing cases and to introduce new cases when no appropriate solution is found. we have tested the mechanism in the robot soccer domain performing experiments  both under simulation and with real robots.
1 introduction
working with robots that interact with their environment is a complex task due to the degree of uncertainty in the robot's perception of the world. but even if we achieve a very accurate perception mechanism  and as consequence  have a very low degree of uncertainty  we still have to consider that the environment is not fully controllable and unpredictable situations can occur. for these reasons  the reasoning system has to introduce some mechanism that  on the one hand  allows to adapt the a priori knowledge given by an expert to the real perception of the robot and  on the other hand  automatically incorporates new knowledge when an unexpected situation occurs.
﹛researchers address the case base acquisition in different ways based on their domains. in the textual case-based reasoning field  cases are extracted from textual sources as for example e-mails  manuals and scientific papers  minor and biermann  1   or medical records  abidi and manickam  1 . in robotics  log files created during the execution of the tasks are used as inputs for case generation  gabel and veloso  1 . interactive approaches have been studied to complete the missing knowledge of the system with a user through concept mapping  leake and wilson  1  or even

figure 1: snapshot of the four-legged league  image extracted from the official robocup rule book .
acquiring case adaptation knowledge combining rule-based methods with user guidance  leake et al.  1 . in the aqua system   ram  1  presents an approach for incremental learning based on the revision of existing cases and the incorporation of new ones when no solution was found. his main challenges were:  i  novel situations;  ii  mis-indexed cases; and  iii  incorrect or incomplete cases. in our approach  we attempt to cover  i  and  iii  in our domain.
﹛this paper is the continuation of previous work  ros et al.  1   where we designed a case-based reasoning system for action selection in the robot soccer domain. before introducing the problem we address  we briefly describe the domain and the most relevant features of the system.
﹛we focus our work on the four-legged league of the robocup soccer competition. in this league teams consist of four sony aibo robots. the robots operate fully autonomously  i.e. there is no external control  neither by humans nor by computers. the field is 1 m long and 1 m wide. there are two goals  cyan and yellow  and four colored markers the robots use to localize themselves in the field. there are two teams in a game: a red team and a blue team. figure 1 shows a snapshot of the field. for details on the official rules refer to the official robocup rule book.
system description
the goal of the cbr system is to determine the actions the robots should execute given a state of the game  a snapshot of the game at time t . instead of considering single actions for each state  we define sequences of actions which we call game plays. some examples are get near the ball and shoot or get the ball  grab it  turn and shoot. hence  a case consists of the description of the environment  problem description  and the game play the robots performed for that state  solution description . the problem description features are: robots positions  ball position  opponents positions  defending goal  time of the game and score difference. the solution description is defined as an ordered list of actions.
﹛henceforward in the paper we will focus on a simplified version of the system: we consider only one robot  the ball and the defending goal as the description of the problem  although the ideas presented are extendableto the otherfeatures described above . the solution description remains the same:
case =   r b g  a 
where r =  x y 牟    b =  x y   g = {cyan yellow}  and a =  a1 a1 ... an   牟 corresponds to the robot's angle with respect to the x axis of the field .
﹛in order to retrieve the most similar case  we model the similarity function1 based on the ball position with a 1d gaussian function. we consider two points in the cartesian plane are similar if their distance is below a given threshold. using a gaussian allows us to model the maximum distances in the x and y axis we consider two values to be similar  as well as different degrees of similarities based on the proportional distances of the points. it is defined as follows:

where are the distances between the points in the x and y axis respectively  and 而x 而y  the maximum distances for each axis. these parameters represent the scope of the case  i.e. the region where the points are considered similar  represented by an ellipse on a 1d plane . each case may have different region sizes  hence  for each case we also store this additional information as part of the system knowledge.
﹛when a new problem is presented to the system  we first filter the cases based on the defending goal g: the problem and the case must have the same defending goal. otherwise  they cannot be considered similar at all. next  we compute the similarity between the position of the balls. if the similarity exceeds a given threshold then we consider the case as a potential solution to the problem. we select the case with higher similarity degree for the reusing step.
﹛the problem we address in this paper is how to ensure the robustness of the system given the high uncertainty of the robot's perception. in a cbr system  the correctness of the case base is one of the main issues that determines the accuracy of the system performance  since it represents its knowledge. given a new problem  its solution is obtained by comparing the description of the new problem with the cases in the case base. in our approach the position of the ball in the field  perception  and the parameters 而x and 而y used in the similarity function  knowledge  are crucial for the correct performance of the retrieval step. otherwise  wrong cases might be returned  or even no case at all. in both situations  it could be either because the ball is not correctly positioned or because the parameters have high values  modelling large regions  or low values  modelling small regions .
﹛we are also interested in creating case bases with  essential  cases  meaning that we expect to include cases that are general enough to cover basic situations  but also include cases that represent specific situations: the first ones are represented with larger scopes  while the second ones  with smaller scopes. having these two types of cases allows us to reduce the number of cases to the most relevant ones  which is a desirable property in any real-time response domain.
﹛since we cannot improve the robot's perception  because of hardware limitations and the need of real time response  and finding by hand the right parameters for the reasoning module is very hard  we propose: first  to include in the reasoning model a mechanism to automatically compute the scope of existing cases based on the actual perception of the robot; and second  to introduce an additional mechanism in combination with the former to include new cases in the system if no case is retrieved. with the former we ensure that the regions the cases cover are adapted to the robots believes of the world  and therefore  they will respond according to its perception. and with the latter  the robot has the ability to be  creative  and to act with a new behavior when the system does not return any possible solution.
﹛briefly  our approach is focused on creating an initial case base with partial information so the system itself can complete it either by modifying the scope of the cases  in order to cover the problem space with the minimum number of cases  or introducing new cases when needed. both processes are guided by a human trainer.
1 learning the scope of existing cases
the initial case base of our system is manually created and represents the a priori expert's knowledge. the idea is to provide the system a set of cases which must be adapted to the robot's actual perception. the expert knows several generic situations  prototypical cases  and their corresponding solutions  but cannot predict the real scope of the cases from the robot's point of view. thus  we propose to create an initial case base composed of prototypicalcases with default scopes  而x1 and 而y1  and then let the robot learn them. to this end  the expert should tell the robot if the case retrieved at each time is correct or not. we refer to this process as the training step.
﹛as mentioned in the previous section  we refer to the case's scope as the region of the field where that case should be retrieved. these regions are modelled as ellipses  which correspond to gaussians' projections on a plane  from now on  we will refer directly to the ellipse . in order to learn the scope of the cases  we propose to modify the size of these ellipses varying the gaussian parameters 而x and 而y. to this end  we must define some policy to determine when and how these parameters should be adjusted.
1 when to adjust the values
the goal of increasing or decreasing the size of the ellipse is to reach the  ideal  region that a case should cover. the center of the ellipse representsthe exact position of the ball on the field specified in that case. as we move towards the boundary

figure 1: example of a security region  gray region  and a risk region  white region defined by 污x = 1 and 污y = 1.
of the ellipse  the uncertainty about whether the points belong to the scope of the case increases. we call the set of points next to the center of the ellipse the security region  and those near the boundary of the ellipse the risk region. we define 污x and 污y as the relative size of the security region with respect to the size of the ellipse  each value corresponds to a radius percentage . figure 1 shows an example of these regions.
﹛when the system proposes a solution for a new problem  we use the expert's feedback for tuning the parameters of the retrieved case. if the proposed solution succeeded  the scope of the case is increased. otherwise  it is decreased.
﹛we will first focus on the increasing process. if the problem is located inside the security region  the position of the ball is in this region  the system cannot introduce new knowledge to the case. its current information is robust enough to determine that the problem corresponds to the scope of that case. on the contrary  if the problem is within the risk region the system can confirm the current scope of the case increasing the size of the ellipse. expanding the ellipse results in expanding the security region as well.
﹛problems are incorrectly solved using a case due to scope overestimation. hence  we have to reduce the size of the ellipse. if the ball is inside the security region  we do not decrease the parameters since it corresponds to a robust region. if the problem is within this region and the feedback is negative  we assume that the error is caused by something else  wrong localization  and not because of a wrong knowledge of the system. as an illustration  imagine the following situation: the robot is not well localized and as a consequence  it perceives the position of the ball incorrectly. it could happen that it retrieves the right case given its own perception. but from the external observer perception  the case used to solve that problem is not the right one. therefore  the feedback given to the robot is negative. if the system reduces the ellipse  it could radically reduce the scope of the case  not because it was overestimated  but because of the high imprecision. however  when the problem is inside the risk region  the system does reduce the scope of the case  since the scope overestimation might be the cause of the negative feedback.
﹛in summary  the system enlarges or reduces the scope of a case  i.e. modifies its knowledge  when the problem presented is correctly or incorrectly solved and it is within the case's risk region.
1 how to adjust the values
the first problem is to determine the increasing values for each parameter  而x 而y   i.e. how much should the system increase the size of the ellipse with respect to each axis. we define 汛x and 汛y as the increasing value  and 汛 x and 汛 y as the maximum increasing value for each axis. we propose three

figure 1: case scope evolution. 污x = 污y = 1
increasing policies  we only show the equations for axis x; the same equations are used for axis y :
  fixed: the increasing amount is a fixed value. thus  we define a step function:
if
otherwise
  linear: we compute the increasing value based on a linear function:
if
x
1 otherwise
  polynomial: we compute the increasing value based on a polynomial function:
if
1 otherwise
after computing the increasing values  we update 而x and 而y adding the computed 汛x and 汛y respectively.
﹛the motivation for decreasing the parameters is to reduce the ellipse size so the new problem solved is not considered inside the region anymore. to this end  we equal 而x and 而y to the values in the problem  only if they are higher than the radius of the security region:
if
﹛= x	而xt 1	otherwise
we update 而y in the same way.
﹛updating both values separately and only when the problem is within the risk region prevents from radically reducing the scope of the case. below we describe a simple example to illustrate the approach.
1 example
figure 1 depicts four steps of the training process. the gray region represents the security region  while the dashed ellipse corresponds to the  ideal  scope of the case  defined by the human trainer  we attempt to reach. any problem located within this ideal area produces a positive feedback by the expert. the black dot represents a new solved problem  ball position with respect to the case . figure 1  a  shows the initial stage at time 1  where the scope of the case is minimum

figure 1:  ideal  case base based on the expert knowledge.
 而x1 而y1 . since the new solved problem is within the risk region and the feedback is positive  we proceed to enlarge the size of the ellipse using one of the policies defined.
﹛at time i  figure 1  b   we can observe that the ellipse has increased  but still has not reached the expected size. hence  we continue to enlarge the scope by solving new problems as long as the expert feedback is still positive.
﹛figure 1  c   time t 1  depicts a situation where the ellipse generated is bigger than the expected size. from now on  the feedback may be positive or negative. if a new problem is within the risk region and the feedback is positive  then we would proceed to increase the ellipse. but  if the feedback is negative  then the decreasing process is used to reduce the ellipse. the figure shows an example of this situation. as we can see  the new problem is located in the risk region  but out of the ideal scope. thus  the current scope is reduced  but only by updating 而x since.
﹛figure 1  d  shows the updated scope  where the problem remains outside the scope of the case. as more problems are solved  the scope of the case will converge to the ideal scope.
﹛in conclusion  we distinguish two phases in the training process: growing the scope of the case and converging to the ideal scope. during the first phase  the feedback is always positive and the scope is always being expanded. the second phase occurs once the expected scope is exceeded. then  the feedback could either be positive or negative. the goal of the positive feedback is to enlarge the scope  while the goal of negative feedback is to converge to the ideal scope the human trainer expects.
1 introducing new cases in the system
finding out all possible situations a robot can encounter during its performance is unfeasible. even more so in a domain with high uncertainty  such as the domain we are working with. because of the domain  a real time game   we cannot afford the robot to stop during the game just because it does not  know  what to do in that situation. somehow  the robot must always execute an action at every time step of the game.
﹛after the training step  the knowledge of the system might present some  gaps   i.e. the scope of the cases may not cover the whole field. of course  this depends on the number of cases used during the training. but as we mentioned  on the one hand  the expert cannot define all possible cases. and on the other hand  we focus our approach on initially defining a set of generic situations  allowing the robot to create new ones based on its own experience afterwards.
﹛figure 1 shows an example of a hand coded case base  only 1 cases . for simplicity  we only show a quarter of the field  the positive-positivequadrant . each ellipse representsa predefined case  given knowledge . as we can see  several gaps appear between them  white regions . they represent the regions where the robot will have to acquire new information to increase its knowledge.
﹛a new case is created using the description of the environment  problem description   and a generated game play  solution of the new case . to create a game play  we provide the system a set of possible actions the robot can perform. the combination of these actions correspond to potential game plays. given a new problem to solve  if it does not retrieve any case  either due to imprecision problems or because the problem is actually in a gap  the system generates a random game play1. the robot executes the suggested action and the expert evaluates the correctness of the solution proposed. only if it succeeds  the new case is created.
﹛when a new case is inserted into the system  it is created with a minimum scope  a small ellipse . from that moment on  the evolution of the new case depends on how often the robot reuses it  enlarging or reducing its scope using the mechanism presented previously. the idea is that at the beginning  the new case could seem to be a good solution for that concrete situation  but its actual effectiveness has to be evaluated when the robot reuses it. as time passes  if the scope of the case does not increase  and instead  it is reduced  we can deduce that the case is not useful for the robot's performance. on the contrary  if its scope increases  or at least  it remains stable  then we consider that the case contributes to the system's knowledge.
1 experimentation
this section describes the experiments performed in order to test the approach introduced in the paper. we divide the experimentation in two stages: simulation and real robots.
1 simulation
the goal of this first phase is to examine the behavior of the policies using different values for the parameters presented in section 1. since we had to test different combinations of values  simulation was the fastest way to obtain orientative results. the most relevant were selected for the experimentation with real robots.
﹛we based the experiments on a single case to observe how the different values of the variables affect the evolution of its scope  i.e. the resulting size of the ellipse for the case. the initial case was defined with a small scope  而x = 1 and 而y = 1. the expected outcome was 而x = 1 and 而y = 1. we randomly created 1 problems. every time the case was retrieved  we used the different policies to modify the scope of the case. the experiment was repeated combining the following values:
  security region size  expressed as percentage : 污x = {1 1 1 1 1}
  maximum increasing value  expressed in mm :
汛 x = {1 1 1 1 1}

	 a 	 b 	 c 
figure 1: resulting 而x using the policies:  a  fixed policy.  b  linear policy.  c  polynomial policy.the same values were used for 污y and 汛 y.
﹛for each combination of values  we ran 1 experiments. figure 1 shows the average of the obtained results. on the one hand  汛 x and 汛 y define how much the ellipse may increase at each time. hence  the higher their values  the bigger the resulting scope of the case. on the other hand  污x and 污y determine the size of the security region and the risk region  low values represent small security regions and large risk regions . the risk region determines when the scope of the case has to be modified. as this region increases  there are more chances of modifying the scope as well. thus  for all three policies  the curves tend to increase from left to right.
﹛with respect to the policies  the fixed policy obtains the highest 而x and 而y values  while the polynomial obtains the lowest ones. the former function has a more aggressive behavior  radically increasing the size of the ellipse. the latter function has a more conservative behavior  computing first small increments and then increasing as we reach the boundary of the ellipse. as a consequence  the fixed policy significantly varies between the different parameters  whereas the behavior of the polynomial policy remains more stable although the values of the parameters change. regarding the linear policy  it has an intermediate behavior with respect to the other two  tending more to the fixed policy .
﹛after the experimentation we can confirm that a more conservative policy  low increasing values and small risk regions is the most appropriate combination to obtain the desired scope of the cases. the conclusion is obvious since we are establishing ideal conditions in order to gradually increase the scope of the cases. but two problems arise when extending the experiments to the real world: time and uncertainty. first  the number of iterations needed to reach the expected result is unfeasible when working with real robots; and second  a noise-free environment is only available under simulation. although we have observed different behaviors in the graphics obtained when gradually modifying the parameters  these differences are not so obvious in a real environment because other issues modify the expected result. therefore  the next stage is to experiment in the real world with the most relevant parameters  understanding relevant as the ones that show more contrasting behaviors  to determine the effectiveness of the approach presented.
1 real robots
three types of experiments were performed with real robots. the first one aims to find out the most appropriate parameters and policy to use in the system. the second one consists in evaluating the convergence of the cases in a given case base. finally  the goal of the third one is to observe if the system is able to acquire new knowledge when no solution is found.
testing theparametersand policies as we mentioned we can divide the training process in two steps: growing the scope of the case and converging to the ideal scope. we are interested in rapidly enlarging the size of the ellipse until reaching the ideal one  and then opt for a more conservative behavior to adjust it. we switch from one strategy to the other when the size of the ellipse is decreased.
﹛we can achieve this strategy either by combining the policies or by modifying the values of the parameters through the process. regarding the policies  we have included two additional strategies: fixed or linear policy for the growing step  and the polynomial for the convergence step. with respect to the parameters  for the first step we define large risk regions and high increasing values  and the opposite for the second step.
﹛the experimentation is similar to the simulation stage  where the experiments are based on a single case. the expected scope of the case is 而x = 1 and 而y = 1. we generated 1 new problems  manually positioning the ball in the field. each experiment combined the five policies with three different sets of parameters:  i  污x = 1  汛 x = 1;  ii  污x = 1  汛 x = 1;  iii  污x = 1  汛 x = 1 and 污x = 1  汛 x = 1  the former are for the growing process  and the latter  for the convergence process . the same values were used for 污y and 汛 y. each parameter varies separately depending on the 而 altered  modifying 而x does not imply modifying 而y as well . we performed 1 trials per set.
﹛comparing the results with respect to the expected scope  we verify that:  i  the fixed and linear policies generate the highest 而x values exceeding it;  ii  the polynomial policy does not even reach the ideal scope because of the low increasing speed;  iii  both fixed-polynomial and linear-polynomial strategies obtain the closest scopes to the expected ones  since they combine the advantages of both policies.

	 a 	 b 	 c 	 d 
figure 1: case base evolution:  a  initial case base.  b  during training.  c  resulting case base.  d  acquiring a new case.﹛regarding the values of the parameters  we confirmed the conclusions drawn from simulation: combining low increasing values and small risk regions ensures reaching the expected result. the problem once again is the number of steps for achieving this goal. hence  combining the values of the parameters -first high increasing values and large risk regions and then the opposite- results in a good alternative: we reach the ideal scope faster and then progressively adjust it.
training the case base we created a simple case base of four cases  figure 1  a  :
  center: midfield. action: get the ball and kick forward.
  side: left edge. action: get the ball and kick right .
  corner: left corner. action: grab the ball  turn until reaching 1 degrees and kick forward.
  front: between the center case and the goal. action: grab the ball  face goal and kick forward.
all cases were initiated with the same scope  而x = 而y = 1 . we performed 1 trials  each with 1 random problems  for the training process. figure 1  b  shows the final steps of the process where the size of the ellipse is converging towards the final outcome. the solved problems are represented with crosses  center case   circles  side case   plus  corner case  and squares  front case . finally  figure 1  c  shows the average of the 1 trials outcomes. as we can see  the initial case base successfully evolves into the expected case base that was designed  figure 1 .
acquiring knowledge once the case base is trained  the robot is ready to acquire new cases. the goal is to verify that the robot is able to fill in the gaps of the trained knowledge. we focused the experiment on learning a single case located between the four cases. the expected action was to get near the ball facing the goal and bump it  the intention is to bring the ball closer to the goal without a forward kick since it is too forceful and would push the ball out of the field .
﹛we performed1 trials  each composed of 1 randomproblems. through all the trials the new case was created. figure 1  d  shows the scope  average of the 1 trials  of the new case after expanding it. as we can see  the gap is almost completely covered with the expected case.
1 conclusions and future work
the aim of the this work is to obtain a robust case base  knowledge from the robot's own observations. the scope of the cases are representedby projectionsof gaussian functions on a plane  ellipses . we proposed a mechanism to adjust the size of these ellipses to their corresponding  ideal  sizes indicated by a trainer  but taking into account the robot's perception. this way we ensure that the errors in its perception are also taken into account when modelling its knowledge.
﹛we also included an automatic mechanism to acquire new cases when no case is retrieved. in combination with the former process  the case base is not only more robust  but is enlarged as well with the most relevant cases.
﹛several experiments have been performed. a first stage to test the parameters and proposed policies  both under simulation and with real robots. and a second stage focused on evaluating the evolution of the case base and the incorporation of new cases in the system.
﹛after verifying that the proposed approach works  we are ready to try more complex situations with the whole system. we also expect to implement a scalable game play generator  which will allow us to introduce more sophisticated solutions for new unresolved problems.
