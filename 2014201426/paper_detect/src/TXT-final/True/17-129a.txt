training and tracking in robotics* 
oliver g. selfridge and richard s. sutton gte laba  waltham  ma 1 
andrew g. barto 
department of computer and information science 
university of massachusetts  amherst  ma 1 
     
a b s t r a c t 
     we explore the use of learning schemes in training and adapting performance on simple coordination tasks. the tasks are 1-d pole balancing. several programs incorporating learning have already achieved this  1  s  1 : the problem is to move a cart along a short piece of track to at to keep a pole balanced on its end; the pole is hinged to the cart at its bottom  and the cart is moved either to the left or to the right by a force of constant magnitude. the form of the task considered here  after  1   involves a genuinely difficult credit-assignment problem. we use a learning scheme previously developed and analysed  1  1  to achieve performance through reinforcement  and extend it to include changing and new requirements. for example  the length or mast of the pole can change  the bias of the force  its strength  and so on; and the system can be tasked to avoid certain regions altogether. in this way we explore the learning system's ability to adapt to changes and to profit from a selected training sequence  both of which are of obvious utility in practical robotics applications. 
     the results described here were obtained using a computer simulation of the pole-balancing problem. a movie will be shown of the performance of the system under the various requirements and tasks. 
i introduction 
     the importance of good training experience it well recognised in pattern classification and inductive inference  where careful choice of rule exemplars and counter-exemplars clearly affects learning progress  e.g.  ref.  . it is also important for learning in symbolic problem solving as illustrated by the problem generation component of lex . here we show that similar pedagogic care can be significant in nonsymbolk problem solving of the kind that is important in robotics; namely  problems of learning to control physical dynamical systems. 
     it is sometimes faster for a system to warn to solve a different problem from the one assigned and then to adapt that solution once it it learned. and it it usually far fatter for a system to adapt to new requirements by modifying old solutions than for it to start again from scratch. in the case of learning to control a physical system  it may be much easier  for example  to first learn to control a related system with simpler dynamics  and then to continue to learn at that system is deformed  continuously or by a sequence of steps  into the system required. this it noi an approach that lends itself to orthodox control theory due to the difficulty of adjusting to nnforeseen changes in dynamics and task requirements. 
     a related issue is that the ability of a system to adjust to unforeseen changes in circumstances and requirements it important even if it 
* this research was supported by the air force office of scientific research and the avionks laboratory  air force wright aeronautical laboratories  through contract f1-1. 
does not lead to increased learning speed through training. although an obvious role of learning is to construct a knowledge base through experience in domains where there is little a priori knowledge  another role that receives less attention is to track a moving optimum  either incrementally or non-incrementally  as unforeseen changes take place. 
     here we show how both training and tracking can be done with a learning system that was described by barto  sutton  and anderson . our domain is the classic problem of balancing a pole in one dimension. this problem has the advantage that it has been considered by several researchers  1  1   and it can be made quite difficult by adopting certain assumptions. specifically: 
a rigid pole is hinged to a cart  which is free to move within the limits of a 1-d track. the learning system attempts to keep the pole balanced and the cart within iti limits by applying a force of fixed magnitude to the cart  either to the left or to the right. 
this task is susceptible to a number of different learning schemes depending on the quality of the evaluative feedback provided to the learning system. here we consider a version of the task similar to the one studied by michie and chambers  in which the only evaluative feedback occurs on failure-hitting the stops with the cart  or having the pole fall over. this sparsity of evaluative feedback create! a genuinely difficult credit-assignment problem. since a failure usually occurs only after a long sequence of individual control decisions  it is difficult to determine which decisions were responsible for it. to solve this problem  the system identifies certain regions as being in some sense  close  to failure  and will try to avoid them as well. the back-propagation method for generating this internal evaluation is like samuel's method  1   but refined and improved by sutton . 
     the learning system usually starts with an empty experiential knowledge base  and it takes some time to fill it with enough data for the system to perform well. we first show that to learn a new task-for example  one with very different parameters  like a vastly heavier pole-can be easier when the system adjusts its knowledge base from a successful state than from scratch. while it is obvious that a suitably primed knowledge base ought to facilitate learning compared to  tabula rata  learning  our point is that the initial knowledge can be that acquired from learning to solve an easier task  which may itself have been learned at a result of facing an even easier task. other results show this explicitly. 
     we also consider tasks that add a constraint  such as avoiding particular pole positions. in these tasks  the learning system must adapt to the constraint without explicit instruction as to how to do it. systems capable of discovering  how  to accomplish a goal  specified only in terms of  what   are more powerful than those that require more explicit specifications. such  operationalization* capabilities are ihown by systems like ours that modify behavior without performing adaptive model reference system identification  as is usual in adaptive :ontrol theoretic techniques. that is  what is used here is a  learning by discovery  method that owes more to ai than to conventional adaptive control. 
ii technical approach and experimental results 
     although the state space of the cart-pole system is continuous- -hat is  the state variables are continuous-we divide the four of them discretely  as in : 

which yield together 1 regions corresponding to the combinations. the system is always in just one region. the job of the learning system  then  is to assign the proper action to each region  so that the system will act correctly. the learning algorithm is given in the appendix and is discussed in detail in refs.  and . 
     the cart-pole system was simulated on a vax-1  using the equations of motion given in  and the following initial parameter values: 

there are two small coefficients of friction  one for the cart and the other for the pole. in the initial state the pole is stationary and upright  and the cart is stationary in the middle of the track. since there is always a force  the pole will not long stay upright. the time from initial position to failure-the cart or pole hitting a stop-is considered one learning trial. time was discretised to a fiftieth of a second. the system was judged to have succeeded in balancing the pole when it achieved a trial that continued without failure for 1 time steps  corresponding to some 1 minutes real time. we used the number of failures before reaching this criteria of success as some measure of the power and efficiency of learning. a run using the standard parameters listed above typically might have 1 trials before a criterion trial is achieved. 
     in the first set of tasks the system was required to adapt to changes in the parameters of the cart-pole system. one task required adaptation to bias in the force  which changed from +1 and -1 newtons to +1 and - 1   and to +1 and -1. this can be considered an  inaccurate  approximation to tilting the track right and left. that problem was solvable: the control surface learned was specific to the direction of the tilt  and a system trained for one tilt did not work well for another. however  after several switches of the direction of tilt  with training to criterion for each  the system was eventually able to balance the pole each way without failure. that is  the system had generalised its solution to satisfy all the problems simultaneously. 
¡¡¡¡the second task required adaptation to an increase in the mass of the pole. the initial training used the standard .1kg pole to the criterion of 1 steps without failure. then the mass of the pole 
	1. selfridge et al. 	1 
was increased one order of magnitude  to 1kg . this made the problem much harder  and performance dropped accordingly. however  criterial performance was soon regained: initial training took an average of 1 failures; subsequent adaptation to the heavier pole took only 1. on the other hand  without pre-training  learning to balance the heavier pole took an average of 1 failures. it is clearly more efficient to use a previous solution as the starting place for this new task than to start from scratch. 
     the system finds it harder to learn to handle shorter poles. if we started training with a pole reduced in length and mass to two-thirds of the original lm/.lkg pole  it took 1 failures on average to reach criterion  compared to 1 with the full-sise pole. when the system was first trained to criterion on the full-sise pole and then switched to the short pole  however  only 1 additional failures were incurred on average after the switch  for a total of 1 failures overall. this result shows not only adaptability to changing requirements  but also improvement in learning rate with  directed* training. 
     directed training resulted in a larger advantage in switching to a shorter track. with the track length 1 meters  instead of 1m   learning took more than 1 failures on the average. but training first at 1m  then at 1m  1m  and finally at 1m  each to criterion  took merely 1 + 1 + 1 + 1- 1 failures. 
     the final task required learning to avoid some region of state space; namely  the region in which the pole is near vertical  ¡À1¡ã . we added a penalty  equal to 1 the penalty for failure  for being within this region. despite the fact that remaining in this state is the  natural  way to balance the pole  the learning system reduced the proportion of time spent there from 1% to 1%. increasing the penalty 
reduced the fraction still further  but then hitting the stops became more attractive relatively  and balancing failures occurred more often. 
	ill 	analysis and discussion 
     the training illustrated in these results consists of selecting a sequence of control tasks that ends with the required task and that presents a graded series of difficulties. since this kind of sequence consists of entire problem-solving tasks  rather than exemplars and counter-exemplars of a pattern class  it is quite different from the usual training sequence in'supervised learning pattern classification or concept formation. the potential utility of this type of training seems clear; these results show that the learning system considered here is in fact able to benefit from it. the learning system treated here is  moreover  a simple one; it does not deal with hierarchical control at all  nor does it take advantage of a specific accessible knowledge base. 
     part of the utility of the training in our examples is due to the fact the external evaluative feedback is in the form of a binary signal- failure or not. such systems are always subject to what has been termed the  mesa phenomenon  . what is needed is a more continuous feedback so that the system can become better and better  rather than merely satisfy some set of binary constraints. one observes this constantly in people performing physical tasks; it would obviously be desirable in robotics as well. one mechanism used by the learning system to provide a more continuous evaluation is the part of the algorithm that constructs an internal evaluation signal. training provides another way of reducing the effect of the mesa phenomenon by effectively leading the system to better regions of the solution space. significantly  this can be done by manipulating aspects of the task without using detailed knowledge of what the performance surface is 
     
1. selfrldge et al. 
like. 
     another way to ease the problems caused by the mesa phenomenon is to provide our robotics systems not with just one purpose  but with a structure of purposes: the first purpose to be satisfied is to fulfill the constraints and avoid outright failure-that is  to keep the pole upright and to avoid allowing the cart to crash into the ends of the track. next the system might try to keep the pole upright  by reducing the average magnitude of 1 ; after that  it might try to minimise the angular velocity  so as to keep the pole still. beyond that  we can imagine a universal purpose of wanting to do all the above with minimum work  just as people's physical efforts become more and more efficient with practice. 
     related to the issue of training  and equally important  is the ability of the learning system to track a changing optimum for the control parameters. in robotics  there are practical reasons for this: the dynamics change with time  sises and viscosities change with temperature  bearings and surfaces suffer wear  and so on. such problems are clearly not feasible to solve analytically  and in any case  the point is to provide systems whose behavior is robust enough to handle unexpected as well as expected situations. 
acknowledgements 
     the authors thank charles w. anderson for assistance with the pole-balancing experiments. 


