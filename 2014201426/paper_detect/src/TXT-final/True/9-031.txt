 
   a programming language is described which is designed to simplify the construction of computer programs to analyze english. this system attempts to merge the best features of pattern matchers and the phrase structure approach to language analysis. several practival problems which occur in dealing with such a system are described. 
introduction 
　　　　　why is it so difficult for machines to understand natural language  perhaps it is because machines do not simulate sufficiently what humans do when humans process language. several years of experience with computer science and linguistic approaches have taught us the scope and limitations of syntactic and semantic parsers. schank tesler and weber 1 simmons 1 winograd 1 woods1  . while extant linguistic parsers perform satisfactorily with carefully edited text sentences or with small dictionaries   they are unable to deal with everyday language behavior characteristic of human conversation. in a rationalistic quest for certainty and attracted by an analogy from the proof theory of logicians in which provability implied computability  computational linguists hoped to develop formalisms for natural language grammars. but the hope has not been realized and perhaps in principle cannot be.  it is difficult to formalize something which can hardly be formulated . 
         linguistic parsers use morphographemic analyses  parts-of-speech assignments and dictionaries containing 
* this research is supported by grant phs mh 1 from the national institute of mental health  by  in part  research scientist award  no. 1-k1-k1  from the national 
institute of mental health to the second author and  in part  by the advanced research projects agency of the office of the secretary of defense sd-1 . 
multiple word-senses each possessing semantic features  programs or rules for restricting word combinations. such parsers perform a detailed analysis of every word  valiantly disambiguating at each step in an attempt to construct a meaningful interpretation. while it may be sophisticated computationally  a conventional parser is quite at a loss to deal with the caprice of ordinary conversation. in everyday discourse people speak colloquially and idiomatically using all sorts of pat phrases  slang and cliches. the number of special-case expressions is indefinitely large. humans are cryptic and elliptic. they lard even their written expressions with meaningless fillers and fragments.they convey their intentions and ideas in idiosyncratic and metaphorical ways  blithely violating rules of 'correct' grammar and syntax. given these difficulties  how is it that people carry on conversations easily most of the time while machines thus far have found it extremely difficult to continue to make appropriate replies indicating some degree of understanding  
　　　　　it seems that people 'get the message1 without always analyzing every single word in the input. they even ignore some of its terms. people make individualistic and idiosyncratic selections from highly redundant and repetitious communications. these personal selective operations  based on idiosyncratic intentions  produce a transformation of the input by destroying and even distorting information. in speed reading  for example  only a small percentage of contentive words on each page need be looked at. these words somehow resonate with the readers relevant conceptualbelief structure whose processes enable him to 'understand1 not simply the language but all sorts of unmentioned aspects about the situations and events being referred to in the language. normal written english text is estimated to be 1 redundant  rubenstein and haberstroh1 . spoken conversations in english are prooably better than 1. redundant carroll' . words can be garbled and listeners nonetheless get the gist or drift of what is being said. they see the  pattern  and thus can supply much of what is missing 
　　　　　to approximate such human achievements we require a new perspective and a practical method which differs from that of current linguistic approaches. this alternate approach should incorporate those aspects of parsers which have been found to work well  e.g.  detecting embedded clauses. also individualistic features characteristic of an idiolect should have dominant emphasis. parsers represent complex and refined algorithms. while on one hand 
1 they subject a sentence to a detailed and sometimes overkilling analysis  on the' other  they are finicky and oversensitive. a conventional parser may simply halt if a word in the input sentence is not present in its dictionary. it finds ungrammatical expressions such as double prepositions  'do you want to get out of from the hospital '  quite confusing. parsers constitute a tight conjunction of tests rather than a loose disjunction. a1 more and more tests are added to the conjunction  the parser behaves like a finer and finer filter which makes it increasingly difficult for an expression to pass through. parsers do not allow for the exclusions typical of everyday human dialogues. 
　　　　　finally  it is difficult to keep consistent a dictionary of over 1 multiple-sense words classified by binary semantic features or rules. for example  suppose a noun  ni  is used by some verbs as a direct object in the semantic sense of a physical object. then it is noticed that ni is also used by other verbs in the sense of a location so 'location1 is added to ni's list of semantic features. now ni suddenly qualifies as a direct object for a lot of other verbs. but some of the resultant combinations make no sense even in an idiolect. if a special feature is then created for ni  then one loses the power of general classes of semantic features. adding a single semantic feature can result in the propagation of hidden inconsistencies and unwanted side-effect.  as the dictionary grows it becomes increasingly unstable and difficult to control. 
　　　　　early attempts to develop a pattern-matching approach using special-purpose heuristics have been described by colby  watt and gilbert 1 weizenbaum  and colby and enea.1 the limitations of these attempts are well known to workers in artificial intelligence. the man-machine conversations of such programs soon becomes impoverished and boring. such primitive context-restricted programs often grasp a topic well enough but too often do not understand quite what is being said about the topic  with amusing or disastrous consequences. this shortcoming is a consequence of the limitations of a pattern- matching approach lacking a rich conceptual structure into which the pattern abstracted from the input can be matched for inferencing. these programs also lack a subroutine structure  both pattern directed and specific  desirable for generalizations and further analysis. 
         the strength of these pattern matching approaches lies in their ability to ignore some of the input. they look for patterns  which means the emphasis of some detail to the exclusion of other detail. thus they can get something out of nearly every sentence-- sometimes more  sometimes less. 
         an interesting pattern-matching approach for machine translation has been developed by wilks.1 his program constructs a pattern from english text input which is matched against templates in an interlingual data base from which in turn  french output is generated without using a generative grammar. 
         in the course of constructing an interactive simulation of paranoia we were faced with the problem of dealing with unedited and unrestricted natural language as it is used in the doctor-patient situation of a psychiatric interview. colby  hilf  weber  and kraemer   colby and hilf1 . this domain of discourse admittedly contains manypsychiatricelly stereotyped expressions and is constrained in topics  newton's laws are rarely discussed . but it is rich enough in verbal behavior to be a challenge to a language understanding algorithm since a variety of human experiences are discussed domain including the interpersonal relation which develops between the interview participants  a look at the contents of a thesaurus reveals that words relating to people and their interrelations make up roughly 1 of english words. 
         the diagnosis of paranoia is made by psychiatrists relying mainly on the verbal behavior of the interviewed patient. if a paranoid model is to exhibit paranoid behavior in a psychiatric interview  it must be capable of handling dialogues typical of the doctor-patient context. since the model can communicate only through teletyped messages.the vis-a-vis aspects of the usual psychiatric interview are absent. therefore the model must be able to deal with unedited typewritten natural language input and to output replies which are indicative of an underlying paranoid thought process during the episode of a psychiatric interview. 
　　　　　in an interview there is always a who saying something to a whom with definite intentions and expectations. there are two situations to be taken into account  the one being talked about and the one the participants are in. sometimes the latter becomes the former. participants in dialogues have intentions and dialogue algorithms must take this into account. the doctor's intention is to gather certain kinds of information while the patient's intention is to give information and get help. a job is to be done; it is not small talk. our working hypothesis is that each participant in the dialogue understands the other by matching selected idiosyncratically- significant patterns in the input against conceptual patterns which contain information about the situation or event being described linguistically. this understanding is communicated reciprocally by linguistic responses judged appropriate to the intentions and expectations of the participants and to the requirements of the situation. in this paper we shall describe only our current input-analyzing processes used to extract a pattern from natural language input. in a later communication we shall describe the inferential processes carried out at the conceptual level once a pattern has been received by memory from the input-analysing processes. 
1          studies of our 1 model of paranoia  parry  indicated that about thirty percent of the sentences were not understood at all   that is  no concept in the sentence was recosnized. in a somewhat larger number of cases some concepts  but not all  were recognized. in many cases these partially recognized sentences lead to a partial understanding that was sufficient to gather the intention of the speaker end thus lead to output an appropriate response. however  misunderstandings occurred too often. for example: 
doctor: how old is your mother   
	parry: 	twenty-eight 
parry has interpreted the question as referring to his own age and answered by giving his age. the purpose of our new language analysis system is to significantly raise the level of understanding by preventing such misunderstandings while not restricting what can be said to parry. we do not expect complete under- standing from this system ~ even native speakers of the language do not completely understand the language. 
　　　　　by 'understanding we mean me system can do some or all of the following: 
1  determine the intention of the interviewer in making a particular utterance. 
1  make common logical deductions that follow from the interviewers utterance 
1  form an idioletic internal representation of the utterance so that questions may be answered  commands carried out  or data added to memory. 
1} determine references for pronouns  and other anaphora. 
1  deduce the tone of the utterance !.e.  hostile  insulting... 
1  classify the input as a question  rejoinder.command ... 
         the approach we are taking consists of merging the best features of pattern directed systems such as the mad doctor 1 eliza  and parsing directed systems for example  
winograd 1 woods.1. by merging the bnf phrase structure approach ot analyzing english with the pattern matching approach  with its attendant emphasis of some concepts to the exclusion of others. the programs to accomplish this are written in ml1sp1  an extensible version of the programming language ml $p 1 and uses an interpreted version of the pattern matcher designed for a new programming language lisp 1. 
         the following is a basic description of the pattern matcher. we shall illustrate its operation using examples of problems common to teletyped psychiatric dialogues. 
pattern matching 
         pattern directed computation involves two kind of operations 	on 	data 	structures: 	decomposition 	and recomposition. decomposition breaks down an input stream into components under the direction of a decompostion pattern   dec  . the inverse operation  recomposition  constructs an output stream under the direction of a recomposition pattern   rec  . 
a rewrite rule is of the form; 
dec --  rec 
it defines a partial function on streams as follows: if the input stream matches the dec  then the output stream is generated by the rec. the following rule  given as an example only  could be part of a question answering function: 
how are you   -  very well and you   
if the input stream consists of the four tokens: 
how are you   
the output stream will consist of the five tokens: 
very weii and you   
rewrite functions 
         a rewrite rule defines a partial function  for example  the mapping of some particular token into some other particular token. a broader partial function can be defined as the union of several rewrite rules. a rewrite function definition is of the form: 
rules of  name  decl -* r e e l   dec1 -  rec1  decn -  recn; 
variables 
　　　　　a function is difficult to define if every case must be enumerated. therefore  rewrite rules allow variables to appear in patterns. the value of a variable can be either a list or an atom. in this paper the notation: 
:x 
where x ia any identifier  will denote the variable x. the variables of each rule are distinct from the variables of all other rules  even if their names are the same. 
         the following definition has only ihree rewrite rules  but handles an unlimrted number of input streams: 



the surrounding a pattern means that the current input stream is to be pushed down  that the function indicated by the first token within the brackets is to be entered with the rest of the pattern appended to the front of the input stream  and that the output stream is to be placed into the restored current input stream. note that mlisp1 functions may be called as well as rewrite functions. 
goals 
         to gain the advantage of goal directed pattern matching and computing  as well as the full power of context sensitive grammars  the following form may be used: 
the identifer between the angled brackets  names a rewrite function the rules of which are to be matched against the input stream. when a match occurs the output stream of the goal will be bound to the associated variable. example; 
rules of auxilary phrase -
 auxllary ia 	  negative :n1 	n1  aux ph :a  : n :nl  : 
if the optional pattern  enclosed in square brackets {     
occurs in the input stream it will be bound to :n. :n1 will be bound to 1. if the  negative  does not occur  :n1 will be bound to 1. on the rec side of the rules if :nl is 1 then :n will be placed in the output stream. if it is 1 then nothing is placed in the output stream at that point. example  given the rule above: 

more examples 
　　　　　we have collected a large number of dialogues using our previous program parry. these dialogues form a large body of examples of the kind of english which we can expect. martin frost  a graduate student in computer science  .stanford university  has written a keyword in context program which enables us to isolate examples centered on particular words so that uses of thoses words in context become more apparent. our general approach is to build a system which can produce desired intreptations from these examples and to incrementally add to the rules in the system as new cases are discovered during the running of the program. 
   following are some examples of commonly occuring situations and examples of the kind of rules we use to handle 
　　　　　although it is conceivable that there are an infinite number of ways to introduce a question in this manner  we have found only about six literal strings are actually used in our data base of dialogues. when we discover a new string we incrementally add a rule. when we have enough examples 
to dectect a more genera  farm we replace the rules for  questionjntr1ducer  by a more elegant and general formulation. this approach allows us to process dialogues before we have a complete analysis of all possible sentence constructions  and it allows us to build a language analyzer based on actually occurring forms. 
           notice that it is possible to make more than one analysis of any given sentence depending on what is being looked for. a poet might be interested in the number of syllables per word and. the patterns of stress. a  full  analysis of english must allow for this possibility  but it it clearly foolish to produce this kind of analysis for parry. our analysis will be partial and idiosyncratic to the needs of our program. this is what is meant by idiolectic. 
fillers 
           it is quite common for interviewers to introduce words of fittle significance to parry into the sentence. for example: 
hell  what is your nahe  
the  well  in this sentence serves no purpose in parry's analysis  although it might to a linguist interested in hesitation phenomena. these fillers can be ignored the following rules accomplish this: 
rules of sentence -
 f llers ;f  sentence :s - :s; 
rules of fillers -
well -   
ok - ; 
punctuation 
           interviewers use little intra-sentence punctuation in talking to parry. when it is used it is often to seperate phrases that might otherwise be ambiguous. example: 
why weren't you very close  frank 
here the comma clearly puts  close  in a different phrase from  frank . punctuation  when used in parry's rules  is generally enclosed in optional brackets     . this has the effect of separating phrases when punctuation is used  but not requiring full punctuation for the system to work  example; 
rules of sentence -
 sentence :s1 c.jtc  sentence connector :sc 
 sentence :s1 
-  conunction sc :s1 :s1 | 
cliches and idioms 
	the 	english 	we 	encounter 	in 	doctor-patient 
dialogues is made up of a great number of cliches and idioms  therefore we anticipate a large number of rules devoted to them. for example: 
rules of time phrases a couple of  time unit ;t ago 
*  tihe  relative past  ref present  :t ; 
rules of tine un1t -
seconds - within conversation   
moments - within conversation   
days -  before conversation days : 
representation correction 
           intermediate results are often produced which are misleading in meaning or are in the wrong form for further processing. we  therefore  incorporate at various points rules which detect certain undesired intermediate results and convert them to the desired form. example: 

unknown words 
           rules can be derived to handle words which were previously unknown to the system. for example: 
rules of unknown word -
or'.  x -  new w1rd name :x   
the :x  verb phrase :y * 
 neu u1rd noun :x   
i :x you -   nbj u1 verb :x i 
here  new word  is a function which adds new words to the dictionary. 
conclusion 
           we are faced with the problems of natural language being used to interview people in a doctor-patient context. we have developed a language processing system which we believe is capable of performing in these interviews at a significantly improved level of performance compared to systems used in the past. we have developed techniques which can measure performance in comparison with the ideal of a real human patient in the same context  '1 we are designing our system with the realization that a long period of development is necessary to reach desired levels of performance  this is a system that can work at a measured level of performance and be improved over time with new rules having minimum interaction with those already existing. our system is designed so that a complete analysis of every word or phrase of an utterance is not neceesary. 
           the basis of this system is a rewrite interpreter which will automatically merge new rules into the set of 

1 

already existing rules so that the system will continue to handle sentences which it handled in the past. 
