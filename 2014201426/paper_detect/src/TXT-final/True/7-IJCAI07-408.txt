
agents often have to construct plans that obey deadlines or  more generally  resource limits for real-valued resources whose consumption can only be characterized by probability distributions  such as execution time or battery power. these planning problems can be modeled with continuous state markov decision processes  mdps  but existing solution methods are either inefficient or provide no guarantee on the quality of the resulting policy. we therefore present cph  a novel solution method that solves the planning problems by first approximating with any desired accuracy the probability distributions over the resource consumptions with phasetype distributions  which use exponential distributions as building blocks. it then uses value iteration to solve the resulting mdps by exploiting properties of exponential distributions to calculate the necessary convolutions accurately and efficiently while providing strong guarantees on the quality of the resulting policy. our experimental feasibility study in a mars rover domain demonstrates a substantial speedup over lazy approximation  which is currently the leading algorithm for solving continuous state mdps with quality guarantees.
1 introduction
recent advances in robotics have made aerial  underwater and terrestrial unmanned autonomous vehicles possible. in many domains for which such unmanned vehicles are constructed  the vehicles have to construct plans that obey deadlines or  more generally  resource limits for real-valued resources whose consumption can only be characterized by probability distributions  such as execution time or battery power. it is a major challenge for ai research to construct good plans for these domains efficiently  bresina et al.  1 . one cannot model the resulting planning problems with generalized semi-markov decision processes  younes and simmons  1  since they can model actions that consume real-valued amounts of resources but not resource limits. one could model them with markov decision processes  mdps  by encoding the available amounts of resources in the states. the resulting continuous state mdp can then be solved approximately by discretizing the state space  for example  by discretizing the probability distributions  and then applying standard dynamic programming algorithms  such as value iteration  boyan and littman  1 . a major drawback of this approach is that discretizing the state space results in a combinatorial explosion of the number of states as the discretization becomes more and more fine-grained. in response  discretization-free approaches have been developed  some of which do not provide strong guarantees on the quality of the resulting policy  lagoudakis and parr  1; nikovski and brand  1; hauskrecht and kveton  1 . lazy approximation  li and littman  1  is currently the leading discretization-free algorithm for solving continuous state mdps with quality guarantees. it solves the planning problems by approximating the probability distributions over the resource consumptions with piecewise constant functions. it then uses value iteration to solve the resulting mdps by repeatedly approximating the value functions with piecewise constant value functions. lazy approximation has two problems that increase its runtime: first  complex piecewise constant functions are often necessary to approximate probability distributions closely. second  the value functions need to be reapproximated after every iteration. we thus present cph  continuous state mdps through phase-type probability distributions   a novel solution method that solves the planning problems by first approximating the probability distributions over the resource consumptions with phase-type distributions  which use exponential distributions as building blocks  neuts  1 . it then uses value iteration to solve the resulting mdps by exploiting properties of exponential distributions to calculate the value functions after every iteration accurately  = without reapproximations  and efficiently. cph shares with lazy approximation that it provides strong guarantees on the quality of the resulting policy but runs substantially faster than lazy approximation in our experimental feasibility study in a mars rover domain.
1 the planning problem
we use time as an example of a real-values resource and solve planning problems that are similar to the ones studied in  li

figure 1: mars rover domain.
and littman  1 . they are modeled as mdps with two sources of uncertainty: action durations and action outcomes. s denotes the finite set of states of the mdp and a s  the finite set of actions that can be executed in state s ¡Ê s. assume that an agent is in state s ¡Ê s with a deadline being t   1 time units away  = with time-to-deadline t   after which execution stops. it executes an action a ¡Ê a s  of its choice. the execution of the action cannot be interrupted  and the action duration t is distributed according to a given probability distribution that can depend on both the state s and the action  then the time-to-deadline after the action execution is non-positive  which means that the deadline is reached and execution stops. otherwise  with probability   the agent obtains reward  and transitions to state with time-toand repeats the process. the time-to-deadline at the beginning of execution is ¦¤   1. the objective of the agent is to maximize its expected total reward until execution stops.
we use a mars rover domain  figure 1  that is similar to one used in  bresina et al.  1  as an example of our planning problems. a rover has to maximize its expected total reward with time-to-deadline ¦¤ = 1. the states of the mdp correspond to the locations of the rover: its start location  site 1  site 1  site 1 and its base station. the rover can perform two actions with deterministic action outcomes outside of its base station:  1  it can move to the next site and collect a rock probe from there. it receives rewards 1  1 and 1 for collecting rock probes from sites 1  1 and 1  respectively.  1  it can also move back to its base station to perform a communication task  which drains its energy completely and thus makes it impossible for the rover to perform additional actions. it receives reward 1 for performing the communication task. all action durations are characterized by the exponential distribution. we also consider the case where all action durations are characterized by either weibull or normal distributions. the mars rover domain might appear small with only 1 discrete states. however  if one discretizes the time-to-deadline into 1 time units and encodes it in the states  then there are already 1 = 1 states.
1 general approach
we can solve our planning problems by encoding the timeto-deadline in the states  resulting in a continuous state mdp that can be solved with a version of value iteration  boyan and littman  1  as follows: let v   s  t  denote the largest expected total reward that the agent can obtain until execution stops if it starts in state s ¡Ê s with time-to-deadline 1 ¡Ü t ¡Ü ¦¤. the agent can maximize its expected total reward by executing the action
arg max
 in state s ¡Ê s with time-to-deadline 1 ¡Ü t ¡Ü ¦¤  which can be explained as follows: when it executes action a ¡Ê a s  in state s ¡Ê s  it incurs action duration t with probability
  then it transitions to statewith
probability and obtains an expected total future reward of.
value iteration calculates the values v n s  t  using the following bellman updates for all states s ¡Ê s  time-todeadlines 1 ¡Ü t ¡Ü ¦¤  and iterations n ¡Ý 1
v 1 s  t :=1
t ¡Ü 1
	1	.
statesit then holds thats ¡Ê s and times-to-deadlinelimn¡ú¡Þ v n s  1t  =¡Ü t v¡Ü  ¦¤s  . unfortu-t  for all
nately  value iteration cannot be implemented as stated since the number of values v n s  t  is infinite for each iteration n since the time-to-deadline t is a real-valued variable. we remedy this situation by viewing the v n s  as value functions that map times-to-deadline t to the corresponding values v n s  t   similar to  liu and koenig  1 . in this context  we make the following contributions: first  we show after how many iterations n value iteration can stop so that the approximation error maxs¡Ês 1¡Üt¡Ü¦¤ |v   s  t  v n s  t | is no larger than a given constant. second  we show that each value function v n s  can be represented exactly with a vector of a small number of real values each. finally  we show how the bellman updates can efficiently transform the vectors of the value functions v n s  to the vectors of the value functions v n+1 s . we present these three contributions in the following as three steps of cph.
1 cph: step 1
 cph first approximates the probability distributions of action durations that are not exponential with phase-type distributions  resulting in chains of exponential distributions  with potentially different constants ¦Ë   1  neuts  1 . cph then uses uniformization  i.e.  creates self-transitions  to make all constants ¦Ë identical without changing the underlying stochastic process. we do not give details on how to implement these two well-known steps  see  neuts  1; younes and simmons  1  for details  but rather provide an example. figure 1 shows how an action with a deterministic action outcome and an action duration that is characterized by a normal distribution n ¦Ì = 1 ¦Ò = 1  is first approximated with a phase-type distribution with three

figure 1: transformation of a normal distribution to a chain of exponential distributions e 1 . p and p denote the action duration and action outcome probabilities  respectively.
phases and then uniformized to yield the exponential distributions e 1 . from now on  we can therefore assume without loss of generality that the action durations t of all actions are distributed according to exponential distributions
  with the same constant ¦Ë   1. value
iteration cannot determine the value functions v   s  with a finite number of iterations since the uniformized mdps have self-transitions. however  theorem 1 in the appendix shows that the approximation error is no larger than a given constant  if value iteration stops after at least

iterations  where . running time of step 1 of cph is negligible since analytic phase-type approximation methods run in time o 1 .
1 cph: step 1
it follows directly from the considerations in the next section that there exist times-to-deadline 1 = ts 1   ts 1   ...   ts ms+1 = ¦¤ such that v n s  t  = vin s  t  for all t ¡Ê
 ts i ts i+1   where
 	¦Ët n 1  
 for the parameters cs i j for all s ¡Ê s  1 ¡Ü i ¡Ü ms and 1 ¡Ü j ¡Ü n + 1. we refer to the times-to-deadline ts i as breakpoints  the  ts i ts i+1  as intervals  the expressions for the value functions vin s  as gamma functions  which is a simplification since they are actually linear combinations of euler's incomplete gamma functions   and the expressions for the value functions v n s  as piecewise gamma functions. we represent each gamma function vin s  as a vector
 and each piecewise gamma
function v n s  as a vector of vectors 
. figure 1  for example  shows the value function v   start  for our mars rover domain.

figure 1: value function v   start  consists of four gamma functions: v   start  =
 1:v1  start   1:v1  start   1:v1  start   1:v1  start   =
 1: 1  	1: 1 1  	1: 1.1 1  
1: 1.1  1 1  .
1 cph: step 1
we now explain how cph uses value iteration. for n = 1  the value functions v n s  = 1 satisfy v n s  =  1:  and thus are  piecewise  gamma. we show by induction that all value functions v n+1 s  are piecewise gamma if all value functions v n s  are piecewise gamma. we also show how the bellman updates can efficiently transform the vectors of the value functions v n s  to the vectors of the value functions v n+1 s . value iteration calculates

we break this calculation down into four stages:	first 
.	third 
.	finally 
stage 1:	calculate
¡¡¡¡¡¡¡¡¡¡¡¡. our induction assumption is that all value functions  are piecewise gamma  i.e.  
. in stage 1  cph calculates
  which is the same as calcu-
lating since is constant. then 

where for all
.
stage 1: calculate 
which is a convolution of.
consider the value functions defined in stage 1. we now show by induction that

. equation  1  holds for i = 1 since  for all. assume
now that equation  1  holds for some i. it then also holds for i + 1 as shown in figure 1. we use the following lemma to notation.  show that is a gamma function and convert it to vector
lemma 1. let p t  = e ¦Ë . then  p    k1 k1 ... kn  =
 k1 k1 k1 ... kn .
proof. by symbolic integration  
for any constant k.	then  p    k1 ... kn 	=	p  
 ni=1 ki    ki+1 ij=1 +  kn jn=1  = in=1 p    ki   ki+1 ij=1 +p  kn nj=1 = in=1 ki ki+1 ij+1 + kn nj=1 =  k1 k1 ... kn .	
we now transform equation  1  to vector notation.
v
with
.
consequently .
stage	1:	calculate	 	:=
. consider the value functions defined in stage 1. since they might
 for different states
 with 
breakpoints 
without changing the value functions. afterwards   where for the unique .
then 

stage 1: calculate .
consider the value functions v n s a  defined in stage 1. since they might have different breakpoints for different actions a ¡Ê a s   cph introduces common breakpoints without changing the value functions v n s a . cph then introduces additional dominance breakpoints at the intersections of the value functions to ensure that  over each interval  one of the value functions dominates the other ones. let  be the set of breakpoints afterwards. then  
where  for actions as i ¡Ê a s  and the unique  and  for all. furthermore  action as i should be executed according to the value function v n+1 s  if the current state is s and the time-todeadline is.
to summarize  the value functions v n+1 s  are piecewise gamma  and the vectors of the value functions v n s  can be transformed automatically to the vectors of the value functions v n+1 s . the lengths of the vectors increase linearly in the number of iterations and  although the number of breakpoints  that are placed automatically during the transformations  can increase exponentially  in practice it stays small since one can merge small intervals after each iteration of value iteration to reduce the number of breakpoints the transformations require only simple vector manipulations and a numerical method that determines the dominance breakpoints approximately  for which cph uses a bisection method. we now show experimentally that the transformations are both efficient and accurate.
1 experiments
we performed an experimental feasibility study in our mars rover domain that compares cph and lazy approximation  la   li and littman  1   currently the leading algorithm for solving continuous state mdps with quality guarantees  figure 1 . we always plot the error maxvalue function1¡Üt¡Ü¦¤ |v   vstart start  t   on the x-axis and the correspond-  v  start  t | of the calculated ing runtime  measured in milliseconds  in log scale on the y-axis.
	t	t
v
.
!
figure 1: proof by induction of equation  1 .
figure 1: empirical evaluation of cph and lazy approximation  experiments 1 1  and the tightness of the theoreticalexperiment 1: we determined how cph and lazy approximation trade off between runtime and error for our mars planning horizon calculated from theorem 1.
rover domain. since the action durations in the mars rover domain are already distributed exponentially and thus phasetype with one phase  there are no errors introduced by approximating the probability distributions over the action durations with phase-type distributions. also  since these distributions are equal  uniformization will not introduce self-transitions and value iteration can run for a finite number of iterations. we therefore varied for cph the accuracy of the bisection method that determines the dominance breakpoints and for lazy approximation the accuracy of the piecewise constant approximations of the probability distributions over the action durations and value functions. our results show that cph is faster than lazy approximation with the same error  by three orders of magnitude for small errors. for example  cph needed 1ms and lazy approximation needed 1ms to compute a policy that is less than 1% off optimal  which corresponds to an error of 1 in the mars rover domain.
experiments 1 and 1: we then determined how cph and lazy approximation trade off between runtime and error when all action durations in our mars rover domain are characterized by either weibull distributions weibull ¦Á = 1 ¦Â = 1   experiment 1  or normal distributions n ¦Ì = 1 ¦Ò = 1   experiment 1   and both methods thus need to approximate the value functions. we fixed for cph the accuracy of the bisection method but varied the the number of phases used to approximate the probability distributions. our results show that cph is still faster than lazy approximation with the same error  by more than one order of magnitude for small errors. for example  cph needed 1ms  with five phases  and lazy approximation needed 1ms to compute a policy that is less than 1% off optimal for the weibull distributions. in addition  we observed that cph converged much faster than the theoretical planning horizon calculated from theorem 1 suggests.
1 conclusions
planning with real-valued and limited resources can be modeled with continuous state mdps. despite recent advances in solving continuous state mdps  state-of-the art solution methods are still either inefficient  li and littman  1  or cannot provide guarantees on the quality of the resulting policy  lagoudakis and parr  1 . in this paper  we presented a solution method  called cph  that avoids both of these shortcomings. the aim of this paper was to establish a sound theoretical framework for cph and provide a first experimental feasibility study to demonstrate its potential. it is future work to study the theoretical properties of cph in more depth  for example  analyze its complexity or extend its error analysis   evaluate cph and its three approximations more thoroughly  for example  in larger domains  additional domains and against additional solution methods  and extend cph  for example  to handle more than one resource  handle replenishable resources or  similarly to lazy approximation  approximate its value functions to be able to reduce its number of breakpoints .
