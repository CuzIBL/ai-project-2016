 
human chess players exhibit a large variation in the amount of time they allocate for each move. yet  the problem of devising resource allocation strategies for game playing did not receive enough attention. in this paper we present a framework for studying resource allocation strategies. we define allocation strategy and identify three major types of strategies: static  semi-dynamic  and dynamic. we then proceed to describe a method for learning semi-dynamic strategies from self generated examples. the method assigns classes to the examples based on the utility of investing extra resources. the method was implemented in the domain of checkers  and experimental results show that it is able to learn strategies that improve game-playing performance. 
1 	introduction 
it is very common to see chess masters spend a considerable amount of time over complicated/crucial board positions  while replying almost instantaneously to others. indeed  the amount of resources devoted to a move strongly determines its quality. this is the reason why errors are much more frequent in blitz games than in regular games. it is therefore very important for players to allocate their resources wisely. however  determining which board deserves more resources is not always easy. 
　substantial research efforts were allocated for devising sophisticated mechanisms for selecting the right move  minimax  alpha-beta  windowing . however  there were only few attempts to study the problem of resource allocation in game playing throughly  levy and newborn  1; hyatt  1 . 
the research described here has the following goals: 
  understanding the problem of resource allocation. 
  studying different types of possible resource alloca-tion strategies. 
  developing a research methodology under which re-source allocation strategies could be created  evaluated  and compared. 
1 	machine learning 
  developing a game-independent method for automatically acquiring resource allocation strategies. 
the rest of this paper is organized as follows : in section 1 we discuss different types of allocation strategies on a knowledge/cost scale. in section 1 we describe a general methodology for automatically acquiring semidynamic strategies  and describe an implementation of this methodology in section 1. in section 1 we describe experiments conducted using the implementation and their results. section 1 concludes. 
1 	resource allocation strategies 
assume that an agent is facing a sequence of tasks that it intends to perform. assume that the results of executing a task depend on the amount of resources devoted for the execution. assume further that the agent has an upper bound on the total amount of resources it can use for the whole sequence. a resource allocation strategy is an algorithm that decides how to distribute the resources between the tasks. assuming the existence of some criterion for evaluating the performance of the task sequence  we can compare strategies based on the performance that they yield. we assume that the evaluation criterion is non-decreasing monotonic  i.e.  that investing more resources cannot lead to deterioration in performance. 
　the research described here focuses on 1-player perfect information games  e.g.  chess  checkers . 1-players perfect information games are a good domain for studying different strategies of resource allocation because the performance of two strategies can be easily compared by letting programs that use the strategies play against each 
other. 
　in the game-playing context  it is very common to limit the total time that players can spend for each k moves. the tasks are the move calculations required while the game is played. a resource allocation strategy for game-playing programs is an algorithm that decides how much resource the program should spend on the calculation of each move. a minimax procedure that is allocated more resources for search is able to search deeper and therefore reach better decisions. 
strategies can be divided into three groups : 
  static strategies - strategies that decide how the re-source should be allocated before starting the game. 
  semi-dynamic strategies - strategies that decide before each move calculation is performed  how much of the resource will be allocated to that move. 
  dynamic strategies - strategies that can communi-cate with the move calculation process  and update their resource allocation decisions  while that process is being carried out. 
these types of strategies could be viewed in light of the knowledge available to them  and with respect to the resource demands of the strategy execution itself. in the next three subsections we will discuss these three classes of strategies. 
1 	static strategies 
a static strategy decides upon the resource allocation before the game starts. naturally  such a strategy has no information about the game  and will therefore always yield the same resource allocation sequence. such a strategy is extremely cheap: it should only be applied once  and the resulted sequence can be used for all games. however  it is very unlikely that a single sequence fits all possible games. if the strategy is given a model of the opponent in some form  it can produce an allocation sequence based on the model. such a static strategy will still be very cheap since it is called only once at the beginning of the game. 
1 	dynamic strategies 
dynamic strategies are located on the other end of the knowledge scale. they can use the information gathered during the search in order to decide the amount of resources allocated for the search. they can thus yield good allocation sequences that are based on a large amount of knowledge. the main problem with such strategies is their cost. in the extreme case the strategy can be called for each node in the search tree. 
　there are various dynamic strategies employed by existing game-playing programs. the most famous one is the quiescence search strategy  beal  1  that keeps on searching branches as long as there are drastic changes in values of nodes in the search tree. another selective deepening method is called singular extension  anantharaman et a/.  1 . the method conducts a secondary search under a leaf node that dominates its siblings  and has therefore greater influence on the search outcome. 
1 	semi-dynamic strategies 
semi-dynamic strategies have access to the board that is at the root of the search tree. thus they have much more knowledge than static strategies. yet  the strategy is executed once for every move  and is therefore much cheaper than dynamic strategies. 
　it is hard to tell what properties of the board should affect the decision of the resource allocation strategy. it is reasonable to devote more resources when the player is in a much inferior position so that it can get out of trouble. but it is also reasonable to devote more resources when the player is in a good position when the right  but hard to find  move will lead it to a victory. another factor that may affect the resource allocation decision is the complexity of the situation. in a complex situation  the search procedure should probably be allocated more resources. 
　finding an algorithm that can consider all the relevant properties of a board in order to decide the amount of resources to allocate is a difficult task. in the following section we will present a methodology that learns such an allocation strategy from examples. 
　in addition to the board itself  a semi-dynamic strategy can also base its decision on the history of the game. human players  for example  devote more resources to the computation of a move after the opponent has made an unexpected move. that type of reasoning requires some model of the opponent. we are not considering opponent modeling in this research. 
1 	research framework 
for a continuous resource  there are an infinite number of ways to partition it to resource sequences. in order to make the research more feasible  we have devised a simplified model of resource allocation strategies: 
  the player searches the game tree using minimax procedure with alpha-beta pruning. 
  no selective deepening techniques  like quiescence search  are employed. 
  the game is stopped after a fixed number of moves  m . 
  the player has two resource allocation options: ei-ther searching to depth k or searching to depth k+n  n   1 . 
  the player is allowed to perform the deeper search at most d times  where d   m. 
under the above model the output of a strategy is simply a vector v1  v 1  ...  vm  where each vi can be either true or false  marking that the strategy decides to perform the deeper search or does not accordingly. the number of k's which are true must be   d. under this model  the total number of allocation sequences that a strategy can produce is y  i 
1 	learning semi-dynamic strategies 
the knowledge that a strategy uses in order to make decisions can be domain independent or domain specific. the advantage of devising a strategy based solely on domain independent knowledge is  that such a strategy is very general  and is applicable to any game. however  it is clear that a strategy using both domain independent knowledge and domain specific knowledge can yield better performance. the problem is that getting domain specific knowledge and incorporating it into a strategy is not an easy task. even human expert players find it hard to formulate explicitly the reasoning behind their resource allocation decisions. 
　one way of overcoming these problems is to build a system that learns good strategies automatically. the input for this learning system should be a game  or more specifically a problem solver designed to play a game  
	markovitch and sella 	1 
and the bounded resource to be allocated. the output of the learning system should be a strategy for allocating the resource. 
1 semi-dynamic strategy learning as a classification problem 
we suggest the following general methodology for constructing the learning system : 
1. generate many examples where each example is a 
board and a correct decision regarding resource allocation for that board. 
1. find general rules that predict which boards require more resources than others. 
assuming that a board is represented by a set of features describing some of its properties  and recalling  that in our model all the decisions regarding resource allocation are binary  the problem can be formulated as a classification problem. thus  in stage 1 of the proposed methodology  any of the known classification algorithms can be employed. what is left to be determined in further details is stage 1  namely  how exactly will the training examples be generated. four questions arise: 
1. where will the training examples come from   
1. how will the class be determined   
1. how will the features representing a board be determined   
1. how will the learned classifier be used in the context of resource allocation  
starting with the first question  the examples must obviously be realistic  i.e.  boards that are likely to be encountered in real games. hence  it seems natural that the examples should be generated by the problem solver during the course of real games. the other three questions will be discussed in the following subsections. 
1 	assigning classes to examples 
ideally  to determine if a board is positive  should be given extra resource   one should compare the expected outcome of the game  when the best move is chosen once with  and once without extra resource  while the rest of the moves stay intact. unfortunately  this is not possible  because once a different move is chosen  the rest of the game takes a different course. 
　alternatively  one can compare the expected outcome on a more local basis  namely compare the expected outcomes of the best moves  with and without extra resource. let 1 be a board  movek  1  be the best move in a search to depth k and class b  be the class assigned to b. we suggest the following methodology to determine the class of a board: 

　the above method assigns each board a discrete class  either positive or negative. more generally  we can estimate the utility of a depth k + n search  and use it as a measure for the positiveness of the board. let us mark 
1 	machine learning 
by vk m  the minimax value assigned to move m by a 
depth k search. the class assigned to 1 will be : 
		 1  
　we measure the utility as the difference between the values of the best moves chosen by depth k + n and k searches. the values are expressed in terms of the evaluation function of the problem solver. note that if move*  1  = movejb+n 1  the utility will be 1. the values of both moves should be taken from the best estimates available. assuming that a search to depth k+n  knows better  than a search to depth ar  we take both values from the former. note also that this measure is unlikely to be discrete  so incorporating it into the system as the class  requires a classification algorithm that can process continuous classes. 
　more generally still  it is possible that upon searching to some depth  there will be more than one best move. we shall change the notation to movesk b  and movesk+n b  to mark that these entities are sets. within the discrete class paradigm  the following criterion should be used in assigning a class to a board: 

 1  
　the board is positive only if depth k + n search can be beneficial or  in other words  there is a chance that a depth ib search will choose one of the wrong moves. note that using movesk 1  = movesk+n{b  is not good enough because it includes the case moresk  b  c movesk+n b . in the latter  although depth k search can choose from a 
　smaller set  this set does not include any wrong moves  yielding that any move chosen by depth k search is correct. 
　within the continuous class paradigm  the difference is now measured between sets  and for each of these sets we have to calculate the value of the expected outcome. for the set moves k +    1   all its members have the same minimax value  in terms of depth k -h n search  by definition. therefore  the value of the expected outcome of a depth ib -i- n search  can be taken as the value of any member ml e mot;e$fc+n & . for the set moves*  1   although all its members have identical v* values  they might have different v k + n values. therefore  the class assigned to 1 will be: 

　note that calculating movesk+n 1  requires a minimax search to depth k+n  without alpha-beta cut-offs at the top level nodes. 
　in this research a discrete class paradigm was used  so equation 1 was adopted as the class assignment procedure. 
1 	extracting features from examples 
features can be domain independent  e.g.  number of possible moves  or domain specific  e.g.  center control   and determining which features should be extracted from the board  especially for domain specific features  is not trivial. some of the problems arising in that matter are: 
  the features must have some predictive power re-garding the class. 
  the features should be easy to compute  to prevent high overhead of the strategy execution. 
  there should not be too many features. too many features  especially irrelevant ones  might damage the quality of the learned rules  over-fitting  schaffer  1    and they also increase strategy execution overhead since their values have to be calculated during game-time. 
it should be noted at this point  that the existence of a relatively-small  cheap-to-compute  and highlypredictive set of features is not guaranteed. ideally  a learning system should be able to generate features automatically  a process called constructive induction . in this research  some of the features are given as input to the learning system and some are self generated. the method that was used for generating features is best explained in the context of the checkers domain and will therefore be described in the next section. 
　usually  one does not know in advance which features of an example are relevant to the target concept and which are not. many candidates for features can be thought of  but due to bad effects of irrelevant features it is important that such features will be filtered out. 
　in recent years the problem of irrelevant features in inductive learning received more attention  and some feature-selection algorithms emerged  such algorithms perform attention filtering according to the framework described in  markovitch and scott  1  . in the experiments described in section 1  we used a feature selection algorithm called relief  kira and rendell  1   that attempts to eliminate features statistically irrelevant to the class. 
1 	using classifiers in resource allocation context 
as noted earlier  many classification algorithms are known  and any of them can be used as the learning module of the system. for the experiments described in the next section  we used a variant of id1  quinlan  1  coupled with relief. decision-trees are relatively cheap classifiers making them an attractive choice for resource allocation usage. in order to decide whether to invest extra resource  the program need only to pass the current board through the decision tree  and invest the extra resource only when the board is classified as positive. 
　there is one problem with the above method. it does not take into account the amount of resources still available. if a large resource is available relative to the time left till the end of the game  we would like our allocation strategy to be less selective and to classify boards as positive more often. if the resource is scarce we would like the strategy to be more careful before deciding that boards are positive. 
　we have devised a method that uses probabilistic decision trees in order to make the allocation strategy behave in the way described above. we divide the training set into two portions. the first portion is used to build a decision tree in the traditional way. the tree is then fixed and the second portion of t examples is used in order to assign probabilities to the leaves. if the number of examples that reach a leaf / is 1} out of which p1 are positive  then the leaf is assigned the probability: 
　assume that during a game  moves are left to play with d deep searches still available. we would like the resource allocation strategy to decide on making deep search with probability we order the set of leaves according to their probabilities and mark the longest prefix of the ordered set  with highest probabilities  that satisfies 
　when the resource allocation procedure is called  it passes the current board through the decision tree. only if the leaf that it reaches is marked  the procedure will 
allocate extra resource. 	  
　after the move is performed  the values of d and m are recalculated  and the process continues. this process can be viewed as placing a dynamic threshold on the positiveness of leaves that is changed according to d and m. since gathering the leaves with highest probability before every move is too expensive  we perform a preprocessing stage after acquiring the decision tree. a table is constructed that specifies for a range of possible values of what is the positiveness threshold. 
1 	implementation 
the above methodology was implemented in a system whose architecture is illustrated in figure 1. the learning system generates boards by having two instances of the player play against each other. the boards are then assigned a class using equation 1. the system then extracts the features from the boards and feeds the classification program with the classified examples. the classification program generates classification rules. the performance system receives a board as input. it calls the resource allocation procedure that uses the learned classification rules to decide the resource allocation for the given board. the search procedure then conducts a minimax search using the resources allocated. 
1 	domain 
the domain that was chosen as a platform for the experiments is the game of checkers. the free parameters were set as follows: basic search depth  k  = 1  additional search depth  n  = 1  maximum number of deep searches per game  d  = 1  maximum number of paired moves per game  m  = 1. 
　a simple evaluation function was used on the leaves of the game-tree which relied only on piece advantage. 
	markovitch and sella 	1 
