 
parallel processors offer a very attractive mechanism for the implementation of large neural networks. problems in the usage of parallel processing in neural computing involve the difficulty of handling the large amount of global communication between processing units and the storage of weights for each of the neural processor connections. this paper will discuss how massive parallelism in the form of a one dimensional simd array can handle indefinitely large networks in near real time by efficiently organizing the memory storage of weights  and input and output signals. very little overhead time is used in communication of signals between processing units  and there is no idle time for any of the units. an advantage of simd array systems is that the arithmetic processing is done bit serially  with the result that trade-offs can be easily be made between the processor speed and precision of the signals and weights. 
1 introduction 
1 	parallel and distributed processing fine grained massive parallelism is an ideal method to enhance the speed of neural processing for real time applications. in these systems  which are also called simd  single instruction  multiple data  architectures there are many processing elements each having a very simple bit serial arithmetic unit with local memory. in a given clock cycle  all units receive the same instruction  and all local memories receive the same address. the simd architecture works well in neural computing because one set of instructions corresponding to the operation of a neural cell is applied to a large data set corresponding to the various signals and neural weights in the network. the most common examples of simd architectures are mesh connected systems  such as the massively parallel processor  potter  1   with inter-processor communication in two dimensions  or a hypercube system such as the connection machine  hillis  1  with processing units at the vertices of an n-dimensional hypercube and inter-processor communication along the hypercube edges. however they are generally too expensive to be considered as a vehicle for dedicated applications. a simd architecture which is low cost and commercially available  is a one-dimensional processor array system  the ais-1  which has up to 1 processing elements and uses the pixie chip  ischmitt and wilson  1  wilson  1 . the system has a wide base of real-time industrial applications in vision processing. a next generation one dimensional array system under development uses the centipede chip  wilson  1 . in the following sections  the one dimensional architecture will be described in more detail  and a discussion of the memory organization of signals and weights will illustrate how this architecture can efficiently handle neural computing. programming a 
simd machine is similar in many ways to programming any serial risc computer. a serial program is written for a simple processor which accesses and operates on data in its memory. the major difference is that when an instruction is executed  a large number of processing units are active. data dependent operations cannot be handled in the same way as for a serial processor  and algorithms must be specifically organized to operate correctly for an ensemble of processing units with multiple data paths. 
1 one-dimensional simd architectures 
the basic model of the one dimensional simd architecture is shown in fig. 1 where the individual processing elements  pes  are connected to their nearest east and west neighbors. in addition  each pe is connected to its own bit-wide memory. one dimensional array architectures have been designed or implemented by a number of people  fisher  1  fountain  et al.  1 . some work has indicated that this architecture has a good cost/performance trade-


off for many imaging algorithms  fountain  1 . the applied intelligent systems ais-1 is shown in fig. 1 where three major components are designated: a general purpose host processor  special purpose high speed i/o hardware for the parallel processor hardware  and the simd array with a controller. 

sor instructions and memory addresses are common and are broadcast to all units by the central controller. 
　a new custom chip under development called the centipede  wilson  1  will be used in one dimensional array systems with an architecture very similar to the ais-1. the boolean  neighborhood  and arithmetic processing sections are similar to the pixie chip  but the major enhancements are in data communications and transformations  and a hardware multiply-accumulator  mac . the centipede will also be capable of indirect addressing of the local memory  1 transposes on sub-arrays  and bitserial or word-parallel modes of arithmetic. a block diagram of the processing units are shown in fig. 1. there will be 1 processing units on an integrated circuit chip with 1 gates  and one processor for each column in the data matrix. the entire memory associated with the parallel array is mapped so that it can be directly read or written by either the host or controller. however  the i/o shift registers are a more efficient means for data transmission. 
　data bytes from one row of memory across the processor array can be shifted up in 1 cycles to the i/o registers in the centipede chip shown in fig. 1. this row can then be shifted east in a number of clock cycles equal to the number of processing units. meanwhile a new row of data can simultaneously be shifted in from the west into the same i/o shift registers  and then stored into the local memory. the east i/o shifting operations do not interfere with processing operations  and are thus very efficient. although these shift registers were made for camera input for vision processing  they also serve efficiently for the types of block data moves needed in neural processing. this i/o architecture easily and naturally scales as more processing units are added. 
wilson 
　a simplified diagram of the mac circuit for a single processing unit is shown in fig. 1. the mac contains hardware for bit-serial multiplications. the design of the bit-serial arithmetic unit allows it to perform operations at the same rate as bits can be read from memory. the u register is an 1 bit register 
1 

figure 1. the multiply-accumulate circuit. 
which holds the multiplicand. the 1 bit adder can sum the multiplicand with the contents of the accumulator. the multiplier is read from the memory  one bit at a time and can control the operation of the adder through the and gates shown in fig. 1  so that multiply operations are done by a succession of shift and conditional add operations where the condition is controlled by whether a one or a zero multiplicand bit is read from the memory to the and gates. the product is read out of the lowest bit of the accumulator to memory as the accumulator is shifted down during the multiply process. in this manner  the mac circuit associated with every unit is maximally efficient when performing fixed-point arithmetic where one of the operands  multiplicand  is 1 bits or less  and the other operand  multiplier  has any precision. 
1 neural processing 
　the neural model to be adapted to the simd array is the general form given by rumelhart  et al.  . using their notation the various components of a single processing unit i include a net: 
activation function: 
output function: 
where w- are weights from unit j to unit i  o. is the output from unit i  and fi. and fi are functions which specify the nature of the neural processing units. the network model for a particular layer is shown in fig. 1  where there are m input signals s. and n outputs 
oj  where m is less than or equal to n. for computations on the simd array  it will be easier to separate the net and weights into two parts  one for input signals  and one for output signals: 
1 	parallel and distributed processing 

1 simd processing 
　the organization and memory allocation of a single neural layer in a one dimensional simd computer system is shown in fig. 1. each neural cell is assigned to one processing unit. more than one cell could be assigned  or  if memory storage capacity is too low  a single neuron cell could be assigned to more than one unit. these extensions will not be covered here. the weights wiij for unit i will be stored in a vertical single bit wide stack in the memory assigned to that unit. subscripts i and j respectively denote column and row indices. the number of storage bits required are bm where b is the number of bits in the weight  and m is the number of input signals. the output weights woij. are stored in a similar manner  where bn bits are required for each column. all neurons are completely interconnected. signals si  and the input net  netl.  are each stored in single rows of the memory  each row comprising a number of bits equal to the size of the word. other rows in the memory can store other vector variables such as output signals ov the output net netoi  activation states  training patterns  
thresholds  function parameters  and perhaps random variables for statistical models. since the outputs of all neurons are fully connected to inputs of all other neurons in a specific layer in this model  a relaxation process must be used. a number of iterations of computations must be performed in order for 


the outputs to converge to stable states. for the first iteration  the outputs start out at some initial state provided by the particular neural model. since the inputs are assumed to be stable  no iterations are needed to compute the input net. the total net was partitioned into a separate input net and output net so that unnecessary iterations could be avoided for the stable input signals. 
   to compute the input net  all signals  si are first sent to the controller via the fast i/o path discussed earlier. in the controller  the signals are treated as row multiplicands  and should now be denoted as sj. 
assume that the signals are eight bits. the controller then broadcasts  and loads the first signal s1. 
into all u registers in all the macs to act as multiplicands. the first row of weights will be the multipliers. using the mac hardware as described earlier  all weights wii1 are multiplied by the first signal s1.  making 1% efficient usage of the parallelism. these products are accumulated in the netl row of the memory. next s1 is loaded and multiplies weights wli1  and accumulates them in netl. this process continues until all products are summed into the netl row. 
 the same procedure occurs for the output weights: outputs o. are multiplied by the output weights 
woij and accumulated in netc1. the input and output nets are summed to get the total net. the activation state and output functions are computed next. since they are only functions of the previous activation state and the total net  the computation does not require any variables or parameters which are stored outside the local memory for each processing unit. it is assumed that those parameters such as decay constants  and sigmoid shape parameters  that are used in the computations of these functions are common to all neural units and would be incorporated in the simd instructions broadcast to all units. thus the activation and outputs for all neural units can be computed in parallel with no inter-processor data communication overhead. since they are simple vector operations they proceed much faster than the matrix computations of the nets. 
 for the next iteration in the relaxation process  the output net is computed again by multiplying the output weights by the new output states to generate a new output net. this new output net is added to the input net to form a new total net. new activation states  and new outputs are computed. these operations can be iterated until adequate convergence occurs according to some relaxation scheme given by the model. finally the output row which is assumed to be eight bits will be sent to the i/o registers in the processing units and rapidly transferred out while the new input states are simultaneously being trans-
ferred in on the same i/o shift registers  as in fig. 1. 
1 learning 
the general form of hebbian learning is given by 
rumelhart  et al.   as 
wij-g ai t  ti t  h oj t  wij     
where t t  is a teaching input  and g and h are functions particular to the model. computations of this form are very efficiently implemented on the one dimensional simd architecture. the g function is simple because the arguments involve row operations only  and are not functions of the column dependent variables. computation of the h function is similar in concept to methods previously discussed  where the o. vector is first read out to the controller. however  rather than a global multiplication of oj with a row of weights  the operation is much simpler. for example in simple hebbian learning  rumelhart et al. 1   or the widrow-hoff rule  sutton and barto  
1   h is equal to oj. in rules used by grossberg 
  h is a simple difference: h = oj - wij 
wilson 

 with the above implementation of neural computing in both learning and network operation  the global inter-processor communication of output states is done without a great deal of data movement. when the output state of a unit is to undergo operations by other units  the value of that state is transmitted to all other units by instructions broadcast by the controller  and not as a series of separate data movements. it is for this reason that the simd architecture is very efficient for large fully connected networks. 
1 performance 
benchmarks for the evaluation of performance of the one dimensional simd array compared to other computation architectures are difficult because there are so many neural models; however a few general remarks can be made. fixed point addition operations are much faster than floating point additions on simd systems because of the difficulty in handling the alignment of the fraction parts of two floating point numbers when the exponent parts are not equal. the alignment and subsequent renormalizing of the floating point numbers in a bit serial machine takes more time than the addition operation itself. if the input and output signals are limited to eight bit fixed point numbers  and the weights are floating point  then the multiplication and addition operations can proceed much faster than if the signals and outputs were also floating point. this is not generally a restriction since strict constraints on the dynamic range of these signals is common. for example in thermodynamic models  hinton and sejnowski  1  the signals can be single bit variables. in connectionist modeling the signals can have a limited number of discrete states  for example  1 to 1   feldman and ballard  1 . 
　floating point operations are used in neural computing because a wide dynamic range is needed for the weights. the precision is less important. the time to compute the sums of products for the input or output net is dependent on the clock period  number of clock cycles for a single row operation  signals times weights + accumulation into the net   and the total number of rows  or the time is given by: t = t  time/cycle  x c cycles/row  x r rows . 
 if we choose an 1 bit fixed point representation for the input and output signals; a 1 bit floating point representation  1 bit fraction with 1 bit exponent  for the weights; and a 1 bit representation  1 bit fraction with 1 bit exponent  for the net  then c = 1 simd clock cycles are needed to compute the product of signals times a single row of weights on the centipede chip. suppose there are r = 1 neurons  and the processor clock rate is 1 mhz  then the time to 
1 	parallel and distributed processing 
compute the input or output net using this partial floating point representation is t = .1 usec x 1 x 1 a 1 msec. this time scales linearly with the number of neurons assuming that there is one processing element per neuron. 
　suppose the signals are 1 bits  the weights are 1 bits  and the net is 1 bits  all in fixed point. then 1 clock cycles per row operation are needed  and the time t to compute the net is 1 msec. for single bit signals  no multiplications are needed. then c = 1  and the time for 1 bit weights and a 1 bit net is 1 msec. performance in terms of equivalent connections per second is given by the total number of connections  1  divided by the processing time per net. these results for 1 neural units are summarized in the table below. 
processing connections signal weight net time/net per second range range range 1 msec 
1 msec 
1 msec 1 million 
1 million 
1 million 1 bit fixed 
1 bit fixed 
l bit fixed 1 bit frac. 1 bit exp. 
1 bit fixed 
1 bit fixed 1 bit frac. 1 bit exp. 
1 bit fixed 
1 bit fixed the above processing times must be multiplied by the number of times that a relaxation iteration is performed. if 1 iterations are needed for convergence  the total time is still reasonable  ranging from 1 seconds to 1 seconds. 
　for highest speeds in processing  a number of centipede arrays can be joined in a multiple simd pipeline as shown in fig. 1  where communication is provided by the i/o registers. this architecture expansion will allow any number of neural layers to be handled as quickly as one. the system shown in fig. 1 for example  can handle three neural layers. 
the first p.c. board receives the external signals  and processes the first layer. output states are sent to the second board while new input signals are being received by the first board. while the second board 

is processing the second layer  the first board is processing the new first layer. the third board processes the third layers in a similar manner and transmits the final output states. 
1 conclusion 
the bit serial nature of the processing units in a 
simd computer allows a wide range of trade-offs in speed vs. precision. usage of various representations of fixed and floating point variables for a 1 x 1 array of weights correspond to an equivalent execution rate of 1 to 1 million neural connections per second. for a larger number of neurons  the number of computations will grow quadratically  but the computation time will only grow linearly. 
   there are a number of other favorable properties of this architecture. one main intent of the design of the current simd arrays was to provide an i/o subsystem which was capable of rapidly moving images in and out of the memory local to the processing units without interfering with the parallel processing. this i/o mechanism also works well with the type of data movement needed for neural processing. rows of signals  outputs  and weights can be quickly transferred in and out of memory with very little time penalty. for multiple layered networks  several simd systems can be connected in a pipeline architecture  where the i/o subsystem transfers data between each simd system in the pipeline. 
   it is generally understood that for a large degree of parallelism to be feasible in neural networks there is also a requirement for a capability of massive global communication because of the obvious intimate interconnection of all neural cells. the one dimensional simd array does not explicitly have the degree of inter-processor communication that a hypercube or mesh connected system contains. yet inter-processor unit communication is not a bottle-neck. the reason the one dimensional system works well is that the inter-processor communication is rather subtly imbedded in the nature of the simd concept. for example  when the system controller has an input signal value  it can cause all processor units to operate with that value simultaneously via the  single instruction  aspect of simd. it is the full broadcasting of an instruction  and not the actual movement of data that results in a global communication of information. for that reason simd architectures have an advantage over other parallel architectures where data movement is the primary means of providing communication in a network. furthermore  the one dimensional simd architecture has a very strong economic advantage over other simd architectures which have intercommunication schemes which are unnecessarily complex for neural networks. 
