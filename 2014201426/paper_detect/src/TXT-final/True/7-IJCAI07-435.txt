
in this paper  we propose the directed graph embedding  dge  method that embeds vertices on a directed graph into a vector space by considering the link structure of graphs. the basic idea is to preserve the locality property of vertices on a directed graph in the embedded space. we use the transition probability together with the stationary distribution of markov random walks to measure such locality property. it turns out that by exploring the directed links of the graph using random walks  we can get an optimal embedding on the vector space that preserves the local affinity which is inherent in the directed graph. experiments on both synthetic data and real-world web page data are considered. the application of our method to web page classification problems gets a significant improvement comparing with state-of-art methods.  
1 introduction 
we consider the problem that embeds nodes on directed graph into a euclidean vector space while preserving locality which is inherent in the graph structure. there is a large amount of problems which can be naturally represented as a directed graph. typical examples are web information retrieval based on hyperlink structure  document classification based on citation graphs  c. lee giles et al.  1  and protein clustering based on the pairwise alignment scores  w. 
pentney and m. meila  1 . some works have been done to deal with the ranking problem on link structure of the web including the pagerank  s. brin and l. page  1  and hits  j. dean and m. henzinger  1  algorithms  yet it is still a hard task to do general data analysis on directed graphs such as classification and clustering. in  d. zhou et al.  1  the authors proposed a semi-supervised learning algorithm for classification on directed graph  and also an algorithm to partition the directed graph. in  w. pentney and m. meila  1  the authors proposed algorithms to do clustering on protein data which was formulated into a directed graph based on asymmetric pairwise alignment scores. however  up to now  works are quite limited due to the difficulty in exploring the complex structure of directed graphs. on the other hand  there are a lot of data mining and machine learning techniques  such as support vector machine  svm   operating data on a vector space or an inner product space. embedding the data of directed graphs to vector spaces becomes quite appealing for tasks of data analysis of directed graphs. the motivations are: 
1  instead of designing new algorithms for each task in data mining on directed graphs that are directly applied to link structure data  we can first provides a unified framework to embed the link structure data into the vector space  and then utilize the mature algorithms that already exist for mining on the vector space. 
1  directly analyzing data on directed graphs is quite hard  since some concepts such as distance  inner product  and margin  which are important for data analysis  are hard to define in directed graph. but for vector data  these concepts are already well defined. tools for analyzing data can be easily obtained. 
1  given a huge directed graph with complex link structure  it is highly difficult to perceive the latent relations of the data.  such information may be inherent in the topological structure and link weights. embedding these data into vector spaces will help people to analyze these latent relations visually.  
some works have been done for embedding on the undirected graph. manifold learning techniques  m. belkin and p. niyogi   1   s.t.roweis and l.k.saul  1  first connect data into an undirected graph in order to approximate the manifold structure where the data is assumed to be lying on. then they embed the vertices of the graph into a low dimensional space. edges of the graph reflect the local affinity of node pairs in the input space. in the next  an optimal embedding is achieved by preserving such a local affinity. however  in the directed case  the edge weight between two graph nodes is not necessarily symmetric. it can not be directly used as a measure of affinity. motivated by  d. zhou et al.  1   we formulate the directed graph in a probabilistic framework. we use random walks to measure the local affinity of vertices on the directed graph. based on that  we propose an algorithm embedding the nodes on the directed graph into a vector space by using random walk metric.  the rest of the paper is organized as follows: in section 1  we give denotations used in our paper. in section 1  we introduce the details of our method. implementation issues are addressed in section 1. the relation between our method and previous works is considered in section 1. experimental results are shown in section 1 and the last part is the conclusion.  
1 preliminary 

 figure 1. the world wide web can be modeled as a directed graph. web pages and hyperlink can be represented as vertices and directed edge of the graph. 
an example of directed graph is the world wide web as shown in fig.1. a directed graph g v e      consists of a finite vertex set v which contains n vertices  together with an edge set e v . an edge of a directed graph is an ordered pair vertex  u v    from u to v . each edge may associate a positive weight w . an un-weighted directed graph can be simply viewed as all the weight of the edge is one. the out-degree do    v of a vertex v is defined as d vo     u w v u        where the in-degree d vi     of a vertex v is defined as d vi     v w u v        u v means u has a directed link pointing to v . on the directed graph  we can define a transition probability matrix p    p u v    u v  of a 
markov random walk through the graph. it satisfies  p u v      1  u . we also assume the stationary dis-
v
tribution for each vertexv is 1   which can be 
guaranteed if the chain is irreducible. for a connected directed graph  a natural definition of the transition probability matrix can be p u   v   w u v     /do   u in which a random walker on a node jumps to its neighbors with a probability proportion to the edge weight. for a general directed graph  we can define a slightly different transition matrix. we will be back to this issue in the implementation section. 
1 algorithm  
we aim to embed vertices on directed graph into a vector space preserving the locality property of vertex u to all its neighbors. first  let's consider the problem mapping the connected directed graph to a line. we define a general optimization target as: 
	t uv    	t u v ye       u yv 1 
	u	v u v 
 yu is the coordinate of vertex u in embedded one dimension space. the term te is used to measure the importance of a directed edge between two vertices. if  t u ve      is large  then the two vertices u and v should be close to each other on the embedded line. the term tv is used to measure the importance of a vertex on the graph. if t uv     is large  then the relation between vertex u and its neighbors should be emphasized.  by minimizing such a target  we are able to get an optimized embedding for the graph on one dimensional space. the embedding considers both the local relation of node pairs and global relative importance of nodes. 
in the following  we address the embedding problem of directed graphs under two assumptions: 
1  two vertices are relevant if there is edge between them. 
the relevance strength is related to the edge weight. 
1  the out-link of the vertex which has many out-links carries relatively low information about the relevance between vertices. 
the assumptions are reasonable in many tasks. let's again take the web by example. web page authors usually insert links to pages which are relevant to their own pages. therefore  if a web page a has a hyper-link pointing to web page b  we assume a and b might be relevant in some sense. we should preserve such a relation in the embedded space. 
let's consider a web page which has many out links  such as the home page of www.yahoo.com. the page linked by the home page of yahoo may share less similarity with it  and then in the embedded feature space the two web pages should have a relatively large distance. we can use the transition probability of random walks to measure the locality property. when a web page has many out-links  each out-link will have a relatively low transition probability. such a measure meets the assumption 1 and 1. 
different web pages have different importance in the web environment. ranking web pages according to their importance is a well studied area. the stationary distribution of random walks on the link-structure environment is well known as a good measure of such importance which is used in many ranking algorithms including pagerank. in order to emphasize those important pages in the embedding feature space  we use the stationary distribution u of random walks to weigh the page u in the optimization target.  taking all above into account  we rewrite the optimization target as follows:  
u
v
thus the problem is equivalent to embedding the vertices into a line while preserving the local symmetric measure u p u v      v p v u      / 1 of each vertices pair  here u p u v      is the probability of a random walker jumps to vertex u then to v   i.e. the probability of the random walker passes the edge  u v    . this also can be deemed as a percentage of flux in the total flux at stationary state when we continuously import water into the graph. this directed force manifests the impact of u on v   or the volume of message that u conveys to v . 
by optimizing the target we consider not only the local property reflected by edges between node pairs  but also the global reinforcement to the relation by taking the stationary distribution of random walks into account. we denote 
l
where p is the transition matrix  i.e. pij p i j         is the diagonal matrix of the stationary distribution  i.e. diag  1 ...  n  . clearly  from the definition l is sym-
metric. then we have the following proposition. 
proposition 1  
                 p u v y      	y  1y lyt	 u	u	v u	v u v  where y	 y1 ...  yn  t . 
the proof of this proposition is given in the appendix. the above l is known as combinatorial laplacian on a directed graph  f. r. k. chung  1 . from the proposition we can see that l is a semi-positive definite matrix. 
therefore  the minimization problem reduces to find  argmin y lyt y	 st. .	yt	y	1
the constraint yt y 1 removes an arbitrary scaling factor of the embedding. matrix  provides a natural measure of the vertex on the graph. the problem is solved by the general eigendecomposition problem: 
	ly	y 
alternatively  we could use y yt 1 as the constraint. then the solution is achieved by solving ly y . 
let e be a vector with all entry 1. it can be easily shown that e is an eigenvector with eigenvalue 1 for l . if the transition matrix is primitive  e is the only eigenvector for 
1 . the meaning of the first eigenvector is to map all data to a single point  which minimizes the optimization target. to eliminate this trivial solution we put an addition constraint of orthogonality: 
argmin y lyt
y
st. . yt1 yt1
thus the solution is given by the eigenvector of the smallest non-zero eigenvalue. generally  embedding the graph into rk  k 1  is given by the nk matrix y   y1...yk   where the ith row provides the embedding of the ith vertex. therefore we minimize  
	1	t
	p u v y     	u	yv	1  tr y ly  
v
it can be rewrite as  
	min tr y ly  t	 
 
t
	s. .t y	y	i
the solution is given by y *   1* ...  k* 1  where i* is the eigenvector of ith smallest eigenvalue of the generalized eigenvalue problem ly y . 
1 implementation issue  
input: adjacency matrix w  dimension of target space k and a perturbation factor 
1. compute p
 is a vector that 
the diagonal matrix of the out degrees. 
1. solve the eigenvalue problem t normalized equation te 1. 
1. construct the combinatorial laplacian of the directed 
	p	pt
	graph l	  where 
1
1. solve the generalized eigenvector problem ly let 1* ...  n* be the eigenvectors ordered according to their eigenvalues with 1* having the smallest eigenvalue 1  in fact zero . the image of x embedded into k dimensional space is given byy *
table 1. the dge algorithm 
the irreducibility of the markov chain guarantees that the stationary distribution vector  exists. we here will build a markov chain with a primitive transition probability matrix p . in general  for a directed graph  the matrix of transition probability p defined by p u v       w u v     /do    u is not irreducible. we will use the so called teleport random walk  a. langville and c. meyer  1  on a general directed graph. the transition probability matrix is given by 
	1	t	1	t
p	 d wo	e  	 1	 	ee n	n
where w is the adjacent matrix of the directed graph   is a vector that 1 if row i of w is 1  and do is the diagonal matrix of the out degree. then p will be stochastic  irreducible and primitive. this can be interpreted as a probability of transiting to an adjacent vertex and a probability 
1 of jumping to any point on the graph uniform randomly. for those vertices that don't have any out link  just uniform randomly jump to any point on the graph. such a setting can be viewed as adding a perturbation to the original graph. the smaller the perturbation is the more accurate result we can get. so in practice we only need to set to a very small value. in this paper  we simply set  to be 1. the stationary distribution vector then can be obtained by solving an eigenvalue problem tp t subject to a normalized equation te 1 . 
the algorithm for embedding vertices on a directed graph into a vector space is summarized in table1. 
1 relation with previous works  
in  m. belkin and p. niyogi   1  the authors proposed the laplacian eigenmap algorithm for nonlinear dimensional reduction. we can see if our algorithm is applied to an undirected graph we will get a similar solution to laplacian eigenmap. in the case of undirected graph  we can define the transition probability as p u v       w u v     /du   where w u v      is the weight of the undirected edge  u v      du is the degree of vertex u . if the graph is connected then the stationary distribution on vertex u can be proved equal to du /vol g      wherevol g  is the volume of the graph  thus 
p u v y       u u p u v     
                         u v  yv 1 w u v     /vol g   
	y dy vol gt	/	    
diag d  1 ... dn  . then the problem reduces to the 
laplacian eigenmap. 
in  d. zhou et al.  1  zhou proposed a semi-supervised classification algorithm on a directed graph  by solving an optimization problem. the basic assumption is the smooth assumption that the class labels of the vertices on the directed graph should be similar if the vertices are closely related. the algorithm is to minimize a regularization risk between the least square error and a smooth term. consider the problem that the data in the same class is scattered and the decision boundary is complicated  and the smooth assumption does not hold. then the classification result may be hindered. another problem is that by using least square error the data far away from the decision boundary also contribute a large penalty in the optimization target. thus considering the imbalanced data  the side with more training data may have more total energy  and the decision boundary is biased. in the experiment section we will show a comparison between zhou's algorithm and our method used together with an svm classifier. 
in  d. zhou et al.  1  the author also proposed the directed version of normalized cut algorithm. the solution is given by the eigenvector corresponding to the second largest eigen-
value of matrix 	  1/ 1p	1/ 1/ 1	t	1/ 1  /1 .  it can be seen that the eigenvector corresponding to the second largest eigenvalue of  is in fact the eigenvector v corresponding to the second smallest eigenvalue of  . 
note that we have such an equation  
 
therefore  the cutting result is equal to embedding data into a line by dge  then using threshold 1 to cut the data. 
1 experiments  
in this section  experiments are designed to show the embedding effect in both toy problems and real world data. using dge as a preprocess procedure  we also consider an application of the proposed directed graph embedding algorithm to a web page classification problem with comparisons to a state-of-art algorithm. 
toy problems 
we first test our algorithm on the toy data shown in fig.1.  the edge weights are set as binary values. fig.1 a  shows the result of embedding the directed graph into a plane. the red nodes and blue nodes in fig.1 a  are corresponding to the three nodes on the left and four nodes on the right in fig.1 respectively. from the figure  we can see the locality property of the graph is well preserved  and the embedding result reflects subgraph structure of the original graph. 
 
 a                                                        b  
figure 1. embedding result of two toy problems on 1d space 
in another experiment  a directed graph consisting of 1 vertices is generated. there are three subgraphs  and each consists of 1 vertices. weights of the inner directed edges in the subgraph are drawn uniformly from interval  1  1 . weights of directed edges between the subgraphs are drawn uniformly from interval  1  1 . by generating the edge weights in such a manner  each subgraph is relatively impact. the graph is a full-connected directed graph. if only given the graph without a prior knowledge of the data  we can hardly see the latent relation of the data. the embedding result by dge in two dimensional space is shown in fig.1 b . we can see that tightly related nodes on the directed graph are clustered in the 1d euclidean space. after embedding the data into a vector space  we can easily perceive the clustered structure of the original graph  which gives us insight on the principal issues such as latent complexity of the directed graph. 
 
 a                                                 b                                                c                                                 d  
figure 1. classification result: a  two-class problem  dimension=1; b  multi-class problem  dimension=1; c  multi-class in different dimension spaces by nonlinear svm; d  accuracy against dimension on a fixed  1 labeled samples  training set by linear 
svm web page data 
to address directed graph embedding task on real world data  we test our method on the webkb dataset. we consider a subset containing the pages from the three universities cornell  texas and wisconsin. we remove the isolated pages  resulting in 1  1 and 1 pages respectively. we may assign a weight to each hyperlink according to the textual content or the anchor text. however here we are only interested in how much we can obtain from link structure only and hence adopt the binary weight function. fig.1 shows the embedding result of the webkb data in three dimensional space. the red  blue and black nodes are corresponding to the web pages of three universities: cornell texas and wisconsin respectively. from the figure we can see that the embedding results of web pages in each university are relatively impact  while those of web pages in different universities are well separated. this shows our method is effective in analyzing the link structure between different universities  where the inner links in one university are denser than that between universities. 
 
 
figure 1. embedding result of webkb data 
application in web page classification 
our method can be used in many applications  such as classification  clustering  information retrieval. as an example  we apply our algorithm to a web page classification task. web pages of four universities cornell  texas  washington and wisconsin in webkb dataset are used. still the binary edge weight setting is adopted. we first use dge to embed the vertices into certain euclidean space  and then train an svm classifier to do the classification task. after that  we compare the results with the state-of-art classification algorithm  referred as zhou  proposed in  d. zhou et al.  1 . here we use nu-svm  b. sch lkopf and a. j. smola  1   a modified version of svm  which is easy for model selection. both linear and nonlinear svm are tested. in the nonlinear setting  rbf kernel is used.  in all experiments  the training data are randomly sampled from the data set. to ensure that there is at least one training sample for each class  we conduct the sampling again when there is no labeled point for some class. the testing accuracies are averaged over 1 times' experimental results. different dimensional embedding spaces are also considered to study the dimensionality of the embedded space. 
the comparing results of binary classification problem are shown in fig.1 a . we consider the web pages of two universities randomly selected from the webkb data set. we first use dge to embed the whole dataset into a 
1-dimensional space  then use svm to do the classification task. the parameter nu is set to 1 for both linear svm and nonlinear svm. the parameter of rbf kernel is set to be 1 for nonlinear svm. the parameter for zhou's algorithm is set to be 1 as proposed in his paper. from the figure  we can see that in all cases where the number of training samples varies from 1 to 1  dge used together with either liner or nonlinear svm consistently achieves better performance than zhou's algorithm. the reason might be that zhou's method directly applies the least square risk to the direct graph  which is convenient and suitable for regression problems  but not so efficient in some case of classification problems  such as imbalanced data. the reason is that the nodes far away from the decision boundary also contribute large penalty for the shape of the decision boundary.  after embedding the data into vector space  we are able to analyze the decision boundary carefully. nonlinear svm can show its advantage in such a situation. fig.1 b  show the result of the multi-class problem  in which each university is considered as a single class  and then training data are randomly sampled. for svm  we use one-against-one extension for multi-class problem. for zhou's algorithm  the multi-class setting in paper  d. zhou et al.  1  is used. the parameter setting is the same as the binary class experiments. from the figure we can see that significant improvements are achieved by our method. zhou's method is not very efficient in multi-class problem. besides the reason we discussed above  another problem of zhou's algorithm is the smooth assumption. when the data in one class are scattered in the space  the smooth assumption can not be well satisfied  and the decision boundary will be complicated  especially in the case of multi-class problem. directly analyzing the decision boundary on the graph is a difficult task. when embedding the data into vector space  complicated geometry analysis can be performed and sophisticated alignment of the boundary can be achieved using methods such as nonlinear svm.  
we also test different dimension settings for classification task. the same parameter setting for svm is used for training models on different dimensional spaces. fig.1 c  shows the comparing experimental results of nonlinear svm on embedded vector spaces where the dimension of the embedded space varies from 1 to 1. from the figure we can see that by first using dge to embed data into vector space the classification accuracies are higher than zhou's work in a large range of dimension settings.  
fig.1 d  shows the experimental results of linear svm on the dimension settings ranging from 1 to 1. the best result is achieved on about 1-dimensional space. in lower dimension spaces  the data may be not linear separable  but still has a rather clear decision boundary. this is why the nonlinear svm works well in those cases  fig.1 c  . in a higher dimension the data become more linear separable  and the classification errors get lower. when the dimension is larger than 1  the data become too sparse to train a good classifier  which hinders the classification accuracy. the experimental results suggest that the data on the directed graph may have a latent dimension in a euclidean vector space which is suitable for further analysis. 
1 conclusion 
in this paper  an efficient algorithm dge for embedding vertices on directed graphs to vector spaces is proposed. dge explores the inherent pairwise relation between vertices of the directed graph by using transition probability and the stationary distribution of markov random walks  and embeds the vertices into vector spaces preserving such relation optimally. experiments show the effectiveness of our method for both embedding problems and applications to classification task.  
