 
this paper presents and compares results for three types of connectionist networks on perceptual learning tasks: 
 a  multi-layered converging networks of neuron-like units  with each unit connected to a small randomly chosen subset of units in the adjacent layers  that learn by re-weighting of their links; 
 b  networks of neuron-like units structured into successively larger modules under brain-like topological constraints  such as layered  converging-diverging hierarchies and local receptive fields  that learn by re-weighting of their links; 
 c  networks with brain-like structures that learn by generation-discovery  which involves the growth of links and recruiting of units in addition to reweighting of links. 
preliminary empirical results from simulation of these networks for perceptual recognition tasks show significant improvements in learning from using brain-like structures  e.g.  local receptive fields  global convergence  over networks that lack such structure; further improvements in learning result from the use of generation in addition to reweighting of links. 
introduction 
connectionist networks are graphs of linked nodes. each node is a simple neuron-like unit. each link has a weight associated with it. the net input to a node is a weighted sum of the outputs of the nodes that fire into it. each node applies some form of non-linear function  such as the threshold or the sigmoid  to its net input and sends the result to other nodes to which it is connected via its output links. the receptive field of a node is defined as the set of nodes that can directly fire into it. 
     it is easy to show that networks of threshold units are universal computing engines  mcculloch  1  in the sense that there exist  sufficiently large  networks of such units that can compute any function computable by a turing machine or a system of post productions; but the problem of finding the necessary  sufficiently powerful  efficient and robust networks for perceptual recognition tasks remains  just as it does no matter how we try to embody intelligent 
¡¡¡¡¡¡this work was partially supported by grants from the national science foundation  the air force office of scientific research  and the university of wisconsin graduate school. a preliminary version of this paper appeared as a technical report  1 lonavar  1b . 
processes. 
     a perceptual recognition system should be capable of interacting with constantly changing environments and  therefore  capable of learning. learning can be viewed as a process of induction  constrained by the structure of the system as well as the input it receives from the environment. given the complexity and the variety present in the real world  the number of possible structures relate the different inputs is extremely large: given n inputs  each capable of taking v values  the number of possible structures relating them is vn. this suggests that a perceptual learning system should be constrained  by its structures and processes  to learn the meaningful subset of relations between its inputs  given its limited resources  and the tasks it has to perform. 
     learning in connectionist networks can involve modification of any of the following: 
 processing functions of the nodes  e.g.  changes in the threshold or the output function   
 the weights associated with the links  
 the topology of the network  addition and deletion of links and nodes   and 
 the learning rules themselves. 
     most of the work on learning in connectionist networks to date has concentrated on . several algorithms for changing weights associated with the links are available  hinton  1a . a learning scheme for  that employs a mechanism for growth of links and recruiting of nodes guided by regulatory mechanisms designed to discover minimally complex networks has been described in  honavar  1; honavar  1a . 
complexity issues 
given the turing equivalence of  sufficiently large  connectionist networks  the problem of building such networks for perceptual recognition tasks is reduced to one of discovering the design principles that yield economically feasible designs  for machine perception  and/or biologically plausible designs  for brain modeling . we briefly examine the complexity of perceptual recognition and list some observations on the physics of the environment and the structure of the brain that could potentially help us in deriving such design principles. 
     the complexity of recognition is 1 vn  for an /vpixel image where each pixel can range through v values. this means that to handle the general recognition problem  including the worst case  a network needs at least vn nodes  
     
1 	parallel and distributed processing 
     
each linked  either directly or via intermediate nodes in layers or some other structure  to all nodes in the input 
     
retina. this of course is combinatorially explosive  and our real problem is the expected case  that is  recognition of real-world images. the human brain and its visual system is clearly capable of perception of real-world objects in realtime  yet it does rather poorly at the worst case e.g.  telling apart two images that differ by a few randomly placed pixels. for the expected case e  the number of nodes needed is clearly within feasible bounds; otherwise nature could not have evolved brains capable of successful recognition. 
     if the structure of the human brain and the visual system is any indication  the necessary number of nodes  ne  is still almost certainly extremely large  and the necessary topology ge   of the network is far from random. a great deal is known about the human brain and the visual system  peters  1; uhr  1; crick  1; zeki  1; deyoe  1; livingstone  1 . neurons predominantly interact with near-neighbors and arc organized into highly ordered structures  columns  hypercolumns  areas ; yet a great deal is unknown about how the neurons get allocated for computing specific functions  and how the detailed topology of the network of neurons emerges as a result of learning through constant exposure to the environment 
     if the desired perceptual recognition abilities are to be attained by a connectionist network through re-weighting of its links alone  it must be initialized to contain a sufficient number of appropriately linked nodes. the only way to guarantee that this kind of network has enough nodes  each with the necessary links  is either to program them in  using a priori knowledge  or to make some guess as to ne - and use a substantially larger number of nodes and links than that to be on the safe side. to handle the full vision problem the only completely safe thing to do would appear to be to use vn nodes  each with n links - but this is impossibly large to actually implement. 
     generation involving the addition of nodes and links enables a network to modify its topology  and appears to offer a way out of this dilemma. given mechanisms to generate  the network can gradually grow  until the number of nodes approaches ne and the network topology approaches ge - whatever ne and ge may be. thus there is no need to estimate e: this is done constructively by the network itself. 
     rather than hope that some particular random or preprogrammed connectivity will work  or pay the excessive costs of complete connectivity  a system that generates can  under the implicit guidance of the environment's inputs and feedback  move toward sufficient connectivity. generation works best hand-in-hand with the fine-tuning of functions provided by re-weighting of links. in addition  generation and re-weighting arc probably best supplemented by mechanisms that break links when appropriate. some of these issues  as well as a specific learning scheme combining generation and re-weighting  have been examined in  honavar  1a . 
connectionist network structures compared experimentally 
the multi-layered converging network structures studied include those that learn by re-weighting of their links  with 

figure 1: summary of multi-layered  feed-forward  converging network structures; cp stands for the connectionist pyramids; r 
for random and l for local receptive fields; g for generation; e for built-in edge-detectors; a - in a given position indicates the absence of the corresponding network property; all use reweighting of links as a learning mechanism; only the last two use generation in addition to reweighting. 

figure 1: a converging pyramid-like structure: each point in a layer has a cluster of nodes; each node in a cluster computes a simple function over the outputs of nodes in the node-clusters in a 
small neighborhood in the layer below. 
no generation   using several types of connectivity - random  as well as restricted to near-neighbors  and those that learn by a combination of generation and re-weighting  where generation takes place within the constraints of nearneighbor connectivity . a summary of these network structures is given in figure 1. 
connectionist networks that learn by re-weighting 
     several multi-layer  converging connectionist networks  using the same number of nodes and links in all the cases  were built  with the following structure: 
     layer l contains l/1th the number of node-clusters found in the adjacent layer l - l . each node at layer l contains 1 times as many nodes per cluster as in the layer l-l. each node in a node cluster at layer l receives input from 1-tuples of nodes drawn from 1 node clusters in layer l - l . in the current implementation  layer 1 is an exception in that each node in layer 1 receives input from 1 nodes  in a 1 window  in the input layer. this forms an overall pyramidlike converging-diverging structure  figure 1 . in all the simulations described in this paper  the input layer  the retina  is a 1 array of pixels. 
     three variants of the basic multi-layered  converging network described above were implemented: 
 cp.l-e  
with local receptive fields preserving topographic 
	honavar and uhr 	1 
     
     mapping between layers: each node in layer l is linked to nodes in the 1 node clusters spatially located directly below it in layer l - l ; layer 1 contains 1 prewired edge detectors  these are simplified versions of the local spot and edge detectors found in the retina and primary visual area  vi  of living primate brains    cp.l--  
same as  cp.l-e  above  but without the built-in edge 
detectors in layer 1  and 
 cp.r-  
with random receptive fields: each node in layer l is linked to nodes in 1 randomly chosen node clusters in layer l - l . 
     in all the simulations  1 detectors  either pre-wired or learnable  were provided at layer 1. all the weights other than those corresponding to the built-in edge detectors were assigned randomly. 
     in all cases  learning involved re-weighting links as a function of the back-propagated error signal. suppose a pattern class cw is implied by the network with a weight ww  and the pattern class indicated by the feedback  cr is implied with a weight wr; the amount of reweighting at the output layer is given by  kx ww-wr   where k is a 
     parameter related to the rate of learning. our current implementation has k set equal to 1. this weight change is distributed equally among all the links firing into the node implying cw. at internal nodes  the weight changes arc computed in a similar fashion. this is similar in spirit to the generalized delta rule  rumelhart  1 . 
connectionist networks that learn by generation and 
discovery as well as re-weighting 
     connectionist network structures that learn by generation and re-weighting of links and recruiting of new nodes from a pool of unused nodes were studied. the topological constraints on the network structure are the same as those present in  cpjl-  and  cp.l-e  described earlier. however  the networks that learn by generation as well as reweighting start with a pool of nodes and no pre-wired links. generation grows new links and adds new nodes to the network from the pool of nodes as the network learns aided by feedback. the weights associated with the links are changed using the same reweighting mechanism as the one used in  cp.l- . a particular implementation of generation and reweighting of this sort is described in  honavar  1a . 
     because generation does not violate the topological constraints of the layered  logarithmically converging organization as well as the local receptive fields  the networks that are discovered through generation and reweighting  e.g.  
 cp.lge  and  cp.lg-   are topologically similar to  cp.l--  and  cp.l-e . but in contrast to  cp.l-   the number of nodes per node-cluster at a given layer in  cp.lg-  and  cp.lge   or the connectivity between node clusters in adjacent layers  is not pre-programmed; it is determined dynamically through learning. 
     runs were made with pre-wired edge-detectors in the first layer -  cp.lge   and without any pre-wired nodes  i.e.  having all the nodes added to the network as part of the learning process  -  cp.lg- . in both these cases  the 
1 	parallel and distributed processing 
reweighting of nodes as a function of feedback proceeds according to the same reweighting rule as the one used in  cp.l--  and  cp.l-e . in addition  the network occasionally generates a new node  when it determines this to be appropriate - on the basis of information provided by substructures that monitor the network's performance on each pattern class on which it is being trained. the design of these sub-structures is motivated by the need to discover the simplest networks capable of the desired accuracy of recognition. a particular implementation of such structures is explained in detail elsewhere  honavar  1a . 
     the rationale behind the design is as follows: continue to reweight existing links so long as the network's performance is improving. when it is observed that the network's performance has leveled off  before reaching the desired accuracy of recognition   generate a new transform. this is accomplished easily by a simple network of neuronlike units  using local computations that are performed incrementally following each training presentation  honavar  1a . 
     generation proceeds as follows: in the 1st layer  a 1by-1 sub-array is extracted from the raw input image  this is done only when feedback indicates an error was made  and the history of the recent past indicates that performance is levelling off rather than improving. these 1 links fire into a new node placed directly above it in the next layer. 
     the extraction is got from a busy part of the input image  one where the network judges there may be useful information. the present simple system insists that a gradient be present  but potentially more powerful mechanisms that enable the system to evaluate a certain region  e.g.  a 1 window  of the input for its information content  and their possible connectionist network implementations are being investigated. 
     in layers other than the 1st  extraction randomly links into a new node from 1 nodes that actively responded to the present  incorrectly identified  input image in the 1-by-1 of node-clusters directly below it in the previous layer. 
     whenever a transform is generated  it is put into a node-cluster at that location  and also at every other location in that layer of the network. this makes translation-invariant recognition of patterns possible. all the links added to the network through generation get tuned through reweighting as a function of feedback. 
experimental results 
several runs were made to compare multi-layered connectionist network structures   cp.r-    cp.l-    cp.l-e    cp.lg-  and  cp.lge  . simple 1-dimensional patterns such as letters of the alphabet  t  d  e  and simple objects  apple  cup  banana  were used for training the networks. the training and test sets were obtained by randomly dividing the set of drawings of each pattern provided by 1 different volunteers into two subsets. the drawings were made using the xgremlin graphics utility on a digital vaxstation-1  in a 1 subarray of a 1 grid. a sample subset of patterns used is shown in figure 1. figure 1 gives a summary of the pattern classes used in the runs  1% correct recognition given the same maximum number of connections that were used in  cp.l--  structures the networks  cp.l--  attained 1% accuracy of recognition with approximately 1xl1 links  which were distributed equally between layers  1    1    1  and  1  in about 1 epochs of training  whereas the networks  cp.l-e  attained the same perfomance with the same network size  in about 1 epochs of training. 
     the network  cp.lg-  attained 1% accuracy of recognition in about 1 epochs with about 1 links  1 new transforms were generated and they were replicated at 
	honavar and uhr 	1 
     
each location in the corresponding layers . the network  cp.lge  reached 1% correct recognition in about 1 epochs of training and at about 1xl1 links  1 new transforms were generated and they were replicated at each location in the corresponding layers . 
     the runs were repeated for  cp.lg-  and  cp.lge  with all 1 pattern classes  t  d  e  apple  banana  cup  and the results were qualitatively similar  but there were more generations  about twice as many  at the higher layers resulting in approximately loxlo1 and 1xl1 links respectively  and about twice as many epochs of training were needed for attaining 1% accuracy of recognition. the exact numbers reported here should not be given too much importance; however the results do suggest that other factors being equal  generation and local structure significantly improve learning  both in terms of the number of training epochs needed as well as the size of the networks necessary to attain the desired accuracy of recognition. 
discussion and summary 
retinotopic mapping and near-neighbor connectivity exploit spatio-temporal contiguity present in the environment. pyramid-like layered hierarchies enable the computation of complex functions as cascades and compounds of many simpler functions. architectures embodying such topological constraints have been studied rather extensively for image processing and computer vision  uhr  1; burt  1; rosenfeld  1; uhr  1; li  1 . the results presented in this paper suggest that the incorporation of similar brain-like constraints on network structure can significantly reduce the complexity  and improve the learning speed  of connectionist networks that learn  as opposed to being carefully programmed  to perceive patterns. the initial choice of network connectivity is important. random connectivity is unlikely to work in most practical problems. similar conclusions were reached in an experiment to train a connectionist network to match random-dot stereograms  qian  1 . 
     our results suggest that the addition of mechanisms that enable the network to grow new links as needed  under guidance from feedback  aided by network structures that enable it to monitor its own performance over time  yield further improvements in learning. 
     intuition suggests that good system performance requires a proper match between the entropy of the source of external stimuli and the connectivity  both between the source and the system  abu-mostafa  1  as well as within the system itself. generation relies on the environmental stimuli to develop the connectivity of the system. the resulting network is therefore likely to have a better match with the entropy of the environment than a network that starts out with a random subset of the possible connections and maintains its initial connectivity unchanged  so that learning can only adjust the weights associated with the links. 
     generation in a multi-layered  converging network with local receptive fields ensures that successively more complex non-linear relations between features in the input encoding of patterns can be discovered at higher layers  to 
1 	parallel and distributed processing 
be assessed by the new transforms that are added. thus the system is biased such that: learning of simpler features preceeds the learning of more complex relations; and successively more global relations are learned at successively higher layers. an examination of the transforms generated in the network simulations supports this intuition. 
     the extraction-generation programs described here do not discard bad transforms or place any limit on the number of nodes generated. neither capability was needed for the test runs reported here  since these programs learned to recognize the pattern-sets they were tested on in relatively small number of training epochs. but to handle larger sets of more complex patterns  the ability to discard is almost certainly necessary - otherwise the network will get bogged down with many poor or worthless transforms. 
     there are a number of promising improvements to be made  including the addition of networks that make better assessments of potential generations  that learn to improve upon these assessments  that evaluate the generations for their usefulness for recognition  that discard poor generations to make room for new ones  that narrow and broaden the tolerance-threshold for matching  and that generate sets of alternate possible transforms that are placed in competition with one another. there are a number of other issues to be investigated  including the development of good subnetworks that realize functions for deciding whether to further re-weight or to generate  the optimal number of nodes in a node-cluster  and the desirability of putting the nodes within a cluster into direct competition. 
     the extent of generalization  i.e.  building of meaningful internal representations by discarding uninteresting details  is an important property of connectionist networks that learn. more compact representations result from better generalization. there is reason to believe that the extent of generalization in connectionist networks is sensitive to the number of hidden units as well as the connectivity  hinton  1b . if the hidden units  or connections  are too many  the network may generalize rather poorly; if they are too few  the network may never learn. therefore  finding the optimal number of hidden units and/or weights is of interest. generation and deletion of links can be seen in this context as providing mechanisms that dynamically determine the number of hidden units and connections needed in the network. thus networks that generate only as needed may exhibit good generalization properties as well. generation makes possible the linking up of an adequate number of units to solve a given problem; minimal generation favors the discovery of the smallest necessary number  and hence  better generalization. it would be interesting to examine this conjecture experimentally. 
     sub-networks that maintain  update  and transmit as appropriate  information about the network's performance over time  e.g.  a portion of the learning curve  used to trigger generation  offer several interesting mechanisms to influence learning that may be worth examining. such structures may be used to alter learning strategies  rates of learning  thresholds of firing  each of which has an impact on the plasticity of the network. future work will address some of these issues. 
     
     in connectionist networks that learn  feedback-guided reweighting of links in by small amounts effectively performs a gradient descent on a function that represents the error between the output desired and the output produced by the network so as to minimize that error. however  there is always a risk of getting caught in a local minimum  a shallow trough  or a valley in the error surface. generation and discarding of transforms can be thought of as providing the network some means of climbing out of such local minima. 
     most of the work on learning in connectionist networks has to date concentrated on reweighdng schemes for modification of weights in a static topology. recent anatomical and physiological studies suggest that learning may involve alteration of the number as well as the pattern of synapdc interconnecdons in the brain  in addition to changes in synapdc weights  greenough  1; honavar  1 . the results presented in this paper suggest that there may be promising improvements to be realized using addidonal learning mechanisms that dynamically alter the network topology  e.g.  generadon   suitable constraints on the network structure for particular domains  such as local receptive fields and global convergence for vision  and regulatory mechanisms that alter the plasticity of the network  choose between different learning strategics  and so on. extensive and systemadc evaluadons of networks incorporating one or more of these features for perceptual learning of pattern sets of varying degrees of complexity are needed in order to determine how they perform individually as well as collectively. the experiments and results discussed in this paper constitute at best  a preliminary exploration of only a few aspects of the problem. work in progress is directed at examining some of these issues in greater detail. 
