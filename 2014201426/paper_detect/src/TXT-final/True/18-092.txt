 
　　　in this paper we address the problem of finding the spatial position and orientation of an object from a single image. it is assumed that the image formation process and an object model are known in advance. sets of image lines are backprojected and constraints on their spatial interpretations are derived. a search space is then constructed where each node represents a space feature with a model assignment. next  a 
　　　hypothesize-and-test recognition strategy is used to select a solution  that is to determine six degrees of freedom of a part from a set of features. finally we discuss the efficiency and the reliability of the method. 
1. introduction 
　　　among various aspects of perception for which computer vision is trying to find a computational theory  an intriguing one is the mechanism by which two-dimensional  1d  shapes are sometimes perceived as three-dimensional  1d  objects. there are at least two reasons for investigating this subject. first  there is evidence that people perform spatial reasoning whenever they deal with images. if they are asked to match two images of the same object  they rotate the object mindly in the 1d space even if the transform occurs in the image plane. this implies that the interpretation space is different from the image plane. and second  we are interested in devising a technique for interpreting images of known things : when is it possible to retrieve the six degrees of freedom of an object from a single view assuming that the object and the camera geometry are known in advance   which are the theoretical and practical limitations of such a method if it were implemented as a computer algorithm   an interesting application could be the recognition of man made parts in an industrial environment. 
　　　in this paper we suggest on possible approach limited to objects bounded by planar faces. first we extract linear edges from an intensity image and these edges are combined to form angles and junctions which are assumed to be projections of 1d object vertices. these sets of image fea-
 the work reported herein is supported by  laboratoire d'electronique et de technologie de i'informatique   grenoble  france. 
**the author is now with lif1a  b.p. 1  1 saint-martin d'heres  france. 
tures are backprojected using an inverse perspective camera model and some constraints on their spatial position and orientation are derived. a search space is built where each node represents a 1d feature with a model assignment. a hypothesize-and-test recognition strategy implemented as a depth-first tree search is used to find a solution  that is three rotations and three translations for each part. the object model together with physical constraints are used as heuristics for reducing the complexity of the search space. 
　　　previous approaches for interpreting line drawings have generally not been designed to deal with the geometry of perspective. they have usually used orthography and the gradient space  l   . others have simplified the problem by using the  support hypothesis  which reduces the problem to three degrees of freedom      1  . our approach is more general and it can include this hypothesis as a physical constraint. more recently the  gaussian mapping  has been introduced as a tool for interpreting perspective views  . a method for finding vanishing points is described and one for retrieving the spatial orientation of planes by backprojecting angles and curvature is suggested. we extend these results to junctions which we believe are more useful than faces. a 
　　　two-stage  model-based recognition procedure is described in . the planning stage computes all possible appearances of an object in terms of sets of simultaneously visible features. the recognition stage consists in a predict-observe-backproject sequence. this approach is different from ours since it doesn't explore the constraints available with sets of features. 
1. backprojection of image features 
　　　this paragraph utilizes the perspective camera model for interpreting image linear features. let us recall briefly this model  . a space point with camera coordinates  x y z  projects onto the image at  x.f/z  y.f/z  f  where f is the focal length  see figure 1 . the camera frame has its origin at the focal center and the image is parallel to the x-y plane at distance f from the center along the z-axis. a unit vector can be expressed as a point on a unit sphere centered at the origin  the gaussian sphere. a 
　　　point on this sphere has two angles as coordinates  the azimuth  a  and the elevation  1 . hence  the orientation of a space plane or the direction of any image or space line can be represented as a point on this sphere. let's now associate an interpretation plane with an image line. this 

plane is defined by an image line and the focal center and it contains all the spatial interpretations of the image line. the possible directions of these spatial lines lie on a great circle  the intersection of the interpretation plane with the gaussian sphere. if we denote by p the vector normal to the interpretation plane and by l the space line direction vector  the equation of the great circle is : 

　　　similarly we can develop constraints for the spatial interpretations of image angles and junctions. the motivation for choosing these features is that they are the projections of object vertices. let l1  and l1  be two image lines forming an angle. their spatial interpretations are denoted l1 and l1 and their interpretation planes are denoted p1 and pp. l1 and l1 are constrained to be coplanar : they belong to a space plane s and form a space angle w. we are seeking the orientation of s when w is known. the following equations stand : 

　　　since w is imposed  equation  1  provides a constraint for the possible orientations of the space plane s. consider now a junction formed by three image lines  1  1 and 1 whose spatial interpretation is a right vertex with edges l1 l1 and l1.  there is no loss of generality in considering a right vertex ; this merely simplifies the exposition   l1 and 1 can be combined just as above to form an angle constraint. notice that l1 is parallel to the vector normal to the plane formed by l1 and l1. l1 is constrained to lie on the great circle corresponding to the spatial interpretations of i1. therefore  the only possible orientations of s are the intersection of this great circle  eq. l   with the angle constraint  eq. 1  . figure 1 shows the solutions for the image junction indicated by an arrow on figure 1. only half of the gaussian sphere is projected and shown on figure 1 with a  horizontal  varying from it/1 to 1n/1 and 1  vertical  varying from -n/1 to n/1. the two solutions correspond to two orientations of s  one for a concave vertex and the other for a convex one. without additional information it is impossible to decide which solution to select. this is a simplified version of the necker's cube illusion. in   l     kanade developed an analytical solution in the case of orthographic projection but his method requires the measurement of the skewed symmetry of all the faces forming a junction. 
1. image to object correspondence 
　　　the ultimate goal of a recognition procedure is to assign an object model to a set of image features and to find the spatial parameters of each object. these parameters will be embedded in a 1 homogeneous matrix that maps an object from model coordinates to camera coordinates. let us show now how such a transform may be computed. 
	r.horaud 	1 
　　　we describe first a simple scheme for modelling objects within the context of visual recognition. for a more complete discussion  see 1j. such a model contains lists of those features and combinations of features that are the most likely to be detected in an image. the features are also ranked according to the contribution they can make for recognition. the model of an object bounded by planar faces provides a list of all faces with pointers from each face to its bounding edges and similarly each edge points back onto the two faces forming i t . another list contains all the vertices and each vertex points onto its three edges. let v be one vertex and l   l1 and l1 its edges. there is a vertex centered coordinate system whose axes are l   l1 and the normal to the face bounded by these two edges. the relation between this frame and an object centered coordinate system is completely defined by the geometry of the object and it can be expressed by a 1 homogeneous transform matrix  am. this transform embeds three rotations and three translations that allow to overlap one frame onto the other. 
　　　suppose now that we know the object assignment of an image junction. that is  there is a unique correspondence between the junction's lines and the edges of the vertex. since the backprojection of the junction constraints the orientations of the face s formed by l1 and l1 to just one direction  we can use equations  1  to determine the vectors l1 and l1. this will determine the rotation part of a matrix ac that maps the vertex centered frame into the camera centered frame. the position of the junction in the image determines two translations. in conclusion  under a junction-to-vertex assignment five degrees of freedom are determined. depth can be computed by triangulation if there is another junction or angle to which a vertex can be assigned. from am and ac one can compute the object-to-camera transform  a : 
		 1  
　　　the actual correspondence between the model and an image feature set is performed by a hypothesize-and-test procedure. a search space is built where each node represents a junction-tovertex assignment. the goal is to find the largest set of nodes that are mutually compatible  i.e.  they uniquely define the six degrees of freedom of the part. an object orientation and location is hypothesized from one node  excluding the depth for which initial lower and upper bounds are given . from this assignment a set of visible vertices is computed and for each such vertex its image projection is determined. this could be a junction  if two or three faces are visible or an angle if only one face is visible. for each prediction  the best image feature match is selected. notice  however that the low level segmentation process is not perfect and the data are noisy. for these reasons some lines may be missing. if the verification step fails in finding a predicted junction or angle it checks for partial descriptions of these items in the line list. for each assignment a score is computed by calculating the percentage of object features 

1 r. horaud 
predicted visible that actually overlap image features. if this score is high enough  the location of the image features as well as their spatial orientation constraints are used for refining the object locational parameters and for estimating tighter bounds for the depth. if the score is too low  the algorithm backtracks to the last choice point. 
1. experimental results 	