integration of syntactic  semantic and contextual information in processing grammatically ill-formed inputs 
o s a m u i m a i c h i and y u j i m a t s u m o t o 
graduate school of information science  
nara institute of science and technology 
1 takayama  ikoma  nara 1  j a p a n 
{osamu-im matsu} is aist-nara acjp 

a b s t r a c t 
this paper describes an integrated method for processing grammatically ill formed inputs we use partial parses of the input for recov ering from parsing failure in order to select partial parses appropriate for error recovery  cost and reward are assigned to them cost and reward represent the badness and goodness of a partial parse  respectively the most appropriate partial parse is selected on the ba sis of cost and reward trade off the system contains three modules module a handles local ill-formedness such as constraint violations module b handles non-local ill formedness such as word order violations  and module c handles non-local ill-formedness such as contextual ellipses these three modules work in a uniform framework based on the notions of cost and reward 
1 	introduction 
interaction with computers using natural language has been a major goal of artificial intelligence database sys tems and expert systems require flexible interfaces that allow users to communicate with the system in natu ral language though many natural language processing systems have been developed  most of them assume that input sentences are grammatically correct how ever  when users communicate with the system  they often use grammatically ill-formed sentences  especially in spoken dialogues for example  the users omit some words  change the word order  or make some careless errors buch as agreement errors  misspellings or adding of extra words to use nlp systems in real applications  we need to construct an nlp system that can handle not only grammatically well-formed inputs but also grammatically ill formed inputs in other words  a robust nlp system should not just reject grammatically ill-formed inputs  but process them in any case 
　this paper describes a new method to deal with grammatically ill-formed inputs our framework adopts a 
　two-stage model consisting of ft normal parsing process and. a recovery proceed if the normal parsing process fails to find a complete parse for the input  the recov ery process is invoked since we adopt bottom up chart parsing  kay  1  for the parsing engine  the recovery process can use partial parses for recovering from parsing failure the main mechanism is the selection of partial parses appropriate for error recovery in order to select the most appropriate one  cost and reward ' are assigned to partial parses cost indicates the degree of inconsistency included in a partial parse  and it is calculated by cost based unification to be described below reward in 
dicates the degree of their contribution to error recovery this mechanism is used in modules a and b 
　we also introduce the mechanism called lexical ex pectation lexical expectation uses: the number and type information of the phrases with which the current phrase can combine in order to become complete since we adopt a japanese grammar based on hpsg  head driven phrase structure grammar   pollard and sag  
1   this information is described in lexical entries this mechanism is used in module b 
　1 formed inputs are handled by three modules de pending on the types of ill-formedness module -1 han dies local ill formedness such as constraint violations module b handles non-local ill-formedness such as word order violations  and module c handles non local ill formedness such as incomplete sentential fragments and contextual ellipses modules a and b try to find a phrase which would cover the whole input  whereas module c 
　tries to find an appropriate interpretation using contex tual information each module alone is not sufficient for handling various types of ill-formedness the modules work on the basis of cost and reward trade off  and thus they are integrated into a uniform framework conse quently  various typeb of ill-formedness can be handled in a flexible way 
　the paper is organized as follows section 1 gives an overview of the related work and discusses the advan tages and disadvantages section 1 reports the result of a corpus analysis of ill formed phenomena and classifies them section a describes cost-based unification section 1 gives the detailed description of modules a.  b and c finally  section 1 summarizes the method and discusses the advantages and disadvantages 
   'the notions of cost and reward were inspired by rele vance theory  sperberand wiliion  1   though our notions are a bit different from those in their theory 
	imaichi and matsumoto 	1 

1 	related works 
in this section  we give an overview of the previous works and discuss their advantages and disadvantages 
　 weischedel and sondheimer  1  proposed the relaxation method based on atn  augmented transition network   woods  1  their relaxation method uses meta rules a meta rule is a kind of production rule in the expert system and each of them describes a recovery method from a certain type of ill-formedness meta rules are used when the normal parsing process has failed to find a complete parse the algorithm is as follows 
when the normal parsing process fails to find a complete parse for the input and is blocked  the recovery process applies meta rules in order to relax some constraints on the blocked parse and resume processing  and repeats the process until a complete parse is found 
　this method has the advantage of providing a uni form way to handle both syntactic and semantic ill formedness moreover  this is an elegant method for extending the coverage of a grammar to include grammatically ill-formed inputs  while retaining a connection between acceptable expressions and unacceptable ones however  since they use an atn parser  which works in a top-down and backtracking manner  the detection of errors is not always easy if the parsing failure does not occur at the erroneous position for the same reason  it is impossible to use the context to the right of the erroneous point in the input in addition  this method can handle only the types of ill formedness considered in advance if the system tries to handle various kinds of ill formedness using meta rules  the number of meta-rules becomes huge and the meta rules also conflict each other in the same way as production rules in expert systems 
　 douglas and dale  1j realized the relaxation method using the patr-ii grammar formalism  shieber  1  they introduce relaxation levels and relaxation packages consisting of a set of constraints the relaxation packages are associated with each patr-ii gram mar rule and they represent sets of constraints which can be relaxed they are defined according to the relaxation levels if all relaxation packages at a relaxation level are satisfied  the grammar rule is used otherwise  the violated relaxation packages are recorded  so that the constraints can be relaxed later 
　this approach describes constraints using feature structure  which is general linguistic data structure in this respect  their approach is superior to weischedel and sondheimer's work however  this method treats constraint violations only it cannot handle ill-formed phenomena that are not caused by constraint violations  such as word order violations to relax a constraint  the method simply removes the relaxation package  but the information indicating the constraint violation should be left for the further process moreover  relaxation pack ages at each relaxation level must be described in ad vance the method does not consider the use of semantic and contextual information 
　a syntax-based approach is taken by  mellish  1   who applied chart parsing  kay  1  to deal with gram matically ill formed inputs the advantage of using 
1 	natural language 
chart parsing is that the information of partial parses can be used for processing grammatically ill formed in puts the aim of his work is to explore purely syntactic and grammar-independent techniques to enable a parser to recover from simple kinds of ill formedness such as unknown and misspelled words  omitted words and extra noise words the method is divided into two phases the bottom-up  bu  phase and the top-down  td  phase the bu phase runs first to find a complete parse if the input is grammatically well formed  it successfully finishes at this phase otherwise the parsing proceeds to the td phase the td phase is performed by hy pothesizing errors that may have caused a failure in the parsing in this system  edges in the chart are assessed by a number of parameters to decide which edges may cause an error 
　the advantages of this method are that the normal parsing process is not affected by the extra mechanism for the recovery process and that the recovery process never repeats the same work done before by the bottom up parsing moreover  owing to the property of a chart parser  in contrast to an atn parser  it is possible to use both left and right contexts in the input for the determi nation of the best parse this method efficiently parses grammatically ill-formed input which contains only one error however  if multiple errors occur in the input  the behavior gets dramatically worse because of the inefficiency of the top down phase this method uses only syntactic knowledge and can handle only simple types of errors it takes no account of the use of semantic and contextual information 
　the advantages and disadvantages of the previous works can be summarized as follows 
the advantages 
 1  to provide a uniform way to handle not only syntactic ill-formedness but also semantic ill formedness  weischedel and sondheimer's   
 1  to disallow the recovery process to affect the normal parsing process  weischedel and sondheimer's  mellish's and douglas and dale's  and 
 1  to be able to use the left and right contexts at erroneous points  mellish's  
the disadvantages 
 1  to work in a backtracking manner  weischedel and sondheimer's   
 1  to be unable to use the left and nght contexts at erroneous points  weischedel and sond heimer's   
 1  to consider the types of lll-formedness in advance  weischedel and sondheimer's and dou glas and dale's   
 1  to handle only simple syntactic errors  mellish's  and 
 1  to take no account of a recovery invoked by semantic and contextual information  mellish's and douglas and dale's  
　our method for dealing with grammatically ill formed inputs overcomes the disadvantages while retaining the 


	imaichi and matsumoto 	1 

the time being  we give a uniform weight to each feature the cost-based unification and the cost value are 
defined as follows 
　in classical unification  we cannot unify the following feature structures 
	number 	singular 
	person 	third 	 i: 
	number 	plural 
	person 	third 	 1; 
however  in cost-based unification  we succeed in the unification  and the resultant feature structure is the fol lowing 
	number 	t{singular  plural} 
	person 	third 	 1  
we call t{singular  plural} an inconsistent set the symbol t indicates an inconsistency an inconsistent set contains inconsistent values as its elements  such as plural and singular in the example above the cost value of the resultant feature structure is defined as the total number of excess elements in the inconsistency sets 
in this case  the value of the cost is 1 
　next we consider how cost based unification works when an inconsistent feature structure and a consistent one are unified there are three types 
　the first type la the case where a new inconsistency is added by the unification in this case  the new inconsistent value is added to the inconsistent set for instance  given the feature structures  1  and  1  the resultant feature structure is as in  1  and its cost value is 1 
	number 	singular 
	person 	t h u d 	 1  
	number 	singular 
	person t{first  second} 	 1  
	number 	singular 
person t{first  second  third} 
　the second type is the case where a new inconsistent value is already contained in the inconsistent set for instance  the cost-based unification of the feature struc tures  1  and  1  results in the feature structure  1  the cost value is 1 
	number 	singular 
	person 	second 	 1  
　the third type is the case where both feature structures contain inconsistency in this case  the result has the union of the inconsistent sets included in the feature structures for example  the resultant feature structure of cost-based unification of the feature structures  1  and  1  is equal to the feature structure  1  and its cost value is 1 
	number 	singular 
	person 	t{second  third} 	 1  
1 	natural language 

figure 1 the flow of partial parses 
1 	i n t e g r a t e d m e t h o d for d e a l i n g w i t h g r a m m a t i c a l l y i l l - f o r m e d i n p u t s 
1 	o u t l i n e 
figure 1 shows the architecture employed in our system and the flow of partial parses there are three components the normal parsing -process  the meta process and the recovery process the normal parsing process per forms syntactic parsing only chart parsing algorithm is used as the parsing engine this process only constructs context free skeletons and passes the intermediate results to the meta-process  which performs cost based unification the meta process returns them to the normal pars ing process only when inconsistent information is not detected under cost based unification the meta process passes all the partial parses to the recovery process 
　if the normal parsing process succeeds in finding a complete parse  the recovery process is not invoked oth erwise  the recovery process is invoked by the metaprocess the recovery process works according to mod ules .1. and b to be discussed below it selects a par tial parse appropriate for error recovery  and passes it to the normal parsing process then the normal parsing process resumes to process this process finishes when the normal parsing process finds a phrase covering the whole input then module c is invoked for interpreting it within the context 
1 	m o d u l e a 
if the normal parsing process fails to find a complete parse for the input  the recovery process is invoked and it selects a partial parse with a cost and passes it back to the normal parsing process  which then resumes its process the recovery process must therefore select a partial parse that gives most plausible explanation of the ill formedness we introduce the following criterion for the selection 
  selection criterion 
select a partial parse with minimal cost and the maximal reward 
the criteria of reward are defined as follows 
  reward criterion a 
1 verb phrases 
1 noun phrases 
1 other phrases 



inputs using contextual information module c handles ill-formed inputs of type 1 and mainly does semantic processing 
　in example 1 b  the normal parsing process finds that  is a complete noun phrase case marked by the postpositional subject case-marker  modules a and b finish their processing because the phrase covers the whole input 
   we believe that such a fragment should not be re covered to form a complete sentence rather  it should be understood within the context therefore  module c does not try to find the missing verb  but checks whether the example 1 b is understandable in the con text  that is  understandable as an answer to the question 1 a in the semantic representation of the question 1 a  the agent role of  is not instantiated since  is syntactically and semantically appropriate for the agent role of the verb  module c regards the answer 1 b as well-formed in the context in this module  we will employ the context selection method proposed by hirasawa  hirasawa  1  
1 	conclusion 
in this paper  we discussed a new method to deal with grammatically ill-formed inputs our method overcomes the disadvantages of the previous works  whde still re taining their advantages it contains the following three modules 
  module a uses partial parses which have a cost  
  module b uses lexical expectation mechanism  which uses syntactic and semantic information  and 
  module c uses contextual information for finding an appropriate interpretation of the input 
　module a handles local ill formedness such as constraint violations module b handles non-local illformedness such as word order violations and module c handles non-local ill-formedness such as incomplete sentential fragments and contextual ellipses modules a and b try to find a phrase covering the whole input  whereas module c receives the result of modules a and b and finds an appropriate interpretation of the input using contextual information 
　we have integrated these modules into a uniform framework on the basis of the notion of cost and reward cobt represents the degree of inconsistency included in a partial parse  and reward represents the goodness of partial parses though each module alone is not enough to deal with various types of ill formedness  by integrat ing these modules  our framework can process them in a flexible way the role of each module is still explicitly separated although they are integrated into a uniform framework this makes it easy to extend the coverage of the framework 
　we did not discuss the types of ill-formedness such as self-repaired utterances  parenthetic phrases and repetitions we need to deal with these types of ill-formedness especially when we handle spoken utterances sagawa et al  sagawa et al  1  propose a method for coping with such ill-formedness their method transforms a self repaired utterance into well-formed one this method 
1 	natural language 
uses some linguistic clues  which include repetitions  un known words and isolated words although we have not fully implemented  we will employ sagawa's method as a pre processor for handling such ill-formedness 
　the objective of most of the works on robust parsing is to construct a mechamsm for dealing with intra sentential ul-formedness however  japanese contains a lot of mter-sentential ill-formedness  such as contextual ellipsis  omission of case elements and zero pronoun  and to deal with these phenomena  we need to construct a 
　module for handling intra-sentential ill-formedness in our framework  module c plays this role at present  module c is not implemented yet in future work  we intend to construct module c on the basis of relevance theory  sperber and wilson  1  a part of the work is done by  hirasawa  1  
a c k n o w l e d g e m e n t s 
i am indebted to dr rnstiina jokinen for helpful com ments on an earlier draft of this paper 
