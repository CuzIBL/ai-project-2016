 
keyphrases are useful for a variety of purposes  including summarizing  indexing  labeling  categorizing  clustering  highlighting  browsing  and searching. the task of automatic keyphrase extraction is to select keyphrases from within the text of a given document. automatic keyphrase extraction makes it feasible to generate keyphrases for the huge number of documents that do not have manually assigned keyphrases. a limitation of previous keyphrase extraction algorithms is that the selected keyphrases are occasionally incoherent. that is  the majority of the output keyphrases may fit together well  but there may be a minority that appear to be outliers  with no clear semantic relation to the majority or to each other. this paper presents enhancements to the kea keyphrase extraction algorithm that are designed to increase the coherence of the extracted keyphrases. the approach is to use the degree of statistical association among candidate keyphrases as evidence that they may be semantically related. the statistical association is measured using web mining. experiments demonstrate that the enhancements improve the quality of the extracted keyphrases. furthermore  the enhancements are not domain-specific: the algorithm generalizes well when it is trained on one domain  computer science documents  and tested on another  physics documents . 
1 introduction 
a journal article is often accompanied by a list of keyphrases  composed of about five to fifteen important words and phrases that express the primary topics and themes of the paper. for an individual document  keyphrases can serve as a highly condensed summary  they can supplement or replace the title as a label for the document  or they can be highlighted within the body of the text  to facilitate speed reading  skimming . for a collection of documents  keyphrases can be used for indexing  categorizing  classifying   clustering  browsing  or searching. keyphrases are most familiar in the context of journal articles  but many other types of documents could benefit from the use of keyphrases  including web pages  email messages  news reports  magazine articles  and business papers. 
　the vast majority of documents currently do not have keyphrases. although the potential benefit is large  it would not be practical to manually assign keyphrases to them. this is the motivation for developing algorithms that can automatically supply keyphrases for a document. section 1 discusses past work on this task. 
　this paper focuses on one approach to supplying keyphrases  called keyphrase extraction. in this approach  a document is decomposed into a set of phrases  each of which is considered as a possible candidate keyphrase. a supervised learning algorithm is taught to classify candidate phrases as keyphrases and non-keyphrases. the induced classification model is then used to extract keyphrases from any given document  turney  1  1; frank et al.  1; witten et al  1  1 . 
　a limitation of prior keyphrase extraction algorithms is that the output keyphrases are at times incoherent. for example  if ten keyphrases are selected for a given document  eight of them might fit well together  but the remaining two might be outliers  with no apparent semantic connection to the other eight or to each other. informal analysis of many machine-extracted keyphrases suggests that these outliers almost never correspond to author-assigned keyphrases. thus discarding the incoherent candidates might improve the quality of the machine-extracted keyphrases. 
　section 1 examines past work on measuring the coherence of text. the approach used here is to measure the degree of statistical association among the candidate phrases  church and hanks  1; church et al  1 . the hypothesis is that semantically related phrases will tend to be statistically associated with each other  and that avoiding unrelated phrases will tend to improve the quality of the output keyphrases. 
　section 1 describes the kea keyphrase extraction algorithm  frank et al.   1; witten et al .    1  1 . each candidate phrase is represented in kea as a feature vector for classification by a supervised learning algorithm. four different sets of features are evaluated in this paper. 
two of the sets have been used in the past  frank et al.   
1  and the other two are introduced here  to address the problem of incoherence. the new features are based on the use of web mining to measure the statistical association among the candidate phrases  turney 1 . 
1 	information extraction 　in section 1  experiments show that the new web mining features significantly improve the quality of the extracted keyphrases  using the author's keyphrases as a benchmark. in the first experiment  section 1   the algorithm is trained and tested on the same domain  computer science documents . in the second experiment  section 1   the algorithm is trained on one domain  computer science  and tested on another  physics . in both cases  the new features result in a significant improvement in performance. in contrast  one of the old feature sets works well in the first case  intradomain  but poorly in the second case 
 interdomain . 
　section 1 considers limitations and future work. section 1 presents the conclusions. 
1 	related work 
section 1 discusses related work on generating keyphrases and section 1 considers related work on measuring coherence. 
1 	assignment versus extraction 
there are two general approaches to automatically supplying keyphrases for a document: keyphrase assignment and key phrase extraction. both approaches use supervised machine learning from examples. in both cases  the training examples are documents with manually supplied keyphrases. 
　in keyphrase assignment  there is a predefined list of keyphrases  in the terminology of library science  a controlled vocabulary or controlled index terms . these keyphrases are treated as classes  and techniques from text classification  text categorization  are used to learn models for assigning a class to a given document  leung and kan  1; dumais et al  1 . usually the learned models will map an input document to several different controlled vocabulary keyphrases. 
　in keyphrase extraction  keyphrases arc selected from within the body of the input document  without a predefined list. when authors assign keyphrases without a controlled vocabulary  in library science  free text keywords or jree index terms   typically from 1% to 1% of their keyphrases appear somewhere in the body of their documents  turney  1 . this suggests the possibility of using author-assigned free text keyphrases to train a keyphrase extraction system. in this approach  a document is treated as a set of candidate phrases and the task is to classify each candidate phrase as either a keyphrase or non-keyphrase  turney  1  1; frank et al.  1; witten et al  1  1 . 
　keyphrase extraction systems are trained using a corpus of documents with corresponding free text keyphrases. the genex keyphrase extraction system consists of a set of parameterized heuristic rules that are tuned to the training corpus by a genetic algorithm  turney  1  1 . the kea keyphrase extraction system uses a naive bayes learning method to induce a probabilistic model from the training corpus  frank et al  1; witten et al  1  1 . after training  both systems allow the user to specify the desired number of keyphrases to be extracted from a given input document. 
　in experimental evaluations  using independent testing corpora  genex and kea achieve roughly the same level of performance  measured by the average number of matches between the author-assigned keyphrases and the machineextracted keyphrases  frank et al  1; witten et ai  1  1 . depending on the corpus  if the user requests five keyphrases to be output  an average of 1 to 1 of the output phrases will match with the author-assigned keyphrases  frank et al  1 . if the user requests fifteen keyphrases  an average of 1 to 1 will match with the author's keyphrases. 
　these performance numbers are misleadingly low  because the author-assigned keyphrases are usually only a small subset of the set of good quality keyphrases for a 
　given document. a more accurate picture is obtained by asking human readers to rate the quality of the machine's output. in a sample of 1 human readers rating keyphrases for 1 web pages  1% of the 1 phrases output by genex were rated as  good   1% were rated as  bad   and 
1% were left unrated  turney  1 . this suggests that about 1% of the phrases are acceptable  not  bad   to human readers  which is sufficient for many applications. 
　asking human readers to rate machine-extracted keyphrases is a costly process. it is much more economical to use author-assigned keyphrases as a benchmark for evaluating the machine-extracted keyphrases. jones and paynter  1  1  argue that it is reasonable to use comparison with authors as a surrogate for rating by human readers. they show  1  the vast majority of kea keyphrases are rated positively by human readers   1  the performance of kea is comparable to genex  according to human ratings   1  readers generally rate author-assigned keyphrases as good  and  1  different human readers tend to agree on the rating of keyphrases. 
　furthermore  gutwin et al.  show that machineextracted keyphrases are useful in an application. the keyphind system uses keyphrases extracted by kea in a new kind of search engine. for certain kinds of browsing tasks  keyphind was judged to be superior to a traditional search engine  gutwin et al.  1 . in summary  there is solid evidence that author-assigned keyphrases are a good benchmark for training and testing keyphrase extraction algorithms  but it should be noted that the performance numbers underestimate the actual quality of the output keyphrases  as judged by human readers. 
1 	coherence 
an early study of coherence in text was the work of halliday and hasan . they argued that coherence is created by several devices: the use of semantically related terms  coreference  ellipsis  and conjunctions. the first device  semantic relatedness  is particularly useful for isolated words and phrases  outside of the context of sentences and paragraphs. halliday and hasan  called this device lexical cohesion. morris and hirst  computed lexical cohesion by using a thesaurus to measure the relatedness of words. recent work on text summarization has used lexical cohesion in an effort to improve the coherence of machinegenerated summaries. barzilay and elhadad  used the wordnet thesaurus to measure lexical cohesion in their approach to summarization. 
　keyphrases are often specialized technical phrases of two or three words that do not appear in a thesaurus such as 

information extraction 	1 

wordnet. in this paper  instead of using a thesaurus  statistical word association is used to estimate lexical cohesion. the idea is that phrases that often occur together tend to be semantically related. 
there are many statistical measures of word association 
 manning and schutze  1 . the measure used here is pointwise mutual information  pmi   church and hanks  
1; church et al  1 . pmi can be used in conjunction with a web search engine  which enables it to effectively exploit a corpus of about one hundred billion words  turney  1 . experiments with synonym questions  taken from the test of english as a foreign language  toefl   show that word association  measured with pmi and a web search engine  corresponds well to human judgements of synonymy relations between words  turney  1 . 
1 	kea: four feature sets 
kea generates candidate phrases by looking through the input document for any sequence of one  two  or three consecutive words. the consecutive words must not be separated by punctuation and must not begin or end with stop words  such as  the    of   to    and    he   etc. . candidate phrases are normalized by converting them to lower case and stemming them. kea then uses the naive bayes algorithm  domingos and pazzani  1  to learn to classify the candidate phrases as either keyphrase or nonkeyphrase} 
　in the simplest version of kea  candidate phrases are classified using only two features: tfxidf and distance  frank et al  1; witten et al  1  1 . in the following  this is called the baseline feature set  section 1 . another version of kea adds a third feature  keyphrase frequency  yielding the keyphrase frequency feature set  section 1   frank et al  1 . this paper introduces a new set of features for measuring coherence. when combined with the baseline features  the result is the coherence feature set  section 1 . when combined with the keyphrase frequency feature set  the result is the merged 
feature set  section 1 . 
　after training  given a new input document and a desired number of output phrases  n  kea converts the input document into a set of candidate phrases with associated feature vectors. it uses the naive bayes model to calculate the probability that the candidates belong to the class keyphrase  and then it outputs the n candidates with the highest probabilities. 
1 	baseline feature set 
tfxidf 	 term 	frequency 	times 	inverse 	document 
frequency  is commonly used in information retrieval to assign weights to terms in a document  van rijsbergen  1 . this numerical feature assigns a high value to a phrase that is relatively frequent in the input document  the tf component   yet relatively rare in other documents  the 
1  the following experiments use kea version 1.1  which is available at http://www.nzdl.org/kea/. 
idf component . there are many ways to calculate 
tfxidf; see frank et al.  for a description of their method. the distance feature is  for a given phrase in a given document  the number of words that precede the first occurrence of the phrase  divided by the number of words in the document. the baseline feature set consists of these two features. 
　the tf*idf and distance features are real-valued. kea uses fayyad and irani's  algorithm to discretize the features. this algorithm uses a minimum description length  mdl  technique to partition the features into intervals  such that the entropy of the class is minimized with respect to the intervals and the information required to specify the intervals. 
　the naive bayes algorithm applies bayes' formula to calculate the probability of membership in a class  using the   naive   assumption that the features are statistically independent. suppose that a candidate phrase has the feature vector  t  d   where t is an interval of the discretized tfxidf feature and d is an interval of the discretized distance feature. using bayes' formula and the independence assumption  we can calculate the probability that the candidate phrase is a keyphrase   as follows  frank et al  1 : 
		 1  
the probabilities in this formula are readily estimated by counting frequencies in the training corpus. 
1 	keyphrase frequency feature set 
the keyphrase frequency feature set consists of tfxidf and distance plus a third feature  keyphrase-frequency  frank et al  1 . for a phrase p in a document d with a training corpus c  keyphrase-frequency is the number of times p occurs as an author-assigned keyphrase for all documents in c except d. like the other features  keyphrase-frequency is discretized  fayyad and irani  1 . equation  1  easily expands to include keyphrase-frequency  frank etal  1 . 
　the idea here is that a candidate phrase is more likely to be a keyphrase if other authors have used it as a keyphrase. frank et al  describe keyphrase-frequency as  domain-specific   because intuition suggests it will only work well when the testing corpus is in the same domain  e.g.  computer science  as the training corpus. this intuition is confirmed in the following experiments  section 1 . 
1 	coherence feature set 
1 	information extraction the coherence feature set is calculated using a two-pass method. the first pass processes the candidate phrases using the baseline feature set. the second pass uses the top k most probable phrases  according to the probability estimates from the first pass  as a standard for evaluating the top l most probable phrases  k  l . in the second pass  for each of the top l candidates  including the top k candidates   new features are calculated based on the statistical association between the given candidate phrase and the top k phrases. the hypothesis is that candidates that are semantically related to one or more of the top k phrases will tend to be more coherent  higher quality keyphrases. 
　the coherence feature set consists of 1k features. the experiments use k = 1 and l = 1  thus there are twelve features. the first four features are based on the first pass with the baseline model. the first two features are tf*idf and distance  copied exactly from the first pass. the third feature is baselinejrank  the rank of the given candidate phrase in the list of the top l phrases from the first pass  where the list is sorted in order of estimated probability. the fourth feature  baseline probability  is the estimated 
probability  p key t  d   according to the baseline model. 
　the remaining eight features are based on statistical association  calculated using pointwise mutual information  pmi  and a web search engine  turney  1 . the following experiments use the altavista search engine. let hits q uery  be the number of hits  matching web pages  returned by altavista when given a query  query. features five to eight are rank low rank low1 rank low1  and rank low1. for i= 1 to 1 and j = 1 to 1  rank low is the normalized rank of the candidate phrase  phrase   when sorted by the following score: 
		 1  
for the query to altavista  phrase - is converted to lower case  lowi  but it is not stemmed. the rank of score low  phrasse  is normalized by converting it to a percentile. the ranking and normalization are performed for each document individually  rather than the corpus as a whole. the query  low  near low/' causes altavista to restrict the search to web pages in which low  appears within ten words of lowj in either order. equation  1  can be derived from the formula for pmi  turney  1 . the intent is that  with rankjowi for example  if a candidate phrase  phrase   is strongly semantically connected with phrase1  then phrase  will likely receive a high score from equation  1   and hence rankjow1 will tend to be relatively high. 
   features nine to twelve are rank cap   rank cap1  rank cap1  and rank cap1. for i= 1 to 1 and j = 1 to 1  rank capj is the normalized rank of the candidate phrase  phrase   when sorted by the following score: 
for the query to altavista  phrase  is converted to cap   in which the first character of each word in phrase  is capitalized  but the other characters are in lower case  and the words are not stemmed. the query  cap  and low  is intended to find web pages in which phrase  appears in a title or heading  where it would likely be capitalized  and 
phrasej appears anywhere in the page  where it would likely be in lower case . the rank of score capj phrase   is normalized by converting it to a percentile. the plan is that rank cap  for example  will be relatively high when the appearance of phrase  in a heading makes it likely that p h r a s e 1 will appear in the body. that is  phrase  would be a 
good phrase to put in the title of a document that discusses 
phrase 1. 
1 	merged feature set 
the merged feature set consists of the coherence feature set plus keyphrase-frequency. as with the coherence feature set  there is a two-pass process  but the first pass now uses the three features of the keyphrase frequency feature set  instead of the baseline features. the second pass now yields thirteen features  the twelve features of the coherence feature set plus keyphrase-frequency. the third and fourth features of the coherence feature set  baselinejrank and baseline  probability  become key freq rank and key freqprobability in the merged feature set. 
1 	experiments 
the first experiment examines the behaviour of the four feature sets when trained and tested on the same domain. the second experiment trains on one domain  computer science  and tests on another  physics . 
1 	intradomain evaluation 
this experiment compares the four feature sets using the setup of frank et al . the corpus is composed of computer science papers from the computer science technical reports  cstr  collection in new zealand. the training set consists of 1 documents and the testing set consists of 1 documents. the keyphrase-frequency feature is trained on a separate training set of 1 documents. all other features  including the coherence features  are trained using only the 1 training documents. the naive bayes algorithm allows the features to be trained separately  since the features are assumed to be statistically independent. the experiments of frank et al.  show that the keyphrase-

frequency feature benefits from more training than the baseline features. figure 1 plots the performance on the testing corpus. 
information extraction 	1 


figure 1. performance on the cstr corpus for the four different feature sets. an output kcyphrase is considered  correct  when it equals an author-assigned keyphrase  after both are stemmed and converted to lower case. 
　the paired t-test was used to evaluate the statistical significance of the results  feelders and verkooijen  1 . there are six possible ways to pair the four curves in figure 1. the differences between all six pairs are statistically significant with 1% confidence when five or more keyphrases arc output. the merged feature set is superior to the keyphrase frequency feature set with 1% confidence when any number of keyphrases are output. 
1 	interdomain evaluation 
this experiment evaluates how well the learned models generalize from one domain to another. the training domain is the cstr corpus  using exactly the same training setup as in the first experiment. the testing domain consists of 1 physics papers from the arxiv repository at the los alamos national laboratory  lanl . figure 1 plots the performance on the testing corpus. 
　the paired t-test shows no significant differences between the keyphrase frequency feature set and the merged feature set. the coherence feature set is superior to the baseline feature set with 1% confidence when four or more keyphrases are output. the baseline feature set is superior to the keyphrase frequency and merged feature sets with 1% confidence when any number of keyphrases are output. 

figure 1. performance on the lanl corpus for the four different feature sets. note that the curves for the keyphrase frequency and merged features sets are intermingled  making it appear that there are only three curves  instead of four. 
1 	discussion 
both experiments show that the coherence features are a significant improvement over the baseline features alone. more of the output keyphrases match with the authors' keyphrases  which is evidence that their quality has improved  jones and paynter  1  1 . the second experiment confirms the  previously untested  hypothesis of frank et al.   that the keyphrase-frequency feature is domain-specific. this feature actually decreases performance below the baseline in the interdomain evaluation. however  the new coherence features are not domain-specific; they improve over the baseline even when the testing domain is different from the training domain. furthermore  the coherence features can work synergistically with the keyphrase-frequency feature  when the training and testing domains correspond. when they do not correspond  at least the merged feature set is no worse than the keyphrase frequency feature set. 
1 	limitations and future work 
the main limitation of the new coherence feature set is the time required to calculate the features. it takes ten queries  1k  k = 1  to altavista to calculate one coherence 

feature vector. each query takes about one second. at 1 queries per feature vector times 1 second per query times 1 feature vectors per document  we have 1 seconds 
1 	information extraction 

  1jql  k = 1  l - 1; roughly 1 minutes  per document. the time required by the other aspects of kea is completely insignificant compared to this. future work might investigate the use of other  less time-consuming  measures of coherence  such as morris and hirst's  measure. however  if we extrapolate current trends in hardware  then it seems that it will not be long before this is no longer an issue  since a local version of altavista will easily run on a desktop computer. 
1 	conclusion 
this paper provides evidence that statistical word association can be used to improve the coherence of keyphrase extraction  resulting in higher quality keyphrases  measured by the degree of overlap with the authors1 keyphrases. furthermore  the new coherence features are not domain-specific. 
acknowledgements 
thanks to eibe frank  gordon paynter  and the 1jcai reviewers for their helpful comments. 
