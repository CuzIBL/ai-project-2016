
in this paper we consider an extension of the multiarmed bandit problem. in this generalized setting  the decision maker receives some side information  performs an action chosen from a finite set and then receives a reward. unlike in the standard bandit settings  performing an action takes a random period of time. the environmentis assumed to be stationary  stochastic and memoryless. the goal is to maximize the average reward received in one unit time  that is  to maximize the average rate of return. we consider the on-line learning problem where the decision maker initially does not know anything about the environment but must learn about it by trial and error. we propose an  upper confidence bound -style algorithm that exploits the structure of the problem. we show that the regret of this algorithm relative to the optimal algorithm that has perfect knowledge about the problem grows at the optimal logarithmic rate in the number of decisions and scales polynomially with the parameters of the problem.
1 introduction
multi-armed bandit problems find applications in various fields  such as statistics  control  learning theory or economics. they became popular with the seminal paper by robbins  and since then they enjoy perpetual popularity.
모the version of the bandit problem we consider here is motivated by the following example: imagine that a sequence of tasks arrive for processing in a computer center that has a single supercomputer. for each of the tasks a number of alternative algorithms can be applied to. some information about the tasks is available that can be used to predict which of the algorithms to try. the processing time depends on the task at hand and also on the algorithm selected and may take continuous values  hence the time instants when the decisions can take place take continuous values  too. the supercomputer has a fixed cost of running  whilst the centre's income is based on the quality of solutions delivered. at any given time only a single task can be executed on the supercomputer. admittedly  this assumption looks absurd at the first sight in the context of our example  however  we think that our results can be extended to the more general case when the number of algorithms that can run simultaneously is bounded by a constant without much trouble. hence we decided to stick to this simplifying assumption.
모an allocation rule decides  based on the side information available about the task just received  which algorithm to use for processing it  the goal being to maximize the return rate. note that this criterion is different from maximizing the total reward. in fact  since processing a task takes some time during which no other tasks can be processed  the rate maximization problem cannot be solved by selecting the algorithm with the highest expected payoff: some tasks may look so difficult to solve that the best thing could be to drop them  which results in no payoff  but in exchange the learner does not suffer any loss due to not processing other  possibly more rewarding tasks.  note that this would not be possible without the presence of side information; in the latter case the problem would simplify to the usual multi-armed bandit problem where one needs to find the best option with highest reward rate.  this example illustrates that a learner whose aim is to quickly learn a good allocation strategy for rate maximization must solve two problems simultaneously: predicting the long-term values of the available algorithms given the information about the task to be processed and balancing exploration and exploitation so that the loss due to selecting inferior options  i.e.  the regret  is kept at minimum. the problem we consider can be thought of as a minimalistic example where the learner faces these two problems simultaneously.
모bandit problems in continuous time have been studied earlier by a number of authors  see e.g.  kaspi and mandelbaum  1; karoui and karatzas  1  and the references therein . these earlier results concern the construction of optimal allocation policies  typically in the form of gittins indexes  given some parametric form of the distributions of the random variables involved. in contrast  here we consider the agnostic case when no particular parametric form is assumed  but the environment is supposed to be stationary and stochastic. the agnostic  or non-parametric  case has been studied extensively in the discrete time case. in fact  this problem was first considered by robbins   who introduced a certainty-equivalence rule with forcing. in the same article robbins showed that this rule is asymptotically consistent in the sense that the frequency of the time instants when the best arm is selected convergesto one almost surely. more recently  agrawal  suggested a number of simple sample-mean based policies and showed that the resulting policies' regret after n decisions is o logn . since it is known that no allocation rule can achieve regret lower than cp logn for an appropriate  problem dependent  constant cp  lai and robbins  1   agrawals' policies are unimprovable apart from constant factors. lately  auer et al.  strengthened these results by suggesting policies that achieve logarithmic regret uniformly over time  rather than just asymptotically. an added benefit of their policies is that they are simple to implement.
모we base our algorithm on algorithm ucb1 from  auer et al.  1   see also  agrawal  1  . we assume a stationary memoryless stochastic environment  where the side information is an i.i.d. process taking values in a finite set  the payoffdelay sequences are jointly distributed for any of the options and their distribution depends on the side information  the precise assumptions will be listed in the next section . like ucb1  our algorithm decides which option to choose based on sample-means corrected by upper confidence bounds. in our case  however  separate statistics are kept for all option side-information pairs. our main result shows that the resulting policy achieves logarithmic regret uniformly in time and hence it is also unimprovable  apart from constant factors.
모the paper is organized as follows: we define the problem and the proposed algorithm in section 1. our main result  a logarithmic regret bound on the algorithm's performance is presented in section 1. conclusions are drawn in section 1.
1 the algorithm
the problem of the previous section is formalized as follows: let k denote the number of options available  and let x denote the set of possible values of the side information  which is assumed to be finite. let x1 x1 ... xt be a random sequence of covariates representing the side information available at the time of the t-th decision  generated independently from a distribution p supported on x. at each decision point the decision maker may select an option it from a = {1 ... k} and receives reward rt = rit t xt   where rit xt  is the reward the decision maker would have received had it chosen option i. unlike in classical bandit problems the collection of the reward takes some random time. when option i is selected and the side informationequals x  this time is 붻it x . we assume that for any fixed x and i   rit x  붻it x   is an i.i.d. sequence  independent of {xt}. we further assume that rit x  뫍  rmin rmax   붻it x  뫍  붻min 붻max  with 붻min   1.  we expect that the boundedness assumptions can be relaxed to 붻it x  뫟 1 and appropriate moment conditions on 붻it x  and rit x .  let
ri x  = e ri1 x  
and 붻i x  = e 붻i1 x   denote the expected reward and delay  respectively  when option i is chosen at the presence of the side-information x.
모the exact protocol of decision making is as follows: decision making happens in discrete trials. let 뷉1 = 1 and let 뷉t denote the time of the beginning of the t-th trial. at the beginning of the tth trial the decision maker receives the side information xt. based on the value of xt and all information received by the decision maker at prior trials  the decision maker must select an option it. upon executing it  the decision maker receives a reward rt = rit t xt  and suffers a delay 붻t = 붻it t xt . that is  the next time point available when the decision maker can select an option is 뷉t+1 = 뷉t + 붻it t xt .
모the goal of the decision maker is to find a good allocation policy. formally  an allocation policy maps possible histories to some index in the set a. the gain  average reward rate  delivered by an allocation policy u is given by
 
where {rtu} is the reward sequence and {붻tu} is the delay sequence experienced when policy u is used. an optimal allocation policy is one that maximizes this gain. note that the problem as stated is a special case of semi-markov decision problems  puterman  1 . the theory of semi-markov decision problems furnishes us with the necessary tools to characterize optimal allocation policies: let us define the optimal gain by 뷂  = sup뷂u.
u
a policy u is said to be optimal if it satisfies 뷂  = 뷂u. it follows from the generic theory that there exist deterministic stationary policies that are optimal. an optimal action for some x 뫍 x can be determined by ordering the options by their relative values. the relative value of option i upon observing x is the expected reward that can be collected minus the expected reward that is not gained during the time it takes to collect the reward:
qi  x  = ri x    붻i x 뷂 .
intuitively it should be clear that a policy that always selects options with best relative values should optimize the overall gain. in fact  it follows from the theory of semi-markov decision problems that this is indeed the case. a stationary deterministic policy u : x 뫸 a is optimal if and only if it obeys the constraints ru x  x    붻u x  x 뷂  = max ri x    붻i x 뷂    1 
i뫍a
simultaneously for all x 뫍 x.
모the  total  regret of an allocation policy is defined as the loss suffered due to not selecting an optimal option in each time step. since we are interested in the expected regret only  our regret definition uses the optimal gain 뷂 :
.
the value of the first term is the maximum reward that could be collected during the time of the first n decisions. the expected regret thus compares the expected value of the latter with the expected value of the actual total payoffs received. it follows that an allocation policy that minimizes the regret will optimize the rate of return.
모when 붻it x  = 1  and x has a single element  the problem reduces to the classical stochastic bandit problem. since for the stochastic bandit problems the regret is lower bounded by o logn   we are seeking policies whose regret grows at most at a logarithmic rate.
모the idea underlying our algorithm is to develop upper estimates of the values qi  x  with appropriate confidence bounds. just like in  auer et al.  1   the upper confidence estimates are selected to ensure that for any given x with p x    1 all options are ultimately selected infinitely often  but at the same time suboptimal options are selected increasingly rarely.
모the algorithm is as follows: let us consider the t-th decision. if we had a good estimate  then for any given x we could base our decision on the estimates of the relative values qi  x  of the options given by. here rit x  denotes the average of rewards during the first n decisions for those time points when the side information is x and option i was selected  and is defined analogously:
 
where ti x t  denotes the number of times option i was selected when side information x was present in trials
.
모the plan is to combine appropriate upper bounds on ri x  and lower bounds on 붻i x  based on the respective sample averages  to obtain an upper estimate of qi  x . however  in order to have a sample based estimate  we also need an appropriate lower estimate of 뷂 . this estimate is defined as follows:
모let u denote the set of stationary policies: u = {u|u : x 뫸 a}. pick any u 뫍 u. let denote the empirical estimate of the gain of policy u:

and let tu t  denote the number of times when an option 'compatible' with policy u was selected:
.
then  the estimate of 뷂  is defined by
.
here ct s is an appropriate deterministic sequence that is selected such that simultaneously for all policies is in the ct tu t -vicinity of 뷂u with high probability. this sequence will be explicitly constructed during the proof where we will also make sure that it depends on known quantities only. in words is the optimal gain that the decision maker can guarantee itself with high probability given the data seen so far.
모our proposed allocation policy  {uucbt }  selects the options it = uucbt  xt  by the rule
 
where  similarly to ct s  c t s is an appropriate deterministic sequence that will be chosen later.
1 main result
our main result is the following bound on the expected regret: theorem 1 let the assumptions of the previous section hold on rit 붻it xt. let rn be the n-step regret of policy uucb. then  for all n 뫟 1 
where l  = 붻max뷂    rmin 
붟i x  = maxqj  x    qi  x  뫟 1 	i = 1 ... k  j뫍a
and the positive constant a is given by  1  in the proof of the theorem.
the proof follows similar lines to that of theorem 1 of  auer et al.  1   with the main difference being that now we have to handle the estimation error of 뷂 . we prove the theorem using a series of propositions.
모the first proposition bounds the expected regret in terms of the number of times when some suboptimal option is chosen: proposition 1 the following bound holds for the expected regret of an arbitrary policy  u =  u1 u1 ... :

u
where  	   
 x  = {i 뫍 a|qi  x  = maxqj x }
j뫍a
denotes the set of optimal options at x  and
l  x  = max 붻j x 뷂    rj x  
j
is the loss for the worst choice at x. further  by l  x  뫞 l  
.
proof. let us consider the t-th term  e 붻t뷂    rt   of the expected regret. we have e 붻t뷂    rt  = and that
ut depends only on the past  i.e.  if ft is the sigma algebra of x1 r1 붻1 ... xt rt 붻t then it = i is ft 1 measurable  we get that

now  using again that ut does not depend on xt  we get

then
.
let wt i|x  = e i ut x  = i   if i 뫍 u  x  and wt i|x  = 1 otherwise  and let
뷃t i|x  뫟 wt i|x   since   the first term of the last expression can be upper bounded by
.
since 뷃t i|x  = 1 if i is not optimal  뷃t defines an optimal
 stochastic  policy and hence  bellman's equation gives 1. therefore 

summing up this last expression over t gives the advertised bound.	
모the next statements are used to prove that with high probability is a good estimate of 뷂 . here and in what follows u  denotes an arbitrary  fixed  optimal policy and.
proposition 1 assume that the following conditions are satisfied:
		 1 
	.	 1 
where the first condition is meant to hold for all stationary policies u 뫍 u. then
	.	 1 

proof. let u be the policy that maximizes 뷂ut   ct tu t . since  1  holds for u  we get that
  proving the upper bound for 뷂t. on the other hand 
                       because of the choice ofwhich can be
further lower bounded by 뷂    1ct tu  t  using  1   proving the lower bound for.	
모the following proposition shows thatis indeed a lower bound for 뷂  with high probability. proposition 1 let

where
.
then
.
proof. according to proposition 1  if  1  holds for all stationary policies u and if  1  holds then .
hence  in order 뷂t   뷂    1ct tu  t  or 뷂t   뷂  to hold  we must have that one of the conditions in proposition 1 is violated. using a union bound we get
 .
fix u. by the law of total probability 
.
define
.
using elementary algebra  we get that
.
exploiting that r tu and 붻 tu are martingale sequences and resorting to a slight variant of the hoeffding-azuma bound  see  e.g.  devroye et al.  1    we get the bound 1/ |u| + 1 t 1. summing over s and u and by an analogous argument for  we get the desired bound. 	
모now we are ready to prove the main theorem. in the proof we put a superscript ' ' to any quantity that refers to the optimal policy u . for example   
  t  x t  = tu  x  x t   etc.
proof of theorem 1. proposition 1 applied to uucb shows that it suffices if for any fixed x 뫍 x and suboptimal choice  we derive an o logn  upper bound on the expected number of times choice i would be selected by uucb when the side information is x. that is  we need to show

모let qit x  = rit x   . using the definition of uucbt   if uucbt  x  = i holds then qit x  + c t ti x t    qt  x  + c t t  x t . hence  for any integer a n x  
.
we write the t-th term in the last sum as follows:
where
 
fix any. using the definition of qit x  
.
the expectations of the second two terms will be bounded by proposition 1. the first term  multiplied by is bounded by
.
when this expression equals one then at least one of the following events hold:
 
here	. now let us give the choices
for the confidence intervals. define

we have already defined cts in proposition 1:  where c1 was defined there  too. we define c ts implicitly  through a definition of which is defined so as to keep the probability of small: let
 
. define
	a =  a1 + a1 	 1 
and c ts =  a1 +a1 uts. using these definitions we bound the probabilities of the above three events. we start with:
 here	we	used	that	   are martingales for any x i 
and the above-mentioned variant of the hoeffding-azuma inequality. plugging in the definition of we get that the probability of event is bounded by 1t 1 |u| + 1  1. the probability of  can be bounded in the same way and by the same expression since. therefore
.
모moreover  define. now  if holds then one must have   where. the above choice makes s large enough so that  cannot hold. hence .
gathering all the terms  we have

this finishes the proof of  1  and hence  by proposition 1 we get the desired bound   1 .		
1 conclusions and further work
we considered a generalization of the multi-armed bandit problem  where performing an action  or collecting the reward  takes a random amount of time. the goal of the decision maker is to maximize the reward per unit time where in each time step some side information is received before the decision is made. in this setting one needs to consider seriously the time needed to perform an action  since spending long times with less rewarding actions seriously limits the performance of any algorithm in a given time period. therefore  efficient methods must predict simultaneously the expected rewards and durations of all actions  as well as to estimate the long term optimal performance. the latter is essential as each action has a hidden cost associated with it: since actions take time  for their correct evaluation their immediate payoffs must be decremented by the optimal reward lost during the time it takes to execute the action.
모in this paper we proposed an algorithm to solve this problem  whose cumulative reward after performing n actions is only o logn  less than that of the best policy in hindsight. the algorithm is based on the upper confidence bound idea of auer et al. . our algorithm  however  extends their ucb1 algorithm proposed for the multi-armed bandit problem in two ways. first of all  it estimates the long term maximum reward per unit time. for this we proposed to adopt a maximin approach: the estimate was chosen to be the optimal gain that can be guaranteed in the worst-case  with high probability  given all the data seen so far. moreover  utilizing the structure of the problem the algorithm chooses its actions based on the sufficient statistics of the probleminstead of considering each policy separately. note that doing so would lead to a constant factor in the regret bound that grows linearly with the number of possible policies  i.e.  exponentially in the size of the problem. on the other hand  because of the specialized form of our algorithm  the constants in our bound depend only polynomially on these parameters. however  we expect that the explicit dependence of the bound on the number of possible side information values can be relaxed. note however  that we have not attempted any optimization of the actual constants that appear in our bounds. therefore  we expect that our constants can be improved easily.
모one problem with the algorithm as presented is that it needs to enumerate all the policies in order to compute the estimate of the optimal gain. however  we would like to note that the problem of computing this quantity is very similar to computing the value of minimax markov games. in fact  the actual definition of is not that important: any estimate that satisfies the conclusion of proposition 1 would do. we speculate that since efficient methods are available for certain minimax markov games  cf.  szepesva뫣ri and littman  1    game theoretic techniques might yield an algorithm that not only utilizes the available information effectively  but is also computationally efficient.
모in the present work we restricted ourselves to the case when the side information is allowed to take values only in a finite set. assuming appropriate smoothness conditions on the reward and delay functions  it seems possible to extend the algorithm to the case of continuous valued side information. the extension of the algorithm presented seems possible to certain semi-markov models when there is a state that is recurrent under all stationary policies. another interesting avenue for further research is to consider continuous time bandit problems in non-stochastic environments.
