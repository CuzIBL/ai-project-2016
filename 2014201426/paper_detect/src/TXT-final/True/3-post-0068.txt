
like many other application areas  task-based domains that employ digital imagery are faced with the problem of information overload. modeling the relationship between images and the tasks being performed is an important step in addressing this problem. we have developed an interactive approach for the capture and reuse of image context information that leverages a measure of a user's intentions with regard to tasks that they address. we analyze aspects of human-computer interaction information that enables us to infer why image contents are important in a particular context and how specific images have been used to address particular domain goals.
1 introduction
recent advances in digital image capture and storage technologies have increased the problem of information overload in imagery task domains. as a consequence  intelligent application support is needed to help manage imagery tasks. the majority of current retrieval techniques search for images based on similarity of appearance  using low-level features such as color and shape or by natural language textual querying where similarity is determined by comparing words in a query against words in semantic image metadata tags. the biggest problem arising from these techniques is the so-called semantic gap  hollink et al.  1  - the mismatch between the capabilities of the retrieval system and user needs.
　in order to address this problem we have developed a decision support system that can integrate information about underlying visual data with more high-level concepts provided by users as they complete specified tasks. capturing human expertise and proficiency allows us to understand why relevant information was selected and also how it was employed in the context of a specific user task  leake et al.  1 . from a case-based standpoint our research focuses on case knowledge acquisition and case knowledge reuse  where a case is represented by a complete user task including all system interactions in the course of carryingout the task. the approach allows us to capture and reuse best practice techniques by automatically constructing a knowledge base of previous user experiences. this knowledge base can then be employed to improve future context-based query processing. similar tasks are no longer interpreted as simply a collection of natural language terms: rather image retrieval requests take into account the context of the specific user domain goals. this approach allows for a dramatic reduction in both the time and effort required to carry out new tasks as amassed contextual knowledge is reused in support of the similar tasks. furthermore  there are benefits from a knowledge management standpoint as contextual knowledge pertaining to particular tasks may now be stored and reused as an additional resource for support  training and preserving organizational knowledge assets.
1 system overview
our system enables a user to search directly for imagery corresponding to their current task needs. a typical task-based query to our image repository may consist of any combination of specified metadata  semantic task information and a sketched configuration of image-objects. the user may search an image database for imagery corresponding to their current goals or a knowledge base of previous user experiences  called sessions  for other user's tasks that may bear resemblance to their own. in either case the imagery/sessions are ranked according to a percentage matching score. the user may then interact with the returned information as part of their task by annotating and highlighting relevant image aspects.
1 capturing case-based context
our researchdraws on work in capturingtask knowledgewith the ultimate goal of performingmore effective image retrieval using annotations  lieberman and liu  1 . to this end we have developed tools for direct image manipulation to assist the user in organizinginformationabout relevant imagery and their task to capture important contextual information about specific user goals. these insights are then distilled into a form the system may use to perform similarity matching. the tools for direct image manipulation include filters  transformation  highlighting  sketching  post-it type and multimedia annotations. they allow the user to identify regions of interest that can be linked to clarifications and rationale. all con-

figure 1: capturing user context through image annotation
textual knowledge is gathered by the system using implicit analysis so that users are shielded from the burden of knowledge engineering  claypool et al.  1 . we unobtrusively monitor  interpret and respond to user actions as they interact with the imagery via the annotational tools and record this as user context.
　figure 1 shows how a user can make use of the image annotation tools as part of a geo-spatial image task. the particular task that this user had in mind was the construction of an airport on the outskirts of an urban area. the user has uploaded some multimedia and textual annotations; these textual annotations are represented by the icons on the image. the notebook icon replaces the text box where the user has typed comments and prevents the image from becoming cluttered. the camera icon replaces the video uploaded by the user  and when clicked on will play back the recording. the dark rectangles around the icons are the areas associated with these annotations and are emphasized once the icons are moused over. the area emphasized by the rectangle surrounding the notebookicon represents an undevelopedarea outlying a residential part of the urban region. the user comment states that this is an area of low elevation  away from residential areas and drained naturally by a large river  i.e. suitable to proceed with the new development . the user has also highlighted a group of buildings by sketching an oval shape around them  they may have concerns that these buildings are quite close to the area selected to build the airport . once the user has finished interacting with the imagery their entire task process is stored as an encapsulated session case in the knowledge base.
1 reusing similar user context for retrieval
the applicationallows for the reuse of similar user contextfor retrieval. more specifically  if a user working in a similar task domain to a previous user queries the system  the application reuses the previously captured context to compute similarity between the new task and the goals of previous users in order to recommend images and user sessions that may be of help to the current user. a more complete description of how

figure 1: session retrieval
this similarity is calculated is described in  o'sullivan et al.  1 . the current user can compare their task to previous work both to find relevant imagery and also to examine the decisions and rationale involved in addressing the previous task. figure 1 shows the interface for displaying retrieved sessions.
　each row in the interface represents a similar user session and is summarized to include the percentage matching score  the most similar queries  the most important annotations added by the similar user  media buttons to play multimedia annotations and thumbnails of any annotated imagery. the user can reuse these similar sessions in carrying out their task by incorporatingall or part of a relevant session into their own task context. because the task-based retrieval system is tightly coupled with the activities that the user is performing  the system has the capacity to make proactive recommendations in an unobtrusivemanner by monitoring the current task context. information requests  increments in the information accessed and annotations provided are no longer taking place in an isolated environment. rather they can be grounded in the context of the activities that the user is performing so the system can correspondingly anticipate and update what previous knowledge would be relevant  making it available to the user. this knowledge is provided unobtrusively  and need only be accessed when required. thus the process does not distract from the task at hand  yet makes relevant knowledge available just-in-time.
1 experimental evaluation
the system was originally designed for the retrieval of geospatial imagery and an earlier evaluation of session retrieval indicated that sessions were useful as a basis for task-based retrieval  o'sullivan et al.  1 . in this paper we show that task-based annotation context has advantages for individual image retrieval by employing the application as an aid for travel planning. we evaluated the system using images from the corel image dataset  wang and li  1 . we began by analyzing a subset of 1 images  corresponding to 1 countries - australia  england  france  ireland and thailand  and manually annotating each of them with four/five keywords describing image objects and scenes depicted in the images. in order to extract contextual information for each image  the set of keywords for each image was entered as a query to web sites offering information on holidays in the outlined countries  for example http://www.visiteurope.com/france.html.
　the resulting web pages were analyzed and the most relevant paragraphsfrom these pages were applied as annotations to the images. these annotations were between 1 and 1 sentences in length. an example of an annotation for a photograph featuring a part of the english lake district was  the lake district national park in the north-west of england is the largest of englands national parks. its 1 square kilometres cover high fells  lush green dales  still lakes  vibrant villages and quiet hamlets . the purpose of these annotations was to act a baseline for comparison with task-based queries relating to holidays in the outlined countries.
　in order to demonstrate the task-based retrieval capabilities of the system  several tasks were outlined relating to holidays in the countries and user behavior was simulated by annotating relevant imagery with task-specific information. an example of a task description for a holiday in england was  planning a trip to england  spend a few days in london  visit the markets  followed by a few days in the countryside . once the images had been annotated according to this first set of tasks another set of tasks corresponding to more specific types of holidays in the countries were outlined. an example of such a task for planning a holiday in england was  interested in an adventure holiday in the english countryside - i am particularly interested in hill-walking and water sports . the purpose of this was to see if the contextual annotations added to the images for the first set of tasks could be re-used to find imagery relevant to the second set of tasks. the 1 images were then categorized by hand as either relevant or not relevant to each outlined task in the second category of tasks. each task was entered as a query to the system and the returned images were analyzed. figure 1 demonstrates the average scores associated with the top five most relevant images  as judged by a real user  returned for the task-based queries. the left/right columns give the similarity respectively for keyword-based retrieval and annotation-based retrieval. for the most part the annotation-basedretrieval vastly outperforms the keyword-based retrieval  demonstrating that task-based annotations can provide useful additional context that can improve image retrieval over a dataset. we plan to expand the library both in terms of the number of images and in terms of the number of applied task-based queries. this experiment is a starting point for automating the construction of datasets from where information can be retrieved based on current context rather than just on keyword or content-based search.
1 conclusions
we have introduced our approach to developing a context aware system with a task-based focus for retrieving imagery and knowledge. our intention is to use the presented experimental results as a baseline for future evaluation frameworks where we will examine the relative contributions of differ-
keyword retrieval v annotation-based retrieval

figure 1: keyword retrieval v annotation-based retrieval
ent levels of user  task and annotation contexts. we hope to perform a comprehensive user trial in the near future. we plan to extend our implicit knowledge acquisition techniques by extracting information from a greater variety of user actions. we also intend to supplement our primarily text-based retrieval system by including multimedia annotational information for retrieval.
