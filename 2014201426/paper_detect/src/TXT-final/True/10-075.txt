 
　　　　we seek here to determine the exact quantitative dependence of performance of best-first search  i.e.  a* algorithm  on the amount of error in the heuristic function's estimates of distance to the goal. comparative performance measurements for three families of heuristics for the 1-puzzle suggest general conjectures that may also hold for more complex best-first search systems. as an example  the conjectures are applied to the coding pnase of the psi program synthesis system. a new worst case cost analysis of uniform trees reveals an exceedingly simple general formula relating cost to relative error. the analytic model is realistic enough to permit reasonably accurate performance predictions for an 1-puzzle heuristic. the analytic results also sharpen the distinction between  knowledge itself  and the  knowledge engine itself . 
one has the sense that the men who conceived these high buildings  gothic cathedrals  were intoxicated by their new-found command of the force in the stone. how else could they have proposed to build vaults of 1 feet and 1 feet at a time when they could not calculate any of the stresses  
j. bronowski  the ascent of man 
introduction 
　　　　building speech understanding systems or other expert problem solving systems can be likened to medieval cathedral building: it can be done but it is by no means easy to do so. this paper attempts to show how some performance measurement experiments with the 1-puzzle and some mathematics of tree search can hope to ease the burden a-little  as civil engineering has done for cathedral builders. 
　　　　some of the difficulties in building such systems arise from an inability to predict performance a priori  to  calculate the stresses   so to speak. suppose it's design decision time for such a system: which will give better performance  heuristic a or heuristic b  debate is sometimes avoided by using both in a multi-term evaluation function  if this system uses an evaluation function of some sort to guide behavior. but performance depends on how much weight each term is given. the choice of the scalar 
this research was supported by the defense advanced 
research projects agency under contract no. f1-
1-c-1 and monitored by the air force office of scientific research. 
weight values remains largely a matter of educated g n o s e s combined with trial and error   .g.  hayes-roth & lesser 1 . these prediction questions -- which is better  what weighting value  - constitute the focus of the experiments and the analysis reported belcw. 
       it is a certainty that in the future we will understand more of ai more mathematically than today. but if we somehow succeed in developing an exact mathematical model of the hearsay-ii system  hayes-roth & lesser 1   say  complete with formulas to predict performance quantitatively under the most diverse parameter settings  will such a theory say anything at all about the hwim speech understanding system  woods 1   or about the f 1 program synthesis system  barstow 1  barstow & kant 
1   or about a chemistry synthesis program  powers 1   this will be possible only if the theory captures a common denominator of these systems. the four systems mentioned use best-first schedulers to decide what to do next. 
　　　　the a* algorithm  hart et.al. 1  nilsson 1  embodies the idea of a best-first search. in basic terms  you have a finite set of discrete options of what to do next  and each time you choose an action and do it  you get a new set of options: the still unchosen ones plus new ones generated by performing the chosen action. if there is no obvious way to totally order these actions in advance   md remember that some don't exist until you perform others   then one approach is to assign a number to each action as it appears  according to how good you think it is to do that one  independently of any you may have done already. then iteratively choose the action that has the smallest value  smallest is best . the a* algorithm  hart et.al. 1  nilsson 1  operates on this principle  using an arbitrary ordering function p s  to solve problems like the 1-puzzle. the graph traverser and the hpa algorithm are essentially the same as a*  doran & michie 1  pohl 1 . 
　　　　the 1-puzzle  schofield 1  is a one-person game the objective of which is to rearrange a given configuration of 1 tiles on a 1 board into another given configuration by iteratively sliding a tile into the orthogonally adjacent empty location  like so: 

this problem can be modeled exactly as a collection 

of points  configurations  and lines connecting them  moves   i.e.  as a graph. since some problems differ only in that they are defined by different graphs  we define a problem to be a finite  sjnongly connected graph with no self- loops and no multiple edges.  throughout  underlining indicates a formal definition.  actually  the 1-puzzle graph is not strongly connected  but rather consists of two disconnected components   can't get there from here  ; for our purposes we consider one such component. the 1puzzle is an undirected graph since every move has an inverse. 
　　　　the 1-puzzle is to this work as the fruit fly is to the geneticist: simple  convenient to manipulate  yet exhibiting interesting phenomena that hypothetically hold for a broader class of subjects. to approach a predictive theory of heuristic search experimentally  we define search performance functions  e.g.  number of nodes expanded and length of solution path found   measure their values experimentally over a range of realistic parameter values  e.g.  as a function of n  the distance to the goal  and of w  a weighting coefficient in the evaluation function   look for patterns in the data  e.g.  cost grows  sub- exponentially with n under certain conditions   and conjecture certain general relations to hold  e.g.  increasing w changes an exponential cost heuristic into sub-exponential . 
cost  quality and error for 1-puzzle heuristics 
　　　　heuristic search is supposed to be better than breadth-first search  but how much better  do heuristics boat the  exponential explosion  that besets breadth-first search  figure 1 shows the number of nodes expanded as a function of the depth of the goal for three particular heuristics for the 1-puzzle. the qualitative difference between kg and the other two is of particular interest: could this have been predicted a priori  this work differs from previous experiments  doran & michie 1  michie 1  doran 1  michie & ross 1  in: a  volume of data collected  giving statistical significance over a large range of parameter values; b  measures of the error in the heuristics themselves; c  different measures of internal behavior. a few definitions are required to make figure 1 meaningful.  note: what we call k   hart et.al. 1  calllv  
　　　　fach possible choice of initial node s r and goal node s of a problem graph defines a distinct problem instance  $rijipl* hence a graph g having v nodes induces a set u g  of v  problem instances. the minimum distance m  he  ♀i ap.h between any two nodes sj and s: is always defined since the graph is strongly connected  and is denoted 
　　　　this section assumes that the a* evaluation function takes the form f s    r s  + k s   where g s  is the distance of node s from the root node of the search tree. note that k is a function of two nodes of g  i.e.  current and goal   but for simplicity we write k s  instead of k s  s p   when goal node s  is implicit. a k function estirrates the distance in the graph from s to the goal node; informally  k contains the knowledge or information about the graph g available to guide the search. 
　　　　for a given g  k  and  s r   s    the cost of search and the goodness of the solution found can be defined  respectively by: x g. k. s r . s    the number of nodes expanded before search terminates  excluding the goal nodr; and p g  k  s r   s.   the length pi the solution path foundkg. k. sr sg.  -     g   k  sr  s r   / h s r t s c   expresses solution quality as a fraction of  he  minimal len tti cm a   1!llu＜j1 path for an instance. so l   1  with equality iff a minimal length solution is found.  we will conveniently drop arguments from formulas when the argument is known implicitly.  
       we will consider three k functions for the 1-puzzle taken from the literature  doran & michie 1  nilsson 1 . 
kj s  = the number of tiles that occupy a board location in s different from the location occupied by that tile in the goal node 
c 
g' 
k 1  s  - the sum  over all 1 tiles in s  of the minimum number of moves required to move the tile from its location in s to its desired location in s p   assuming that no other tiles were blocking the way. 
k 1  s    k 1  s  + 1 ＊ seq s   where seq s  counts 1 if the non-central squares in s match those in s  up to rotation about the board perimeter  and counts 1 for each tile not followed by the same tile as in the goal node. 
       for comparison purposes we also measure the performance of kq s  * 1  which gives breadth-first search. 
what can be predicted about the performances of 
these three k functions for arbitrary problem instances  little  beyond that l - 1 for kj and k1  by the a* admissibility theorem  hart et.al. 1   since 
kj c.j  s.  s h r.j  s:  for 	all  sjt s.  and similarly for 	k1 . 
regarding the x measure  formal theory  hart et.al. 1  pohl 1  pohl 1  nilsson 1  harris 1  vanderbrug 1  tells us nothing about these particular heuristics for this particular problem  not even that x k1  is always less than x k1 . intuitively  k1 may seem to be better than k1  nilsson 1  p. 1   but is this true always  sometimes  or never  two example problem instances suffice to suggest that it is risky to guess on the basis of limited data  example on left from  doran & michie 1  : 
	x 	   p  	x p  
	1 	1 
	k* 1 	k* 1 
 s   s   = 
 1.so1  1.1  	 1.1  1.1  
　　　　figure 1 shows the result of measuring x  by executing the search  for a set of 1 randomly chosen problem instances of the 1-puzzle  of 1 possible instances . the instances are grouped on the abscissa according to the actual minimum distance h s r   s g   between initial node and goal node. for example  for n * 1 there are random 1 problem instances such that h s r   sg  = 1  and hence 1 measurements of values of x k1  s r   sg . the mean of these 1 experimentally measured values is plotted as xmean k1  1 . in general  the true value of xmean g  k  n  is defined to be the mean number of nodes oxpanded usinr heuristic k over al    s     l in u g  such that h s r   s g      n. the vertical bar measures twice the standard deviation of the sample xmean - a statistical measure of how accurately the experimentally measured value of xmean approximates the true value of xmean. 

p r o b l e m - s o l v i n * - 1 : 	gasching 
1 






problem-solving 

　　　　theoretically  the fact that performance is a function of relative error rather than of absolute error would seem to merit careful thought. what makes relative error more special  the  limits to growth  tabulated in the preceedling section are sobering: a* must be given a very accurate heuristic in order to guarantee good performance  n the worst case. what about average case  
　　　　the symbolic result--a mapping from a lattice of heuristic  error  functions to a lattice of performance functions-- can serve as a definition for the  worst case  performance capability of the a* algorithm. the lattice representing heuristic functions exists independently of a*; it happens to be the domain of a particular function we have called xworst. in words  the  knowledge itself  is distinct from the  knowledge engine itself   for  knowledge  in this special sense. hence a similar xworst function can perhaps be derived  assuming as the  knowledge engine  ordered depth-first search or the b* algorithm  berliner 1  instead of a*. if you give an engine more knowledge  i.e.  less errorful knowledge  then it performs better. but some engines can do more than others  or do it faster  with the knowledge they are given. 
　　　　i greatfully acknowledge many fruitful discussions with herbert simon regarding this work. 
