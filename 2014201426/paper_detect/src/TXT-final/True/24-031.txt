 
we describe arachne  a concept formation system that  uses explicit constraints on tree structure and local restructuring operators to produce well-formed probabilistic concept trees. we also present a quantitative measure of tree quality and compare the system's performance in artificial and natural domains to that of cobweb  a well-known concept formation algorithm. the results suggest that arachne frequently constructs higher-quality trees than cobweb  while still retaining the ability to make accurate predictions. 
1 	background and motivation 
the task of concept formation involves the incremental acquisition of concepts from unlabeled training instances  fisher &; pazzani  in press . much of the recent research on this topic builds on fisher's  1  cobweb. fisher's system assumes that each instance is described as a conjunction of attribute-value pairs  and employs a probabilistic representation for concepts. in particular  cobweb represents each concept ck as a set of attributes 
ai and a subset of their possible values vij. associated with each value is the conditional probability of that value given membership in the concept  p ai = vij ck . in addition  each concept has an associated probability of occurrence  p ck - cobweb organizes its conceptual knowledge into a hierarchy  with nodes partially ordered according to their generality; thus  the root node summarizes all instances that have been observed  terminal nodes correspond to single instances  and intermediate nodes summarize clusters of observations. 
   cobweb integrates the processes of classifying instances and incorporating them into memory. the system sorts each new instance i down the hierarchy  starting at the root  locating nodes that summarize classes into which the instance fits well. at a given node tv  cobweb retrieves all children and considers placing the instance in each child node c in turn; it also considers creating a new child based on the instance. the algorithm uses an evaluation function  category utility  gluck & corter  1   to determine the  best  resulting partition  then incorporates the instance into memory accordingly. the system then recurses  sorting the instance through memory until it produces a disjunct or reaches a terminal node. other research on concept formation *also affiliated with sterling federal systems. 
learning and knowledge acquisition 
 anderson & matessa  in press; hadzikadic & yun  1; lebowitz  1  has assumed a similar control structure. 
   our designs for icarus  langley  thompson  iba  gennari &: allen  in press  - an integrated cognitive architecture - use cobweb as the underlying engine for classification and concept formation. icarus invokes fisher's algorithm to acquire primitive concepts  which in turn serve as background knowledge for the rest of the system. in theory  cobweb produces structures containing concepts that correspond to concepts in the data  at several levels of generality. however  our experience with. cobweb suggests that its ability to form identifiable concepts is limited. because the system's evaluation function is oriented toward maximizing predictive accuracy  the hierarchies it constructs may not reflect the underlying class structure of the domain. this behavior is especially apparent with noisy data and with certain orders of training instances. this has implications for systems that use these concepts as building blocks for other knowledge structures. for example  if a unified cat concept has not been formed  it cannot be used as part of a larger knowledge structure  such as a living room. 
   in this paper we describe arachne  a concept formation system that seeks to construct well-formed concept hierarchies while maintaining high predictive accuracy. arachne's focus on the structural quality of the hierarchies it constructs is new to unsupervised learning. however  van de velde's  1  supervised idl algorithm seeks to induce decision trees with high accuracy and desirable structural properties. although arachne's structural goals are different from those of idl  both systems use structural principles to guide tree formation. 
   the arachne algorithm bears many similarities to cobweb  but employs different criteria for tree formation and uses alternative restructuring operators  which we describe in the next section. we then present experimental studies that compare the behavior of the two systems on both accuracy and tree quality. we close with general observations about the two systems and directions for future work. 
1 	the arachne system 
   like cobweb  a r a c h n e represents knowledge as a hierarchy of probabilistic concepts  and it classifies new instances by sorting them down this hierarchy. the system differs from cobweb in its concern for the structure 

of the concept tree it constructs  in the learning algorithm it employs  and in the way it classifies instances. below we discuss each of these differences in turn. 
1 	constraints on m e m o r y organization 
arachne's main goal is to create well-structured concept trees  but this requires some specification of  desirable  structures. we have chosen to state these as formalized constraints  but we have been careful to focus on local constraints that can be tested efficiently. our hope has been that global properties would tend to emerge from these local concerns  even though we could not guarantee this would occur. the system's approach assumes that one has some similarity metric s that lets one determine the similarity of two instances  an instance and a concept  or two concepts. the constraint framework does not depend on any particular metric. 
   recall that  in our framework  each concept is a probabilistic abstraction of the nodes below it in the hierarchy  and each child is a specialization of its parent. this structure suggests two local constraints on the structure of concept trees  which we believe reflect useful notions of well-formed hierarchies. the first deals with the relation between a child  its parent  and its siblings: 
definition. concept n is horizontally well placed in a concept tree w.r.t. similarity metric s if p is the parent of tv and for all siblings a' of tv  s n  p    s n  k . 
thus  a concept is horizontally well placed if it is of equal or greater similarity to its parent than to any sibling. if a node tv violates this constraint  it suggests that one should consider merging tv with one of its siblings. 
   a second constraint concerns the relative similarity between a child  its parent  and its grandparent: 
definition. concept tv is vertically well placed in a concept tree w.r.t. similarity metric s if p is the parent of tv  g is the parent of p  and s tv  p    1 tv  g . 
thus  a concept is vertically well placed if it is more similar to its parent than to its grandparent. if a node tv violates this constraint  it suggests that one should consider promoting tv to become a child of its grandparent  and thus a sibling of its parent . 
¡¡taken together and applied over all the nodes in a concept tree  these constraints can be used to define a characteristic of the entire tree: 
definition. a concept tree is well organized w.r.t. similarity metric s if all concepts in the tree are horizontally and vertically well placed with respect to 1. 
a r a c h n e seeks to generate concept hierarchies that are well organized for a given similarity metric. we hypothesize that such hierarchies will reflect concepts that are inherent in the domain. 
1 	arachne's control structure 
the a r a c h n e learning algorithm has many similarities to cobweb but also important differences. the system accepts an instance / and a concept node tv as arguments  and incorporates / into the hierarchy below tv. if tv is a terminal node  a r a c h n e  like cobweb  extends the hierarchy downward  creating a new concept p that summarizes tv and /  making tv and / children of the new concept. if tv is not a terminal node  the system averages i into the existing probabilistic description and stores the instance as a new child. at this point  a r a c h n e considers two operators for restructuring the hierarchy  and this is where it most diverges from its predecessor. 
   the system first checks each child c of tv in turn  including the new child /  to make sure it obeys the constraint that it be vertically well placed. if c violates this condition  a r a c h n e promotes c  removing it as a child of tv and making it a child of tv's parent.  actually  the system must recheck each constraint after applying the promote operator  since this changes the description of tv . this ensures that no children of tv are more similar to their grandparent than to their parent. thus  storing a new instance as a child of a concept can cause a sibling instance or concept to  bubble up  to a higher location in memory. 
   arachne's next step involves checking each child of n to make sure it obeys the constraint that it be horizontally well placed. if two or more children are more similar to each other than either is to tv  the system merges the most similar pair. this involves replacing these siblings with a new node that is their probabilistic average  taking the union of their children as its children. arachne then recursively considers merging this new node's children. in some cases  this leads to recreation of the original siblings at a lower level in the hierarchy; in other cases  it produces further reorganizations in the subhierarchy. in particular  if the original instance is merged with an existing concept  recursive calls of the merge operator can effectively sort it down through memory. 
once it has merged two nodes at a given level  
arachne checks the remaining nodes for satisfaction of horizontal well placement. if it finds two or more nodes that violate this constraint  it again merges the most similar  then repeats this process until all nodes at this level satisfy the constraint. in this way  a single new instance can cause the system to merge successively many of the nodes previously stored at a given level  including pairs of nodes dissimilar from it. for instance  suppose a r a c h n e had stored four instances of cats under a common parent  and a dog instance is added  through merging from above . here the system would first merge the two most similar cats  then merge a third into the resulting node  and finally the fourth. the result would be two concepts  one representing the abstraction of four cat instances and the other based on a single dog. this iterative merging process differs from that used in cobweb  which merges nodes only when they are similar to a new instance. thus  we expect arachne will create well-structured trees regardless of the order in which instances are presented. 
1 	similarity and prediction in arachne 
recall that arachne's constraints and control structure rely on the ability to measure the similarity between nodes and/or instances. at each level  the system uses its similarity metric to decide in which class an instance belongs by determining which class descrip-
mckusick and langley 

tion is most similar to that of the instance. a r a c h n e also uses its similarity measure to determine the depth to which it should sort an instance  halting whenever the best similarity score at the next level is no better than that at the current level. the metric also plays a role in deciding when to invoke the merge and promote operators. hadzikadic and yun  1  have also used a similarity metric to guide the concept formation process; both their i n c system and a r a c h n e differ in this way from c o b w e b   which uses an evaluation function over an entire partition of nodes. although arachne can use different similarity functions  our tests with the system have used a simple measure of  probabilistic overlap  between the attributes of nodes and instances. 
   a r a c h n e uses the same similarity function and essentially the same control structure for prediction that it uses in learning. the system sorts an instance down the hierarchy in accordance with its constraints  except that no promotion is allowed and only merges that involve the instance are executed.1 thus an instance sorts to the class at which it would ordinarily become a disjunct  and a prediction is made from the last node to which it sorted. a r a c h n e includes a simple recognition criterion to foster prediction from internal nodes and thus avoid overfitting. as it sorts an instance through memory  the system makes a prediction from an internal node if its modal values perfectly match all the values of the instance. 
1 	comparative studies 
now that we have described arachne  we must still demonstrate that hierarchies constructed according to its constraints have desirable structural properties  and that a r a c h n e is competitive with c o b w e b in terms of predictive ability. in designing the algorithm  we suspected that good structure would  if anything  enhance the latter ability  and sought to show this experimentally. to this end  we designed a set of comparative studies  which we report after summarizing the dependent variables we used to measure the systems' behaviors. 
1 	a c c u r a c y o f class p r e d i c t i o n 
typically  researchers have evaluated inductive learning systems by training them on a set of examples and then measuring their ability to make predictions about new examples. for supervised learning methods  the prediction task involves identifying the class name of a novel instance  given training instances that include this information as part of their description. in contrast  the training data for unsupervised systems like c o b w e b and a r a c h n e does not include class information. thus  
fisher  1  introduced the task of flexible prediction  which requires the system to predict the values of one or more arbitrary attributes that have been excised from the test instances. m a r t i n  1  and gennari  1  have used similar performance measures. 
1
¡¡¡¡ recall that during learning  arachne can merge any two nodes at the current level. it is not limited to considering only merges that involve the instance being sorted. 
learning and knowledge acquisition 
   however  the class name can be used for prediction straightforwardly  even with unsupervised learning methods. a typical unsupervised system does not include the class name in the description of training instances  but there is nothing to prevent one from including such class information  provided the system does not use it to determine concepts. to take advantage of this idea in evaluating systems like arachne and c o b w e b   we associate the class name with each instance description as an extra attribute  hiding the label so that it does not affect clustering. however  we do let the system retain probabilities at each concept for the class labels of instances summarized by the node. to predict the class name of a new instance  the system simply classifies the instance to a node in the hierarchy and predicts the most frequently occurring label at that node. 
   we chose to predict class names in our comparative studies because they provide a good baseline for predictive ability. class names are never provided by the environment; domain experts assign them based on regularities they have observed over time. thus  they are designed to be predictable from observed features. in contrast  fisher's notion of flexible prediction fails to distinguish between attributes that can be predicted trivially  ones that can be predicted with appropriate knowledge  and ones that cannot be predicted at all. 
1 	q u a l i t y o f t r e e s t r u c t u r e 
informal inspections of the trees constructed by a r a c h n e and c o b w e b suggested that the former system was frequently building  better  trees  in that fewer instances were situated in classes where they did not seem to fit well. arachne also seemed to construct fewer  junk  nodes - spurious clusters of instances that have little in common. the challenge was to quantify these observations and to construct a measure of tree quality that could be applied to the hierarchies of both systems. this measure should not favor the guiding organizational constraints of either system. for example  we might have evaluated whether the category utility of the top-level partitioning was optimized  as in fisher's  1  study of tree quality in c o b w e b   or how well the hierarchies adhered to global variants of the constraints set forth for arachne. but these measures would be biased in favor of one of the systems. 
   instead  we devised two dependent measures which we could apply to trees in artificial domains for which we knew the  correct  concept hierarchy. a good hierarchy should contain nodes that correspond to concepts embodied in the data. if one knows that a domain contains well-defined  distinct concepts  then a natural measure of tree quality should reflect the degree to which the learned tree respects the known structure of the data used to build it. in keeping with this idea  we counted the percentage of formed concepts  or those nonterminal nodes whose modal values exactly matched the modal values of a concept known to exist in the data. 
   furthermore  we defined a measure of well-placed instances  singleton nodes that are descendents of a target concept and that match 1% or more of the modal attribute values of the target concept. concepts containing well-placed instances tend to adhere closely to their expected concept description  and show minimal presence of attribute values in frequencies that vary from the expected. a high percentage of well-placed instances in a tree implies a large number of accurate concepts. 
1 	e x p e r i m e n t a l 	p r o c e d u r e 
in our experimental studies  we carried out groups of ten runs 1 presenting a r a c h n e and c o b w e b with identical sets of randomly selected training instances for each run. we used the same test set for every run in a group. in experiments not concerned with order effects  the training instances were randomly ordered. in all cases  test instances were taken from the same distribution as the training instances. thus  if the training data had a noise level of 1%  on average the test data would have that noise level as well. 
   both systems sorted training instances one at a time through their hierarchies. learning took place as each instance was incorporated  as probabilities in the concept descriptions changed and the hierarchies were restructured. after each training instance  the entire test set was presented to each system  which used the hierarchy it had formed thus far to predict the class name of each test instance. we compared this to the  actual  class name associated with the test instance  giving the system a score of one if the prediction was correct and zero otherwise. no learning was done on the test instances  so it was possible to construct learning curves which plot average accuracy on the test set as a function of the number of training instances seen. 
   for our studies we used two versions of c o b w e b that built identical hierarchies but differed in their prediction mechanisms. recall that a r a c h n e has a recognition criterion to foster prediction from internal nodes; it stops sorting an instance if its values perfectly match the modal values of a node it has reached in memory. to control for the effect on predictive accuracy  we created a version of c o b w e b   denoted c o b w e b '   which included this mechanism. since this noticeably affected performance only in the noisy artificial domain  we report c o b w e b ' results only in that section. 
   after each system had processed the complete training set on each run  we ran the final hierarchies through our assessor of tree quality  which reported the number of concepts found and the percentage of well-placed nodes. since we devised these measures especially for our artificial data sets  which had known  clearly-defined concepts  we did not attempt to apply them in the natural domains we tested. 
1 	b e h a v i o r o n n a t u r a l d o m a i n s 
we now consider some hypotheses about the relative behavior of c o b w e b and a r a c h n e   and the experiments we carried out to test them using the dependent measures and procedure described above. our first step in evaluating a r a c h n e was to examine its behavior on some standard problems from the machine learning literature. for this purpose  we selected the domain of con-

figure 1. learning curves for arachne and cobweb on congressional voting records  using predictive accuracy as a performance measure. 
gressional voting records  which fisher  1  has used in tests of c o b w e b   and the domain of soybean diseases  which michalski and chilausky  1  used in their experiments on supervised learning. the prediction in this case is straightforward: 
h y p o t h e s i s : a r a c h n e will show better predictive accuracy than cobweb in natural domains. 
the congressional voting domain contains 1 instances of sixteen boolean attributes each  corresponding to yea or nay votes   with each falling into one of two classes  democrat and republican . in contrast  the soybean data set contains 1 instances from 1 classes  each described in terms of 1 symbolic attributes.1 thus  the two data sets differ in the number of attributes and diverge even more in the number of prespecified classes. 
   to evaluate our hypothesis  we presented both c o b w e b and a r a c h n e with random samples of 1 training instances and a test set of 1 instances from the congressional domain and 1 training instances and a test set of 1 instances  five from each class  from the soybean domain. the results  averaged over ten runs for the first domain and five for the latter  reveal similar accuracies in class prediction throughout the course of learning. on the congressional records  a r a c h n e reaches its asymptote slightly earlier than c o b w e b   at about 1 instances rather than 1 instances. on the soybean data  c o b w e b reaches asymptote earlier  at about 1 instances rather than 1 instances. but in both cases the two asymptotes are basically equivalent and  in general  the differences we had anticipated did not emerge  forcing us to reject our hypothesis. however  we should not conclude too swiftly that a r a c h n e and c o b w e b always perform at comparable levels; these two domains simply may not have characteristics that bring out their differences. this suggests another approach  to which we now turn. 

1
¡¡¡¡the soybean results are one exception to this rule; in this case our averages are based on five runs. 
1
¡¡¡¡this data set is much more complex than the four-class version used by stepp  1  and fisher  1 . 
mckusick and langley 

figure 1. learning curves for arachne and cobweb on an artificial domain with high attribute noise  using predictive accuracy as a performance measure. 
1 	effects of a t t r i b u t e noise 
although studies with natural domains show relevance to real-world problems  artificial domains are more use-
ful for understanding the reasons for an algorithm's behavior. a common use of such domains involves varying the noise level. noise in a training set confounds a learning system; it blurs boundaries between classes  making misclassification of instances more likely and the underlying concepts more difficult to discern. a system trained on noisy data is susceptible to overfitting  predicting attributes at a more specific concept than it should. 
   we designed arachne's performance and learning algorithms to be robust in noisy domains  and this suggests a simple prediction about its behavior: 
hypothesis: arachne will be less affected by noise than cobweb w.r.t. both accuracy and tree structure. 
intuitively  arachne should build better trees because of its more powerful reorganization operators and concern with constraints  and it should be less subject to overfitting because of its ability to predict from wellformed internal nodes. 
   to test this hypothesis  we designed artificial data sets at two levels of noise. each contained instances with four attributes  each of which could take on ten distinct values. the attributes had a prototypical value but could take on other  noise  values at some specified probability. in the low-noise data set  noise level i   the modal value for each attribute occurred with probability 1  while three  noise  values occurred with probability 1. in the noisier data set  noise level ii   the modal value for each attribute occurred with probability 1  while five noise values occurred with probability 1. because a noise value appearing in one class was the modal value of some other class  class descriptions overlapped to some extent. about 1% of the noise level i instances and only about 1% of the noise level ii instances should conform perfectly to the modal class description  with the remainder being noisy variants. 
learning and knowledge acquisition 
   we carried out ten runs at each of these noise levels  presenting c o b w e b and a r a c h n e with 1 training examples in each case. figure 1 shows the learning curves for noise level i i ; similar results were achieved at noise level i. the graph plots the average predictive accuracy against the number of instances seen. at noise level i i   a r a c h n e asymptotes at 1% accuracy  while c o b w e b asymptotes at 1% and c o b w e b '   which predicts from internal nodes  at 1%. in this domain  tree quality was lower for c o b w e b  1 target concepts and 1% wellplaced nodes  than for arachne  1 target concepts and 1% well-placed nodes . accuracy differences were significant at the .1 level for arachne and c o b w e b and at the .1 level for a r a c h n e and c o b w e b ' . differences in tree quality were significant at the .1 level. 
   these results only partly agree with our hypothesis. a r a c h n e ' s asymptotic accuracy is higher than c o b w e b ' s at both noise levels  with the difference increasing with noise. by predicting from well-formed internal nodes  arachne is less susceptible to overfitting. however  the picture is more ambiguous with respect to tree quality. both systems lose tree quality as noise increases  but a r a c h n e suffers less than c o b w e b   presumably because of its restructuring operators. this experiment lends evidence to the view that predictive ability and tree quality are not perfectly correlated. good tree structure does not guarantee good prediction  and good predictive performance does not mean the underlying hierarchy is organized to contain concepts inherent in the data  but both are important factors. 
1 	effects o f p r i m i n g i n n o i s y d o m a i n s 
our explanation of the previous results supposed that noise in early training instances could mislead both systems  but that c o b w e b was more susceptible to this effect than a r a c h n e . if so  we should be able to eliminate this difference by priming both systems with wellordered  noise-free training instances from each class. this is equivalent to giving them background knowledge about idealized categories. this suggests a third prediction about their behavior: 
h y p o t h e s i s : a r a c h n e and c o b w e b will behave similarly on both accuracy and tree structure when primed with well-ordered  noise-free training data. 
the intuition here is that  w i t h less need to reorganize memory to recover from misleading observations  a r a c h n e ' s restructuring operators become less important and differences between the systems should be reduced. to test this prediction  we provided each system with 1 noise-free instances at the start of each run  four identical prototypes from each class  producing an idealized hierarchy. we followed these data with the same noisy training sets used for the second experiment. 
   figure 1 shows the results of this experiment at noiselevel i i   which are somewhat surprising; results at noiselevel i are analogous though less pronounced. priming improves a r a c h n e ' s predictive accuracy significantly  raising it to 1% from 1% without priming. c o b w e b shows an accuracy of only 1%  as did c o b w e b ' . this was an improvement over c o b w e b ' s former level of 1%  


number of instances seen 
figure 1. learning curves for arachne and cobweb on noisy artificial data with primed concept trees. 
but still below a r a c h n e ' s performance. for both systems  priming improves tree quality. a r a c h n e ' s quality  initially higher than c o b w e b ' s   improves from 1% wellplaced nodes without priming to 1%; the 1alters tree quality rises from 1% to 1% with priming. betweensystem differences in predictive accuracy were statistically significant at the .1 level; tree quality differences were significant at the .1 level. 
   this experiment disconfirms our hypothesis. though priming generally improves the predictive accuracy of the systems  a r a c h n e still performs significantly better than c o b w e b . furthermore  with priming a r a c h n e builds better trees at both noise levels. this suggests that a r a c h n e can make better use of the background knowledge encoded in a primed tree. this may be partly due to c o b w e b ' s greater tendency to misplace instances  incorporate them into an inappropriate concept   which affects both the accuracy of the concept description  it obtains superfluous noise  and presumably the system's ability to index and retrieve the object. notably  although priming led both systems to higher accuracy initially  a r a c h n e reached the same asymptote as without priming  whereas c o b w e b ' s accuracy actually dropped as it saw noisy training instances. this provides further evidence that arachne benefits from its ability to predict from well-formed internal nodes  thus avoiding the overfitting to which c o b w e b is susceptible. 
1 	effects o f i n s t a n c e o r d e r 
another one of our concerns in designing arachne was stability w i t h respect to different orders of training instances. order effects are apparent in c o b w e b when different presentations of the same data produce hierarchies with different structure. a r a c h n e ' s operators for merging and promotion should enable recovery from nonrepresentative training orders  and this leads to another prediction: 
h y p o t h e s i s : c o b w e b will suffer more from order effects than arachne w.r.t. tree quality but not accuracy. 
gennari  1  has reported that training order affects the structure of c o b w e b trees but does not alter their table 1. tree quality in artificial domains  measured as per centage of well-placed nodes  for  1  unprimed and  1  primed runs   1  poor and  1  random training orders  at low noise; for  1  unprimed and  1  primed runs  at high noise. 

accuracy  so we did not expect a r a c h n e to outperform its predecessor on the latter measure. however  we did expect it to construct well-structured concept hierarchies regardless of the training order. our experience with c o b w e b suggested it has difficulty when every member of a class is presented at once  followed by every member of a new class  and so forth. thus  we tested the above hypothesis by presenting both systems with ten random orderings and ten   b a d   orderings of 1 training instances from the low-noise data set described earlier. the bad orderings were strictly ordered by class  so the systems saw 1 examples of each class in turn. 
   in this experiment our hypothesis was confirmed. instance order did not affect predictive accuracy  although naturally the learning rate for the bad ordering was slower  since the systems did not see a representative of the final class until the 1st instance. random and bad orderings produce hierarchies capable of analogous predictive accuracy  approximately 1% for arachne and 1% for c o b w e b . however  tree quality differs significantly in the two situations. both systems locate most or all of the concepts at some level  but c o b w e b is vulnerable to misplaced instances with the pathological ordering. whereas a r a c h n e arrives at 1% well-placed nodes with the bad ordering  and a similar 1% with random ordering  c o b w e b averages only 1% well-placed nodes when learning from the bad ordering  compared to 1% for the random ordering. differences in predictive accuracy were significant for both conditions at the .1 level. differences in tree quality between the two systems were not significant for random orderings  but were significant at the .1 level for bad orderings. 
   this experiment provides additional evidence that predictive accuracy is not an adequate measure of tree quality. apparently order effects that lead to a decrease in tree quality do not affect c o b w e b ' s ability to index instances and predict accurately. this is consistent with gennari's results and with our hypothesis. 
1 	c o s t of c l a s s i f i c a t i o n 
analysis of c o b w e b reveals an average-case assimilation cost that is logarithmic in the number of objects in the tree and quadratic in the branching factor  fisher  1 . analysis of a r a c h n e is confounded by the fact that in theory its restructuring of the hierarchy is not guaranteed to halt. such cases are pathological and a r a c h n e ' s behavior on all our data sets was tractable. 
mckusick and langley 

   to quantify each system's efficiency in practice  we measured the number of attribute values inspected as a function of n  the number of objects incorporated  over five runs of the soybean data set  the more challenging of the natural domains we tested. both systems appeared linear in n; c o b w e b with a correlation of 1 and a r a c h n e with one of 1. however  a r a c h n e actually inspected more attributes than c o b w e b   having a linear coefficient thirty times that of its predecessor. we believe that a heuristic cutoff mechanism that  limits reorganization of the hierarchy would reduce cost without loss of predictive accuracy or tree quality. 
1 	discussion 
in this paper we identified some problems with fisher's 
 1  c o b w e b   a concept formation system that achieves high predictive accuracy but does not always create well-structured concept trees. in response  we developed a r a c h n e   an algorithm with explicitly-stated constraints for well-formed probabilistic concept hierarchies  which incrementally constructs such trees from unsupervised training data. we also reported four experiments that compared a r a c h n e with c o b w e b on both predictive accuracy and tree quality. we found the systems achieved comparable accuracy on two natural domains  but we found significant differences in both accuracy and tree quality using artificial data. in particular  c o b w e b tends to overfit more than a r a c h n e when trained and tested on noisy instances  and it benefits less from priming w i t h noise-free data with respect to tree quality. also  the quality of c o b w e b trees suffers from misleading orders of training instances  while arachne's tree structure is relatively unaffected. 
   despite these encouraging results  we need more comparative studies before drawing firm conclusions about one system's superiority over the other. also  a r a c h n e has clear limitations that should be removed in future work. preliminary studies suggest that the current similarity metric has difficulty distinguishing irrelevant attributes from relevant ones. however  since relevance can be estimated from stored conditional probabilities  we are confident that a different similarity function will add this capability. also  the existing system can handle numeric attributes  but only by using euclidean distance between means as its distance metric; future versions of a r a c h n e should employ a probabilistic measure that lets it combine symbolic and numeric data in a unified manner. the system still tends to form  junk  concepts; avoiding them may require additional constraints or more powerful operators for restructuring the concept tree. we should also compare a r a c h n e ' s behavior to other noise-tolerant learning algorithms  including fisher's  1  variant on c o b w e b and supervised methods for pruning decision trees  quinlan  1 . 
   nevertheless  we believe the present work has shown the importance of examining the structural quality of concept hierarchies in addition to their predictive accuracy. it has also shown that explicit constraints on tree structure  combined with restructuring operators for correcting violated constraints  can produce well-formed trees with high predictive accuracy despite noise and 
learning and knowledge acquisition 
misleading orders of training instances. we expect future work in this paradigm will lead to even more robust systems for incremental  unsupervised concept learning. 
acknowledgements 
we thank w. iba  j. allen  k. thompson  d. kulkarni  and w. buntine for discussions that led to many of the ideas in this paper. the above also provided comments on an earlier draft  as did m. d r u m m o n d and l. leedom. j. alien formatted the figures. 
