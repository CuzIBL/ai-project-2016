 
this paper presents a network model of the mental lexicon and its formation. models of word meaning typically postulate a network of nodes with connection strengths  or distances  that reflect semantic similarity  but seldom explain how the network is formed or how it could be represented in the brain. the model presented here is an attempt to address these questions. the network organizes semantically similar words into clusters when exposed to sequentially presented text. lexical co-occurrence information is calculated and used to create a hierarchical semantic representation. the output is similar to semantic networks first described by  collins and loftus  1   but is created automatically. 
1 	i n t r o d u c t i o n 
the mental lexicon refers to the representations that allow word recognition on the basis of auditory and visual stimuli. the lexicon is understood as two linked levels of representation: the first level consists of formbased representations that reflect a word's phonological or graphemie properties. the second level contains semantic representations that reflect its meaning relations with other words  marslen-wilson  1   
　priming studies are an important source of evidence for the semantic organization of the lexicon. when sub-
jects are presented briefly with a letter string  followed by another  and asked to decide whether the latter is a real word  the response time when both strings are related is reliably faster than when they are unrelated. priming effects can be found using stimuli that are graphemically  morphologically  or semantically related   taft  1  for a review . 
    the author is supported by a medical research council studentship. 
1 	neural networks 
　substantial progress has been made modelling the form-based lexical representations in the light of graphemie or phonological similarity  plaut et a/.  1   but there is currently no principled measure of semantic similarity. word meaning is much more difficult to quantify. 
　the network described here is an attempt to address this problem. it is inspired by two relatively independent approaches to semantic representation from cognitive psychology and computational linguistics. after considering each approach i describe the network's implementation and present results. the next section describes the structure and development of semantic representations  and how the model relates to previous work. finally 1 consider the model's psychological relevance with reference to developing categorizations and semantic priming. 
1 	lexical-semantic networks 
a highly influential theory of lexical-semantic representation from cognitive psychology is based on the semantic network. a semantic network consists of a set of nodes and connections of varying strengths  or lengths  between them  collins and loftus  1 . each concept is assigned a node  and connection strengths reflect the amount of conceptual relevance each node has to its partner. the stronger  or shorter  connections represent a high level of similarity. weaker  or longer  connections hold between less related nodes. in a lexical-semantic network  lsn   each node represents a word and the distance between nodes reflects the amount of semantic similarity between each word. the logogen model  morton  1   interactive-activation model and spreading activation accounts  are all types of lsn  neely  1 . 
　lsn accounts explain semantic priming effects in the following way: each node has an activation level. when a stimulus is presented it activates all nodes in the network to some degree. if one node is activated strongly enough its activation will pass a threshold and fire. the stimulus will be recognized as that word. each*time a 

word is presented  activation spreads from the most activated node to nearby nodes  decaying over time. for example  if'doctor' is presented shortly before 'nurse'  the node associated with 'nurse' will reach threshold faster and fire sooner. its resting activation level is raised by activation spreading from 'doctor' during the interstimulus interval. 
1 	data-intensive semantics 
recent work in computational linguistics suggests that large amounts of semantic information can be extracted automatically from large text corpora on the basis of lexical co-occurrence information  lund et al.  1; schiitze  1 . this approach is particularly well suited to neural network implementation  finch  1  because co-occurrence statistics track conditional probabilities  and neural networks have straightforward interpretations as statistical models  bishop  1 . 
　the data-intensive approach to semantics is consistent with  and inspired by theories of meaning that emphasize the importance of use  wittgenstein  1   see also  church and mercer  1  . lexical co-occurrence information reflects a word's distributional profile  which is a reflection of its use. 
　the success of the data-intensive semantics research shows that  with a large enough sample  there is sufficient information in a strictly linguistic environment to recover much semantic structure. it seems plausible  therefore  to investigate the possibility that the brain makes use of such information. the recent discovery that semantic and associative priming effects in the lexical decision task are significantly correlated with co-occurrence statistics  lund et al  1; spence and owens  1  support this possibility. lund et al. constructed a highdimensional space on the basis of lexical co-occurrence counts. words that were close together in the space gave larger priming effects than those further away. 
1 	modelling the lexicon 
lsn theories provide an intuitive way to understand word meaning and its relation to priming. however  there is no theory of how the nodes of a network are formed  or how the distance  or strength  relations between them become organized. 
　the data-intensive approach to semantics is an effective predictor of semantic priming  and reflects an influential approach to understanding word meaning. however  the approach requires an extremely highdimensional co-occurrence space for lexical-semantic representation. it is not obvious how such a space could be represented in the brain. 
　the model presented below is a first attempt at explaining how the semantic level of the lexicon could be organized  consistent with the lsn and data-intensive semantics approaches  in a way that is computationally tractable and biologically reasonable. 
1 	overview of the m o d e l 
the model consists of an input layer that picks out words from a text stream  a dynamic proto-lexicon which records co-occurrences between the present target word and words either side of it  and a self-organizing map. the proto-lexicon is initially empty and the selforganizing map weights are set to random values. 
1 	i m p l e m e n t a t i o n 
proto-lexicon 
the proto-lexicon represents each word in terms of the number of times it has been seen to co-occur directly before and after each other word in the vocabulary. specifically  in an n-word vocabulary each word is associated with the vector ized to unit length  where frequency with which wj has preceded wi  before t  and  denotes the frequency with which wk 
has succeeded wi . thus at each time step  xi represents the model's best guess for the conditional probabilities 
 1  
and 
 1  
for all words j  k and time t. each successive xi is an improved estimation of the true distributional profile of each word. 
　in large-scale applications it is usual to distinguish a fixed subset of high-frequency words to serve as context  see  church and mercer  1  for a review . co-occurrence vectors calculated using high-frequency words are less sparse and provide better samples. this technique complicates the relation between the cooccurrence vectors and quantities  1  and  1   though the results are robust to approximation. the model is presented without approximation. 
self-organizing map 
the self-organizing map is presented with the current proto-lexical representation for each word as it is encountered in the text stream. the winning unit is the unit i* with weight vector wi*such that 
		 1  
for all  output unit weights are updated after each word presentation using a variation of kohonen's self-organizing map algorithm  kohonen  1 : 
		 1  
	lowe 	1 

input 
input consists of words taken sequentially from a 1 word corpus generated by a stochastic context-free grammar described in  elman  1 . a sample section of input is shown below. 
man l i k e boy l i o n eat mouse 
each time a word is recognized in the text stream  its representation in the proto-lexicon is updated and presented to the self-organizing net as an training item. 
1 	results 
after moving through the corpus once  clusters of semantically related words emerge. each word is presented to the network the winning output unit is recorded and labelled. figure 1 shows the winning unit for each word after 1 word presentations. 
　consistent with the lsn approach  the network clusters each word with other words that are used in similar contexts. similar words tend to be nearer to one another than to dissimilar words. verbs have been represented together on the right side: psychological verbs 'see' and 'smell' are represented together  as are destructive verbs 'smash' and 'break' within the main verb group. on the left side  categorial similarity among the nouns is equally well preserved - human and animal nouns group separately  adjacent to one another. 
   however  figure 1 does not reflect the full extent of the net's categorization. 'man' is equidistant from 'boy' and from 'book'  but is related much more closely to one than the other. this fact is represented by the network  not in the pattern of winning units  but by the pattern of activation across all output units. figures 1 and 1 show activation plots for 'man'  and for 'boy' and 'book' with the unit specialized for 'man' marked. 'boy' is associated much more strongly with 'man'  than with 'book' because 'boy' is the highest unit on a plateau containing all the human nouns  whereas 'book' is in a separate region shared by inanimate nouns. 
　figures 1 and 1 also show how the network creates a hierarchical semantic representation: 'see' and 'smell' 

dragon 
lion 
woman 
mouse monster 
man 
girl 
cat rock 
car 
book 
boy 
dog 	like 	move 
chase 
think exist 
sleep 
plate 
glass 
bread 
cookie sandwich see 
smell 
smash 
break 
eat figure 1: output map after 1 word presentations. 

figure 1: activation plot for the word 'man'. 


1 	neural networks 

figure 1: activation plot for the word 'boy' with 'man' marked. 


figure 1: activation plot for the word 'book' with 'man' marked. 

figure 1: 	activation plot for the psychological verb 
'smell'. 

figure 1: activation plot for the psychological verb 'see'. share an activated region but this region is a section of the larger region that covers all verbs. 
1 structure and development of semantic representation 
structure 
although figure 1 resembles a classical semantic net  without connections   unlike many other lsn models  it uses a distributed coding scheme; the activation levels of all units are used to express semantic relationships among words. distributed coding is both more reliable and more biologically realistic than classical localist coding. it is more reliable because representations need not be compromised by the loss of single units. in topographic maps  if a unit specialized to some input pattern is lost  one of its neighbours will become the winner for that pattern on subsequent tests  but the shape of the output map will remain largely intact due to graded transitions in feature-specificity across neighbouring units. graded transitions between cell response profiles due to topographic organization have been reported in many brain regions  knudsen et a/.  1 . 
　in the model graded transitions between winning units also allow uncommitted units to capture new words with distributions similar to more than one word in the initial vocabulary. 
　the data-intensive approach to semantics explains semantic similarity in terms of points in a high-dimensional space. by the end of training  the proto-lexical representations define such a space. here 1 words create a 1-dimensional space. in order to form an output representation of the type required by the lsn approach  and to explain how such a space could be represented in the more limited dimensions of the brain  this space must be reduced to a more manageable size. two properties of the self-organizing map algorithm make it especially well-suited to this task: 
1. the self-organizing map algorithm creates a nonlinear projection from a collection of data points in a high dimensional space defined by each input vector to a one or two-dimensional grid of output units. the projection attempts to preserve the topology of the input space in the lower dimensional output space. thus the algorithm performs precisely the data-reduction necessary. 
1. the algorithm has a straightforward physiological interpretation: it models the development of feature selectivity due to lateral inhibition among cortical nerve cells  sirosh and miikkulainen  1; kohonen  1  
　it is possible to pinpoint cell groups relevant to naming and semantic memory tasks using electrode mapping techniques during neurosurgery  ojemann  1; 
	lowe 	1 
1 . in a review of language localization studies   bradshaw and mattingley  1  conclude that lexical representation probably depends on many areas distributed across the cerebral cortex. this is consistent with the structure of the model. 
development 
during the course of training  output units become increasingly sensitive to a particular input pattern. the weight update equation ensures that the weight vector of the winning unit and its neighbours are moved toward the input pattern. however the input patterns are also moving. representations in the proto-lexicon alter each time a new word is read. this means that different words get different amounts of training devoted to them. during development  the region over which updates occur is shrinking. this entails that input patterns that are far from one another in the input space are separated first on the output map  and patterns that are very close to each other are separated later  kohonen  1 . if the algorithm is interrupted before completion  the final output representation provides a usable partial categorization of the input data  see also section 1 . 
1 	related work 
the self-organizing map algorithm has been used successfully to model semantic clustering  miikkulainen  1   but miikkulainen's networks require a static set of input patterns to be constructed by a separate extendedbackpropagation system with theta-role assignments  before clustering can begin. in contrast  this system operates with raw text  without staged processing  and without supervised components.  ritter and kohonen  1  have also used self-organizing maps  but used heavily pre-processed input data  and a much smaller corpus. 
   elman's simple recurrent networks  elman  1  also make use of of co-occurrence statistics  represented in the hidden node activations. however  in the simple recurrent network  hidden unit response profiles must be manually extracted and submitted to cluster analysis before semantic similarity information becomes directly available: in the model presented here co-occurrence information is explicit. 
1 	psychological relevance 
the model presented here illuminates two psychological phenomena: learning to distinguish between semantically similar words  and semantic priming effects. 
discrimination 
the shape of the output map reflects the network's increasing ability to distinguish words that are semantically distinct. the semantic difference between 'mouse' and 'sandwich' is greater than between 'mouse' and 'cat'. 
1 	neural networks 
the developmental schedule of the network reflects this  by learning to distinguish 'mouse' from 'sandwich' be-
fore distinguishing it from 'cat'. while 'cat' remains undistinguished the two words are essentially the same to the system. the discriminative capacities of the system also depend upon exposure; high frequency words are distinguished sooner in general. these observations are consistent with research into child language acquisition  suggesting that broad semantic distinctions between frequent items are discovered first  harris  1 .  finch and chater  1  have shown how partial categorizations may be used to bootstrap more complex representational structure during language development. 
semantic priming 
the network described above is a type of lsn. consequently it is possible to formulate an account of semantic priming within this framework. with the current architecture each word that is encountered activates the output layer independently  via its proto-lexical representation. to allow an explanation of priming it is necessary to relax this restriction and assume that word recognition advances all unit activations to the levels particular to the recognized word in small amounts over a brief time period  and that unit activations then decay over time until the activation surface is flat. 
　the explanation of semantic priming is then straightforward: if a prime word is presented before activation levels have fully decayed  then residual activation will still be present in some units. for example  let 'man' be the target word  and let the related word 'boy' and the unrelated 'book' be primes. 'man'  fig. 1  will take longer to be recognized as a word when preceded by 'book'  fig. 1  than when it is preceded by 'boy'  fig. 1  because the activation surface for 'man' is almost identical to that of 'boy'  but quite different from the activation surface for 'book'; fewer increments are necessary to convert the activation surface for 'boy' into the surface for 'man' than to convert the surface for 'book' into the surface for 'man'. 
　this account of priming is similar to other lsn models. priming effects depend on the distance between the prime and target words because the the map is organized such that activation tends to drop off with distance from the winning node. 
1 	conclusion 
the network presented here models the formation and arrangement of the semantic level of the mental lexicon. it is consistent with the lexical-semantic network approaches to lexical arrangement and semantic priming  and with the data-intensive approach to semantics. the network represents semantically similar words together using lexical co-occurrence information that is calculated as the network moves through a text corpus. the network uses a topographic mapping technique that is widespread in the brain  and provides a biologically reasonable account of mental lexicon. 
acknowledgments 
i would like to thank my supervisors richard shillcock and mark ellison  and joanna bryson for encouragement and much useful discussion. 
