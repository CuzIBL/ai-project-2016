dependency relationship based decision combination in multiple classifier systems* 
	h e e - j o o n g k a n g ' a n d j i n h 	k i m 
computer science department and center for artificial intelligence research 
korea advanced institute of science and technology 
1 kusong-dong yusong-gu taejon 1  korea 

a b s t r a c t 
although many decision combination methods have been proposed  most of them did not focus on dependency relationship among classifiers id combining multiple decisions that makes classification performance of combining multiple decisions be degraded and biased  in case of adding highly dependent inferior classifiers to overcome such weaknesses and obtain robust classification performance  the present study used dependency relationship for better combining multiple decisions in order to identify dependency relationship by observing outputs of multiple classifiers  two methods are used on the basis of first-order dependency relationship one is to use the concept of mutual information  and the other one is to use the concept of statistically measured association the first-order dependencies identified are used to combine multiple decisions  using bayesian formalism a number of multiple classifier systems are applied to totally uncontrained on-line handwritten numerals and the english alphabet recognition the experimental results show thai the classification performance of a multiple classifier system is superior to that of individual classifiers also  they show that considering the dependency relationship outperforms others in accuracy  when the highly dependent inferior classifiers are added 
1 	introduction 
generally  two directions have been developed in the area of character and pattern recognition for improving classification performance one is to improve the classification performance of a classifier itself the other one is to improve a multiple classifier system which consists of a set of classifiers and a decision combination method although a number of classifiers are available  none of them is as good as expected the major difficulty comes 
    this work ia partly supported by the notepad consortium and the offline consortium 
   'to whom all the correspondence should be addressed he also works for samsung electronics  co 
1 	learning from the fact that although a number of features in diversified forms are available  but it is not easy to lump them together into a single classifier the multiple classifier system is motivated from the assumption that a number of complementary features or classification algorithms can facilitate the classification performance  if they are used simultaneously this research idea in combining multiple decisions seems to be promising 
　many studies in multiple classifier systems have focused on the fact that a classifier corresponds to an expert in multiple experts group they conducted a study about the methodology of integrating multiple decisions from different classifiers mandler and schuermann  1  hull et a!  1  suen et al  1  xu et al  1  franke and mandler  1  huang and suen  1  
ho 1 ho ei al 1  although many studies have been conducted for better combining multiple decisions  most of them did not focus on dependency relationship among classifiers instead  they considered multiple classifiers as being independent 
　as seen in previous studies  if the highly dependent inferior classifiers are added to a multiple classifier system  and they have equal weights  then the classification performance of it can be degraded and biased by some of those decision combination methods the motivation of this paper is to overcome the shortage of previous studies 
　it is desirable to construct a multiple classifier system in which classifiers complement each other for better performance  and to combine multiple decisions by using the dependency relationship  if necessary in this paper  we will provide some methods in identifying the dependency relationship  and in combining multiple decisions  using bayesian formalism 
to identify dependency relationship of a classifier from 
a classifiers  we should consider all the possible cases of lower order subdistnbutions from the definition of product approximation by lewis lewis  1  because that requires enormous computations and storage spaces  we will approximate k-dimensional distribution with the k - 1 first-order dependency distributions by considering only the classification results of classifiers  and by using two identification methods proposed two methods are considered for the work one of them depends on the principle of maximizing mutual information by chow chow and liu  1   and the other one is to use 

maximally measured association the first-order dependencies identified are used in order to combine multiple decisions  using bayesian formalism 
　all the classifiers were created by hidden markov models  hmms  the hmms have been used as a framework of on-line cursive script recognition sin et al  1  and can model well for both variations of temporal se1quences and spatial movements also  they were used as components of a multiple classifier system a number of multiple classifier systems are applied to totally unconstrained on-line numerals and the english alphabet recognition 
　in our experiment  we will apply a number of decision combination methods to combine multiple decisions in multiple classifier systems  and to compare their classification performances each other in particular  several combinations of multiple classifiers for demonstrating the effects of highly dependent classifiers will be carried out  and then tested by decision combination methods proposed 
1 	related works 
a number of studies related to the idea of using multiple classifiers for improving classification performance will be described the researches of combining multiple decisions are divided according to the types of classification results these types include measurement scores of classes  rankings of classes  and single top choice of classes from the measurement scores of classes  we can assign a ranking to each class by ordering classes as to their scores single top choice of a class is chosen by the best measurement score or by the first ranked 
　decision combination methods based on the measurement scores are averaged bayes classified 

and an integration of multiple classification results using fuzzy integral tanahi and keller  1  or fuzzy logic in case of single top choice  there are voting methods suen et al  1  franke and mandler  1  xu et al  1   the use of bayesian formalism under an independence assumption  the use of dempster-shafer formalism used in evidential reasoning mandler and schuermann  1  franke and 
mandler  and behavior-know ledge 
space  bks  huang and suen  1  in particular  in the use of bayesian formalism by   

1   they assume that the classifiers are independent because they use independent feature sets or they are trained independently this approach happens to cause a problem when they are not independent ho et al  ho  1  ho et al  1  support the use of rankings and have studied the decision combination methods using such rankings from multiple classifiers which include the highest rank method  borda count method  and logistic regression borda count method is well known one of social choice functions used in our experiment in this paper  we concentrate on the type of ranking decisions due to the advantages of rankings 
　lewis lewis  1  tackled a problem of approximating a nth-order binary distribution by a product of several of its component distributions of lower order using the idea of extension by hartmarue he showed that the product approximation  under suitably restricted conditions  had the property of minimum information the approximation method was based on an information measure for the closeness of two distributions and on the criterion of maximum entropy two or more proposed approximations could be compared and the beat one be selected without any knowledge of actual distribution beyond that given by the approximations in 
order words  the process of comparison consists of selecting that approximation containing the greatest amount of correlation however  the problem of selecting a set of component distributions of a given complexity to compose the best approximation remained unsolved 
　chow et al  chow and liu  1  studied to solve the unsolved problem by lewis and to best approximate an nth-order distribution by a product of n - 1 second-order component distributions a method was presented to approximate optimally an n-dimensional discrete probability distribution by a product of second-order distributions  or the distribution of the first-order tree dependencies to find an optimum set of n - 1 first-order dependency relationship among n variables  a procedure was derived to yield an approximation of a minimum difference in information an optimal procedure was on the basis of maximizing the mutu al information between two variables and maximum weight spanning tree  mwst  algorithm by kruskal 
1 	m u l t i p l e classifier s y s t e m s 
a multiple classifier system  mcs  consists of a set of classifiers and a decision combination method ho  1  in this paper  a set of classifiers is built based on a hmm structure and a few stochastic modeling methods sin et al  1  sin and kim  1  the hmm structure is left-to-right transitionary and consists of 1 state nodes and only lefl-lo-nght arcs in order to create classifiers consisting of hmms modeled from training data  we use a few stochastic modeling methods which are standard modeling  i e baum-welch algorithm   duration modeling  and nonstationary modeling one hmm was modeled to represent one class at training although they have the same hmm structure  they would be different classifiers if they are modeled by different modeling methods 
　the graphical representations of stochastic modeling methods are shown in figure 1 by focusing on transition probabilities at state nodes standard modeling method is easy and takes a short time to train the hmms  but it does not properly model duration information of a pattern duration modeling method supplements such a weakness oi standard modeling method  but it takes a long tune to train nonstationary modeling method is proposed by sin et al  sin and kim  1  for overcoming the weakness of duration modeling method which does not properly consider the duration information their approach is based on the idea that the duration in a 
　state node should be modeled as a function of duration period  and by that idea it can properly consider the duration information this method has an advantage of the best modeling for typical patterns  but it takes a very long time to train and it has a disadvantage of the worst 
kangandkim 


figure 1 stochastic modeling methods of hmms 
modeling for unusual patterns a mcs is a good choice why there are no complete stochastic modeling methods for improving the classification performance 
   for the recognition of numerals and the english alphabet characters  a classifier consists of 1 hmms  or 1 hmms  or 1 hmms which are stochastically modeled for respective domain problem and assigns a likelihood score as to degree of match to each hmm for a given input so  we have to convert the likelihood scores of hmms into the rankings of classes modeled to hmms before combining multiple decisions 
　in this paper  a number of mcss are constructed one of them consists of three classifiers shown in figure 1 which are based on the hmms trained by three stochastic modeling methods it is called by base-type in our experiment and the others are constructed by adding 

figure 1 a mcs consists of three classifiers 
the highly dependent inferior or superior classifier to the previous three classifiers which is respectively denoted by st1%t  du1ult and ns1ut the highly depen-
dent classifier is made by faking one of the previous three classifiers 
　multiple decisions from individual classifiers of mcs will be combmed according to a combining rule assigned to decision combmator and a winner class will be decided or not the input of all classifiere are 1-directional chain codes in common but different stochastic modeling methods cause classifiers to classify differently for a given input the output of each classifier is converted as the rankings of classes  and then they are used as the input of decision combtnator any classifier producing the rankings of classes can join into our mcs 
　only classifiere producing valid decisions take part in a process of decision combmaior also  only classes having valid rankings are used in decision combination under those constraints  decision combtnator includes a number of decision combination methods available in 
1 	learning 
this paper they are voting methods  some of social choice functions  bks method  the uses of bayesian formalism under an independence assumption and dependency relationship among classifiers 
1 d e p e n d e n c y r e l a t i o n s h i p b a s e d d e c i s i o n c o m b i n a t i o n 
on the assumption that multiple classifiers are independent  the use of bayesian formalism is proposed by xu 
et al  in this paper  since an independence assumption would not be appropriate if some highly dependent classifiers join in multiple classifiers  we focus on the dependency relationship among classifiers we will propose some methods for identifying the dependency relationship and combining multiple decisions  using bavesian formalism we will denote a set of classes by a set of classifiers by and an input pattern by z 
1 	identifying t h e d e p e n d e n c y relationship 
in order to build a it -dimensional joint probability distribution in a direct way  we can use if-dimensional samples observed but when the k becomes larger and each classifier takes one of a set of m distinct classes  it requires much computation and massive storage of the order of   and that is impractical in theoretical viewpoints in other way  which is based on the chain rvie of probability  we can express the joint probability distribution in terms of conditional probabilities 

　in considering the dependency relationship  we assume that it is determined by the observations of outputs of individual classifiers given the same inputs with a classifiers  the precise dependency relationship should be determined by considering maximum order dependencies by the definition of chain rule  since we do not know in advance the exact order of dependency relationship among them when k classifiers are applied to the same input x  k events     k and 
   m  wll happen basing on the fact that it is desirable to consider first-order dependency relationship rather than to assume independence of classifiers  we will approximate the joint probability distribution with most appropriate second-order conditional probabilities based on the first-order dependency relationship 

then the problem is how to identify an optimal set of first-order dependencies from classification results of classifiers our approaches to tackle that problem are to use two methods one is to maximize the mutual information and the other one is to maximize the measures of association between classifiers 
m u t u a l information 
when we approximate k-dimensional distribution with a set of k-  first- and second-order component distributions  we should identify the first-order dependence distribution such that the relative entropy  of unknown true distribution p c  and the first-order dependence distribution from a set of all possible first-order dependence trees is as small as possible  lewis 1  chow and liu  1  when the relative entropy is zero  we assure that a couple of distributions involved are equal 
	from the expression of 	minimizing 
the relative entropy is equivalent to maximizing the sum of mutual information  1 e between a classifier and a classifier c1  jj a c l a s s i f i e r   is the parent of a classifier in first-order dependency association as criteria of statistical dependence while the correlation coefficient represents the degree of linear dependence between numeric data  our classifiere output non-numerical nominal data classes therefore  the correlation coefficient is not appropriate to identify the dependency relationship of our classifiers 
　in computing measures of association in statistics  there are cramer's value  l e v  and contigeney coefficient  i e cc  computed from pearson  statistic  entropy symmetric measure  i e egvm  from reduction in uncertainties in predicting the relationship between two classifiers  and lambda symmetric measure  l e as an index of predictive association hays and winkler  1  they are non-negative real values we think that 

those measures represent the degree of depencence relationship between classifiers for expressing those measures  let n be the number of input samples  / be the number of output classes by classifier i  j be the number of output classes by classifier j  or n be the number of observed outcomes from classifiers i and be the number of expected outcomes from classifiers i and j  that 
measures of association 
whole the mutual information contains the average amount of uncertainty to be resolved between two distributions  the measures of association measure the strength of association between two distributions from 
	1 	t h e use of bayeaian f o r m a l i s m 
sampling data for applying some coincidence measures 
such as correlation coefficient in numerical data to quali-	with 	an 	optimal set 	of first-order 	dependencies  tative nominal data classes  we will use some measures of 	single confusion matrices  and pairwise joint con-
kang and kim 

fusion matrices  we can compute the group confusion probabilities of each output class to every class at test stage for each class mi  using 
bayesian theorem and an optimal set of first-order 

　from the above expression  if we remove constant terms  
1 	analysis of some empirical results 
an base-type multiple classifier system  mcs  consists of only original three classifiers  see figure 1  for demonstrating the effects of highly dependent classifiers to combining multiple decisions  we have built a number of m c s s by adding the highly dependent classifier to the base-type mcs by faking one of its component classifiers for example  ust1ut faked mcs consists of original three classifiers and the one created by faking st1ui classifier we apply them to totally unconstrained online numerals and the english alphabet recognition 
　besides voting methods with absolute majority principle and simple majority principle  some of social choice functions are implemented  hwang and lin  1  social choice functions include condorcet function  borda function  and nanson function  also called by borda elimination method with the lowest borda score  these functions have been widely used to choose the winner based on group consensus from ranking decisions of alternatives in the area of group decision support systems 
　no training stage is required for applying voting methods and some of social choice functions  but it is needed for applying bks method and some methods in bayesian formalism with training data  we use 1 items of numerals written by 1 writers  1 items of the english lowercases written by 1 writers  and 1 items of the english uppercaees written by 1 writers and with 
1 	learning 
test data  there are 1 items of numerals written by 1 writers  1 items of the english lowercases written by 1 writers  and 1 items of the english uppercases written by 1 wntere the writers of the test data are different from those of the training data the sign words in following tables are denned in section 1 the sign word u n means a rejection and contains the number of samples rejected the recognition rates are computed in percentage from the rank of the winner matched with the input label class the recognition rates of classifiers for test data in application problems are shown in table 1 

table 1 recognition rates of classifiers for test data 
from the experimental resultb of test numerals  see 
table 1 and table 1   the recognition rates of voting methods and social choice functions are lowered when inferior du1ut or superior ns1vi classifiers are faked and added the extent of degradation in recognition rates is larger when an inferior classifier is faked and added there is hardly difference in classification performance among some methods of bayesian formalism although one of classifiers in base-type mcs is faked and added  the recognition rates of bks method are unchanged the recognition rates of dependency relationship based decision combination methods show a little higher than those of an independence assumption based decision combination in use of bayesian formalism  but that is not statistically significant by /-test at significance level 1 
　from the experimental results of test lowercases  see table 1 and table 1   the recognition rates of voting methods and social choice functions are lowered when inferior stsout or superior ns1ut classifiers are faked and added the extent of degradation in recognition rates is much larger when an inferior classifier is faked and added the recognition rates of dependency relationship based decision combination methods show higher than those of an independence assumption based decision combination in use of bayesian formalism by 1 1 % the unproved correctness of a base-type mcs by 

the dependency relationship is statistically significant by  -test at significance level 1  and in case of a sisout faked mcs  its improved correctness is significant by ttest at significance level 1  and in case of a nssout mcs  its improved correctness is significant by  -test at significance level 1 we will describe one example to perform  -test in a st1ut faked mcs to verify whether the improved correctness by considering the dependency relationship is statistically significant or not like the case of numerals  the recognition rates of bks method are unchanged 

　from the experimental results of test uppercases  see table 1 and table 1   the recognition rates of voting methods and social choice functions are a little lowered when inferior ns1ut or superior du1ut classifiers are faked and added the extent of degradation in recognition rates is larger when an inferior classifier is faked and added the improved correctness of a ns1vt faked mcs by the dependency relationship is statistically significant by  -test at significance level 1 and in case of a du1 out faked mcs  its degraded correctness is in significant- by  -test at significance level 1 like the case of numerals  the recognition rates of bks method are unchanged 
　they are the straightforward instances of problems before mentioned therefore  it is problematic to add simply any classifiers to existing multiple classifier system and to apply voting methods or some of social choice functions the classification performance of bks method is also lowered by adding classdiers 
　from the analysis of some empirical results  we come to some conclusions 
  the classification performance of a .base-type mcs is almost superior to that of individual classifiers 
  voting methods and social choice functions show similar behaviors in combining multiple decisions 
  all the combination methods in bayesian formalism almost outperforms the other combination methods 
  incorporating the dependency relationship into bayesian formalism helps improving the classification performance of a mcs  especially when the highly dependent inferior classifiers are added to it some empirical results for lowercase significantly support our assertion by showing statistically signif icant  -test results 
  it is problematic to add simply any classifiers or to combine multiple decisions without considering the dependency relationship among classifiers because the highly dependent inferior classifiers can degrade the classification performance of a mcs  if voting methods or some of social choice functions are used 
  the classification performance of bks method is unchanged when one of component classifiers are faked and added 
1 	c o n c l u s i o n a n d f u r t h e r w o r k s 
in order to prove the effectiveness of dependency relationship for combination of multiple decisions  and for construction of multiple classifier systems  we proposed a couple of methods for identifying an optimal set of firstorder dependencies approximated from sampling data  
kang and kim 

and a decision combination method of multiple decisions in bayesian formalism  using the first-order dependencies identified this research applied the proposed methods to totally unconstrained on-line numerals and the english alphabet recognition the results suggest that the dependency relationship should be considered not only for combming multiple decisions  but also for constructing multiple classifier systems 
　further studies should examine both the robust criteria for identifying the dependency relationship correclty  and the good approximation method s  for obtaining the dependency relationship of a group of classifiers with the high-order dependency relationship these proposed ideas will play important roles in resolving the following questions how many classifiers are chosen1  what kind of classifiers should be used1  how a subset of classifiers is selected dynamically1  and how multiple decisions are combined1 
acknowledgments 
the authors wish to thank bongkee sin for providing the classification results of three individual classifiers st1ovt  du1ut  and nssout which form the basis of the experiments performed the authors would also like to thank the anonymous reviewers for their helpful comments 
