 
machine translation should be semanticalty-accurate  linguisticallyprincipled  user-interactive  and extensible to multiple languages and domains. this paper presents the universal parser architecture that strives to meet these objectives. in essence  linguistic knowledge bases  syntactic  semantic  lexical  pragmatic   encoded in theoretically-motivated formalisms such as lexical-functional grammars  are unified and precompiled into fast run-time grammars for parsing and generation. thus  the universal parser provides principled run-time integration of syntax and semantics  while preserving the generality of domain-independent syntactic grammars  and language-independent domain knowledge bases; the optimized cross product is generated automatically in the precornpllation phase. initial results for bi-directional english-japanese translation show considerable promise both in terms of demonstrating the theoretical feasibillty of the approach and in terms of subsequent practical utility. 
1 . introduction 
　accurate translation requires a degree of comprehension  and several projects have developed prototype systems to demonstrate the feasibility of knowledge-based machine translation  1 1 . these approaches combine syntactic and semantic information to produce an intermediate knowledge representation of the source text1 which is then generated in the target language. this paper does not attempt to revisit the ample rationale for the knowledge-based machine translation concept - such discussion may be found in the literature  1 1  - but rather discusses the universal parser  up  approach for combining syntactic  semantic and lexical knowledge in order to analyze source text for re-generation in multiple target languages  preserving imvariance of meaning. 
　first  however  let us review the set of performance objectives that contributed to the design of the universal parser: 
  semantic accuracy - the translation should maintain semantic invariance above an else. paralleling syntactic form  maintaining equivalent length of text  and other such criteria are considered of secondary importance. thus  the knowledge-based approach was the only logical choice. 

1 natural lanqu age   multi- llngual generality - the system should be able to handle any natural language and any semantic domain. the addition of any new language should enable immediate translation to and from all the previous languages  without requiring explicit handbuilt transfer grammars for all pairs of languages. 
  interactive translation - translation should occur in real time  interacting with the user if required. whereas most existing practical machine translation systems are designed to batchprocess large documents  there is a growing need for immediate interactive translation of business letters  telexes  and eventually interactive communication  by telephone  at an airport counter  at a foreign hospital  etc. . such interactive usage adds the following demands: 
  no post-editing should be required  as one cannot cany along a personal post-editor in case he or she is needed. 
  real-time performance is an absolute requirement  as participants in a dialog will not wait minutes or hours for a response. 
  speech compatibility is an equally strong requirement  as the utility for real-time kbmt translation systems increases dramatically when coupled with speech recognition and synthesis. speech recognition imposes the requirement to handle unsegmented input  with multiple word candidates present at any point in the input stream.  i.e.  the input is typically a lattice rather than a linear string .  
  linguistic generality - linguistic information  syntactic  semantic  and lexical  should be expressed in elegant  theoreticalty-motivated formalisms - ones that linguists can use to develop and modify grammars and knowledge bases rapidly  such as lfg . 
  dlscourse phenomena - extra-sentential phenomena such as anaphora  ellipsis  metalanguage  and speech acts  should be handled within the framework  as should inference required to support cross-linguistic variation  such as politeness levels  inference of missing constituents - e.g.. subjects in japanese and finer grain lexical selection required in some target languages . 
  multiple utility - in addition to machine translation proper  the methods developed should be applicable to multi-lingual natural language interfaces  to data bases  expert systems  etc. . and 
other application such as automated skimming and indexing of texts. 
　we have achieved the majority of these objectives in an experimental system at cmu's center for machine translation  and we are actively working on developing the other capabilities. the system  consisting of the universal parser and universal generator  is an openarchitecture approach to knowledge-based machine translation  integrating multiple off-line knowledge sources into a fast on-line run-
time system . we have chosen english and japanese as our initial languages  and simple doctor-patient communications as our initial test domain  and have produced real-time  semantically-accurate  bidirectional translations at the sentential level. the rest of this paper gives an overview of the parser architecture and our system. for more detail  see . 
1. the universal parser: a new architecture for multilingual parsing 
　multi-lingual systems require parsing multiple source languages  and thus a universal parser  which can take a language grammar as input  rather than building the grammar into the interpreter proper  is much preferred for reasons of extensibility and generality. however  semantic information can remain invariant across languages  though  of course  not across domains . therefore  it is crucial to keep semantic knowledae sources separate from syntactic ones. if new linguistic information is added it will apply across all semantic domains  and if new semantic information is added it will apply to all relevant languages. the question  of course  is how to accomplish this factoring  without making major concessions to either run-time efficiency or semantic accuracy. 

1. the system architecture 



figure 1: universal parser concept 
　the idea of the universal parser is depicted in figure 1. there are two kinds of knowledge sources: one containing syntactic grammars for different languages and the other containing semantic knowledge bases for different domains. syntactic grammars and domain knowledge bases are written in a highly abstract  human-readable manner. this organization makes them easy to extend or modify. the grammar compiler takes one of the syntactic grammars  say language l   and one of the domain knowledge bases  say domain dj   along with mapping rules  that determine which semantic concept is expressed by what word and what linguistic structure   and produces an object grammar  containing an optimized legal cross-product of both syntactic and semantic information. whereas the compiled grammar is not human-readable  it must is extremely machine-enicient in terms of on-line run-time parsing speed. when the user inputs sentences in language l   and domain d    the run-time parser parses the sentences very efficiently  referencing only the compiled grammar  and producing semantic representations of the sentences. 
figure 1: system architecture 
　figure 1 shows the architecture of the universal parser. we adopt semantic case frames for domain knowledge representation and the functional grammar formalism for syntactic grammar representation. the run-time grammar produced by the multi-phase compiler is an augmented context-free grammar  acfg  which is further compiled into an augmented lr table to be used by a run-time parser based on the tomita parsing algorithm. these components are described in detail in the following subsections. 
1. semantic frame representation 
  action 
  i a - a  valua *sentential   whan  sem *time     t a r t  sem  time   
 :and  sem  time   
	 :fxaq 	 aaa *frequency   
  : d u r a t i o n  sem  duration     
  patient-action 
	 is.-a 	 valua  action   
 :agant  earn *patient     
  ingest-medicine 
  i a - a  valua *patient-action   
	 :objact 	 .am *medicine   
  : i n g a . t - w i t h  sem *food  medicine     
 *medicine 
	  i a - a 	 valua *physical-object   
 :quant  sem *medicine-quantity     
figure 1: fragment of domain semantics specification 
　we use framekit  as our knowledge representation language to encode domain semantic knowledge. framekit is a compact and fairly efficient general-purpose frame-representation language with multipleinheritance  procedural attachment  and default semantics. domain knowledge consists of a set of frames organized into an inheritance hierarchy. each frame represents a concept such as object  event  etc. in the domain with appropriate semantic links to other 
	tomita and carbonell 	1 1 
related frames in the hierarchy. frames encode typing information  functional dependencies and express compositional constraints used in the parser to block non-productive computations. 
   let us consider the domain of simple doctor-patient conversations. entities in this domain include an event frame *ingest-medicine and object frames 'medicine 'patient and so on. example frame definitions are shown in figure 1. sentences with different surface forms that should be recognized as instantiations of these frames include the following examples. 
take the medicine with three glasses of water every six hours for two days. 
what medicine did you take today  i took two aspirin three hours ago. 
　as a more direct example  the final semantic representation of the sentence take the medicine with three classes of water every six hours for two days* produced by instantiatinp  frames is shown in the first sample translation in the appendix. this knowledge structure may then be given to any back-end process  whether it be a language generator  to translate into the target language   a paraphraser. a data-base query system  or an expert system. 
1. the functional grammar formalism 
　we adopt the functional grammar formalism for syntactic knowledge representation of each particular language. two wel-known functional grammar formalisms are functional unification grammar  ug   and lexical function grammar  lfg  . figure 1 is a lfg grammar fragment written in a notation similar to patr-ii . 

figure 1: fragment of english lfg in the patr-like notation 
　there are two main advantages of using the functional grammar formalism in multi-lingual nlp systems over more traditional linguistic theories: 
  a grammar in this formalism can be used for both parsing and generation. thus  we do not need to write separate grammars tor parsing and generation. 
  functional grammar formalisms such as ug and lfg are well-known among oomputational linguists  and therefore they need not be trained  with some justifiable resistance  to write grammars in arcane system-specific formalisms. 
1. grammar compilation and efficient on-line parting 
　the previous two sections have described how to represent domain semantics and language syntax. the universal parser unifies both knowledge sources and optimizes the grammar for runtime performance in a series of offline precompilation phases. the first compear named syn/sem grammar compter compiles the syntactic and semantic knowledge  as weft as morphological rules and dictonary  into a single large lfg grammar called syn/sem grammar. the compiled syn/sem grammar is basically the aame as its original syntactic grammar except that it acquired many additional semantic equations generated automaticaly by the compiler. the semantic aquations check samantic constraints and bulld samantic representations rather than syntactic f-structures. 
　this syn/sem grammar in patr-like notation is further oomplled into an augmented context-free grammar  acfg  by the second oompiler named the lfg compiler  this acfg grammar is represented by a set of context-free phrase structure rules  each of which is augmented with a lisp program for test and action as in atns. am the lisp functions are generated automatically by the oompiler from the constraint equations in the syn/sem grammar. also note that those lisp functions are further compiled down to machine code by the standard lisp compiler. 
　once we have a grammar in this form  we can apply efficient context-free parsing algorithms. in fact  we subject this grammar to a 
　final round of completion  where the context free rules are compiled 
1 1 	natural language 
into a large augmented lr table for a generalized shift-reduce parser based on the tomita algorithm  1 . whenever the parser reduces constituents into a higher-level nonterminal using a phrase structure rule  the lisp program associated with the rule is simply evaluated. the lisp program handles such tasks as: 
  blocking partial parses that violate syntactic or semantic constraints  thus enforcing subject-verb agreement  type checking on the arguments to a proposed semantic relation  etc   
  constructing a semantic representation of the input sentence from its constituent parts  an instantiated frame or causally related set of frames   and 
  passing attribute values among constituents at different levels in order to have the information that is needed to perform the constraint-checking and frame-instantiation tasks. 
　the tomita algorithm has three major advantages for real-time parsing over other methods: 
  the algorithm is fast  due to the lr table precompilation. 
  the efficiency of the algorithm is not affected very much by the size of its grammar  once the lr parsing table has been precompiled. 
  the algorithm parses a sentence on-line  i.e.  strictly from left to riant and it starts the moment the user types the first word  
without waiting for completion of the sentence. 
1. concluding remarks 
　the first pilot integrated implementation of the universal parser was completed and demonstrated in october 1. demonstrating the computational feasibility of the concept. 
　we have written a fairly comprehensive english syntactic grammar and japanese syntactic grammar in lfg  each containing somewhat under 1 rules of grammar and regular morphology  the english grammar handies declaratives. imperatives  yes-no questions   wh questions and other gapped constructions  auxiliary complexes and related phenomena. additionally we built grammar rules for specialized constructions such as times and dates. the japanese grammar corresponds roughly in coverage to the english grammar  in addition to having far more comprehensive morphological analysis rules  in lfg notation  required for japanese. both grammars are still being refined and extended to achieve full syntactic and morphological coverage. we have started grammar development for a third language  french  to make our system tri-llngual. 
　we also developed a non-trivial domain semantic knowledge base in framek it for certain classes of doctor-patient conversations  plus mapping rules and a corresponding lexicon including over 1 disease names. this domain was chosen as our test bed for developing the universal parser and generator architectures  and we are currently starting on a second domain  small computer manuals . 
all modules are programmed in common lisp and running on 
symbolics 1s  hp bobcats  and ibm rts - the entire system should be portable to any other workstations running common lisp  explorer  micro vax  sun  etc.  
　one of our main research activities at present lies in the area of discourse  as our initial system operates only on a sentential basis. first  we intend to borrow the successful case-frame ellipsis resolution methods developed recently in xcalibur  1  languagecraft   and psli-1   and integrate them into the universal parser architecture. 
these methods rely primarily on case-frame semantics and on functional properties of the syntax. second  we expect to work on extending and applying the embryonic work on practical anaphora resolution in xcalibur and work on handllng metalinguistic utterances . third  we will focus attention on default inference processes to fill in implicit information lacking in the source text  but required for accurate translation. such information includes subjects in japanese  which are optional when inferable from context  but which must be stated explicitly in translating to english. at present we utilize a handful of ad-hoc rules to supply default subjects  levels of politeness  etc.  but a more principled and systematic approach is required. fortunately  the universal parser architecture provides an ideal computational framework into which new knowledge sources may be introduced. and  the knowledge-based translation task provides oopious and severe empirical tests for our theoreticallyinsepired ideas and methods. 

1. 