 
multi-view learners reduce the need for labeled data by exploiting disjoint sub-sets of features  views   each of which is sufficient for learning. such algorithms assume that each view is a strong view  i.e.  perfect learning is possible in each view . we extend the multi-view framework by introducing a novel algorithm  aggressive co-testing  that exploits both strong and weak views; in a weak view  one can learn a concept that is strictly more general or specific than the target concept. aggressive co-testing uses the weak views both for detecting the most informative examples in the domain and for improving the accuracy of the predictions. in a case study on 1 wrapper induction tasks  our algorithm requires significantly fewer labeled examples than existing state-of-the-art approaches. 
1 	introduction 
labeling training data for learning algorithms is a tedious  error prone  time consuming process. active learning addresses this issue by detecting and asking the user to label only the most informative examples in a domain. in this paper  we focus on co-testing  muslea et al  1  an active learning technique for domains with multiple views; i.e.  domains with disjoint sub-sets of features  each of which is sufficient for learning. co-testing is a 1-step iterative algorithm that  1  uses the few available labeled examples to learn a hypothesis in each view and  1  queries  i.e.  asks the user to label  examples on which the views predict a different label. such queries are highly informative because they correct mistakes made by one of the views: whenever the views disagree  at least one of them must be wrong. 
　co-testing was successfully applied to wrapper induction  muslea et al  1j  an industrially important application. in wrapper induction the goal is to learn rules that extract the relevant data from collections of web pages that share the same underlying structure; e.g.  extract the book titles and prices from amazon. com. for wrapper induction  co-testing uses two views: the sequences of tokens that precede and follow the extraction point  respectively. the extraction rules learned in these views are finite automata that consume an item's prefix or suffix within the page  respectively. 
the main limitation of existing co-testing algorithms 
 muslea et al  1; 1a  is that they are designed to use only views that are adequate for learning  thus being unable to also exploit imperfect views that would permit a faster convergence to the target concept. to address this problem  we extend the multi-view learning framework by introducing the idea of learning from strong and weak views. by definition  a strong view consists of features that are adequate for learning the target concept; in contrast  in a weak view one can only learn a concept that is more general or specific than the target concept. we introduce a novel algorithm  aggressive co-testing  that exploits both strong and weak views without additional data engineering costs. we also describe a case study on wrapper induction  which shows that aggressive cotesting clearly outperforms state-of-the-art algorithms. 
　to illustrate the idea of strong and weak views  consider the task of extracting fax numbers from a directory of restaurant web pages such as zagat. the two wrapper induction views described above are strong views because each of them is  typically  sufficient to extract the item of interest  muslea et al.  1 . in addition to these two strong views  we can also exploit a view that consists of tokens within the item to be extracted. in this view  we learn the grammar     number   number - number  that describes the content of the fax numbers. this additional view is a weak view because the grammar above represents a concept more general than the target one; i.e.  it cannot discriminate between fax and phone numbers that appear within the same web page. 
　aggressive co-testing for wrapper induction works as follows: first  it uses a few labeled examples to learn a rule in each view  i.e.  one weak and two strong rules . then it queries an unlabeled example on which the two strong rules extract different strings  both of which are inconsistent with the content-based grammar. each such query is likely to represent a mistake not only in one  but in both strong views  thus leading to faster convergence. we use a collection of 1 difficult extraction tasks to show that using the weak view dramatically reduces the need for labeled data: compared with existing state of the art active learners  our novel algorithm requires between 1% and 1% fewer labeled examples. 
1 	related work 
the idea of exploiting complementary information sources  i.e.  types of features  appears in various multi-strategy 

learners. of particular interest are two recent papers  kushmerick et al  1; nahm and mooney  1  in which the authors use sets of features that clearly do not have the same expressive power. this work can be seen as learning from strong and weak views  even though it was not formalized as such  and it was not used for active learning. 
　kushmerick et al. 1  focus on classifying the lines of text on a business card as a person's name  affiliation  address  phone number  etc. in this domain  the strong view consists of the words that appear on each line  based on which a naive bayes text classifier is learned. in the weak view  one can exploit the relative order of the lines on the card by learning a hidden markov model that predicts the probability of a particular ordering of the lines on the business card  e.g.  name followed by address  followed by phone number . 
　this weak view defines a class of concepts that is more general than the target concept: all line orderings are possible  even though they are not equally probable. the order of the text lines cannot be used by itself to accurately classify the lines. however  when combined with the strong view  the ordering information leads to a classifier that clearly outperforms the stand-alone strong view  kushmerick et al.  1 . 
　another algorithm that can be seen as learning from strong and weak views is discotex i nahm and mooney  1   which extracts job titles  salaries  locations  etc from computer science job postings to the newsgroup austin. jobs. discotex proceeds in four steps: first  it uses rapier fcaliff and mooney  1  to learn extraction rules for each item of interest. second  it applies the learned rules to a large  unlabeled corpus of job postings and creates a database that is populated with the extracted data. third  by text mining this database  discotex learns to predict the value of each item based on the values of the other fields. for example  it may discover that  if the job requirements include c++ and corba then the development platforms include windows . finally  when the system is deployed 
and the rapier rules fail to extract an item  the mined rules are used to predict the item's content. 
　in this scenario  the rapier rules represent the strong view because they are sufficient for extracting the data of interest. in contrast  the mined rules represent the weak view because they cannot be learned or used by themselves. furthermore  as discotex discards all but the most accurate of the mined rules  which are highly-specific  it follows that the weak view can be used to learn only concepts that are more specific than the target concept. nahm and mooney show that these mined rules improve the extraction accuracy by capturing information that complements the rapier extraction rules. 
1 	preliminaries 
in this section we first explain the main idea behind cotesting algorithms  muslea et al.  1; muslea  1   and then we describe the strong and weak views that we use for wrapper induction. 
1 	background: the co-testing approach 
figure 1 provides a formal description of the co-testing family of algorithms. given a base learner ♀  a set l of labeled 

figure 1: co-testing algorithms repeatedly query examples for which the two views make a different prediction. 
examples  and a set u of unlabeled ones  co-testing works as follows: first  it learns the classifiers h1 and h1 by applying the algorithm c to the projection of the examples in l onto the two views  vi and v1. then it applies h1 and h1 to all unlabeled examples in u and detects the set of contention 
points  which are unlabeled examples for which h1 and h1 predict a different label. finally  it asks the user to label one of the contention points and repeats the whole process. 
　the various members of the co-testing family differ from each other with two respects: the strategy used to select the next query  and the manner in which the output hypothesis1 is constructed. in other words  each co-testing algorithm is uniquely defined by the choice of the heuristics selectqueryo and createoutputhypothesis  . in turn  these two heuristics depend on the properties of both the application domain and the base learner c. 
we consider here two types of query selection strategies: 
- random: randomly choose a contention point. this strategy is appropriate for base learners that lack the capability of estimating the confidence of their predictions. 
- max-confidence: choose the contention point on which both h1 and h1 make the most confident prediction. this strategy is appropriate for high accuracy domains  e.g.  wrapper induction   in which there is little or no noise. on such tasks  discovering examples that are misclassified  with high confidence  translates into queries that  fix big mistakes   thus leading to fast convergence. 
we also consider two  output hypothesis  heuristics: 
- winner-takes-all. the output hypothesis is the one learned in the view that makes the smallest number of mistakes over the tv queries. 
- majority vote: examples are labeled according to the prediction of most views  requires at least three views . 
1 	wrapper induction: the strong views 
in wrapper induction  each item of interest is described by three strings of variable length: the item's content  together 
     1  once training is completed  the output hypothesis is used to predict the label of all new  unseen examples. 

1 	information extraction 


r1:  number  number - number 
figure 1: the forward and backward strong rules  i.e.  rl and r1  find the beginning of the phone number by consuming its suffix or prefix  respectively. r1 is a content-based grammar that describes the structure of the item to be extracted. 

with its prefix and suffix within the document. as this is not a typical machine learning representation in which an example's description in each view consists of a fixed set of features   we describe here in detail how co-testing can be applied to wrapper induction. as a first step  we introduce the basic ideas in stalker imuslea et al  1   which is the wrapper induction algorithm that we use as base learner. 
　consider the illustrative task of extracting phone numbers from web pages similar to the one shown in figure 1. in stalkhr  an extraction rule consists of a start rule and an end rule that identify the beginning and the end of the item  respectively. given that start and end rules are extremely similar  we describe here only the former. in order to find the beginning of the phone number  one can use the start rule 
	rl = 	skipto  phone: i   
this rule is applied forward  from the beginning of the document  and it ignores everything until it finds the string phone:  i . for a slightly more complicated extraction task  in which toll-free numbers appear in italics and the other ones in bold  one can use a disjunctive start rule such as 

　an alternative way to detect the beginning of the phone number is to use the start rule 

which is applied backward  from the end of the document. r1 ignores everything until it finds  fax  and then  again  skips back to the first open parenthesis. 
　as shown in  muslea et al.  1   the above extraction rules can be learned based on user-provided examples of items to be extracted. note that rl and r1 represent descriptions of the same concept  i.e.  start of phone number  in two different views. that is  the views vi {forward view  and v1  backward view  consist of the sequences of tokens that precede and follow the beginning of the item  respectively. 
　note that both vi and v1 represent strong views: as the web pages to be wrapped share the same underlying structure  stalker can be seen as uncovering and exploiting this underlying structure for extraction purposes. consequently  both the forward and backward rules are expected to extract the relevant data from any page. 
1 	wrapper induction: the weak view 
besides the two strong views above  one can also use a third  content-based view  which describes the actual item to be extracted. for example  when extracting phone numbers  one may exploit the fact that they can be described by a simple grammar:     number   number - number'. similarly  when extracting urls  one can take advantage of the fact that a typical url starts with the string  http: //www.   ends with the string  . html   and contains no html tags. 
　in this paper  we use the following features to describe the content of each item to be extracted: 
- the length range  in tokens  of the seen examples. for instance  phone numbers in the format    number   number - number' consist of six tokens  i.e.  the three numbers  the dash  and the two parentheses . 
- the token types that appear in the training examples. this feature consists of the set of the most specific wildcards  e.g.  number  allcaps  etc  that match the tokens encountered in the item to be extracted. for example  in the phone number case  this list consists of two wildcards: number and punctuation. the complete hierarchy of wildcards is described in figure 1. 
- a start-pattern such as  http: //www.  or     number     which describes the beginning of the item of interest. 
- an end-pattern such as  alphanum.html  or  number number''  which describes the end of the item. 
　in order to learn the content-based description of an item  we use as base learner a simplified version of the datapro algorithm  lerman and minton  1 . after tokenizing each of the user-provided examples of strings to be extracted  the weak-view learner proceeds as follows: 
- the length range is determined by finding the examples that contain the largest and the smallest number of tokens; 
- the token types are obtained by going through the tokens that appear in the labeled examples and adding to the set of  seen types  the most specific wildcard that covers it. 
- a start-pattern of length one consists of the most specific wildcard that covers the first token in all labeled examples; if all examples start with the same token  such as       in the phone number example  the actual token is preferred to the most specific wildcard. a startpattern of length k is generated by repeating the procedure above for the first  second ...  up to k-th position. 
- the end-pattern is learned in the same manner as the start pattern  but using the a; tokens at the end of the item. 
　note that  unlike the forward and backward views  the content-based view is a weak view because  for many extraction tasks  this view does not uniquely define the item of inter-


figure 1: the hierarchy of wildcards used for wrapper induction. the parent-child relationship denotes the ls moregeneralthen relationship. for example  the most general wildcard is anytoken  which matches all possible tokens. non-html  which is a child of anytoken  denotes all tokens than arc not html tags  i.e.  alphanumeric tokens and punctuation signs . 
est. this is a consequence of the fact the view uses only features that describe the content of each item. for web pages that contain several items with similar descriptions  such as multiple email addresses  phone numbers  urls  or names  the content-based grammar cannot discriminate between the various items with similar descriptions. 
1 	aggressive co-testing 
we introduce now aggressive co-testing  which provides a framework for naturally exploiting both strong and weak views. for aggressive co-testing  the contention points are defined as unlabeled examples on which the strong views predict a different label. for the wrapper induction problem  aggressive co-testing uses the two strong and one weak views described earlier  i.e.  the forward  backward  and contentbased views . consequently  the contention points are unlabeled documents from which the forward and backward rules extract different strings. aggressive co-testing uses the labeled examples to learn one hypothesis in each view  detects the contention points  and then uses the following heuristics: 
- selectqueryo returns the contention point on which both strong rules violate the largest number of constraints learned in the weak view; e.g.  the extracted strings are longer than the seen examples  the start- and end- patterns do not match  etc. this is a max-confidence querying strategy because the content-based view is maximally confident that the strong rules extract incorrect strings. 
- createoutputhypothesiso uses the three views for majority voting. that is  given a new  unseen document  both strong rules are applied to it; if they extract the same string  this string is returned as the answer. otherwise the  winner  is the strong rule that violates the fewest constraints learned in the weak view. note that this flexible approach allows co-testing to use the most appropriate strong rule for each document in the dataset. 
　to better understand how aggressive co-testing works  we contrast it now with naive co-testing  muslea et al.  1   which uses only the two strong views. both algorithms detect the contention points in the same manner  but they use different query selection strategies and output hypotheses. more precisely  naive co-testing randomly queries one of the contention points and generates a winner-takes-all output hypothesis  i.e.  the rule that makes the fewest mistakes on the queries extracts the data from all documents . 
1 	empirical evaluation 
 the algorithms in the experimental comparison in this empirical evaluation we compare the following algorithms: aggressive co-testing  naive co-testing  qucryby-bagging  and random sampling. the first two algorithms were described in the previous section; random sampling  which is used as strawman  is identical with naive cotesting  except that it randomly queries one of the unlabeled examples instead of one of the contention points. 
　query-by-bagging labe and mamitsuka  1  is the only single-view active learner that can be used in a straightforward manner with stalker and  more generally  for wrapper induction.1 even query-by-boosting  abe and mamitsuka  1  which is similar to query-by-bagging  cannot use stalker as a base learner: as stalker rarely - if ever makes mistakes on small training sets  it eliminates the ability of the boosting algorithm to generate a diverse committee. 
　query-by-bagging is based on the idea of creating a committee of extraction rules and then querying the example on which the committee is the most split  i.e.  the rules in the committee extract the largest number of distinct strings ; the algorithm's actual predictions are made by majority voting the committee of rules. query-by-bagging generates a committee of 1 extraction rules  each of which is learned by training stalker with examples obtained by re-sampling with replacement the original training set. we are forced to use such a small committee because of the scarcity of the training data: as stalker is expected to train on a handful of examples  sampiing-with-replacement from a few examples leads to few distinct training sets for creating the committee. query-by-bagging is run once in each view  and we report only the best results  which are obtained in the forward view. 
the datasets 
in order to empirically compare the algorithms above  we use the wrapper induction testbed introduced by kushmerick . it consists of 1 extraction tasks from 1 web-based information sources.1 as shown in  muslea et al.  1   on most of these 1 tasks stalker learns 1% accurate rules from just one or two randomly-chosen labeled examples. we consider here the 1 most difficult tasks in the testbed  which were also used in previous work on multi-view learning  muslea et al.  1; 1b : 
- the 1 tasks on which 1 random examples are insufficient for learning 1%-accurate rules in both strong views; 
   1 typical wrapper induction algorithms do not have the properties that active learners require of their base learners; e.g.  the ability to evaluate the confidence of each prediction  lewis and gale  1   or to randomly sample hypotheses from the version space  seung et al.  1   or to generate most specific and most general extraction rules  cohn etai  1 . 
   1 these datasets can be obtained from the rise repository: http: //www. i s i .edu/~muslea/rise/index.html. 

1 	information extraction 

- the five additional tasks on which  in order to learn 1%accurate rules in both strong views  stalker requires a large number of random examples  muslea  1 . 
the empirical results 
for each of these 1 tasks  we use 1-fold cross-validation to compare the performance of the algorithms above. within each fold  the algorithms start with the same two randomlychosen examples and then make a succession of queries. in the end  the error rate is averaged over the 1 folds. 
figure 1 summarizes the algorithms1 performance over the 
1 tasks. in each graph  the x axis shows the number of queries made by the algorithm  while the y axis shows the number of tasks for which a 1% accurate rule was learned after exactly x queries. each algorithm is allowed to make 1 queries  for a total of 1 labeled examples. by convention  the  1 queries  data point denotes tasks for which a 1% accurate rule is not learned even after 1 queries. 
　aggressive co-testing clearly outperforms the other algorithms: it makes an average of 1 queries over the 1 tasks that are solved with 1% accuracy; furthermore  on 1 of these 1 tasks  a single query is sufficient to learn the correct rule. in contrast  naive co-testing  which comes second  makes an average of 1 queries per task and converges in a single query on just four of the 1 tasks. also note that aggressive co-testing solves correctly two of the five tasks that cannot be solved by naive co-testing; the other two algorithms fail to solve 1 and 1 of the 1 tasks  respectively. 
　even though aggressive co-testing makes 1% fewer queries that naive co-testing  at first glance the difference between 1 and 1 queries per task may seem small. however  one must take into account that wrapper induction is used in information agents  knoblock et al  1   which typically use hundreds of extraction rules; in this context  aggressive co-testing makes a tremendous difference. 
　to put our work into a larger context  we briefly compare the results above with the ones obtained by wien  kushmcrick  1  which is the only wrapper induction system for which there are published results for all the extraction tasks used here. as the two experimental setups are not identical 1 this is just an informal comparison that contrasts co-testing with a state-of-the-art approach to wrapper induction. 
　the results in  kushmerick  1  can be summarized as follows: wien  which uses random sampling  learns the correct extraction rule on 1 of the 1 task. on these 1 tasks  wien requires between 1 and 1 labeled examples1 to learn the correct rule. for the same 1 tasks  both aggressive and naive co-testing learn 1% accurate rules from at most eight labeled examples  two random plus at most six queries . 
   1 instcad of using cross-validation  wien repeatedly splits the dataset into randomly chosen training and test sets. 
   1 for wien  an example consists of a document in which all items of interest arc labeled; e.g.  a page with 1 labeled names represents a single example. in contrast  stalker counts the 1 labeled strings as 1 examples. we convert the wien results into equivalent stalker-like ones by multiplying the number of wien labeled pages by the average number of item occurrences per page. 
discussion 
the empirical results deserve several comments. first of all  the experiments illustrate the benefits of a framework that naturally integrates strong and weak views: aggressive cotesting exploits the strengths and mitigates the weaknesses of each individual view. for example  we do not use the weak view to identify the contention points because its mistakes may be  unfixable   remember that in a weak view one learns a concept more general/specific than the one of interest . on the other hand  we use the weak view both to detect the highly informative contention points and to find the most appropriate strong view for each prediction. 
　in contrast to aggressive co-testing  existing multi-view learners  blum and mitchell  1; muslea et al  1  can use only the strong views  thus losing an important source of information. similarly  single-view learners must either pool all features together or simply ignore all but one view. note that  in practice  pooling the features together may not be a straightforward task: 
- in discotex inahm and mooney  1   the text mining features  the weak view  are the extracted items  which become available only after the extraction rules are learned and applied to the unlabeled corpus. 
- the main contribution of ikushmerick et al  1   consists of a novel algorithm that exploits features from both the strong and weak views  i.e.  the words in each text line  and the lines's order within the business card . 
　second  we ran an additional experiment to determine the usefulness of the weak view with respect to each heuristic  i.e.  query selection and output hypothesis . we considered two hybrid algorithms between aggressive and naive cotesting: one uses the random and majority vote heuristics  while the other uses max-eonfidence and winner-takes-all; i.e.  each hybrid exploits the weak view in only one of the two heuristics. because of space constraints  we just summarize our findings: the two hybrids outperform naive and underperform aggressive co-testing; more precisely  they make an average of 1 and 1 queries per task  respectively. in other words  the weak view improves both the query selection and the output hypothesis. 
　finally  note that on three of the 1 tasks  aggressive cotesting fails to learn 1%-accurate rules; in fact  on these tasks query-by-bagging and random sampling obtain more accurate rules than both aggressive and naive co-testing. this happens because  on these three tasks  the backward view is significantly less accurate than the forward one. consequently  the distribution of the queries is skewed towards mistakes of the  bad view   which arc not informative for either view: the  good view  extracts the correct string anyway  while the  bad view  is inadequate to learn the target concept. to cope with this problem  we plan to use view validation  muslea et al  1b   which predicts whether or not the strong views are appropriate for a particular task. 
1 	conclusion and future work 
in this paper we introduce the concepts of strong and weak views and present a novel active learner that naturally integrates and exploits both types of views. in a case study on 

wrapper induction  we show that weak views represent a powerful source of information that can be used both to detect highly informative examples and to improve the algorithm's predictions. on a set of 1 difficult extraction tasks  our novel algorithm converges by making up to 1 % fewer queries than other state of the art active learners. 
　we intend to continue our work along several directions. first  we plan to investigate the use of weak views for semisupervised multi-view learning  blum and mitchell  1. second  we intend to extend view validation  muslea et al  1b  so that it also accounts for weak views. finally  we are interested in a theoretical analysis of learning from strong and weak views. 
acknowledgments 
this material is based upon work supported in part by the defense advanced research projects agency  darpa  and air force research laboratory under contract/agreement numbers f1-c-1 and f1-1  in part by the air force office of scientific research under grant numbers f1 -1 -1 and f1-1 -1  in part by the united states air force under contract number f1c-1  and in part by a gift from the microsoft corporation. the u.s.government is authorized to reproduce and distribute reports for governmental purposes notwithstanding any copy right annotation thereon. the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements  either expressed or implied  of any of the above organizations or any person connected with them. 
