learning one more thing 
	sebastian 	t h r u n 	tom m 	mitchell 
	umversitat bonn 	school of computer science 
	institut fur informatik 	iii 	carnegie mellon university 
	romerstr 1  d-1 bonn germany 	pittsburgh  pa 1 usa 

thrun and mitchell 




application of ebnn to learning with invanance 
the value constraints in slopes  tangents  for the hypothesis h 


figure 1 objects  left  and corresponding network inputs  right  a hundred images of a bottle a hat a hammer a coke can and a book were used to train and test the invanance network afterwards  the classification network was trained to distinguish the shoe from the glasses 
be large given that slight color changes imply that the object would belong to a different class 
   when training the classification network slopes provide additional information about the sensitivity of the target func tion with respect lo its input features hence  the invanance network can be said to bias the learning of the classification network however since ebnn trains on both slopes and values simultaneously errors in this bias  incorrect slopes due lo approximations in the learned invariance network  can be overturned by the observed training example values in   the robustness of ebnn lo errors in estimated slopes has been verified empirically in robot navigation  mitchell and thrun 
1  and robot perception  o sullivan et al  1  domains 
1 	example 
1 	the domain object recognition 
to illustrate the transfer of knowledge via the invariance net work  we collected a database of 1 color camera images of seven different objects  1 images per object  as depicted in fig 1  lefl columns  
object color size bottle 
hat 
hammer can book shoe glasses green blue and while 
brown and black 
red yellow brown black medium 
large 
medium medium 
depending on perspecuve 
medium small    the objects were chosen so as to provide color and size cues helpful to their discrimination the background of all images consisted of plain while cardboard different images of the same object vaned by the relative location and orientation of the object within the image in 1% of all snapshots the location of the light source was also changed producing bright 

figure 1 images along with the corresponding network inputs of the objects shoe and glasses these examples illustrate some of the invanances in the object recognition domain 
reflections at random locations in various cases in some of the images the objects were back 1 in which case they appeared to be black fig 1 shows examples of two of the objects the shoe and the glasses 
   images were encoded by a 1-dimensional vector  provid ing color brightness and saturation information for a down scaled image of size 1 by 1 examples for the down-scaled images arc shown in figures 1  rightcolumns and1 although each object appears to be easy to recognize from the original image in many cases we found it difficult to visually classify objects from the subsampled images however subsampling was necessary to keep the networks to a reasonable size 
   the set of target functions f was the set of functions that recognize objects one for each object for example the indicator function for the bottle   was 1 if the image showed a bottle and 1 otherwise since we only presented distinct objects all sets of positive instances were disjoint consequently  f obeyed the invanance property the set of hypotheses h was the set of all artificial neural networks with 1 input units 1 hidden units and j output unit  as such a network was employed to represent the target function 
the objective was to learn to recognize shoes i e   
 five other objects namely the bottle  the hat the hammer die can and the book were used to construct the support sets to avoid any overlap in the training set a and the sup port sets in  we exclusively used pictures of a scvcndi object glasses as counterexamples for fshoe each of the five support sets in   	contained 
1 images of the corresponding object  positive examples  and 1 randomly selected images of other objects  negative examples  when constructing training examples for the in variance network we randomly selected a subset of 1 pairs of images 1 of which were used for training and 1 for cross-validation 1% of the final training and cross-validation examples were positive examples for the invanance network  i e   both images showed the same object  and the other 1% were negative examples the invanance network was trained using the back-propagation algorithm1 after training the in-
1
　　the classification accuracy of the invanance network was sig nificantiy improved using a technique described in isuddarth and 
kergosien 1  see  thrun and mitchell 1  for details 


figure 1 generalization accuracy with  solid black curve  and without  gray curve  the invanance network and ebnn measured on an independent lest set and averaged over 1 runs  a  neural network training curves one training example per class and  b  generalization curves with 1c confidence intervals  as a function of the number of training examples 

variance network managed to determine whether or not two objects belong to the same class with 1% generalization accuracy it also exhibited 1% accuracy when tested with images- of shoes and glasses 
1 	learning to recognize shoes 
having trained the invanance network we were now inter ested in training the classification network fshoe the network employed in our experiments consisted ol 1 input units 1 hidden units and 1 output unit-no effort was made to opti mi ze the network topology a total of 1 examples of images showing the shoe and the glasses were available for training and lesting the shoe classification network in our first exper iment  we trained the classification network using only two of 
these a randomly selected image of the shoe  positive example  and a randomly selected image of the glasses  negative example  slopes were computed using the previously learned invanance network 
   our experiments mainly addressed the following two ques tions which are central to the lifelong learning framework and the invanance approach 
1 how important arc the support sets i e to what extent does the invanance network improve the generalization accuracy when compared to standard supervised learning  
1 how effectively can ebnn overcome errors in the invan ance network  how does ebnn compare to using the in variance nelwork as a learned generalized distance metric  cf eq  1    
   fig 1a shows the average generalization curve as a function of training epochs with and without the invanance network the curve shows the generalization accuracy of the classifica tion network each trained using one positive and one negative example without the invanance network and ebnn the av erage generalization accuracy for backpropagation is 1% however  ebnn increases the accuracy to 1% the invanance network alone  when used as generalized distance metric  classifies 1% of unseen images correctly notice the accuracy of random guessing would be 1% 
　　'since in our expenment the negative class i e the glasses forms itself a disjoint class of images those images are also used in de nve slopes  the slopes of u were simply multiplied by -1  this effectively doubles the number of slopes considered in eq  1  the corresponding probabilities 1 - o{  : tm    can also be incorporated into eq  1  see  thrun and mitchell 1  lor details 
   the difference between  the performance with and without support sets which is statistically significant at the 1% level can be assessed in several ways in terms of residual error backpropagation exhibits a misclassification rate that is 1 i % larger than that of ebnn a second interpretation is to look at the performance .increase which is defined as the difference in classification accuracy after learning and before learning assuming that the accuracy before learning is 1% ebnn s performance increase is 1% which is 1 tiems better uian backpropagation s 1% on the other hand the difference between ebnn and the invanance network is not statistically significant  at the 1% confidence level  
   each of these numbers has been obtained by averaging 1 expenments examining a single experiment provides addi tional insight for example when the neural network is trained using the single image of the shoe and the single image ol the glasses depicted in fig 1 plain backpropagation classifies only 1% of the test images correctly here the generalization rate is particularly poor since the location ot the objects within the image differs and backpropagation mistakenly considers location the crucial feature for object recognition ebnn pro duces a nelwork that is much less sensitive to object location resulting in a 1% generalization accuracy in this particular experiment 
   notice that the results summarized above refer to the classification accuracy after 1 training epochs using just one positive and one negative training example as can be seen in fig 1a  backpropagation suffers from some over fitting as the accuracy drops after a peak at about 1 training epochs the average classification accuracy ai this point in time is 1% however due to lack of data it is impossible in this domain to use early slopping methods that rely on cross validation and it is not clear that such methods would have improved the results for backpropagation significantly 
   these results illustrate that support sets can significantly boost generalization accuracy when training data for the target function is scarce they also illustrate that ebnn manages to make very effective use of the invanance knowledge captured in a- results lor expenments with larger training set sjzes are depicted in fig 1b as the number of training exam pies increases backpropagation approaches the performance of ebnn after presenting 1 randomly drawn training examples of each class ebnn classifies 1% and backpropagation classifies 1% of the testing data correctly this 

matches our expectations as the need for background knowledge decreases as the number of training examples increases  the invanance network alone using eq  1   dashed curve  performs slightly worse than both of these methods its generalization accuracy is 1% which is significantly worse than that of ebnn  at the 1% confidence level  
1 	the role of the invanance network 
the improved classification rales of ebnn which illustrate the successful transfer of knowledge from the support sets via the invanance network raise the question of what exactly are the invanances represented in this network what type information do the slopes convey  
　a plausible  but only approximate  measure of the impor lance of a feature is the magnitude of its slopes the larger the slopes the larger the effect of small changes in the feature on the classification hence the more relevant the feature in order lo empirically assess the importance of features average slope magnitudes were computed for all input pixels  averaged over all 1 pairs of training instances the largest average slope magnitude was found for color information oil in comparison saturation slopes were on average only 1  this is 1% of the average for color slopes  and brightness slopes only 1  1%  
　these numbers indicate that according to the invanance network color information was most important for classification to verify this hypothesis we repeated our experiments omitting some of the image information more specifically in one experiment color information was omitted from the images in a second saturation  and in a third brightness the results 

confirmed our belief that color information indeed dominates classification it is dear that without color the generalization accuracy over the test set is poor although ebnn still generalizes belter if saturation or brightness is omitted however the generalization rate is approximately equivalent to the results obtained for the full images reported above however learning required significantly more training epochs in the absence of brightness information  not shown here  
　fig 1 shows average slope matrices for the target category  shoes  with respect to the three input feature classes measuring color brightness and saturation grey colors indicate that the average slope for an input pixel is zero bright and dark colors indicate strongly positive and strongly negative slopes respectively notice that these slopes are averaged over all 1 explanations used for training 
　as is easily seen average color slopes vary over the ln age showing a slight positive tendency on average average saturation slopes are approximately zero brightness slopes however exhibit a strong negative tendency which is strongest in the center of the image one possible explanation for the latter observation is the following both the shoe and the glasses are dark compared lo the background shoes are on average larger than glasses and hence fill more pixels in addition in the majority of images the object was somewhere near the center of the image whereas the border pixels showed significantly more noise lack of brightness in the image center 

figure 1 slopes of the target concept  glasses  with respect to  a  color   b  saturation  and  c  brightness white  black  color represents positive  negative  values 

is therefore a good indicator for the presence of the shoe as is clearly reflected in the brightness slopes derived from the invanance network the less obvious results for color and sal uration might be attributed lo the fact that optimal classifiers are non linear in color and saturation to discriminate objects by color for example the network has  to spot a specific interval in color space hence the correct slopes can be either positive or negative depending in the particular color of a pixel cancelling each other out in this plot 
　as pointed out earlier slopes provide first-order information and invanances may well be hidden in higher order derivatives however both the superior performance of ebnn and the clear correlation of slope magnitudes and generalization accuracy show that ebnn manages to extract useful invanance information in this domain even if these invariances defy simple interpretation 
1 	using support sets as hints 
a related family of methods for the transfer of knowledge across learning tasks are proposed in  suddarth and kergosien 1   pratt  1   caruana  1  in a nutshell these approaches develop improved internal representations by consid enng multiple functions in f  sequentially or simultaneously  following these ideas we trained a single classification net work providing the support data as hints for the development of more appropnate internal representations this approach re suited in 1%  1 hidden units  or 1%  1 hidden units  generalization accuracy when only a single pair of training in stances was used these numbers can directly be compared to the experiments reported above however  we observed significant overfitting when using this architecture the peak generalization rate of 1%  1 hidden units  or 1%  1 hidden units  respectively occurred after approximately 1 training epochs this generalization accuracy is significantly higher than that of standard backpropagation though not as high as that of the invanance approach with ebnn 
1 	discussion 
in the lifelong learning framework the learner faces a collec tion of related learning tasks the challenge of this framework is lo transfer knowledge across tasks in order to generalize 
better from fewer training examples of the target function itself 
　this paper investigates a particular type of lifelong learning in which binary classifiers are learned in a supervised manner in the approach taken here invanances are learned and trans ferred using the ebnn learning algorithm the experimental results provide clear evidence of supenor generalization in the 

object recognition domain when invanances learned from re lated tasks are used to guide generalization when learning to recognize a new object however the the invariance approach relies on several critical assumptions 
1 well-defined invanance functions rest on the assumption that f obeys the invariance property note even if the invanance property is not satisfied by f the support sets can be used to train an invanance network. even the object recogni tion domain presented above provides an example in which the invariance property may hold only approximately this is because different objects may look alike in sufficiently coarse-grained  noisy images 
1 it is also assumed that functions in f possess certain mvariances which can actually be fearned by the invanance network this does not follow from the invanance property the exact invanances that will be learned depend crucially on the input representation and function approximator used foro  
1 we also assumed that the output space o of functions in / is binary however this assumption is not essential for the invanance approach in principle invariante functions may be defined for arbitrary high dimensional output spaces given that a notion of difference between output vectors is available as demonstrated in  thrun and mitchell  
in the experiments reported above all three assumptions were at least approximately fulfilled wc conjecture that the real world offers a variety of tasks where learned invanances can boost generalization problems such as face recognition cur sive handwriting recognition stock market prediction and speech recognition possess non invial bul imponanl invari ances for example consider the problem of learning lo rec ognize faces o  various individuals here certain .aspects are important for successful recognition  e g the distance between the eyes  whereas others are less important  c g the direction in which the person is looking  alter training on a num ber of individuals wc conjecture that an invanance network might grasp some of these invanances reducing the difficulty of learning faces of new individuals 
　the central question raised in this paper is whether learn ing can be made easier when the learner has already learned other related tasks will a system that is trained to learn generalize better than a novice learner  this paper provides encouraging results in an object recognition domain however most questions that arise in the context of lifelong learning still lack satisfactory more general answers wc expect that future research in this direction will be important lo going beyond the intrinsic bounds associated with learning single isolated functions 
acknowledgment 
this research is sponsored in part by the national science 
foundation under award iri 1 and by the wnght laboratory  aeronautical systems center air force ma lenel command usaf and the advanced research projects agency  arpa  under grant number f1-1 views and conclusions contained in this document are those of the authors and should not be interpreted as necessanly representing official policies or endorsements either expressed or implied of nsf wnght laboratory or the united states government 
