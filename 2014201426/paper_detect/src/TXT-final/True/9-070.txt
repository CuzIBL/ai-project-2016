 
　　a problem that arises in g e t t i n g computers to perceive 1-d scenes is r e l a t i n g information from several d i f f e r e n t viewpoints. in p a r t i c u l a r   if the computer 
moves i t s sensor  it has to be able to predict changes in images of objects it has seen without having to completely re-recognize them. a s o l u t i o n to this problem has been implemented at stanford using a c a l i b rated camera model which expresses the r e l a t i o n b e t -
ween object space and image space as a function of the computer's c o n t r o l v a r i a b l e s . the modelling problem is r e l a t i v e l y w e l l understood. c a l i b r a t i o n techniques  however  are not. this a r t i c l e deals with these. 
descriptive terms 
robots  	computer v i s i o n   	a r t i f i c i a l i n t e l l i g e n c e   
photogrammetry  v i s u a l l y guided manipulators  automatic assembly 
introduction 
　　image analysis for 1-d scene perception is computa t i o n a l l y expensive. it is thus important that a computer which moves i t s sensor be able to p r e d i c t changes in the images of objects it has already seen 
without having to completely re-recognize them. we w i l l present a s o l u t i o n to t h i s problem that has been implemented at stanford f o r a v i s u a l l y guided manipu l a t o r system. 
t the stanford hand-eye project is organized around a dual-processor pdp-1/pdp-1 computer system. two electrically-powered mechanical arms and two standard tv cameras w i t h p a n - t i l t heads are interfaced to the computer; they serve as hands and eyes respectively. in a d d i t i o n   the central part of the work space is a  lazy susan  turntable  see f i g u r e 1 . to have the c a p a b i l i t y to v i s u a l l y locate objects in 1 dimensions  it is desireable to be able to see any p o i n t in the work space from two d i s t i n c t viewpoints. in general t h i s requires several fixed cameras  and/or a highly 
mobile camera  and/or the a b i l i t y f o r manipulating the environment so as to turn things around to see them b e t t e r . the two cameras w i t h p a n - t i l t heads  and the lazy susan  f a c i l i t a t e t h i s . the computer is capable of 
moving the 'arms  t u r n t a b l e   and cameras  and sensing the p o s i t i o n of a l l moveable j o i n t s . information flow is schematized f o r the   r i g h t h a l f   of the system in f i g u r e 1. one camera is f i t t e d w i t h a zoom lens  while the other has a rotatable t u r r e t w i t h 1 lenses. both are f i t t e d with color wheels. zoom  t u r r e t - l e n s - s e l e c t i o n   and c o l o r - f i l t e r - s e l e c t i o n are under computer c o n t r o l . also under computer c o n t r o l are focus  i r i s  on the zoom camera   and vidicon s e n s i t i v i t y . visual information is transmitted to the computer by quantizing a tv image into an array of 1 * 1 samples. each sample is a 1 - b i t number representing 1 of 1 possible l i g h t l e v e l s . a whole image or any rectangular s u b f i e l d 
may be read i n t o the computer memory from a camera. 
this discussion deals with only one aspect of the 
*the research reported here was performed at the stanford a r t i f i c i a l i n t e l l i g e n c e project and was supported in part by the advanced research projects agency of the o f f i c e of the secretary of defense under contract sd1 hand-eye system. for more information of a general and h i s t o r i c a l nature  the reader is r e f e r r e d t o 1 ' ' * 1 * 1 ' 
z *1. 	four phd. theses1* 1 1 describe other major 
aspects of the system. in addition another project thesis is forthcoming1 and is referenced in a n t i c i p a t i o n of i t s p u b l i c a t i o n . for d e t a i l e d information about 
manipulators  	see1'1. 
the problem 
　　the problem can be separated i n t o two main subproblems - modelling and c a l i b r a t i o n . the modelling problem is straightforward geometry; parts of it have been 
worked out in many places1'**1'1. to review it b r i e f l y   we want an analytic camera model r e l a t i n g 1-d coordinates of points in object space to the 1-d locations of t h e i r images in a d i g i t i z e d p i c t u r e . an adequate model f o r these purposes can be derived by t r e a t ing a picture as a c e n t r a l p r o j e c t i o n of object points onto a  image  plane. such a p r o j e c t i o n can be described by  see f i g u r e 1a : 
a -  a  a  a   the l o c a t i o n of the center of p r o j -
　　　x v z 	r - j ection in object space  camera l o c a t i o n   . 
r     r i j   a 1-d r o t a t i o n matrix for specifying the camera's o r i e n t a t i o n . it orients an  xyz  camera frame at a w . r . t . the  xyz  object frame  at 1 in the f i g u r e   . such a matrix is orthonormal and thus has only 1 degrees of freedom which in our case were chosen to be the elementary r o t a t i o n angles  see f i g u r e 1b : 
pan - about the unrotated y-axis  assumed p a r a l l e l to the object z-axis  
tilt - about the once rotated x - a x i s 
         swing - about the twice rotated z-axis f - the normal distance from a to the image plane 
the 1 degrees of freedom contained in a and r locate and orient the camera  while f determines the basic scale of the p r o j e c t i o n . a and it are called external geometry parameters of the camera. 
　　the process' of d i g i t i z a t i o n f u r t h e r scales the image by quantization factors mx my which represent the dens i t y of samples per u n i t length in the x and y d i r e c tions respectively. in addition the coordinate o r i g i n is translated from the p r i n c i p a l point pe to the upper l e f t comer of the image* the combined scales fmx and fmy  and p o are called i n t e r n a l geometry parameters of the camera. the r e s u l t i n g d i g i t i z e d image coordinates  hv  are c a l l e d  image  h o r i z o n t a l and v e r t i c a l respect i v e l y  see figure 1c . 
　　controlling the camera here means providing the a b i l i t y to sense and change some or a l l of the external/ i n t e r n a l geometry parameters. in general feedback sensors do not measure these parameters d i r e c t l y   but functions of them. we c a l l these sensed functions c o n t r o l v a r i a b l e s . for a given configuration we can 
1 *the p r i n c i p l e point is the p i e r c i n g point of the ray from x normal to the image plane. for tv systems a small c o r r e c t i o n    1 p i x e l   is also made for the skewness of the scanning r a s t e r . 
find expressions for the camera geometry parameters in terms of the control variables. these expressions a l -
ways contain fixed  but unknown  parameters which must be measured. the process of satisfactorily measuring these fixed parameters is what we call calibration. since we are interested in developping a predictive model  it is desireable to find a set of such parameters that best accounts for images of known object configurations. an analytical derivation of such a  best  set typically leads to complicated simultaneous transcendental equations which do not yield to closed form solution. moreover  the form of these equations depends strongly upon camera geometry and the number and type of control variables. 
   to be more precise  modelling can be described as follows: write down the transformation relating the 1-d coordinates of objects to the 1-d coordinates of their images. this depends upon the position and orientation of the camera in object space - so-called external geometry - and on its internal geometry. all elements of external and internal camera geometry under computer control  should be expressed as functions of the computer's control variables ex ＊  a1 .. .  1n  . these 
w i l l typically be outputs of feedback sensors. the transformation is schematized in figure 1. typical geo-
metric variables for a camera on a pan-tilt head are the angles of pan and tilt  and f which is the distance from the lens rear nodal point to the image  f is affected by both zooming and focusing motions. the corresponding control variables are potentiometer or operational system1 for calibration of a camera with a zoom lens on a pan-tilt head  bee figure 1 * there are in fact m - 1 s's: 
1 in class  a  above - 1 each for relating sensor outputs to geometric quantitias associated with pan  t i l t   zoom  focus. 
of the remaining 1  1 represent the external camera geometry and f a l l into class  b  above  while 1 represent the internal camera geometry - 1 in  b  and 1 in  c . 
there are n = 1 control variables  a's  for pan  tilt  focus  and zoom. the system calculates a  best  b 
vector given 1 or more object-image point-pairs and their associated a's. it is divided into two parts: 
data collection and model optimization  see figure 1 . 
shaft-encoder readings. 	they are usually linearly but not always - related to the geometric quantities. e.g-   for data collection  pictures are taken of reference objects with known shape  usually cubic  and location that are provided to the program by a human operator. at the time an image is transferred to the computer  the program reads the camera-control feedback sensors  pan  t i l t   zoom  and focus pots} to ascertain the current a. an edge-follower program ' then processes the image to extract boundary points of the object. a polygon is f i t to the points. the polygon vertices are ordered to match up with the object space coordinates. thus for the j- picture of a reference cube  the collection routines w i l l generate the data 

		 1  
these are different from  1  in that h v are measurements and b is to be calculated. assuming non-degenerate equations  we need at least 1-1 such triplets to completely constrain b. the triplets should also be independent in the sense that each triplet yields new information  e.g. 1 p's in the same image along the same central ray yield the same p  and are not independent . 
we can get an idea of the minimum number images   i . 
e. distinct values of a  needed  by using the fact that there are 1 basic geometric parameters for a fixed camera  a - const ; a   pan  tilt  swing  fmy  mrat  and p d - we can write constraint equations similar to  1  with the b's replaced by the geometric parameters  and deduce that we need 1-1 points p to completely solve for these. of these basic parameters  mrat  po * and swing  are also b's. another image  taken with different values for a l l the a's  w i l l only have 1 
parameters unknown of  pan  tilt  fmy   and only 1-1 p's w i l l be needed to solve for the new values. they may in fact be part of the original 1. t h e a l u e s of a are sufficient to solve for 1 more  j's to bring the total to 1. the two values each of pan and tilt are also sufficient to solve for their a associated b's  leaving only the 1 b's relating the focus and zoom pots to fmy. unfortunately each new image  created by changing focus and/or zoom gives us only one new equation i n . thus we need at least 1 more images of one point not on the camera axis  to get a total of 1 values for fmy and associated pot readings to give a soluble system of 1 equations in the remaining 1 b's in summary we have a specific method of solving for the 1 bv's using 1 images using 1+1-1 data triples. 
the total number of distinct points can be the original 1. thus though considerations of numbers of variables and constraint equations t e l l us that 1 data triples are necessary  the form of the equations seems to say that we need at least 1  arranged in 1 images as explained. 
   one of the features of the optimization program is that it is interactive: 
1 - it has a rather elaborate dynamic display  figure 1b . 
1 - it allows the user to restrict the search to any specified subspace of  space. 
1 - it allows him to choose between two search algor i t h m s   '   ' 1 . 
1 - it allows him to initialize  to any values he cares to. 
1 - it allocs the search to be interrupted at any time and the residual error/image to be dis-
played as a function of any of the a's. this is to see if there systematic deviations w.r.t. any given control variable. such a deviation would indicate a defect in the model or the equipment associated with that 
1 - it has a test mode for exploring the convergence of the optimization algorithms in the neighborhood of any desired b the user can either manually input 1 or take a typical value arrived at from data. upon entering test mode  p replaces ip  forcing e to zero. the user is then allowed to add a simulated distortion function to the 
new p if he chooes. he is finally asked to perturb  j and observe the resulting convergence 
back to the ideal point. 
   the search is complicated by the fact that subsets of parameters are interdependent - e.g. to f i r s t order accuracy  changes in e 	due to changes in po  can be 
offset by the pan and tilt offset  it has been empirically determined that there are multiple minima of e1  but they are spaced sufficiently far apart so that pur i n i t i a l guesses are good enough not to go astray. starting points are usually arrived at from rough manual measurements and calculations of the system geometry. a typical optimization sequence consists of: 
1 - reading in about 1 data sets 
1 - if a previous i n i t i a l  has not been stored with the data  it must now be typed in. 
1 - choosing an appropriate subspace of to search  maybe a l l of i t   
1 - letting the f i r s t algorithm  usually more efficient  converge 
1 - applying the second algorithm to try to improve the result if not good enough 
1 - possibly returning to 1 and changing the subspace  optimizing over the whole space if not done  
   the residual rros error e is a measure of the goodness of the model  and is typically i to 1 picture elements  pixels   which corresponds to 1 to 1 
milliradians on the camera axis - with the particular optics used. convergence typically takes about 1 min. of pdp-1 cpu time for a l l 1 parameters. the residual errors are due mainly to mechanical vibration of the optical system and electrical j i t t e r of the scan electronics. these have so ear masked lens and scan distortions. 

   at present there is a program at the hand-eye project to expand the scope and efficiency of the c a l i bration system. the f i r s t step is to elimenate the 
operator to manually measure and specify referenceobject coordinates  see figure 1a . we hope to do this by using the mechanical arm to place the objects according to a prespecified calibration sequence. the resulting scheme w i l l look as shown in figure 1. the prediction error in effect then becomes a  coordination error  between the hand and eye; the arm controller w i l l be reporting the object vertex positions to the precision of the model relating its control 
variables a a r m to requested hand position and orientation. meanwhile the camera calibration w i l l be using 
the reported position information  to 
predict image coordinates. gill1 measured this coordination error in his system for precise object manipulation. he adjusted several camera b's suspected of drifting so as to minimize i t . 

1 - the behavior of the overall error function e! 
w.r.t. a l l these has not been investigated in particular  for the presence of local minima near the global minimum. 
1 - unless the i n i t i a l e1 is already quite small  our current optimization procedures may well 
1 bog down on so many parameters  so as to be useless. 
   this strongly suggests that we use joint optimization only as a last refinement  and even then possibly 
with approximate transform equations for faster convergence near the minimum. joint calibration is ultimately desireable  since the principal use of camera calibration is to get world coordinates for the purpose of guiding the arm. 
　　another system being considered utilizes two cameras  and optimizes their relative orientation based upon simultaneous measurement of the same calibration object from two different viewpoints. thus  if a first camera has been calibrated relative to the reference object-space  a second can be calibrated relative to it etc. this technique is used extensively in photogrammetry1 for compiling maps from large numbers of aerial photographs. a two-camera system  once calibrated  provides a 1-d measuring tool which can in turn be used for basic arm calibration. 
   what is really needed to simplify and speed-up optimization is a separable error function. for example  something of the form 

or even 

which is more likely  would allow us to separately optimize parts of the system while s t i l l guarantying 
minimum overall error. 
   yet another direction for improvement is that of calibration-updating; that is  re-calibrating the system rapidly whenever observations in the course of nor-
mal operation show that errors have grown intolerably large. the emphasis here is on the word rapid - hopefully near real-time. one way to accomplish this is to collect data from measurements made during normal operation of the system. 
   the fully automatic calibration system of the future w i l l probably go through a bootstrapping phase; either an arm or a pair of eyes - whichever is more accurate -
w i l l f i r s t be calibrated. this w i l l be done possibly with the aid of a special calibration device for providing highly accurate reference data. the calibrated half of the system w i l l then be used to provide data for calibrating the other half  at which point the total system w i l l be jointly optimized to minimize coordination errors. 
