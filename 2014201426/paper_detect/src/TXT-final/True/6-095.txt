 
this paper is about the evolutionary design of multi-agent systems. an important part of recent research in this domain has been focusing on collaborative revolutionary methods. we expose possible drawbacks of these methods  and show that for a non-trivial problem called the  blind mice  problem  a classical ga approach in which whole populations are evaluated  selected and crossed together  with a few tweaks  finds an elegant and non-intuitive solution more efficiently than cooperative coevolution. the difference in efficiency grows with the number of agents within the simulation. we propose an explanation for this poorer performance of cooperative coevolution  based on the intrinsic fragility of the evaluation process. this explanation is supported by theoretical and experimental arguments. 
1 introduction 
evolutionary algorithms are methods that apply the principles of darwinian evolution to the generation and adaptation of artificial  logical entities  function parameters  rulesets  programs. .. . their usability as a search technique has been supported both analytically  e.g. the schema theorem  goldberg  1  for genetic algorithms   and empirically by uncountable applications. however  the overwhelming majority of these applications are about the generation of individuals. 
　collective evolution  that is  the generation and/or adaptation of collaborating populations of agents  has attracted comparatively less attention. there has been significant research in this domain though  especially over the last decade. this research led to algorithms of ever-growing complexity. this paper will first describe some of the work in that field  and more particularily the principle of cooperative coevolution  which seems to be the most popular type of method today. 
　we then expose what we feel are possible drawbacks of cooperative evolution  and propose a simpler way to adapt the canonical genetic algorithm to the generation of populations. we describe an experiment  based on the  blind mice  problem  and show that while an adapted genetic algorithm works pretty well with that problem  cooperative coevolution has more difficulties. finally  we give an explanation for these difficulties and for the difference in behaviour with the simpler genetic algorithm. 
1  some of the  related work 
the simplest way to evolve a team of collaborating agents is to have all agents be identical  that is  to have homogeneous populations. these methods are not really different from individual evolution  except at evaluation time: to evaluate a given genotype  n agents are created out of this genotype instead of just one  and the resulting population is evaluated. then start again with a different genotype  etc. while being very rigidly constrained  this method makes perfect sense in situations where one does not need heterogeneity at all. this method was used by  n.zaera et  /.  1  to evolve small groups of fish-like animats  controlled by neural networks  to perform extremely simple tasks  dispersion  aggregation  etc. . 
　a similar method was used by  luke  1  to evolve competitive teams of soccer players for the robocup competition 
 kitano et a/.  1 . the author used an adapted version of genetic programming  koza  1. there was also an attempt at introducing a limited degree of heterogeneity by decomposing teams into small sub-teams  defenders  attackers  etc.  and evolving different program trees for each such subteams. however  because of the enormous search space  and of the delays imposed by the robocup server software   this approach proved intractable in practice: gp runs took days to produce meaningful results. lack of time thus prevented the semi-heterogeneous teams from outperforming homogeneous teams. 
　a way to obtain some degree of heterogeneity is to have only one population and make it change gradually over time  replacing some agents by others based on some evaluation method. these new agents can be obtained by crossover or by duplication with mutation. if there is a way to evaluate the impact of one given individual  it is perfectly possible to perform a simple genetic algorithm over the population. this is  in essence  the idea behind classifier systems  iholland and reitman  1    where a set of rules cooperate to control an animat  and where individuals are evaluated after the animates performance through a credit-sharing system. 
　in the same vein  we proposed a simple scheme in  imiconi  1    in which all agents were given an arbitrary index  and agents of index k could only mate with agents whose 
　
multiagent systems 	1 
　
indices fell within the  k-r; k+r  range. evaluation occured simply by replacing one of the two parents by the offspring  then the second parent  and keeping the best of these two populations  with the possibility of discarding any changes if it decreased the performance of the system . this simple algorithm led to the emergence of sub-species that appeared  grow and shrank according to the needs of the population. an interesting feature of this algorithm was its incrementality  which allowed for long-term  adaptive evolution of the system. 
　trying to obtain fully heterogeneous systems brings us to another level of complexity  right into the realm of cooperative coevolution. while coevolution has been most frequently applied in a competitive way  by confronting individuals to each other and using the result of this confrontation as an evaluation for individuals   it can also be used in a cooperative way  in order to evolve sets of collaborating agents. 
　in cooperative coevolutionary ipotter and dejong  1  methods  each agent within the system is actually taken from a hidden subpopulation  or pool. to evaluate a given individual  it is associated with a set of collaborators  one from each other pool  and the resulting population is evaluated as a whole. the resulting score is then attributed to the currently evaluated individual. based on this evaluation method  the classical ga cycle  evaluate  select and reproduce  is applied to each pool in turn  as many times as needed. in the first version of the cooperative coevolutionary algorithm  ccga-1  collaborators are chosen by taking the best individual from every pool. however  in the ccga-1 version  evaluation is refined by re-evalluating every agent with random collaborators  then taking the better score obtained between these two evaluations. the number of collaborators  the way these collaborators are chosen  the way the overall score is computed  averaging the different scores  or taking the best score  or taking the worst score  etc.  are important parameters that can influence the performance of the algorithms. the influence of these parameters has been studied to some extent by  wiegand et ai  1   but this study applied only to simple function optimization problems with only two variables. 
　as happens frequently with good ideas  cooperative coevolution has been  re- discovered a number of times under different names. enforced subpopulations  esp   for example  are exactly like cooperative coevolution  in which each agent is evaluated with only one set of collaborators: the best agents from all other pools. in other words  esp is the ccga-1 algorithm. while this method was initially devised for the evolution of neural networks  gomez and mikkulainen  1   it was successfully applied to multi-agent evolution by  yong and mikkulainen  1   who used it in a predator-prey simulation. the algorithm managed to find efficient strategies for predators  such as having two predators  chase  the prey while another one blocked it. 
　for some reason  the idea of simply using the standard genetic algorithm to whole populations seems to have fallen slightly out of fashion. the most probable reason is that it is simply too obvious to be talked about. the second reason is that it does have intrinsic drawbacks  such as a more massive search space. the third one is that it requires a few modifications to be adapted to the evolution of populations. all these aspects are discussed in section 1. 
1 two methods for evolving heterogeneous populations 
1 cooperative coevolution 
cooperative coevolution is quite an elaborate mechanism. intuition indicates  and evidence confirms  that by focusing on each and every agent  it requires a huge number of evaluations to converge towards a solution. this algorithm concentrates on optimizing each individual agent in regard with the rest of the population; it is rather different from the more holistic approach of traditional gas  in which full genomes are manipulated  and the  co- adaptation of genes emerges naturally from selection  crossover and mutation - at least  in theory. 
　why would it not be possible to simply use traditional gas for the generation of multi-agent systems  regarding whole systems  not just each agent within them  as individuals  a simple answer is that this approach leads to very large genotypes  since the genotype for an  individual  has to code for several agents instead of just one  and the resulting search space might become intractable for gas. cooperative coevolution can thus be seen as a simple way to decompose a big problem into several smaller ones  even though these smaller problems are still strongly interlaced with each other. 
　however cooperative coevolutionary algorithms seem to have an important drawback: they basically evaluate each agent by assessing its impact on the performance of the whole system. the problem is that when the number of agents within the system grows  the influence of one single agent over the system's performance tends to decrease  thus possibly making its assessment more difficult. this may become troublesome when the problem has a stochastic component  as is the case in many simulations. in this case  evaluating the same population several times can lead to different results. the consequence of this may be more important than one might think  as we will see below. 
　but first  it might be interesting to see how classical genetic algorithms can be adapted to the evolution of populations  and whether these population-oriented genetic algorithms can compete with cooperative coevolution. 
1 	population-oriented genetic algorithms 
genetic algorithms work by evaluating individuals  selecting some of them according to their performance  crossing them together and mutating them  and starting over again. is possible to apply exactly the same method to whole multi-agent systems. we can evaluate populations  cross them together  thus creating new populations that inherit agents from both parents   mutate them by changing one of their agents  etc. 
　however  the fact is that multi-agent systems are not simple individuals. they do have an obvious level of decomposition  the agent   and this can be exploited in several ways. 
　the most obvious idea is that in order to cover the search space efficiently  one must not only make new populations out of existing agents  one must also create new agents. to do this  we may introduce an inner crossover operator that allows us to cross two agents together. thus  when creating a new population by importing agents from both parents  some of these imported agents would actually the result of an inner crossover between agents from the parents. 
　
1 	multiagent systems 
　

figure 1: normal crossover  top  can be  spiced up  with inner crossover between individual agents  bottom . this allows for the creation of new agents  which is necessary to cover the search space efficiently 
　it is possible to make an analogy with traditional gas: from the viewpoint of the whole population  these  crossed  agents have some similarity with bit-wise mutations in the standard genetic algorithm. they are part of the children's genotype  yet they were not present in any of the parents' genotypes. however  these are not exactly random mutations  since the genetic material still comes from the parents' genotypes. this suggests that at first sight   inner crossover  rate should be slightly higher than the usual mutation rate in a classical ga  usually about 1%-1% for each bit . 
　another possibility is to enhance traditional crossover by occasionally swapping agents between populations in the final offspring. this  too  could allow for a better covering of the search space. however  we will not explore that possibility in the present paper. 
1 	application: the blind mice problem 
1 	description of the experiment 
the experiment presented here is based on the  blind mice  problem. a flock of mice  controlled by simple feed-forward neural networks  have to escape a number of cats running after them in a toroidal world. 
now the  game  has three very simple rules: 
1. the cats can see the mice and always run after the closest mouse around. 
1. the mice run faster than the cats. 
1. the mice can not see the cats. neither can they see each other  they are  blind  . their only input is a pair of numbers: the x and y coordinates of the center of the flock. 
　when a cat touches a mouse  the cat is teleported to another  random location  and the population's  catch counter  is increased; nothing else is changed  and the simulation is not interrupted in any way. 
　let us consider these rules: they seem to make the problem extremely difficult for the mice. how is it possible to escape predators that can see you  but that you can't sec  running around as fast as possible will just make them bump into any cat coming from the opposite direction. the same is true for random movement strategies. given the enormous asymmetry of information between mice and cats  the survival chances of the poor rodents appear to be desperately thin. even for a human designer  finding a solution to this 

figure 1: the successful strategy. mouse d attracts the cats  mouse b plays a  balancing'* role  and other mice move together in a tight flock 
problem is not a trivial task. yet  as we will sec below  evolution managed to come up with an elaborate solution to this problem. 
　in our experiments  the mice are controlled by simple feedforward neural networks with 1 inputs  1 outputs and 1 hidden neurons. the two inputs are the coordinates of the center of gravity of the flock  that is  the sum of the x-  resp. y-  coordinates of all mice  divided by the number of mice. the two outputs are two real numbers in the  -1; +1  range  indicating the horizontal and vertical speed of the mouse. all weights are real numbers in the  -1; +1  range. all simulations use 1 cats. 
1 	the evolved strategy 
there seems to be an optimal strategy for this problem. this strategy emerged in all successful runs  sometimes with variants. this strategy is described in figure 1. no other strategy led to a really efficient behaviour. 
　let us explain this strategy: as we can see  most mice are aggregated together and move in a tight flock. this minimizes the probability that a  stray  cat might touch them  but it is not sufficient in itself to ensure a minimal catch rate. the really important behaviour is that of the mouse labelled d  the  dancer  . 
　this mouse has a strange behaviour: it seems to revolve around the rest of the flock  but not in a strictly circular fashion. instead  it constantly bounces around the flock  always staying at a respectable distance from it  and moving very fast along its path. the purpose of this behaviour becomes obvious when one sees the position of the cats: they are all following this  dancing  mouse  because it is simply the closest to them  thus leaving the rest of the flock alone. 
　in other words  the purpose of this dancing behaviour is simply to attract all the cats. the dancer moves very fast  so that it cannot be touched by cats   but along a sinuous path  so that: 
　
multiagent systems 	1 
　
- it can  drag  cats more efficiently in the initial stage  when cats and mice are at random position 
- it never gets too far from the cats  which allows the cats to follow it endlessly even though it runs much faster than them. 
　the final touch of this strategy can be seen in the behaviour of the mouse labelled b  as  balance    even though it didn't appear in all successful runs. this mouse also revolves around the flock  but much closer to it. in some runs it moves around the flock in a circular fashion  in other runs it bounces around it  but it usually stays on the opposite side of the flock with respect to the dancer. we believe that this mouse has a ''balancing  role  in that it counterbalances the effect of the dancer on the position of the center of gravity of the population  thus allowing the flock to be more stable. 
　many variants appeared  such as having several dancers  or no balancer. but the essential traits of the strategy were consistent: aggregation of most mice  except for one or a few to attract the cats away from the flock. 
　note that this strategy is very interdependent: each agent's performance is highly dependant on other agents' behaviour. this is even more true when you consider that in oider to behave that way  they must calculate their trajectories out of only one input: the position of the center of gravity of the whole flock  which is based on the position of all other agents. this fact plays a significant role in the results described below. 
1 experimental results 
1 	experimental settings 
we used two algorithms for this problem: a simple genetic algorithm  adapted with an inner crossover operator  as described above   and a full-featured cooperative revolutionary algorithm. both methods were used with 1  then with 1 mice. we used 1 populations of 1  resp. 1  mice for the first algorithm  and 1  resp. 1  pools of 1 mice for the second one. each algorithm was run several times with different random seeds. 
　in the first algorithm  reproduction of populations occured through tournament selection and 1-point crossover at a rate of 1%. every time two populations were thus crossed together  an inner crossover rate of 1% was applied  meaning that each mouse in the offspring had a 1% chance to be the result of the crossing of the parents' corresponding mice. in the second algorithm  reproduction within each pool occured by tournament selection and 1-point crossover at a rate of 1%  which proved to be the most efficient. in both algorithms  mutation appeared only when crossing two mice together  by choosing a new random value for a connection weight with a 1% probability. 
　note that in both method  we use a limited form of elitism  in that the best individual from a given generation was preserved in the next generation. this ensured better performance - and made the obtained results even more puzzling  as explained below. 
　finally  cooperative coevolution specifies that each agent must be evaluated with a set of collaborators. several sets of 

figure 1: performance of a population-adapted genetic algorithm  with 1 mice  top  and 1 mice  bottom . the y-axis indicates the number of mice catched during this evaluation round  while the x-axis indicates the number of evaluations. both the fitness of the best population and the average fitness of all populations are shown. 
collaborators can be used in turn in order to refine the evaluation  and the final result can be calculated from these successive evaluations in various ways  average  best  random...  however  we found that with this problem  increasing the number of collaborators  and thus the number of evaluation rounds  brought absolutely nothing  and was even damaging if the final score was anything else than the best score found. the most successful method was simply to evaluate each mouse by joining it with the best individual from each pool  exactly as in the enforced subpopulations algorithm. this is not a surprising result  however  as we will explain it below. 
1 	comparison of results 
the first algorithm  simple genetic algorithm with two levels of crossover  proved remarkably efficient with this problem. all runs led to the strategy described in section 1  whatever the number of mice within the simulation  although of course it took more time with 1 mice than with 1. two typical runs  with respectively 1 and 1 mice  are described in figure 1. 
　cooperative coevolution led to different results. with 1 mice  in some runs  the algorithm failed to evolve any competitive behaviour. in other runs it managed to find the good strategy  but took more evaluations than with the previous algorithm. many runs  however  achieved performance comparable with that of the simple genetic algorithm. with 1 mice  the success rate was much lower. most runs did not converge after 1 evaluations. others converged  then suddenly diverged quickly. all the runs  with 1 or 1 mice  exhibited a intriguing pattern of oscillation. 
　
1 	multiagent systems 
　

	1 	1 1 1c 
figure 1: performance of the best run for a cooperative coevolutionary algorithm. the curves indicate the performance of the best individual in the currently evaluated pool  and the average fitness of individuals in this pool. notice the brutal variations in the curves. 
　the fitness curve described in figure 1 shows a good example of this pattern. it is the result of the most successful run with 1 mice. the population seems to converge towards a better behaviour  but then it suddenly diverges and seems to loose all that had been found. the pattern starts again a few cycles later. 
　this is not what one could expect. in this algorithm  the population remains quite stable  being composed of the best individual from each pool. only one individual can be changed at each cycle  and only for a better one  since the best individual from previous generation is preserved. so how do we explain these brutal changes in fitness happening every now and then  
　our explanation is that this pattern is the result of cooperative revolution's main drawback: when the number of agents grow  and when the result of evaluation undergoes important stochastic variations  its performance is bound to decrease. 
　what happens is that at some point  the algorithm does find a  good  population  and keeps optimizing it by replacing each agent by a better one in turn - as it should do. but then at some point  because of the stochastic variations in the evaluation process  one of these  good  agents is found to perform not as well as another  non-optimal one. in classical evolutionary algorithms  these occasional mistakes arc blurred over a large number of trials and errors  and besides  one population's performance has no impact on another's. but in cooperative coevolution  this exceptional case is sufficient to wreck the behaviour of the whole population  because it can replace an essential agent  say  a dancer  by a poorly performing agent  say  a random wanderer . 
　then other agents from other pools are evaluated in turn  but since the conditions have changed dramatically  other agents are selected instead of the previous  quasi-optimal ones. thus  an error in the selection of one agent has an influence over all other pools. when the evaluation cycle comes back to the first pool  the previously essential agent may not be selected again  because the rest of the population has co-adapted towards a different state. however  it is possible that the this agent is selected again  thus resulting in a sharp increase in performance - until the next  error  in the evolutionary process. this seems to cause the up-and-down oscillations in the fitness curve. 
　there is little we can do about this problem. as we said  increasing the number of collaborators in a classical way  that is  by selecting random sets of collaborators  simply does not work  which is understandable: in this problem  the solution requires highly interdependent behaviours. an agent's performance can only be good if other agents play their role. evaluating each agent by fitting it in a random population is not likely to help reach this state. in a problem with many strongly interdependant agents  adding random collaborators 
doesn't seem to be a helpful solution. * 
　we could devise more elaborate schemes  e.g. using the second-best from each pool as a second set of evaluators  but that would not suppress the problem. it seems that in problems with a non-deterministic component  cooperative coevolution is essentially fragile  or at least  more so than the standard genetic algorithm. 
1 	conclusion 
we have described possible pitfalls in cooperative coevolution  and we have proposed a way to apply the canonical genetic algorithm to populations of agents. we have applied both algorithms to a non-trivial problem  the blind mice problem  that is both conceptually simple and non-trivial to solve. for this problem  the populations-oriented genetic algorithm significantly outperformed cooperative coevolution. we proposed an explanation for the difference in results  based on the intrinsic fragility of cooperative coevolution in regard to excessive stochastic variations in the evaluation proces. 
　as a final note  an old french proverb goes:  quand le chat n'est pas la  les souris dansent   when the cat is away  the mice dance . artificial evolution has demonstrated that the opposite can be true as well. 
