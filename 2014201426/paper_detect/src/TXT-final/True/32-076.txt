 
in this paper we describe two optimization techniques that are specially tailored for information gathering. the first is a greedy minimization algorithm that minimizes an information gathering plan by removing redundant and overlapping information sources without loss of completeness. we then discuss a set of heuristics that guide the greedy minimization algorithm so as to remove costlier information sources first. in contrast to previous work  our approach can handle recursive query plans that arise commonly in practice. second  we present a method for ordering the access to sources to reduce the execution cost. sources on the internet have a variety of access limitations and the execution cost in information gathering is affected both by network traffic and by the connection setup costs. we describe a way of representing the access capabilities of sources  and provide a greedy algorithm for ordering source calls that respects source limitations  and takes both access costs and traffic costs into account  without requring full source statistics. finally  we will discuss implementation and empirical evaluation of these methods in emerac  our prototype information gathering system. 
1 introduction 
the explosive growth and popularity of the world-wide web have resulted in thousands of structured queryable information sources on the internet  and the promise of unprecedented information-gathering capabilities to lay users. unfortunately  the promise has not yet been transformed into reality. while there are sources relevant to virtually any userqueries  the morass of sources presents a formidable hurdle to effectively accessing the information. one way of alleviating this problem is to develop information gatherers  also called mediators   which take the user's query  and develop and execute an effective information gathering plan  that accesses the relevant sources to answer the user's query efficiently. 
1 	search    *this research is supported in part by nsf young investigator award  nyl  iri-1  arpa/rome laboratory planning initiative grant f1-c-1  army aasert grant daah1-1  afosr grant f1-1 and nsf grant iri1. we thank selcuk candan for many helpful comments. 
　several first steps have recently been taken towards the development of a theory of such gatherers in both database and artificial intelligence communities. the information gathering problem is typically modeled by building a virtual global schema for the information that the user is interested in  and describing the accessible information sources as materialized views on the global schema. the user query is posed in terms of the relations of the global schema. since the global schema is virtual  in that its extensions are not stored explicitly anywhere   computing the answers requires rewriting the query such that all the extensional  edb  predicates in the rewrite correspond to the materialized view predicates that represent information sources. several researchers from ai and database communities have addressed this rewriting problem  1  1  1 . recent research by duschka and his co-workers  1  1j subsumes most of this work  and provides a clean methodology for constructing information gathering plans for user queries posed in terms of a global schema. 
　generating source complete plans however is only a first step towards efficient information gathering. a crucial next step  which we focus on in this paper  is that of query plan optimization. the plans produced by dushka's methodology  while complete  in that they will retrieve all  accessible  answers to the query  tend to be highly redundant in that they access any information source that may be remotely relevant to the query. they need to be minimized first by removing redundant sources before being executed. the execution phase also presents several challenges since traditional execution optimization models and techniques  do not apply in the context of information gathering on internet. 
　in this paper we describe the query optimization techniques that we have developed in the context of emerac  a prototype information gathering system that we are developing. figure 1 provides a schematic illustration of the query planning and optimization process in emerac. it contains two steps: logical optimization and execution optimization. for logical optimization  we describe a technique that operates on the recursive plans generated by dushka's algorithm and greedily minimizes them so as to remove access to costly and redundant information sources  without affecting the completeness of the plan. for this purpose  we use the so-called localized closed world  lcw  statements that characterize the completeness of the contents of a source relative to either the global  virtual  database schema or other sources. our techniques are based on an adaptation of sagiv's  method for minimizing datalog programs under uniform equivalence. al-


　　　　figure 1: query planning phases in emerac though there exists some previous research on minimizing information gathering plans using lcw statements  1  1   none of it is applicable to minimization of information gathering plans containing recursion. our ability to handle recursion is significant because recursion appears in virtually all information gathering plans either due to functional dependencies  binding constraints on information sources  or recursive user queries |1 . additionally  in contrast to existing methods  which do pairwise redundancy checks on source accesses  our approach is capable of exploiting cases where access to one information source is rendered redundant by access to a combination of sources together. large performance improvements in our prototype information gatherer  emenic  attest to the cost-effectiveness of our minimization approach. 
　ultimately execution optimization boils down to doing joins between the sources efficiently. this problem differs significantly from the traditional database query optimization problem  as sources on the internet have a variety of access limitations  aka  source capabilities   and the execution cost in information gathering is affected both by network traffic and by the connection setup costs. our second contribution is a way of representing the access capabilities of sources  and a greedy algorithm for ordering source calls that respects source limitations. the algorithm also takes both access costs and traffic costs into account  without requring full source statistics. 
　the paper starts with a brief review of dushka's query plan formation methodology in section 1. section 1 presents plan minimization preliminaries  and section 1 presents our plan minimization algorithm. section 1 explains how source accesses in the minimized plan can be ordered using the knowledge of sourece access capabilities. section 1 describes empirical evaluation of our ideas in the context of emerac system. section 1 discusses the related work and section 1 presents our conclusions. 
1 	building query plans: background 
suppose our global schema contains the world relation advisor  s  a   where a is the advisor of s. further more  suppose we have an information source a d d b   such that for every tuple  s a  returned by it  a is the advisor of s. this can be represented as a materialized view on the global schema as follows: 

　suppose we want to retrieve all the students advised by weld. we can represent our goal by the query q: 

　　dushcka et. al.  1  1  show how we can generate an information gathering plan that is  maximally contained  in that it returns every query-satisfying tuple that is stored in any of the accessible information sources. this method works by inverting all source  materialized view  definitions  and adding them to the query. the inverse    of the materialized view definition with head is a set of logic rules in which the body of each new rule is the head of the original view  and the head of each new rule is a relation from the body of the original view. when we invert our definition above  we get: 

　when this rule is added to the original query we effectively create a datalog1 program whose execution produces all the tuples satisfying the query. 
constrained sources & recursion: the materialized view inversion algorithm can be modified in order to model databases that have binding pattern requirements. suppose we have a second information source  condb that requires the student argument to be bound  and returns the advisor of that given student. we denote this in its view as follows: 
　　　　　　　　notation denotes that s must be bound for any query sent to c o n d b . a straightforward inversion of this source will get us a rule of the form: 

which is ununexecutable as s is not bound. this is handled 
by making up a new relation called dom whose extension is made to correspond to all possible constants that can be substituted for s. in our example  assuming that we have both the addb source and the con db source  the complete plan for the query  which we shall refer to as p  is: 

　notice that all extensional  edb  predicates in the progam correspond to source predicates  materialized views . notice also the presence of dom s  relation in the rule r1. rules 1  r1 and r1 define the extension of dom by collecting all possible constants that can be derived from source calls. finally  note that rule r1 is recursive  which makes the overall plan recursive  even though the original query as well as the source description are non-recursive. given the ubiquitousness of constrained sources on the internet  it is thus important that we know how to handle recursive information gathering plans. 
1 	plan minimization preliminaries 
the plan v above accesses two different advisor databases to answer the query. it would be useful to try and cut down redundant accesses  as this would improve the execution cost of the plan. to do this however  we need more information about the sources. while the materialized view characterizations of sources explicate the world relations that are respected by 

lambrecht  kambhampati  and gnanaprakasam 	1 

each tuple returned by the source  there is no guarantee that all tuples satisfying those properties are going to be returned by that source. 
　one way to support minimization is to augment the source descriptions with statements about their relative coverage  using the so-called localized closed world  lcw  statements . an lcw statement attempts to characterize what information  tuples  the source is guaranteed to contain in terms of the global schema. suppose  we happen to know that the source addb is guaranteed to contain all the students advised by weld and hanks. we can represent this information by the statement  note the direction of the arrow : 
addb s. a  
addb s  a  
pair-wise rule subsumption: given the lcw statement above  intuitively it is obvious that we can get all the tuples satisfying the query q by accessing just a d d b . we now need to provide an automated way of making these determinations. suppose we have two datalog rules  each of which has one or more materialized view predicates in its body that also have lcw statements  and we wish to determine if one rule subsumes the other. the obvious way of checking the subsumption is to replace the source predicates from the first rule with the bodies of their view description statements  and the source predicates from the second rule with the bodies of the lcw statements corresponding to those predicates. we now have the transformed first rule providing a  liberal  bound on the tuples returned by that rule  while the transformed second rule gives a  conservative  bound. if the conservative bound subsumes the liberal bound  i.e.  if the transformed second rule  contains   entails  the transformed first rule  we know that second rule subsumes the first rule. duschka  1  shows that this check  while sufficient  is not a 
necessary condition for subsumption. he proposes a modified version that involves replacing each source predicate s with sav in the first rule  and with in the second rule  where v is the view description of s  and is the conjunction of lcw statements of .s. if after this transformation  the second rule contains the first  then the first rule is subsumed by it.1 
minimization under uniform equivalence: pair-wise rule subsumption checks alone are enough to detect redundancy in non-recursive plans 1  1   but are inadequate for minimizing recursive plans. specifically  recursive plans correspond to infinite union of conjunctive queries and checking if a particular rule of the recursive plan is redundant will involve trying to see if that part is subsumed by any of these infinite conjuncts  1  pp. 1 . we instead base our minimization process on the notion of uniform containment for datalog programs  presented in  1 . to minimize a datalog program  we might try removing one rule at a time  and checking if the new program is equivalent to the original program. two datalog programs are equivalent if they produce the same result for all possible assignments of edb predicates f 1 . checking equivalence is known to be undecidable. two datalog programs are uniformly equivalent if they produce the same result for all possible assignments of edb and 1db predicates. uniform equivalence is decidable  and implies equivalence. sagiv  offers a method for minimizing a datalog program under uniform equivalence that we illustrate by an example  and later adapt for our information gathering plan minimization . suppose that we have the following datalog program: 


　　we can check to see if rl is redundant by removing it from the program  then instantiating its body to see if the remaining rules can derive the instantiation of the head of this rule through simple bottom-up evaluation. our initial assignment of relations is . if the remaining rules in the datalog program can derive from the assignment above  then we can safely leave rule rl out of the datalog program. this is indeed the case. given we can assert via rule r1. then  given and we can assert from rule r1 . thus the above program will produce the same results without rule rl in it. 
1 	greedy minimization of recursive plans 
we now adapt the algorithm for minimizing datalog programs under uniform equivalence to remove redundant sources and unnecessary recursion from the information gathering plans. our first step is to transform the query plan such that the query predicate is directly related to the source calls. this is done by removing global schema predicates  and replacing them with bodies of inversion rules that define those predicates  see  1  sec. 1  .1 our example plan p  from section 1  after this transformation with the lcw statements in section 1 looks as follows: 

　we are now ready to consider minimization. our basic idea is to iteratively try to remove each rule from the information gathering plan. at each iteration  we use the method of replacing information source relations with their views or lcw's as in the rule subsumption check  see previous section  to transform the removed rule into a representation of what could possibly be gathered by the information sources in it  and transform the remaining rules into a representation of what is guaranteed to be gathered by the information sources in them. then  we instantiate the body of the transformed removed rule and see if the transformed remaining rules can derive its head. if so  we can leave the extracted rule out of the information gathering plan  because the information sources in the remaining rules guarantee to gather at least as much information as the rule that was removed. the full algorithm is shown in figure 1. 
for our example plan above  we will try to prove that rule 
r.   containing an access to the source c o n d b   is unnecessary. first we remove r1  from our plan  then transform it and the remaining rules so they represent the information gatherered by the information sources in them. for the removed rule  we want to replace each information source in it with a representation of all the possible information that the infor-

replace all global schema predicates in with bodies of their inversion rules. repeat 
let be a rule in that has not yet been considered let be the program obtained by deleting rule r from and simplifying it by deleting any unreachable rules. let let 
if there is a rule  
	such that 	is uniformly contained by 
	then replace 	with 
	until each rule in 	has been considered once 
figure 1: the greedy plan minimization algorithm 
mation source could return. specifically  we want to transform it to this produces: 
	query s a  	:- 	dom s  	condbfs  a  
for the remaining rules  we transform them into which represents the information 
guaranteed to be produced by the information sources in the rules. for our example  we produce: 
r1: query s  a  :r1: query s  a  :r1: querv $  a  
dom s  :- 	addb s.a  dom s    advisor s  a  dom a  :- 	addb s.a  dom a  :- 	advisor s  a  dom a  dom a  	:-
　when we instantiate the body of the transformed removed rule r1  wc get the ground terms: dom  s    condb  s    a    a= weld''  advisories    a  . after evaluating v the remaining rules given with these constants  we find that we can derive query  s    a    using the rule r1  which means we can safely leave out the rule r1 that we've removed from our information gathering program. 
　if we continue with the algorithm on our example problem  we will not be able to remove any more rules. the remaining dam rules can be removed if we do a simple reachability test from the user's query  as they are not referenced by any rules reachable from the query. 
heuristics for ordering rules for removal: the final information gathering plan that we end up with after executing the minimization algorithm will depend on the order in which we remove the rules from the original plan. in the example above  suppose we had another lcw statement: 
	condbfs  a  	advisor s  	a  
　in such a case  we could have removed r1 from the original information gathering plan p  instead of removing r1. since both rules will lead to the generation of the same information  the removal would succeed. once r1 is removed however  we can no longer remove r1. this is significant  since in this case  a plan with rule r1 in it is much costlier to execute than the one with rule r1 in it. the presence of r1 triggers the dom recursion through rules r1...r1  which would have been eliminated otherwise. recursion greatly increases the execution cost of the plan  as it can generate potentially boundless number of accesses to remote sources  see section 1 . we thus consider for elimination rules containing non-recursive predicates before those containing recursive predicates  such as dom terms . beyond this  we also consider any gathered statistics about the access costs of the sources  such as contact time  response time  probability of access etc.  to break ties . 
complexity of minimization: the complexity of the minimization algorithm in figure 1 is dominated by the cost of uniform containment checks. as sagiv  points out  the running time of the uniform containment check is in the worst case exponential in the size of the query plan being minimized. however  things are brighter in practice since the exponential part of the complexity comes from the  evaluation  of the datalog program. the evaluation here is done with respect to a  small  database - consisting of the grounded literals of the tail of the rule being considered for removal. nevertheless  the exponential complexity justifies our greedy approach for minimization  as finding a globally minimal plan would require considering all possible rule-removal orders. 
1 	ordering source calls during execution 
after the minimization phase  the information gathering plan is ready for execution. a crucial practical choice we have to make during the execution of the minimized plans  datalog programs  is the order in which predicates are evaluated. ultimately plan execution in our context largely boils down to doing joins between the sources efficiently. although there is a large body of work on join-ordering   most of it assumes that all data sources are fully relational databases  ignores source access costs  concentrating only on the traffic costs   and assumes the availability of elaborate source statistics. such approaches are not particularly suited for emcrac. in the information gathering domain  the assumption that information sources are fully relational databases is rarely valid  as sources tend to have a variety of access limitations. source access costs  connection set up costs etc.  can outweigh the traffic costs. finally  due to the decentralized nature of internet  full statistics about sources arc rarely available. we now discuss how emcrac represents the source limiations  and provide a greedy algorithm for ordering sources that uses this representation to reduce both traffic and access costs during execution. 
1 	representing source limitations 
on internet  an information source may be a wrapped web page  a form interfaced database  or a fully relational database. a wrapped web page is a www document interfaced through a wrapper program to make it appear as a relational database. the wrapper retrieves the web page  extracts the relational information from it  then answers relational queries. normal selection queries are not supported. a forminterfaced database refers to a database with an html form interface on the web which only answers selection queries over a subset of the attributes in the database. a www airline database that accepts two cities and two dates and returns flight listings is an example of a form interfaced database. 
　in emcrac  we use a simple way to inform the gatherer as to what types of queries on an information source would accept. we use the  $  annotation to identify variables that must be bound  and  %  annotation to identify unselectable 

lambrecht  kambhampati  and gnanaprakasam 	1 

attributes  i.e.  those that must not be bound . thus a fully relational source would be adorned source x  y   a form interfaced web-page that only accepts bindings for its first argument would be adorned source x  %k   while a wrapped web-page source would have all its attributes marked unselectable  represented as source %x %y . finally  a form interfaced web-page that requires bindings for its first argument  and is able to do selections only on the second argument would be adorned as source $x  y  %z . 
　given a source with annotations s1  $ar  %y  z   only the binding patterns of the form  are feasible  where   -   stands for either bound or /ree argument . similarly  we are not allowed to push selection constraints on y to the source 
  they must be filtered locally . thus the call  must be executed as  filtered locally with the binding on y. 
 finally  given two binding patterns a and for a source 1   is said to be more general than  written 
if every selectable  non '% -annotated  variable that is free in is also free in a  but not vice versa. finally  we define as as the number of bound variables in a that are not %-annotated. notice that holds only between binding patterns of the same source while can be used to relate binding patterns of different sources. 
1 	a greedy algorithm for ordering source calls 
the normal heuristic for ordering subgoals in a datalog program is to use  'bound is easier  assumption   and call sources with more specific binding patterns before those with more general ones. the idea is to reduce costs associated with data transfer  number of tuples transferred . it turns out that bound-is-easier assumption can wind up increasing the connection and access costs. to elaborate  reducing the network traffic involves accessing sources with less general binding patterns. this in turn typically increases the number of sep arate calls made to a source  and leads to increased access costs. 
　emerac source-call ordering method considers the connection costs to be of primary importance and the network traffic costs to be of secondary importance. to reduce connection costs  we attempt to access sources with the most general feasible binding patterns. to take traffic costs into account  we also maintain a table htbp of least general  w.r.t.  source binding patterns that are still known to be high-
tralfic producing. our algorithm  shown in figure 1 attempts to pick  for each source  the most general feasible binding pattern that is neither equal to  nor more general than any binding pattern for that source listed in htbp. an assumption motivating this approach is that while full source statistics are rarely available  one can easily gain partial information on the types of binding patterns that cause excessive traffic. 
　if all of the feasible binding patterns of all sources are found to be in htbp in a given step  then the algorithm selects the source with the binding pattern containing most number of bound variables that are not %-annotated  adopting the  bound-is-easier  assumption . this selection then gives rise to further bound variables  enlarges vr in the algorithm above   and makes low traffic binding patterns feasible at the next step. 
　when the algorithm terminates successfully  the array c specifies which sources are to be called in each stage  and what binding patterns are to be used in those calls. execution 
1 	search 
inputs: fbp: table of forbidden binding patterns 
htbp: table of high traffic binding patterns 
v := all variables bound by the head; 
	array where lists sources chosen at 	stage; 
           array where lists sources postponed at stage for i := 1 to m  where m is the number of subgoals  do begin c i  for each unchosen subgoal s do begin 
b := all feasible binding patterns for 1 w.r.t. v and fbp sorted using   relation. for each do begin 
then begin 
push s with binding pattern into mark s as  chosen ; add to v all variables appearing in s; 
end 
end if 	and s is not chosen 
then push 	into  where has the maximuni value; 
end if 
then begin 
take the source with maximum value and push it into add to v all variables appearing in 1; 
   else fail end 
return the array 
figure 1: a greedy source call ordering algorithm that considers both access costs and traffic costs. 
involves issuing calls to sources with the specified binding pattern; where each bound variable in the binding pattern is instantiated to all values of that variable collected upto that point during execution. if the bound variable is a %-annotated variable  then the call is issued without variable instantiation  and the filtering on the variable values is done locally. the tuples returned by the source calls in each stage are locally joined. 
　notice that each element of c is a  possibly non-singleton  set of source calls with associated binding patterns  rather than a single source call . this parallelism supports  bushy join trees  |1| and cuts down the overall time wasted during connection delays. the complexity of our ordering algorithm is  where n is the length of the rule. 
　it is worth noting that the behavior of our algorithm depends on the contents of the htbp table. when htbp contains no binding patterns  the algorithm essentially concentrates on reducing the source accesses  similar to  . when all source binding patterns are listed in htbp  the algorithm winds up focusing on the network traffic  and reduces to a variant of conjunct ordering by bound-is-easicr assumption 
. 
1 implementation and evaluation 
emerac is a prototype information gathering system underdevelopment that implements the ideas in this paper. it is written in java  and is intended to be a library used by applications that need a uniform interface to multiple information sources. emerac is internally split into two parts: the query planner and the plan executor. the default planner uses duschka's 


 a  cumulative costs of lcw vs.  b  cumulative costs of lcw vs.  c  effect of unsubsumed constrained naive  artificial sources  naive  artificial sources  sources on lcw 
figure 1: results characterizing utility of minimization algorithm. 

|1| plan generation techniques coupled with our plan minimization techniques. the plan is executed by traversing the relational operator graph   1 corresponding to the minimized plan. when a union node is encountered during traversal  new threads of execution arc created to traverse the children of the node in parallel. use of separate threads also allows us to return answers to the user asynchronously. the executor uses the algorithm in section 1 to determine the order to access each information source in a join of multiple sources  as described in section 1. recursion in the relational operator graph is controlled by using a depth-limit. 
　we used the prototype implementation of emeruc to evaluate the effectiveness of the optimization techniques proposed in this paper. we used two sets of experimental data. the first were a set of small artificial sources containing 1 tuples each. our second data set was derived from the university of trier's database and logic programming  dblp  online database  which contains bibliographical information on database-related publications. individual sources used in the experiments corresponded to different subsets of dblp data  ranging from 1 to 1 tuples . in each case  some of the sources are unconstrained  while others have binding restrictions  leading to recursive plans . to normalize for differences caused by individual source implementations  we extracted the data into tables which we stored on disk as java serialized data. all experiments were conducted using a simple wrapper  written in compiled java  to return the contents of the serialized tables. 
　the sources delay answering each query for a set period of time in order to simulate actual latency on the internet. in all our experiments  this delay was set to 1 seconds  which is quite reasonable in the context of current day internet sources. 
utility of minimization: to see how the planner and executor performed with and without minimization  we varied the number of duplicate information sources available and relevant to the query  and compared the total time taken for optimization  if any  and exection. given that the minimization step involves an exponential  uniform containment  check  it is important to ensure that the time spent in minimization is made up in improved execution cost. notice that we are looking at only the execution time  and ignoring other costs  such as access cost for premium sources   which also can be reduced significantly with the minimization step. the naive method simply builds and executes source complete plans. the  lcw  method builds source complete plans  then applies the minimization algorithm described in section 1 before executing the plans. for both methods  we support fully parallel execution at the union nodes in the r/g graph. since in practice  recursive plans are handled with depth bounded recursion  we experimented with a variety of depth limits  i.e.  the number of times a node is executed in the rule-goal graph   starting from i  which in essence prunes the recursion completely . 
　the plots in figure 1 show the results of our experiments. plot a is for the artificial sources  and shows the relative time performances of lcw against the naive algorithm when the number of redundant constrained sources is increased. in this set of experiments  lcw statements allow us to prove all constrained sources to be redundant  and the minimization algorithm prunes them. the y-axis shows the cumulative time taken for minimization and execution. we note that the time taken by the lcw algorithm remains fairly independent of recursion depth as well as number of constrained sources. the naive algorithm  in contrast  worsens exponentially with increasing number of constrained sources. the degradation is more pronounced for higher recursion depths  with the lcw method outperforming the naive one when there are two or more redundant constrained sources. plot b repeats the same experiment  but with the sources derived from the dblp data. the sources are such that the experimental query returns upto 

lambrecht  kambhampati  and gnanaprakasam 	1 

1 tuples. the experiment is conducted for recursion depth limits 1 and 1. we note once again  that lcw method remains fairly unaffected by the presence of redundant constrained sources  while the naive method degrades exponentially. plot c considers dblp data sources in a scenario where some constrained sources are left unsubsumed after the minimization. as expected  lcw performance degrades gracefully with increased number of constrained sources. naive algorithm would not have shown such graceful degradation no sources would be removed through subsumption. 
　although we have not completed a formal evaluation of the source ordering strategy described in section 1  informal experiments with artificial sources indicate that the technique produces plans with better cumulative access and traffic costs than those offered by ordering based on bound-is-easier assumption. 
1 	related work 
early work on optimizing information gathering plans  c.f.  1   combined the phases of query plan generation and optimization and posed the whole thing as a problem of search through the space of different executable plans. by starting with duschka's work  1  1  which gives a maximally contained plan in polynomial time  and then optimizing it  we make a clean separation between generation and optimization phases. 
　friedman and weld 1  offer an efficient algorithm for minimizing a non-recursive query plan through the use of lcw statements. their algorithm is based on pair-wise subsumption checks on conjunctive rules. recursive rules correspond to infinite unions of conjunctive queries  and trying to prove subsumption through pair-wise conjunctive rule containment checks will not be decidable. the approach in duschka  also suffers from similar problems as it is based on the idea of conjunctive  un foldings of a query in terms of source relations f 1 . in the case of recursive queries or sources with binding restrictions  the number of such foldings is infinite. in contrast  our minimization algorithm is based on the notion of uniform containment for recursive datalog programs. this approach can check if sets of rules subsume a single rule. 
thus it can minimize a much greater range of plans. 
　our source-access ordering technique assumes that statistics regarding source relations are not easily available  and thus traditional join-ordering strategies are not applicable. an interesting alternative is to try and learn the source statistics through experience. zhu and larson 1  describe techniques for developing regression cost models for multi-database systems by selective querying. adali et. al  discuss how keeping track of rudimentary access statistics can help in doing cost-based optimizations. 
1 	conclusion 
in this paper  we considered the query optimization problem for information gathering plans  and presented two novel techniques. the first technique makes use of lcw statements about information sources to prune unnecessary information sources from a plan. for this purpose  we have modified an existing method for minimizing datalog programs under uniform containment  so that it can minimize recursive information gathering plans with the help of source subsumption information. the second technique is a greedy algorithm for or-
1 	search 
dering source calls that respects source limitations  and takes both access costs and traffic costs into account  without requring full source statistics. we have then discussed the status of a prototype implementation system based on these ideas called emerac  and presented an evaluation of the effectiveness of the optimization strategies in the context of emerac. our current directions involve integrating the minimization and source-call ordering phases more tightly  explicitly modeling and exploiting cost/quality tradeoffs  dealing with runtime exceptions such as sources that become inaccessible  as well as run-time opportunities such as the use of caches  1 . we are also exploring the utility of learning rudimentary source models by keeping track of time and solution quality statistics  and the utility of probabilistic characterizations of coverage and overlaps between sources. 
