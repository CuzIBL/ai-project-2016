 
as distributed systems of computers play an increasingly important role in society  it will be necessary to consider ways in which these machines can be made to interact effectively. especially when the interacting machines have been independently designed  it is essential that the interaction environment be conducive to the aims of their designers. these designers might  for example  wish their machines to behave efficiently  and with a minimum of overhead required by the coordination mechanism itself. the rules of interaction should satisfy these needs  and others. formal tools and analysis can help in the appropriate design of these rules. 
we here consider how concepts from fields such as decision theory and game theory can provide standards to be used in the design of appropriate negotiation and interaction environments. this design is highly sensitive to the domain in which the interaction is taking place. different interaction mechanisms are suitable for different domains  if attributes like efficiency and stability are to be maintained. 
1 	machines controlling and sharing resources 
computers are making more and more decisions in a relatively autonomous fashion. telecommunications networks are controlled by computers that decide on the routing of telephone calls and data packets. electrical grids have decisions made by computer regarding how their loads will be balanced at times of peak demand. similarly  research is being done on how computers can react to  and control  automotive and airplane traffic in real time. 
   some of the decisions that these computers are generating are made in concert with other machines. often  this inter-machine consultation is crucial to the task at hand. for example  with personal digital assistants 
   *this research has been partially supported by the israeli ministry of science and technology  grant 1 . 
1 	invited speakers 
 pdas   the individual's palmtop computer will be expected to coordinate schedules with others' pdas  e.g.  my software agent determines whether my car has been fixed on time at the garage; if not  it contacts the taxi company  reschedules my order for a cab  and updates my day's schedule . no scheduling will take place without inter-machine communication. rarely will it take place without the resolution of inter-machine conflict  because the humans that these machines represent have conflicting goals . 
   similarly  the concept of intelligent databases relies on sophisticated interactions among autonomous software agents. a user's request for a piece of information may require collecting and synthesizing information from several distributed databases. machines need to formulate the necessary collection of requests  arrange access to the data  which may be partially restricted   and cooperate to get the information where it is needed. 
   even when a computer's tasks do not have to involve other machines  it may be beneficial to involve them. sometimes  for example  we find automated systems controlling resources  like the telecommunications network mentioned above . it is often to the benefit of separate resource-controlling systems to share their resources  e.g.  fiber optic lines  short and long term storage  switching nodes  with one another. 
   all of this inter-machine coordination will be taking place within some kind of interaction environment. there will inevitably be  protocols  for how machines deal with one another. what concerns us here are not the details of how to stuff information into a packet on the network; it's not even the higher-level issue of how agents will communicate with one another  in a common language  or perhaps using translation filters . rather  once we assume that agents can communicate and understand one another  how will they come to agreements  these  interaction rules  will establish the basis for inter-machine negotiation  agreement  coordination  and cooperation. 
¡¡if the inter-machine protocols are primitive and incapable of capturing the subtleties of cooperative opportunities  the machines will act inefficiently. they will make the wrong decisions. the people who depend on those decisions will suffer. 

1 	heterogeneous  self-motivated agents 
in the field of distributed artificial intelligence  dai   researchers explore methods that enable the coherent interaction of computers in distributed systems. one of the major distinctions in dai is between research in distributed problem solving  dps   smith  
1; conry et a/.  1; durfee  1; clark et al.  1   in which the distributed system has been centrally designed  and multi-agent  ma  systems  kraus and wilkenfeld  1; ephrati and rosenschein  1; kreifelts and von martial  1; sycara  1   in which the distributed system is made up of independently designed agents. in dps  there is some global task that the system is performing  and there exists  at least implicitly  a global notion of utility that can constrain or direct agent activity. in ma systems  each agent is concerned only with its own goals  though different agents' goals may overlap   and there is no global notion of utility. the ma system agent  concerned with its own welfare  i.e.  the welfare of its designer  doyle  1    acts accordingly to increase that welfare. 
¡¡the approach of ma system research is particularly appropriate for the kinds of scenarios mentioned above. when the a t & t and m c i computers communicate with the purpose of load balancing their message traffic  each is concerned with its own company's welfare. any interaction environment must take into account that each of these software agents  in coming to an agreement  will be primarily concerned with its own increased benefit from that agreement. we are not looking for benevolent or altruistic behavior from these machines. similarly  these systems of interacting machines tend to be  open   gasser  1   in the sense that the system composition is not fixed. with pdas  for example  new agents  and even new types of agents  will constantly be entering the environment. my pda  to be effective in negotiation and coordination  must be able to deal with these open  dynamic  configurations of agents. research in multi-agent systems is thus the appropriate model with which to analyze these independent software agents and their interactions. 
1 	the aim of the research 
the purpose of the research described in this paper is to consider how we might build machines that are capable of making constructive agreements. we want our machines to interact flexibly. we want them to represent our interests  and compromise when that is to our advantage. we may want them to be secretive at times  not revealing all their information  and we most likely want them to recognize duplicity on the part of others  when possible. in short  we want our agents to faithfully act as our surrogates in encounters with other agents. 
1 	social e n g i n e e r i n g for machines 
when humans interact  they do not do so in a vacuum. 
there are social conventions and laws that constrain their behavior; the purpose of social conventions and laws is to do exactly that. a tax levied on a company that pollutes the air is intended as a disincentive to a certain kind of behavior. positive publicity showered on a philanthropic company provides it with benefit for its behavior. one can think of a complicated system of laws and conventions as a kind of social engineering  intended to produce certain behavior among people. 
¡¡we are interested in social engineering for machines. we want to understand the kinds of negotiation protocols  and punitive and incentive mechanisms  that would motivate individual designers to build machines that act in ways that all those designers find beneficial. the development of  social laws  has parallels with the work of  shoham and tennenholtz  1 . there  however  the social laws are for centrally designed systems of agents  dps   and will not necessarily make sense for independently designed agents. for example  a rule might encourage efficient behavior if everyone followed it  but if any single agent could benefit more by not following the rule  the system as a whole will not be stable. since each of our agents will do what is necessary to maximize its benefit  stability is a critical issue-we need rules that agents will independently find in their best interests to follow. we will return to this issue of stability below. 
1 	t h e setting of standards 
the scenario we consider is as follows. imagine representatives of various companies  agent designers  coming together to agree on interaction protocols for their automated agents. given a particular domain  such as balancing telecommunications traffic among wide area networks  or meeting scheduling   they are presented with various interaction mechanisms  and shown that each mechanism has certain provable properties. for example  one mechanism might arrive at guaranteed globally optimal solutions  but at the cost of one agent possibly doing very badly. another mechanism might ensure that the gap between agents' benefits are minimized  but at the cost of everyone doing a little worse. moreover  it is shown to these company representatives that protocol a is immune to deception: it will be in no one's interest to design a cheating agent that deviates from the protocol in any way  e.g.  by reporting higher  or lower  network traffic than is actually present . the representatives consider the various options  and decide among themselves which protocol to build into their agents. the meeting adjourns  agents are built  and beneficial agreements are reached among them. 
   it turns out that the attributes of a given mechanism are highly sensitive to the domain in which the agents are operating. the rules of interaction that might be appropriate in one domain might be quite inappropriate in another. when those company representatives sit down at the meeting  they need to be told  in this domain  protocol a has properties 1  1  and 1  and is immune to deception. protocol b has properties 1  1  and 1  and is not immune to deception.  our research explores the space of possibilities  analyzing negotiation mechanisms in different domains. when the designers of automated agents meet  this is the kind of information they will need. the alternative to having this analysis is to wander in the dark  and to build negotiation modules 

without understanding their properties. will they result in good deals  could our machines do better  will someone build a deceptive agent that takes advantage of mine  should i  myself  design my agent to be secretive or deceptive  will this further my own goals  our research is intended to answer these kinds of questions. 
¡¡the builders of complex distributed systems  like interconnected networks  shared databases  assembly line monitoring and manufacturing  and distributed processing  can broaden the range of tools that they bring to bear on issues of inter-agent coordination. existing techniques generally rely on the goodwill of individual agents  and don't take into account complex interactions of competing goals. new tools can be applied to the high-level design of heterogeneous  distributed systems through the creation of appropriate negotiation protocols. 
1 	protocol design 
how can machines decide how to share resources  or which machine will give way while the other proceeds  negotiation and compromise are necessary  but how do we build our machines to do these things  how can the designers of these separate machines decide on techniques for agreement that enable mutually beneficial behavior  what techniques are appropriate  can we make definite statements about the techniques' properties  
¡¡the way we have begun to address these questions is to synthesize ideas from artificial intelligence  e.g.  the concept of a reasoning  rational computer  with the tools of game theory  e.g.  the study of rational behavior in an encounter between self-interested agents . assuming that automated agents  built by separate  self-interested designers  will interact  we are interested in designing protocols for specific domains that will get those agents to interact in useful ways. 
¡¡the word  protocol  means different things to different people. as used to describe networks  a protocol is the structure of messages that allow computers to pass information to one another. when we use the word protocol  we mean the rules by which agents will come to agreements. it specifies the kinds of deals they can make  as well as the sequence of offers and counter-offers that are allowed. these are high-level protocols  dealing not with the mechanisms of communication but with its content. protocols are intimately connected with domains  by which we mean the environment in which our agents operate. automated agents who control telecommunications networks are operating in a different domain  in a formal sense  than robots moving boxes. much of our research is focused on the relationship between different kinds of domains  and the protocols that are suitable for each. 
¡¡given a protocol  we need to consider what agent strategy is appropriate. a strategy is the way an agent be-
haves in an interaction. the protocol specifies the rules of the interaction  but the exact deals that an agent proposes is a result of the strategy that his designer has put into him. as an analogy  a protocol is like the rules governing movement of pieces in the game of chess. a strategy is the way in which a chess player decides on 
invited speakers 
his next move. 
1 	the game theory/automated agent match 
game theory is the right tool in the right place for the design of automated interactions. game theory tools have been primarily applied to analyzing human behavior  but in certain ways they are inappropriate: humans are not always rational beings  nor do they necessarily have consistent preferences over alternatives. automated societies  on the other hand  are particularly amenable to formal analysis and design. automated agents can exhibit predictability  consistency  narrowness of purpose  e.g.  no emotions  no humor  no fears  clearly defined and consistent risk attitude   and an explicit measurement of utility  where this can have an operative meaning inside the program controlling the agent . 
¡¡even the notion of  strategy   a specification of what to do in every alternative during an interaction   a classic game theory term  takes on a clear and unambiguous meaning when it becomes simply a program put into a computer. the notion that a human would choose a fixed strategy before an interaction  and follow it without alteration  leads to unintuitive results for a person. moreover  it seems to be more a formal construct than a realistic requirement-do humans consider every alternative ahead of time and decide what to do  on the other hand  the notion that a computer is programmed with a fixed strategy before an interaction  and follows it without alteration  is a simple description of the current reality. 
¡¡of course  neither humans nor computer programs are ideal game theory agents. most importantly  they are not capable of unlimited reasoning power  as game theory often assumes. nevertheless  it seems that in certain ways automated agents are closer to the game theory idealization of an agent than humans are. the work described here  the design of interaction environments for machines  is most closely related to the field of mechanism design in game theory  fudenberg and tirole  
1 . 
1 	attributes of standards 
what are the attributes that might interest those company representatives when they meet to discuss the interaction environment for their machines  this set of attributes  and their relative importance  will ultimately affect their choice of interaction rules. 
¡¡we have considered several attributes that might be important to system designers. 
1. efficiency: the agents should not squander resources when they come to an agreement; there should not be wasted utility when an agreement is reached. for example  it makes sense for the agreements to satisfy the requirement of pareto optimally  no agent could derive more from a different agreement  without some other agent deriving less from that alternate agreement . another consideration might be global optimality  which is achieved when the sum of the agents' benefits are maximized. 
neither kind of optimality necessarily implies the other. since we are speaking about self-motivated agents  who care about their own utilities  not the sum of system-wide utilities-no agent in general would be willing to accept lower utility just to increase the system's sum   pareto optimality plays a primary role in our efficiency evaluation. among pareto optimal solutions  however  we might also consider as a secondary criterion those solutions that increase the sum of system-wide utilities. 
1. stability: no agent should have an incentive to deviate from agreed-upon strategies. the strategy that agents adopt can be proposed as part of the interaction environment design. once these strategies have been proposed  however  we do not want individual designers  e.g.  companies  to have an incentive to go back and build their agents with different  manipulative  strategies. 
1. simplicity: it will be desirable for the overall interaction environment to make low computational demands on the agents  and to require little communication overhead. this is related both to efficiency and to stability: if the interaction mechanism is simple  it increases efficiency of the system  with fewer resources used up in carrying out the negotiation itself. similarly  with stable mechanisms  few resources need to be spent on outguessing your opponent  or trying to discover his optimal choices. the optimal behavior has been publicly revealed  and there is nothing better to do than just carry it out. 
1. distribution: preferably  the interaction rules will not require a central decision maker  for all the obvious reasons. we do not want our distributed system to have a performance bottle-neck  nor collapse due to the single failure of a special node. 
1. symmetry: we may not want agents to play different roles in the interaction scenario. this simplifies the overall mechanism  and removes the question of which agent will play which role when an interaction gets under way. 
¡¡these attributes need not be universally accepted. in fact  there will sometimes be trade-offs between one attribute and another  for example  efficiency and stability are sometimes in conflict with one another  zlotkin and rosenschein  1b  . but our protocols are designed  for specific classes of domains  so that they satisfy some or all of these attributes. ultimately  these are the kinds of criteria that rate the acceptability of one interaction mechanism over another. 
¡¡as one example  the attribute of stability assumes particular importance when we consider open systems  where new agents are constantly entering and leaving the community of interacting machines. here  we might want to maintain stability in the face of new agents who bring with them new goals and potentially new strategies as well. if the mechanism is  self-perpetuating   in that it is not only to the benefit of society as a whole to follow the rules  but also to the benefit of each individual member  then the social behavior remains stable even when the society's members change dynamically. when the interaction rules create an environment in which a particular strategy is optimal  beneficial social behavior is resistant to outside invasion. 
1 	domain theory 
i have several times alluded to the connection between protocols and domains-for a given class of interactions  some protocols might be suitable while others are not. we have found it useful to categorize domains into a three-tier hierarchy of task oriented domains  state oriented domains  and worth oriented domains. this hierarchy is by no means complete  but does cover a large proportion of the kinds of real-world interactions in which we are interested. 
1 	task oriented domains 
these are domains in which an agent's activity can be defined in terms of a set of tasks that it has to achieve. these tasks can be carried out without concern about interference from other agents; all the resources necessary to accomplish the tasks are available to the agent. on the other hand  it is possible that agents can reach agreements where they redistribute some tasks  to everyone's benefit  for example  if one agent is doing some task  he may  at little or no cost  be able to do another agent's task . the domains are inherently cooperative. negotiation is aimed at discovering mutually beneficial task redistribution. 
¡¡the key issue here is the notion of task  an indivisible job that needs to be carried out. of course  what constitutes a task will be specific to the domain. many kinds of activity  however  can be conceived of in this way  as the execution of indivisible tasks. for example  imagine that you have three children  each of whom needs to be delivered to a different school each morning. your neighbor has four children  and also needs to take them to school. delivery of each child can be modeled as an indivisible task. although both you and your neighbor might be interested in setting up a carpool  there is no doubt that you will be able to achieve your tasks by yourself  if necessary. the worst that can happen is that you and your neighbor won't come to an agreement about setting up a carpool  in which case you are no worse off than if you were alone. you can only benefit  or do no worse  from your neighbor's existence. 
¡¡assume  though  that one of my children and one of my neighbor's children both go to the same school  that is  the cost of carrying out these two deliveries  or two tasks  is the same as the cost of carrying out one of them . it obviously makes sense for both children to be taken together  and only my neighbor or i will need to make the trip to carry out both tasks. 
¡¡what kinds of agreements might we reach  we might decide that i will take the children on even days each month  and my neighbor will take them on odd days; perhaps  if there are other children involved  we might have my neighbor always take those two specific children  while i am responsible for the rest of the children  his and mine . another possibility would be for us to flip a coin every morning to decide who will take the children. 
an important issue  beyond what deals can be reached  is how a specific deal will be agreed upon  see section 1 below . 
¡¡consider  as further examples  the postmen domain  the database domain  and the fax domain  these domains are described in more detail  and more formally  in a paper  zlotkin and rosenschein  1a  that appears in these proceedings . in the postmen domain  each agent is given a set of letters to deliver to various nodes on a graph; starting and ending at the post office  the agents are to traverse the graph and make their deliveries. there is no cost associated with carrying letters  they can carry any number   but there is a cost associated with graph traversal. the agents are interested in making short trips. agents can reach agreements to carry one another's letters  and save on their travel. 
¡¡the database domain similarly assigns to each agent a set of tasks  and allows for the possibility of beneficial task redistribution. here  each agent is given a query that it will make against a common database  to extract a set of records . a query  in turn  may be composed of subqueries  i.e.  the agent's tasks . for example  one agent may want the records of  all female employees making over $1 a year   while another agent may want the records of  all female employees with more than three children.  both agents share a sub-task  the query that involves extracting the records of all female employees  prior to extracting a subset of those records . by having only one agent get the female employee records  another agent can lower its cost. 
¡¡the third example is the fax domain. it appears very similar to the postmen domain  but is subtly different. in the fax domain  each agent is given a set of faxes to send to different locations around the world  each fax is a task . the only cost is to establish a connection. once the connection is made  an unlimited number of faxes can be sent. of course  if two agents both have faxes to send to paris and to london  they may redistribute their faxes  with one sending all the faxes to paris and the other sending all the faxes to london. 
¡¡despite the seemingly minor differences in these domains  the attributes of suitable protocols are very different for each. 
1 	state oriented domains 
the state oriented domain  sod  is the type of domain with which most ai research has dealt. the blocks world  for example  is a classic state oriented domain. sods are a superset of tods  i.e.  every tod can be cast in the form of an sod . 
¡¡in an sod  each agent is concerned with moving the world from an initial state into one of a set of goal states. there is  of course  the possibility of real conflict here. because of  for example  competition over resources  agents might have fundamentally different goals. there may be no goal states that satisfy all agents. at other times  there may exist goal states that satisfy all agents  but that are expensive to reach-and which require the agents to do more work than they would have had to do in isolation. mechanisms for dealing with state 
oriented domains are examined in  zlotkin and rosen-
invited speakers 
schein  1 . again  negotiation mechanisms that have certain attributes in task oriented domains  e.g.  efficiency  stability  do not necessarily have these same attributes in state oriented domains. 
1 	worth oriented domains 
worth oriented domains  wod  are a generalization of 
state oriented domains  where agents assign a worth to each potential state  which establishes its desirability for the agent  as opposed to an sod  in which the worth function is essentially binary-all non-goal states have zero worth . this establishes a decision theoretic flavor to interactions in a wod. one example of a wod is the the world  as discussed in  pollack and ringuette  1 . the key advantage of a worth oriented domain is that the worth function allows agents to compromise on their goals  sometimes increasing the overall efficiency of the agreement. every sod can be cast in terms of a wod  of course  with binary worth function . negotiation mechanisms suitable for an sod need not be suitable for a wod  that is  the attributes of the same mechanism may change when moving from an sod to a wod . 
1 	the building blocks of a negotiation mechanism 
designing a negotiation mechanism  the overall  rules of interaction   is a three-step process. first  the agent designers must agree on a definition of the domain  then agree on a negotiation protocol  and finally propose a 
negotiation strategy  
1 	domain definition 
the complete definition of a domain should give a precise specification to the concept of a goal  and to the agent operations that are available. for example  in the postmen domain  the goal of an agent is the set of letters that the agent must deliver  as in any tod  the goal is the set of tasks that need to be carried out   along with the requirement that the agent begin and end at the post office. 
¡¡the specification of agent operations that are available define exactly what an agent can do  and the nature of those actions' cost. in the postmen domain  again  it is part of the domain definition that an agent can carry an unlimited number of letters  and that the cost of a graph traversal is the total distance traveled. 
¡¡this formal domain definition is the necessary first step in analyzing any new domain. if agents are negotiating over sharing message traffic in telecommunications networks  it is necessary to specify completely what constitutes a goal  and what agent operations are available. similarly  pdas involved in negotiations over schedules need their goals and operators precisely defined. 
1 	negotiation protocol 
once the domain has been specified  we need to specify the negotiation protocol  which establishes the rules of interaction among agents. here  we need to be concerned both with the space of possible deals  and with the negotiation process. 

  space of possible deals: first  we must spec-ify the set of candidate deals. specifically  what kinds of agreements can the agents come to  for example  we might restrict our agents to only discussing deals that do not involve redundant work  e.g.  in the carpool example  the parents will not consider deals that have two parents visiting the same school . similarly  we might specify that deals cannot involve tossing a coin. 
  negotiation process: given a set of possible deals  what is the process that agents can use to converge to agreement on a single deal  in other words  what are the rules that specify how consensus will be reached  how will one agreed-upon deal be differentiated from the other candidates  in the carpool example  we might specify that each parent will in turn offer a delivery schedule and assignments; the next parent can either accept the offer  or reject it and make his own counter-offer. we might also allow as part of the negotiation process that any parent can  at any point  make a  take-itor-leave-it  proposition  that will either be accepted or end the negotiation without agreement. 
1 	negotiation strategy 
given a set of possible deals and a negotiation process  what strategy should an individual agent adopt while participating in the process  for example  one strategy for a parent in the carpool scenario is to compute a particular delivery schedule and present it as a  take-itor-leave-it  deal. another strategy is to start with the deal that is best for you  and if the other parent rejects it  minimally modify it as a concession to the other parent. 
¡¡the specification of a negotiation strategy is not strictly part of the interaction rules being decided on by the designers of automated agents. in other words  the designers are really free to build their agents as they see fit. no one can compel them to build their agents in a certain way  having a certain strategy   and such compulsion  if attempted  would probably not be effective. however  we can provide strategies with known properties  and allow designers to incorporate them. more specifically  we may be able to bring to the table a given strategy  and show that it is provably optimal  for the agent itself . there will be no incentive for any designer to use any different strategy. and when all agents use that strategy  there will be certain  beneficial  global properties of the interaction. so a negotiation strategy is provided to the designers as a service; if a compelling case is made  the designers will in fact incorporate that strategy into their agents. we generally are interested in negotiation protocol/strategy combinations. 
1 	three classes of tods 
as mentioned above  the domain examples given in section 1 are all tods  and seem to have a great deal in common with one another. there are  however  critical differences among them  all focused on the domains' cost functions. to demonstrate these differences  we categorize tods based on three possible attributes of the cost function: subadditivity  concavity  and modularity. this is a hierarchy; modularity implies concavity  which in turn implies subadditivity. protocols and strategies that are stable in one kind of tod are not necessarily stable in other kinds. these issues are discussed at greater length in  zlotkin and rosenschein  1a . 
1 	subadditive 
in some domains  by combining sets of tasks we may reduce  and can never increase  the total cost  as compared with the sum of the costs of achieving the sets separately. the postmen domain  for example  is subadditive. if x and y are two sets of addresses  and we need to visit all of them  then in the worst case we will be able to do the minimal cycle visiting the x addresses  then do the minimal cycle visiting the y addresses. this might be our best plan if the addresses are disjoint and decoupled  the topology of the graph is against us . in that case  the cost of visiting all the addresses is equal to visiting one set plus the cost of visiting the other set. however  in some cases we may be able to do better  and visit some addresses on the way to others. that's what subadditivity means. 
¡¡as another example  consider the database query domain. in order to evaluate two sets of queries  x and y  we can of course evaluate all the queries in x  then independently evaluate all the queries in y. this  again  might be our best course of action if the queries are dis-
joint and decoupled; the total cost will be the cost of x plus the cost of y. however  sometimes we will be able to do better  by sharing the results of queries or sub-queries  and evaluate x u y at lower total cost. 
¡¡a relatively minor change in a domain definition  however  can eliminate subadditivity. if  in the postmen domain  the agents were not required to return to the post office at the end of their deliveries  then the domain would not be subadditive. 
1 	concave 
in a concave domain  the cost that arbitrary set of tasks 
z adds to set of tasks y cannot be greater than the cost z would add to a subset of y. the fax domain and the database query domain are concave  while the postmen domain is not. intuitively  a concave domain is more  predictable  than a subadditive domain that is not concave. there is an element of monotonicity to the combining of tasks in a concave domain that is missing from non-concave domains. you know  for example  that if you have an original set of tasks  x   and are faced with getting an additional outside set  z   you will not suffer greatly if you enlarge the original set-the extra work that z adds will either be unaffected or reduced by the enlargement of the original set. in a non-concave domain  even if it is subadditive  you might find that the extra work that z adds is much greater than it would have been before the enlargement. 
1 	modular 
in a modular domain  the cost of the combination of two sets of tasks is exactly the sum of their individual costs minus the cost of their intersection. this is  intuitively  

the most well-behaved subadditive domain category of all. when task sets are combined  it is only their overlap that matters-all other tasks are extraneous to the negotiation. only the fax domain from the above t o d examples is modular. 
1 	incomplete information 
much of the research that we have been conducting on this model of negotiation considers issues relating to agents that have incomplete information about their encounter  zlotkin and rosenschein  1 . for example  they may be aware of their own goal without knowing the goal of the agent with whom they are negotiating. thus  they may need to adapt their negotiation strategy to deal with this uncertainty. 
   one obvious way in which uncertainty can be exploited can be in misrepresenting an agent's true goal. in a task oriented domain  such misrepresentation might involve hiding tasks  or creating false tasks  phantoms  or decoys   all with the intent of improving one's negotiation position. the process of reaching an agreement generally depends on agents declaring their individual task sets  and then negotiating over the global set of declared tasks. by declaring one's task set falsely  one can in principle  under certain circumstances   change the negotiation outcome to one's benefit. much of our research has been focused on negotiation mechanisms that disincentivize deceit. these kinds of negotiation mechanisms are called  incentive compatible  mechanisms in the game theory literature. when a mechanism is incentive compatible  no agent designer will have any reason to do anything but make his agent declare his true goal in a negotiation. although the designer is free to build his agent any way he pleases  telling the truth will be shown to be the optimal strategy. 
   this concern for honesty among agents  and for encouraging that honesty by the very structure of the negotiation environment  is an absolutely essential aspect of work on multi-agent systems. situations in which agents have an incentive to lie are  in general  not stable. although agent designers may discuss a strategy  they will then be motivated to go back and build their agents differently. this will ultimately result in less efficient systems  and outcomes that are worse for the individual agents . first  agents might reasonably expend a great deal of energy in discovering the true goal of the other negotiator  and all of this effort lowers the simplicity and efficiency of the system. second  they will be tempted to risk strategies that may result in inferior outcomes. two agents  coming together  each trying to outguess the other  will sometimes make choices that benefit no one. 
   thus efficiency and stability are closely related. there is no point  in multi-agent systems  in considering efficiency without considering stability.1 without stability  efficiency cannot be guaranteed  as agents are tempted to deviate from the efficient strategy. 
1
¡¡¡¡though in distributed problem solving systems  where there is a central designer of all the agents  stability need not be a serious issue  shoham and tennenholtz  1 . 
invited speakers 
1 	conclusions 
computers are making an increasing number of decisions autonomously  and interacting machines  capable of reaching mutually beneficial agreements  have an important role to play in daily life. the field of distributed artificial intelligence  and particularly its subfield of multi-agent systems  provides an appropriate model for studying these systems of heterogeneous  selfmotivated agents. 
¡¡to provide the agents with a suitable interaction environment in which they can coordinate  it is necessary to establish high-level protocols  that motivate socially beneficial  and individually beneficial  behavior. game theory can provide tools appropriate to the design of these distributed systems. some of the attributes that designers might like to see in interaction environments are efficiency  stability  and simplicity. 
¡¡the design of suitable protocols is closely connected to the domain in which agents will be acting. certain protocols might be appropriate for one domain  and inappropriate for another. in almost all cases  it is important to provide protocols that can deal with incomplete information on the part of agents  while maintaining the stability of the overall mechanism. 
acknowledgments 
the research issues described in this paper have been developed over a period of several years in close collaboration with gilad zlotkin. 
