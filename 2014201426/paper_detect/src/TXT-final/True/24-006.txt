 
in this paper we describe a method for hybridizing a genetic algorithm and a k nearest neighbors classification algorithm. we use the genetic algorithm and a training data set to learn real-valued weights associated with individual attributes in the data set. we use the k nearest neighbors algorithm to classify new data records based on their weighted distance from the members of the training set. we applied our hybrid algorithm to three test cases. classification results obtained with the hybrid algorithm exceed the performance of the k nearest neighbors algorithm in all three cases. 
1 	introduction 
there has been a great deal of progress recently in the development of automated classification techniques. many of the new techniques have arisen from combinations of artificial intelligence and statistical classification approaches. in this paper we describe such an approach. we have developed an improvement on a classical technique  the k nearest neighbors algorithm  that we believe shows a good deal of promise. our improvement involves using an artificial intelligence technique  a genetic algorithm  to enhance the performance of the classical algorithm. 
　the k nearest neighbors algorithm classifies a new instance by noting its distance from each member of a database of classified examples and assigning the new instance to the class of the majority of its nearest neighbors. this algorithm can be quite effective when the attributes of the data are equally important. it can be less effective when many of the attributes are misleading or irrelevant to the classification. the approach we describe here involves finding a vector of weightings of the attributes that make the distance measure more meaningful. 
　it is not a simple matter to find an optimal vector of attribute weightings. in this paper we show how to use a real-valued genetic algorithm to find vectors of attribute weightings that are good in the sense that they out-perform a vector composed of identical values. 
　below we describe the nearest neighbors algorithm  the k nearest neighbors algorithm  the weighted k nearest neighbors algorithm  the genetic algorithm  our hybrid classifier  
lawrence davis tica associates 1 hampshire street cambridge  ma 	1 
and the performance of our hybrid algorithm on three classification problems. 
　in one respect this work is similar to the technique of memory-based reasoning  described in  stanfill and waltz  1 . there the authors detail a method for reasoning about new instances which involves the weighting of attributevalue pairs based on their frequency of occurrence. our algorithm learns attribute weights based on demonstrated classification importance. both techniques are based on the need to vary attribute influence for classification. while the work of stanfill and waltz focuses on the development of a general reasoning technique  ours is concerned with the optimization of a classical statistical classification technique. 
　a technique for the analysis of attributes from the field of statistics is principal components analysis  jolliffe  1 . the focus of principal components analysis  however  is on the variability of attribute values and variance may or may not relate to an attribute's classification utility. 
1 	the k nearest neighbors classification algorithm 
the nearest neighbor classification algorithm  nn  is based on the idea that  given a data set of classified examples  an unclassified individual should belong to the same class as its nearest neighbor in the data set. the measurement of proximity  or similarity  between two individuals is a sub-
ject of much research  e.g.   vosniadou and ortony  1  . although this is an important issue for classification  our research did not focus on this problem. our implementations of nearest neighbor algorithms used the euclidean distance metric  with which the distance between two data points i and j is computed as follows: 
 1  
where xia is the value of the a attribute for the datum.1 the techniques we describe below for improving the performance of these algorithms will be effective no matter what distance metric is employed. 
　a common extension to the nearest neighbor algorithm is to classify a new instance by looking at its it nearest neighbors  then the unclassified individual is assigned 

to the class of the majority of its k closest neighbors. there are various ways to adjudicate disputes among an instance's neighbors  the most basic being a simple vote among the k nearest neighbors. our techniques should be effective regardless of the specific conflict resolution technique employed. 
　the k nearest neighbors algorithm  knn  is simple  quick  and often effective. there are many cases in which its performance is at least as good as other  more sophisticated algorithms. it can go wrong  however  in cases in which the proportion of attributes that are significant in the classification process is small with respect to the number of attributes that are irrelevant or misleading. given a vector of attributes  the knn algorithm effectively treats each as equal in the process of classification. in terms we develop below  the knn algorithm uses a weighting vector in which each attribute is equally weighted 
1 	weighted attributes and the knn algorithm 
assigning variable weights to the attributes of the instances before applying the knn algorithm distorts the space  modifying the importance of each attribute to reflect its relevance for classification. in this way  closeness or similarity with respect to important attributes becomes more critical than similarity with respect to irrelevant attributes. figure 1 graphically displays a simple example of the effect of attribute weighting. in this example an incorrect classification is corrected when the influence of an an attribute is decreased.  consider the classification of the datum at the center of both diagrams if it were not known to be  unfilled circle . in the left diagram it is closest to the filled circle datum and thus would be classified incorrectly. the right diagram shows the effect of decreasing the influence of attribute y from 1 to 1: classification of the middle datum based upon the nearest neighbor is now correct.  when attributes are appropriately weighted  the performance of the knn algorithm will not be degraded it is possible that the optimal weight vector will consist of identical values  as is the case in the knn algorithm. in this case the performance of the knn and weighted knn algorithm  wknn  algorithms will be identical. in all other cases  a wknn algorithm with an optimal weight vector will outperform a knn algorithm. 
　the distance metric we use for the wknn is a slight variant of the euclidean metric  equation 1  above   where wa are attribute weights: 
		 1  
　we also extended the mechanism in which the k nearest neighbors determine the class of the target datum. as there is an ordering of neighbors from 1 to k based on calculated distances  each of the k neighbors should not necessarily have an equal influence in classification decisions. it is likely that the vote of the closest neighbor should have more influence than the kth neighbor. thus  in addition to associating weights with attributes for use in the calculation of inter-datum distances  we also associate weights with the 
1 	learning and knowledge acquisition 

x scale factor = 1 	x scale factor = 1 
y scale factor =1 	y scale factor = 1 
figure 1: example of the effect of attribute weighting. 
k nearest neighbors for use in the actual class determination. the voting strength v for a class i is calculated by: 
		 1  
where wj is the weight associated with the jth nearest neighbor and cj is the jth neighbor's class. the class of the focus datum is determined to be that class with the largest v. 
　although the performance of the wknn algorithm will nearly always exceed the performance of a knn algorithm given an optimal vector of attribute weights  an optimal vector of attribute weights is difficult to find. one method is to manually set weights. in addition to the difficulties involved in such a process  the manual setting of weights is inherently biased and may preclude the discovery of unforeseen relationships. brute force exploration of the search space is another method  however it can be quite time-consuming. if there are n instance attributes and we allow only 1 values for each weight  then there are 1n possible vectors to consider. in the biocriteria database described below  there were 1 attributes. exhaustive test of the weight vectors for this problem  even considering only five values for each weight  would have involved the consideration of 1 vectors and taken months of computer time. 
　these considerations show why the knn algorithm  which requires no training time  is often employed: it generates good solutions quickly. these considerations also show why variably-weighted solutions have not in general been employed: finding high-performance weight vectors can take a tremendous amount of time. 
　in the next two sections we describe an approach to learning attribute weight vectors. our technique uses a genetic algorithm to leam vectors of variable weights. while the genetic algorithm does not necessarily find the optimal weight vector for a problem  it does generate vectors which produce levels of classification performance superior to that obtained with the knn algorithm. 
1 	genetic algorithms 
genetic algorithms are optimization and machine learning algorithms based loosely on processes of biological evolution. john holland created the genetic algorithm field  and 

 holland  1  is the first major genetic algorithm publication. 
　interest in genetic algorithms has increased recently in conjunction with an increase in interest in other algorithms based on natural processes  including simulated annealing and neural networks. for further source material on genetic algorithms the reader is referred to any of the recent proceedings of genetic algorithm conferences  to david e. goldberg's textbook on genetic algorithm theory  goldberg  
1   or to lawrence davis's book on the application of genetic algorithms to optimization problems  davis  1 . 
　put simply  genetic algorithms  gas  solve optimization problems by manipulating a population of chromosomes  encoded solutions to the problem. each chromosome is assigned a fitness that is related to its success in solving the problem. given an initial population of chromosomes  a genetic algorithm proceeds by choosing chromosomes to serve as parents and then replacing members of the current population with new chromosomes that are  possibly modified  copies of the parents. the process of reproduction and population replacement goes on until a stopping criterion  the achievement of a performance target or the usage of an allotted amount of cpu time  for instance  has been met. 
　genetic algorithms have two important features that underlie their success. the first is their employment of an algorithmic equivalent of natural selection. when chromosomes are chosen as parents during the reproduction process  the probability that a given chromosome will be chosen is biased in accord with its fitness. thus  the fittest chromosomes  those that solve the problem best  will tend to have more children than the less fit ones. the use of fitness-based reproduction generally leads to an improvement in the population as a genetic algorithm runs. the second feature is the use of mutation and crossover operators during reproduction. mutation operators cause children to differ from their parents through the introduction of localized change. crossover operators create children that combine chromosomal matter from two parents. the production of high-performance chromosomes can be greatly speeded up with crossover working to combine subparts of good solutions from multiple parents on a single child. 
　there is a good deal of art and theory to account for the success of genetic algorithms in solving hard optimization problems. for discussions of these matters  the reader is referred to the aforementioned source material on genetic algorithms. 
　the discovery of effective weight vectors for a wknn algorithm is a hard optimization problem with a very large search space. this is just the sort of problem that genetic algorithms have been shown to be good at  and so it seemed to us that the hybridization of a ga with a wknn algorithm was a likely candidate for a high-performance classification algorithm. 
　the genetic algorithm we used differed from those that have been employed in more classical studies in the field. in holland's work and in that of his students the encoding of numbers on chromosomes is done in binary notation. a growing body of research   goldberg  1  and  davidor  1   for instance  suggests that problems like this one  in which good solutions tend to occur close to each other instead of being distributed periodically over the search space  may be more efficiently solved with genetic algorithms that use real-valued chromosomes. we employed real-valued chromosomes here. 
　the real-number genetic algorithm we used is an early version of one of the genetic algorithm programs which accompanies  davis  1 . it is a genetic algorithm applicable to a wide variety of real-number optimization problems. the parameter settings and operator set were not tailored to the wknn domain. it is possible that domain-based heuristics could be added to the suite of genetic algorithm operators  particularly in the biocriteria test set described below. we did not add such heuristics in this stage of our research  preferring to use the more generic version of a genetic algorithm. 
1 	the ga-wknn algorithm 
the ga-wknn algorithm combines the optimization capabilities of a genetic algorithm with the classification capabilities of the weighted k nearest neighbors algorithm. the goal of the algorithm is to leam an attribute weight vector which improves knn classification. specifics of the ga-wknn algorithm are: 
  chromosomes are vectors of real-valued weights. each chromosome is a vector of decimal numbers between 1 and 1 inclusive. a vector value is associated with each classification attribute and one is associated with each of the k neighbors. thus the length of the vector is the number of attributes plus k. the initial population of chromosomes in each run of the ga-wknn algorithm was randomly generated. 
  chromosomes are evaluated by iterating through each data set element and classifying each datum by using its associated weights in equation 1  to determine the k closest neighbors  and then in equation 1  to make the class determination. these computations can be used in a number of ways to rank chromosomes; we used two evaluation functions in the experiments reported here: 
1. number of misclassifications. this method sums the number of data which were assigned to an incorrect class by the ga-wknn algorithm. thus  for any chromosome an evaluation of 1 is optimal. chromosome x is ranked higher than chromosome 
y if the number of misclassifications it generates is lower. 
1. multiple value ranking. among chromosomes which generate equal numbers of misclassifications there are finer degrees of difference. this method orders such chromosomes by additionally considering 1  the number of k neighbors which are of the same class  k same neighbors   1  the total distance to the k same neighbors  1  the number of k neighbors which are not of the same class  k different neighbors   and 1  the total distance to the k different neighbors. while there are many ways to use these factors to rank chromosomes  we settled on the following cascaded method  which ranks chromosome x higher than chromosome y if: 
	kelly and davis 	1 

　in the experiments reported here  the ga-wknn algorithm learns a single weight vector which is used to discriminate all classes of data. a natural extension of our technique is to train a vector of weights for each class and assign new instances to the class whose weight vector produces the closest neighbors. 
1 	experimentation 
the primary focus of our experimentation was a comparison of the classification performance of the ga-wknn algorithm to that of the knn algorithm. a secondary issue was a comparison of the two chromosome ranking functions. many empirical studies of classification  e.g.   weiss and kapouleas  1   have compared the performance of a broad range of techniques  including the knn algorithm  and our results can be interpolated into the results of such studies. 
　in this section we discuss the specific ga-wknn operating parameter settings used in our experiments  our data sets and testing methods  and our results. 
1 	ga-wknn parameters 
each run of the genetic algorithm maintained a population of size 1. the runs terminated after 1 individuals had been produced. the algorithm used the steady-state without duplicates reproduction technique  whitley  1    syswerda  1 . 
　the operators used were uniform crossover  average values  real number mutation  and two varieties of real number creep  one with a large creep range and one with a smaller creep range. the parameter settings of these operators were as follows. real number mutation replaced a field on a chromosome with a 1% probability. the new number was randomly generated from the interval between 1 and 1. the first creep operator  large creep  altered a chromosome field with 1% probability. the amount of the alteration was a randomly generated number between 1 and 1 in magnitude. the second creep operator  small creep  altered a 
1 	learning and knowledge acquisition chromosome field with 1% probability. the amount of the alteration was a randomly generated number between 1 and 1 in magnitude. the creep operator altered values up or down with equal probability. 
　only one of these operators was involved in any reproduction event. the number of parents used and children created in a reproduction event depended on the operator employed. the uniform crossover and average values operators used two parents. uniform crossover produced two children and average values produced one. the other three operators  real number mutation and large and small creep  used one parent and produced one child. the relative probabilities that these operators would be selected for use in a reproduction event was held constant at 1%  1%  1%  1%  and 1%  respectively  over the course of the run. these values had been found to perform well over a range of real number optimization problems during the research reported in  davis  1 . 
　fitness was assigned in the following way. the population was rank ordered using one of the two ranking functions described above. then each member was assigned a fitness from the series  
except that where any of these values fell below 1 it was replaced by 1. the value of c was not held constant during the runs. at the beginning of each run c was set to 1. at the end  c was set to 1. intermediate values were the result of interpolating between 1 and 1. the effect of this technique is to produce mild pressure in favor of the best population members when the run begins. the curve steepens over the course of the run as the population of solutions converges on similar individuals. the steeper curve increases the selection pressure  causing the algorithm to focus more and more on the best individuals in the population. while we experimented with various k values throughout this research  we did not do an exhaustive search for optimal settings. for the sake of consistency we set k = 1 during our experiments. 
1 	testing 
the ga-wknn algorithm was tested using three data sets  one of which  biocriteria  is previously only described in state of ohio environmental protection agency  epa  documents  ohio epa  1 . 
  iris the iris data set  fisher's classical test data  fisher  1   contains attributes of three types of iris plants. each of the 1 examples in this data set is described by four attributes. 
  glass this data set consists of attributes of glass samples taken from the scene of an accident.1 each of the 1 examples is a member of one of six classes. there are nine attributes. 
  biocriteria this data set contains biological mea-surements  taken at surface water sites by the state of ohio epa  and the associated pollution impact type. it contains 1 examples  with 1 attribute values associated with each  such as number of sunfish species and total number of deformities  eroded fins  and/or lesions present on all species. the eight decision classes 

represent principal pollution impact types such as complex municipal/industrial pollution and combined sewer overflows. the data has a high noise level  due in part to sampling methods and the fact that most sites are impacted in multiple ways. in addition  naturallyoccurring events such as droughts and floods impact the attributes of water sites and may mask the effect of human pollution.1 
　we used the cross validation error estimation technique  described in  breiman et al  1 . each data set was divided into five partitions. the only constraint on otherwise random partitioning was that classes be represented equally in each partition. we generated five training/test sets for each data set. four-fifths of the data were used for training and the remaining fifth was used for testing. thus no training example was used as a test example in our experiments. for each data set we experimented with misclassification ranking versus multiple value ranking. as a result  we ran two different ga-wknn algorithms for five different partitions of each of the three data sets  for a total of thirty different experiments. 
1 	results and discussion 
figure 1 summarizes our results. column 1 contains the benchmark performance of the k nearest neighbors algorithm  i.e.  each attribute weighted equally . each cell contains the test set classification error rates  averaged across the five partitions. 
　our primary result is that the ga-wknn algorithm outperforms the knn algorithm on these data. in all six comparisons  knn vs. ga-wknn for both ranking functions for all three data sets  the ga-wknn algorithm has a lower test set error rate than knn. for random events such results have an occurrence probability of less than 1%. the genetic algorithm has indeed found weighting vectors that  while 
  not optimal  nonetheless produce better classification performance given the parameter settings above. allowing the genetic algorithm to spend more time on the data improves its performance  although the amount of improvement falls off as the amount of additional time increases. this is an interesting feature of the genetic algorithm that is different from many other classification techniques: one can allot the algorithm whatever optimization resources one has available  with the expectation of obtaining better results the more resources one allots. 
　a secondary result is that in two-thirds of the trials the multiple value ranking function outperforms the number of misclassifications ranking function. while this result is not statistically significant  we believe that this occurs because the multiple value ranking function provides a gradient in cases in which two chromosomes produce the same number of misclassifications. we found that genetic algorithms ranked by the number of misclassifications tended to wander aimlessly when the best population members generated equal numbers of mismatches. genetic algorithms using the multiple value ranking function tended to improve long 
　　the collection and analysis of such biological data is a branch of environmental research which is focused on the development of standards for biological integrity  to be used in conjunction with more standard chemical assessments. 
k n n ga-wknn 
ranking function: 
number of 	multiple 
misclassi- 	value fications iris 1 	1 	1 glass 1 	1 	1 biocriteria 1 	1 	1 figure 1: classification error rates. 
after genetic algorithms using the number of misclassifications ranking method were stalled. the multiple value function takes longer to compute than the number of misclassifications function  but the difference is not great and performance improvement may warrant using the more timeconsuming technique. 
　for experimental purposes an identical stopping criterion was used for each training trial  the completion of 1 reproduction/evaluation cycles . iris  the smallest data set in terms of number of attributes and number of instances  generated the best results  i.e.  largest decrease in knn error rate by ga-wknn . while the rate of performance improvement decreases as more cycles are allotted  one conjecture supported by the comparison of the iris results with those from the other two data sets is that an optimal stopping criterion is related to the data set size; as the number of data points  attributes x instances  increases  there must be a corresponding increase in the resources allocated to the genetic algorithm. 
our experiments were run on a symbolics ivory-based 
lisp machine. rough operating times were on the order of ten minutes for the smallest data set and one hour for the largest  biocriteria . the minimization of operating time was not a goal of this experimentation and certain software optimizations will decrease operating times. but even the aforementioned times are reasonable for many applications and not a prohibitive factor to the use of the ga-wknn algorithm. 
   finally  although the ga-wknn algorithm improves on knn performance in each case  the amount of improvement is incremental. however  improvements are consistently yielded  and this fact demonstrates the potential of the algorithm. for problems in which the k nearest neighbors classification technique outperforms other techniques  we have shown that additional performance increments can be obtained relatively cheaply. for many applications such performance improvements can provide a critical increment of success. 
1 	conclusions 
we have described an algorithm that hybridizes the classification power of knn algorithms with the search and optimization power of the genetic algorithm. the result is 
	kelly and davis 	1 

an algorithm that requires computational capabilities above that of the knn algorithm  but achieves improved classification performance in a reasonable time. we anticipate that extensions to the research will improve the algorithm's performance and there are a number of issues that we plan to address in further work  including: 
  alternative distance/similarity metrics; 
  alternative k values; 
  formal characterization of the example sets on which this algorithm outperforms other classification techniques such as id1 and cart  
  other techniques for learning real-valued weight vec-tors; 
  the correspondence of the learned weights to the fea-ture selection/variable selection problem; 
  class-based derivation of attribute weight vectors; 
  incorporation of domain knowledge into the genetic al-gorithm in the form of heuristic operators; and 
  performance issues. 
　several of these issues will be addressed in  kelly and davis  1   including the use of a rotation parameter  in addition to the scaling described here  and a comparison to id1 classification results. 
acknowledgements 
discussions with ken anderson  albert boulanger  herb gish  william salter  and gilbert syswerda contributed significantly to the evolution of the ideas expressed in this paper. charles t walbridge of the united states environmental protection agency suggested the application of genetic algorithms to the biocriteria domain. chris yoder and edward rankin of the state of ohio environmental protection agency provided the biocriteria data and feedback on classification results. finally  the authors thank bolt beranek and newman  inc.  systems and technologies division for supporting this research. 
