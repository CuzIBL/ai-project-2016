 
       a new method for efficient recognition of general relational structures is described and compared with existing methods. patterns to be recognized are defined by templates consisting of a set of predicate calculus relations. productions are representable by associating actions with templates. a network for recognizing occurrences of any of the template patterns in data may be automatically compiled. the compiled network is economical in the sense that conjunctive products  subsets  of relations common to several templates are represented in and computed by the network only once. the recognition network operates in a bottom-up fashion  in which all possibilities for pattern matches are evaluated simultaneously. the distribution of the recognition process throughout the network means that it can readily be decomposed into parallel processes for use on a multiprocessor machine. the method is expected to be especially useful in errorful domains  e.g.  vision  speech  where parallel treatment of alternative hypotheses is desired. the network is illustrated with an example from the current syntax and semantics module in the hearsay ii speech understanding system. 
introduction 
       the work described in this paper was motivated by certain problems involved in the task of recognizing general structured patterns and  in particular  the problem of parsing continous spoken speech. from the point of view of the language parser  an essential quality of speech is its errorful nature. ambiguities in acoustic segmentation  phonetic labelling  word hypothesization  and semantic interpretation necessitate understanding systems which can deal efficiently with multiple alternative hypotheses about each portion of the input.   i i   the usual methods of dealing with such multiple hypotheses typically entail an expensive search through a combinatorial space  since they consider only one hypothesis for each portion of input at a time  and then exploit contextual relationships to eliminate certain combinations of adjacent hypotheses as impossible. the data structure and associated recognition procedure described in this paper can be thought of as effectively reversing this process by first exploiting context -- thereby eliminating all but a few combinations from consideration - and then testing contextuatly related hypotheses for adjacency. since the contextual information is statically embedded in the data structure itself  comparatively little work needs to be done at recognition time. this work requires only the computation of a few  simple operations rather than a complex search. moreover  the method provides an efficient way to handle the spurious insertions and deletions characteristic of speech. 
template crammars 
       in this section  we define template grammars for recognizing relational structures. a template normal form 
	 tnf  	for 	template 	grammars 	is 	defined. 	an 	algorithm 
1 this research was supported in part by the defense advanced research projects agency under contract no. 
f1-c-1 and monitored by the air force office of scientific research. 
1 
       
        1  every non-primitive relation appears on the right side of exactly one rule. hence we can define the type of a relation to be the type of its unique defining template; a primitive relation is simply said to be of primitive type. 
       it is clear that any template grammar g can be translated into an equivalent grammar g* in tnf by means of adding new relations and rules. the task of the automatic translator is to do this in such a way as to minimize the number of new relations added. the algorithm we employ is described in   the result of applying the algorithm to the 
the rec1niti1n network 
       given a template grammar in tnf  a corresponding recognition network  acorn   as first described in   is constructed as follows. for each relation r appearing in the tnf grammar  there is a unique node  node r   in the network.  hence minimizing the number of relations in the tnf grammar is equivalent to minimizing the number of nodes in the network.  for every rule   t -   r; a   an. arc is drawn from 
1 
node sj  to node r   or each relation sj in the template t. each node sj  is said to be a constituent of node r   and node r  a derivative of node s| . a node may have zero  one  or more derivatives. the recognition network for the sample grammar gap  constructed from the tnf grammar gap*  is shown in figure 1. 
       node r  contains various information: its type  i.e.  the type of relation r ; the action a in the rule  t =  r; a   if any; and the correspondence between the arguments of relation r and the arguments of its constituent relations sj. this correspondence consists of two parts  a set of tests and a generator. the tests represent any requirements for agreement between the arguments supplied by the constituents node sj . the generator is a list of the arguments which are to be supplied in turn to the derivatives of node r . the arguments are encoded according to a canonical numbering scheme best described by an example. consider node tell me . its constituents are node tell   which supplies arguments t1  t1  and node me   which supplies arguments t1 t1 let concatenated argument list  t1 t1't1 t1 . then node tell*me  can specify its arguments by their indices in l. thus node tell+mers only test is l 1  = l 1   denoted by  1  below node tell*me  in the network.  see figure 1.  similarly  node tell*me 's generator is the list   l l   l 1     denoted by     1   1 m above node tell me  in the network. arguments which are not supplied by a node*s constituents but instead originate at the node itself are specified by negative indices. for example  node t1p!c/1vs generator is denoted by     1   1  -1  h the -1 specifies the argument expr  which originates at node t1pic/1 . the action stored in node t1pic/1  assigns this argument the value  rockefeller  
       all of the recognition network components described so far are static. there is also associated with each node r  a dynamic instance list 1l. each instance in the instance list of node r  represents a single recognized occurrence  instantiation  of the relation r in the input utterance. an instance has several components: a unique identification number i; the time interval   x j   x   containing the occurrence; the values x1 xp of additional attributes of the occurrence; and a support set ss containing one or two instance identification numbers. an instance is denoted i: xi  .... * n ; ss . during the recognition process  instances are created and deleted dynamically. 
	the 	recognition 	process 	is 	bottom-up  	as 	follows. 
initially all instance lists are empty. a lexical analyzer is invoked and begins to scan for occurrences of primitive relations in the input utterance. since the lexical analyzer receives imperfect  incomplete information from the phonetic labelling routine  the best it can do is to identify possible occurrences. when it finds a possible occurrence of a relation r  it adds a new element to the instance list of node r  containing the appropriate information. to understand the recognition process  imagine each node r  as having a demon. the node r  demon continuously monitors the instance list of each constituent node st  of node r . whenever a new instance is added to the instance list of node sj   the node r  demon adds a reference to this new instance to its node sj  add set. similarly  whenever an existing instance of sf is deleted  the node r  demon saves a copy of it in its node sj  delete set. add sets and delete sets are referred to collectively as change sets.  the demon then activates  wakes  node r  itself by invoking code pointed to by node r . 
       when node r  is activated  it updates its instance list according to the information in its constituents' instance lists and change sets. if node r  can derive  construct  any new instances from instances of its constituents  it does so  adding the new instances to its instance list. the support set of each instance contains the identification numbers of the instances from which it has been derived. node r  deletes from its 
1 
       

       the original motivation which led to the acorn concept was the development of a general automatic recognition system for spoken utterances  visual scenes  and other structured patterns in which context is a fruitful source of information. since the speech understanding acorn treats an utterance as a relational structure  it is related both to natural language parsers and to general pattern-matching mechanisms. 
       the acorn's closest relative among natural language parsers is parry   a program which simulates a paranoid individual being interviewed by a psychiatrist. parry employs a large library of stored concept sequence templates which are compared with segments of typewritten input sentences. generalization is achieved by rules which rewrite words as synonymous concepts  delete unrecognized words and  if necessary  delete one recognized word at a time until a template is matched. while the approach underlying parry is very successful with typed input  it appears to be too risky for spoken input. unlike the  perfect  input which parry receives  the input to the syntax module of a speech understanding system such as hearsay ii  is highly imperfect. parry can say  with confidence   this portion of the input is such-and-such  e.k  the word  oh    so iii ignore it;  hearsay ii can only say  if this portion of the input is  oh   i can ignore it; but if it's really the word  no   then hi need it.  an acorn can be thought of as a non-deterministic version of a parry-ltke system in which aft possible parses are followed simultaneously in parallel. on the other hand  an acorn is capable of recognizing general graph structures and is more powerful than any context-sensitive language parser  string recognizer . 
       woods' augmented transition network  atn   is a mechanism for parsing natural language. it works top-down  uses backtracking  and produces a formal parse of the input sentence. in contrast  an acorn works bottom-up  does no backtracking  and extracts only those features of the utterance which are relevant to the particular application. an acorn can be thought of as a state-saving  bottom-up version of an atn. 
       miller  has proposed a parser for spoken english which builds multiple partial parse trees and employs a complicated and heuristic search to combine them. an acorn differs from miller's parser in handling all combinations simultaneously rather than sequentially  and in the simplicity of the matching operations it uses. 
       current artificial intelligence programming systems such as planner   qa1   and sail  can match a given relational template against a data base. however  the method they use is an exhaustive  iterative  and associative search. if several templates are to be matched against the data base  they must be matched one at a time in contrast  the associative matching operation performed by acorns effectively tests all the relations of all the templates simultaneously. 
       the acorn's nearest relative among general patternmatching methods is hierarchical synthesis . consider the task of matching a template  such as a schematic representation of a building  against an input set of line segments. a recognition algorithm employing hierarchical synthesis replaces the single  many-component template for  building  with a hierarchy of templates for  doors    windows    stories   etc. a higher-level template can be matched only if its lower-level constituents are. hierarchical synthesis considerably reduces recognition time for two reasons. first  it can exploit the repetition of subtemplates by recognizing all instances of a single subpattern just once. second  before considering whether or not the entire pattern specified by a template is present  it can insure that all necessary subpatterns are present. 
       however  hierarchical synthesis as described in   i   depends on a hierarchy defined a priori by the user. this limitation is transcended by hayes-roth's interference matching method   which does hierarchical synthesis in parallel in all possible directions  thereby obviating the need for a predefined hierarchy. in interference matching  a template is represented as a set of relations. each relation is a predicate with one or more symbolic variables. the input is also a set of relations  whose arguments are constants. a partial match consists of an assignment of input constants to the symbolic variables of a subset of template relations which makes them all true. interference matching works by finding partial matches and combining them into complete matches. 
1        like interference matching  the acorn method is an improved version of hierarchical synthesis in that it requires no predefined hierarchy. the acorn compiler itself determines an economical hierarchy  and embeds it in the form of a recognition network hierarchy selection can be factored out into a separate compilation phase because the choice of hierarchy depends only on the templates and not on the individual input utterance. in interference matching  on the other hand  hierarchy selection depends on the input pattern  and is therefore a part of the recognition process. thus the acorn method combines the convenience of automatic hierarchy selection with the efficiency which comes from using a predefined hierarchy in the recognition process. 
       in real-world applications  input is matched against several top-level templates. current methods of hierarchical synthesis and interference matching involve matching the input against one template at a time. such an approach is clearly undesirable for tasks such as speech recognition  which may involve large numbers of templates. the acorn compiler takes a whole set of templates and produces a single  unified recognition network for it; common subtemplates are shared not just within top-level templates but also between them. an instance of a subtemplate ts recognized just once -- not separately for each top-level template in which it occurs. hence recognition time depends not on the total number of templates  but just on the number of templates which match some portion of the input. this property is encouraging  since the number of templates required to recognize a significant subset of english would probably be several thousand. 
       in sum  an acorn can be looked at as a bottom-up version of an atn; a parallel and non-determinislic version of a parry-like system; a general pattern-matcher; or an improved mechanism tor rmerarchital synthesis  with automatic hierarchy selection and subtemplate sharing between templates. 
applications. implications anp extensions 
       in order for art acorn to be efficient  the templates and input data characteristic of the chosen problem domain should tend to be asymmetric  so that a template will usually match a given portion of the input in at most one way. let us illustrate with a negative example suppose the template we wish to match js k 1   a   c  d  e   the complete graph on five vertices  represented by the conjunction of relations line a  b  a hne a  c  a ... a line. then any occurrence of k1  as a subgraph  say  in the input corresponds to 1! =1 instances of t since there are 1! ifferent ways to bind the variables a  b  c  d  e to the five vertices of the k 1 i n the input. for symmetries on a larger scale  the problem grows combinatorially worse clearly  anacorn would be inefficient in such a domain  since it would insist on finding all instances of every template. 
       fortunately  many problem domains do not exhibit this bothersome property. speech  in particular  is highly asymmetric  partly because it is embedded in a onedimensional ordered temporal domain. if t e l l  t1 t1  is true  then  cannot be true. symmetries at a higher level can occur only if there is more than one syntactically and semanticaliy valid way to group the input words into phrases  i.e   if the input is inherently ambiguous. 
       what are the advantages of acorns for speech understanding  the bottom-up template-oriented approach is especially conducive to handling natural  idiomatic  conversational natural language robustly. consider the problem in spoken speech of spurious insertions such as  oh    urn    er.  we wish to treat them the same as silences. we do this by adding rules like  oh t1 .t1  =  silence t1  t 1  ;  to our template grammar  and relaxing the test t1=t1 for temporal adjacency between two relation instances  such as tell t1  t1  and me t1  t 1    to compute t1=t1 v silencettp  t1  t1 . 
	this 	example 	also 	illustrates 	the 	reason 	for 	non-

actually contains the word  no   it is still there for the lexical analyzer to find. in contrast  when parry ignores information  it throws it away altogether. 
       another phenomenon common to conversational speech is the idiomatic expression  e.g  how $re you   using an acorn  we can simply include explicit template rules for such expressions  e.g.  

       thereby short-circuiting the detailed syntactic parse which would be attempted by a more formal system such as woods'. 
the two techniques just described can be combined. 
certain idioms such as  by the way  carry essentially no useful information and can be treated as spurious insertions by rules 
like 

       some expressions occur either as meaningless idioms or as meaningful phrases  depending on context. consider  for example  the utterance  i see  could 1 see the midnight digest    which occurred in an actual experimental protocol the first occurrence of  j see  is idiomatic and can be ignored; the second is essential to the meaning of the utterance. an acorn  in processing this utterance  would recognize both occurrences as instances of silence  without discarding any information. the first occurrence would be ignored  as desired  but the second one would still be available to match other templates. 
       spurious deletions can also be handled by acorns. to handle spurious deletions  we want to permit partial matching of templates we can do this within the acorn framework simply by adding extra templates corresponding to commonly occurring partial matches of the original templates. the obvious weakness of this method is that it requires a priori knowledge of which deletions are likely to occur. the success of the method might require many iterations over a large corpus of lest utterances  with new templates added as needed. hopefully this process would converge  after a reasonable number of such iterations  to acceptable performance with respect to handling spurious deletions.  this method of  massive iteration  seems to have worked successfully for parry.  
       partial templates can be used for another purpose as well. although the bottom-up approach has several advantages  as described above  it is useful to have certain properties associated with top-down processing one such property is the ability to focus the attention of lower-level modules on critical portions of input. another is the ability to hypothesize words from above  for lower-level modules to confirm or reject. although we earlier referred to a lexical analyzer which finds all instances of primitive relations  words  in the input utterance  this would in practice be too expensive. 
the 	actual 	hearsay ii 	system 	seeks 	to 	constrain 
hypothecation as  much as possible; to do this it applies highlevel information to cut down the number of plausible words matched against each portion of the input. thus it is desirable to have a speech understanding acorn generate intermediate partial information telling the lower level modules which portions of the input they should concentrate on processing  and which words are likely to occur at a given place in the input  on the basis of the already recognized portions of the surrounding context. 
       this top-down extension to the basic bottom-up mechanism requires knowledge about the predictive value of partial templates. for example  we know that  what time  often occurs in the phrase  what time is it   we can incorporate this information in an acorn by including a rule 

       
1 
       
	evaluation and conclusions a full evaluation of the acorn method must of course 1  . miller  p.l. 	a locally-organized parser for spoken input. 
await 	experience 	with 	large-scale implementations. 	in the 	communications of the acm  1  1  1. 
meantime  there are several properties we observe from the 	1 . newell  a.  barnett  j.  forgie  j.  green  c  klatt  d.  current  partial implementation. 	licklider  j.c.r.  munson  j.  reddy  r.  ft woods  w. 
speech understanding systems: final report of a study 
 1  the recognizer is efficient. group. new york: american elsevier  1. 
 1  it ts extremely easy to modify  since changes are 
	1 . newell  	a. 	production 	systems: 	models 	of 	control 
restricted to the template grammar. structures. in w.c. chase  ed   visual information 
 1  using an acorn makes it possible to dispense with a 
processing. new york: academic press  1. 
formal parse. 	1 . ruhfson  j.f.  derksen  j.a.  ft waldmger  r.j. 	qa1: a 
 1  even 	when 	an 	acorn 	cannot 	fully 	parse 	an 
	procedural calculus for intuitive reasoning. 	menlo park: 
utterance  it can still provide a partial parse. 	stanford research institute  1. 
 1  acorns are organized so as to factor recognition 1. woods  w.a. transition network diagrams for natural processing into simple  universal  and independent operations language analysis. communications o  the. acm  1  performed at the nodes. this has made them trivial to 1  1. 
implement and  in addition  makes them well-suited to parallel execution on a multiprocessor. 
       where test is the action invoked upon recognition of the template the effect of the test is to look for the missing instance of  is it  starting at the time t1 in the input utterance. if it is found  it is added to the instance list for is+it  leading to the desired completion of the full template  what time is it.  
       in the above example  a partial template was used to predict downwards in the network. partial templates can also be good upward predictors for example  given an instance of the partial template t1 =  time is it   the probability p t1it1  that it occurs as part of the template t1=  what time is it  may approach certainty. if p t1|t1  is high enough  say .1  we may wish to save processing time by simply predicating that t1 does in fact occur. such a scheme is currently being implemented. 1. hayes-roth  f. the representation of structured events and efficient procedures for their recognition. pittsburgh: department of computer science  carnegie-mellon 
university  1. 
1. hayes-roth  f.  and mostow  d.j. an automatically compilable recognition 	network 	for 	structured 	patterns. pittsburgh: 	department 	of 	computer 	science  
carnegie-mellon university  1. 
1. hewitt  c. description and theoretical analysis  using schemata  of planner: a language for proving theorems and manipulating models in a robot. 
cambridge: mit project mac  1. 
1. lesser  v. r  fennel  r. d  erman  l. d.  ft reddy  d. r. organization of the hearsay ii speech understanding system. proceedings ieee symposium on speech understanding  1.        finally  we expect acorns to have a broad range of applications  since they seem well-suited to recognizing any sort of relational pattern which manifests few symmetries. both spoken utterances and real-world scenes appear to be in this class. at this point  we have implemented one acorn processor for the syntax and semantics in speech  sass  module of hearsay ii. another acorn processor has been built for recognizing the occurrence of inferred patterns  abstractions  in pattern learning training data. the abstractions themselves are produced by a program called sprouter which grows a minimal acorn to recognize all subtemplates common to two or more relational patterns.  from these experiences  it seems that acorns may provide an effective mechanism for general recognition. 
