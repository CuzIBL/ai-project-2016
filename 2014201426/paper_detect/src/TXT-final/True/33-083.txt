 
     a framework for inductive inference in logic is presented: a model inference problem is defined  and it is shown that problems of machine learning and program synthesis from examples can be formulated naturally as model inference problems. a general  incremental inductive inference algorithm for solving model inference problems is developed. this algorithm is based on popper's methodology of conjectures and refutations  ii . the algorithm can be shown to identify in the limit  any model in a family of complexity classes of models  is most powerful of its kind  and is flexible enough to have been successfully implemented for several concrete domains. 
	the 	model 	inference 	system 	is a 	prolog 
implementation of this algorithm  specialized to infer theories in horn form. it can infer axiomatizations of concrete models from a small number of facts in a practical amount of time. 
/. introduction. 
　　a model inference problem is an abstraction of the problem faced by a scientist  working in some domain under a fixed conceptual framework  performing experiments and trying to find a theory capable of explaining their results. 
　　in a model inference problem we assume some unknown model m for a given first order language l. we distinguish two types of sentences in l. observational sentences  which correspond to descriptions of experimental results  and hypotheses  which can serve as explanations for these results. the model inference problem is. 
　　given the ability to test observational sentences for their truth in some unknown model m  find a finite set of hypotheses  true in m  that imply all true observational sentences. 
j j. 	some model inference problems  
an example of a model inference problem is illustrated in 
figure 1. in this example the domain of inquiry is the integers  and the given first order language contains one constant 1  the successor function x'  and three predicates: 
x y for x is less than or equal to y  plus x y z  for x plus 
y is 1 and times x y z  for x times y is z assume that w  test whether these relations hold between concrete numbers  is  we can test ground  variable-free  atoms such as 1 plus 1' 1' 1   and tim*s 1  1  1    for their truth in m it setting  the model inference problem is to find a finite s sentences that are true of arithmetic and imply all true gr  atoms. figure i shows such a set of sentences. we use the arrow - to stand for  is implied by . the sentence p- qa read  f is implied by the conjunction of q and r . 

　　note that we do not need to discover all the properties o functions and predicates involved to solve this model infer problem. in particular  the above set of axioms does not cor axioms for associativity of addition  transitivity of the orde relation  etc.. if t is a set of sentences true in m that implic ground atoms of l true in m then t  is called an atomic-comi axiomatization of m. the set of sentences in figure 1 ii atomic complete axiomatization of arithmetic. it has i 

　'this work was supported in part by the national science foundation  grim no. mcs1. 
this paper is an informal summary of the results described in . inferred by the model inference system from 1 facts ii seconds of cpu time. 
　　another type of inductive inference problem that natui fits in this framework is program synthesis from exam  1 1 . the task is to infer a program inductively  g examples of its input-output behavior. this task can be rest as a model inference problem  and in this case the programs t 

1 

inferred are logic programs  1 1 . 
　　a logic program is a collection of horn clauses  which are universally quantified sentences of the form 
/ * - qv a q% a  ... & qn for n 1  where / and the q%% are atoms. such a sentence is read  p is implied by the conjunction of the 
qy  and is interpreted procedurally  to satisfy goal p  satiify goals quqt fin  if the   *s are missing  the sentence reads  p is true** or  goal p is satisfied**. if p is missing  the sentence reads  the g's are false**  or  satisfy the qy  a collection of horn clauses can be executed as a program  using this procedural interpretation. 

　　figure 1 shows two logic programs. in this example of a model inference problem  the language l contains the two place function symbol  x y   the prolog list constructor  the equivalent of the lisp function cona . the constant   prolog's nil  and the two predicates append x y z  and reverae x.y . the model m for this language is defined as follows: the elements of m are all lists constructed from  x|y  and ; the atom append x y z  is true in m just in case the list z is the result of appending the list x to the list y  e.g. append |a b c   d e   afb c d e  ; the atom reverae x.y  is true in m just in case that the list y is the reverse of the list x  e.g. reverae ia b c   c b a  . the horn clauses in figure 1 are an atomic-complete axiomatization of the model thus defined  and are also prolog programs for computing append and reverae. the model inference system synthesized the logic program for append in 1 cpu seconds from 1 facts* and a similar program for reverae from 1 facts in 1 cpu seconds. 
　　in this paper we restrict ourselves to model inference problems in which the hypothesis language is the horn clauses and the observational language is the ground atoms of l. in  we discuss a more general setting  and show that problems of grammatical inference and concept learning can also be formulated as model inference problems. our restricted model inference problem is the following: 
　　given the ability to test ground atoms for their truth in some unknown model at  find an atomic-complete horn axiomatization ofm. 
1. 	solutions to model inference problems. 
　　an algorithm that solves model inference problems it called a model inference algorithm. such an algorithm tests the truth of ground atoms in the model  and once in a while produces a conjecture  a collection of horn clauses. since a finite number of facts about a model can not in general determine it uniquely among all possible models  and since a model inference algorithm always bases its conjectures on a finite number of facts  it is bound to make mistakes. the most one can expect of a model inference algorithm is that after examining a finite number of facts about the model  and making a finite number of wrong conjectures  the algorithm will correctly conjecture an atomiccomplete axiomatization of a model and never change its conjecture afterwards. following gold   we say that in such a case the algorithm identifies the model in the limit. note that a 
　　model inference algorithm cannot  in general  determine whether it actually has identified a model. 
　　one inductive inference technique that can be used to solve model inference problems is enumeration. algorithm i exemplifies this technique. 

　　we say that a conjecture t is too strong with respect to some model m if it implies an observational sentence false in m. we say that it is too weak if it does not imply an observational sentence true in m. note that a conjecture can be simultaneously too strong and too weak. the test of whether a conjecture is too strong or too weak is  in general  undecidable. to implement algorithm 1 we choose some fixed complexity bound and use is to bound the resources allocated to this test. the complexity bound we choose determines the class of models the algorithm can identify in the limit  as explained below. 
　　let h be some total computable function from ground atoms to non-negative integers. a finite set of horn clauses t is called h-easy if its atomic consequences are easy to derive modulo h  that is  if t derives a in at most h a  derivation steps for almost every  that is  for all except a finite number of  ground atom o such that t implies a. a model hi is h-easy if it has a finite atomic-complete h-easy axiomatization. using this concept  we can characterize the power of algorithm i: if we choose such an h to bound the resources allocated to the test in the while loop  then the algorithm can identify in the limit exactly h-easy models. 
1 　　although the enumerative approach is very powerful  its inherent inefficiency limits its practical applications. making better use of the syntactic and semantic properties of logic  we develop an incremental algorithm that overcomes some of this inefficiency without loss of power. in developing the incremental algorithm we focus on two questions: 
/. how to weaken a conjecture if it is discovered to be too strong  
1. how to strengthen a conjecture if it is discovered to be too weak  
　　if we have a solution to these questions  we could develop an algorithm as in figure 1. 
figure 1: a scheme for an incremental algorithm. 
choose some initial conjecture t. 
repeat 
examine the next fact. 
repeat while the conjecture t is too strong do weaken it. 
while the conjecture t is too weak do strengthen it. 
until the conjecture t is neither too strong nor too weak with respect to the known facts. 
　　output t forever. 
1. refuting hypotheses with crucial experiments  
　　 the only thing the experiment teaches us is that among the propositions used there is at least one error; but where this error lies is just what it does not tell us** 
-- pierre duhem  
the aim and structure of physical theory 
　　 ...it has to be admitted that we can often test only a large chunk of theoretical system  and sometimes perhaps only the whole system  and that  in these cases  it is a sheer guesswork which of its ingredients should be held responsible for any falsification** 
- karl r. popper  
conjectures and refutations: the growth of scientific knowledge 
　　if a conjecture is too strong  i.e. it implies a false observational sentence  one can conclude that at least one of its hypotheses is false. in this section we develop the contradiction backtracing algorithm  which can detect such an hypotheses by performing crucial experiments in the model. the conjecture can then be weakened by removing this false hypothesis from it. 
     crucial experiments are experiments that have a potential to decide between competing theories. a successful crucial experiment can eliminate at least one of the theories by providing a counterexample to its prediction. although one crucial experiment can  in general  refute only a collection of hypotheses  the contradiction backtracing algorithm suggests a way of sequencing crucial experiments  which guarantees singling out a particular false hypothesis. the algorithm can be applied whenever a contradiction is derived between some hypotheses and the facts. its input is an ordered resolution tree of the empty sentence q from a set of hypotheses and true observational sentences 1. the algorithm assumes that an oracle for m  that can tell the truth of all ground atoms of l  is given. the algorithm performs a finite number of experiments in m  bounded by the depth of the derivation tree  and outputs an hypothesis pes which is false in m. 
we demonstrate 	what the algorithm does on the propositional calculus example in figure 1. 	in this example 
 the resolution tree is ordered so 
that the atom resolved upon appears in the condition of the left son and in the conclusion of the right son of the resolvent. 
figure 1: backtracing contradictions in propositional logic. 
　　the algorithm starts from the root the atoms resolved upon. if the atom is true in m i t chooses the left subtree  otherwise the right subtree  until it reaches a leaf. the hypothesis in the leaf is false in m. in the illustrated example  assume that the hypothesis  is false  which means that r and q are true in m and p is false. the algorithm first tests whether r is true in m  and since  by our assumption  it is true  it chooses the left subtree. next it tests p  finding that it is false in the model  and chooses the right branch. finally it tests q  finds it to be true  chooses the left branch which leads to a leaf  outputs the hypothesis  which is false in m and terminates. 
　　the contradiction backtracing algorithm for a first order language is based on the same idea of detecting a false hypothesis by systematically constructing a counterexample to it  although the way this counterexample is constructed is slightly more involved. the technique used is based on collecting substitutions in a resolution proof  which is similar to the one suggested by green  and is used by the prolog interpreter. 
　　there is  however  another complication in the predicate calculus case. since the atom p resolved upon need not be ground  one cannot always test its truth directly with the oracle for m. the solution is to instantiate p to a ground atom before testing it. the choice of how to instantiate p is arbitrary  but once it has been made all further experiments should be done with the same substitutions  in order for their results to constitute a counterexample to the hypothesis reached in the leaf. 
　　the following is an example of the use of contradiction backtracing by a model inference algorithm  while it is trying to infer an axiomatization of the relation over the natural numbers. assume that the algorithm already conjectured the hypotheses and encountered the fact 
               it suggested the hypothesis 1 so 	together 	with 	the 	hypothesis 	the 	sentence can be derived. however  after adding the new hypothesis the derivation in figure 1 can also be constructed. so we can apply the contradiction backtracing algorithm and find which of the three hypotheses involved is false. 
　　the atom resolved upon at the root is  the oracle is called on  and answers urue  so the left branch is chosen. the atom resolved upon at that node is and applying the 

1 over integers  figure i   the subsequence relation over lists  concatenation relations over lists  figure 1   and the subtree relation for binary trees. 
　　with every refinement operator we associate an hypothesis language: the set of sentences that can be generated from the empty sentence via a finite sequence of refinement operations. the hypothesis language associated with the above refinement operator contains all atoms and context-free transformations of l we say that one refinement operator is more general than another if the hypothesis language associated with the first contains the hypothesis language associated with the second. there are some immediate generalizations of the refinement operator described above  whose hypotheses languages suffice to axiomatize binary tree isomorphism  multiplication  exponentiation  string reversal and insertion sort. these refinement operators are implemented in the model inference 

　　the contradiction backtracing algorithm is applicable just as well to general clauses  and it can be shown that in either case the number of experiments needed to single out a false hypothesis is bounded by the depth of the resolution tree. 
　　since the prolog interpreter automatically maintains all the substitutions as the resolution proof progresses  it is extremely simple to implement the contradiction backtracing algorithm in it. in  we describe such an implementation and discuss its application to the debugging of logic programs. 
1. refining refuted hypotheses. 
   this section addresses the question of how to strengthen a conjecture that is too weak. for this task we devise refinement operators. intuitively speaking  a refinement operator suggests a logically weaker plausible replacement to a refuted hypothesis. if a conjecture is too weak  it can be strengthened by adding to it refinements of previously refuted hypotheses. the following is an example of one particular refinement operator used by the model inference system. 
　　we call sentences of the form that satisfy condition 1. above context-free transformations. it can be shown that any context-free transformation can be generated from the empty sentence via a finite sequence of refinements  using this refinement operator. some non-trivial predicates have an atomic complete axiomatization via atoms and context-free transformations. examples are the order relation and addition system. 
　　the effect of a more general refinement operator is a more powerful  though less efficient algorithm. if the syntactic class of the intended axiomatization is known  one can tailor a refinement operator for that class  thus increasing the efficiency of the algorithm. we may not always have such information  however. to ensure the theoretical completeness of this approach  we show in  the existence of a most general refinement operator. 
1. a general  incrementalmodelinference algorithm. 
　　in the last two chapters we have developed mechanisms to weaken and strengthen the logical power of a conjecture when needed. we can instantiate now the algorithm scheme in figure 1  and obtain algorithm 1. 
　　as in algorithm i  the tests in the while and repeat loops are  in general  undecidable. to implement them we choose some fixed complexity bound h and use is to bound the resources allocated to these tests. another unspecified part in the algorithm is which refinements to add in the second while loop. one possible approach is to add them in a breadth-first order. 
that is  all hypotheses generated by two refinement will be added before those generated by three refinements  etc. . the main theorem proved in  is that using this approach  algorithm 1 
1 
can identify in the limit any a-easy model  or  in other words  that it is as powerful as algorithm i. 
     another theoretical result obtained in   based on the work of the blums*  says that under some constraints  this is as powerful as model inference algorithms can get. we say that a model inference algorithm is sufficient  if whenever it examines a 
     new fact  the last conjecture it has output implies all the observational sentences it already knows to be true. it can be shown that if a model inference algorithm is sufficient  then there exists a recursive function h such that a-casy models is all it can identify. since algorithm i is sufficient  these results establish that it is the most powerful of its kind. 
1. relation to other work on machine lemming. 
　　the approach to inductive inference in logic presented here follows the direction set by gold   which attempted to formulate problems of machine learning in a precise way  and to devise valid criteria for success of solutions to such problems. it is hoped that the model inference problem provides a natural setting for the continuing ai work on machine learning. the framework proposed here makes the results and theoretical tools developed in the recursion- and complexity-theoretic research in inductive inference applicable to the more concrete and experimental work in ai  and provides a solid basis for further development. it should be emphasized that the model inference algorithm described here is only one possible approach to inductive inference in logic  and other approaches to machine learning may use this theoretical framework with equal success. 
　　the algorithm described here is most similar to the modeldirected  top-down approach of the version spaces algorithm of mitchell . both the version spaces algorithm and algorithm 
1 converge by finding some hypotheses that  match  the data  although the notions of  matching  used by the two are quite different: in the version spaces algorithm the pattern should match the instances. in algorithm 1 the hypotheses should agree with the facts. the question whether a pattern matches an instance is always decidable  and usually by a fast algorithm; on the other hand the corresponding question of whether a theory agrees with a fact may be undecidable  and in such a case can only be approximated by some resource-bounded computation. 
　　in most of the recent work on program synthesis from examples the target programming language is lisp  1 1 . several approaches were used; smith  provides a good survey of them. we have compared the performance of the model inference system  restricted to infer list-processing logic programs  to the works of summers and biermann  i . we summarize briefly the results of this comparison. 
most of the example lisp programs synthesized by 
summers* system thesys have equivalent logic programs which are context-free transformations. some of the more complex functions  can be axiomatized using term-free transformations with auxiliary predicate. an example of one is pack  a program that packs a list of lists into one list. packumd-
p*ek m y  z  - pack y w  & append x w z  
using the appropriate refinement operator  the model inference system inferred most of the examples described in his thesis  in less than one minute of cpu time. for example  it has inferred the program for pack above in 1 cpu seconds and from 1 facts  most of them negative. summers does not give statistical information on the performance of his system  but it seems that the number of positive facts needed by the systems is comparable. thesys does not need negative facts. 
     biermann's system for the synthesis of regular lisp programs from examples  i  is strongly influenced by summers' method  although it has an enumerative component which summers' system does not. biermann gives a structural definition of the class of programs synthesized by his algorithm  and provides more information on the performance of his system. the simpler examples described in his paper can also be axiomatized by context-free transformations. for the more complex examples  context-free transformations with an auxiliary predicate suffice. this class is strictly contained in the class of term-free transformations used to infer some of summers* examples. 
     as to the performance of the two systems in this domain  most of programs were synthesized by biermann*s system from one example. the model inference system needed anywhere between 1 and 1 facts. biermann's system needed between a fraction of a second to half an hour for these examples. the time taken by the model inference system on the same examples ranged between 1 and 1 seconds. the systems behaved similarly on the examples: what is harder for biermann's system is also harder for the model inference system. the program that took biermann's system half an hour to synthesize collects all first elements in a list of lisp-atoms and lists. the model inference system synthesized the program following program for this task from 1 facts and in 1 seconds. 
hoads  u  heads   x y |z . x wj  - heads z.w  haads  xjy  z  - atom x  & heads y.z  
the actual timing figures are not very informative; what should be noted is the major difference in the growth rate. the systems' behavior suggest that the asymptotic time complexity of the model inference system compares favorably with biermann's for this class of functions. 
　　the model inference system has synthesized several programs that  as far as i know  have not been synthesized from examples before. among them are programs for exponentiation  binary tree isomorphism and satisfiability of boolean formulas. 
1 　　the most important difference between the lisp systems and the model inference system is that the former usually incorporate some hard-wired synthesis algorithm  which can synthesize only a fixed class of functions. generalizing such an algorithm is not a trivial task  as the work of kodratoff  on generalizing summers' method shows. the model inference system  on the other hand  incorporates the refinement operator as a parameter. to illustrate the flexibility of this approach  note that one refinement operator is sufficient for synthesizing almost all the examples of summers. to get a more efficient inference of the restricted class of functions inferred by biermann  a more specific refinement operator was designed. the implementation of the new refinement operator required about 1 minutes of thought and rewriting five lines of prolog code. 
1. concluding remarks. 
　　this paper has presented a general  incremental algorithm that infers theories from facts. its theoretical analysis shows that it is comparable to some of the most powerful algorithms known from the complexity-theoretic approach to inductive inference. its implementation is comparable to existing systems for inductive inference and program synthesis from examples. 1 believe that these results were made possible by the use of first order logic as the underlying model of computation. 
　　here are some of the reasons for the success of logic as a medium for inductive inference: 
　　logic has natural semantics. if a turing machine computes an incorrect result on a certain input  there is no sense in which one of the transitions in its finite control is  wrong  for every such candidate to be a  wrong  transition  one can always patch the turing machine without changing this transition  so it will behave correctly on this input. on the other hand  if a set of logical axioms has a false conclusion  there is a natural sense in which at least one of the axioms is strictly false. this fact enables the existence of error detecting algorithms such as contradiction backtracing. 
　　logic has an intimate relation between its syntax and semantics. this is the reason why there are natural ways to weaken the logical  computational  power of a refuted hypothesis  or  in other words  why natural and easy-to-compute refinement operators exist. 
　　logic is monotonic and modular. altering an axiomatization by adding or removing axioms has clear effects on the expressive  computational  power of this axiomatization. there are not many practical programming languages for which such syntactic alterations to a program have predictable effects on what it computes. 
　　logic is a programming language that separates logic and control. it seems that one of the reasons for the efficiency of the model inference system is that it infers only the  logic component  of a program and leaves the  control component  unspecified . the logic component of a program contains more than its specification  and the task of imposing control on a logic program is similar to the task of program optimization. the problems of program optimization and program synthesis from examples are hard enough by themselves to justify refraining from solving them simultaneously. we propose separating the task of synthesizing efficient programs from examples to two sub-tasks: inference of  sometimes inefficient  programs from examples  and program optimization. 
a cknowledgements. 
dana angluin and drew mcdermott supervised this work. 
i also thank ernie davis  john ellis  bob kowalski  bob nix  chris riesbeck  david warren and steve wood for their various contributions. 
