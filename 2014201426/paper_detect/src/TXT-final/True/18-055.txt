 
depth reconstruction from the two-dimensional image plays an important role in certain visual tasks and has been a ma-
jor focus of of computer vision research. however  in this paper we argue that most instances of recognition in human and machine vision can best be performed without the preliminary reconstruction of depth. three other mechanisms are described that can be used to bridge the gap between the two-dimensional image and knowledge of three-dimensional objects. first  a process of perceptual organization can be used to form groupings and structures in the image that are likely to be invariant over a wide range of viewpoints. secondly  evidential reasoning can be used to combine evidence from these groupings and other sources of information to reduce the size of the search-space during model-based matching. finally  a process of spatial correspondence can be used to bring the projections of three-dimensional models into di-
rect correspondence with the image by solving for unknown viewpoint and model parameters. these methods have been combined in an experimental computer vision system named scerpo. this system has demonstrated the use of these methods for the recognition of objects from unknown viewpoints in single gray-scale images. 
introduction 
the standard model for much recent research in computer vision has been based on the reconstruction of depth information from the image prior to recognition. however  in this paper we will argue that this is not the primary pathway used for most instances of recognition in human vision. although depth measurement has an important role in certain visual problems  it is often not available and is not needed for typical instances of recognizing familiar ob-
jects. instead  we will propose that the primary bottom-up descriptive analysis of the image can best be performed by a process of perceptual organization. this process leads to the formation of significant groupings and structures directly from the two-dimensional image data. these groupings are partially invariant to viewpoint and can be matched directly against three-dimensional object models. the verification of these matches can be performed by spatially mapping the projection of three-dimensional object models onto the image data  through a process of viewpoint and modelparameter determination. 
　　these methods have been combined in a vision system named scerpo  for spatial correspondence  evidential reasoning  and perceptual organization . while seemingly solving a more difficult problem-the direct recognition of objects from unknown viewpoints in two-dimensional images-the approach is shown to be apparently simpler and more flexible than those that rely upon depth reconstruction. while it is true that the appearance of a threedimensional object can change completely as it is viewed from different viewpoints  it is also true that many aspects of an object's projection remain invariant over large ranges of viewpoints  examples include instances of connectivity  collinearity  parallelism  repetitive textures  and certain symmetries . it is the role of perceptual organization to detect those image groupings that are unlikely to have arisen by accident of viewpoint or position. once detected  these groupings can be matched to corresponding structures in the objects through a knowledge-based process of evidential reasoning. these methods for evidential reasoning were initially developed for combining probabilistic information in diagnostic expert systems  but they can be readily adapted to combining information regarding probabilistic associations between particular image features and object models. this probabilistic information is used to order the search strategy so that the most reliable and informative information is tested first. 
　　the reliability of the search process depends upon the the final verification of each hypothesized interpretation. scerpo uses a quantitative method to simultaneously determine the best viewpoint and object parameter values for fitting the projection of a three-dimensional model to given two-dimensional features. it allows a few initial hypothesized matches to be extended by making exact predictions for the locations of other object features in the image. this provides a highly reliable method for verifying the presence of a particular object  since it can make use of the spatial information in the image to the full degree of available resolution. 
1 d lowe 
the role of depth recovery in human vision 
a substantial fraction of recent computer vision research has been aimed at the bottom-up derivation of depth or surface orientation from image data  using information such as stereo  motion  shading or texture. this has come to be known as the  shape from x  paradigm. marr  suggested that these sources of information could be combined in a representation known as the 1-d sketch that would allow one source of information to compensate for the absence of another. the depth representation would then be used to determine correspondence with three-dimensional object representations  the assumption being that it would be easier to match a three-dimensional model to a depth representation than to two-dimensional image data. 
　　human vision contains many of these components for recovering depth  and they presumably have important functions. however  biological visual systems have many objectives  so it docs not follow that these components are central to the problem of visual recognition. in fact  the available evidence would seem to indicate the opposite. the first problem with these methods is that depth information is often unavailable or requires an unacceptably long interval of time to obtain. stereo vision is only useful for objects within a restricted portion of the visual field and range of depths for any given degree of eye vergence  and is never useful for distant objects. motion information is available 
only when there is sufficient relative motion between observer and object  which in practice is also usually limited to nearby objects. recognition times are usually so short that it seems unlikely that the appropriate eye vergence movements or elapsed time measurements could be taken prior to recognition even for those cases in which they may be useful. depth measurements from shading or texture are apparently restricted to special cases such as regions of approximately uniform reflectance or regular texture  and they lack the quantitative accuracy or completeness of stereo or motion. 
　　secondly  human vision exhibits an excellent level of performance in recognizing images-such as line drawings- in which there is very little potential for the bottom-up derivation of depth information. whatever mechanisms are being used for line-drawing recognition have presumably developed from their use in recognizing three-dimensional scenes. the common assumption that line-drawing recognition is a learned or cultural phenomena is not supported by the evidence. in a seemingly definitive experiment  hochberg and brooks  describe the case of a 1-monthold human baby who had had no previous exposure to any kinds of two-dimensional images  yet was immediately able to recognize ordinary line drawings of known objects. 
　　finally  there has been no clear demonstration of the value of depth information for performing recognition  even when it is available. the recognition of objects from complete depth images  such as those produced by a laser scanner  has not been shown to be much easier than for systems that begin only with the two-dimensional image. this paper will describe methods for directly comparing the projection of three-dimensional representations to the two-dimensional image without the need for any prior depth information. 
　　of course  none of this is meant to imply that depth recovery is an unimportant problem or lacks a significant role in human vision. depth information may be crucial for the initial stages of visual learning or for acquiring certain types of knowledge about unfamiliar structures. it is also clearly useful for making precise measurements as an aid to manipulation or obstacle avoidance. however  it seems likely that the role of depth recovery in common instances of recognition has been overstated. 
matching 1-d knowledge to the image 
although knowledge of object shape  context  and surface properties must naturally be represented in threedimensional form  this knowledge can be matched directly against the two-dimensional image through the use of projection. a major practical difficulty is in using image measurements to determine the unknown projection parameters. six parameters are needed to specify an arbitrary position and orientation of an object with respect to the camera  and there may be other unknown parameters internal to the ob-
ject. however  each match between a point in the image and a point on the object allows us to solve for two parameters. therefore  only three or four hypothesized matches between the image and an object model are typically needed to solve for the projection parameters. once these parameters have been determined  it is straightforward to carry out the projection and extend the match by making accurate predictions for the locations of other model features in the image. these further matches may be used to solve for any remaining model parameters  but their most important function is to provide reliable confirmation for the correctness of an interpretation. 
　　the author has previously presented a mathematical technique  1  1  for solving for viewpoint and model parameters given some matches between image and model. briefly  this method linearizes the projection equations and uses newton-raphson iteration to solve simultaneously for the unknown parameters. since the projection equations are very smooth  consisting of linear combinations of sin and cos functions of viewpoint   the method has quadratic con-
vergence and typically requires only 1 iterations to achieve high accuracy. this basic technique has been extended to perform least-squares solution of over-determined systems  and to allow matching of image lines to model lines  without concern for the location of line terminations . given these methods  the problem of verification is largely solved for well-specified objects  and the remaining problems of recognition are those of reducing the size of the search space to produce the few initial matches. 
　　there is experimental evidence that human recognition also relies upon the determination of viewpoint parameters for projecting a three-dimensional object description onto the image. cooper &; shepard  describe experiments in which subjects are asked to compare images at varying orientations to previously memorized shapes. they found that the recognition time varied linearly in the angle of rotation between the image and the orientation of the original memorized shape. in conjunction with their other work on mental rotation  this would seem to indicate that recognition is performed by bringing a prior representation into spatial correspondence with image data by manipulating viewpoint parameters. 
allowing for variations in object models 
the capability for recognizing objects from their two-dimensional projections is possible only because of previous knowledge regarding the objects. however  recognition does not imply that we must know every aspect of an object's appearance prior to recognition. object models may be parameterized with variable sizes  angles  or articulations between components  with expected bounds given for each parameter. as already mentioned  it is possible to backsolve for these parameters using the same methods as when solving for viewpoint. just as important is the fact that there is no precise boundary between what is an object and what is a component. it is possible to recognize commonlyoccurring components  such as cylinders  rectangular solids  or repeated patterns  as parameterized objects in their own right. the only requirement is that there be fewer unknown parameters to the description than there are useful measurements to be made from the image data. these recognized components-even if the identification is only tentative- can then be used to suggest the identity of the more specific structure of which they are a part. if the identification of the components is quite certain  then they can even be combined into previously unknown or very loosely parameterized relationships. most objects can be represented both in terms of their overall shape and in terms of a combination of components  and different images can best make use of each type of description depending upon such variables as image resolution  viewpoint  and occlusion. 
previous work on model-based vision 
there is a considerable body of previous research in modelbased vision. the remarkable early work of roberts  demonstrated the recognition of certain polyhedral objects by precisely solving for viewpoint and object parameters. unfortunately  this work was poorly incorporated in later vision research  which tended to emphasize less quantitative methods. the acronym system of brooks  used a general symbolic constraint solver to calculate bounds on viewpoint and model parameters from image measurements. these bounds could then be used to check the consistency 
d. lowe 1 
of interpretations produced by general matching operations  and were capable of handling wide classes of generic object descriptions. goad  describes the use of automatic programming methods to precompute a highly efficient search path and viewpoint-solving technique for each object to be recognized. this research has been incorporated in an in-
dustrial computer vision system by silma inc. which has the capability of performing all aspects of recognition within as little as 1 second. because of their runtime efficiency  these precomputation techniques are likely to remain the method of choice for industrial systems dealing with small numbers of objects. other closely related research on model-based vision has been performed by shirai  1  and walter & tropf 
 1 . 
perceptual organisation in scerpo 
unlike previous model-based systems  scerpo makes use of perceptual organization as the central process for bottomup analysis of an image. perceptual organization refers to a basic capability of the human visual system to derive relevant groupings and structures from an image without prior knowledge of its contents. for example  people will immediately detect clustering  connectivity  collinearity  parallelism  and repetitive textures when shown an otherwise randomly distributed set of image elements. this grouping capability of human vision was studied by the early gestalt psychologists  and is related to research in texture description . a major function of perceptual organization is to distinguish non-accidental groupings from the background of groupings that arise through accident of viewpoint or random positioning  1  1 . those groupings that are non-accidental in origin will also be partially invariant with respect to viewpoint and be most suited to model-based recognition  see  for a much more detailed discussion . 
　　in order to provide image features for input to perceptual organization  the first few levels of image analysis in scerpo use established methods of edge detection  as shown in figures 1. the 1-by-1-pixel image shown in figure 1 was convolved with a laplacian of gaussian function  a - 1 pixels  as suggested by the marr-hildreth  theory of edge detection. the zero-crossings of this function are shown in figure 1. of course  many of these zero-crossings do not correspond to significant edges in the image. we remove those corresponding to insignificant intensity changes by applying the sobel gradient operator to the 1g convolution. only those points that are above a chosen gradient threshold and lie on a zero crossing are retained in figure 1. these remaining zero-crossings are linked into lists of points on the basis of connectivity. 
　　the first stage of perceptual organization is to group the linked lists of points into perceptually significant curve segments. the author has previously described a method for finding straight-line and const ant-curvature segmentations at multiple scales and for measuring their significance  1  chap. 1   however  here we use a simplified method that 1 d lowe 
selects only the single highest-significance line representation at each point along the curve. the significance of a straight line fit to a list of points is measured as the ratio of its length divided by the maximum deviation of a point from the line. this provides a scale-independent measure of significance that places no prior bounds on the allowable deviations. this is then used in a modified version of the recursive endpoint subdivision method. a segment is subdivided at the point with maximum deviation from a line connecting its endpoints. if the maximum significance of any of the subsegments is greater than the significance of the complete segment  then the subsegments are returned. otherwise the single segment is returned. this procedure is applied recursively until each segment contains fewer than 1 points. the procedure will return a segment covering every point along the curve  but those with a length-to-deviation ratio less than 1 are discarded. this method is implemented in only 1 lines of lisp code  yet does a reasonable job of detecting the most perceptually significant straight line groupings in the linked point data. the results are shown in figure 1. 
　　the straight line segments are indexed according to endpoint locations and orientation. then a sequence of procedures is executed to detect instances of collinearity  endpoint proximity  connectivity   and parallelism. a region around each endpoint or segment is examined to determine candidates for grouping. each potential grouping is assigned a significance value that is roughly inversely proportional to the likelihood that it is accidental in origin. this is done in a scale-independent manner  i.e.  measurements of endpoint proximity or separation of parallel lines are divided by the length of the shortest of the two line segments . after the execution of this grouping process  the many groupings are ranked in order of significance. unfortunately  it is difficult to display the results of this grouping process without showing a separate image for each grouping that has been detected. although several hundred significant groupings were detected in the line segments of figure 1  we show in figure 1 only the two sets of highly-ranked groupings that were actually used for successful recognition. 
evidential reasoning 
evidential reasoning refers to the combination of different sources of information or evidence in order to reach a conclusion with a specified level of certainty. this form of reasoning has been developed for use in diagnostic expert systems  among other applications. it can be used  for example  to calculate the likelihood that a particular disease is present given a number of symptoms. we are faced with a very similar problem in vision when we wish to calculate the likelihood that a particular object is present in an image given a number of detected features and other sources of information. the performance requirements for evidential reasoning in vision are much less stringent than in medical expert systems  since we have a reliable procedure for final verification and only need to use the evidential reasoning to suggest the most efficient ordering for our search. 
　　in order to minimize the search time  we would like to order our consideration of hypotheses according to decreasing values of pk/wk  where pk is the probability that a particular hypothesis for the presence of object k is correct  and wk is the amount of work required to verify or refute it. evidence can come from many sources: we may have initial expectations for the presence of certain objects  contextual expectations resulting from the presence of alreadydetected objects  and information from many forms of image data such as perceptual groupings  texture  color  or metric measurements. the initial researchers in medical expert systems rejected the use of bayesian methods for combining evidence   since they assumed that it would either require unrealistic independence assumptions or an impossibly large number of known statistical parameters. however  recent work by charniak  has shown that it is possible to formalize the previous apparently ad-hoc methods within a bayesian framework. the application of charniak's methods to ordering search during recognition is discussed in  1  chap. 1 . an important aspect of evidential reasoning is that it offers a strong basis for building learning systems in which the required statistical parameters are moved towards their correct values as the system gains experience. 
　　the evidential reasoning component of scerpo has not yet been developed as fully as other parts of the system. since the system has only been used with a single object under consideration  the performance requirements for minimizing search have not been great. the system makes use of a list of perceptual groupings and the model features that could give rise to them. this list is entered by the user at the same time as model specification. for example  the groupings shown in figure 1 consist of particular combinations of parallelism and endpoint proximity that could be matched to various parts of the object model. the probabilities of non-accidentalness for the image relations that make up a grouping are multiplied together to calculate the probability for the grouping as a whole. this is multiplied by an estimate of the likelihood of correctness for the match  assuming a non-accidental grouping  that has been entered for each element of the association list  and these final values are used to order the search. we plan to explore methods for incrementally learning the required probability values in future research. 
verification of interpretations 
the verification component of scerpo is able to take a tentative match between a couple of image features and model features and return a reliable answer as to whether the match is correct. if the object is present  this module will extend the match as much as possible and determine the precise viewpoint. 
given the initial set of correspondences  the iterative 

d. lowe 1 

figures 1: the original image of some desk staplers is shown in fig. 1. this image was convolved with a v1g function  a - 1 pixels . the zero-crossings of this function are shown in fig. 1. the gradiant of the convolved image was measured  and fig. 1 shows only those zero-crossings at locations where the function had a gradient above a selected threshold value. fig. 1 shows the segments that resulted from linking of zero-crossings and selection of the most significant straight-line segmentations  shown superimposed on the original image . fig. 1 shows the two perceptual groupings that were actually used to initiate successful recognition. after solving for model viewpoint  selecting new segments most consistent with model predictions  and iterating  the segments shown in fig. 1 were selected as being consistent with one viewpoint. 

1 d. lowe 

figures 1: these final figures show the object model projected onto the image from the two final calculated viewpoints. the slight orientation error in one direction in fig. 1 is due to small inaccuracies in the model and image measurements as well as the small amount of data being used to determine viewpoint. 

viewpoint-solving procedure described earlier is used to determine the best viewpoint that would project the model features onto the image features. the current implementation solves only for viewpoint and does not allow variable model parameters. if large errors remain following the least-squares fit  the solution is rejected as inconsistent. all edge features from the model are then projected onto the image using the calculated viewpoint  and the image data structure is searched for segments that are close to the predictions. matches are evaluated according to the degree of agreement in transverse location  orientation  and length with the prediction  and according to the lack of ambiguity between competing matches for a single object feature. this evaluation is used to rank the potential matches and only those above a high threshold value  or else the single highestranked match  is selected. the selected matches are combined with the original matches and the least-squares viewpoint determination is repeated. an estimate is maintained of the error bounds  based upon the number of matches and the least-squares deviations  so that instances of ambiguity become less likely as the viewpoint estimate improves. the set of matches is repeatedly extended until no more can be found. the final result of this process is the selection of a set of segments  as shown in figure 1  that are consistent with a single viewpoint of the model  as shown in figure 1. 
　　the current verification process in scerpo could clearly be extended to include many other aspects of verification than just the matching of line segments. for example  the viewpoint determination for the model instance shown in figure 1 has a small error in orientation  due to errors in image measurements and the small number of segments being used for the least-squares matching. however  given this degree of recognition  it would now be straightforward to go back to the original image data or zero-crossings and make further image measurements. 
implementation details 
scerpo is written in several different languages. the image processing components are executed on a vicom image processor under the vsh software facility developed by robert hummel and dayton clark  1j. the vicom can perform a 1 convolution against the entire image in a single video frame time. however  our edge detection method uses an 1 convolution that is performed by 1 of the 1 convolutions and the appropriate image translations and additions. the steps up to figure 1 are performed on the vicom  after which the zero-crossing image is transferred to a vax 1 running unix 1 for subsequent processing. a program written in c reads the original image and produces a file of linked edge points  requiring about 1 seconds of cpu time . all other components are written in franz lisp. segmentation into straight line segments requires 1 seconds  indexing and grouping operations require about 1 minute and the later stages of matching and verification 
took 1 seconds for this example. 
　　the object models used by the system consists merely of a set of straight 1-d line segments. each segment has a simple visibility specification  listing viewpoint ranges over which it is visible. a full hidden-line algorithm and more complete object models would improve the performance of the system. 

d. lowe 1 
conclusions and future research 

the current capabilities of scerpo provide a framework that could be used to incorporate numerous additional capabilities  each of which would improve the generality of the system or its level of performance. a brief list of these possible extensions might include the following: incorporation of a wider range of perceptual grouping operations  the ability to handle variable model parameters  the recognition of object components and their subsequent combination  more complete modeling with surface information and hidden-line algorithms  the use of color and texture information  the expanded use of evidential reasoning  the incremental learning of associations and probabilities  the detection of curve segments as well as straight lines  and more detailed verification in terms of the original image data. 
　　perceptual organization and the methods for achieving spatial correspondence offer an alternative to the use of depth reconstruction and matching in three-dimensions. it has been argued in this paper that most instances of recognition in human vision also work directly from twodimensional data. it should be possible to provide a definitive answer to this question by designing psychophysical experiments that test human recognition capabilities with different combinations of available information. a final answer to this question would carry many implications for the future design of knowledge-based vision systems. 
acknowledgments 
implementation of the scerpo system relied upon the extensive facilities and software of the nyu vision laboratory  which are due to the efforts of robert hummel  jack schwartz  and many others. robert hummel  in particular  provided many important kinds of technical and practical assistance during the implementation process. mike overton provided help with the 
numerical aspects of the design. much of the theoretical basis for this research was developed while the author was at the stanford artificial intelligence laboratory  with the help of tom binford  
rod brooks  chris goad  david marimont  andy witkin  and many others. 
