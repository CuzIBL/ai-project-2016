
inverse reinforcement learning  irl  is the problem of learning the reward function underlying a markov decision process given the dynamics of the system and the behaviour of an expert. irl is motivated by situations where knowledge of the rewards is a goal by itself  as in preference elicitation  and by the task of apprenticeship learning  learning policies from an expert . in this paper we show how to combine prior knowledge and evidence fromthe expert's actions to derive a probability distribution over the space of reward functions. we present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. experimental results show strong improvement for our methods over previous heuristic-based approaches.
1 introduction
the inverse reinforcement learning  irl  problem is defined in  russell  1  as follows: determine the reward function that an agent is optimizing. given 1  measurement of the agent's behaviour over time  in a variety of circumstances 1  measurements of the sensory inputs to that agent; 1  a model of the environment. in the context of markov decision processes  this translates into determining the reward function of the agent from knowledgeof the policy it executes and the dynamics of the state-space.
¡¡there are two tasks that irl accomplishes. the first  reward learning  is estimating the unknown reward function as accurately as possible. it is useful in situations where the reward function is of interest by itself  for example when constructing models of animal and human learning or modelling opponent in competitive games. pokerbots can improve performance against suboptimal human opponents by learning reward functions that account for the utility of money  preferences for certain hands or situations and other idiosyncrasies  billings et al.  1 . there are also connections to various preferenceelicitation problemsin economics sargent  1 .
¡¡the second task is apprenticeship learning - using observations of an expert's actions to decide one's own behaviour. it is possible in this situation to directly learn the policy from the expert  atkeson and schaal  1 . however the reward function is generally the most succint  robust and transferable representation of the task  and completely determines the optimal policy  or set of policies . in addition  knowledge of the reward function allows the agent to generalize better i.e. a new policy can be computed when the environment changes. irl is thus likely to be the most effective method here.
¡¡in this paper we model the irl problem from a bayesian perspective. we consider the actions of the expert as evidence that we use to update a prior on reward functions. we solve reward learning and apprenticeship learning using this posterior. we perform inference for these tasks using a modified markov chain monte carlo  mcmc  algorithm. we show that the markov chain for our distribution with a uniform prior mixes rapidly  and that the algorithm converges to the correct answer in polynomial time. we also show that the original irl is a special case of bayesian irl  birl  with a laplacian prior.
¡¡there are a number of advantages of our technique over previous work: we do not need a completely specified optimal policy as input to the irl agent  nor do we need to assume that the expert is infallible. also  we can incorporate external information about specific irl problems into the prior of the model  or use evidence from multiple experts.
¡¡irl was first studied in the machine learning setting by  ng and russell  1  who described algorithms that found optimal rewards for mdps having both finite and infinite states. experimental results show improved performance by our techniques in the finite case.
¡¡the rest of this paper is organised as follows: in section 1 we define our terms and notation. section 1 presents our bayesian model of the irl process. section 1 discusses how to use this model to do reward learning and apprenticeship learning while section 1 discusses the sampling procedure. sections 1  1 and 1 then present experimental results  related work and our conclusions respectively.
1 preliminaries
we recall some basic definitions and theorems relating to markov decision processes and reinforcement learning.
	a	 finite 	markov	decision	problem	is	a	tuple
 s a t ¦Ã r  where
  s is a finite set of n states.
  a = {a1 ... ak} is a set of k actions.
is a transition probability func-
tion.
  ¦Ã ¡Ê  1  is the discount factor. is a reward function  with absolute value
bounded by rmax.
¡¡the rewards are functions of state alone because irl problems typically have limited information about the value of action and we want to avoid overfitting.
¡¡a markov decision process  mdp  is a tuple  s a t ¦Ã   with the terms defined as before but without a reward function. to avoid confusion we will use the abbreviation mdp only for markov decision processes and not problems.
¡¡we adopt the following compact notation from  ng and russell  1 for finite mdps : fix an enumerations1 ...sn of the finite state space s. the reward function  or any other function on the state-space  can then be represented as an ndimensional vector r  whose ith element is r si .
¡¡a  stationary  policy is a mapand the  discounted  infinite-horizon value of a policy ¦Ð for reward function r at state s ¡Ê s denoted v ¦Ð s r  is given by:
v ¦Ð st1 r  = r st1 +est1 st1 ... ¦Ãr st1 +¦Ã1r st1 +...|¦Ð  where pr sti+1|sti ¦Ð  = t sti ¦Ð sti  sti+1 . the goal of standard reinforcement learning is to find an optimal policy ¦Ð  such that v ¦Ð s r  is maximized for all s ¡Ê s by ¦Ð = ¦Ð . indeed  it can be shown  see for example  sutton and barto  1   that at least one such policy always exists for ergodic mdps. for the solution of markov decision problems  it is useful to define the following auxilliary q-function:
 r  
we also define the optimal q-function q  ¡¤ ¡¤ r  as the qfunction of the optimal policy ¦Ð  for reward function r. finally  we state the following result concerning markov decision problems  see  sutton and barto  1   :
theorem 1  bellman equations . let a markov decision
problem m =  s a t ¦Ã r  and a policybe given. then 
1. for all s ¡Ê s a ¡Ê a v ¦Ð and q¦Ð satisfy
1. ¦Ð is an optimal policy for m iff  for all s ¡Ê s 
	¦Ð s  ¡Ê argmaxq¦Ð s a 	 1 
a¡Êa
1 bayesian irl
irl is currently viewed as a problem of infering a single reward function that explains an agent's behaviour. however  there is too little information in a typical irl problem to get only one answer. for example  consider the mdp shown in figure 1. there are at least three reasonable kinds of reward functions: r1 ¡¤  has high positive value at s1  and low values elsewhere  which explains why the policy tries to return to this state  while r1 ¡¤  and r1 ¡¤  have high values at s1 and s1 respectively. thus  a probability distribution is needed to represent the uncertainty.

figure 1: an example irl problem. bold lines represent the optimal action a1 for each state and broken lines represent some other action a1. action a1 in s1 has probabilities 1 1 and 1 of going to states s1 s1 s1 respectively  and all other actions are deterministic.
1 evidence from the expert
now we present the details of our bayesian irl model  fig. 1 . we derive a posterior distribution for the rewards from a prior distribution and a probabilistic model of the expert's actions given the reward function.
consider an mdp m =  s a t ¦Ã  and an agent x
 the expert  operating in this mdp. we assume that a reward function r for x is chosen from a  known  prior distribution pr. the irl agent receives a series of observations of the expert's behaviour ox = { s1 a1   s1 a1 ... sk ak } which means that x was in state si and took action ai at time step i. for generality  we will not specify the algorithm that x uses to determine his  possibly stochastic  policy  but we make the following assumptions about his behaviour:
1. x is attempting to maximize the total accumulated reward according to r. for example  x is not using an epsilon greedy policy to explore his environment.
1. x executes a stationary policy  i.e. it is invariant w.r.t. time and does not change depending on the actions and observations made in previous time steps.
for example  x could be an agent that learned a policy for  m r  using a reinforcement learning algorithm. because the expert's policy is stationary  we can make the following independence assumption:
r 
¡¡the expert's goal of maximizing accumulated reward is equivalentto finding the action for which the q  value at each state is maximum. therefore the larger q  s a  is  the more likely it is that x would choose action a at state s. this likelihood increases the more confident we are in x's ability to select a good action. we model this by an exponential distribution for the likelihood of  si ai   with q  as a potential function:

where ¦Áx is a parameter1 representing the degree of confidence we have in x's ability to choose actions with high

figure 1: the birl model
value. this distribution satisfies our assumptions and is easy to reason with. the likelihood of the entire evidence is :

where e ox r  = pi q  si ai r  and z is the appropriate normalizing constant. we can think of this likelihood function as a boltzmann-type distribution with energy e ox  r  and temperature.
¡¡now  we compute the posterior probability of reward function r by applying bayes theorem 
		 1 
¡¡computing the normalizing constant z is hard. however the sampling algorithms we will use for inference only need the ratios of the densities at two points  so this is not a problem.
1 priors
when no other information is given  we may assume that the rewards are independently identically distributed  i.i.d.  by the principle of maximum entropy. most of the prior functions considered in this paper will be of this form. the exact prior to use however  depends on the characteristics of the problem:
1. if we are completely agnostic about the prior  we canuse the uniform distribution over the space  rmax ¡Ü r s  ¡Ü rmax for each s ¡Ê s. if we do not want to specify any rmax we can try the improper prior p r  = 1 for all r ¡Ê rn.
1. many real world markov decision problems have parsimonious reward structures  with most states having negligible rewards. in such situations  it would be better to assume a gaussian or laplacian prior:


on ¦Áx as well  fig 1 . but it will be simpler to treat ¦Áx as just a parameter of the distribution.

1. if the underlying mdp represented a planning-typeproblem  we expect most states to have low  or negative  rewards but a few states to have high rewards  corresponding to the goal ; this can be modeled by a beta distribution for the reward at each state  which has modes at high and low ends of the reward space:

¡¡in section 1  we give an example of how more informative priors can be constructed for particular irl problems.
1 inference
we now use the model of section 1 to carry out the two tasks described in the introduction: reward learning and apprenticeship learning. our general procedure is to derive minimal solutions for appropriateloss functionsoverthe posterior eq. 1 . some proofs are omitted for lack of space.
1 reward learning
reward learning is an estimation task. the most commonloss functions for estimation problems are the linear and squared error loss functions:
r   r  1
r   r  1
where r and r  are the actual and estimated rewards  respectively. if r is drawn from the posterior distribution  1   it can be shown that the expected value of lse r r   is minimized by setting r  to the mean of the posterior  see  berger  1  .
similarily  the expected linear loss is minimized by setting r  to the median of the distribution. we discuss how to compute these statistics for our posterior in section 1.
¡¡it is also common in bayesian estimation problems to use the maximum a posteriori  map  value as the estimator. in fact we have the following result:
theorem 1. when the expert's policy is optimal and fully specified  the irl algorithm of  ng and russell  1  is equivalent to returning the map estimator for the model of  1  with a laplacian prior.
¡¡however in irl problems where the posterior distribution is typically multimodal  a map estimator will not be as representative as measures of central tendency like the mean.
1 apprenticeship learning
for the apprenticeship learning task  the situation is more interesting. since we are attempting to learn a policy ¦Ð  we can formally define the following class of policy loss functions:
 
where v   r  is the vector of optimal values for each state acheived by the optimal policy for r and p is some norm. we wish to find the ¦Ð that minimizes the expected policy loss over the posterior distribution for r. the following theorem accomplishes this:
theorem 1. given a distribution p r  over reward functions r for an mdp  s a t ¦Ã   the loss function lppolicy r ¦Ð  is minimized for all p by ¦Ðm    the optimal policy for the markov decision problem m =  s a t ¦Ã ep r  .
proof. from the bellman equations  1  we can derive the following:
                 v ¦Ð r  =  i   ¦Ãt¦Ð  1r  1  where t¦Ð is the |s|¡Á|s| transition matrix for policy ¦Ð. thus  for a state s ¡Ê s and fixed ¦Ð  the value function is a linear function of the rewards:
v ¦Ð s r  = w s ¦Ð  ¡¤ r
where w s ¦Ð  is the s'th row of the coefficient matrix  i   ¦Ãt¦Ð  1 in  1 . suppose we wish to maximize e v ¦Ð s r   alone. then 
maxe v ¦Ð s r   = maxe w s  ¦Ð ¡¤r  = maxw s ¦Ð ¡¤e r  ¦Ð ¦Ð ¦Ð
¡¡by definition this is equal to  the optimum value function for m  and the maximizing policy ¦Ð is ¦Ðm    the optimal policy for m. thus for all states s ¡Ê s  e v ¦Ð s r   is maximum at ¦Ð = ¦Ðm  .
¡¡but v   s r  ¡Ý v ¦Ð s r  for all s ¡Ê s  reward functions r  and policies ¦Ð. therefore

is minimized for all p by ¦Ð = ¦Ðm  .	
¡¡so  instead of trying a difficult direct minimization of the expected policy loss  we can find the optimal policy for the mean reward function  which gives the same answer.
1 sampling and rapid convergence
we have seen that both reward learning and apprenticeship learning require computing the mean of the posterior distribution. however the posterior is complex and analytical derivation of the mean is hard  even for the simplest case of the uniform prior. instead  we generate samples from these distributions and then return the sample mean as our estimate of the true mean of the distribution. the sampling technique we use is an mcmc algorithm gridwalk  see  vempala  1   that generates a markov chain on the intersection points of a grid of length ¦Ä in the region r|s|  denoted r|s|/¦Ä .
¡¡however  computing the posterior distribution at a particular point r requires calculation of the optimal q-function for r  an expensive operation. therefore  we use a modified version of gridwalk called policywalk  figure 1  that is more efficient: while moving along a markov chain  the sampler also keeps track of the optimal policy ¦Ð for the current reward vector r. observe that when ¦Ð is known  the q function can be reduced to a linear function of the reward variables  similar to equation 1. thus step 1b can be performed efficiently. a change in the optimal policy can easily be detected when moving to the next reward vector in the chain r   because then for some  s a  ¡Ê  s a   q¦Ð s ¦Ð s  r     q¦Ð s a r   by theorem 1. when this happens  the new optimal policy is usually only slightly different from the old one and can be computed by just a few
algorithm policywalk distribution p  mdp m  step size ¦Ä  
1. pick a random reward vector r¡Êr|s|/¦Ä.
1. ¦Ð := policyiteration m r 
1. repeat
 a  pick a reward vector r  uniformly at random from the neighbours of r in r|s|/¦Ä.
 b  compute q¦Ð s a r   for all  s a  ¡Ê s a.
 c  if   s a  ¡Ê  s a   q¦Ð s ¦Ð s  r     q¦Ð s a r  
i. ¦Ð  := policyiteration m r  ¦Ð  ii. set r := r  and ¦Ð := ¦Ð  with probability
else
i. set r := r  with probability
1. return rfigure 1: policywalk sampling algorithm
steps of policy iteration  see  sutton and barto  1   starting from the old policy ¦Ð. hence  policywalk is a correct and efficient sampling procedure. note that the asymptotic memory complexity is the same as for gridwalk.
¡¡the second concern for the mcmc algorithm is the speed of convergence of the markov chain to the equilibrium distribution. the ideal markov chain is rapidly mixing  i.e. the number of steps taken to reach equilibrium is polynomially bounded   but theoretical proofs of rapid mixing are rare. we will show that in the special case of the uniform prior  the markov chain for our posterior  1  is rapidly mixing using the following result from  applegate and kannan  1  that bounds the mixing time of markov chains for pseudo-logconcave functions.
lemma 1. let f ¡¤  be a positive real valued function defined on the cube {x ¡Ê rn|   d ¡Ü xi ¡Ü d} for some positive d  satisfying for all ¦Ë ¡Ê  1  and some ¦Á ¦Â

and
f ¦Ëx +  1   ¦Ë y  ¡Ý ¦Ëf x  +  1   ¦Ë f y    ¦Â
where f x  = logf x . then the markov chain induced by gridwalk  and hence policywalk  on f rapidly mixes to within	of steps.
proof. see  applegate and kannan  1 .	
theorem 1. given an mdp m =  s a t ¦Ã  with |s| =
n  and a distribution over rewards p r  = prx  r|ox   defined by  1  with uniform prior pr over c = {r ¡Ê rn|   rmax ¡Ü ri ¡Ü rmax}. if rmax = o 1/n  then p can be efficiently sampled  within error steps by algorithm policywalk.
proof. since the uniform prior is the same for all points r  we can ignore it for sampling purposesalong with the normalizing constant. therefore  let f r  = ¦Áxe ox  r . now choose some arbitrary policy ¦Ð and let


figure 1: reward loss.
note that f¦Ð is a linear function of r and f r  ¡Ý f¦Ð r   for all r ¡Ê c. also we have 

similarly  . therefore  f r  ¡Ü
¦Áxnrmax
	1 ¦Ã	andand hence

so for all r1 r1 ¡Ê c and ¦Ë ¡Ê  1  
f ¦Ër1 +  1   ¦Ë r1 ¡Ýf¦Ð ¦Ër1 +  1   ¦Ë r1 ¡Ý¦Ëf¦Ð r1  +  1   ¦Ë f¦Ð r1 ¡Ý¦Ëf r1  +  1   ¦Ë f r1  1¦Áxnrmax
 	1   ¦Ã¡¡therefore  f satisfies the conditions of lemma 1 with ¦Â =  and

hence the markov chain induced by the gridwalk algorithm  and the policywalk algorithm  on p mixes rapidly to within of in a number of steps equal to
.	
¡¡note that having rmax = o 1/n  is not really a restriction because we can rescale the rewards by a constant factor k after computing the mean without changing the optimal policy and all the value functions and q functions get scaled by k as well.
1 experiments
we compared the performance of our birl approach to the irl algorithm of  ng and russell  1  experimentally. first  we generated random mdps with n states  with n varying from 1 to 1 and rewards drawn from i.i.d. gaussian priors. then  we simulated two kinds of agents on these mdps and used their trajectories as input: the first learned

figure 1: policy loss.

figure 1: scatter diagrams of sampled rewards of two arbitrary states for a given mdp and expert trajectory. our computed posterior is shown to be close to the true distribution.
a policy by q-learning on the mdp + reward function. the learning rate was controlled so that the agent was not allowed to converge to the optimal policy but came reasonably close. the second agent executed a policy that maximized the expected total reward over the next k steps  k was chosen to be slightly below the horizon time .
¡¡for birl  we used policywalk to sample the posterior distribution  1  with a uniform prior. we compared the results of the two methods by their average  1 distance from the true reward function  figure 1  and the policy loss with  1 norm  figure 1  of the learned policy under the true reward. both measures show substantial improvement. note that we have used a logarithmic scale on the x-axis.
¡¡we also measured the accuracy of our posterior distribution for small n by comparing it with the true distribution of rewards i.e. the set of generated rewards that gave rise to the same trajectory by the expert. in figure 1  we show scatter plots of some rewards sampled from the posterior and the true distribution for a 1-state mdp. these figures show that the posterior is very close to the true distribution.
1 from domain knowledge to prior
to show how domain knowledge about a problem can be incorporated into the irl formulation as an informative prior 

figure 1: ising versus uninformed priors for adventure games
we applied our methods to learning reward functions in adventure games. there  an agent explores a dungeon  seeking to collect various items of treasure and avoid obstacles such as guards or traps. the state space is represented by an m-dimensional binary feature vector indicating the position of the agent and the value of various fluents such as haskey and doorlocked. if we view the state-space as an m-dimensional lattice ls  we see that neighbouring states in ls are likely to have correlated rewards  e.g. the value of doorlocked does not matter when the treasure chest is picked up . to model this  we use an ising prior  see  cipra  1  :

where n is the set of neighbouring pairs of states in ls and j and h are the coupling and magnetization parameters.
¡¡we tested our hypothesis by generating some adventure games  by populating dungeons with objects from a common sense knowledge base  and testing the performance of birl with the ising prior versus the baseline uninformed priors. the results are in figure 1 and show that the ising prior does significantly better.
1 related work
the initial work on irl was done by  ng and russell  1  while  abbeel and ng  1  studied the special case where rewards can be represented as linear combinations of features of the state space and gave a max-margin characterization of the optimal reward function.  price and boutilier  1  discusses a bayesian approach to imitating the actions of a mentor during reinforcement learning whereas the traditional literature on apprenticeship learning tries to mimic the behaviour of the expert directly  atkeson and schaal  1 .
¡¡outside of computer science  irl-related problems have been studied in various guises. in the physical sciences  there is a body of work on inverse problem theory  i.e. infering values of model parameters from observations of a physical system  tarantola  1 . in control theory   boyd et al.  1  solved the problem  posed by kalman  of recovering the objective function for a deterministic linear system with quadratic costs.
1 conclusions and future work
our work shows that improved solutions can be found for irl by posing the problem as a bayesian learning task. we provided a theoretical framework and tractable algorithms for bayesian irl and our solutions contain more information about the reward structure than other methods. our experiments verify that our solutions are close to the true reward functions and yield good policies for apprenticeship learning. there are a few open questions remaining:
1. are there more informative priors that we can constructfor specific irl problemsusing backgroundknowledge 
1. how well does irl generalize  suppose the transitionfunction of the actor and the learner differed  how robust would the reward function or policy learned from the actor be  w.r.t the learner's state space 
acknowledgements
the authors would like to thank gerald dejong and alexander sorokin for useful discussions  and nitish korula for help with preparing this paper. this research was supported by
nsf grant iis 1 and cerl grant daca1-d1.
