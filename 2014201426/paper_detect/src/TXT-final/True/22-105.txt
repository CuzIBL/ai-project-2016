 
one goal of explanation-based learning is to transform knowledge into an operational form for efficient use. typically  this involves rewriting concept descriptions in terms of the predicates used to describe examples. in this paper we present r i n c o n   a system that extends domain theories from examples with the goal of maximizing classification efficiency. rincon's basic learning operator involves the introduction of new intermediate concepts into a domain theory  which can be viewed as the inverse of the operationalization process. we discuss the system's learning algorithm and its relation to work on explanation-based learning  incremental concept formation  representation change  and pattern matching. we also present experimental evidence from two natural domains that indicates the addition of intermediate concepts can improve classification efficiency. 
1 	introduction 
knowledge is necessary but not sufficient for intelligent behavior. in addition  knowledge must be stored in some form that lets it be used effectively. one of the central goals of machine learning is to devise mechanisms that transform knowledge from inefficient forms into more efficient ones. most research on this topic has focused on explanation-based learning  mitchell et al .  1  dejong and mooney  1   which augments a domain theory with rules that are more 'operational' than the original ones. such operational rules let one bypass intermediate concepts  producing shallower proofs on future cases with the same structure. 
   in this paper  we show that more operational knowledge does not always lead to more efficient behavior. in addition  we describe an alternative approach that involves the introduction of new intermediate concepts into the domain theory - effectively the inverse of operationalization. we show that  at least in some domains  this form of learning leads to more efficient forms of knowledge than do explanation-based methods. 
　in the following section we describe rincon  retaining intermediate concepts   a learning system that implements our approach to the transformation of domain knowledge. after this  we report experiments with the system on two natural domains. finally  we show how rincon provides a framework for integrating explanation-based learning  incremental concept formation  representation change  and pattern matching. 
1 	overview of rincon 
1 	representation and organization 
r i n c o n is a system that forms domain theories from examples with the goal of maximizing classification efficiency. instances are represented as conjunctions of n-ary predicates  allowing one to represent not only attributes  but also relations  vere  1 . for example  father  a b  a female  b  expresses a father-daughter relationship. instances also contain a class label that is used for supervised learning. 

figure 1. a domain theory/hierarchy for family relationships. 
　instances and concepts are stored hierarchically in a domain theory that is partially ordered according to the generality of the concepts. figure 1 shows a simple hierarchy of concepts from a domain theory for family relationships. the highest-level concepts in the domain theory are the primitive features  predicates  used to represent instances. the lowest-level concepts correspond to the classes found in the training examples and may be disjunctive. the learned internal concepts must be con-
junctive  appearing in the head of only one rewrite rule. all concepts are expressed in terms of higher-level concepts in the domain theory. for example  figure 1 shows primitive features used to describe the concept brother  which is used to describe the concept uncle. 
	wogulis and langley 	1 
1 	t h e performance system 
the domain theory is used to classify instances. given an instance and a concept  rincon determines if the instance is described by the concept. if the concept is relational  conjunctions of n-ary predicates   then the system also determines all of the ways  different bindings  in which the instance is a member of the concept. the matching process is goal directed  starting with the concept to be determined and recursively finding all matches for each subconcept composing the concept.1 each time a concept node is matched  the resulting bindings are stored with that concept's node. by storing all matches for all relevant sub concepts  time may be saved if the bindings are needed again. the match algorithm is shown in table 1. 
   as an example of how internal concepts can improve overall match efficiency  consider the following simple domain theory for the concept uncle:1 
uncle x y   - male x  a sibling x z  a mother z y  uncle x y   - male x  a sibling x z  a father z y . 
now suppose this domain theory is used to determine all of the uncle relations in the instance male  pat  a sibling pat john  a father john jean  a male frank  a sibling frank marie  a mother marie jean . since there are two uncles in the instance  the matcher would have to re-join the bindings from the male and sibling concepts. instead  suppose the domain theory included the concept brother: 

this domain theory would be more efficient to use since the work of matching the brother concept would only be done once when matching against the two definitions for 
1
　　this differs from logic programming. instances in rlncon may contain variables but are treated as constants by the matcher. hence  it does not perform unification. 
1
 another type of uncle is the husband of an aunt. 
1 	machine learning 
uncle. the next section describes how one can acquire such internal concepts. 
1 	t h e rincon learning a l g o r i t h m 
the rincon system begins with an initial domain theory and incrementally extends it to incorporate new instances. at present  the learned theory does not go beyond the data; it simply organizes the instances according to the existing domain theory and any learned intermediate concepts. rincon's goal is to produce domain theories that maximize the classification efficiency for both seen and unseen instances. table 1 presents the algorithm for learning new intermediate concepts. 
table 1. algorithm for learning intermediate concepts 
   rlncon's learning algorithm carries out incremental hill climbing  gennari et a/.  1  through the space of domain theories. the system starts by matching the new instance against the concept with the same label. if the instance is described by the domain theory  then no learning occurs and the existing theory is retained. otherwise  it collects the most specific concepts that match the instance and the most general concepts that do not match the instance. the system then re-expresses the instance in terms of the concepts it does match and adds it to the domain theory as a new disjunct for its concept class. the re-expressed instance is then generalized  vere  1  with each concept in the set of most general concepts it does not match. each of these generalizations is a candidate for a new internal concept. rlncon's evaluation function selects the generalization that can be used to re-express the most concepts in the domain theory. the selected generalization is then added to the theory and used to re-express all of the concepts in the domain theory that it can. 
　as an example  assume the following domain theory  which contains only one instance: 

if rlncon is presented with the new instance 

uncle pat  jean   - male pat  a sibling pat john  a tather j ohn j ean  
it finds that the concept uncle in the domain theory does not match this instance. the system then finds the most specific concepts in the theory that do match  male  sibling  and father   and the most general concepts that do not match  uncle . illncon then rewrites the instance using the highest-level concepts matched. since these are simply the primitive features  the instance description remains unchanged. the instance is then added to the domain theory and is generalized with all of the lowest-level concepts that do not match  in this case uncle. the only maximally specific generalization is male x  a sibling x y   which is added to the domain theory. this generalization is used to rewrite both of the uncle definitions to produce the following domain theory:1 

rlncon continues processing new instances  extending the domain theory to incorporate each new instance. 
1 	experimental evaluation of rlncon 
the goal of rlncon is to improve the efficiency of matching instances. since the system currently does no induction  classification accuracy is irrelevant. instead  the natural unit of measure is the amount of work required to match or reject an instance. we measure work in terms of the number of join operations performed in the match process. a join occurs when two lists of bindings are combined to form a new consistent bindings list  which might be empty if the bindings are inconsistent . for attribute-value representations the join of n attributes is n - 1  since multiple bindings are never produced. the number of joins provides a reasonable measure of work since at least one join occurs whenever a concept node in the hierarchy is matched  see match-disjunct in table 1 . also  the time required to perform a join is bounded by a constant for any given domain. 
　as a baseline for comparison in all of our experiments we measured the work performed by a corresponding domain theory with no intermediate concepts.1 this 'flat' domain theory is simply an extensional description of all the observed instances. 
　our first experiment involved building a domain theory from instances of mushrooms  schlimmer  1  in which each instance was described as a conjunction of 1 attribute-value pairs. a total of 1 instances were available. the experiment began with an empty domain theory  to which rlncon incrementally added randomly chosen instances. after every ten instances were incorporated into the domain theory  we computed the average amount of work required for matching each of the pre-
1
 we have named the new concept brother only for clarity. 
1
　　this is equivalent to a domain theory containing only 'operational' definitions. 
viously seen instances. we also measured the average amount of work for matching the same number of mushroom instances not described by the domain theory. figure 1 presents the learning curves for the average work of matching an instance as a function of the number of instances stored in the domain theory. each curve shows the average over 1 different runs. 

　the figure shows that the domain theory containing intermediate concepts was on average more efficient at matching previously seen instances than was the corresponding flat domain theory. surprisingly  the flat theory also required more match time to reject previously unseen instances than did the learned domain theory. this suggests that the learned theory contains intermediate concepts shared among all mushroom instances. such intermediate concepts would save on the overall match time for unseen instances  since they would store bindings often needed in the match process. 
　the results presented in figure 1 seem to run counter to the notion that operational domain theories are more efficient to use than those containing intermediate concepts. however  for some instances the flat domain theory is more efficient. at the end of each of the 1 experiments  for each 1 mushroom instances processed  we computed the percentage of work saved by using the learned domain theory over the flat one. figure 1 shows the distribution of instances as a function of the percentage of work saved. although work is saved on average  intermediate concepts sometimes do reduce efficiency. this suggests a trade-off between retaining intermediate concepts and operationalizing concepts. 
　the mushroom experiments measured the efficiency of learned domain theories as a function of the number of instances processed. the size of each mushroom instance was constant. our second experiment measured the efficiency of learned domain theories as a function of the size of the instances matched while holding the number of instances in the domain theory constant. this experiment involved using rincon to organize the rules of a production system. in this case  the instances' used to build the domain theory were the condition sides of production rules. unlike the mushroom domain  these instances were relational and contained variables. the production system solved multi-column subtraction problems  langley and ohlsson  1  such as 1 - 1 using a set of nine 
	wogulis and langley 	1 

rules. the rule set included such operators as subtracting two numbers in a column  shifting attention from one column to another  and borrowing ten from a colu m n . the production rules were written such that only one rule w i t h one set of bindings ever matched against working memory. 

   the experiment consisted of running the production system on sets of subtraction problems of varying complexity  measured as the maximum number of columns in the problem. each problem was solved using the domain theory of rules built by rlncon and the corresponding flat theory to find which rules matched against working memory. we computed the average work  number of 
joins  per production system cycle for both of the domain theories when solving each problem. each cycle of the production system requires matching the rules in the domain theory against working memory. 

   the graph in figure 1 shows the average amount of work per cycle as a function of instance size for both of the domain theories. each point in the graph is the average over 1 different subtraction problems at a given level of problem complexity. the curves for the flat domain theory and for the domain theory built by rlncon suggest that the average work per cycle is a linear function of the number of columns in the subtraction problem. this reflects the fact that the working memory increases linearly in the number of columns. overall  the domain theory built by r i n c o n required about half as much work as the flat domain theory. 
1 	machine learning 
1 	discussion 
the learning mechanism used in r i n c o n is closely related to methods used in four ai paradigms that have traditionally been viewed as quite diverse - explanationbased learning  incremental concept formation  representation change  and pattern matching. below we expand on these relations noting some directions for future research. 
1 . 1 	r e l a t i o n t o e x p l a n a t i o n - b a s e d l e a r n i n g 
our approach to learning has much in common with work on explanation-based learning  mitchell et a/.  1  dejong and mooney  1 . in both cases  domain knowledge is organized as a set of inference rules  recognition involves constructing a proof tree by chaining off those rules  and learning alters the structure of the domain theory by adding new inference rules. moreover  in both cases this process may affect the efficiency of recognition  but no induction is involved. 1 
   however  the basic operations used in the two frameworks differ radically. explanation-based learning modifies the knowledge base through a 'knowledge compilation* mechanism. the structure of an explanation is compiled into a new inference rule; this lets the performance system bypass intermediate terms on future cases w i t h the same structure  giving shallower explanations. in contrast  our approach creates new intermediate terms  leading to deeper explanation structures on future cases. one can view r i n c o n ' s mechanism for creating new terms as a 'decompilation' process - the inverse operation of that in explanation-based systems. 
   our experimental results indicate it is sometimes better to operationalize than to introduce intermediate concepts. an obvious extension to rlncon would be to include a mechanism for knowledge compilation in addition to that for new term creation. upon encountering a previously unseen situation  the system would extend the knowledge base  generating new terms in the process. upon recognizing a previously seen case  it would construct a compiled rule for matching the instance in a single inference step. to determine whether the compiled or uncompiled knowledge was more efficient  the system would keep statistics on each rule  eventually eliminating ones w i t h low utility  minton  1 . such an extension would constitute an important step towards unifying inductive and analytic approaches to learning. 
1 	r e l a t i o n t o i n c r e m e n t a l c o n c e p t f o r m a t i o n 
gennari  langley  and fisher  have reviewed work on incremental concept formation. in this framework one incrementally induces a taxonomy of concepts  which can then be used in classifying new instances and in making predictions. each instance is sorted through the taxonomy  altering the knowledge base in passing. 
   such learning can be characterized as an incremental form of hill climbing  in that only a single concept hier-
r 
     ln incremental mode  one can view rlncon as changing the deductive closure of its knowledge base  since it accepts new instances as input. however  the system does not move beyond the instances it is given. 

archy is retained in memory. examples of concept formation systems include levinson's  self-organizing system  lebowitz's  u n i m e m   fisher's  c o b -
w e b   and gennari et a/.'s  classit. 
　the learning method in r i n c o n can be viewed as a form of incremental concept formation. the domain theory constitutes a taxonomy  with primitive predicates as the most general concepts  instances as the most specific concepts  and defined terms as concepts of intermediate generality. new instances are 'sorted' down this concept hierarchy  and new concepts are introduced in the process. rincon's search control is an incremental form of hill climbing  preferring new terms that will be used by more existing concepts. 
   however  there are also some important differences between the two approaches. research on concept formation has typically focused on attribute-value representations  whereas rincon employs a relational formalism. most concept formation methods construct disjoint taxonomies  whereas rincon forms a nondisjoint hierarchy in which a concept may have multiple parents. finally  most earlier methods have employed partial matching techniques in the classification process  which let them make predictions about  unseen data. in contrast  our approach uses complete matching and thus only summarizes the observed instances. 
　the last difference suggests extensions to rincon that would let it move beyond the data to make predictions about unseen instances  i.e.  to do induction . the current system allows disjunctions only at the final level of the concept hierarchy  but the basic learning operator can be extended to create disjuncts at any level. the introduction of multiple disjuncts into a concept definition leads to coverage of unseen instances. a more radical approach involves deleting these structures entirely  so one need not match against them at all. in either case  the system would need to collect statistics to estimate the desirability of such drastic actions. 
1 	relation to representation change 
another active area of machine learning research focuses on changing representations by introducing new terms into the language of concept descriptions. for instance  given a primitive set of features  a learning system might define new terms as conjunctions or disjunctions of these features  and then attempt to induce a concept description over this extended language. a variety of researchers have taken this general approach to representation change in induction  fu and buchanan  1  schlimmer  1  muggleton  1  pagallo and haussler  1  rendell  1 . 
   rlncon's learning method involves a variety of rep resentation change. when the system introduces a new concept into its domain theory  it redefines existing concepts using this term. also  it uses these intermediate terms during the matching process to redescribe new instances. the more concepts in which an intermediate term is used  the more efficiently the system matches or rejects new instances. thus  the change in representation has a definite impact on performance. 
muggleton's  d u c e system employs constructive induction in much the same way as rincon  but has more operators for introducing new concepts. however  before a new concept is actually retained  the user is required to either accept or reject the concept. duce's main goal is to maximize the symbol reduction of the rule base while creating meaningful intermediate concepts. on the other hand  rincon's main goal is to improve the domain theory's efficiency of recognizing instances. also  rincon processes instances incrementally and handles relational input whereas d u c e is non-incremental and is limited to propositional calculus. 
　with the exception of fu and buchanan   most earlier research on representation change has emphasized classification accuracy rather than efficiency. another difference between rincon and other approaches involves its use of a relational formalism rather than a feature-based language. however  our work to date has dealt only with introducing new conjunctive terms. future versions of rincon should introduce disjunctive relational terms as well  as do most other methods for representation change. 
1 	relation to pattern matching 
research on production-system architectures has led to algorithms and data structures for efficient pattern matching. one of the best-known schemes involves rete networks  forgy  1   a memory organization that allows sharing of redundant conditions and storage of partial matches. this technique leads to significant reductions in the match time required for certain large production systems.1 
　the rete network approach to matching has many similarities to rincon's scheme. in both cases  the performance element stores partial matches at nodes in the network. more important  both methods construct internal nodes for this purpose  based on shared structures in the inputs. finally  in both cases the resulting 'domain theory' is purely conjunctive  in that internal nodes have only one definition. 
   however  rincon also differs in some significant ways from systems based on rete networks. first  forgy's framework assumes a binary network  in which each internal node is defined as the conjunction of two other nodes. in contrast  our system can use an arbitrary number of nodes in its definitions. second  methods for constructing rete networks typically detect shared structures only if they occupy the same positions in the condition sides of productions  and they automatically create nodes when they are found. rincon carries out a more sophisticated search for shared structures  and it employs an evaluation function to select among alternative concepts that it might construct. thus  our scheme can be viewed as a heuristic approach to constructing generalized rete networks  and future work should compare the two methods empirically. 
　levinson's  work on self-organizing retrieval for graphs also extends forgy's idea of improving retrieval efficiency by creating intermediate concepts. as in rincon  intermediate concepts correspond to common structures found among the relational examples stored in the database. they may be added or deleted according to a heuristic information-theoretic measure of retrieval efficiency. levinson's experiments in the retrieval of chemical structures show that introducing intermediate concepts results in only a fraction of the database  on the order of the log of the number of elements in the database  being compared to the query structure during retrieval. he also provides theoretical justification for this increase in efficiency. this reduction in search is critical in structured domains  in which the cost of comparison is potentially exponential in the size of the objects being compared. 
1 	conclusion 
rincon incrementally learns domain theories from examples with the goal of maximizing classification efficiency. the version described in this paper is only an initial step toward our goal of integrating inductive and explanation-based learning. we have focused here on aspects of the efficient use of knowledge  but future work should also address induction and the associated goal of maximizing classification accuracy. 
　our preliminary results indicate that introducing intermediate concepts into a domain theory can increase overall match efficiency. this result seems counter to the work on explanation-based learning  which holds that operationalization is the key to efficiency. however  our results suggest that both views are correct. by adding an operationalization component to rincon  we will be able to explore the efficiency tradeoff between operationalization and introducing new intermediate concepts. 
　finally  the rincon framework is also closely related to research in the areas of incremental concept formation  representation change  and pattern matching. our work impacts each of these areas and provides a framework for integrating these diverse fields. 
acknowledgements 
　we have benefited from discussions with robert levinson at the university of california  santa cruz. we would also like to thank wayne iba  john gennari  and mike pazzani for their discussions on this work. 
