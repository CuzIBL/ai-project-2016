
in many practical machine learning and data mining applications  unlabeled training examples are readily available but labeled ones are fairly expensive to obtain. therefore  semi-supervised learning algorithms such as co-training have attracted much attention. previous research mainly focuses on semi-supervised classification. in this paper  a co-training style semi-supervised regression algorithm  i.e. coreg  is proposed. this algorithm uses two k-nearest neighbor regressors with different distance metrics  each of which labels the unlabeled data for the other regressor where the labeling confidence is estimated through consulting the influence of the labeling of unlabeled examples on the labeled ones. experiments show that coreg can effectively exploit unlabeled data to improve regression estimates.
1	introduction
in many practical machine learning and data mining applications such as web user profile analysis  unlabeled training examples are readily available but labeled ones are fairly expensive to obtain because they require human effort. therefore  semi-supervised learning methods that exploit unlabeled examples in addition to labeled ones have attracted much attention.
　many current semi-supervised learning methods use a generative model for the classifier and employ expectationmaximization  em   dempster et al.  1  to model the label estimation or parameter estimation process. for example  mixture of gaussians  shahshahani and landgrebe  1   mixture of experts  miller and uyar  1   and naive bayes  nigam et al.  1  have been respectively used as the generative model  while em is used to combine labeled and unlabeled data for classification. there are also many other methods such as using transductive inference for support vector machines to optimize performance on a specific test set  joachims  1   constructing a graph on the examples such that the minimum cut on the graph yields an optimal labeling of the unlabeled examples according to certain optimization functions  blum and chawla  1   etc.
　a prominent achievement in this area is the co-training paradigm proposed by blum and mitchell   which trains two classifiers separately on two sufficient and redundant views  i.e. two attribute sets each of which is sufficient for learning and conditionally independent to the other given the class label  and uses the predictions of each classifier on unlabeled examples to augment the training set of the other.
　dasgupta et al.  have shown that when the requirement of sufficient and redundant views is met  the co-trained classifiers could make few generalization errors by maximizing their agreement over the unlabeled data. unfortunately  such a requirement can hardly be met in most scenarios. goldman and zhou  proposed an algorithm which does not exploit attribute partition. this algorithm requires using two different supervised learning algorithms that partition the instance space into a set of equivalence classes  and employs cross validation technique to determine how to label the unlabeled examples and how to produce the final hypothesis. although the requirement of sufficient and redundant views is quite strict  the co-training paradigm has already been used in many domains such as statistical parsing and noun phrase identification  hwa et al.  1  pierce and cardie  1  sarkar  1  steedman et al.  1 .
　it is noteworthy that previous research mainly focuses on classification while regression remains almost untouched. in this paper  a co-training style semi-supervised regression algorithm named coreg  i.e. co-training regressors  is proposed. this algorithm employs two k-nearest neighbor  knn  regressors  each of which labels the unlabeled data for the other during the learning process. in order to choose appropriate unlabeled examples to label  coreg estimates the labeling confidence through consulting the influence of the labeling of unlabeled examples on the labeled examples. the final prediction is made by averaging the regression estimates generated by both regressors. since coreg utilizes different distance metrics instead of requiring sufficient and redundant views  its applicability is broad. moreover  experimental results show that this algorithm can effectively exploit unlabeled data to improve regression estimates.
　the rest of this paper is organized as follows. section 1 proposes the coreg algorithm. section 1 presents an analysis on the algorithm. section 1 reports on the experimental results. finally  section 1 concludes and raises several issues for future work.
1	coreg
let l = { x1 y1  ，，，  x|l| y|l| } denote the labeled example set  where xi is the i-th instance described by d attributes  yi is its real-valued label  i.e. its expected realvalued output  and |l| is the number of labeled examples; let u denote the unlabeled data set  where the instances are also described by the d attributes  whose real-valued labels are unknown  and |u| is the number of unlabeled examples.
　two regressors  i.e. h1 and h1  are generated from l  each of which is then refined with the help of unlabeled examples that are labeled by the latest version of the other regressor. here the knn regressor  dasarathy  1  is used as the base learner to instantiate h1 and h1  which labels a new instance through averaging the real-valued labels of its k-nearest neighboring examples.
　the use of knn regressor as the base learner is due to the following considerations. first  the regressors will be refined in each of many learning iterations. if neural networks or regression trees are used  then in each iteration the regressors have to be re-trained with the labeled examples in addition to the newly labeled ones  the computational load of which will be quite heavy. since knn is a lazy learning method which does not hold a separate training phase  the refinement of the knn regressors can be efficiently realized. second  in order to choose appropriate unlabeled examples to label  the labeling confidence should be estimated. in coreg the estimation utilizes the neighboring properties of the training examples  which can be easily coupled with the knn regressors.
　it is noteworthy that the initial regressors should be diverse because if they are identical  then for either regressor  the unlabeled examples labeled by the other regressor may be the same as these labeled by the regressor for itself. thus  the algorithm degenerates to self-training  nigam and ghani  1  with a single learner. in the standard setting of cotraining  the use of sufficient and redundant views enables the learners be different. previous research has also shown that even when there is no natural attribute partitions  if there are sufficient redundancy among the attributes then a fairly reasonable attribute partition will enable co-training to exhibit advantages  nigam and ghani  1 . while in the extended co-training algorithm which does not require sufficient and redundant views  the diversity among the learners is achieved through using different learning algorithms  goldman and zhou  1 . since coreg does not assume sufficient and redundant views and different learning algorithms  the diversity of the regressors should be sought from other channels.
　here the diversity is achieved through utilizing different distance metrics. in fact  a key of knn learner is how to determine the distances between different instances. the minkowsky distance shown in eq. 1 is usually used for this purpose. note that different concrete distance metrics can be generated through setting different values to the distance order  p. roughly speaking  the smaller the order  the more robust the resulting distance metric to data variations; while the bigger the order  the more sensitive the resulting distance metric to data variations. therefore  the vicinities identified for a given instance may be different using the minkowsky distance with different orders. thus  the knn regressors h1 and h1 can be diverse through instantiating them with different p values. such a setting can also bring another profit  that is  since it is usually difficult to decide which p value is better for the concerned task  the functions of these regressors may be somewhat complementary to be combined.

　in order to choose appropriate unlabeled examples to label  the labeling confidence should be estimated such that the most confidently labeled example can be identified. in classification this is relatively straightforward because when making classifications  many classifiers can also provide an estimated probability  or an approximation  for the classification  e.g. a naive bayes classifier returns the maximum posteriori hypothesis where the posterior probabilities can be used  a bp neural network classifier returns thresholded classification where the real-valued outputs can be used  etc. therefore  the labeling confidence can be estimated through consulting the probabilities of the unlabeled examples being labeled to different classes. for example  suppose the probability of the instance a being classified to the classes c1 and c1 is 1 and 1  respectively  while that of the instance b is 1 and 1  respectively. then the instance a is more confident to be labeled  to class c1 .
　unfortunately  in regression there is no such estimated probability that can be used directly. this is because in contrast to classification where the number of class labels to be predicted is finite  the possible predictions in regression are infinite. therefore  a key of coreg is the mechanism for estimating the labeling confidence.
　heuristically  the most confidently labeled example of a regressor should be with such a property  i.e. the error of the regressor on the labeled example set should decrease the most if the most confidently labeled example is utilized. in other words  the most confidently labeled example should be the one which makes the regressor most consistent with the labeled example set. thus  the mean squared error  mse  of the regressor on the labeled example set can be evaluated first. then  the mse of the regressor utilizing the information provided by  xu y u  can be evaluated on the labeled example set  where xu is an unlabeled instance while y u is the real-valued label generated by the original regressor. let  u denote the result of subtracting the latter mse from the former mse. note that the number of  u to be estimated equals to the number of unlabeled examples. finally   xu y u  associated with the biggest positive  u can be regarded as the most confidently labeled example.
　since repeatedly measuring the mse of the knn regressor on the whole labeled example set in each iteration will be time-consuming  considering that knn regressor mainly utilizes local information  coreg employs an approximation. that is  for each xu  coreg identifies its k-nearest neighboring labeled examples and uses them to compute the mse. in detail  let   denote the set of k-nearest neighboring labeled examples of xu  then the most confidently labeled example is identified through maximizing the value of  xu in eq. 1  where h denotes the original regressor while h1 denotes the table 1: pseudo-code describing the coreg algorithm

algorithm: coreg
input: labeled example set l  unlabeled example set u  number of nearest neighbors k  maximum number of learning iterations t  distance orders p1 p1 process:
l1 ○ l; l1 ○ l
create pool u1 by randomly picking examples from u h1 ○ knn l1 k p1 ; h1 ○ knn l1 k p1  repeat for t rounds: for j （ {1} do for each xu （ u1 do
y u ○ hj xu    ○ neighbors xu k lj 
 h1jx○u ○knnp   ljy“ {i   hxju  xy i  u 1}  k p yji   h1j xi ♂1 
xi（ 
end of for if there exists an  xu   1 then x j ○ argmax xu; y j ○ hj x j 
（u1
else end of for
l1 ○ l1 “ π1; l1 ○ l1 “ π1
if neither of l1 and l1 changes then exit else
h1 ○ knn l1 k p1 ; h1 ○ knn l1 k p1 
   replenish u1 by randomly picking examples from u end of repeat
output: regressor 

refined regressor which has utilized the information provided by  xu y u . note that y u = h xu .
	u	x	1 xi  1 	 1 
	 x =	  yi   h xi      yi   h
xi（ 
　the pseudo code of coreg is shown in table 1  where the function knn lj k pj  returns a knn regressor on the labeled example set lj  whose distance order is pi. the learning process stops when the maximum number of learning iterations  i.e. t  is reached  or there is no unlabeled example which is capable of reducing the mse of any of the regressors on the labeled example set. according to blum and mitchell 's suggestion  a pool of unlabeled examples smaller than u is used. note that in each iteration the unlabeled example chosen by h1 won't be chosen by h1  which is an extra mechanism for encouraging the diversity of the regressors. thus  even when h1 and h1 are similar  the examples they label for each other will still be different.
1	analysis
this section attempts to analyze whether the learning process of coreg can use the unlabeled examples to improve the regression estimates. in order to simplify the discussion  here the effect of the pool u1 is not considered as in  blum and mitchell  1 . that is  the unlabeled examples are assumed as being picked from the unlabeled example set u directly.
　in each learning iteration of coreg  for each unlabeled example xu  its k-nearest neighboring labeled examples are put into the set  . as mentioned before  the newly labeled example should make the regressor become more consistent with the labeled data set. therefore  a criterion shown in eq. 1 can be used to evaluate the goodness of xu  where h is the original regressor while h1 is the one refined with  xu y u . if the value of  u is positive  then utilizing  xu y u  is beneficial.

 1 
　in the coreg algorithm  the unlabeled example which maximizes the value of  xu is picked to be labeled. therefore  the question is  whether the unlabeled example chosen according to the maximization of  xu will result in a positive  u value or not.
　first  assume that  xu y u  is among the k-nearest neighbors of some examples in    and is not among the k-nearest neighbors of any other examples in l. in this case  it is obvious that utilizing  xu y u  will only change the regression estimates on the examples in    therefore eq. 1 becomes eq. 1. comparing eqs. 1 with 1 it can be found that the maximization of  xu also results in the maximization of  u.
1 x
 u =		 yi   h xi  1   1 x  yi   h1 xi  1  1 
	k	k
	xi（ 	xi（ 
　second  assume that  xu y u  is not among the k-nearest neighbors of any example in  . in this case  the value of  xu is zero  therefore  xu y u  won't be chosen in coreg.
　third  assume that  xu y u  is among the k-nearest neighbors of some examples in   as well as some examples in l      and assume these examples in l     are
. then eq. 1 becomes eq. 1.
	 u = 1 x	1 xi  1 +
  yi   h xi      yi   h
k
xi（ 

　maximizing  xu will maximize the first sum term of eq. 1  but whether it can enable  u be positive should also refer the second sum term. unfortunately  the value of this sum term is difficult to be measured except that the neighboring relationships between all the labeled examples and  xu y u  are evaluated. therefore  there may exist cases where the unlabeled example chosen according to the maximization of  xu may decrease  u  which is the cost coreg takes for using  xu that can be more efficiently computed to approximate  u. nevertheless  experiments show that in most cases such an approximation is effective.
　it seems that using only one regressor to label the unlabeled examples for itself might be feasible  where the unlabeled examples can be chosen according to the maximization of  xu.
while considering that the labeled example set usually contains noise  the use of two regressors can be helpful to reduce overfitting.
　let Λ denote the subset of noisy examples in l. for the unlabeled instance xu  either of the regressors h1 and h1 will identify a set of k-nearest neighboring labeled examples for xu. assume these sets are  1 and  1  respectively. since h1 and h1 use different distance orders   1 and  1 are usually different  and therefore  1 ” Λ and  1 ” Λ are also usually different. suppose xu is labeled by h1 and then  xu h1 xu   is put into l1  where h1 xu  suffers from  1 ” Λ. for another unlabeled instance xv which is very close to xu  its k-nearest neighbors identified by h1 will be very similar to  1 except that  xu h1 xu   has replaced a previous neighbor. thus  h1 xv  will suffer from  1 ” Λ more seriously than h1 xu  does. while  if the instance xu is labeled by h1 and  xu h1 xu   is put into l1  then h1 xv  will suffer from  1 ” Λ only once  although xu is still very close to xv.
1	experiments
experiments are performed on ten data sets listed in table 1 where  # attribute  denotes the number of input attributes. these data sets have been used in  zhou et al.  1  where the detailed descriptions of the data sets can be found. note that the input attributes as well as the real-valued labels have been normalized to  1 1 .
table 1: experimental data sets
data set# attributesize1-d mexican hat1 1-d mexican hat1 1friedman #11friedman #11friedman #11gabor1 1multi1 1plane1 1polynomial1 1sinc1 1　for each data set  1% data are kept as the test set  while the remaining 1% data are partitioned into the labeled and unlabeled sets where 1%  of the 1%  data are used as labeled examples while the remaining 1%  of the 1%  data are used as unlabeled examples.
　in the experiments  the distance orders used by the two knn regressors in coreg are set to 1 and 1  respectively  the k value is set to 1  the maximum number of iterations t is set to 1  and the pool u1 contains 1 unlabeled examples randomly picked from the unlabeled set in each iteration.
　a self-training style algorithm is tested for comparison  which is denoted by self. this algorithm uses a knn regressor and in each iteration  it chooses the unlabeled example which maximizes the value of  xu in eq. 1 to label for itself. moreover  a co-training style algorithm  denoted by artre  is also tested. since the experimental data sets are with no sufficient and redundant views  here an artificial redundant view is developed through deriving new attributes from the original ones. for example  on 1-d mexican hat two new attributes  i.e. x1 and x1  are constructed from x1 + x1 and x1   x1  and then a knn regressor is built on x1 and x1 while the other is built on x1 and x1. in each iteration  each knn regressor chooses the unlabeled example which maximizes the value of  xu in eq. 1 to label for the other regressor. the final prediction is made by averaging the regression estimates of these two refined regressors. besides  a knn regressor using only the labeled data is tested as a baseline for the comparison  which is denoted by labeled.
all the knn regressors used in self  artre  and la-
beled employ 1nd-order minkowski distance  and the k value is set to 1. the same pool  u1  as that used by coreg is used in each iteration of self and artre  and the maximum number of iterations is also set to 1.
　one hundred runs of experiments are carried out on each data set. in each run  the performance of all the four algorithms  i.e. coreg  self  artre  and labeled  are evaluated on randomly partitioned labeled/unlabeled/test splits. the average mse at each iteration is recorded. note that the learning processes of the algorithms may stop before the maximum number of iterations is reached  and in that case  the final mse is used in computing the average mse of the following iterations.
　the improvement on average mse obtained by exploiting unlabeled examples is tabulated in table 1  which is computed by subtracting the final mse from the initial mse and then divided by the initial mse.
table 1: improvement on average mean squared error
data setselfartrecoreg1d mexican hat1%1%1%1d mexican hat1%1%1%friedman #1-1%-1%1%friedman #1-1%-1%1%friedman #1-1%-1%1%gabor1%1%1%multi-1%-1%1%plane-1%-1%-1%polynomial1%1%1%sinc1%1%1%　table 1 shows that self and artre improve the regression estimates on only five data sets  while coreg improves on eight data sets. moreover  table 1 discloses that the improvement of coreg is always bigger than that of self and artre. these observations tell that coreg can effectively exploit unlabeled examples to improve regression estimates.
　for further studying the compared algorithms  the average mse of different algorithms at different iterations are plotted in fig 1  where the average mse of the two knn regressors used in coreg are also depicted. note that in each subfigure  every curve contains 1 points corresponding to the 1 learning iterations in addition to the initial condition  where only 1 of them are explicitly depicted for better presentation.

figure 1: comparisons on average mean squared error of different algorithms at different iterations　fig. 1 shows that coreg can exploit unlabeled examples to improve the regression estimates on most data sets  except that on friedman #1 there is no improvement while on plane the performance is degenerated. while  self and artre degenerate the regression estimates on five data sets  i.e. friedman #1 to #1  multi  and plane. moreover  the average mse of the final prediction made by coreg is almost always the best except on friedman #1 where artre is slightly better and on plane where labeled is the best while all the semisupervised learning algorithms degenerate the performance. these observations disclose that coreg is apparently the best algorithm in the comparison.
　pairwise two-tailed t-tests with 1 significance level reveal that the final regression estimates of coreg are significantly better than its initial regression estimates on almost all the data sets except that on plane the latter is better while on friedman #1 there is no significant difference. moreover  the final regression estimates of coreg are significantly better than these of artre on almost all the data sets except on friedman #1 where the latter is better. furthermore  the final regression estimates of coreg are significantly better than these of self and labeled on almost all the data sets except on plane where labeled is better and on friedman #1 where there is no significant difference. these results of t-tests confirm that coreg is the strongest among the compared algorithms  which can effectively exploit unlabeled data to improve the regression estimates.
1	conclusion
this paper proposes a co-training style semi-supervised regression algorithm coreg. this algorithm employs two knearest neighbor regressors using different distance metrics. in each learning iteration  each regressor labels the unlabeled example which can be most confidently labeled for the other learner  where the labeling confidence is estimated through considering the consistency of the regressor with the labeled example set. the final prediction is made by averaging the predictions of both the refined knn regressors. experiments show that coreg can effectively exploit unlabeled data to improve the regression estimates.
　in contrast to standard co-training setting  coreg does not require sufficient and redundant views  which enables it have broad applicability. however  this forces coreg generate diverse initial regressors with specific mechanisms. in this paper the diversity is obtained by instantiating the minkowski distance with different distance orders. it is obvious that using completely different distance metrics may be more helpful. moreover  trying to obtain the diversity of the initial regressors from channels other than using different distance metrics is an issue to be investigated in future work. note that although this paper uses knn regressor as the base learner  an important idea of coreg  i.e. regarding the labeling of the unlabeled example which makes the regressor most consistent with the labeled example set as with the most confidence  can also be used with other base learners. therefore  designing semi-supervised regression algorithms with other base learners along the way of coreg is another interesting issue to be explored in the future. furthermore  designing semi-supervised regression algorithms outside the co-training framework is also well-worth studying.
acknowledgments
supported by nsfc  1   fanedd  1   and jiangsusf  bk1 .
