 
the long-term goal of our field is the creation and understanding of intelligence. productive research in ai  both practical and theoretical  benefits from a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results. this paper outlines a gradual evolution in our formal conception of intelligence that brings it closer to our informal conception and simultaneously reduces the gap between theory and practice. 
1 	artificial intelligence 
ai is a field in which the ultimate goal has often been somewhat ill-defined and subject to dispute. some researchers aim to emulate human cognition  others aim at the creation of intelligence without concern for human characteristics  and still others aim to create useful artifacts without concern for abstract notions of intelligence. 
　this variety is not necessarily a bad thing  since each approach uncovers new ideas and provides fertilization to the others. but one can argue that  since philosophers abhor a definitional vacuum  many of the damaging and ill-informed debates about the feasibility of ai have been about definitions of ai to which we as ai researchers do not subscribe. 
　my own motivation for studying ai is to create and understand intelligence as a general property of systems  rather than as a specific attribute of humans. i believe this to be an appropriate goal for the field as a whole  and it certainly includes the creation of useful artifacts-both as a spin-off and as a focus and driving force for technological development. the difficulty with this  creation of intelligence  view  however  is that it presupposes that we have some productive notion of what intelligence is. cognitive scientists can say  look  my model correctly predicted this experimental observation of human cognition   and artifact developers can say  look  my system is saving lives/megabucks   but few of us are happy with papers saying  look  my system is intelligent.  this difficulty is compounded further by the need for theoretical scaffolding to allow us to design complex systems with confidence and to build on the results of others.  intelligent  must be given a definition that can be related directly to the system's input  structure  and output.1 
'such a definition must also be general. otherwise  ai sub-
1 
　in this paper  i shall outline the development of such definitions over the history of ai and related disciplines.1 i shall examine each definition as a predicate p that can be applied  supposedly  to characterize systems that are intelligent  for each p  i shall discuss whether the statement  look  my system is p  is interesting and at least sometimes true  and the sort of research and technological development to which the study of p-systems leads. 
　i shall begin with the idea that intelligence is strongly related to the capacity for successful behaviour-the so-called  agent-based  view of ai. the candidates for formal definitions of intelligence are as follows: 
  p1: perfect rationality  or the capacity to generate maximally successful behaviour given the available information. 
  p1 calculative rationality  or the in-principle capacity to compute the perfectly rational decision given the initially available information. 
  p1: metalevel rationality  or the capacity to select the optimal combination of computation-sequence-plus-action  under the constraint that the action must be selected by the computation. 
  p1: bounded optimality  or the capacity to generate maximally successful behaviour given the available information and computational resources. 
all four definitions will be fleshed out in detail  and i will describe some results that have been obtained so far along these lines. then i will describe ongoing and future work under the headings of calculative rationality and bounded optimality. 
　i shall be arguing that  of these candidates  bounded optimality comes closest to meeting the needs of ai research. there is always a danger  in this sort of claim  that its acceptance can lead to  premature mathematization   a condition characterized by increasingly technical results that have increasingly little to do with the original problem-in the case of ai  the problem of creating intelligence. is research on bounded optimality a suitable stand-in for research on intelligence  i hope to show that p1  bounded optimality  is closer than p  through p1 because it is a real problem with real and desirable solutions  and also because it satisfies some 
sides into a smorgasbord of fields-intelligence as chess playing  intelligence as vehicle control  intelligence as medical diagnosis. 
　　1 in doing so i shall draw heavily on previous work with eric wefald  russell and wefald  1 a  and devika subramanian  russell and subramanian  1 . the latter paper contains a much more rigorous analysis of the concepts presented here. 
essential intuitions about the nature of intelligence. some important questions about intelligence can only be formulated and answered within the framework of bounded optimality or some relative thereof. only time will tell  however  whether bounded optimality research  perhaps with additional refinements  can generate enough theoretical scaffolding to support significant practical progress in ai. 
1 	agents 
until fairly recently  it was common to define ai as the computational study of  mental faculties  or  intelligent systems   catalogue various kinds  and leave it at that. this doesn't provide much guidance. instead  one can define ai as the problem of designing systems that do the right thing. now we just need a definition for  right.  
   this approach involves considering the intelligent entity as an agent  that is to say a system that senses its environment and acts upon it. formally speaking  an agent is defined by the mapping from percept sequences to actions that the agent instantiates. let o be the set of percepts that the agent can observe at any instant  and a be the set of possible actions the agent can carry out in the external world. thus the agent function f : o* -* a defines how an agent behaves under all circumstances  including those where it does nothing . what counts in the first instance is what the agent does  not necessarily what it thinks  or even whether it thinks at all. this initial refusal to consider further constraints on the internal workings of the agent  such as that it should reason logically  for example  helps in three ways: first  it allows us to view such  cognitive faculties  as planning and reasoning as occurring in the service of finding the right thing to do; second  it encompasses rather than excludes the position that systems can do the right thing without such cognitive faculties  agre and chapman  1; brooks  1 ; third  it allows more freedom to consider various specifications  boundaries  and interconnections of subsystems. 
   the agent-based view of ai has moved quickly from workshops on  situatedness  and  embeddedness  to mainstream textbooks  russell and norvig  1; dean et al  1  and buzzwords in newsweek. rational agents  loosely speaking  are agents whose actions make sense from the point of view of the information possessed by the agent and its goals  or  the task for which it was designed . rationality is a property of actions and does not specify-although it does constrain- the process by which the actions are selected. this was a point emphasized by simon 1   who coined the terms substantive rationality and procedural rationality to describe the difference between the question of what decision to make and the question of how to make it. that rod brooks's 1 computers and thought lecture was titled  intelligence without reason  emphasizes the fact that reasoning is  perhaps  a derived property of agents that might  or might not  be a good implementation scheme to achieve rational behaviour. the justification of cognitive structures that many ai researchers take for granted is not an easy problem. 
　one other consequence of the agent-based view of intelligence is that it opens ai up to competition from other fields that have traditionally looked on the embedded agent as a natural topic of study. control theory is foremost among these  but evolutionary programming and indeed evolutionary biology itself also have ideas to contribute.1 the prevalence of 
1 i view this as a very positive development. ai is a field defined the agent view has also helped the field move towards solving real problems  avoiding what brooks calls the  hallucination  problem that arises when the fragility of a subsystem is masked by having an intelligent human providing input to it and interpreting its outputs. 
1 	perfect rationality 
perfect rationality constrains an agent's actions to provide the maximum expectation of success given the information available. we can expand this notion as follows  see figure 1 . the fundamental inputs to the definition are the environment class e in which the agent is to operate and the performance measure u which evaluates the sequence of states through which the agent drives the actual environment. let v f  e  u  denote the expected value according to u obtained by an agent function/ in environment e. then a perfectly rational agent is defined by an agent function/   such that 
fopt = argmaxfv f e u  
this is just a fancy way of saying that the best agent does the best it can. the point is that perfectly rational behaviour is a well-defined function of e and u  which i will call the task environment. the problem of computing this function is addressed below. 

figure 1: the agent receives percepts from the environment and generates a behaviour which in turn causes the environment to generate a state history. the performance measure evaluates the state history to arrive at the value of the agent. 
　the theoretical role of perfect rationality within ai is welldescribed by newell's paper on the knowledge level  newell  1 . knowledge-level analysis of ai systems relies on an assumption of perfect rationality. it can be used to establish an upper bound on the performance of any possible system  by establishing what a perfectly rational agent would do given the same knowledge. 
　the question of learning in perfectly rational agents is much less well-understood than the question of action selection  yet it is equally essential in the specification of perfectly rational behaviour since it determines the agent's expectations. in the logical view of rationality  learning has received almost no attention-indeed  newell's analysis precludes learning at the 
by its problems  not its methods. its principal insights-among them the learning  use  and compilation of explicit knowledge in the service of decision making-can certainly withstand the influx of new methods from other fields. this is especially true when other fields are simultaneously embracing the insights derived within ai. 
1 
knowledge level. in the decision-theoretic view  bayesian updating provides a model for rational learning  but this pushes the question back to the prior  carnap  1 . the question of rational priors remains unsettled. 
　another aspect of perfect rationality that is lacking is the development of a suitable body of techniques for the specification of utility functions. in economics  many results have been derived on the decomposition of overall utility into attributes that can be combined in various ways  keeney and raiffa  1   yet such methods have made few inroads into ai  but see  wellman  1  . we also have little idea how to specify utility over time  and although the question has been raised often  we do not have a satisfactory understanding of the relationship between goals and utility. 
　the good thing about perfectly rational agents is that if you have one handy  you prefer it to any other agent. furthermore  if you are an economist you can prove nice results about economies populated by them. the bad thing is that the theory of perfect rationality does not provide for the analysis of the internal design of the agent: one perfectly rational agent is as good as another. the really bad thing  as pointed out by simon  is that perfectly rational agents do not exist. physical mechanisms take time to process information and select actions  hence the behaviour of real agents includes long sequences of inaction. unless the environment is static  see below   inaction is suboptimal. 
1 calculative rationality 
before discussing calculative rationality  it is necessary to introduce a distinction between the agent function and the agent program. in ai  an agent is implemented as a program  which i shall call /  running on a machine  which i shall call m. an agent program receives as input the current percept  but also has internal state that reflects  in some form  the previous percepts. it outputs actions when they have been selected. from the outside  the behaviour of the agent consists of the selected actions interspersed with inaction  or whatever default actions the machine generates . 
　calculative rationality is displayed by programs that  if executed infinitely fast  would result in perfectly rational behaviour. unlike perfect rationality  calculative rationality is a requirement that can be fulfilled by many real programs. also unlike perfect rationality  calculative rationality is not necessarily a desirable property. for example  a calculatively rational chess program will choose the  right  move  but may take 1 times too long to do so. 
　the pursuit of calculative rationality has nonetheless been the main activity of theoretically well-founded research in ai. in the early stages of the field  it was important to concentrate on  epistemological adequacy  before  heuristic adequacy  - that is  capability in principle rather than in practice. the methodology that has resulted involves designing programs that exhibit calculative rationality  and then using various speedup techniques and approximations in the hope of getting as close as possible to perfect rationality. another common aspect of the methodology is the imposition of restrictions on the task environment to render decision problems tractable. 
　this methodology has been pursued in both the logical and the decision-theoretic traditions. in the logical tradition  the performance measure accepts behaviours that achieve the specified goal in all cases and rejects any others. thus newell  defines rational actions as those that are guaranteed to achieve one of the agent's goals. logical plan-
1 
ning systems  such as theorem-provers using situation calculus  satisfy the conditions of calculative rationality under this definition. in the decision-theoretic tradition  the design of calculatively rational agents has largely gone on outside ai-for example  in stochastic optimal control theory. representations have usually been very impoverished  state-based rather than sentential  and solvable problems have been either very small or very specialized. within ai  the development of probabilistic networks or belief networks has opened up many new possibilities for agent design. systems based on influence diagrams  probabilistic networks with action and value nodes added  satisfy the decision-theoretic version of calculative rationality. 
　ai has also developed a very powerful armoury of methods for reducing complexity  including the decomposition of state representations into sentential form; sparse representations of environment models  as in strips operators ; solution decomposition methods such as partial-order planning and abstraction; approximate  parameterized representations of value functions for reinforcement learning; compilation  chunking  macro-operators  ebl etc. ; and the application of metalevel control. although some of these methods can retain guarantees of optimality and are effective for moderately large problems that are well structured  it is inevitable 
that intelligent agents will be unable to act rationally in all circumstances. this observation has been a commonplace since the very beginning of ai. there are two common responses: one can rule out sources of exponential complexity in the representations and reasoning tasks addressed  as described in two fascinating computers and thought lectures  by hector levesque in 1 and henry kautz in 1 ; or one can design systems that select suboptimal actions. suboptimal methods fall outside calculative rationality per se  however  and we need a better theory to understand them. 
1 metalevel rationality 
metalevel rationality  also called type ii rationality by i. j. good  is based on the idea of finding an optimal tradeoff between computational costs and decision quality. although good never made his concept of type ii rationality precise  it is clear that the aim was to take advantage of some sort of metalevel architecture to implement this tradeoff. metalevel architecture is a design philosophy for intelligent agents that divides the agent into two  or more  notional parts. the object level carries out computations concerned with the application domain-for example  projecting the results of physical actions  computing the utility of certain states  and so on. the metalevel is a second decision-making process whose application domain consists of the object-level computations themselves and the computational objects and states that they affect. metareasoning has a long history in ai  going back at least to the early 1s. teiresias  davis  1  established the idea that explicit  domain-specific metaknowledge was an important aspect of expert system creation. 
　the theory of rational metareasoning provides an alternative to the view that metaknowledge is a sort of  extra  domain knowledge  over and above the object-level domain knowledge  that one has to add to an ai system to get it to work well. the basic idea is that object-level computations are actions with costs  the passage of time  and benefits  improvements in decision quality . a rational metalevel selects computations according to their expected utility. the important thing is that the metatheory describing the effects of computations is domain-independent. in principle  no additional domain knowledge is needed to assess the benefits of a computation  although in practice the results of metalevel analysis for particular domains can be compiled into domainspecific metaknowledge. thus  there is an interesting sense in which algorithms are not a necessary part of ai systems. instead  one can imagine a general process of rationally guided computation interacting with properties of the environment to produce more and more efficient decision making. to my mind  this way of thinking finesses one major puzzle of ai: if what is required for ai is incredibly devious and superbly efficient algorithms far surpassing the best efforts of computer scientists  how did evolution  and how will machine learning  ever get there  
　rational metareasoning has as a precursor the theory of information value  howard  1 -the notion that one can calculate the decision-theoretic value of acquiring an additional piece of information by simulating the decision process that would be followed given each possible outcome of the information request  thereby estimating the expected improvement in decision quality averaged over those outcomes. the application to computational processes  by analogy to information-gathering  seems to have originated with matheson . in ai  horvitz   breese and fehling   and russell and wefald  1; 1a; 1b  all showed how the idea of value of computation could solve the basic problems of real-time decision making. 
　the work done with eric wefald was aimed especially at revising the traditional notion of algorithms. we looked in particular at search algorithms  in which the object-level computations extend projections of the results of various courses of actions further into the future. for example  in chess programs  each object-level computation expands a leaf node of the game tree. the metalevel problem is then to select nodes for expansion and to terminate search at the appropriate point. the principal problem with metareasoning in such systems is that the local effects of the computations do not directly translate into improved decisions  because there is also a complex process of propagating the local effects at the leaf back to the root and the move choice. it turns out that a general formula for the value of computation can be found in terms of the  local effects  and the  propagation function   such that the formula can be instantiated for any particular object-level system  such as minimax propagation   compiled  and executed efficiently at runtime. this method was implemented for two-player games  two-player games with chance nodes  and single-agent search. in each case  the same general metareasoning scheme resulted in efficiency improvements of roughly an order of magnitude over traditional  highly-engineered algorithms. 
　another general class of metareasoning problems arises with anytime  dean and boddy  1  or flexible  horvitz  1  algorithms  which are algorithms designed to return results whose quality varies with the amount of time allocated to computation. the simplest type of metareasoning trades off the expected increase in decision quality for a single algorithm  as measured by a performance profile  against the cost of time  simon  1 . a greedy termination condition is optimal if the second derivative of the performance profile is negative. more complex problems arise if one wishes to build complex real-time systems from anytime components. first  one has to ensure the interruptibility of the composed system-that is  to ensure that the system as a whole can respond robustly to immediate demands for output. the solution is to interleave the execution of all the components  allocating time to each component so that the total time for each complete iterative improvement cycle of the system doubles at each iteration. in this way  we can construct a complex system that can handle arbitrary and unexpected real-time demands exactly as if it knew the exact time available in advance  with 
just a small    1  constant factor penalty in speed  russell and zilberstein  1 . second  one has to allocate the available computation optimally among the components to maximize the total output quality. although this is np-hard for the general case  it can be solved in time linear in program size when the call graph of the components is tree-structured  zilberstein and russell  1 . thus  although these results are derived in the relatively clean context of anytime algorithms with welldefined performance profiles  there is reason to expect that the general problem of robust real-time decision-making in complex systems can be handled in practice. 
　significant open problems remain in the area of rational metareasoning. one obvious difficulty is that almost all systems to date have adopted a myopic strategy-a greedy  depth-one search at the metalevel. obviously  the problem of optimal selection of computation sequences is at least as intractable as the underlying object-level problem. nonetheless  sequences must be considered because in some cases the value of a computation may not be apparent as an improvement in decision quality until further computations have been done. this suggests that techniques from reinforcement learning could be effective  especially as the  reward function  for computation-that is  the improvement in decision quality-is easily available to the metalevel post hoc. other possible areas for research include the creation of effective metalevel controllers for more complex systems such as abstraction hierarchy planners  hybrid architectures  and so on. 
　although rational metareasoning seems to be a useful tool in coping with complexity  the concept of metalevel rationality as a formal framework for resource-bounded agents does not seem to hold water. the reason is that  since metareasoning is expensive  it cannot be carried out optimally. within the framework of metalevel rationality  there is no way to understand the appropriate tradeoff of time for metalevel decision quality. any attempt to do so via a metametalevel simply results in a conceptual regress. furthermore  it is entirely possible that in some environments  the most effective agent design will do no metareasoning at all  but simply to respond to circumstances. these considerations suggest that the right approach is to step outside the agent  as it were; to refrain from micromanaging the individual decisions made by the agent. this is the approach taken in bounded optimality. 
1 	bounded optimality 
the difficulties with perfect rationality and metalevel rationality arise from the imposition of constraints on things  actions  computations  that the agent designer does not directly control. specifying that actions or computations be rational is of no use if no real agents can fulfill the specification. the designer controls the prog ram. in  russell and subramanian  1   the notion of feasibility for a given machine is introduced to describe the set of all agent functions that can be implemented by some agent program running on that machine. this is somewhat analogous to the idea of computability  but is much stricter because it relates the operation of a program on a formal machine model with finite speed to the actual temporal behaviour generated by the agent. 
　given this view  one is led immediately to the idea that optimal feasible behaviour is an interesting notion  and to the 
1 
　

1 
　
possible to define some basic properties of task environments that  together with the complexity of the problem  lead to identifiable requirements on the corresponding rational agent designs  russell and norvig  1  ch. 1   the principal properties are whether the environment is fully observable or partially observable  whether it is deterministic or stochastic  whether it is static  i.e.  does not change except when the agent acts  or dynamic  and whether it is discrete or continuous. while crude  these distinctions serve to lay out an agenda for basic research in ai. by analysing and solving each subcase and producing calculatively rational mechanisms with the required properties  theoreticians can produce the ai equivalent of bricks  beams  and mortar with which ai architects can build the equivalent of cathedrals. unfortunately  many of the basic components are currently missing. others are so fragile and non-scalable as to be barely able to support their own weight. this presents many opportunities for research of far-reaching impact. 
　the logicist tradition of goal-based agent design  based on the creation and execution of guaranteed plans  is firmly anchored in fully observable  deterministic  static  and discrete task environments.  furthermore  tasks are usually specified as logically defined goals rather than general utility functions.  this means that agents need keep no internal state and can even execute plans without the use of perception. 
　the theory of optimal action in stochastic  partially observable environments goes under the heading of pomdps  partially observable markov decision problems   a class of problems first addressed in the work of sondik  1   but almost completely unknown in ai until recently. similarly  very little work of a fundamental nature has been done in ai on dynamic environments  which require real-time decision making  or on continuous environments  which have been largely the province of geometry-based robotics. since most realworld applications are partially observable  nondeterministic  dynamic  and continuous  the lack of emphasis is somewhat surprising. 
　there are  however  several new bricks under construction. for example  dynamic probabilistic networks  dean and kanazawa  1  provide a mechanism to maintain beliefs about the current state of a dynamic  partially observable  nondeterministic environment  and to project forward the effects of actions. also  the rapid improvement in the speed and accuracy of computer vision systems has made interfacing with continuous physical environments more practical. in particular  the application of kalman filtering  kalman  1   a widely used technique in control theory  allows robust and efficient tracking of moving objects. reinforcement learning  together with inductive learning methods for continuous function representations such as neural networks  allow learning from delayed rewards in continuous  nondeterministic environments. recently  parr and russell   among others  have had some success in adapting reinforcement learning to partially observable environments. finally  learning methods for static and dynamic probabilistic networks with hidden variables  i.e.  for partially observable environments  may make it possible to acquire the necessary environment models  lauritzen  1; russell etal  1 . 
　the bayesian automated taxi  a.k.a. batmobile  project  forbes et al.t 1  is an attempt to combine all these new bricks to solve an interesting application problem  namely driving a car on a freeway. technically  this can be viewed as a pomdp because the environment contains relevant variables  such as whether or not the volvo beside you is intending to change lanes to the left or right  that are not observable  and because the behaviour of other vehicles and the effects of ones own actions are not exactly predictable. in 
a pomdp  the optimal decision depends on the joint probability distribution over the entire set of state variables. it turns out that a combination of real-time vision algorithms  kalman filtering  and dynamic probabilistic networks can maintain the required distribution when observing a stream of traffic on a freeway. the batmobile currently uses a hand-coded decision tree to make decisions on this basis  and is a fairly safe driver  although probably far from optimal  on our simulator. we are currently experimenting with lookahead methods to make approximately rational decisions  as well as supervised learning and reinforcement learning methods. 
　as well as extending the scope of ai applications  new bricks for planning under uncertainty significantly increase the opportunity for metareasoning to make a difference. with logical planners  a plan either does or does not work; it has proved very difficult to find heuristics to measure the  goodness  of a logical plan that does not guarantee success  or to estimate the likelihood that an abstract logical plan will have a successful concrete instance. this means that it is very hard to identify plan elaboration steps that are likely to have high value. in contrast  planners designed to handle uncertainty and utility have built-in information about the likelihood of success and there is a continuum from hopeless to perfect plans. getting metareasoning to work for such systems is a high priority. it is also important to apply those methods such as partial-order planning and abstraction that have been so effective in extending the reach of classical planners. 
1 directions for bounded optimality 
ongoing research on bounded optimality aims to extend the initial results of  russell and subramanian  1  to more interesting agent designs. the general idea is that the space of agent designs can be divided up into  architectural classes  such that in each class the structural variation is sufficiently limited. then abo results can be obtained either by analytical optimization within the class or by showing that an empirical adaptation process results in an approximately abo design. once this is done  it should be possible to compare architecture classes directly  perhaps to establish asymptotic dominance of one class over another. for example  it might be the case that the inclusion of an appropriate  macro-operator formation  or  greedy metareasoning  capability in a given architecture will result in an improvement in behaviour in the limit of very complex environments-that is  one cannot compensate for the exclusion of the capability by increasing the machine speed by a constant factor. 
　work by tash and russell  can be seen as a step in this direction  although the abo results have not yet been established. the basic architecture investigated is a decisiontheoretic planner based on applying policy iteration within a limited  envelope  of states around the current state. the agent can either extend the envelope and recompute the locally optimal policy or act based on the current policy. when an approximately rational metareasoning component was added  the agent was able to do a much better job of selecting states to add to the envelope. it also exhibited some basic behaviours appropriate to a real-time environment: reducing the amount of deliberation in response to an increase in time pressure or a decrease in predictability. addition of a simple metalevel reinforcement learning mechanism  see above  led to a significant improvement in performance. when a  reflec-
1 
　
tive  capability was added that took into consideration the amount of computation already expended in ascertaining the desirability of a given state  the agent exhibited beaten paths behaviour-that is  it often preferred to follow paths within the environment with which it was familiar even if this meant taking a long detour around unfamiliar territory. 
　showing that these agent designs will converge to abo configurations within each class involves showing that the adaptation mechanism is in approximate equilibrium if and only if the agent is in an abo configuration. in this sense  the notion of bounded optimality helps to distinguish correct from incorrect adaptation mechanisms. one can imagine that such mechanisms could become quite complex  especially when they include inductive learning methods for improving the agent's knowledge of the environment as well as reinforcement learning methods for improving the utility function at the object level and metalevel. it is to be expected that the topic of agnostic learning  kearns et a/.  1   which analyses the convergence of inductive learning algorithms working in arbitrary environments within a fixed hypothesis language  will be an important adjunct to the theory of bounded optimal agent design. 
　besides inductive and reinforcement learning  probably the most important mechanisms for adaptation are the compilation of the results of decision-making into more efficiently executable forms and the formation of new abstractions within abstraction-based planners. getting all these architectural devices to work together smoothly is an important unsolved problem in ai and must be addressed before we can make progress on understanding bounded optimality within these more complex architectural classes. extending these devices to the decision-theoretic context is also a vital task. 
　it has been noted that this gradual accumulation of performance-enhancing and scope-enhancing devices such as abstraction  partial ordering  first-order expressiveness  and so on would lead to the emergence of the lap  or long acronym problem-the spectre of systems with names such as foplbmldthtnipemucpopmea  interpretation left to the reader . this is an inevitable result of one of the intuitions behind bounded optimality  namely that complex system designs are needed to overcome computational complexity. as mentioned above  the complexity of the design is needed to ensure that high-value computations are available to the agent whenever possible. if the notion of  architectural device  can be made sufficiently concrete  then ai may eventually develop a grammar for agent designs  describing the devices and their interrelations. as the grammar develops  so should the accompanying abo dominance results. 
　the above discussion of adaptation in abo agents makes the simplifying assumption that the adaptation process itself is not subject to the requirement of asymptotic bounded optimality-the results that would be obtained are  eventually converges to abo  results. when the architectural class within which optimization takes place includes the learning mechanism  some very interesting questions arise. for example  one can imagine that the appropriate initial design for an agent will depend on the relationship between the degree of variability to be expected in the environment and the size of the agent's memory. it is possible that the best strategy is for the agent to retain very little in the way of declarative knowledge  but to continually compile its experience into reactive policies that are expected to be appropriate only in the medium term. as the environment changes  the agent might effectively rewrite its entire internal state to fit the new world order  retaining only the basic structure needed to repeat the process in 
1 	awards 
the future. with devika subramanian  i am planning to investigate the possible paths followed by such an agent viewed as a dynamical system with internal state in the form of various amounts of compiled and uncompiled knowledge and internal processes of inductive learning and compilation. 
　my hope is that with these kinds of investigations  it will eventually be possible to develop the conceptual and mathematical tools to answer some basic questions about intelligence. for example  why do complex intelligent systems  appear to  have declarative knowledge structures over which they reason explicitly  this has been a fundamental assumption that distinguishes ai from other disciplines for agent design  yet the answer is still unknown. indeed  rod brooks  hubert dreyfus  and others flatly deny the assumption. what is clear is that it will need something like a theory of bounded optimal agent design to answer this question. 
　most of the agent design features that i have discussed here  including the use of declarative knowledge  have been conceived within the standard methodology of  first build calculative rationality and then speed it up.  yet one can legitimately doubt that this methodology will enable the ai community to discover all the design features needed for general intelligence. the reason is that no conceivable computer will ever be remotely close to approximating perfect rationality for even moderately complex environments. it may well be the case that agents based on improvements to calculatively rational designs are not even close to achieving the level of performance that is potentially achievable given the underlying computational resources. for this reason  i believe it is imperative not to dismiss ideas for agent designs that do not seem at first glance to fit into the  classical  calculatively rational framework. instead  one must attempt to understand the potential of the bounded optimal configurations within the corresponding architectural class  and to see if one can design the appropriate adaptation mechanisms that might help in realizing these configurations. 
1 summary 
i have outlined some directions for formally grounded ai research based on bounded optimality as the desired property of ai systems. i have suggested that such an approach should allow synergy between theoretical and practical ai research of a kind not afforded by other formal frameworks. in the same vein  i believe it is a satisfactory formal counterpart of the informal goal of creating intelligence. in particular  it is entirely consistent with our intuitions about the need for complex structure in real intelligent agents  the importance of the resource limitations faced by relatively tiny minds in large worlds  and the operation of evolution as a design optimization process. one can also argue that bounded optimality research is likely to satisfy better the needs of those who wish to emulate human intelligence  because it takes into account the limitations on computational resources that are presumably responsible for most of the deviation from perfect rationality exhibited by humans. 
　bounded optimality and its asymptotic cousin are  of course  nothing but formally defined properties that one may want systems to satisfy. it is too early to tell whether abo will do the same kind of work for ai that o   complexity has done for theoretical computer science. creativity in design is still the prerogative of ai researchers  but it may be possible to systematize the design process somewhat and to automate the process of adapting a system to its computational resources and the demands of the environment. the concept of bounded 
　
optimality provides a way to make sure the adaptation process is  correct.  
　as mentioned in the previous section  there is still plenty of work to do in the area of making more general and more robust  bricks  from which to construct ai systems for more realistic environments  and such work will provide added scope for the achievement of bounded optimality. in a sense  under this conception ai research is the same now as it always should have been. 
references 
 agre and chapman  1  philip e. agre and david chapman. pengi: an implementation of a theory of activity. in proceedings of the tenth international joint conference on artificial intelligence  ijcai-1   pages 1  milan  italy  august 1. morgan kaufmann. 
 breese and fehling  1  j. s. breese and m. r. fehling. control of problem-solving: principles and architecture. in r. d. shachter.t. levitt  l. kanal  and j. lemmer  editors  uncertainty in artificial intelligence 1. elsevier/north-holland  amsterdam  london  new york  1. 
 brooks  1  r. a. brooks. engineering approach to building complete  intelligent beings. proceedings of the spie-the international society for optical engineering  1-1. 
 carnap  1  rudolf carnap. logical foundations of probability. university of chicago press  chicago  illinois  1. 
fcherniak  1  c. cherniak. minimal rationality. mit press  cambridge  massachusetts  1. 
 davis  1  randall davis. meta-rules: reasoning about control. artificial intelligence  1 : 1  december 1. 
 dean and boddy  1  thomas dean and mark boddy. an analysis of time-dependent planning. in proceedings of the seventh national conference on artificial intelligence  aaai-1   pages 1  st. paul  minnesota  1 august 1. morgan kaufmann. 
 dean and kanazawa  1 thomas dean and keiji kanazawa. a model for reasoning about persistence and causation. computational intelligence  1 : 1 1. 
 dean etal  1  thomas l. dean  john aloimonos  and james f. allen. 	artificial intelligence: 	theory and practice. 	bcnjamin/cummings  redwood city  california  1. 
 dennett  1 daniel c. dennett. the moral first aid manual. tanner lectures on human values  university of michigan  1. 
 forbes etal.  1  jeff forbes  tim huang  keiji kanazawa  and stuart russell. the batmobile: towards a bayesian automated taxi. submitted to ijcai-1  1. 
 harman  1  gilbert h. harman. change in view: principles of reasoning. mit press  cambridge  massachusetts  1. 
 horvitz  1  e. j. horvitz. problem-solving design: reasoning about computational value  trade-offs  and resources. in proceedings of the second annual nasa research forum  pages 1  moffett field  california  1. nasa ames research center. 
 howard  1  ronald a. howard. information value theory. ieee transactions on systems science and cybernetics  ssc-1-1  1. 
 kalman  1  r. e. kalman. a new approach to linear filtering and prediction problems. journal of basic engineering  pages 1  march 1. 
 kearns et al.  1  m. kearns  r. schapire   and l. sellie. toward efficient agnostic learning. in proceedings of the fifth annual acm workshop on computational learning theory  colt-1   pittsburgh  pennsylvania  july 1. acm press. 
 keeney and raiffa  1  ralph l. keeney and howard raiffa. decisions with multiple objectives: p