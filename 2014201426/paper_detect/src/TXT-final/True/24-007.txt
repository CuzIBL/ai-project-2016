 
in this paper we explore the use of an adaptive search technique  genetic algorithms  to construct a system gabel which continually learns and refines concept classification rules from its interaction with the environment. the performance of the system is measured on a set of concept learning problems and compared with the performance of two existing systems: id1r and c1. preliminary results support that  despite minimal system bias  gabil is an effective concept learner and is quite competitive with id1r and c1 as the target concept increases in complexity. 
1 introduction 
an important requirement for both natural and artificial organisms is the ability to acquire concept classification rules from interactions with their environment. in this paper we explore the use of an adaptive search technique  namely genetic algorithms  gas   as the central mechanism for building a system which continually learns and refines concept classification rules from its interaction with the environment. we show how concept learning tasks can be represented and solved by gas  and we provide empirical results which illustrate the performance of gas relative to more traditional methods. finally  we discuss the advantages and disadvantages of this approach and describe future research activities. 
1 concept learning problems 
supervised concept learning involves inducing concept descriptions for the concepts to be learned from a set of positive and negative examples of the target concepts. examples are represented as points in an n-dimensional feature space which is defined a priori and for which all the legal values of the features are known. concepts are therefore represented as subsets of points in the given ndimensional space. 
　　　a concept learning program is presented with both a description of the feature space and a set of correctly 
william m. spears 
naval research laboratory washington  d.c 1 usa spears aic.nrl.navy.mil 
classified examples of the concepts  and is expected to generate a reasonably accurate description of the  unknown  concepts. since concepts can be arbitrarily complex subsets of a feature space  an important issue is the choice of the concept description language. the language must have sufficient expressive power to describe large subsets succinctly and yet be able to capture irregularities. the two language forms generally used are decision trees  quinlan  1  and rules  michalski  1 . 
　　　another important issue arises from the situation that there is a large  possibly infinite  set of concept descriptions which are consistent with any particular finite set of examples. this is generally resolved by introducing either explicitly or implicitly a bias  preference  for certain kinds of descriptions  e.g.  shorter or less complex descriptions may be preferred . 
　　　finally  there is the difficult issue of evaluating and comparing the performance of concept learning algorithms. the most widely used approach is a batch mode in which the set of examples is divided into a training set and a test set the concept learner is required to produce a concept description from the training examples. the validity of the description produced is then measured by the percentage of correct classifications made by the system on the second  test  set of examples during which no further learning takes place. 
　　　the alternative evaluation approach is an incremental mode in which the concept learner is required to produce a concept description from the examples seen so far and to use that description to classify the next incoming example. in this mode learning never stops  and evaluation is in terms of learning curves which measure the predictive performance of the concept learner over time. 
　　　this incremental and continuous model of concept learning matches more closely the kind of concept learning that an organism performs as it explores a complex and changing world. consequently  we use predictive learning curves as our evaluation methodology. 
	de jong and spears 	1 

1 genetic algorithms and concept learning 
in order to apply gas to a particular problem  we need to select an internal representation of the space to be searched and define an external evaluation function which assigns utility to candidate solutions. both components are critical to the successful application of the gas to the problem of interest.l 
1 representing the search space 
　　　the traditional internal representation used by gas involves using fixed-length  generally binary  strings to represent points in the space to be searched. however  such representations do not appear well-suited for representing the space of concept descriptions which are generally symbolic in nature  which have both syntactic and semantic constraints  and which can be of widely varying length and complexity. 
　　　there are two general approaches one might take to resolve this issue. the first involves changing the fundamental ga operators  crossover and mutation  to work effectively with complex non-string objects  rendell  1 . this must be done carefully in order to preserve the properties which make the gas effective adaptive search procedures  see  dejong  1  for a more detailed discussion . alternatively  one can attempt to construct a string representation which minimizes any changes to the gas. 
　　　we are interested in pursuing both approaches. our ideas on the first approach will be discussed briefly at the end of the paper. in the following sections we will describe our results using the second approach in which we try to apply classical gas with minimal changes. 
1 defining fixed-length classifier rules 
　　　our approach to choosing a representation which results in minimal changes to the standard ga operators involves carefully selecting the concept description language. a natural way to express complex concepts is as a disjunctive set of  possibly overlapping  
classification rules. the left-hand side of each rule  disjunct  consists of a conjunction of one or more tests involving feature values. the right-hand side of a rule indicates the concept  classification  to be assigned to the examples which match its left-hand side. collectively  a set of such rules can be thought of as representing the  unknown  concepts if the rules correctly classify the elements of the feature space. 
　　　if we allow arbitrarily complex terms in the conjunctive left-hand side of such rules  we will have a very powerful description language which will be difficult to represent as strings. however  by restricting the complexity of the elements of the conjunctions  we are able to use 
           1 excellent introductions to gas can be found in  holland  1  and  goldberg  1 . 
1 	learning and knowledge acquisition 
a string representation and standard gas  with the only negative side effect that more rules may be required to express the concept. this is achieved by restricting each element of a conjunction to be a test of the form: 
return true if the value of feature i of the example is in the given value set; return false otherwise. 
for example  a rule might take the following symbolic form:  if  f1 = large  and  f1 = tall or thin  then it's a widget . since the left-hand sides are conjunctive forms with internal disjunction  there is no loss of generality by requiring that there be at most one test for each feature  on the left hand side of a rule . 
　　　with these restrictions we can now construct a fixed-length internal representation for classifier rules. each fixed-length rule will have n feature tests  one for each feature. each feature test will be represented by a fixed-length binary string  the length of which will depend of the type of feature  nominal  ordered  etc. . for simplicity  the examples used in this paper will involve features with nominal values. in this case we use k bits for the k values of a nominal feature. so  for example  if the legal values for fl are the days of the week  then the pattern 1 would represent the test for fl being a weekday. 
　　　as an example  the left-hand side of a rule for a 1 feature problem would be represented internally as: 
	fl 	f1 	f1 	f1 	f1 
	1 	1 	1 	1 
notice that a feature test involving all vs matches any value of a feature and is equivalent to  dropping  that conjunctive term  i.e.  the feature is irrelevant . so  in the above example only the values of f l   f1  and f1 are relevant for completeness  we allow patterns of all 1's which match nothing. this means that any rule containing such a pattern will not match  cover  any points in the feature space. while rules of this form are of no use in the final concept description  they are quite useful as storage areas for gas when evolving and testing sets of rules. 
the right-hand side of a rule is simply the class 
 concept  to which the example belongs. this means that our  classifier system  is a  stimulus-response  system with no message passing. 
1 evolving sets of classifier rules 
　　　since a concept description will consist of one or more classifier rules  we still need to specify how gas will be used to evolve sets of rules. there are currently two basic strategies: the michigan approach exemplified by holland's classifier system  holland  1   and the pittsburgh approach exemplified by smith's ls-1 system 
 smith  1 . systems using the michigan approach maintain a population of individual rules which compete with each other for space and priority in the population. in contrast  systems using the pittsburgh approach maintain a population of variable-length rule sets which compete with each other with respect to performance on the domain task. 
　　　very little is currently known concerning the relative merits of the two approaches. in this paper we report on results obtained from using the pittsburgh approach.1 that is  each individual in the population is a variablelength string representing an unordered set of fixed-length rules  disjuncts . the number of rules in a particular individual is unrestricted and can range from 1 to a very large number depending on evolutionary pressures. 
　　　our goal was to achieve a representation that required minimal changes to the fundamental genetic operators. we feel we have achieved this with our variable-length string representation involving fixedlength rules. crossover can occur anywhere  i.e.  both on rule boundaries and within rules . the only requirement is that the corresponding crossover points on the two parents  match up semantically . that is  if one parent is being cut on a rule boundary  then the other parent must be also cut on a rule boundary. similarly  if one parent is being cut at a point s bits to the right of a rule boundary  then the other parent must be cut in a similar spot  i.e.  1 bits to the right of some rule boundary . 
　　　the mutation operator is unaffected and performs the usual bit-level mutations. 
1 choosing a payoff function 
　　　in addition to selecting a good representation  it is important to define a good payoff function which rewards the right kinds of individuals. one of the nice features of using gas for concept learning is that the payoff function is the natural place to centralize and make explicit any biases  preferences  for certain kinds of concept descriptions. it also makes it easy to study the effects of different biases by simply making changes to the payoff function. 
　　　for the experiments reported in this paper  we wanted to minimize any a priori bias we might have. so we selected a payoff function involving only classification performance  ignoring  for example  length and complexity biases . the payoff  fitness  of each individual rule set is computed by testing the rule set on the current set of examples and letting: 
payoff {individual i  = {percent correct 1 
this provides a non-linear bias toward correctly classifying all the examples while providing differential reward for imperfect rule sets. 
         1  previous ga concept learners have used the michigan approach. see  wilson  1  and  booker  1  for details. 
1 the ga concept learner 
　　　given the representation and payoff function described above  a standard ga can be used to evolve concept descriptions in several ways. the simplest approach involves using a batch mode in which a fixed set of examples is presented  and the ga must search the space of variable-length strings described above for a set of rules which achieves a score of 1%. we will call this approach gabl  ga batch concept learner . 
　　　the simplest way to produce an incremental ga concept learner is to use gabl incrementally in the following way. the concept learner initially accepts a single example from a pool of examples. gabl is used to create a 1% correct rule set for this example. this rule set is used to predict the classification of the next example. if the prediction is incorrect  gabl is invoked to evolve a new rule set using the two examples. if the prediction is correct  the example is simply stored with the previous example and the rule set remains unchanged. as each new additional instance is accepted  a prediction is made  and the ga is re-run in batch if the prediction is incorrect. we refer to this mode of operation as batchincremental and we refer to the ga batch-incremental concept learner as gabil. 
1 evaluating concept learning programs 
as suggested in an earlier section  there arc many ways to evaluate and compare concept learning programs: in either batch or incremental modes. an incremental concept learner will make a prediction for each new instance seen. each prediction is either correct or incorrect. we are interested in examining how an incremental system changes its predictive performance over time. suppose each outcome  correct or incorrect  is stored. we could look at every outcome to compute performance  but this would only indicate the global performance of the learner  a typical batch mode statistic . instead  we examine a small window of recent outcomes  counting the correct predictions within that window. performance curves can then be generated which indicate whether a concept learner is getting any better at correctly classifying new  unseen  examples. the graphs used in the experiments in this paper depict this by plotting at each time step  after a new example arrives  the percent correct achieved over the last 1 arrivals  recent behavior . 
1 initial experiments 
the experiments described in this section arc designed to demonstrate the predictive performance of gabil as a function of incremental increases in the size and complexity of the target concept. we invented a 1 feature world in which each feature has 1 possible distinct values  i.e.  there are 1 instances in this world . this means that rules map into 1-bit strings and the length of individual 
	de jong and spears 	1 

rule sets is a multiple of 1. 
　　　in addition to studying the behavior of gabil as a function of increasing complexity  we were also interested in comparing its performance with an existing algorithm. id1r  utgoff  1   which is a well-known incremental concept learning algorithm  was chosen for comparison. id1r uses decision trees as the description language and always produces a decision tree consistent with the instances seen. 
　　　we constructed a set of 1 concept learning problems  each consisting of a single target concept of increasing complexity. we varied the complexity by increasing both the number of rules  disjuncts  and the number of relevant features per rule  conjuncts  required to correctly describe the concepts. the number of disjuncts ranged from 1 to 1  while the number of conjuncts ranged from 1 to 1. each target concept is labeled as ndmc  where n is the number of disjuncts and m is the number of conjuncts. 
　　　each target concept is associated with one experiment. within an experiment the number of disjuncts and conjuncts for the target concept remains fixed. the variation in target concept occurs between experiments. for each of the concepts  a set of 1 unique  noise free examples was generated from the feature space and labeled as positive or negative examples of the target concept. for the more complex concepts  this resulted in learning primarily from negative examples. 
　　　for each concept  the 1 examples were randomly shuffled and then presented sequentially as described above. this procedure was repeated 1 times for each concept and for each learning algorithm. the performance curves presented are the average behavior exhibited over 1 runs.1 
　　　figures 1 and 1 present the comparative results of applying both gabil and id1r to the 1c and 1c concepts. the remainder of the graphs are not shown due to space limitations. they represent intermediary results between the 1c and 1c extremes illustrated in figures 1 and 1. recall that each point on a curve represents the percentage of correct predictions achieved over the previous 1 instances presented  and averaged over 1 runs . note that this implies that the learning curves can be and are  in general  non-monotonic. in particular  they will remain at 1% indefinitely only when the algorithms have correctly learned the target concept. 
　　　the graphs indicate that  on the simpler concepts  the predictive performance of id1r improves more rapidly than that of gabil. however  id1r degrades in performance as the target concept becomes more complex  with significant deterioration in predictive power 
         1  it is not always possible for id1r to make a prediction based on the decision tree. if it cannot use the tree to predict  we let id1r make a random prediction. 
% 
instances processed fig 1. 1c 

instances processed 
fig1dlc 
seen as the number of conjuncts and disjuncts are increased. the performance of gabil  on the other hand  is relatively insensitive to the increase in concept complexity  resulting in significantly better predictive capability than id1r already on 1 disjunct concepts. the analysis below suggests that this trend will continue with even larger numbers of disjuncts and conjuncts. 
　　　we were surprised to see id1r suffer the most on the 1c target concept  since syntactically the concept is only moderately complex. the target concept is of the form: 
if  fl = 1  or  f1 = 1  or  f1 = 1  or 
 f1 = 1  then it's positive 
　　　although it is natural to expect that a simple target concept  from a syntactic viewpoint  would have a small decision tree representation  this is only a rough generalization. this target concept is represented by id1r as a decision tree of over 1 nodes. in fact  each negative 

example is represented by a unique leaf node in the 
1 	learning and knowledge acquisition 

decision tree. for this reason  id1r cannot generalize over the negative examples  and has a good chance of predicting any negative example incorrectly. furthermore  even the positive examples are not generalized well  resulting in prediction errors for positive examples. it is clear that the decision tree representation is poor for representing this particular concept. target concept 1c represents a worst case  which explains why the difference between gabil and id1r is greatest for this concept. a similar situation occurs for target concepts 1c  1c  and 1c  although to a lesser degree. 
　　　id1r relies upon quinlan *s information theoretic entropy measure to build its decision trees. the information theoretic measure favors those concepts in which individual features clearly distinguish target class membership. id1r's biases also favor concepts that can be represented with small decision trees. the experiments presented above indicate the effect of these built-in biases: the predictive power of id1r can vary dramatically depending on how well-matched the concept is to these biases. 
　　　gabil  however  performs much more consistently on target concepts of varying complexity. gabil is not significantly affected by the number of conjuncts  since with our fixed-length rule representation  large conjunctions are no more difficult to find than small ones. there is also no built-in bias towards a small number of disjuncts  although this could be achieved if desired by changing the payoff function  rather than gabil itself. the overall effect is similar to what has been noted in other ga applications  namely that the overhead of using an adaptive search process is quite evident on simpler problems  but the payoff is clearly seen as the problem complexity increases  see  for example   spears and de jong  1  . 
1 further analysis and comparisons 
having characterized the behavior of gabil in this controlled concept world  we have begun to extend the analysis to more complex and challenging problems. one of our first steps was to look at the family of multiplexor problems introduced to the machine learning community by wilson  wilson  1 . multiplexor problems fall into the general area of trying to induce a description of a kinput boolean function from input/output examples. because no single individual input line is useful in distinguishing class membership  information-theoretic approaches like quinlan's id1 system have a particularly hard time inducing decision trees for multiplexor problems. wilson's work indicated that his ga-based classifier system boole did not have such difficulties. some of these issues were addressed by quinlan in the development of his c1 system. quinlan subsequently reported that c1 outperforms boole on the multiplexor problems  quinlan  1 .1 
since we had access to c1  a successor to c1 
 quinlan  1    we felt that a direct comparison of gabil and c1 on multiplexor problems would be enlightening. since c1 is a batch-mode system  we have to run it in a batch-incremental mode in the same manner as gabil in order to provide meaningful comparisons. this can be achieved by running c1 in batch mode for every new instance seen  and using the resulting decision tree to predict the class of the next instance. 
　　　the 1-input multiplexor problem has 1 features in which each feature has 1 possible distinct values  i.e.   there are 1 instances in this world . this means that rules map into 1-bit strings and the length of individual rule sets is a multiple of 1. for this concept we randomly generated a set of 1 examples from the feature space  each example labeled positive or negative. since there are only 1 possible unique examples  the set does not contain unique examples  although they are noise free. this methodology allows for direct comparison with quinlan's reported results  quinlan  1 . 
　　　the set of 1 examples was randomly shuffled and then presented sequentially. this procedure was repeated 1 times for both learning algorithms. the performance curves presented in figure 1 are the average behavior exhibited over 1 runs. 
　　　gabil clearly outperforms c1 on the 1-input multiplexor problem. as noted above  the weaker performance of c1 is not due to the choice of representation  decision tree . in fact  a compact decision tree can be created to describe the concept. the problem lies with the information theoretic bias itself  which makes it hard to find this compact tree. preliminary results suggest similar performance differentials on larger multiplexor problems. 


　　　the concept description language and the search algorithm constitute strong biases for any concept learner. the above experiments indicate that id-like systems can suffer both from their decision tree language bias  see fig. 
1  and from their information theoretic search bias  see fig. 1 . when the biases are appropriate  id-like systems perform quite well. gabil  however  due to its minimal system bias  performs uniformly well on target concepts of varying complexity. these initial results support the view that gabil can be used as an effect concept learner although it may not outperform more strongly biased concept learning algorithms whose bias is appropriate for learning simpler target concepts. 
1 conclusions and future research 
this paper presents a series of initial results regarding the use of gas as the key element in the design of a system capable of continuously acquiring and refining concept classification rules from interactions with its environment. it is interesting to note that reasonable performance is achieved with minimal a priori bias. the initial results support the view that gas can be used as an effective concept learner although they may not outperform algorithms specifically designed for concept learning when simple concepts are involved. 
　　　this paper also sets the stage for the design of three additional ga-based concept learners. first  we wish to implement a variation of the current system that is truly incremental. second  we are also very interested in understanding the difference between using the pittsburgh approach and the michigan approach in this problem domain. the current fixed-length rule representation can be used directly in michigan-style classifier systems. third  we noted early in the paper that there were two basic strategies for selecting a representation for the concept description language. in this paper we developed a representation which minimized the changes to standard ga implementations. we also plan to explore the alternative strategy of modifying the basic ga operators to deal effectively with non-string representations. we feel that the development and analysis of such systems is an important direction the research community should follow in order to develop additional results on these and other problems of interest. 
acknowledgements 
we would like to thank diana gordon for her support and for many discussions on the biases in supervised concept learning systems. diana was also instrumental in helping us design our experimental methodology. we would also like to thank john grefenstctte and alan schultz for many useful comments about gabil and crossover  j. r. quinlan for c1  and paul utgoff for id1r. 
1 	learning and knowledge acquisition 
