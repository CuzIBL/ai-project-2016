 actions 
freek stulp and michael beetz
intelligent autonomous systems group  technische universitat m몮	unchen몮 boltzmannstrasse 1  d-1 munich  germany
{stulp beetz} in.tum.de
모
abstract
many plan-based autonomous robot controllers generate chains of abstract actions in order to achieve complex  dynamically changing  and possibly interacting goals. the execution of these action chains often results in robot behavior that shows abrupt transitions between subsequent actions  causing suboptimal performance. the resulting motion patterns are so characteristic for robots that people imitating robotic behavior will do so by making abrupt movements between actions.
in this paper we propose a novel computation model for the execution of abstract action chains. in this computation model a robot first learns situation-specific performance models of abstract actions. it then uses these models to automatically specialize the abstract actions for their execution in a given action chain. this specialization results in refined chains that are optimized for performance. as a side effect this behavior optimization also appears to produce action chains with seamless transitions between actions.
1 introduction
in recent years  a number of autonomous robots  including witas  doherty et al.  1   minerva  thrun et al.  1   and chip  firby et al.  1   have shown impressive performance in long term demonstrations. these robots have in common that they generate  maintain  and execute chains of discrete actions to achieve their goals. the use of plans enables these robots to flexibly interleave complex and interacting tasks  exploit opportunities  and optimize their intended course of action.
모to allow for plan-based control  the plan generation mechanisms are equipped with libraries of actions and causal models of these actions. these models include specifications of action effects and of the conditions under which the actions are executable. for good reasons  the action models are specified abstractly and disregard many aspects of the situations before  during  and after their execution. this abstractness has several big advantages. programmers need to supply

 
모모the work described in this paper was partially funded by the deutsche forschungsgemeinschaft in the spp-1.fewer actions because viewed at an abstract level the actions are applicable to a broader range of situations. also the models themselves become more concise. this not only eases the job of the programmers but also the computational task of the automatic planning systems. at more abstract levels the search space of plans is substantially smaller and fewer interactions between actions need to be considered.
모the advantages of abstraction  however  come at a cost. because the planning system considers actions as black boxes with performance independent of the prior and subsequent steps  the planning system cannot tailor the actions to the contexts of their execution. this often yields suboptimal behavior with abrupt transitions between actions. the resulting motion patterns are so characteristic for robots that people trying to imitate robotic behavior will do so by making abrupt movements between actions. in contrast  one of the impressive capabilities of animals and humans is their capability to perform chains of actions in optimal ways and with seamless transitions between subsequent actions.
모let us illustrate these points using a simple scenario in autonomous robot soccer that is depicted in figure 1. the starting situation is shown in the left sub-figure. the planner issues a three step plan: 1  go to the ball; 1  dribble the ball to shooting position; 1  shoot. if the robot naively executed the first action  as depicted in the center sub-figure   it might arrive at the ball with the goal at its back. this is an unfortunate position from which to start dribbling towards the goal. the problem is that in the abstract view of the planner  being at the ball is considered sufficient for dribbling the ball and the dynamical state of the robot arriving at the ball is considered to be irrelevant for the dribbling action.

figure 1: alternative execution of the same plan
모what we would like the robot to do instead is to go to the ball in order to dribble it towards the goal afterwards. the robot should  as depicted in the figure 1c  perform the first action sub-optimally in order to achieve a much better position for executing the second plan step. the behavior shown in figure 1c exhibits seamless transitions between plan steps and has higher performance  achieving the ultimate goal in less time.
모in this paper we propose a novel computational model for plan execution that enables the planner to keep its abstract action models and that optimizes action chains at execution time. the basic idea of our approach is to learn performance models of abstract actions off-line from observed experience. these performance models are rules that predict the situation- and parameterization-specific performance of abstract actions  e.g. the expected duration. then at execution time  our system determines the set of parameters that are not set by the plan and therefore define the possible action executions. it then determines for each abstract action the parameterization such that the predicted performance of the action chain is optimal.
모the technical contributions of this paper are threefold. first  we propose a novel computational model for the execution time optimization of action chains  presented in section 1. second  we show how situation-specific performance models for abstract actions can be learned automatically  section 1 . third  we describe a mechanism for subgoal  post-condition  refinement for action chain optimization. we apply our implemented computational model to chains of navigation plans with different objectives and constraints and different task contexts  section 1 . we show for typical action chains in robot soccer our computational model achieves substantial and statistically significant performance improvements for action chains generated by robot planners  section 1 .
1 system overview
this section introduces the basic concepts upon which we base our computational model of action chain optimization. using these concepts we define the computational task and sketch the key ideas for its solution.
1 conceptualization
our conceptualization for the computational problem is based on the notion of actions  performance models of actions  teleo-operators  teleo-operator libraries  and chains of teleooperators. in this section we will introduce these concepts.
모actions are control programs that produce streams of control signals  based on the current estimated state  thereby influencing the state of the world. one of the actions used here is gotopose  which navigates the robot from the current pose  at time t   xt yt 뷋t  to a future destination pose  xd yd 뷋d  by setting the translational and rotational velocity of the robot:
gotoposeaction xt yt 뷋t xd yd 뷋d  뫸vtra vrot
모teleo-operators  tops  consist of an action  as well as pre- and post-conditions  nilsson  1 . the post-condition represents the intended effect of the top  or its goal. it specifies a region in the state space in which the goal is satisfied. the pre-condition region with respect to a temporally extended action is defined as the set of world states in which continuous execution of the action will eventually satisfy the post-condition. they are similar to action schemata or strips operators in the sense that they are temporally extended actions that can be treated by the planner as if they were atomic actions.

모the gotoposetop has the empty pre-condition  as it can be executed from any state in the state space. its postcondition is  xt 뫘 xd yt 뫘 yd  뷋t 뫘 뷋d . its action is gotoposeaction.
모top libraries contain a set of tops that are frequently used within a given domain. in many domains  only a small number of control routines suffices to execute most tasks  if they are kept general and abstract  allowing them to be applicable in many situations. our library contains the tops: gotoposetop and dribbleballtop.
모a top chain for a given goal is a chain of tops such that the pre-condition of the first top is satisfied by the current situation  and the post-condition of each step satisfies the precondition of the subsequent top. the post-condition of the last top must satisfy the goal. it represents a valid plan to achieve the goal.

모performance models of actions map a specific situation onto a performance measure. in this paper the performance measure is time. alternatives could be chance of success or accuracy. these models can be used to predict the performance outcome of an action if applied in a specific situation  by specifying the current state  satisfying the pre-conditions  and end state  satisfying the post-conditions .
gotoposeaction.performance xt yt 뷋t xd yd 뷋d  뫸t
1 computational task and solution idea
the on-line computational task is to optimize the overall performance of a top chain. the input consists of a top chain that has been generated by a planner  that uses a top library as a resource. the output is an intermediate refined subgoal that optimizes the chain  and is inserted in the chain. executing the top chain is simply done by calling the action of each top. this flow is displayed in figure 1.
모to optimize action chains  the pre- and post-conditions of the tops in the top chains are analyzed to determine which variables in the subgoal may be freely tuned. these are the variables that specify future states of the robot  and are not constrained by the pre- and post-conditions of the respective top. for the optimization of these free variables  performance models of the actions are required. off-line  these models are learned from experience for each action in the top library. they are used by the subgoal refinement system during execution time  but available as a resource to other systems as well.
one of the big advantages of our approach is that neither
top library  nor the generation of top chains  the planner  nor the top chain executor need to be modified in any way to accommodate the action chain optimization system.
1 learning performance models
the actual optimization of top chains  to be discussed in section 1  needs performance models of each action in the top library. for each action  the robot learns a function that maps situations to the cost of performing this action in the respective situation. in this paper  the performance measure is time  although our mechanisms applies to other cost functions without requiring any change. the robot will learn the performance function 1  from experience 1  using a transformed state space 1  by partitioning the state space 1  by approximating functions to the data in each of these partitions. we will first motivate why  and then explain how this has been implemented.
모let us consider the navigation action gotoposeaction. this navigation action is based on computing a bezier curve  and trying to follow it as closely as possible. dribbleballaction uses the same method  but restricts deceleration and rotational velocity  so as not to loose the ball. we abstract away from their implementation  as our methods consider the actions to be black boxes  whose performance we learn from observed experience.
모the robot learns the performance function from experience. it executes the action under varying situations  observes the performance  and logs the experience examples. since the method is based solely on observations  it is also possible to acquire models of actions whose internal workings are not accessible. the robot executed each action 1 times  with random initial and destination poses. the robot recorded the direct variables and the time it took to reach the destination state at 1hz  thereby gathering 1 examples of the format  xt yt 뷋t xd yd 뷋d time  per action. these examples were gathered using our simulator  which uses learned dynamics models of the pioneer i platform. it has proven to be accurate enough to port control routines from the simulator to the real robot without change. using our pioneer i robots  acquiring this amount of data would take approximately two hours of operation time.
모the variables that were recorded do not necessarily correlate well with the performance. we therefore design a transformed feature space with less features  but the same potenfigure 1: transformation of the original state space into a lower-dimensional feature space.
모currently  we perform the transformation manually for each action. in our ongoing research we are investigating methods to automate the transformation. by explicitly representing and reasoning about the physical meaning of state variables  we research feature language generation methods.
모the last step is to approximate a function to the transformed data. this is done using model trees. model trees are functions that map continuous or nominal features to a continuous value. the function is learned from examples  by a piecewise partitioning of the feature space. a linear function is fitted to the data in each partition. model trees are a generalization of decision trees  in which the nominal values at the leaf nodes are replaced by line segments. we use model trees because 1  they can be transformed into sets of rules that are suited for human inspection and interpretation 1  comparative research shows they are the best  belker  1; balac  1  1  they tend to use only relevant variables. this means we can start off with many more features than are needed to predict performance  having the model tree function as an automatic feature selector. the tree was actually learned on an 1-dimensional feature space  x y 뷋 xg yg 뷋g dx dy dist angle to dest angle at dest   the model tree algorithm automatically discovered that only  dist angle to dest angle at dest  are necessary to accurately predict performance.
모we have trained a model tree on the gathered data  yielding rules of which we will now present an example. in figure 1  we depict an example situation in which dist and angle to dest are to 1m and 1  respectively. given these values we could plot a performance function for varying values of angle at dest. these plots are also depicted in figure 1  once in a cartesian  once in a polar coordinate system. in the linear plot we can clearly see five different line segments. this means that the model tree has partitioned the feature space for dist=1 and angle to dest=1 into five areas  each with its own linear model. below the two plots one of the learned model tree rules that applies to this situation is displayed. an arrow indicates its linear model in the plots.
모the polar plot clearly shows the dependency of predicted execution time on the angle of approach for the example situation. approaching the goal at 1 degrees is fastest  and would take a predicted 1s. approaching the goal at 1

figure 1: an example situation  two graphs of time prediction for this situation with varying angle at dest  and the model tree rule for one of the line segments.
degrees means the robot would have to navigate around the goal point  taking much longer  1s . p to evaluate the accuracy of the performance models  we again randomly generate 1 new test situations. for the gotoball routine  the mean absolute error and root-mean-square error between predicted and actual execution time were 1s and 1s. for the dribbleball routine these values were 1s and 1s. as we will see  these errors are accurate enough to optimize action chains.
1 automatic subgoal refinement
as depicted in figure 1  the automatic subgoal refinement system takes the performance models and a chain of teleooperators as an input  and returns a refined intermediate goal state that has been optimized with respect to the performance of the overall action chain. to do this we need to specify all the variables in the task  and recognize which of these variables influence the performance and are not fixed. these variables form a search space in which we will optimize the performance using the learned action models.
1 state variables
in the dynamic system model  dean and wellmann  1  the world changes through the interaction of two processes: the controlling process  in our case the low-level control programs implementing the action chains generated by the planner  and the controlled process  in our case the behavior of the robot. the evolution of the dynamic system is represented by a set of state variables that have changing values. the controlling process steers the controlled process by sending control signals to it. these control signals directly set some of the state variables and indirectly other ones. the affected state variables are called the controllable state variables. the robot for instance can set the translational and rotational velocity directly  causing the robot to move  thereby indirectly influencing future poses of the robot.
모for the robot  a subset of the state variables is observable to its perceptive system  and they can be estimated using a state estimation module. for any controller there is a distinction between direct and derived observable state variables. all direct state variables for the navigation task are depicted in figure 1. direct state variables are directly provided by state estimation  whereas derived state variables are computed by combinations of direct variables. no extra information is contained in derived variables  but if chosen well  derived variables are better correlated to the control task.

figure 1: direct state variables relevant to the navigation task
모state variables are also used to specify goals internal to the controller. these variables are bound  conform to planning terminology. it is the controller's goal to have the bound internal variables  approximately  coincide with the external observable variables. the robot's goal to arrive at the intermediate position could be represented by the state variables  xi yi . by setting the velocities  the robot can influence its current position  xt yt  to achieve  xt 뫘 xi yt 뫘 yi .
1 determining the search space
to optimize performance  only variables that actually influence performance should be tuned. in our implementation  this means only those variables that are used in the model tree to partition the state space at the nodes  or used in the linear functions at the leaves.
모in both the learned model trees for the actions gotoposeaction and dribbleballaction  the relevant variables are dist  angle to dest and angle at dest. these are all derived variables  computed from the direct variables  xt yt 뷋t xi yi 뷋i  and  xi yi 뷋i xg yg 뷋g   for the first and second action respectively. so by changing these direct variables  we would change the indirect variables computed from them  which in effect would change the performance.
모but may we change all these variables at will  not xt yt  or 뷋t  as we cannot simply change the current state of the world. also we may not alter bound variables that the robot has committed to  being  xi yi xg yg 뷋g . changing them would make the plan invalid.
모this only leaves the free variable 뷋i  the angle at which the intermediate goal is approached. this acknowledges our intuition from figure 1 that changing this variable will not make the plan invalid  and that it will also influence the overall performance of the plan. we are left with a one-dimensional search space to optimize performance.
1 optimization
to optimize the action chain  we will have to find those values for the free variables for which the overall performance of the action chain is the highest. the overall performance is estimated by summing over the performance models of all actions that constitute the action chain. in figure 1 the first two polar plots represent the performance of the two individual actions for different values of the only free variable  which is the angle of approach. the overall performance is computed by adding those two  and is depicted in the third polar plot.
gotopose +

figure 1: selecting the optimal subgoal by finding the optimum of the summation of all action models in the chain.
모the fastest time in the first polar plot is 1s  for angle of approach of 1 degrees. the direction is indicated from the center of the plot. however  the total time is 1s  because the second action takes 1s for this angle . these values can be read directly from the polar plots. however  this value is not the optimum overall performance. the minimum of the overall performance is 1s  as can be read from the third polar plot. below the polar plots  the situation of figure 1 is repeated  this time with the predicted performance for each action.
모we expect that for higher-dimensional search spaces  exhaustive search may be infeasible. therefore  other optimization techniques will have to be investigated.
1 results
to determine the influence of subgoal refinement on the overall performance of the action chain  we generated 1 situations with random robot  ball and final goal positions. the robot executed each navigation task twice  once with subgoal refinement  and once without. the results are summarized in table 1. first of all  the overall increase in performance over the 1 runs is 1%. we have split these cases into those in which the subgoal refinement yielded a higher  equal or lower performance in comparison to not using refinement. this shows that the performance improved in 1 cases  and in these cases causes a 1% improvement. in 1 cases  there was no improvement. this is to be expected  as there are many situations in which the three positions are already optimally aligned  e.g. in a straight line   and subgoal refinement will have no effect.
모unfortunately  applying our method causes a decrease of performance in 1 out of 1 runs. to analyze in which cases subgoal refinement decreases performance  we labeled each of the above runs higher  equal or lower. we then
before filteringtotalhigherequallower# runs11improvement1%1%1%-1%after filteringtotalhigherequallower# runs11improvement1%1%1%-1%table 1: results  before and after filtering for cases in which performance loss is predicted.
trained a decision tree to predict this nominal value. this tree yields four simple rules which predict the performance difference correctly in 1% of given cases. the rules declare that performance will stay equal if the three points are more or less aligned  and will only decrease if the final goal position is in the same area as which the robot is  but only if the robot's distance to the intermediate goal is smaller than 1m. essentially  this last rule states that the robot using the bezier-based gotoballaction has difficulty approaching the ball at awkward angles if it is close to it. in these cases  small variations in the initial position lead to large variations in execution time  and learning an accurate  general model of the action fails. the resulting inaccuracy in temporal prediction causes suboptimal optimization. note that this is a shortcoming of the action itself  not the chain optimization methods. we will investigate if creating a specialized action for the cases in which bezier based navigation is unsuccessful could solve these problems.
모we then gathered another 1 runs  as described above  but only applied subgoal refinement if the decision tree predicted applying it would yield a higher performance. although increase in overall performance is not so dramatic  from 1% to 1%   the number of cases in which performance is worsened by applying subgoal refinement has decreased from 1  1%  to 1  1% . apparently  the decision tree correctly filtered out cases in which applying subgoal refinement would decrease performance.
모summarizing: subgoal refinement with filtering yields a 1% increase in performance half of the time. only once in a hundred times does it cause a small performance loss.
1 related work
most similar to our work is the use of model trees to learn performance models to optimize hierarchical transition network plans  belker  1 . in this work  the models are used to select the next action in the chain  whereas we refine an existing action chain. therefore  the planner can be selected independently of the optimization process.
모reinforcement learning  rl  is another method that seeks to optimize performance  specified by a reward function. recent attempts to combat the curse of dimensionality in rl have turned to principled ways of exploiting temporal abstraction. several of these hierarchical reinforcement learning methods  e.g.  programmable  hierarchical abstract machines  maxq  and options  are described in the overview paper  barto and mahadevan  1 . all these approaches use the concept of actions  called 'machines'  'subtasks'  or 'options' respectively . in our view  the benefits of our methods are that they acquire more informative performance measures  facilitate the reuse of action models  and scale better to continuous and complex state spaces.
모the performance measures we can learn  time  success rate  accuracy  are informative values  with a meaning in the physical world. future research aims at developing meaningful composites of individual models. we will also investigate dynamic objective functions. in some cases  it is better to be fast at the cost of accuracy  and sometimes it is better to be accurate at the cost of speed. by weighting the performance measures time and accuracy accordingly in a composite measure  these preferences can be expressed at execution time. since the  q- value compiles all performance information in a single non-decomposable numeric value  it cannot be reasoned about in this fashion.
모the methods we proposed scale better to continuous and complex state spaces. we are not aware of the application of hierarchical reinforcement learning to  accurately simulated  continuous robotic domains.
모in hierarchical reinforcement learning  the performance models of actions  q-values  are learned in the calling context of the action. optimization can therefore only be done in the context of the pre-specified hierarchy/program. in contrast  the success of action selection in complex robotic projects such as witas  minerva and chip depends on the on-line autonomous sequencing of actions through planning. our methods learn abstract performance models of actions  independent of the context in which they are performed. this makes them reusable  and allows for integration in planning systems.
모the only approach we know of that explicitly combines planning and rl is rl-tops  reinforcement learning teleo operators   ryan and pendrith  1 . abrupt transitions arise here too  and the author recognizes that  cutting corners  between actions would improve performance  but does not present a solution.
모many behavior based approaches also achieve smooth motion by a weighted mixing of the control signals of various actions  saffiotti et al.  1 . since there are no discrete transitions between actions  they are also not visible in the execution. since achieving optimal behavior is not an explicit goal  it is left to chance  not objective performance measures.
모a very different technique for generating smooth transitions between skills has been developed for quadruped robots  hoffmann and duffert  1몮    also in the robocup domain. the periodic nature of robot gaits allows their meaningful representation in the frequency domain. interpolating in this domain yields smooth transitions between walking skills. since the actions we use are not periodic  these methods do unfortunately not apply.
1 conclusion and future work
on-line optimization of action chains allows the use of planning with abstract actions  without losing performance. optimizing the action chain is done by refining under-specified intermediate goals  which requires no change in the planner or plan execution mechanisms. to predict the optimal overall performance  performance models of each individual abstract action are learned off-line and from experience  using model trees. it is interesting to see that requiring optimal performance can implicitly yield smooth transitions in robotic and natural domains  even though smoothness in itself is not an explicit goal in either domain.
모applying subgoal refinement to the presented scenario yields good results. however  the computational model underlying the optimization is certainly not specific to this scenario  or to robot navigation. in principle  learning action models from experience using model trees is possible for any action whose relevant state variables can be observed and recorded. the notion of controllable  bound and free state variables are taken directly from the dynamic system model and planning approaches  and apply to any scenario that uses these paradigms. our future research therefore aims at applying these methods in other domains  for instance robots with articulated arms and grippers  for which we also have a simulator available.
모currently  we are evaluating if subgoal refinement improves plan execution on real pioneer i robots as much as it does in simulation. previous research has shown that action models learned in simulation can be applied to real situations with good result  buck et al.  1; belker  1 .
