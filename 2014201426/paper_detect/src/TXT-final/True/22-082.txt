 
in this paper  we present a model of opportunistic planning that uses planning-time rear soning about the opportunities that might arise during plan execution. the model is composed of three parts: a planning-time mechanism that places blocked goals into memory for later activation  an understanding system that activates suspended goals as a byproduct of parsing the world  and an executiontime process for evaluating opportunities and merging newly activated goals into the planning/execution agenda. we discuss this model in terms of examples from trucker  a routescheduling system  and runner  an errand planner. 
1 	planning and acting 
current research in planning has taken a rather dramatic change in course in the past few years. the notion that a planner can exhaustively preplan for a set of goals prior to execution has been largely abandoned. in part  this change is the result of demonstrations that planning for conjunctive goals is undecidable  chapman  1 . a more important factor has been the realization that the dual assumptions of traditional planning  a closed world and complete knowledge  are untenable in any but the simplest domains. 
　the planning theories that have grown out of this shift in paradigm differ from earlier theories in that they attempt to integrate planning and execution into a single process. the most extreme example of this has been the idea of situated activity  agre & chapman  1   which argues that plan-like behavior rises out of reflexive responses to external cues rather than from the guidance of a declarative plan. less extreme theories center around the notion that plans have to be refined and repaired at execution time. these theories include: 
  alterman's adaptive planning   in which execution-time problems are handled by moving between  semantically  similar plan steps. 
   *this work was supported in part by the office of naval research under contract number n1-k-1 and by darpa contract f1-c-1. 
1 	cognitive models 
  firby's raps   which allow a hierarchical planner to select between alternative plans on the basis of bottom-up information obtained at execution time. 
  georgeff and lansky's procedural reasoning system   in which changing goals and beliefs about the state of the world determine which plans are chosen to be put on the execution stack. 
  hammond's case-based planning  and simmon's and davis' generate  test  and debug   both of which use causal explanations of executiontime failures to choose between a variety of repairs. 
   moderate or extreme  each of these theories argues that the world changes  and that a planner must respond to those changes. they define a class of planners which  for lack of a better name  we will refer to as active planners: planners that both produce a plan and then actively alter that plan in the face of a changing environment. 
　thus far  the stress on integrating planning and execution in this work has been on the issue of recovering from plans that unexpectedly fail. little work has been done on the corollary concern of exploiting unexpected opportunities. in this paper  we will examine this concern and suggest a memory organization that addresses this issue. we will also describe two planners that are aimed at implementing the theory: trucker  a planner developed at chicago in the domain of pickup and delivery scheduling  and runner  a new planner under development in the more general domain of errand running. 
1 	opportunistic m e m o r y 
our approach in both trucker and runner uses episodic memory to organize  recognize and exploit opportunities. briefly  the algorithm includes the following features: 
  goals that cannot be fit into a current ongoing plan are considered blocked and  as such  are suspended. 
  suspended goals are associated with elements of episodic memory that can be related to potential opportunities. 
  these same memory structures are then used to parse the world so that the planner can make routine execution-time decisions. 
  as elements of memory are activated by conditions in the world  the goals associated with them are also activated and integrated into the current processing queue. 
in this way  suspended goals are brought to the planner's attention when conditions change so that the goals can be satisfied. 
　because the planner's recognition of opportunities depends on the nature of its episodic memory structures  we call the overall algorithm presented here opportunistic memory. 
1 	an example 
before we get into any details  it is important to understand the type of behavior we want to capture. we will do this by looking at a simple example. although this example is couched in terms of a story  we are interesting in modeling the planning behavior described  not in understanding the text. 
　this example is taken from the runner domain of errand running: 
on making breakfast for himself in the morning  john realized that he was out of orange juice. because he was late for work he had no time to do anything about it. 
on his way home from work  john noticed that he was passing a seven-eleven and recalled that he needed orange juice. having time  he stopped and picked up a quart and then continued home. 
　there are a number of interesting aspects to this example. first of all  the planner is confronted with new goals during execution as well during planning. this makes complete preplanning impossible. second  the planner is able to stop planning for a goal before deciding exactly how to satisfy it. in effect  he is able to say  i don't have all the information or the time to completely integrate a plan for this goal into my current agenda.  using schank's vocabulary  we call this the ability to suspend a goal  schank & abelson  1 . and third  although the goal is suspended  the planner is able to recognize the conditions that potentially lead to its satisfaction.1 
　there is a final element to this example that does not lie quite so close to the surface: in order to decide to suspend planning for the goal to possess orange juice  
   *one could argue that in this example the planner does completely preplan for this goal and that the plan is to get the orange juice on the way home. but this is begging the question  in that we can take any version of a plan designed to satisfy this goal and still argue that opportunities to satisfy it in other ways should be exploited. for example  if john passes by someone giving away free samples of orange juice on the way to work  we would certainly want him to recognize that this is a chance to satisfy a currently suspended goal. the point is that we want a planner to exploit opportunities to satisfy goals  whether or not it has already planned for them. 
john has to do some reasoning about what a plan for that goal entails. that is  he has to see that the goal is blocked by lack of time to go to the store. as a result  he has a clear idea  at planning time  as to what an execution-time opportunity would look like. 
1 	opportunistic planning 
the idea of opportunistic memory builds on two views of opportunism in planning-that of hayes-roth and hayes-roth   and that of birnbaum and collins 
. 
　hayes-roth and hayes-roth presented the view that a planner should be able to shift between planning strategies on the basis of perceived opportunities  even when those opportunities are unanticipated. their model  which they called opportunistic planning  consisted of a blackboard architecture  lesser et a/.  1  and planning specialists that captured planning information at many levels of abstraction. these specialists included domain-level plan developers  e.g.  specialists that know about routes  stores  or conditions for specific plans  as well as more strategic operators  e.g.  specialists that would look for clusters of goals and goals with similar preconditions . the planner could jump between strategies as different specialists  noticed  that their activation conditions were present. for example  in scheduling a set of errands  a specialist with knowledge of clustering of errands by location could be invoked while another specialist was scheduling them by goal priority. in this way  the planner could respond to opportunities noticed at planning time. 
   unfortunately  there are some problems with this view of opportunism. primarily  it includes no model of execution. as with many planners  all planning is done in the absence of the ability to execute the plan and thus respond to the effects of that execution. it is a model of opportunism at planning time rather than execution time. it fails to capture the behavior we are interested 
in modeling. 
   more recently  birnbaum and collins  presented a view of opportunism that does include a role for execution. under their model  goals are viewed as independent processing entities that have their own inferential power. when a goal is suspended because of resource constraints  it continues to examine the ongoing flow of objects and events that pass by the agent. if circumstances that would allow for the satisfaction of the goal arise  the goal itself recognizes them and projects a plan into the current action agenda. 
　they present a simple yet compelling example of the behavior that interests them in a description of an agent trying to obtain both food and water in the wild. in their example  the agent suspends the goal to find water while trying to satisfy the goal to find food. while searching for food  however  the agent jumps over a stream and is able to recognize that the stream affords an opportunity to satisfy a suspended goal. 
　birnbaum and collins argue that this is managed by giving the suspended goal the ability to examine the current situation and inference directly off of it. they argue that this must be the case  since there can be no way to 
	hammond 	1 
decide  at the time of suspending a goal  the exact conditions under which it should be activated. any planner that has to store and then activate suspend goals will miss opportunities that it could not anticipate. 
   birnbaurn  has argued further that indexing of suspended goals by descriptions of the conditions that allow their satisfaction is an unworkable approach. not only will the indices be too complex  but the planner will have to constantly compare the current state of the world to the features used to index suspended goals in what amounts to a memory of unsatisfied tasks. as birnbaurn says  this is hardly opportunism. 
　we share birnbaum's and collins' philosophical stance of trying to explain complex opportunistic behavior 
 even up to the subtle form of opportunism exhibited in freudian slips . however  we disagree that this behavior results from goals constantly monitoring the world. we believe that indexing suspended goals is a far better explanation. 
　birnbaum's arguments against indexing apply to models that separate a memory of goals from the planner's memory and understanding of the world. under our model  however  there is no such distinction between types of memory. there is only one memory that is used to understand the world and to store suspended goals. thus the act of recognition is the same as the act of indexing. 
1 	the assumptions 
the ideas of opportunistic memory in this paper exist in the context of a more general theory of planning and memory called case-based planning  hammond  1; kolodner et a/.  1 . case-based planning views planning as a memory task. memories of past successes are used as prototypes for new plans  memories of past failures are used to avoid repeating the failures  and memories of past plan modifications are used to tailor old plans to new situations. this contrasts with the view of planning as a task of composition in which large plans are constructed piece-by-piece out of primitive actions. 
　there are certain assumptions that we make in casebased planning: first  we do not have a closed world. this limits the usefulness of preplanning and projection. second  we have an incomplete and imperfect model of the world. this is assumed of not just the domain in general but also of the steps in plans that we use on a regular basis. in practice  this means that a planner cannot fully trust either its knowledge of the world's physics or its understanding of the world's current state. third  we cannot do projection. that is  we cannot run complete simulations of our plans in order to tease out problems due to step interactions. this follows from our first and second assumptions. while we can project on the basis of what the planner knows  there is no guarantee that the projection will match what really happens. 
　we do not make these assumptions because we want to. we make them because we have been forced to. such is the nature of real-world domains. but human planners are able to plan in complex domains  often with little knowledge of the true physics of those domains. this 
1 	cognitive models 
makes us believe that it is possible for a planner to plan and do where it cannot understand. 
　strangely enough  the need for opportunistic reasoning and the tools for developing it both grow out of these assumptions. having assumed that the planner cannot completely model the effects of its own actions or predict those of other agents  we must provide it with some sort of mechanism that will allow it react to the world as well as act in it. 
   likewise  because the planner cannot completely model the world  we must assume a mechanism for parsing or understanding the world that provides the planner with enough information to make execution-time decisions. in effect  the lack of a perfect world model requires that any planner must watch what it is doing as it is doing it. we will use this understanding process as the core of our opportunistic reasoning mechanism. 
　there are other ramifications to the use of a case-based planner  having to do with what constitutes a planning step and the nature of projection. the most important point we want to make in this section  however  is that a planner must understand and interact with the world in which it executes plans. 
1 	trucker and runner 
our opportunistic memory algorithm is only part of an overall approach to planning. because of this  we must include some discussion of the two planners trucker and runner. both planners are case-based in design and active in the sense that they test their plans through interaction with complex simulated worlds. 
1 	t r u c k e r 
trucker is a university of chicago planner that interacts with a simulated world in order to test out its plans and learn from both failure and success. its domain is a ups-like pickup and delivery task in which new orders are received during the course of a day's execution. its task is to schedule the orders and develop the routes for its trucks to follow through town. 
　trucker creates new plans using two means: a map and a memory. trucker uses its map when it initially builds a route for an area. once it has built a new route and verified it by running it in the world  it stores the route in a case memory  indexed by its literal endpoints as well as by neighborhood and part-of-town descriptors. when it is able to find a plan for an order in memory  the plan is expanded and placed at the end of trucker's action agenda. trucker never tries to optimize over multiple goals unless it already has a plan in memory that does so. 
　trucker optimizes its planning for multiple goals only when it notices an opportunity to do so during execution. if trucker notices an opportunity to satisfy a goal that is scheduled later in the queue  it stops and reasons about the utility of merging the later plan with the steps it is currently running. if it is able to construct a plan that is significantly better than one which treats the plans independently  it uses the new plan. it also stores the new plan in memory  indexed by each of the separate goals. when either goal reoccurs  trucker searches its action queue for for the partner goal and uses the plan that it has created for the pair. 
   even when a goal is placed on the action queue  trucker treats it as though it were blocked. that is  it establishes the conditions that would allow trucker to satisfy the goal and then associates the goal with the memory structures that would be active during the recognition of those conditions. for example  while planning for a pickup at the sears tower later in the day  trucker associates the goal with its internal representation of the tower. this allows it to activate and then satisfy the goal if it recognizes the sears tower earlier in the day. 
1 	r u n n e r 
the runner project is the direct descendent of trucker. runner's domain is similar to the errandrunning domain used by hayes-roth and hayes-roth to study opportunistic planning . unlike trucker  runner has only a single agent to control  but runner's agent is capable of a wider range of activity  and its domain allows for a richer set of goal interactions than does trucker's. this change of domain was motivated by our desire to study a wider range of issues in both opportunism and execution-time failure recovery. 
   runner's plans are mop structures  schank  1  that organize steps by their functions: establishing preconditions  side conditions  goal satisfaction  or postcondition cleanup. for each new goal being planned for  runner checks for prototypical plan interactions that would allow it to merge preconditions  piggyback plans  or subsume them under single plans. for example  runner looks for plans that have similar precondition steps in order to splice them together. 
　like trucker  runner is designed to put off optimization until it obtains cues during execution that inform it of positive plan interactions. so runner ends up suspending many of its goals in much the same way that trucker does. the content of runner's goals and plans  however  is more than just the simple pickup and delivery structures used by trucker. as a result  runner must do more of an analysis of what might constitute an opportunity to satisfy a goal than trucker was forced to do. 
   runner analyzes blocked plans and goals using two knowledge sources: precondition information associated with each plan and knowledge of opportunities with which it should be concerned. for example  in the case of john realizing that he cannot go to the grocery store to get orange juice  runner would examine the preconditions  e.g.  the planner has money  is at the store  and has time  and look for specific conditions that might lead to an opportunity  e.g.  the planner has a missing resource  the planner is at the appropriate location or the planer has a time window . each potential opportunity is evaluated as to how normative it is-with nonnormative features ranking as better opportunities-as well as whether or not it is actually blocked in the current situation. 
　in our example  having money  being at a grocery store  and having time are all preconditions for buying orange juice. but there is a difference between them  in that having money is a normative condition and as such does not constitute an opportunity  while being near a store is a non-normative precondition and as such does constitute an opportunity. 
1 	opportunism in trucker and runner 
in both trucker and runner  blocked goals are associated with the elements of memory that will be active when opportunities to satisfy those goals arise. the main difference between the two planners is that establishing these conditions is relatively trivial in the trucker domain-goals are of the same basic type. 
while the details of goal blockage and the analysis of opportunity differs between these planners  the basic approach to dealing with blocked goals is the same. 
　while these two planners are different  both require the ability to halt planning on a goal  suspend it  and then recall it when opportunities for execution present themselves. the approach used by trucker and runner to suspend and recall blocked goals has three basic parts. 
　first  the planner suspends the blocked goals by associating them with the elements of memory that describe potential opportunities. this requires that the planner have access to a vocabulary that differentiates between the different types of planning problems  e.g.  resource limitations  time constraints  and the planner's limitations . 
　next  the planner executes the plans for its active goals. during execution  it has to monitor the ongoing effects of its plan as well as the effects of the plans of others in its world. the representational elements used to do this parsing are the same elements with which suspended goals have been associated. as a result  the planner's general recognition of a situation that constitutes an opportunity can immediately activate any goals that have previously been associated with that situation. 
　finally  any activated goals are integrated into the current set of scheduled steps  and the plan is executed. this requires reasoning about resources and protections  as well as the effects of actions. 
　in our example of john and the orange juice  these steps translate into: 
  john's goal to possess orange juice is blocked by lack of time to run the default plan. he decides  on the basis of the preconditions on his plan to possess orange juice  that being at a store would constitute an opportunity to get the orange juice.1 as a result  he links the suspended goal to the condition of being near a store. 
  while coming home  he sees and recognizes a seven-eleven. this activates the goal to obtain orange juice that he associated with this condition earlier 
in the day. 
　　1 other preconditions are noted as well  e.g.  having money and time  but these conditions are discarded because the first is a normative condition of the planner and the second is difficult to recognize. 
	hammond 	1 

  he then tests the preconditions on the plan and merges it into his current agenda. 
1 	suspending blocked goals 
when either trucker or runner finds a goal blocked  that goal is suspended. in trucker  this requires associating a request with the memory token for the locations it involves. for runner  this requires somewhat deeper reasoning about the various conditions that might provide opportunities to satisfy a blocked goal. 
　when trucker receives a new request for a pickup and delivery  it attempts to satisfy the order using a variety of methods. first it checks all active requests on its truck's agendas for one that has a known positive interaction with the new request. if this fails  trucker attempts to find a truck that is currently idle to take up the order. if this also fails  trucker searches its  desktop  for a suspended request that might be usefully combined with the new order. if all else fails  trucker is forced to place the request on a queue of orders waiting for idle trucks. 
　when this is done  trucker considers the goal blocked  and thus suspends it. to suspend a goal  trucker marks its representation of the goal's pickup and delivery points with an annotation that there is a goal related to those locations. because trucker plans for only one type of goal  it doesn't have to do any more reasoning than this to identify good opportunities to satisfy the suspended goals. 
　runner's domain and the goals it must plan for are more complex than trucker's. as a result  it must reason far more than trucker about the conditions that might constitute opportunities to satisfy a blocked goal. 
　in general  opportunities to run plans can be derived from the preconditions on each of the steps of a plan. a planner could  given time  move through a plan step by step and collect the preconditions that have to obtain at that point in the plan. but this would require the examination of many conditions that are not particularly useful in the context of opportunism. some preconditions for obtaining orange juice-having money  having time  and being able to carry the carton-are not useful if we are looking for the features that will allow us to recall the suspended goal at the appropriate time. this is because there are more constraints on  opportunities  than on simple preconditions. these constraints include features such as ease of recognition  likelihood of occurrence  and predictiveness. 
　for example  having money is a strong precondition for buying orange juice  but it is also a normative condition. as a result  it is a bad predictor of an opportunity to satisfy the goal to have orange juice. if the suspended goal is tied to having money  the planner will be reminded of the goal far too often. 
　rather than test all preconditions of a plan for these constraints  runner uses a taxonomy of opportunity types to derive the conditions that will serve as opportunities to satisfy the plan. this taxonomy guides runner's search through the plan for appropriate precon-
1 	cognitive models 
ditions. once a plan has been analyzed in terms of this taxonomy  it is annotated with pointers to the features associated with opportunities to run it. features are removed from this list if they cause the goal and plan to be recalled at inappropriate times. 
　this taxonomy takes the form of a set of tests or questions that runner asks of a plan: 
  is there a special 1 resource that is needed to r u n the plan  
if so  associate the goal with the resource. 
  is there a special tool that is needed to r u n the plan  
if so  associate the goal with the tool. 
  is there a special location associated w i t h the plan  
if so  associate the goal with the location. 
  is there a special agent or skill associated w i t h the plan  
if so  associate the goal with the agent or skill. 
  is there a specific time constraint associated w i t h the plan  
if so  associate the plan with the time. 
there are also special-purpose rules for suspending particular goals. possession goals  for example  are associated with the object of the possession. 
   using these rules  runner associates the blocked goal to possess orange juice with the location  grocery-store  and the object itself  orangejuice. this association takes the form of a suspend link from the representations of these items to the suspended goal itself. runner associates the goal with the least likely conditions with the hope that most of the other conditions will obtain when the suspended goal is activated. the other conditions are checked when the goal is recalled  but they are not linked to the suspended goal in memory. 
1 	recalling suspended goals 
both trucker and runner tie execution of actions to locations  landmarks and addresses that they recognize in the world. thus they must parse and interpret the objects in the world. it is during this parse that they both recognize and recall previously suspended goals. 
   a typical trucker plan  when fully expanded  is a route in the form of a list of the turns that have to be made  described in terms of street names and compass directions. so the plan step  goto  1 e-1th   after a pick-up at  1 s-v1dlawn  expands into: 
	 start north 	 1 w1dlawn   
 turn east e-1th  
 turn north s-cornell  
 turn east e-1th  
 stop  1 e-1th   
　as trucker moves through its world  it parses the objects at its current location and responds to any 
　　1  a feature is  special  if it does not predictably reoccur as a product of the planner's policy decisions. 

changes that the tokens it has recognized suggest: turning  for example  when it recognizes the 1 block of woodlawn. but trucker does more than this when it recognizes an individual token. it also checks the token for any annotation of a goal that might be associated with it. if one is found  trucker activates the suspended goal and attempts to integrate it into the current schedule. this allows trucker to easily and effectively activate suspended goals when the opportunities to satisfy them arise. further  the overhead on this activation is trivial  in that all that trucker has to do is look for a specific type of link on each of the objects it recognizes. 
　runner has a wider variety of planning options for any one goal. runner can pick up orange juice at any grocery store  not just a particular one. as a result  we are using a much more general and realistic approach to parsing than we used in trucker. 
　to deal with this  runner is designed to use the dmap parser  martin &  riesbeck  1  as its recognition system. dmap uses mops  schank  1  to represent concept sequences to recognize concepts by recognition of their parts. dmap uses a smart marker-passing algorithm in which two types of markers are used to activate and predict concepts in an isa and part-of network. activation markers are passed from primitive features up an abstraction hierarchy. in runner  these features include type descriptions such as  road    wall    window   and  sign  but no tokens such as  the seveneleven on cornell and 1th . when any part-of a concept is active  prediction markers are spread to its other parts. when a predicted concept is handed an activation marker  it becomes active. likewise  when all parts of a concept are activated  the concept itself is activated. 
　for our uses  we add a new type of link to dmap. this link associates suspended goals with concepts that represent opportunities to achieve them. pointing from concepts to goals  this suspend link is traversed by any activation marker that is placed on the concept. so  the activation of a concept also activates any suspended goals associated with it. 
   in our example  the suspended goal to get the orange juice is associated with the concept representing 'the planner is at a grocery store . as the world is parsed  a seven-eleven is recognized as a sequence of  parking lot    building   and  seven-eleven sign . because dmap is passing activation markers up isa links  the seven-eleven is recognized as a particular seven-eleven  an instance of seven-elevens in general  a 
convenience-store  a grocery-store  and a store. while the suspended goal is not directly associated with the concept  seven-eleven   it is associated with grocery-store. so the recognition of the seven-eleven causes the activation of the suspended goal. in general  runner uses this property of dmap to recall goals associated with general characterizations of opportunities through the recognition of specific situations. 
　in both planners  the basic approach is the same. in order to execute a plan  the low-level features must be disambiguated into tokens representing specific objects in the world. as this is done  each token is checked for an associated goal. and if any goal is found  it is considered a candidate for immediate satisfaction. 
1 	exploiting the opportunities 
once a suspended goal is reactivated  it has to be evaluated for integration into the current execution agenda. 
　here again  the trucker approach uses specialpurpose techniques tailored to the domain. when a suspended goal is recalled by the planner  it attempts to find the best placement in the current route for the awakened request. scheduling the pickup is trivial  in that a truck is at the pickup location. the difficulty lies in scheduling the delivery. trucker does this by stepping through each location already scheduled and finding the section of the route that will be the least altered by the insertion of the delivery. this can be done even before the exact routes are selected  by using the map and simple rules of geometry. 
because this optimization is fairly time-consuming  
trucker saves the resulting route  so that it can reuse it when the same conjunct of goals arises again. we see the recognition of execution-time opportunity as a special case of expectation failure  schank  1  and treat it as an indication of a gap in trucker's knowledge 
base.1 
runner deals with a wider variety of goals than does 
trucker. as a result  the special-purpose techniques used in trucker are not applicable. still  runner's plan-merging techniques are not as general as those used by most planners. runner falls back on the same techniques for merging plans that it uses for plan construction. 
　steps in runner's plans are specifically labeled as to function  precondition  goal-satisfaction  post-condition  clean-up . runner uses these labels to search for specific ways to merge plans. although runner goes through the same process during preplanning  the task is somewhat easier when applied to a newly activated goal. just as trucker knows that it is already at the pickup location  runner knows that the conditions associated with the activation of the suspended goal are already satisfied. if they were not  the suspended goal would not have been noticed in the first place. 
　in our orange-juice example  the steps required to get the planner to a store can be ignored  in that being at the store is the condition that activated the goal in the first place. but the planner can also ignore other steps. in particular  the steps that are used to  recover  from the precondition of being at the store once the plan is over can be ignored. because the planner did not need to run the steps in the grocery-store plan to get to the store  it will not have to run the steps in that plan that will get it away from the store. in general  precondition/cleanup pairs can be canceled together. the planner knows that the running plan must include the 
　　1  for a more detailed discussion of this type of learning  see  hammond et a/.  1 . 
	hammond 	1 
same pair  and need not be concerned with the part of the recalled plan that includes it. 
　the remaining steps-going into the store  buying the orange juice  and exiting-have to be integrated in a fairly traditional way. the planner checks the preconditions not set by the activation conditions  and it notes the use of resources and their interactions with existing protections. the final product is a small change in the overall plan that takes the planner into the store for a 
　moment before resuming his trip home. 
　in both trucker and runner  the task of integrating recalled goals is essentially the same as the task of creating an initial plan. the only difference is that both planners have additional information about the conditions that currently hold and  as a result  are able to avoid consideration of the steps that establish those conditions. runner is able to go beyond this and avoid consideration of steps involving postcondition cleanup. 
1 	conclusions 
in this paper  we have argued the need for an executiontime ability to recognize and exploit planning opportunities. our goal was to present a model of opportunistic planning that provides this ability with little overhead. 
　we argue that our model of opportunistic memory does exactly that. by associating blocked goals with the same structures used to represent the planner's world  we are able to get activation of suspended goals as a by-product of the understanding process. 
　the process  implemented in trucker and currently being expanded upon in runner  requires three basic steps. suspended goals are associated with the elements of memory that are related to potential opportunities. these memory structures are then used to parse the world during execution. as elements of memory are activated by conditions in the world  any goals associated with them are also activated and integrated into the current planning queue. 
　this combination of planning-time suspension and execution-time activation gives both trucker and runner the ability to halt consideration of a goal with the assurance that the goal will be brought back to mind when conditions change to allow its satisfaction. 
1 	acknowledgements 
i'd like to thank mitch marks and tim converse for their work on trucker as well as jeff berger  neil hurwitz  and greg hajek for their comments and conversation. i'd also like to thank larry birnbaum and gregg collins for letting me steal these ideas in the first place. 
1 	references 
 agre k chapman  1  agre  p.e. k chapman  d. pengi: an implementation of a theory of activity. in: proceedings of the sixth national conference on artificial intelligence  seattle  washington  july 1. 
 alterman  1  alterman  r. adaptive planning: refitting old plans to new situations. in: proceedings of 
1 	cognitive models 
the seventh annual conference of the cognitive science society  irvine  california  august 1. 
 birnbaum  1  birnbaum  l. integrated processing in planning and understanding. yale technical report #1  1. 
 birnbaum k collins  1  birnbaum  l. k collins  g. opportunistic planning and freudian slips. in: proceedings of the sixth annual conference of the cognitive science society  boulder  colorado  june 1. 
 chapman  1  chapman  d. planning for conjunctive goals. technical report tr 1  mit artificial intelligence laboratory  1. 
 firby  1  firby  r.j. an investigation into reactive planning in complex domains. in: proceedings of the sixth national conference on artificial intelligence  seattle  washington  july 1. 
 georgeff & lansky  1  georgeff  m.p. k lansky  a.l. reactive reasoning and planning. in: proceedings of the sixth national conference on artificial intelligence  seattle  washington  july 1. 
 hammond  1  hammond  k. case-based planning: viewing planning as a memory task. academic press  cambridge  massachusetts  1. 
 hammond et al  1  hammond  k.  converse  t.  k marks  m. learning from opportunities: storing and reusing execution-time optimizations. in: proceedings of the seventh national conference on artificial intelligence  st. paul  minnesota  august 1. 
 hayes-roth k hayes-roth  1  hayes-roth  b.  k hayes-roth  f. a cognitive model of planning. cognitive science 1-1. 
 kolodner et al  1  kolodner  j.l.  simpson  r.l.  & sycara-cyranski  l. a process model of case-based reasoning in problem solving. in: proceedings of the ninth international joint conference on artificial intelligence  los angeles  california  august 1. 
 lesser et al  1  lesser  v.r.  fennell  r.d.  erman  l.d.  k reddy  d.r. organization of the hearsay-ii speech understanding system. in: ieee transactions on acoustics  speech and signal processing. assp-1  1  1. 
 martin k riesbeck  1  martin  c  k riesbeck  c. uniform parsing and inferencing for learning. in: proceedings of the fifth national conference on artificial intelligence  philadelphia  pennsylvania  august 1. 
 schank  1  schank  r. dynamic memory: a theory of reminding and learning in computers and people. cambridge university press  cambridge  england  1. 
 schank k abelson  1  schank  r. k abelson  it. scripts  plans  goals and understanding. lawrence erlbaurn associates  hillsdale  new jersey  1. 
 simmons k davis  1  simmons  r.  k davis  r. generate  test  and debug: combining associational rules and causal models. in proceedings of the tenth internation joint conference on artificial intelligence  milan  italy  august 1. 
an adaptive model of decision-making in planning 
	gregg collins1 	lawrence birnbaunv 	bruce krulwiclv 
	university of illinois 	yale university 	yale university 
dept of computer science 	dept. of computer science 	dept. of computer science 
	urbana  illinois 	new haven  connecticut 	new haven  connecticut 
	1 	1 
collins p.cs.uiuc.edu birnbaum yale.arpa krulwich yale.arpa abstract 
 learning how to make decisions in a domain is a critical aspect of intelligent planning behavior. the ability of a planner to adapt its decision-making to a 
 domain depends in part upon its ability to optimize the tradeoff between the sophistication of its decision procedures and their cost. since it is difficult to optimize this tradeoff on a priori grounds alone  we propose that a planner start with a relatively simple set of decision procedures  and add complexity in response to experience gained in the application of its decision-making to real-world problems. our model of this adaptation process is based on the explanation of failures  in that it is the analysis of bad decisions that drives the improvement of the decision procedures. we have developed a test-bed system for the implementation of planning models employing such an approach  and have demonstrated the ability of such a model to improve its procedure for projecting the effects of its moves in chess. 
1 introduction 
the ability to plan effectively involves many different forms of reasoning: projecting the effects of actions in a current or hypothetical situation  deciding which goal to pursue from among the many that might be pursued at any given time  constructing sequences of actions that can achieve a given goal  determining whether to execute such a sequence  and so on. by and large  most research on planning in al has concentrated on those aspects of planning that in vol ve reasoning about actions  and more specifically  on how sequences of actions can 
1 this research was supported in part by the defense advanced 
research projects agency  monitored by the air force office of scientific research  under contract number f1-c-1  and in part by the office of naval research under contract number n1-k-1. 
be constructed to achieve particular and well-defined goals. considerably less attention has been devoted to those aspects of planning that we might term decisionmaking: deciding whether  or when  to pursue a given goal  or whether to use a given plan for the goal  or for that matter whether even to consider the goal in the first place. 
to make such decisions correctly  a planner must know a great deal about the particular environment in which they arise. for example  a person's decision about whether to turn around and look if he hears footsteps behind him depends crucially on the situation in which he finds himself: his decision is likely to be quite different on a moderately populated suburban street in the middle of the day than in an empty city street late at night. the relevant characteristic of the environment  obviously  is the rate at which threats can be expected to arise  and in particular the threat of being victimized by a criminal. 
one of the major issues confronting any planner is determining how much effort to expend on evaluating a given decision. decision-making in general is subject to a tradeoff between the sophistication of the procedure employed to make the decision  and the time and effort required to perform the analysis. a more sophisticated decision-making procedure will  generally speaking  either consider more alternatives  consider each alternative in more depth  or do both. this requires more reasoning  and is correspondingly more expensive. whether a more sophisticated procedure is worth using depends on whether the gain from its superior performance outweighs this extra cost. consider  for example  the tradeoffs involved in deciding which goal to pursue: the planner must first determine which of a myriad possible goals-including goals to acquire resources  protect desired states  or improve the current situation  among others-are reasonable candidates  and then compare the expected benefits from each of these candidates. obviously  the more goals the planner considers as reasonable candidates  the more likely it will be to find a better goal to pursue. on the other hand  the more goals 
	collins  birnbaum and krulwich 	1 
the planner must compare in making its decision  the more this procedure will cost. 
the key point about such tradeoffs in the sophistication of the decision-making process is that  like the decisions themselves  they are highly dependent upon the particular planning environment. for example  in a highly timelimited situation  e.g.  basketball or some other fastpaced game   a planner simply cannot afford to consider very many candidate goals when trying to determine what it should do next. thus  when confronted with a novel planning environment  a planner must be prepared to adapt its decision-making processes to that environment  balancing the tradeoff between their sophistication and their cost. 
it seems difficult to imagine how an agent might find the roughly optimal level of decision-making sophistication on the basis of a priori reasoning alone. this suggests that planners should be prepared to alter their decisionmaking processes on the basis of experience within a new planning environment. the approach we have taken  therefore  is to start with a relatively simple model of decision-making-one that ignores many potential alternatives  and many of the factors that might bear on deciding among them-and to progressively make it more sophisticated in response to poor decisions  i.e.  decisions to pursue goals and plans that ultimately fail. in other words  our model of how a planner adapts its decision-making to new planning domains is failuredriven  see  e.g.  sussman  1; schank  1; hayesroth  1; minton  1; hammond  1 . in such an approach  when a plan fails  the goal of preventing a similar failure in the future leads to an attempt to explain the current failure.1 the explanation of the failure  in turn  suggests ways in which similar failures might subsequently be avoided. 
we have elsewhere presented our model of failuredriven learning in planning domains  birnbaum and collins 1;collinsand birnbaum  1a and 1b . in our approach  the planner encodes a description of the intended functioning of its plans in explicit justification structures. when the expectation that a plan will succeed is violated  the planner can trace back through the justification structure to identify the culprit among the initial assumptions  see  e.g.  dekleer  et ah  1; doyle  1 . by identifying which assumptions have failed  the planner is able to arrive at a characterization of how the particular si tuation brought about this failure  which can then serve as the basis for a rule that suggests what to do when similar situations arise subsequently. 
this paper describes the application of our model to the progressive adaptation of decision-making procedures to new planning environments. such an application requires spelling out the assumptions involved in the expectation that simple decision procedures will suffice  how those assumptions can fail  and how the procedures can be improved upon the diagnosis of such failures. we first discuss how these issues arise in a case study of the chess fork. we next briefly describe a program which implements some of our ideas  and then discuss its application to a second case study  concerning the development of a planner's ability to predict its opponent's move in deciding its own move in chess. 
1 case study: the fork 
as our first case study in improving decision-making in planning  we will consider how a novice chess planner can learn to deal with a fork-a situation in which the planner's opponent confronts it with an attack on two pieces simultaneously. unless a move can be made that will defend both pieces at once-or alternatively one piece can be moved or defended in a way that at the same time poses a strong counter-threat to the opponent-one of the pieces in question will surely be captured. the fork is something that all chess players eventually learn about  usually as a result of being victimized by it. the question is  what does a novice learn about the fork from this experience  
a planner victimized by an opponent's tactic should learn two things: how to avoid being victimized in the future  and how to use the same tactic to victimize others. we will concentrate here on the first of these  the question of how a planner learns to avoid the fork. since the fork takes advantage of the planner's plan for defending its pieces  we must begin by considering what the planner knows about this plan  and in particular  what he knows about the assumptions upon which its efficacy depends. 
the plan that almost every novice chess player seems to have for defending his pieces is this: before each turn  check to see if any of the opponent's pieces is in position to move to the location of one of your pieces. if this is the case  then you can prevent the execution of the threat by moving the threatened piece  or by guarding it with another piece. this plan makes the assumption that a one-move warning of a threat to a piece is sufficient to to allow it to be defended  and this is generally the case. it is this assumption  however  that the fork takes advantage of: when two imminent threats are detected on the same turn  there is not time to block both of them. 
given that a novice planner understands that its plan depends upon this assumption  a plausible explanation 

for how it comes to understand the fork can be constructed. the fork results in the failure of the plan to protect materiel  since it results in the capture of a piece. this failure can be traced to the violation of an underlying assumption of the plan  namely that a one-move warning of any threat would be sufficient.1 the violation of this assumption was  in turn  brought about by the opponent's positioning of a piece so that it attacked two pieces simultaneously. the key point here is this: such an analysis not only explains the fork  it suggests the way to guard against it. the failure of the assumption that a onemove warning is sufficient implies the need to detect at least one of the two threats earlier. moreover  the fact that the failure was brought about by the positioning of a piece to attack two pieces at once means that efforts at earlier detection can be limited to situations where the opponent can  with a single move  produce a threat to two pieces. thus  understanding how the fork violates its assumptions enables a planner to determine how it can avoid being victimized by the fork in the future: it must modify its plan for scanning the board to include a 
check for opponent's pieces that can be moved into position to attack two unguarded pieces of its own. 
this explanation of how a novice planner learns from experiencing the fork leaves us with a question  however: why would the planner have assumed that a onemove warning was enough in the first place  the most obvious answer is that the planner was simply unable to imagine a circumstance in which this would not be the case. unfortunately  this explanation does not hold up upon further reflection. the idea that multiple threats pose a problem for defenses that involve detecting and blocking individual threats is well known to all human planners: this is the fundamental reason  for example  why  ganging up  allows a superior force to be defeated by a collection of inferior forces in any competitive situation. it seems likely that any human planner would have experienced this phenomenon  and grasped its significance  long before learning to play chess. but given the knowledge that detection-based plans for blocking threats are vulnerable to an overload of threats  plus the lack of any particular reason to believe that this problem would not arise in chess  it seems unlikely that the planner would nevertheless conclude that a onemove warning would always be enough. to argue this would be to conclude that vulnerability to the fork is the result of a mistake-a mistake made consistently by almost every person who ever learns the game of chess. it would be difficult to explain why such an error in reasoning should be made by nearly everyone. 
in fact  the answer lies not in the logic of the situation  but in its economics. the reason for not detecting threats two 
'see collins and birnbaum  1b  for a discussion of how this is accomplished. 
moves in advance is that it would cost too much to do so: the planner would be swamped by the need to respond to a massive number of threats  since in two moves most of the opponent's pieces would be able to move to at least one square currently occupied by one of the planner's pieces. most of these  threats  would never develop  and the planner would at best waste a great deal of time that could better be used plotting strategy. there is  in other words  a tradeoff: by detecting threats early  the planner provides more time to deal with multiple threats  but at the cost of being forced to consider an enormous number of possible-but unlikely-threats. by detecting threats later  the planner runs the risk of not having enough time to deal with multiple threats  but avoids being swamped with too many low probability reports. in this case  the tradeoff clearly is on the side of later detection  and this is likely to be true in many domains. 
a key characteristic of the appropriate defense against the fork is that it avoids a blow-up in the number of detected threats by looking only for cases in which one piece can be moved so as to attack two pieces at once. in other words  by looking for forks in particular  rather than detecting all threats two moves in advance  the planner focusses its attention narrowly enough to make the extra effort worthwhile. so this is what the planner really learns from the fork: not that one move's warning might not be enough-it already knew that-but a characterization of the circumstances in which it is not that is precise enough to allow them to be detected efficiently. 
we can now provide a coherent account of how the fork is learned. the planner begins with the understanding that threats to pieces will arise periodically in chess. the standard strategy for dealing with threats in any domain is this: first  construct methods for detecting individual instances of threats in advance. second  construct methods for blocking threats that can be executed when a threat is spotted. and third  ensure that all threats will be detected in time for a blocking routine to be carried out. 
in chess  first  the obvious method for detecting threats is to determine which of the opponent's pieces could move to locations occupied by the planner's pieces in some particular number of moves. second  the obvious method for blocking these threats is to move the threatened piece out of harm's way; a slightly more sophisticated plan is to guard the piece with another. and third  since any piece can generally either be moved or guarded in a single turn  and since-all other things being equal-it is best to detect threats as late as possible  a planner can conclude that one move's warning will generally be enough. however  the planner cannot prove that one move will always be enough  and so is forced to make the assumption that this will generally be the case. by 
	collins  birnbaum and krulwich 	1 
making this assumption explicit  and by ensuring that it is monitored whenever a threat arises  the planner assures itself of being alerted to cases in which the assumption fails  as it does in the fork. so  while the planner is forced to accept the cost of a few failures as the price of an efficient threat-detection strategy  by monitoring the assumptions it is forced to make  and analyzing the failures that occur  it is able to gradually improve its performance by dealing with problematic cases one at a time. 
in a sense  this explanation argues that a planner is deliberately courting failure as a shortcut to a better plan. viewed this way  the alternative would have been for the planner to reason out  a priori  the circumstances in which multiple threats could occur  rather than waiting for those circumstances to arise in practice. in effect  this would mean inventing the fork before actually playing the game. while this is possible in principle  in practice the computational cost is likely to be extremely high- high enough that it is almost certainly cheaper to wait for the fork to happen. we believe that similar explanations underlie the acquisition of a wide range of decisionmaking strategies employed by planners. 
1 an implementation and a second case study 
 in order to explore more concretely our failure-driven approach to the adaptation of decision-making processes in planning  we have constructed a test-bed- comprising mechanisms for inference and rule application  justification maintenance  expectation handlers  failureexplanation  and rule patching-and implemented within it a simple model of decision-making and planning for turn-taking games. this model  in turn  has been used for exploring rudimentary planning and learning in tic-tac-toe and in chess. 
 def-brule proj-factor-1 
 world  move move   player player  result  weight integer  
 opp-move move   time time  
 better-move move   better-goal goal  
 better-value integer   orig-value integer    
 project-factor proj-factor-1 world move player result weight  
 and  = world  world-at-time time   
 decide  player-opponent player  
 possible-moves-at-time  player-opponent player  time  world simple-dec-factors opp-move  
 = result  world-after-move opp-move  world-after-move move world     = weight 1  
  
 justification herel  
figure 1: initial projection method 
the planning model is implemented in 1 fairly general rules  concerning the detection of threats and opportunities  making choices  forming and checking expectations  and the like; some typical rules are shown later in 
1 	cognitive models 
this section. in addition  there are 1 chess-specific reasoning rules. all of these rules  general as well as specific  have associated justification structures  as do all particular facts and expectations contained in the system's data base. these justification structures are constructed and utilized by the rule applier  the failure explanation algorithm  and the rule patching mechanism. we devote the rest of this section to describing the program's application to a second case study  concerning the development of a planner's ability to predict its opponent's move in deciding its own move in chess. 
the decision-making process within our planning model is composed of three interleaved components: a decider  which determines what move to make  a projector  which projects the results of a possible move in a given situation  and an evaluator  which evaluates a situation from a given player's perspective. our planner starts with extremely primitive methods for accomplishing each of these decision-making tasks. first  to decide what move to make it simply projects the results of each move available to it  evaluates each resulting situation  and chooses the move that yields the best result. to project the results of making a given move  the planner simply assumes that its opponent will behave as it would in any given situation; the projection method that implements this is shown in figure 1. finally  to evaluate a given situation  the planner computes the difference between the total values of the two player's pieces remaining on the board. 

figure 1: game board sequence consider now how a planner endowed with these primitive decision-making methods would behave in the situation shown in figure 1. to such a planner  moving the rook to take the knight looks like the best move. this decision is based on the erroneous expectation that its opponent will take the planner's knight. it forms this expectation because the projection method shown in figure 1 simply evaluates the opponent's options in the original situation  i.e.  without taking into account the effects of the planner's own move. the use of such an unsophisticated procedure considerably simplifies the projection problem  because it obviates the need to recompute the opponent's response individually for each move contemplated by the planner. however  its validity  and hence the validi ty of theexpectationsthe planner generates concerning the opponent's moves  depends on an assumption that nothing will occur that will enable the opponent to make a better move than those currently available to it. this assumption  is an instance of what is sometimes referred to as an inertia assumption  namely an assumption that things will stay as they are as much as possible. in our planner  this assumption is itself justified by a conjunction that says roughly  nothing will happen to give him a better move because he can't do anything to give himself one  i won't do anything to give him one  and no outside forces will give him one.  because the planner's decision to take the knight is justified by its projection that its opponent will take a knight in response  it monitors the status of this expectation. the expectation  in turn  is justified by the assumptions underlying the projection method used to produce it. 
applying rules in new-expectation-failure-rules -expectation failure: 
item= move-to-make  move opponent move-take pawn 
　　　　　　　　　　　　　　 ro loc 1   rc- loc 1  knight  opponent  goal-unknown  1  traversing... 
checking fact 
# fact 1:  project  move computer move-take rook 
 move opponent move-take pawn 
 rc- loc 1}  rc- loc 1  knight  
checking b-rule #{b-rule proj-factor-1  -  valid: 
 not  exists  c event  
          move-to-make  e.1 opponent  better-goal.1    -  valid: 
 not  exists  e event  
 and  extraneous-event  e.1  
　　　　　 event-enables-event  e.1  better-move.1     -  found a bug: 
 no  and  move-enables-move  move.1  better-move.1  
 move-possible  better-move.1  time.1  
 move-legal  better-move.1  
 evaluate  world-after-move  better-move.1 
　 world-after-move  move.1  world.1  eval-factors 
 player-opponent  player.1  
 better-value.1  
 evaluate  world-after-move  opp-move.1 
    world-after-move  move.1  world.1d  eval-factors 
 player-opponent  player.1  
 orig-value.1  
    better-valuc.1  orig-value.1    
*** returning a bug *** 
figure 1: traversing an expectation's justification 
unfortunately  as we can see  the opponent is not going to make the move that the computer expected  and the move that he will make-namely taking the planner's rook-is a better move than the one the planner expected. when this happens  the planner detects that its expectation that the opponent will take the knight has failed. this then causes the planner to traverse the justification structure for that expectation  checkingthe assumptions upon which it depends. 
 in particular  the planner will discover that its assumption that the opponent will not have a better move has been violated. a partial transcript of this process is shown in figure 1  but it should be clear that the assumption in the justification that will fail is the inertia assumption that no event will occur that will give the opponent a better move than we expect him to have  and in particular the assumption that no action of the program's will give the opponent a better move. 
as can be seen in figure 1  the system knows more than just the identity of the assumption that failed: it also knows how that assumption failed. the program's domain knowledge allows it to determine that the opponent's move was in fact a legal one  and a comparision of the justification for this belief with the program's prior expectation reveals that the event that enabled the opponent to make a better move than expected was in fact the computer's own move. once the program discovers this  it must modify its reliance on the inertia assumption to take the problem into account  using the information provided in its justification structures and its analysis of the failure. the program uses standard goal regression techniques  see  e.g.  dejong and mooney  1; mitchell etal.  1  todetermine just those aspectsof the situation that caused the expectation to fail. 
the resulting generalized explanation for the failure is used to patch the method that predicts the opponent's response  as utilized by the projection component  so that it checks for opportunities presented to the opponent by the planner's own move  thus ensuring that this mistake will be avoided in the future  see figure1 . the new rule says roughly the following: 'to see what the world will be like after making move m  see what you would do in the opponent's situation at the current time  and assume 
	collins  birnbaum and krulwich 	1 

that he will make that move  as long as your move m doesn't enable a better move for him.  when the same example is run with the new rule antecedant  the computer does not make the same mistake. 
 def-brule proj-factor-1 
 world  move move   player player  result  weight integer  
 opp-move move   time time  
 better-move move   better-goal goal  
 better-value integer   orig-value integer    
 project-factor proj-factor-1-mod world move player result weight  
 and  = world  world-at-time time   
 decide  player-opponent player  
　 possible-moves-at-time  player-opponent player  time  world simple-dec-factors opp-move  
 = result  world-after-move opp-move 
 world-after-move move world    
 = weight 1  ; here's the new stuff: 
 no  and  move-enables-move move better-move  
 move-possible better-move  current-time   
 move-legal better-move  
 evaluate result eval-factors 
 player-opponent player  orig-value  
 evaluate  world-after-move better-move 
          world-after-move move world   eval-factors  player-opponent player  
better-value  
   better-value orig-value       
　　　figure 1: the modified projection method 1 conclusion 
learning how to make decisions in a domain is a critical aspect of intelligent planning behavior. the ability of a planner to adapt its decision-making to a domain depends in part upon its ability to optimize the tradeoff between the sophistication of its decision procedures and their cost. since it is difficult to optimize this tradeoff on a priori grounds alone  we propose that a planner start with a relatively simple set of decision procedures  and add complexity in response to experience gained in the application of its decision-making to real-world problems. our model of this adaptation process is based on the explanation of failures  in that it is the analysis of bad decisions that drives the improvement of the decision procedures. we have developed a test-bed system for the implementation of planning models employing such an approach  and have demonstrated the ability of such a model to improve its procedure for projecting the effects of its moves in chess. 
