 
we present a new algorithm  gm-sarsa o   for finding approximate solutions to multiple-goal reinforcement learning problems that are modeled as composite markov decision processes. according to our formulation different sub-goals are modeled as mdps that are coupled by the requirement that they share actions. existing reinforcement learning algorithms address similar problem formulations by first finding optimal policies for the component mdps  and then merging these into a policy for the composite task. the problem with such methods is that policies that are optimized separately may or may not perform well when they are merged into a composite solution. instead of searching for optimal policies for the component mdps in isolation  our approach finds good policies in the context of the composite task. 
keywords: reinforcement learning 
1 introduction 
traditional reinforcement learning algorithms can successfully solve small  single-goal tasks. the main challenge in the area of reinforcement learning is scaling up to larger and more complex problems. the scaling problem takes a number of forms. we may have a problem that has a very large state space  a problem that is best described as a set of hierarchically organized goals and subgoals  or a problem that requires the learning agent to address several tasks at once. it is the last form of scaling that this paper is concerned with. 
　the naive approach to learning to solve composite tasks is to create a state space that includes all of the information that is relevant to each sub-task. the agent would then learn in this joint space  receiving reward when any of the sub-goals are accomplished. the problem with this approach is that it suffers from the curse of dimensionality; as additional state dimensions are added for each new sub-task  the size of the 
joint state space grows exponentially. 
　a more promising approach to this sort of multiple-goal problem is to use the well known q-learning algorithm to train one learning module to handle each of the sub-goals. the internal q-values of the different learning modules can 
dana ballard computer science department university of rochester 
 rochester  ny 1 dana cs. rochester edu 
then be used to fairly distribute control among the modules. this approach has been independently explored in  humphrys  1  and  karlsson  1 . it is attractive in its simplicity  and it has shown good empirical performance in a number of domains. 
　in this paper we will highlight a previously unrecognized problem with existing modular q-learning algorithms. existing algorithms learn component policies that may be highly sub-optimal in the context of the composite task  because they do not take into account the fact that the component modules are forced to share control. we will show how to fix this problem by replacing the q-learning with the closely related sarsa o  learning rule. the resulting algorithm shows improved performance on a large sample problem. 
1 the problem formalized 
the underlying formalism for many reinforcement learning algorithms is the markov decision process. an mdp  denoted m is described by a 1-tuple  1  a  t  r   where s is the state space  a is the action space  and t s  a  s'  is the transition function that indicates the probability of arriving in state s' when action a is taken in state s. the reward function r s  a  denotes the expected one-step payoff for taking action a in state s. the goal of reinforcement learning algorithms is to discover an optimal policy 1r* s  that maps states to actions so as to maximize discounted long term reward. 
　here we consider the problem of discovering a joint policy for a set of n mdp's  throughout we will use subscripts to distinguish the mdps. these mdp's each have a distinct state space  but they share a common action space  and are required to execute the same action on each time step. this model is intended to map to the case of a single agent that is simultaneously faced with a set of different goals. 
　the n component mdps implicitly define a larger composite mdp. formally  the goal is to find the optimal policy for this composite mdp. the optimal composite policy is defined as the policy that maximizes summed discounted reward across the component mdps. 
　the state space of the composite mdp is the cross product of the state spaces of the component mdps: s - s  x ♀1 x ... x sn- the composite reward function is defined as: 
r s  a  =  in the case where the component 
poster papers 	1 mdps are independent  the composite transition function can be written as:  in the case 
where the component mdps are not independent  the exact composite transition function will depend on on the particular dependencies between the models. 
　in theory  there is no reason that the composite mdp could not be solved directly using the traditional q-learning algorithm. however  this is generally not practical because the size of the composite state space may grow exponentially with the number of component mdps. 
1 modular q-learning 
humphrys and karlsson  humphrys  1; karlsson  1  independently developed similar approaches to the problem of multiple-goal reinforcement learning. the idea is that a separate learning module is created for each component mdp. the agent takes actions in the environment  and each module i is trained with the standard q-learning update rule: 
  1  
where i  is the immediate reward  a is the learning rate parameter  and  is a discount factor applied to future rewards. in single goal reinforcement learning problems  these q-values are used only to rank order the actions in a given state. the key observation here is that the q-values can also be used in multiple-goal problems to indicate the degree of preference that modules have for different actions. there are several possible ways these values can be used to select a compromise action to execute. the different approaches will be referred to as action selection mechanisms. 
　karlsson's suggestion  which he calls  greatest mass   is to generate an overall q-value as a simple sum of the q-values of the individual modules: q s a  -  the action with the maximum summed value is then chosen to execute. we will refer to this approach as gm-q for greatest mass q-learning. 
　humphrys considers the greatest mass approach  but raises the objection that the action with the highest sum may not be particularly good for any of the modules  with the result that no module is able to reach its goal. he explores several winner-take-all alternatives that constrain the chosen action to be optimal for at least one module. for a given state .s each of the n modules promotes its own action with a value wl sl . the module with the largest w value is then allowed to execute its preferred action. 
　the simplest method for generating the w-values  which we will refer to as top-q  is to set wt st  = thus giving control to the module with the highest q-value in the current state. this method suffers from the drawback that the module with the highest q-value may have no preference over what action is chosen  while another module stands to lose a great deal if its action is not selected. the method sometimes exhibits reasonable performance  but this is strongly dependent on the structure of the reward functions. 
　a better alternative  referred to as negotiated w-learning  is to grant control to the module that stands to lose the most long term reward if it is not selected. this module can be discovered by examining the q-values for the current state. refer to  humphrys  1  for a detailed description of the algorithm. 

figure 1: results of one training run for gm-sarsa o  and three q-learning based algorithms on a food gathering task. training is divided into trials lasting 1 time steps. data points are generated by suspending training every 1 trials and computing the mean performance for 1 trials without exploration. each algorithm uses an e-grecdy exploration policy with e linearly reduced from .1 to 1 during the first half of trials. all algorithms use a fixed learning rate of .1  and a discount factor of .1. 
1 	the problem with modular q-learning 
q-learning has some attractive qualities as a basis for multiple-goal reinforcement learning. chief of these is the fact that it is an off-policy learning method. this means that q-learning for a single mdp is guaranteed to converge to the optimal solution regardless of what policy is followed during training  as long as each state-action pair is visited infinitely often in the limit. this fact makes it easy to prove convergence results for the composite reinforcement learning algorithms introduced above. in particular  it is easy to see that each module is guaranteed to converge to the optimal policy and value function for its own mdp. since the action selection mechanisms generate a policy deterministically from the component value functions  the composite policy is also guaranteed to converge  although there is no guarantee concerning the quality of the composite solution. 
　unfortunately  the off-policy character of q-learning is also a serious limitation. the difficulty is that the one-step value updates for each module are computed under the assumption that all future actions will be chosen optimally for that mdp. this assumption is not valid under the action selection mechanisms described above; future actions will represent some compromise policy in which the different modules share control. this means that the computed q-values do not converge to the actual expected return under the composite policy. instead  the max in equation  1  results in q-values with a positive bias. 
1 modular sarsa o  
1 	poster papers a possible solution is to the problem of positive bias is to replace q-learning with an on-policy learning algorithm. in particular we will explore the use of sarsa o   rummery and niranjan  1; singh and sutton  1; sutton  1 . the update rule for sarsa o  is: 
  1  
this update rule is virtually identical to that for q-learning except that the max over q-values on the right has been replaced with the q-value of the state action pair that is actually observed on the next step. for the case of single mdps sarsa o  has been proved to converge to the optimal policy as long as the exploration rate is asymptotically decayed toward zero according to an appropriate schedule  singh et al  1 . 
　the key observation for our purposes is that  since sarsa o  is an on-policy method  it does not suffer from the problem of positive bias. since updates are based on the actions that are actually taken  rather than on the best possible action  we expect sarsa o  based modules to discover q-values that are closer to the true expected return under the composite policy. any of the action selection mechanisms from section 1 could be recast to use sarsa o  rather than q-learning to train the modules. however  we focus on the method of greatest mass. we refer to the resulting algorithm as gm-sarsa o . recall that the goal is to maximize the summed reward across all of the component mdps. assuming that we have trustworthy utility estimates from each of the modules  it makes sense to choose the action that has the highest summed utility across all of the modules. by definition  this is the action that will lead to the greatest summed long term reward. this reasoning did not hold under q-learning  because the utility estimates were inaccurate under the composite policy. 
　thus far we have no convergence proof for the gmsarsa o  algorithm. refer to the associated technical report 
 sprague and ballard  1  for a discussion of the possible convergence characteristics. 
1 examples 
figure 1 demonstrates the performance of gm-sarsa o  on a sample composite task  the task is adapted from  singh and cohn  1  . the goal of the agent in this task is to gather stationary food items while avoiding a predator in a 1 x 1 grid. there are three food items present at all times. 
　the agent moves in any of the eight possible directions at each time step. a random move is made with a probability of .1. if the agent contacts any of the food items it receives a reward of 1  and the item is randomly moved to a new position. the agent receives a reward of .1 for every time step that it avoids the predator. the predator moves deterministically one position toward the agent on every other time step. 
　the positions of the food items as well as the positions of the agent and predator result in 1 ` 1 million distinct states. this is too large for a monolithic tabular learning algorithm to be practical. the task is a good candidate for a 
　modular reinforcement learning algorithm because it can be decomposed into several small mdps. one mdp describes the agent's interaction with the predator  and three mdps describe the interaction with the food items. each of these component mdps has 1 = 1 states. figure 1 shows the performance of gm-sarsa o  as well as the three q-learning based algorithms from section 1 on this task. of the four algorithms  gm-sarsa o  exhibits the best performance. 
1 conclusion 
we have presented a method for learning approximately optimal policies for a certain class of composite markov decision processes. empirical results demonstrate that our approach performs better than a number of existing algorithms. future work will focus on proving convergence results for our algorithm. a longer version of this paper  including a discussion of related work is available as  sprague and ballard  1 . 
acknowledgments 
this material is based upon work supported by a grant from the department of education under grant number p1  a grant from the national institutes of health under grant number 1rr1 and a grant from the national science foundation under grant number el a-1. any opinions  findings  and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the above mentioned institutions. 
