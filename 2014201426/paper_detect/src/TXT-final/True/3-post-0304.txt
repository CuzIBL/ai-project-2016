 
computer-generated texts  whether from natural language generation  nlg  or machine translation  mt  systems  are often post-edited by humans before being released to users.  the frequency and type of post-edits is a measure of how well the system works  and can be used for evaluation.  we describe how we have used post-edit data to evaluate sumtime-mousam  an nlg system that produces weather forecasts. 
1 introduction 
in this paper we describe an evaluation technique  which looks at how much humans need to post-edit texts generated by an nlg system before they are released to users.  postedit evaluations are common in machine translation  hutchins and somers  1   but we believe that ours is the first large-scale post-edit evaluation of an nlg system. mitkov and an ha  reported a small scale post-edit evaluation of their nlg system. 
　the system being evaluated is sumtime-mousam  sripada et al  1   an nlg system  which generates weather forecasts from numerical weather prediction  nwp  data. the forecasts are marine forecasts for offshore oilrigs.  sumtime-mousam is operational and is used by weathernews  uk  ltd to generate 1 draft forecasts per day  which are post-edited by weathernews forecasters before being released to clients. 
 
time wind dir wind spd 1m wind spd 1m gust 1m gust 1m 1 w 1 1 1 1 1 w 1 1 1 1 1 wsw 1 1 1 1 1 sw 1 1 1 1 1 ssw 1 1 1 1 1 s 1 1 1 1 1 s 1 1 1 1  
table 1. weather data produced by an nwp model for 1-jun 
1 
 
　table 1 shows a small extract from the nwp data for 1  and table 1 shows part of the textual forecast that sumtime-mousam generates from the nwp data. 
 
field text wind kts  1m w 1 backing sw by mid afternoon and s 1 by midnight. wind kts  1m w 1 backing sw by mid afternoon and s 1 by midnight.  
table 1. extract from sumtime-mousam forecast produced from nwp data in table 1. 
 
　weathernews uses sumtime-mousam to generate draft forecasts; we call them 'pre-edit texts'. the forecasters then post-edit them to produce 'post-edit texts'.  when the forecaster is done  the complete forecast is sent to the customer. 
1 post-edit evaluation  
the evaluation was carried out on 1 forecasts  collected during period june to august 1.  each forecast was roughly of 1 words  so there are about one million words in all in the corpus. 
for each forecast  we have the following data: 
  data: the final edited nwp data 
  pre-edit 	text: 	the 	draft 	forecast 	produced 	by sumtime-mousam. 
  post-edit text: the manually post-edited forecast  which was sent to the client. 
  background information: includes date  location  and forecaster 
　the following procedure is performed automatically by a software tool: 
  first  we break sentences up into phrases  where each phrase describes the weather at one point in time. 
  the second step is to align phrases from the previous step as a preparation for comparison in the next step. our alignment procedure first generates an exhaustive 
list of possible alignments and uses a scoring scheme to select aligned phrases. 
  the third step is to compare aligned phrases and label each pre-edit/post-edit pair as match  replace  add  or delete. 
　for example  a and b of figure 1 are analyzed as in table 1 where phrases are shown separated by a shaded row. 
 
 a. pre-edit text: sw 1 backing ssw 1 by midday   then gradually increasing 1 by midnight.  b. post-edit text: sw 1 gradually increasing ssw 1. 
 
figure 1. example pre-edit and post-edit texts from the post-edit corpus 
 
pos a b label direction sw sw match speed 1 1 replace     conjunction then  none  delete adverb gradually gradually match verb increasing increasing match direction  none  ssw add speed 1 1 match time by midnight  none  delete  
table 1. detailed edit analysis 
 
　we processed 1 forecast pairs  pre-edited and postedited . these were divided into 1 phrases. out of these  the alignment procedure failed to align 1  1%  phrases. out of the successfully aligned phrases  1  1%  are perfect matches  and the remaining 1  1%  are mismatches.  table 1 summarizes the mismatches and suggests that the major problem is ellipsis.  most  1 out of 1  1%  of these errors are deletions  where the forecaster deletes words sumtime-mousam's texts. 
 
s. no. mismatch type freq. % 1. ellipses  word additions and deletions  1 1 1. data related replacements  range and direction replacements  1 1 1. lexical replacements 1 1  total 1   
table 1. results of the evaluation showing summary categories and their frequencies 
1 discussion of post-edit evaluations  
we were attracted to post-edit evaluation because we believed that  a  people would only edit things that were clearly wrong; and  b  post-editing was an important usefulness metric from the perspective of our users.  looking back   b  was certainly true.  the amount of postediting that generated texts require is a crucial component of the cost of using sumtime-mousam  and hence of the attractiveness of the system to users. 
　   a  however was perhaps less true than we had hoped. wagner  also described post-edited texts in mt as at times noisy. during the development of sumtime-
mousam  our analysis of manually written forecasts  reiter and sripada  1  had highlighted a number of  noise  elements that made it more difficult to extract information from such corpora. while collecting the post-edit data  we assumed that people would only post-edit mistakes  where the generated text was wrong or sub-optimal  and hence post-edit data would be better for evaluation purposes than corpus comparisons. 
　  in fact  however  there were many justifications for postedits.  some post-edits fixed problems in the generated texts  such as overuse of then ; some post-edits refined/optimized the texts  such as using for a time ; some post-edits reflected individual preferences  such as easing vs decreasing ; and some post-edits were downstream consequences of earlier changes  such as introducing ssw before '1' in b  in the example of section 1 .  we wanted to use our post-edit data to improve the system  not just to quantify its performance  and we discovered that we could not do this without attempting to analyze why post-edits were made.  probably the best way of doing this was to discuss post-edits with the forecasters. 
1 conclusion 
we have used analysis of post-edits  a popular evaluation technique in machine translation  to evaluate sumtimemousam  an nlg system that generates weather forecasts. while we encountered some problems  such as the need to identify why post-edits were made  on the whole we found post-editing to be a useful evaluation technique which gave us valuable insights as to how to improve our system. 
