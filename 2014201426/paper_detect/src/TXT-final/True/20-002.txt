a microfeature based scheme for modelling semantics 
lawrence a. bookman 
computer science department 
brandeis university 
waltham  ma 1 us.a. 

a b s t r a c t 
　　　one fundamental problem of natural language processing is word sense disambiguation. solving this problem involves the integration of multiple knowledge sources: syntactic  semantic  and pragmatic. recent work has shown how this problem can be modelled as a constraint satisfaction process between competing syntactic and semantic structures. we have defined and implemented a  locally-distributed  microfeature based model called mibs  that uses a distributed short-term memory  stm  composed of microfeatures to represent the underlying sentence semantics. this work represents an improvement over previous work  as it provides a natural language understanding system a means to dynamically determine the current context and adjust its relationship with the sentences that follow. here  the meaning of a word is represented not as a symbol in some semantic net  but as a collection of smaller features. the values of the microfeatures in stm vary dynamically as the sentence is processed  reflecting the system's  settling  in on the sentence's meaning. in addition they represent an automatic context mechanism that helps the system to disambiguate the sentences that follow. 
	i. 	i n t r o d u c t i o n 
　　　one fundamental problem of natural language processing is word sense disambiguation. solving this problem involves the integration of multiple knowledge sources: syntactic  semantic  and pragmatic. recent work  cottrell & small  1; cottrell  1; waltz & pollack  1  has shown how this problem can be modelled as a constraint satisfaction process between competing syntactic and semantic structures. the above models rely primarily on local representations  one concept per node   as opposed to distributed representations  although waltz and pollack  1  suggest ways that a static microfeature-based representation could be used to represent global contextual influences for competing word senses. cottrell  1  states that one of the major weaknesses of most nlp programs is their representation of meaning. each meaning of a word is usually represented by a node with an  awkward lexeme  as a label  whereas its meaning is best represented not as a 
　　　symbol  but as collection of variable valued microfeatures. 
　　　for a machine to  understand  language  it must have a means of  knowing  the context of a sentence and its relationship with the sentences that follow. building upon the work of waltz & pollack  1   we have defined and implemented a  locally-distributed  microfeature based model called mibs  microfeature based semantics   that uses a distributed short-term  stm  memory composed of microfeatures  to represent with more gradations of meaning  a sentence's underlying semantics. our system provides a means to dynamically determine the current context and adjust its relationship with the sentences that follow. the model developed here  provides a basis for understanding this dynamic relationship and thus  this work presents a starting point from which one can build a more dynamic natural language understanding system. note  waltz & pollack  1  used a static microfeature-based representation  that had to be initially primed by a user. in addition  their system could not process multiple sentences in a continuous fashion  as their microfeatures were not dynamically updated. 
our microfeatures act as intermediate-level units 
 minsky 1  that are intended to capture underlying structural fragments  that is  pieces of semantic structure. these intermediate-level units represent intermediate concepts which may be indispensable for understanding language  because the comprehension of a complex sentence often hinges on composing a meaningful variation on a familiar theme. although the microfeatures do not represent the relationships between the pieces of the semantic structure  they might provide a means for representing the underlying themes of the sentence and maybe an enriched notion of what the sentence  means . thus  the microfeatures may be useful pieces for a cognitive model of language understanding.  see bookman 
1 for details . 
　　　our use of microfeatures is similar to wilks'  1  use of semantic primitives  in the sense that they are only partially definitional. in addition  we are using the microfeatures as a short-term memory  where currently active microfeatures influence the sentences to follow. this is similar to hlnton's  1  notion where each active unit in the network represents a  microfeature . 
	bookman 	1 
h. description of t h e m o d e l 
　　　the system is composed of two layers. the top layer is a  local  connectionist model  the bottom layer  a distributed layer of  microfeatures   fig. 1 . the  local  connectionist layer encodes the syntactic and lexical structure of the sentence  see section 1 . while the microfeatures are used as a basis for defining the nodes  i.e. concepts  hypotheses  in the top layer  at least partially  and for associating each node with others that share its microfeatures. in addition they form the basis for our memory system. each microfeature is potentially connected to every node in the top layer. each node in the top layer is connected via bi-directional links to only those microfeatures that describe it. collections of closely related nodes in the top layer will have many common 

figure 1. system architecture 
given some initial setting it is the memory system 
 i.e.  the distributed layer of microfeatures  that drives the system to eventually settle on the intended interpretation of the given sentence. here memory is used in following limited sense: a short-term microfeature memory that stores the currently active and inactive microfeatures; and a long-term semantic memory that stores  semantic  knowledge in the form of connections between the microfeatures and concepts. this  semantic  knowledge is pre-wired into the network  and in the current implementation does not change.  note  there is no  episodic  memory  as the model currently has no way of storing events . for example  in figure 1 the local node speech has a mild association with the microfeatures threatening and safe  and a strong association with social and entertainment 
　　　here stm is represented by a microfeature vector  where each position of the vector corresponds to an independent microfeature  and the numerical value at that position corresponds to the level of activation of that feature. the initial activation of the microfeature vector primes local node concepts  and then the primed concepts change the activation of the microfeature values in stm  in turn activating new concepts  in this case the different word senses  in the top layer of the network. this is similar to quillian  1   except here successively activated  i.e. related   concepts  are joined by  sub bundles of microfeatures  rather than by single 
1 	natural language 
marker-passing links. the initial priming of memory can be set up by the experimenter  or can be automatically setup by the processing of a previous sentence. a major difference between this system and that of waltz & pollack  1  is that this system performs relaxation on the microfeature set. as a by-product we get constellations of microfeatures that persist over time  that can be used to dynamically constrain the processing of the sentences that follow. 
　　　the basic computational units at each node are punits with decay and p-units with conjunctive connections  for encoding semantic constraints . these units compute the potential  activation  of a node and are similar to the ones described in feldman & ballard  1 . in addition we are using meta-network structures called network regions for representing groups of competing word senses  chun  bookman & afshartous 1 . one such example is the cn-region in figure 1. these structures provide more stable competition  are more tolerant to initial noise  and eliminate the premature  lock-in  effect of traditional wta  winner-take-all  structures  feldman & ballard 1 . it is important to note that most of the computation here is local to each node and thus the relaxation algorithm can be performed by massively parallel hardware. currently  the computation is performed on a symbolics 1  using a general purpose massively parallel simulator called ainet-1  chun 1 . 
　　　the heart of the algorithm consists of two steps: node relaxation followed by microfeature relaxation. node relaxation computes the amount of activation a node is to receive from all its connecting neighbors  plus any activity from the currently active nodes of stm memory. the activity contributed by stm is computed by taking the product of a node's microfeature set  its  long-term semantic  knowledge  with the node's activation level  and  then taking the dot product of this result with the microfeature vector in stm memory. this result is then normalized  so that the relative contribution of the microfeature set is appropriately scaled. this computation allows the reactivation of previously active sets of nodes  and is similar in this regard to minsky's  k-lines   1   where some agent  in our case the node  is associated with specific activated nodes  in our case the microfeatures   and this agent is used to recover the whole from any sufficiently large part. 
　　　the microfeature relaxation cycle computes the amount of activation each node contributes to the microfeatures currently active in stm memory. this is accomplished by taking the product of a node's microfeature set times the node's activation level  computed by the node relaxation computation above  and updating memory by adding the sum of all such computations to the old values in memory. the net effect is that evidence is accumulated  with memory being modified by recent experience. 
         by dividing the sum of the reiult by the turn of the absolute values of the microfeature values in stm. 
both of the above computations are performed at each simulation step for all nodes in the network. 
　　　interestingly  the model proposed seems to correlate with some psychological processes  as evidenced by psycholinguistic data where subjects maintain the initial activation of all word senses and where a  post-access  decision takes place  following lexical access  that utilizes context to determine the appropriate meaning of the word  swinney 1 . in summary  the computation models a constraint satisfaction process that uses the semantics of microfeatures to set context. the microfeature relaxation computation can be viewed as the post-access decision process that performs word sense disambiguation. 
hi. t h e m i c r o f e a t u r e set 
　　　below is a description of the microfeatures used in this research. we have tried to be somewhat systematic and fair in choosing them. we make no claims  however  that this is the only such set. our purpose is only to demonstrate that microfeatures are useful. the features we chose can be broken down into the following categories: 
  lengths of events: second  minute  hour  day  week  month  year  decade. 
  temporal relationships between events: before  after  current. 
  locations: house  store  office  school  factory  casino  bar  restau-rant  theatre  racetrack  city street  city park  city  rural  forest  lake  desert  mountain  seashore  canyon. 
  events: competition  social  business  entertainment. 
  distinctions needed to survive: threatening/safe  animate/inanimate  edible/inedible  good outcome/neutral outcome/bad outcome  moving/still  intentional/unintentional  inside/outside  temporary/permanent. 
  life themes: sleep  hunger  thirst  sex  sickness  health  death  crime  subsistence  marriage  education  learning  profession  work  hobby. 
  methods of communication: speech  written  machine  telephone  satellite. 
  means of transportation: walk  bus  car  train  airplane. 
  object site: very large  large  medium  small  very small. 
  state change 
  goals: enjoyment goals  achievement goals  preservation goals  satisfaction goals.  see schank & riesbeck  1 . 
  some primitives to encode event-types: atrans  mtrans  ptrans  propel.  see schank & riesbeck  1 . 
the categories: locations  events  methods of communication  means of transportation are important to a culture and were chosen on this basis. the categories: life themes  distinctions needed to survive  state change  length of events  and temporal relationships are common across cultures and were chosen for this reason. the size category te context dependent and provides a means for relational comparison. goals and the schank-inspired primitives were chosen as a basis for encoding  scriptlike  knowledge. there are roughly 1 microfeatures; however  probably several thousand are needed for a sufficiently rich semantics. 
iv. some e x p e r i m e n t a l results 
　　　in the following examples we will illustrate the utility of the approach in determining the ''meanings  of the sentences: 
 1  john went to mary's party. he had a good time. 
 1  john ran the 1 meters yesterday. he had a good time. 
 1  john was talking to his boss. the language he used was inappropriate. 
 1  john was programming at his computer. the language he used was inappropriate. 
the networks that model these sentences are shown in figures 1 and 1  actually only the local connectionist layer is shown . the networks for sentences s1 and s1 are not shown as they are similar to the ones for si and s1 respectively. the rectangular nodes in the figure represent the input words. the elliptical nodes below them represent the competing word senses  and the structure above them the syntactic parse tree for the sentence. 

figure 1. network:  john went to mary's party. he had a good time   after 1 cycles of relaxation. 

figure 1. network:  john was talking to his boss. the language he-used* was inappropriate   after 1 cycles of relaxation. 
automatic context setting 
in the first simulation  the two sentences  si : 
 john went to mary's party. he had a good time.  were processed. note  the meaning of the second sentence is ambiguous. in the context of party it means he had a good experience  but in the context of track meet  it 
　　　　note: we are using he used as as adjective to simplify tht parse tree. the correct parse could be represented as follows: np  det  the in n|   g   comp  |g he 
used is nn 
	bookman 	1 
means the measured time for the race he has just run was quite good. in this example we have used four senses of the input word  time : 
rhythm - a noun  meaning the grouping of the beats of music. 
duration - a noun  meaning the measured time for an event. 
experience- a noun  meaning a person's experience during a specified period. 
schedule - a verb  meaning to arrange or set the time of. 
the purpose of this experiment was to demonstrate the resolution of the ambiguity of the second sentence  through the normal processing of the first sentence  but without any context being initially primed by an experimenter. instead  this was to occur automatically  and dynamically through the constellations of microfeatures. after 1 cycles of processing  the system settled into the correct stable state  with the experience sense of time winning out  see fig. 1 . note we have sequenced the processing so that we first process the first sentence and then we process the second. if we process only the second sentence then the system will settle into an ambiguous state in which the two senses of time: duration and experience are both equally active. this demonstrates that the second sentence is really ambiguous to the system. 
　　　it is interesting to note that if the sentences were input in the reverse order  i.e. he had a good time. john went to mary's party   the system still resolves the ambiguity. however  in this case it took the system 1 cycles of processing before it settled into the correct stable state. the increase in time correlates .with our own difficulty in trying to understand an ambiguous sentence that is out of context - it takes us a little longer also. 
in the second simulation the two sentences  s1 : 
 john ran the 1 meters yesterday. he had a good time.  were processed. in this experiment we wanted to see if the system could correctly disambiguate the word time given the context of a different first sentence. after 1 cycles of relaxation  the system settled into the correct state  with the duration sense of time winning out. a plot  not shown  of the activation of the different word senses of time shows that the incorrect senses: rhythm  experience  and schedule quickly die out. as before  we have sequenced the processing so that we first process the first sentence and then we process the second sentence. 
　　　the sentences  s1  and  s1  were also tried and similar results were obtained. in either case the system was able to resolve the ambiguity of the second sentence  namely  john's manner of speech was inappropriate or the programming language he used was inappropriate. figure 1 depicts the final state after having processed sentence s1. the processing of sentence s1 is similar  except here the prog-lang sense of language wins out. 
v. summary 
　　　this paper has presented some preliminary results toward understanding the  meaning  of sequences of 
1 	natural language 
sentences. its basic conclusions are that the semantics of microfeatures can be used to automatically and dynamically perform context setting as a sentence is being processed  helping to disambiguate the different word senses in the sentences that follow; and that the underlying sense of a sentence can be realized through the use of a distributed shared set of microfeatures. for a machine to  understand  language  it must have a means of  knowing  what the context is and what is its relationship with the sentences that follow. the model developed here provides a basis for understanding this dynamic relationship and thus  this work presents a starting point from which one can build a dynamic natural language understanding system. in addition  the approach presented allows for massively parallel networks to be used as the basis for implementing this mechanism. 
acknowledgements. special thanks to dave waltz for suggesting many improvements to earlier drafts. also special thanks to hon wai chun for many discussions and comments on this paper  and james pustejovsky for providing useful comments. 
