 
this paper proposes the frequency modulation neural network as an alternative to current neuralnet models. this proposal is for an architecture of a heterogeneous neural-network in which information is propagated using frequency modulation of pulses oscillated by groups of neurons. the fmnn model enables operations including variable-binding  sequential recognitions and predictions. the use of fm signals for communication among neural clusters also enables the model to avoid communication bottlenecks arising in most massively parallel computer architectures. 
1 	introduction 
one weakness of the traditional neural network architecture is commonly known as the binding problem. another weakness are difficulties in predicting and recognizing sequential patterns such as word sequences. due to these problems in the traditional neural network models  application of the model to any serious natural language processing has not been conducted. in the frequency modulation neural network  fmnn 1  one solution to the binding problem is attained through propagation of the activation source information through the frequency modulation of pulses generated by certain groups of neurons. combined with local modular circuits which enable sequential prediction and activation  our model attains structured marker passing from the neural level. first  we briefly summarize the marker-passing and constraint propagation schemes which we assume as our basis of natural language and inferential processing and describe what needs to be attained by the neural network. then we describe some features of the fmnn architecture relevant to such tasks. 
   *also with the laboratory for computational linguistics  carnegie mellon university. 
1
also with nec corporation. 
     1  also see  kitano and tomabechi  ms  for mathematical details and the descriptions of some of the circuits not included in this paper. 
1 	parallel and distributed processing 
1 marker-passing and constraint propagation 
1 marker-passing models 
the spreading-activation marker-passing based models of cognitive processing reflect the highly interactive and parallel nature of human cognitive activities1. one of the ma-
jor differences between the marker-passing based models and the neural network models is that the marker-passing scheme allows some tokens  typically the sources of activation  to be passed with the activation propagations. the direct memory access  dma  model iriesbeck and martin  1   tomabechi  1  is a marker-passing scheme with case-based inference capability. in the dma model  the spreading-activation of activation source tokens enables the models to instantiate generalized memory structures to capture the specific meaning of the input utterance. the tokenized activation source propagation can be seen as one way of solving the binding problem. recently  researchers at carnegie mellon have adopted the dma paradigm for various natural language and inference tasks.  tomabechi and tomita  1a  is a dma natural language interface and inferences for development of knowledge-based systems and in  tomabechi and tomita  1b   dma based contextual inferencing was integrated into a unification-based  lfg  
 kaplan and bresnan  1   phoneme-parser   saito and tomita  1   as a part of real-time speaker-independent speech-to-speech translation system   tomabechi et. al.  1a  . independently  a basic dma algorithm has been implemented on vlsi chips  kitano  1 . also  the first dma based speech-to-speech translation has been publicly demonstrated at carnegie mellon1   tomabechi et. al.  1b    kitano et. al.  1b  . 
1 constraint propagation 
the massively-parallel constraint propagation  mcp   tomabechi and tomita  ms.   scheme extends the dma philosophy and proposes the models to propagate constraints as well as other information such as the tokens for the sources 
1
　　such as  quillian  1    charniak  1    hirst  1    riesbeck and martin  1    charniak  1    norvig  1    tomabechi  1   and  hendler  1 . 
1
　　other efforts include the application of dma to phonemebased parsing  tomabechi et. al.  1   discourse-based processing  kitano et. al.  ms.   cost-based ambiguity resolution  kitano et. al.  1a  and concurrent parsing and generation  kitano  1 . 

of activations. the head-driven massively-parallel constraint propagation  hmcp   tomabechi and levin  ms.  proposes a scheme of propagating head-feature constraints to increase dma's capacity for handling syntactic phenomena such as word order  agreement  case marking  inflectional and expletive morphology  control  and unbounded dependencies. in the mcp paradigm  words  or some smaller linguistic units  in the input string trigger the propagation of constraint information through the network. the propagated information includes the source of the activation  syntactic head-features  etc.. concepts that represent heads of phrases contain bundles of syntactic features which constrain their complements. when concept activations of complements collide with activations of heads  the syntactic features of the complement and head are unified with each other. 
below are the node entries taken from  tomabechi and 
levin  ms.  representing1 the lexical concepts for the verbs give and try: 
 lex-node 	*give 
	  i s - a 	 *action   
 phonology  /g/ / i / /v/   
 syn-head-feature   maj v   v form fin   aux 
minus    
	 giver 	 *person 1   
	 receiver 	 *person 1   
	 given 	 *object 1   
	 subcat 	  np nom  	1   	 np acc  1     	 np acc  
1      
 lex-node *try 
	 is-a 	 *action   
	 phonology   / t / 	/ r / 	/ a / 	/ i /     
	 syn-head-feature   maj v  	 vform fin  	 aux 
minus    
  t r i e r  * person 1   
	 circumstance 	 *action 1   
	 subcat   nplnom  	1   	    maj v  
　 vform inf   subcat  np 1    
1      
the list  np nom  1  in the subcat feature is a short-hand for    maj n   case nom   1 . 
　the list in subcat represents the postulation for the constraints that are to be satisfied by the nodes that will fill the conceptual roles  that are co-numbered . the contents of the syn-head-feature are the head-features that are propagated with the specific node activations along with other information. in the entry for try we can see that subcat specifies that it subcategorizes for an unsaturated verb phrase as an instance to fill the circumstance role  whose abstraction is the concept * action . an input string such as he can trigger the propagation of information such as np nom  as well as the semantic propagation through the abstraction and other links. the hmcp models demonstrate that constraint propagation schemes can handle sentential constructions such as control  as in he tried to give mary the book  assigning the correct interpretations and attaining the correct grammaticality judgement 
　　1 the lexical entries are originally taken from the representation under hpsg  head-driven phrase structure grammar   pollard and sag  1  . hmcp attains the hpsg unification based analysis under the dma framework. 
1 msp vs conventional architecture 
although the dma models and the mcp  hmcp  mod-
els are appealing schemes for natural language processing and inference  because these models require propagation of information  i.e.  sources of activation and constraint information   it has been difficult to support them in the conventional neural-net architecture. in this paper  we refer to the structured marker-passing algorithms that the dma and mcp models assume in their implementation as 'massivelyparallel structured-marker passing*  msp  algorithms. because the msp algorithms assume a massively-parallel machine architecture  the implementations  such as on multilisp   halstead  1  have been generally slow. the conventional massively-parallel machines  such as connection machine   hillis  1   are not desirable for msp algorithms because they are incapable of propagating information without large overhead. in other words  the massivelyparallel machine architecture  which assumes the propagation of small amounts of scalar values is not suitable for msp algorithms. 
1 frequency modulation neural network 
we would like to introduce a model of neural network and a possible hardware architecture which directly support the msp paradigm. we call this model the frequency modulation neural network  fmnn  and the major features which distinguish fmnn from other neural network are as follows:   simulates spikes instead of activation levels. 
  assumes neural clusters with certain circuit topologies: heterogeneous neural network. 
  patterns of spikes  modulated as fm signals  have sig-nificance in propagating information. 
1 	simulating spikes 
unlike most neural networks which simulate the activation level of each node  the fmnn simulates the spikes which each neuron emits. spikes are electric pulses emitted from neurons to propagate through axons. we are proposing to simulate the neural spikes because  1  we are interested in simulating a detailed biological process  and  1  patterns of pulses can carry information impossible to transmit in past neural network architecture. biologically  we are not able to define the precise roles of spikes in the human brain with regard to cognitive activities. it is also probably true that mere simulation of spikes will not contribute much to our understanding of the physiological nature of the brain  because other factors  such as chemical substances  neurogenesis  and plasticity of the brain  are also involved. however  we expect that spike simulation may lead to the discovery of new computational features of the brain1. apart from biological observations  the use of spikes in the neural network makes the flow of structured information attainable. 
     1 in fact   loeb  1  has shown that differences between arrival times down to 1 microseconds can be detected by neurons leading to  scjonwski  1 's analysis that submillisecond timing information could be important in the cerebral cortex as well. 
	tomabechi and kitano 	1 
1 	heterogeneous neural network 
the second assumption of the fmnn is the heterogeneous structure. we assume the existence of several types of groups of neurons and certain local and global structures in the network. most neural networks assume homogeneity. although these models employ certain global structures  such as layers composed of input  hidden  and output units  rumelhart et al.  1  or a recurrent structure  elman  1   each node is assumed to have homogeneous characteristics. in addition  the existence of a local structure is not assumed. motivations for assuming heterogeneous structure are based on both biological and technical considerations. biological observations on neurogenesis  murphy et. al.  
1   nottebohm  1   the existence of local structure of neurons  rosen et. al.  1   and the existence of the specific global innate configurations for the specific tasks such as the papez circuit involving the hippocampus  papez  1  indicate that neural circuits in our brain have certain innate structures already encoded in the dna. in addition  strong evidence of hypercolumn in the a1 field of visual cortex  models of the hippocampus  zipser  1  and studies on the topological circuits of neurons  von der malsburg  1  lead us to assume a structured functional module which we call hypermodule. we claim such local circuits provide technical benefits because they will allow us to predefine highly functional neural circuits without abandoning the basic features of the neural network. 
1 	frequency modulation 
we use frequency modulation  fm  signals as one of the basic communication mechanisms in our model. when we look into the physiological data of signal transmission between neurons  frequency of pulses varies depending on the strength of the input given to the neuron.  treutlein and schulten  1  analyzed the fokker-plank equation corresponding to the stochastic bonhocffer-van der pol  bvp  model  and concluded that the noise level can be employed to tune firing frequency of hodgkin-huxley type neurons. in their analysis  the noisy bvp model tends to limit cycle in which the mean period of pulses is depending upon the noise level. this means that the more energy the cell gets  the higher the frequency of impulses it emits1. this property has been known since  adrian  1 ; however  it has been neglected by the past neural network models. moreover  there exist neurons whose activation levels vary dependent on the regular brain wave  1 -cells associated with the hippocampal 1-wave. . also  the importance of arrival time of impulses and relative timing of impulses for processing in the cortex has been noted  such as  sejonwski  1  and  sachs et. al.  1  ; however  no neural network model so far has captured this phenomenon1. this characteristic is well simulated when we consider fm as its approximation. fm is a kind of an angle modulation technique originally developed for telecommunication of signals  marubayashi  1 . a signal is encoded into a carrier wave  acos wc1 + 1c   by modulating its phase angle 

since we are going to deal with impulses instead of analog waves  instantaneous frequency is a probability density of the impulse. apart from physiological aspects  the use of fm signals as representation and communication scheme of a massively parallel computer would provide significant advantages over traditional computer architecture. 
1 modular neural circuits in the fmnn 
in this section  we describe some of the circuit topologies of modular neural circuits and their behavior. we will be discussing the circuits which are of particular relevance from the viewpoint of inferencing and natural language processing. relevant work has been done extensively by  amari  1 . a basis of our analysis also assumes lamari  1   although we made some reformulation in order to simulate spikes . 
1 	simple random circuit 
a simple random circuit is a neural circuit in which each neuron is connected to the others at random i amari  1 . the external behavior of this circuit is almost equivalent to the mcculloch-pitt model assumed in most neural network theories  mcculloch and pitts  1 . this circuit is also the simplest form of a bi-stable circuit. a simple random circuit stays at the low level equilibrium until input exceeding the threshold comes in. in this case  the circuit will transit to the high level equilibrium. as the signal goes off  the system will go back to the low level equilibrium. the other way to attain a simple bistable circuit is by connecting two excitory neural groups and one inhibitory group. an implication of such a circuit is that this neural complex can act as a node which is commonly assumed in connectionist  such as  waltz and pollack  1  and  bookman  1   literature. the circuit is a threshold device and it holds its activation for a certain period of time. a sequence of pulses emitted from such a circuit is random and has no significant meaning as a carrier of information. 
1 	induced excitory oscillator 
induced excitory oscillators  ieos  are types of circuits that generate pulses of a certain frequency whenever a certain external stimulus is given. this is a kind of bi-stable circuit in which one of the attractors is a periodic attractor1. an oscillator unit of neurons generates a group of pulses of a certain frequency. each unit has its own frequency called its 'characteristic frequency' or 'eigen frequency'. small 

circuits consist of a few neurons and with a large number of such circuits with a phased array configuration  output spikes form a simple sequence of pulses with a specific interval. moreover  a circuit with a group of neurons that outputs a sequence of pulses with a specific modulation is also possible. such circuits tend to limit a cycle. a spontaneous frequency or a probability density of the pulses pi  follows: 
in the modular neural circuits  we only require ieos to satisfy a weak form of the poincare-bendixson-theorem  beltrami  1 . the theorem indicates that for the system x = f x  x in r1  an orbit 1 tends to a limit cycle t if 
   = r and r is a non-constant closed orbit. 1+ is the positive limiting set which is a set of points p in r1 for which 

our neural circuit is stable if there is a cycle 1 which is asymptotically stable. this would require the existence of an open neighborhood Ω such that every orbit starting from i o has 1 as its positive limiting set. this is known as a weak form of the poincare-bendixson-theorem  the circuit that entails limit cycles that satisfy this theorem is useful since it continuously emits characteristic frequency  until the energy is dispersed or inhibition is imposed  which can identify the source of activation. 
1 	resonance circuit 
resonance circuits react only to a sequence of pulses whose frequency matches the characteristic frequency of the circuit. such circuits can be built by combining a series of nodes forming a loop and some inhibitory connections to the input node. when a pulse comes in  the input node gets activated and emits an impulse to the next node in the loop. that node emits an impulse to the next node in turn. at the same time  the node emits an impulse to the inhibitory node in which the inhibitory link is connected to the input node. thus  the input node can be activated until the inhibition is removed. by adjusting the size of loops and inhibitions  the characteristic frequency of the circuit can be adjusted. 
1 	sequential prediction and activation circuit 
the capability to handle natural language requires the system to recognize sequential patterns. this type of nodes is assumed in the dma type connectionist  or pseudoconnectionist  architecture  such as  riesbeck and martin  1  and  tomabechi  1    in which time-sequenced activation of nodes is assumed to model subcategorization of syntactic units  episodic knowledge invocations  etc. in our model  sequential pattern recognition and predictions are attained by assuming a scries of interconnected multi-stable modular circuits. as one example of such circuits  we can assume a complex of modular nodes each having inactive  prediction and recognition states. when input signals are less than a certain level  l1   the circuit stays inactive. once the input strength exceeds that threshold  the excitation level goes up to the pre-excitory level. however  if the input strength is below the excitation threshold  l1   the excitation level stays at prediction level for a while and goes back to an inactive state quickly. the circuit jumps up to a recognition state when the input signals are more than l1. a recognition state is a stable state  but returns to an inactive state as a result of dispersion of the energy. inter-module connections are created so as to transfer sufficient impulses to the next module so that the next module can be activated to the prediction state whenever the current module gets to the recognition state. such network configuration has not been assumed in the traditional neural network. however  we assume the existence of such local circuit topologies in the neural network. the excitation level of the 1-th multi-stable node can be described by: 
1 
where denotes the excitation level of the 1-th node  external stimulus to the 1-th node  and excitation level of the i-th element of the 1-th node. ienotes a coefficient  weights  and threshold  respectively. the activation level of the node that packages the series of sequential nodes is: 
one interesting feature of this circuit is that it can handle the transposition of activation sequences by utilizing the cost of excitation propagation  instead of not recognizing the incorrectly ordered sequence at all . to be more specific  a correct input activation sequence will excite ecsc node at a normal level  say 1. however  an incorrect ordering can exist at a lower level  say 1. this is because an incorrectly ordered sequence will consume more energy than the correctly ordered one due to the pre-excitation mechanism which carries the next module to the prediction state. one of the reasons that we use groups of neurons instead of asymmetrically connected single neurons is thay they can be combined with resonance circuits to handle context-free rules with some syntactic constraints which are implicitly encoded in form of the modulation frequency. for example  we can implicitly encode the obliqueness order of hpsg  pollard and sag  1. 
1 	knowledge representation in fmnn 
a complex knowledge structure can be built by modulating carrier waves by a modulation signal. suppose that a certain group of neurons is representing the cluster for some instance of an already known person such as john  and this cluster of neurons has the eigen-frequency of 1 hz. now suppose some activations have triggered the syntactic realization that an input noun phrase that activated the clusters for john concurrently activated another group of neurons that is representing the feature case nominative whose characteristic frequency was 1 hz. while propagation for the activations of these clusters are performed  the carrier frequency representing case nominative can be modulated by a signal representing john. such a scheme for modulation is nothing new to the current communication technology as has already been well accepted and performed by fm radios  tvs and other frequency modulated methods of communication. intuitively  our concept of variable binding is similar to fm broadcasting in which carrier frequency of 1mhz 
	tomabechi and kitano 	1 


 fm-tokyo  carries the music of the boston symphony orchestra  bso  as a modulation signal  value to be bound . in this case  the music of bso is bound to fm-tokyo. a complex structure as seen in figure-1  which partially captures the subcat feature for give as seen previously  can be built by modulating signals. 
　we assume that one group of neurons is oscillating at a certain frequency. a series of pulses are emitted toward a 
　group of neurons which eventually emit frequency modulated pulses. a modulation signal is given from the group of neurons which oscillate in the frequency representing the value to be bound to the variable. the resulting pulse is modulated based on the frequency of the modulation signal. in case of figure-1  the data structure would be represented in the following equations: 

1 	fmnn machine 
building an fmnn machine should be the best way to take advantage of this architecture. it is analogous to the motivation for building the connection machine   hillis  1   for implementing the connectionist network. in this section  we discuss some of the possible implementation strategies and outline relevant technologies. 
1 	parallel and distributed processing 
1 processors 
a straightforward way of implementing the fmnn model is to build a vlsi neural network chip with heterogeneous connections and spike generation capabilities as described in this paper. one other approach is to build components that are equivalent to each local neural circuit in their functions. such components would include energy-spike converters  oscillators  resonators  etc.. several technologies are available to actually implement such functional modules. analog circuit technology is a well-established technology that can attain functionalities of intended modules. especially  recent studies on analog vlsi  ryckebusch et. al.  1  may provide a hardware basis from the vlsi level. digital signal processing  dsp  technology is one other possibility which can be more compatible with current computer architecture. one advantage of using dsp technology is that the filtering and control of signals can be more precise than with analog technology. also  it is be possible to implement fmnn using current computer technology. however  it would be more likely to be a simulation of fmnn rather than a direct hardware implementation. figure-1 is a summary of differences of the fmnn model and other models of computation. 
1 	communication 
in the fmnn machine  communication between processor units is performed by sending sequences of pulses. the conventional approach of communicating pulses between processors is using bus  n-cube  and a network which has problems such as load bottlenecks  routing  packet collision  etc.. a new approach which we are proposing is to send them using carrier waves. the pulses are carried on a specific carrier frequency possessed by each generator. resonators in each node can tune to the desired carrier wave in order to establish appropriate connections. this communication method enables us to send many independent pulse sequences at a 
time; thus enabling us to simulate a very large-scale neuralnetwork  vlnn  in which a massive amount of information is transmiued throughout the network. as briefly described earler  an fm radio is a good analogy to our approach  where the signal is sent by a certain carrier wave and an audio signal is modulated by frequency. to send a huge amount of impulses  communication media need to have a large capacity to communicate on many independent channels. recent advancements in communication technologies including optical wavelength division multiplexing or optical frequency division multiplexing may provide such capabilities. 
1 discussion: beyond pdp 
1 bridging the gap between neural network and cognitive processing 
the fundamental premise of connectionism has been that individual neurons do not transmit large amounts of symbolic information   feldman and ballard  1  . we are extending this claim to add a phrase that  however  a group of neurons is capable of oscillating pulses to carry information. as a basis of this extension  we are assuming the existence of local functional circuits which we refer to as hypermodules. the impact of this addition leads to the significant enhancement of the neural network because;  1  we now have a 

heterogeneous network with highly functional local circuits of neurons that are oscillating  receiving and modulating signals   1  the fm signals propagated in the network can contain information such as which oscillating node initiated the propagation and what constraints to inhibit/delay/impose and slow/accelerate certain propagation. especially the capacity to propagate information results in the capacity to perform operations such as variable-binding  by specifying where the oscillation was initiated  and structure-building  modulating the oscillations on top of receiving frequencies and utilizing the variable-binding capacity . this is significant for cognitive research which requires propagation of information and constraints. indeed the network is capable of supporting the massively-parallel structured-marker passing  msp  from the hardware level. as discussed in detail in  tomabechi and tomita  ms.   the msp network is a model that is capable of performing inferencing and natural language processing that is not currently possible with the conventional connectionist model due to its lack of capacity to propagate information  which is done in msp as a passing of structured-markers . also as reported in  kitano and tomabechi  ms.   we can build circuits of neurons within the fmnn architecture to capture nodes that respond to certain features of sensory input such as sound-wave length and light-wave length1 . representation of knowledge in our model is very different from other existing models. especially  our model does not assume a static representation of knowledge as it has been the case in other models. we are assuming constant information flow in the brain  as opposed to the specific portion of the network statically representing some knowledge  and the knowledge is implicitly represented as time-varying signals. 
1 physiological relevance 
from the physiological point of view  recent studies of neurophysiology including the one by eckhorn's group  eckhom et. al.  1  provide some biological relevance to our model. they report stimulus dependencies of oscillatory responses1. moreover  coherence of se-resonance was found within a vertical cortex column  between neighbouring hypercolumns  and between two different cortical areas. they claim that  se-resonance can be phase-locked within half an oscillation cycle up to a distance of 1mm at 1hz.  and conclude that coherent se-resonances arc likely to con-
　　such a property of neurons are known to exist   crick and asanuma  1    zcki  1    michael  1    but has not been captured in the past neural network models. 
　　1   the type of visual stimulation was found to influence the frequency of the dominant spectral peak of oscillatory responses.  and  the mean frequency of the spectral peak increased by 1hz if the stimulus drift velocity was doubled.  
stitute a second higher stage of sensory coding. they further claim that  we are convinced that se-resonances are a general phenomenon  forming the basis of a correlation code which is used within and between different sensory systems and perhaps even throughout the entire brain.  these eckhorn's findings are consistent with our speculations on the basis of the fmnn model which assumes the significance of resonance frequency created by a group of neurons. 
1 conclusion 
we have proposed an fmnn architecture which is a neural network with a heterogeneous composition and with a capacity to attain variable binding and structure building using the frequency modulation of pulses by groups of neurons. assumption of hypermodule allows us to assume modular circuits with sequence prediction and recognition capability. cognitive processing using the msp models can be attained by combining hypermodules. in our model  cognitive activities are performed through the modulation and propagation of pulses in a neural network  whereas the conventional neural network models have been based on the propagation of scalar values. although the formulation of the details of the formal character of the fmnn architecture is yet to be completed  the introduction of the hypothesis for frequency modulation of neural activation pulses is a significant enhancement to the conventional neural network architecture. three assumptions we introduced in our model  i.e.  spike simulation  a heterogeneous neural network  and frequency modulation  provide our model with an information flow and local functional capability which conventional neural networks have not attained. linkage of distant neural circuit through oscillatory signals is one of the interesting features postulated in our model. this is also interesting from the viewpoint of neuro-physiology supported by the discovery of coherent oscillations  eckhorn et. al.  1 . we have also proposed that it is possible to build an fmnn machine and that an fmnn machine would be able to perform tasks such as inferencing and natural language processing that are run under a simulated massive-parallelism in the massively-parallel structured-marker passing  msp  models at the hardware level. 
acknowledgements 
the authors wish to thank jaime carbonell  masaru tomita  hitoshi iida  james mcclelland and david touretzky. we would also like to thank other members of the carnegie mellon community for fruitful discussions. margalit zabludowski was especially helpful in preparing the final version of this paper. 
	tomabechi and kitano 	1 

