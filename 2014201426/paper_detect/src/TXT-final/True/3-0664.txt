
statistical relational learning  srl  constructs probabilistic models from relational databases. a key capability of srl is the learning of arcs  in the bayes net sense  connecting entries in different rows of a relational table  or in different tables. nevertheless  srl approaches currently are constrained to use the existing database schema. for many database applications  users find it profitable to define alternative  views  of the database  in effect defining new fields or tables. such new fields or tables can also be highly useful in learning. we provide srl with the capability of learning new views.
1 introduction
statistical relational learning  srl  focuses on algorithms for learning statistical models from relational databases. srl advances beyond bayesian network learning and related techniques by handling domains with multiple tables  representing relationships between different rows of the same table  and integrating data from several distinct databases. srl techniques currently can learn joint probability distributions over the fields of a relational database with multiple tables. nevertheless  they are constrained to use only the tables and fields already in the database  without modification. many human users of relational databases find it beneficial to define alternative views of a database-further fields or tables that can be computed from existing ones. this paper shows that srl algorithms also can benefit from the ability to define new views  which can be used for more accurate prediction of important fields in the original database.
　we will augment srl algorithms by adding the ability to learn new fields  intensionally defined in terms of existing fields and intensional background knowledge. in database terminology  these new fields constitute a learned view of the database. we use inductive logic programming  ilp  to learn rules which intensionally define the new fields.
　we test the approach in the specific application of creating an expert system in mammography. we chose this application for a number of reasons. first  it is an important practical application with sizable data. second  we have access

	figure 1: expert bayes net	 
to an expert developed system. this provides a base reference against which we can evaluate our work. third  a large proportion of examples are negative. this distribution skew is often found in multi-relational applications. last  our data consists of a single table. this allows us to compare our techniques against standard propositional learning. in this case  it is sufficient for view learning to extend an existing table with new fields. it should be clear that for other applications the approach can yield additional tables.
1 view learning for mammography
offering breast cancer screening to the ever-increasing number of women over age 1 represents a great challenge. costeffective delivery of mammography screening depends on a consistent balanceof high sensitivity and high specificity. recent articles demonstrate that subspecialist  expert mammographers achieve this balance and perform significantly better than general radiologists  1; 1 . general radiologists have higher false positive rates and hence biopsy rates  diminishing the positive predictive value for mammography  1; 1 . despite the fact that specially trained mammographers detect breast cancer more accurately  there is a longstanding shortage of these individuals .
　an expert system in mammography has the potential to help the general radiologist approach the effectiveness of a subspecialty expert  thereby minimizing both false negative and false positive results.
　bayesian networks are probabilistic graphical models that have been applied to the task of breast cancer diagnosis from mammography data  1; 1; 1 . bayesian networks produce diagnoses with probabilitiesattached. because of their graphical nature  they are comprehensible to humans and useful for training. as an example  figure 1 shows the structure of a bayesian network developed by a subspecialist  expert mammographer. for each variable  node  in the graph  the bayes net has a conditional probability table giving the probability distribution over the values that variable can take for each possible setting of its parents. the bayesian network in figure 1 achieves accuracies higher than that of other systems and of general radiologists who perform mammograms  and commensurate with the performance of radiologists who specialize in mammography .
　figure 1 shows the main table  with some fields omitted for brevity  in a large relational database of mammography abnormalities. data was collected using the national mammography database  nmd  standard established by the american college of radiology. the nmd was designed to standardize data collection for mammography practices in the united states and is widely used for quality assurance. figure 1 also presents a hierarchy of the types of learning that might be used for this task. level 1 and level 1 are standard types of bayesian network learning. level 1 is simply learning the parameters for an expert-supplied network structure. level 1 involves learning the actual structure of the network in addition to its parameters.
　notice that to predict the probability of malignancy of an abnormality  the bayes net uses only the record for that abnormality. nevertheless  data in other rows of the table may also be relevant: radiologists may also consider other abnormalities on the same mammogramor previousmammograms. for example  it may be useful to know that the same mammogram also contains another abnormality  with a particular size and shape; or that the same person had a previous mammogram with certain characteristics. incorporating data from other rows in the table is not possible with existing bayesian network learning algorithms and requires statistical relational learning  srl  techniques  such as probabilistic relational models . level 1 in figure 1 shows the state-of-the-art in srl techniques  illustrating how relevant fields from other rows  or other tables  can be incorporated in the network  using aggregation if necessary. rather than using only the size of the abnormality under consideration  the new aggregate field allows the bayes net to also consider the average size of all abnormalities found in the mammogram.
　presently  srl is limited to using the original view of the database  that is  the original tables and fields  possibly with aggregation. despite the utility of aggregation  simply considering only the existing fields may be insufficient for accurate predictionof malignancies. level 1 in figure 1 shows the key capability that will be introducedand evaluated in this paper: using techniques from rule learning to learn a new view. the new view includes two new features utilized by the bayes net that cannot be defined simply by aggregation of existing features. the new features are defined by two learned rules that capture  hidden  concepts central to accurately predicting malignancy  but that are not explicit in the given database tables. one learned rule states that a change in the shape of the abnormality at a location since an earlier mammogram may be indicative of a malignancy. the other says that an increase in the average of the sizes of the abnormalities may be indicative of malignancy. note that both rules require reference to other rows in the table for the given patient  as well as intensional backgroundknowledge to define concepts such as  increases over time.  neither rule can be captured by standard aggregation of existing fields.
1 view learning framework
one can imagine a variety of approaches to perform view learning. our closing section discusses a number of alternatives  including performing view learning and structure learning at the same time  in the same search. for the presentwork  we apply existing technology in a new fashion to obtain a view learning capability.
　any relational database can be naturally and simply represented in a subset of first-order logic . inductive logic programming  ilp  provides algorithms to learn rules  also expressed in logic  from such relational data   possibly together with background knowledge expressed as a logic program. ilp systems operate by searching a space of possible logical rules  looking for rules that score well according to some measure of fit to the data. we use ilp to learn rules to predict whether an abnormality is malignant. we treat each rule as an additional binary feature; true if the body  or condition  of the rule is satisfied  and otherwise false. we then run the bayesian network structure learning algorithm  allowing it to use these new features in addition to the original features. below is a simple rule  covering 1 positive examples and 1 negative examples:
abnormality a in mammogram m may be malignant if:
a's tissue is not asymmetric 
m contains another abnormality a1  a1's margins are spiculated  and a1 has no architectural distortion.
　this rule can now be used as a field in a new view of the database  and consequently as a new feature in the bayesian network. the last two lines of the rule refer to other rows of the relational table for abnormalities in the database. hence this rule encodes information not available to the current version of the bayesian network.
1 experiments
the purposes of the experiments we conducted are two-fold. first  we want to determine if using srl yields an improvement compared to propositional learning. secondly  we want to evaluate whether using inductive logic programming  ilp  to create features  which embody a new  view  of the database  adds a benefit over current srl algorithms. we looked at adding two types of relational attributes  aggregate features and horn-clause  ilp  rules. aggregate features represent summaries of abnormalities found either in a particular mammogram or for a particular patient. we performed
patient   abnormality     date  mass shape  ...   mass size    location   benign/malign  p1             1            1       spic                1       ru1            b  p1             1            1       var                 1       ru1            m  p1             1            1       spic                1       ll1             b  ...            ...             ...        ...                      ...          ...             ...            parameter learning: 
given: features  node labels  or fields in database   data  bayes net structure learn: probabilities.  note: 
probabilities needed are pr be/mal   pr shape|be/mal   pr  size|be/mal  structure learning: 
given: features  data                learn: bayes net structure and probabilities.  note: with this structure  now will need pr size|shape be/mal  instead of pr size|be/mal .  aggregate learning: 
given: features  data  background knowledge - aggregation functions such as average  mode  max  etc.           learn: useful aggregate features  bayes net structure that uses these features  and probabilities.  new features may use other rows/tables. view learning: 
given: features  data  background knowledge - aggregation functions and intensionally-defined relations such as  increase  or  same location  learn: useful new features defined by views  equivalent to rules or sql queries   bayes net structure  and probabilities.      	  
figure 1: hierarchy of learning types. levels 1 and 1 are available through ordinary bayesian network learning algorithms  level 1 is available only through state-of-the-art srl techniques  and level 1 is described in this paper.a series of experiments  aimed at discovering if moving up in the hierarchy outlined in figure 1 would improve performance. first  we tried to learn a structure with just the original attributes which performed better than the expert structure. this corresponds to level 1 learning. next  we added aggregate features to our network. finally  we created new features using ilp. we investigated adding the features proposed by the ilp system as well as the aggregate to the network.
　we experimented with a number of structure learning algorithms for the bayesian networks  including na： ve bayes  tree augmented na： ve bayes   and the sparse candidate algorithm . however  we obtained best results with the tan algorithm in all experiments  so we will focus our discussion on tan. in a tan network  each attribute can have at most one other parent in addition to the class variable. the tan model can be constructed in polynomial time with a guarantee that the model maximizes the log
likelihood of the network structure given the dataset  1;
1 .
1 methodology
the dataset contains 1 malignant abnormalities and 1 benign abnormalities. to evaluate and compare these approaches  we used stratified 1-fold cross-validation. we randomly divided the abnormalities into 1 roughly equal-sized sets  each with approximately one-tenth of the malignant abnormalities and one-tenth of the benign abnormalities. when evaluating just the structure learning and aggregation  1 folds were used for the training set. when performing aggregation  we used binning to discretize the created features. we took care to only use the examples in the train set to determine the cut bin widths. when performing  view learning   we had two steps in the learning process. in the first part  1 folds of data were used to learn the ilp rules. afterwards  the remaining 1 folds were used to learn the bayes net structure and parameters.
　when using cross-validation on a relational database  there exists one major methodological pitfall. some of the cases may be related. for example  we may have multiple abnormalities for a single patient. because these abnormalities are related  same patient   having some of these in the training set and others in the test set may cause us to perform better on those test cases than we would expect to perform on cases for other patients. to avoid such  leakage  of information into a training set  we ensured that all abnormalities associated with a particular patient are placed into the same fold for crossvalidation. another potential pitfall is that we may learn a rule that predicts an abnormality to be malignant based on properties of abnormalities in later mammograms. we never predict the status of an abnormality at a given date based on findings recorded with later dates.
　we present the results of our first experiment using both roc and precision recall curves. because of our skewed class distribution  or large number of benign cases  we prefer precision-recall curves over roc curves because they better show the number of  false alarms   or unnecessary biopsies. therefore  we use precision-recall curves for the remainder of the results. here  precision is the percentage of abnormalities that we classified as malignant that are truly cancerous. recall is the percentage of malignant abnormalities that were correctly classified. to generate the curves  we pooled the results over all ten folds by treating each prediction as if it had been generated from the same model. we sorted the estimates and used all possible split points to create the graphs.
1 results
the relational database containing the mammography data contains one row for each abnormality in a mammogram. fields in this relational table include all those shown in the bayesian network of figure 1. therefore it is straightforward to use existing bayesian network structure learning algorithms to learn a possibly improved structure for the bayesian network. we compared the performance of the best learned networks against the expert defined structure shown in figure 1. we estimated the parameters of the expert structure from the dataset using maximum likelihood estimates with laplace correction. figure 1 shows the roc curve for these experiments  and figure 1 shows the precision-recall curves. figure 1 shows the area under the precision-recall curve for the expert network  l1  and with learned structure  l1 . we only consider recalls above 1%  as for this application radiologists would be required to perform at least at this level. we further use the paired t-test to compare the areas under the curve for every fold. we found the difference to be statistically significant with a 1% level of confidence.
　with the help of a radiologist  we selected the numeric and ordered features in the database and computed aggregates for each of these features. we determined that 1 of the 1 attributes were suitable for aggregation. we computed aggregates on both the patient and the mammogram level. on the patient level  we looked at all of the abnormalities for a spe-

false positive rate
figure 1: roc curves for structure learning.  level 1 

recall
figure 1: precision recall curves for structure learning.
 level 1 
cific patient. on the mammogram level  we only considered the abnormalities present on that specific mammogram. to discretize the averages  we divided each range into three bins. for binary features we had predefined bin sizes  while for the other features we attempted to get equal numbers of abnormalities in each bin. for aggregation functions we used maximum and average. the aggregation introduced 1 new features. for the interested reader  the following paragraph presents further details of our aggregation process.
　we used a three step process to construct aggregate features. first  we chose a field to aggregate. second  we selected an aggregation function. third  we needed to decide over which rows to aggregate the feature  that is  which keys or links to follow. this is knownas a slot chain in prm terminology. in our database  two such links exist. the patient id field allows access to all the abnormalities for a given patient  providing aggregation on the patient level. the second key is the combination of patient id and mammogram date  which returns all abnormalities for a patient on a specific mammogram  providing aggregation on the mammogram level. to demonstrate this process we will work though an example of
patient        abnormality       date           mass shape          ...           mass size       location         average           average              be/mal                                                                                                                                                           patient             mammogram                                                                                                                                                           mass size        mass sizep1                    1                    1              spic                 ...            1                ru1                 1               1                        bp1         
p1                    1             
           1                    1    
       1              var                
          spic                 ...        ...             1         
       1                ru1         
       ll1                 1       
         1               1              
        1                      m            b..............................figure 1: database after aggregation on mass size fieldcomputing an aggregate feature for patient 1 in the database given in figure 1. we will aggregate on the mass size field and use average as the aggregation function. patient 1 has three abnormalities  one from a mammogram in may 1 and two from a mammogram in may 1. to calculate the aggregate on the patient level  we would average the size for all three abnormalities  which is .1. to find the aggregate on the mammogram level for patient 1  he have to perform two separate computations. first  we follow the link p1 and 1  which yields abnormality 1. the average for this key mammogram is simply .1. second  we follow the link p1 and 1  which yields abnormalities 1 and 1. the average for these abnormalities is .1. figure 1 shows the database following construction of these aggregate features.
　next  we tested whether useful new fields could be computed by rule learning. specifically  we used the ilp system aleph  to learn rules predictive of malignancy. several thousand distinct rules were learned for each fold  with each rule covering many more malignant cases than  incorrectly covering  benign cases. in order to obtain a varied set of rules  we ran aleph using every positive example in each fold as a seed for the search. we avoid the rule overfitting found by other authors  by doing breadth-first search for rules and by having a minimal limit on coverage. each seed generated anywhere from zero to tens of thousands of rules. we post processed the rules using a greedy algorithm  where we selected the best scoring rule that covers new examples first. for each fold  the 1 best clauses were selected based on 1 criteria:  1  they needed to be multi-relational;  1  they needed to be distinct;  1  they needed to cover a significant number of malignant cases. the resulting views were added as new features to the database. figure 1 includes a comparison of all levels of learning.
　we can observe very significant improvements when adding multi-relational features. both rules and aggregates achieved better performance. aggregates do better for higher recalls  while rules do better for medium recalls. we believe this is because ilp rules are more accurate than the other features  but have limited coverage.
comparison of all levels of learning

recall
figure 1: precision recall curves for each level of learning
　level 1 performs as well as aggregates for high recalls  and close to ilp for mediumrecalls. accordingto the paired t-test the improvement of level 1 over level 1 is significant  using the area under the curve metric  at the 1% level. meanwhile  level 1 presents an improvement over level 1  using the area under the curve metric  at the 1% confidence level.
　levels 1 and 1 correspond to standard propositional learning whereas levels 1 and 1 incorporate relational information. in this task  considering relational information is crucial for improving preformance. furthermore  the process of generating the views in level 1 has been useful to the radiologist as it has identified novel correlations between attributes.
1 related work
research in srl has advanced along two main lines: methods that allow graphical models to represent relations  and frameworks that extend logic to handle probabilities. along the first line  probabilistic relational models  or prms  introduced by friedman  getoor  koller and pfeffer  represent one of the first attempts to learn the structure of graphical mod-

figure 1: area under the curve for recalls above 1%
els while incorporating relational information. recently heckerman  meek and koller have discussed extensions to prms and compared them to other graphical models. a statistical learning algorithm for probabilitistic logic representations was first given by sato  and later  cussens  proposed a more general algorithm to handle log linear models. additionally  muggleton  has provided learning algorithms for stochastic logic programs. the structure of the logic program is learned using ilp techniques  while the parameters are learned using an algorithm scaled up from that used for stochastic context-free grammars.
　newer representations garnering arguably the most attention are bayesian logic programs   blps   constraint logic programming with bayes net constraints  or clp      and markov logic networks  mlns  . markov logic networks are most similar to our approach. nodes of mlns are the ground instances of the literals in the rule  and the arcs correspond to the rules. one major difference is that  in our approach  nodes are the rules themselves. although we cannot work at the same level of detail  our approach makes it straightforward to combine logical rules with other features  and we now can take full advantage of propositional learning algorithms.
　the present work builds upon previous work on using ilp for feature construction. such work treats ilp-constructed rules as boolean features  re-represents each example as a feature vector  and then uses a feature-vector learner to produce a final classifier. to our knowledge  pompe and kononenko  were the first to apply na： ve bayes to combine clauses. other work in this category was by srinivasan and king   who use rules as extra features for the task of predicting biological activities of molecules from their atomand-bond structures. popescul et.al.  use to derive cluster relations  which are then combined with the original features through structural regression. in a different vein  relational decision trees  use aggregation to provide extra features on a multi-relational setting  and are close to our level 1 setting. knobbe et al.  proposed numeric aggregates in combination with logic-based feature construction for single attributes. perlich and provost discuss several approaches for attribute construction using aggregates over multi-relational features . the authors also propose a hierarchy of levels of learning: feature vectors  independent attributes on a table  multidimensional aggregation on a table  and aggregation across tables. some of these techniques in their hierarchy could be applied to perform view learning in srl.
1 conclusions and future work
we presented a method for statistical relational learning which integrates learning from attributes  aggregates  and rules. our example application shows benefits from the several levels of learning we proposed. level 1  structure learning  clearly outperformsthe expert structure. we furthershow that multi-relational techniques can achieve very significant improvements  even on a single table domain  and that the most consistent improvement is obtained by using level 1  both aggregates and new views.
　we believe that further improvements are possible. it makes sense to include aggregates in the background knowledge for rule generation. alternatively  one can extend rules with aggregation operators  as proposed in recent work by vens et al. . we have found the rule selection problem to be non-trivial. our greedy algorithm often generates too similar rules  and is not guaranteed to maximize coverage. we would like to approach this problem as an optimization problem weighing coverage  diversity  and accuracy.
　our approach of using ilp to learn new features for an existing table merely scratches the surface of the potential for view learning. a more ambitious approach would be to more closely integrate structure learning and view learning. a search could be performed in which each  move  in the search space is either to modify the probabilistic model or to refine the intensional definition of some field in the new view. going further still  one might learn an intensional definition for an entirely new table. as a concrete example  for mammography one could learn rules defining a binary predicate that identifies  similar  abnormalities. because such a predicate would represent a many-to-manyrelationship among abnormalities  a new table would be required.
1 acknowledgments
support for this research was partially provided by u.s. air force grant f1-1. elizabeth burnside is supported by a general electric research in radiology academic fellowship. ine s dutra and v＞ tor santos costa are on leave from federal university of rio de janeiro  brazil. v＞ tor santos costa was partially supported by the fundac a o para a cie ncia e tecnologia. we would like to thank rich maclin  jill davis and allison holloway for reading over drafts of this paper. we would also like to thank the referees for their insightful comments.
