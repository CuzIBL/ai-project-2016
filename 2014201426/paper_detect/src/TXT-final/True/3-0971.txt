
we enhance machine learning algorithms for text categorization with generated features based on domain-specific and common-sense knowledge. this knowledge is represented using publicly available ontologies that contain hundreds of thousands of concepts  such as the open directory; these ontologies are further enriched by several orders of magnitude through controlled web crawling. prior to text categorization  a feature generator analyzes the documents and maps them onto appropriate ontology concepts  which in turn induce a set of generated features that augment the standard bag of words. feature generation is accomplished through contextual analysis of document text  implicitly performing word sense disambiguation. coupled with the ability to generalize concepts using the ontology  this approach addresses the two main problems of natural language processing-synonymy and polysemy. categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the documents alone. experimental results confirm improved performance  breaking through the plateau previously reached in the field.
1 introduction
the state of the art systems for text categorization use induction algorithms in conjunction with word-based features   bag of words  . after a decade of improvements  the performance of the best document categorization systems became more or less similar  and it appears as though a plateau has been reached  as neither system is considerably superior to others  and improvements are becoming evolutionary  sebastiani  1 .
　the bag of words  bow  approach is inherently limited  as it can only use pieces of information that are explicitly mentioned in the documents  and even that provided the same vocabulary is consistently used. specifically  this approach has no access to the wealth of world knowledge possessed by humans  and is easily puzzled by facts and terms not mentioned in the training set.
　to illustrate the limitations of the bow approach  consider document #1 in reuters-1  which is one of the most frequently used datasets in text categorization research. this document discusses a joint mining venture by a consortium of companies  and belongs to the category  copper . however  the document only briefly mentions that the aim of this venture is mining copper; instead  this fairly long document mainly talks about the mutual share holdings of the companies involved  teck corporation  cominco  and lornex mining   as well as discusses other mining activities of the consortium. consequently  three very different text classifiers we used  svm  knn and c1  failed to classify the document correctly. this comes as no surprise- copper  is a fairly small category  and neither of these companies  nor the location of the venture  highland valley in british columbia  are ever mentioned in the training set for this category. the failure of the bag of words approach is therefore unavoidable  as it cannot reason about the important components of the story. we argue that this needs not be the case. when a reuters editor originally handled this document  she most likely knew quite a lot about the business of these companies  and easily assigned the document to the category  copper . it is this kind of knowledge that we would like machine learning algorithms to have access to.
　to date  quite a few attempts have been made to deviate from the orthodox bag of words paradigm  usually with limited success. in particular  representations based on phrases  dumais et al.  1; fuernkranz et al.  1   named entities  kumaran and allan  1   and term clustering  lewis and croft  1; bekkerman  1  have been explored. in the above example  however  none of these techniques could overcome the underlying problem-lack of world knowledge. in order to solve this problem and break through the existing performance barrier  a fundamentally new approach is apparently necessary. one possible solution is to completely depart from the paradigm of induction algorithms in an attempt to perform deep understanding of the document text. yet  considering the current state of natural language processing systems  this does not seem to be a viable option  at least for the time being .
　we propose an alternative solution that capitalizes on the power of existing induction techniques while enriching the language of representation  namely  exploring new feature spaces. prior to text categorization  we employ a feature generator that uses common-sense and domain-specific knowledge to enrich the bag of words with new  more informative features. feature generation is performed completely automatically  using machine-readable hierarchical repositories of knowledge such as the open directory project  odp   yahoo! web directory  and the wikipedia encyclopedia.
　in this paper we use the odp as a source of background knowledge. thus  in the above example the feature generator  knows  that the companies mentioned are in the mining business  and that highland valley happens to host a copper mine. this information is available in web pages that discuss the companies and their operations  and are cataloged in corresponding odp categories such as mininganddrilling and metals. similarly  web pages about highland valley are cataloged under britishcolumbia. to amass this information  we crawl the urls cataloged in the odp  thus effectively multiplying the amount of knowledge available many times over. armed with this knowledge  the feature generator constructs new features that denote these odp categories  and adds them to the bag of words. the augmented feature space provides text classifiers with a cornucopia of additional information. indeed  our implementation of the proposed methodology classifies this document correctly.
　feature generation  fg  techniques were found useful in a variety of machine learning tasks  markovitch and rosenstein  1; fawcett  1; matheus  1 . these techniques search for new features that describe the target concept better than the ones supplied with the training instances. a number of feature generation algorithms were proposed  pagallo and haussler  1; matheus and rendell  1; hu and kibler  1; murphy and pazzani  1   which led to significant improvements in performance over a range of classification tasks. however  even though feature generation is an established research area in machine learning  only a few works applied it to text processing  kudenko and hirsh  1; mikheev  1; cohen  1 . with the exception of a few studies using wordnet  scott  1; urena-lopez et al.  1   none of them attempted to leverage repositories of world knowledge.
　the contributions of this paper are threefold. first  we propose a framework and a collection of algorithms that perform feature generation based on very large-scale repositories of human knowledge. second  we propose a novel kind of contextual analysis performed during feature generation  which views the document text as a sequence of local contexts  and performs implicit word sense disambiguation. finally  we describe a way to further enhance existing knowledge bases by several orders of magnitude by crawling the world wide web. as we show in section 1  our approach allows to break the performance barrier currently reached by the best text categorization systems.
1 feature generation methodology
the proposed methodology allows principled and uniform integration of external knowledge to construct new features. in the preprocessing step we use knowledge repositories to build a feature generator. applying the feature generator to the documents produces a set of generated features. these features undergo feature selection  and the most discriminative ones are added to the bag of words. finally  we use traditional text categorization techniques to learn a text categorizer in the augmented feature space.
　suitable knowledge repositories satisfy the following requirements:
1. the repository contains a collection of concepts organized in a hierarchical tree structure  where edges represent the  is-a  relationship. using a hierarchical ontology allows us to perform powerful generalizations.
1. there is a collection of texts associated with each concept. the feature generator uses these texts to learn the definition and scope of the concept  in order to be able to assign it to relevant documents.
　we currently use the odp as our knowledge base  however  our methodology is general enough to facilitate other sources of common-sense and domain-specific knowledge that satisfy the above assumptions. the open directory comprises a hierarchy of approximately 1 categories that catalog over 1 1 web sites  each represented by a url  a title  and a brief summary of its contents. the project constitutes an ongoing effort promoted by over 1 volunteer editors around the globe  and is arguably the largest publicly available web directory. being the result of pro bono work  the open directory has its share of drawbacks  such as nonuniform coverage  duplicate subtrees in different branches of the hierarchy  and sometimes biased coverage due to peculiar views of the editors in charge. nonetheless  odp embeds a colossal amount of human knowledge in a wide variety of areas  covering even most specific scientific and technical concepts. in what follows  we refer to the odp category nodes as concepts  to avoid possible confusion with the term categories  which is usually reserved for the labels assigned to documents in text categorization.
1 building a feature generator
we start with a preprocessing step  performed once for all future text categorization tasks. we induce a hierarchical text classifier that maps pieces of text onto relevant odp concepts  which later serve as generated features. the resulting classifier is called a feature generator according to its true purpose in our scheme  as opposed to the text categorizer  or classifier  that we build ultimately. the feature generator represents odp concepts as vectors of their most characteristic words  which we call attributes  reserving the term features to denote the properties of documents in text categorization . we now explain how the attribute vectors are built.
　we use the textual descriptions of odp nodes and their urls as training examples for learning the feature generator. although these descriptions alone constitute a sizeable amount of information  we devised a way to increase the volume of training data by several orders of magnitude. we do so by crawling the web sites pointed at by all cataloged urls  and obtain a small representative sample of each site. pooling together the samples of all sites associated with an odp node gives us a wealth of additional information about it.
　texts harvested from the www are often plagued with noise  and without adequate noise reduction crawled data may do more harm than good. to remedy the situation  we perform attribute selection for each odp node; this can be done using any standard feature selection technique such as information gain. for example  consider the top 1 attributes selected for the odp concepts science-science  research  scientific  biology  laboratory  analysis  university  theory  study  scientist  and artificialintelligence-neural  artificial  algorithm  intelligence  aaai  bayesian  probability  ieee  cognitive  inference. additional noise reduction is achieved by pruning nodes having too few urls or situated too deep in

         figure 1: feature generation example the tree  and thus representing overly specific concepts   assigning their textual content to their parents.
　in our current implementation  the feature generator works as a nearest neighbor classifier-it compares its input text to  the attribute vectors of  all odp nodes  and returns the desired number of best-matching ones. the generator also performs generalization of concepts  and constructs features based on the classified concepts per se as well as their ancestors in the hierarchy.
　let us revisit the example from section 1. while building the feature generator  our system crawls the web sites cataloged under mining-related odp concepts such as business/mininganddrilling  science/technology/mining and business/industrialgoodsandservices/materials/metals.
these include www.teckcominco.com and www.miningsurplus.com that belong to the  now merged  teck cominco company. due to the company's prominence  it is mentioned frequently in the web sites we have crawled  and consequently the words  teck  and  cominco  are included in the set of attributes selected to represent the above concepts. figure 1 illustrates the process of feature generation for this example.
1 contextual feature generation
traditionally  feature generation uses the basic features supplied with the training instances to construct more sophisticated features. in the case of text processing  however  applying this approach to the bag of words leads to losing the important information about word ordering. therefore  we argue that feature generation becomes much more powerful when it operates on the raw document text. but should the generator always analyze the whole document as a single unit  similarly to regular text classifiers 
　we believe that considering the entire document may often be misleading  as its text can be too diverse to be readily mapped to the right set of concepts  while notions mentioned only briefly may be overlooked. instead  we propose to partition the document into a series of non-overlapping segments  called contexts   and then generate features at this finer level. each context is classified into a number of concepts in the knowledge base  and pooling these concepts together results in multi-faceted classification for the document. this way  the resulting set of concepts represents the various aspects or
sub-topics covered by the document.
　potential candidates for such contexts are simple sequences of words  or more linguistically motivated chunks such as sentences or paragraphs. the optimal resolution for document segmentation can be determined automatically using a validation set. we propose a more principled multi-resolution approach that simultaneously partitions the document at several levels of linguistic abstraction  windows of words  sentences  paragraphs  up to taking the entire document as one big chunk   and performs feature generation at each of these levels. we rely on the subsequent feature selection step to eliminate extraneous features  preserving only those that genuinely characterize the document.
　in fact  the proposed approach tackles the two most important problems in natural language processing  namely  synonymy and polysemy. classifying individual contexts implicitly performs word sense disambiguation  and thus resolves word polysemy to some degree. a context that contains one or more polysemous words is mapped to the concepts that correspond to the sense shared by the context words. thus  the correct sense of each word is determined with the help of its neighbors. at the same time  enriching document representation with high-level concepts and their generalizations addresses the problem of synonymy  as the enhanced representation can easily recognize that two  or more  documents actually talk about related issues  even though they do so using different vocabularies.
　let us again revisit our running example. during feature generation  document #1 is segmented into a sequence of contexts  which are then mapped to mining-related odp concepts  e.g.  business/mininganddrilling . these concepts  as well as their ancestors in the hierarchy  give rise to a set of generated features that augment the bag of words  see figure 1 . observe that the training documents for the category  copper  underwent a similar processing when a text classifier was induced. consequently  features based on these concepts were selected during feature selection thanks to their high predictive capacity. it is due to these features that the document is now categorized correctly  while without feature generation it consistently caused bow classifiers to err.
1 feature selection
using support vector machines in conjunction with bag of words  joachims  found that svms are very robust even in the presence of numerous features  and further observed that the multitude of features are indeed useful for text categorization. these findings were corroborated in more recent studies  brank et al.  1; bekkerman  1  that observed either no improvement or even small degradation of svm performance after feature selection.1 consequently  many later works using svms did not apply feature selection at all  leopold and kindermann  1; lewis et al.  1 .
　this situation changes drastically as we augment the bag of words with generated features. first  nearly any technique for automatic feature generation can easily generate huge numbers of features  which will likely aggravate the  curse of dimensionality . furthermore  it is feature selection that allows the feature generator not to be a perfect classifier. when at least some of the concepts assigned to the document are correct  feature selection can identify them and seamlessly eliminate the spurious ones.
1 empirical evaluation
we implemented the proposed methodology using an odp snapshot as of april 1.
1 implementation details
after pruning the top/world branch that contains nonenglish material  we obtained a hierarchy of over 1 concepts and 1 1 urls. applying our methodology to a knowledge base of this scale required an enormous engineering effort. textual descriptions of the concepts and urls amounted to 1 mb of text. in order to increase the amount of information for training the feature generator  we further populated the odp hierarchy by crawling all of its urls  and taking the first 1 pages  in the bfs order  encountered at each site. this operation yielded 1 gb worth of html files. after eliminating all the markup and truncating overly long files at 1 kb  we ended up with 1 gb of additional textual data. compared to the original 1 mb of text supplied with the hierarchy  we obtained over a 1-fold increase in the amount of data. after removing stop words and rare words  occurring in less than 1 documents  and stemming the remaining ones  we obtained 1 1 distinct terms that were used to represent odp nodes as attribute vectors. up to 1 most informative attributes were selected for each odp node using the document frequency criterion  other commonly used feature selection techniques  such as information gain  χ1 and odds ratio  yielded slightly inferior results . we used the multi-resolution approach for feature generation  classifying document contexts at the level of words  sentences  paragraphs  and finally the entire document.features were generated from the 1 best-matching odp concepts for each context.
1 experimental methodology
the following test collections were used:
1. reuters-1  reuters  1 . following common practice  we used the modapte split  1 training  1 testing documents  and two category sets  1 largest categories and 1 categories with at least one training and testing example. 1. reuters corpus volume i  rcv1   lewis et al.  1  has over 1 documents  and presents a new challenge for text categorization. to speedup experimentation  we used a subset of the corpus with 1 training documents  dated 1/1  and 1 testing ones  1/1 . following brank et al.   we used 1 topic and 1 industry categories that constitute a representative sample of the full groups of 1 and 1 categories  respectively. we also randomly sampled the topic and industry categories into several sets of 1 categories each  table 1 shows 1 category sets in each group with the highest improvement in categorization performance .1
1. 1 newsgroups  1ng   lang  1  is a well-balanced dataset of 1 categories containing 1 documents each.
1. movie reviews  movies   pang et al.  1  defines a sentiment classification task  where reviews express either positive or negative opinion about the movies. the dataset has 1 documents in two categories  positive/negative .
　we used support vector machines1 as our learning algorithm to build text categorizers  since prior studies found svms to have the best performance for text categorization  dumais et al.  1; yang and liu  1 . following established practice  we use the precision-recall break-even point  bep  to measure text categorization performance. for the two reuters datasets we report both micro- and macroaveraged bep  since their categories differ in size significantly. micro-averaged bep operates at the document level and is primarily affected by categorization performance on larger categories. on the other hand  macro-averaged bep averages results for individual categories  and thus small categories with few training examples have large impact on the overall performance. for both reuters datasets we used a fixed data split  and consequently used macro sign test  stest   yang and liu  1  to assess the statistical significance of differences in classifier performance. for 1ng and movies we performed 1-fold cross-validation  and used paired t-test to assess the significance.
1 the effect of feature generation
we first demonstrate that the performance of basic text categorization in our implementation  column  baseline  in table 1  is consistent with other published studies  all using svm . on reuters-1  dumais et al.  achieved micro-bep of 1 for 1 categories and 1 for all categories. on 1ng  bekkerman  obtained bep of 1. pang et al.  obtained accuracy of 1 on movies. the minor variations in performance are due to differences in data preprocessing used in different systems; for example  for the movies dataset we worked with raw html files rather than with the official tokenized version  in order to recover sentence and paragraph structure for contextual analysis. for rcv1  direct comparison with published results is more difficult  as we limited the category sets and the date span of documents to speedup experimentation.
　table 1 shows the results of using feature generation with significant improvements  p   1  shown in bold. for both reuters datasets  we consistently observed larger improvements in macro-averaged bep  which is dominated by categorization effectiveness on small categories. this goes in line with our expectations that the contribution of external knowledge should be especially prominent for categories with few training examples. as can be readily seen  categorization performance was improved for all datasets  with notable improvements of up to 1% for reuters rcv1 and 1% for movies. given the performance plateau currently reached by the best text categorizers  these results clearly demonstrate the advantage of knowledge-based feature generation.
1 actual examples under a magnifying glass
thanks to feature generation our system correctly classifies the running example document #1. let us consider additional testing examples from reuters-1 that are incor-
datasetbaselinefeatureimprovementgenerationvs. baselinemicromacromicromacromicromacrobepbepbepbepbepbepreuters-1 categories1111+1%+1%1 categories1111+1%+1%rcv1
industry-1.1.1.1.1+1%+1%industry-1a1111+1%+1%industry-1b1111+1%+1%industry-1c1111+1%+1%topic-1.1.1.1.1+1%+1%topic-1a1111+1%+1%topic-1b1111+1%+1%topic-1c1111+1%+1%1ng11+1%movies11+1%
	1 1 1	1	1	1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1
	context window length  words 	fraction of generated features selected	fraction of generated features selected
figure 1: varying context length  movies 	figure 1: feature selection  movies 	figure 1: feature selection  rcv1/topic-1  table 1: text categorization with and without feature generation rectly categorized by the bow classifier. document #1 belongs to the category  money-fx   money/foreign exchange  and discusses the devaluation of the kenyan shilling. even though  money-fx  is one of the 1 largest categories  the word  shilling  does not occur in its training documents even once. however  the feature generator easily recognizes it as a kind of currency  and produces features such as recreation/collecting/papermoney and recreation/collecting/coins/worldcoins. these high-level features were also constructed for many training examples  and consequently the document is now classified correctly.
　similarly  document #1 discusses italy's balance of payments and belongs to the category  trade   while the word  trade  itself does not occur in this short document. however  when the feature generator considers document contexts discussing italian deficit as reported by the bank of italy  it correctly maps them to concepts such as society/government/finance  society/issues/economic/international/trade  business/internationalbusinessandtrade. due to these features  which were also generated for training documents in this category  the document is now categorized correctly.
1 the effect of contextual analysis
we now explore the various possibilities to define document contexts for feature generation. figure 1 shows how text categorization performance on the movies dataset changes for various contexts. the x-axis measures context length in words  and the fg/words curve corresponds to applying the feature generator to the context of that size. with these wordlevel contexts  maximum performance is achieved when using pairs of words  x=1 . the fg/doc line shows the result of using the entire document as a single context. in this case  the results are somewhat better than without feature generation  baseline   but are still inferior to the more fine-grained word-level contexts  fg/words . however  the best performance by far is achieved when using the multi-resolution approach  fg/multi   in which we use a series of linguistically motivated chunks of text  starting with individual words  and then generating features from sentences  paragraphs  and finally the entire document.
1 the utility of feature selection
under the experimental settings defined in section 1  feature generation constructs approximately 1 times as many features as are in the bag of words. we conducted two experiments to understand the effect of feature selection in conjunction with feature generation.
　since earlier studies found that most bow features are indeed useful for svm text categorization  section 1   in our first experiment we only apply feature selection to the generated features  and use the selected ones to augment the  entire  bag of words. in figures 1 and 1  the bow line depicts the baseline performance without generated features  while the bow+gen curve shows the performance of the bag of words augmented with progressively larger fractions of generated features  sorted by information gain . for both datasets  the performance peaks when only a small fraction of the generated features are used  while retaining more generated features has a noticeable detrimental effect. similar phenomena have been observed for other datasets; we omit the results owing to lack of space.
　our second experiment was set up to examine the performance of the generated features alone  without the bag of words  gen curve in figures 1 and 1 . for movies  discarding the bow features hurts the performance somewhat  but the decrease is far less significant than what could be expected-using only the generated features we lose less than 1% in bep compared with the bow baseline. for 1ng  a similar experiment sacrifices about 1% off the bow performance  as this dataset is known to have a very diversified vocabulary  for which many studies found feature selection to be particularly harmful. however  the situation is reversed for both reuters datasets. for reuters-1  the generated features alone yield a 1% improvement in micro- and macro-bep for 1 categories  while for 1 categories they only lose 1% in micro-bep and 1% in macro-bep compared with the bag of words. for rcv1/industry-1  disposing of the bag of words hurts the bep by 1%. surprisingly  for rcv1/topic-1  figure 1  the generated features per se command a 1% improvement in macro-bep  rivalling the performance of bow+gen that only gains another 1% improvement  table 1 . we interpret these findings as a further reinforcement of the quality of representation due to the generated features.
1 conclusions and future work
we proposed a feature generation methodology for text categorization. in order to render machine learning algorithms with common-sense and domain-specific knowledge possessed by humans  we use large hierarchical knowledge bases to build a feature generator. the latter analyzes documents prior to text categorization  and augments the conventional bag of words representation with relevant concepts from the knowledge base. the enriched representation contains information that cannot be deduced from the document text alone.
　we further described the multi-resolution analysis that examines the document text at several levels of linguistic abstraction  and performs feature generation at each level. considering polysemous words in their native context implicitly performs word sense disambiguation  and allows the feature generator to cope with word synonymy and polysemy.
　empirical evaluation definitively confirmed that knowledge-based feature generation brings text categorization to a new level of performance. interestingly  the sheer breadth and depth of the odp  further boosted by crawling the urls cataloged in the directory  brought about improvements both in regular text categorization as well as in the  non-topical  sentiment classification task.
　we believe that this research only scratches the surface of what can be achieved with knowledge-rich features. in our future work  we plan to investigate new algorithms for mapping document contexts onto hierarchy concepts  as well as new techniques for selecting attributes that are most characteristic of every concept. we intend to apply focused crawling to only collect relevant web pages when crawling cataloged urls. in addition to the odp  we also plan to make use of domain-specific hierarchical knowledge bases  such as the medical subject headings  mesh . finally  we conjecture that knowledge-based feature generation will also be useful for other information retrieval tasks beyond text categorization  and we intend to investigate this in our future work.
acknowledgments
we thank lev finkelstein and alex gontmakher for many a helpful discussion. this research was partially supported by technion's counter-terrorism competition and by the muscle network of excellence.
