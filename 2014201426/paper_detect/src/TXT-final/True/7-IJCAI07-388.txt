
there is currently much interest in using external memory  such as disk storage  to scale up graph-search algorithms. recent work shows that the local structure of a graph can be leveraged to substantially improve the efficiency of externalmemory graph search. this paper introduces a technique  called edge partitioning  which exploits a form of local structure that has not been considered in previous work. the new technique improves the scalability of structured approaches to external-memory graph search  and also guarantees the applicability of these approaches to any graph-search problem. we show its effectiveness in an external-memory graph-search algorithm for domain-independent strips planning.
1 introduction
breadth-first search  a*  and related graph-search algorithms store generated nodes in memory in order to be able to detect duplicates and prevent node regeneration. the scalability of these graph-search algorithms can be dramatically increased by storing nodes in external memory  such as disk storage. because random access of disk is several orders of magnitude slower than internal memory  external-memory graphsearch algorithms use duplicate-detection strategies that serialize disk access in a way that minimizes disk i/o.
　a widely-used approach to external-memory graph search is delayed duplicate detection  stern and dill  1; korf  1; edelkamp et al.  1 . in its original and simplest form  delayed duplicate detection expandsa set of nodes  e.g.  the nodes on the search frontier  without checking for duplicates  stores the generated nodes  including duplicates  in a disk file  and eventually removes the duplicates by sorting the file. in keeping with its use by theoretical computer scientists in analyzing the complexity of external-memory graph search  munagala and ranade  1   delayed duplicate detection makes no assumptions about the structure of the search graph  exceptthat it is undirected and unweighted . recent work shows that the performance of externalmemory graph search can be significantly improved by exploiting the structure of a graph in order to localize memory references. zhou and hansen  1; 1; 1b  propose a technique called structured duplicate detection that exploits local structure that is captured in an abstract representation of the graph. for graphs with sufficient local structure  structured duplicate detection outperforms delayed duplicate detection because it never generates duplicates  even temporarily  and thus has lower overhead and reduced complexity. korf and schulze  1  show how to improve the performance of delayed duplicate detection by using similar local structure to reduce the delay between generation of duplicates and their eventual detection and removal.
　although approaches that exploit a graph's local structure are very effective  they depend on a graph-search problem having the appropriate kind of local structure  and a sufficient amount of it  in order to be effective. thus one can legitimately question whether these approaches will be equally effective for all graphs  or effective at all for some graphs. in this paper  we introduce a technique that exploits a different kind of local structure than is exploited in previous work. the kind of local structure exploited by this technique is in some sense created by the way the algorithm expands nodes  in particular  by use of a novel form of incremental node expansion. the new technique  called edge partitioning  can localize memory references in any graph  no matter what its structure - even a fully-connected graph. moreover  the way this new techniquelocalizes memoryreferencescomplements the kind of local graph structure that is exploited by previous approaches. this allows the new technique to be combined with previously-developed approaches in order to create a more powerful external-memory graph-search algorithm. in this paper  we focus on how to use edge partitioning to improve the performance of structured duplicate detection. we implementit in an external-memorygraph-searchalgorithm for domain-independent strips planning that uses structured duplicate detection  and show that it greatly improves scalability. at the close of the paper  we discuss how edge partitioning can also be used to exploit local structure in delayed duplicate detection.
1 structured duplicate detection
structured duplicate detection  sdd   zhou and hansen  1  is an approach to external-memory graph search that leverages local structure in a graph to partition stored nodes between internal memory and disk in such a way that duplicate detection can be performed immediately  during node expansion  and no duplicates are ever generated.
　the local structure that is leveraged by this approach is revealed by a state-space projection function that is a manyto-one mapping from the original state space to an abstract state space  in which each abstract state corresponds to a set of states in the original state space. if a state x is mapped to an abstract state y  then y is called the image of x. one way to create a state-space projection function is by ignoring the value of some state variables. for example  if we ignore the positions of all tiles in the eight puzzle and consider only the position of the  blank   we get an abstract state space that has only nine abstract states  one corresponding to each possible position of the blank.
　given a state-space graph and state-space projection function  an abstract state-space graph is constructed as follows. the set of nodes in the abstract graph  called the abstract nodes  corresponds to the set of abstract states. an abstract node y is a successor of an abstract node y if and only if there exist two states in the original state space  such that  is a successor of x  and  1  map to  respectively  under the state-space projection function.
　figure 1 b  shows the abstract state-space graph created by the simple state-space projection function that maps a state into an abstract state based only on the position of the blank. each abstract node bi in figure 1 b  corresponds to the set of states with the blank located at position i in figure 1 a .
　in structured duplicate detection  stored nodes in the original search graph are divided into  nblocks  with each nblock corresponding to a set of nodes that maps to the same abstract node. given this partition of stored nodes  structured duplicate detection uses the concept of duplicate-detection scope to localize memory references in duplicate detection. the duplicate-detection scope of a node x in the original search graph is defined as all stored nodes  or equivalently  all nblocks  that map to the successors of the abstract node y that is the image of node x under the projection function. in the eight puzzle example  the duplicate-detection scope of nodes that map to abstract node b1 consists of nodes that map to abstract node b1 and those that map to abstract node b1.
　the concept of duplicate-detection scope allows a search algorithm to check duplicates against a fraction of stored nodes  and still guarantee that all duplicates are found. an external-memory graph search algorithm can use ram to store nblocks within the current duplicate-detection scope  and use disk to store other nblocks when ram is full.
　sdd is designed to be used with a search algorithm that expands a set of nodes at a time  such as breadth-first search  where the order in which nodes in the set are expanded can be adjusted to minimize disk i/o. sdd's strategy for minimizing disk i/o is to order node expansions such that changes of duplicate-detection scope occur as infrequently as possible  and  when they occur  they involve change of as few nblocks as possible. when ram is full  nblocks outside the current duplicate-detection scope are flushed to disk. writing the least-recently used nblocks to disk is one way to select which nblocks to write to disk. when expanding nodes in a different nblock  any nblocks in its duplicate-detection scope that are stored on disk are swapped into ram.

figure 1: panel  a  shows all possible positions of the blank for the eight puzzle. panel  b  shows an abstract state-space graph that is created by the state-space projection function that considers the position of the  blank  only.
1 edge partitioning
the kind of local structure that is exploited by sdd is captured in an abstract state-space graph when the maximum outdegree of any abstract node is small relative to the total number of abstract nodes. the largest outdegree of any abstract node reflects the largest duplicate-detection scope  and this in turn determines the internal-memory requirements of the search algorithm. experiments in several domains suggest that this form of local structure is present in many search problems  zhou and hansen  1; 1; 1b . however it is present in varying degrees  and there is no guarantee that it is present in every search problem. moreover  the degree to which it is present can affect the degree of scalability that is possible.
　this motivates the development of a technique that makes sdd effective regardless of whether  and to what degree  this kind of local structure is present. in fact  the new technique we introduce is effective even if the abstract state-space graph is fully-connected  and thus captures no local structure at all. the idea behind this technique is that the local structure exploited by sdd can be created  in some sense  by expanding nodes incrementally  which means generating only some of the successors of a node at a time  and generating the other successors later. incremental node expansion makes it possible to partition the original duplicate-detection scope into smaller duplicate-detection scopes  and this can significantly reduce the internal memory requirements of the algorithm.
　in the original form of sdd  the duplicate-detection scope of a node being expanded is defined as all stored nodes that map to any abstract node that is a successor of the abstract node that is the image of the node being expanded. thus the largest duplicate-detection scope reflects the largest number of successors of a node in the abstract graph. but this assumes that all successors are generated at the same time. if nodes are expanded incrementally  it is possible to subdivide this duplicate-duplicate scope into smaller scopes  one for each pair of abstract node and successor abstract node  or  equivalently  one for each outgoing abstract edge.
　this is the key idea of edge partitioning. it considers a set of nodes that map to a particular abstract node  and a particular outgoing abstract edge  and only applies the operators that correspond to the abstract edge  in order to generate only the successor nodes that correspond to the successor abstract node along that edge. as a result  in edge partitioning  the duplicate-detection scope consists of only a single nblock corresponding to the single abstract node that is the successor of an abstract edge. at a later point in the algorithm  a different outgoing abstract edge is considered  and a different set of operators are applied to the same set of nodes  in order to generate additional successor nodes with potential duplicates in a different nblock. eventually  all operators are applied to a set of nodes and they become fully expanded. note that full expansion of a node now requires a sequence of incremental expansions.
1 operator grouping
the state-space projection function and abstract state space graph used by sdd with edge partitioning are the same as for sdd in its original form. but since nodes are expandedincrementally  and only one abstract edge is considered at a time  it is now important to identify which operators are associated with each abstract edge  in order to know which successors to generate. we refer to this annotation of the edges of the abstract graph as operator grouping. we point out that an  operator  here refers to an instantiated  or grounded  operator. for example  the eight puzzle has a total of 1 grounded operators  even though there are only four  left  right  up  and down  operators prior to instantiation.
　operator grouping is built on top of state abstraction. let o be the set of all instantiated operators of a search problem. an operator o （o is applicable to an abstract node y if and only if there exists a state x in the original state space  such that  a  o is applicable to x  and  b  x maps to y. consider the eight puzzle. there are 1〜1= 1 operators that are applicable to abstract node b1  because the blank  when located at the top-left corner of the puzzle board  can move either right  b1 ★ b1  or down  b1 ★ b1   and each move has 1 different instantiations  depending on which tile of the eight puzzle is moved into the blank position. similarly  each of the abstract nodes b1 b1  and b1 has 1 applicable operators. abstract nodes b1 b1 b1  and b1 each have 1 〜 1 = 1 applicable operators  and abstract node b1 has 1 〜 1 = 1 applicable operators.
　once the set of applicable operators for each abstract node is determined  operator grouping identifies  for each applicable operator  the abstract edge it is associated with. an abstract edge  is an edge in the abstract graph that connects a pair of abstract nodes  if and only if y is a successor of y. we refer to  as the source  destination  of abstract edge .
　let oy be the set of operators applicable to abstract node y. an operator o （ oy is associated with an abstract edge  if and only if there exists two states in the original state space  such that  1  o is applicable to is the resulting state after applying o to x  and  1  and map to y and y  respectively. for operators with deterministic effects  it is easy to see that for every o （oy  there is a unique abstract edge  that o is associated with. essentially  there is a many-to-one mapping from the operator space to the abstract-edge space.
　to exploit local structure in the operator space  edge partitioning uses operator grouping to divide the set of applicable operators oy for abstract node y into operator groups  one for each successor of y in the abstract graph. an operator group is a subset of oy that consists of all the operators that are associated with abstract edge . note that
for all  and
 
where successors y  is the set of successors of y in the abstract graph.
　although the technique of operator grouping is presented here in the context of searching implicitly-represented graphs  i.e.  graphs represented by a start state and a set of operators for generating successors   it should be clear that the same technique applies with little modification to searching explicitly-representedgraphs  i.e.  graphsrepresentedby a set of vertices and a set of edges .
1 edge-partitioned duplicate-detection scope
the idea of edge partitioning for sdd is to subdivide the duplicate-detection scope into smaller scopes  one for each abstract edge  and use only the operator group that is associated with the abstract edge to generate successors at a time. this leads to the concept of duplicate-detection scope for an abstract edge  which is defined as follows.
definition 1 the duplicate-detection scope for an abstract edge is the set of stored nodes that map to the destination of the abstract edge.
　the duplicate-detection scope for an abstract edge is guaranteed to contain only nodes that map to a single abstract node  regardless of the structure of the abstract graph. the following theorem follows from the definition.
theorem 1 the duplicate-detection scope for an abstract edge contains all stored duplicates of the successors generated by applying its corresponding operator group to the set of nodes that map to the source of the abstract edge.
　theorem 1 guarantees that edge partitioning only needs to store a single nblock in ram  in order to catch all the duplicates that could be generated  even in the worst case. this works in the following way. for each abstract edge   edge partitioning uses operators to generate successors for nodes that map to abstract node y. after edge partitioning has expanded these nodes using one operator group  it uses a different operator group to generate successors for the same nblock  until all operator groups have been used in generating successors for the nblock. then it chooses a different nblock to expand next.
　because not all successors are generated by edge partitioning when a node is expanded  we call a node expansion in edge partitioning an incremental expansion. nodes eventually become fully expanded  once all operators are applied.
1 example
we use the following example to illustrate how edge partitioning works in sdd. let xi be a search node that represents a state of the eight puzzle with the blank located at position i as shown in figure 1 a . suppose there are only two stored nodes {a1 b1} that map to abstract node b1 shown in figure 1 b . let {a1 a1} and {b1 b1} be the successors of a1 and b1  respectively.  the subscript encodes the position of the blank.  when edge partitioning expands nodes a1 and b1  it first uses operators o （ ob1 b1. this corresponds to moving the blank to the right  to generate the first two successor nodes a1 and b1. note that only nodes that map to abstract node b1 need to be stored in ram when a1 or b1 is being generated. next  edge partitioning uses operators o （ ob1 b1  which correspond to moving the blank down  to generate the third and fourth successor nodes a1 and b1. this time  only nodes that map to abstract node b1 need to be stored in ram.
1 implementation
before discussing some strategies for implementing sdd with edge partitioning in an efficient way  we review the key steps in implementing sdd.
　first  before the search begins  sdd uses a state-state projection function to create an abstract graph that  hopefully  captures local structure in the original search graph. the state-space projection function partitions stored nodes into nblocks  one for each node in the abstract graph  that can be moved between ram and disk  and so each nblock must be able to fit in ram. the state-space projection function can be hand-crafted or automatically generated  as described in  zhou and hansen  1b .
　like delayed duplicate detection  sdd is designed to be used as part of a search algorithm that expands a set of nodes at a time  such as the frontier nodes in breadth-first search. the idea of sdd is to expand these nodes in an order that minimizes disk i/o. this is accomplished by expandingnodes with the same duplicate-detection scope consecutively  and minimizing changes of duplicate-detection scope during expansion of all nodes. a simple and effective heuristic is to expand nodes in order of nblock  with nblocks ordered according to a breadth-first traversal of the abstract graph. when ram is full  sdd needs to decide which nblocks to move from ram to disk. a simple and effective heuristic is to write the least-recently used nblocks to disk.
　sdd with edge partitioning uses a similar strategy of trying to minimize changes of duplicate-detection scope during expansion of a set of nodes. the difference is that it considers the duplicate-detectionscope for an abstract edge  and this requires incremental node expansion. a simple and effective heuristic is to apply operators to nodes in order of nblock  and  for each nblock  in order of outgoing abstract edge.
　we next consider some ways to improve performance. to reduce the overhead of operator grouping  our implementation uses a lazy approach in which operator grouping for an abstract node is only computed immediately before the first time a node that maps to it is expanded. because there could be a number of abstract nodes that do not have any nodes that map to them during search  this approach avoids the overhead of operator grouping for these abstract nodes. we have observed that the effectiveness of this approach tends to increase with the size of the abstract graph.
our implementation also uses a lazy approach to reading nodes from disk. upon switching to a duplicate-detection scope that consists of nodes stored on disk  our implementation does not read these nodes from disk immediately. instead  it waits until the first time a node is generated. the reason for this is that when a single operator group is used to generate successors for nodes in an nblock  it may not generate any successor node  if  a  the nodes to which the operators in the group are applicable have not yet been generated  or  b  the generated successor nodes have an f-cost greater than an upper bound used in branch-and-bound search. the lazy approach avoids the overhead of reading nodes from disk  in order to setup the duplicate-detection scope in ram  if no successors are generated by an operator group for an nblock.
　as previously discussed  sdd needs to decide which nblocks to move from ram to disk  when ram is full. except for the nblocks that make up the current duplicatedetection scope  any nblocks can potentially be flushed to disk. but this means if an nblock does not include itself as part of its own duplicate-detection scope  it may be flushed to disk even when its nodes are being expanded. while this is allowed in our implementation  it should be avoided as much as possible for efficiency reasons. we make two simple modifications to the least-recently used algorithm to ensure this. first  instead of updating the time stamp of an nblock every time it is accessed  its time stamp is only updated when  a  the current duplicate-detection scope changes and  b  the nblock is the next to be expanded or is part of the new scope. this also simplifies the maintenance of the clock  which needs no updates until the duplicate-detection scope changes. the second modification is that instead of moving forward the clock by one clock tick when the duplicate-detection scope changes  our algorithm advances it by two clock ticks. then the time stamp of the to-be-expanded nblock is set to one clock tick earlier than the new clock time. finally  the time stamps of all the nblocks within the new duplicate-detection scope are updated to the new clock time. as a result  if the nblock to be expanded next does not belong to the new duplicate-detection scope  it is the last to be flushed to disk  since its time stamp is more recent than any other flushable nblock and earlier than any non-flushable nblock  which has a time stamp equal to the current clock time.
　finally  recall that edge partitioning expands nodes in a single nblock multiple times  one for each operator group. this affects the strategy with which to remove nodes stored on disk for the currently-expanding nblock. while the simplest strategy is to remove these nodes from disk as soon as they are swapped into ram  it may incur extra overhead if these nodes must be written back to disk shortly after  in order to make room for newly-generatednodes. note that nodes in the currently-expanding nblock do not change as long as the operator group used to generate the successors is not associated with an abstract edge whose source and destination are the same  i.e.  a  self loop  . because self loops are easy to detect  our implementation postpones the removal of nodes stored on disk for the currently-expandingnblock until a  self loop  operator group  which  if any  is always the last operator group applied to an nblock in our implementation  is used to expand the nblock.
sddsdd + edge partitioningproblemramdiskexpsecsramdiskincrem expsecsdepots-1 11 1 11 1 11 1blocks-1 11 1 11 11 1 11trucks-1 11 1 1111 1 11storage-1 11 1 1111 1 1 1 1freecell-1 11 1 111 1 11 11elevator-1 11 1 1111 1 1 1 1gripper-1 11 1 1 1 1 11 1 1 1 1logistics-1 11 1 1 1 1 1 11 11driverlog-1 11 11 111 1 1 1 1 1 1satellite-1 11 1 1111 1 1 1 1trucks-1----1 1 11 11depots-1----1 1 1 1 1 1 1table 1: comparison of structured duplicate detection  sdd  with and without using edge partitioning on strips planning problems. columns show peak number of nodes stored in ram  ram   peak number of nodes stored on disk  disk   number of full node expansions  exp   number of incremental node expansions  increm exp   and running time in cpu seconds  secs . a '-' symbol indicates that the algorithm cannot solve the problem within 1 gb of ram.1 computational results
we implemented sdd with edge partitioning in a domainindependent strips planner that uses as its underlying search algorithm breadth-first heuristic search  zhou and hansen  1a . the reason for using breadth-first heuristic search is that it uses internal memory very efficiently. building sdd with edge partitioning on top of it improvesthe overall efficiency of search by limiting the need to access disk.
　our search algorithm uses regression planning to find optimal sequential plans. as an admissible heuristic  it uses the max-pair heuristic  haslum and geffner  1 . we tested our external-memory strips planner in ten different domains from the biennial planning competition  including two domains  trucks and storage  from the most recent competition. experiments were performed on an amd operton 1 ghz processor with 1 gb of ram and 1 mb of l1 cache.
　table 1 compares the performance of sdd with and without edge partitioning. these problems are among the largest in each of the ten planning domains that sdd with edge partitioning can solve without either  a  using more than 1 gb of ram or  b  taking more than 1 cpu days of running time. two problems  trucks-1 and depots-1  can only be solved within these limits using edge partitioning. for these two domains  the table also includes the largest instances that can be solved without edge partitioning. both versions of sdd use the same state-space projection function.
　table 1 shows a couple of interesting things. first  it shows that edge partitioning can reduce the internal-memory requirements of sdd by an average factor of 1 times for these planning problems. in doing so  it only increases the peak number of nodes stored on disk by about 1%. second  it shows that the overhead that results from using an incremental approach to expanding nodes is rather inexpensive. although on average there are 1 times as many incremental expansions when edge partitioning is used as there are full expansions when it is not  this only increases running time by 1% on average. note that the extra time taken by edge partitioning includes time for operator grouping.
indirectly  the table shows roughly how much internal memory is saved by using sdd with edge partitioning instead of a*. the number of full node expansions in sdd gives an estimate of how many nodes a* would need to store in order to solve the problem  since a* has to store every node it expands  and breadth-first heuristic search  with an optimal upper bound  expands roughly the same number of nodes as a*  disregarding ties. based on the number of full node expansions shown in table 1  a* would need  on average  at least 1 times more internal memory to solve these problems than breadth-first heuristic search with sdd and edge partitioning . because this estimate ignores the memory needed by a* to store the open list  which is usually larger than the closed list  it is actually a considerable underestimate.
　as the results show  sdd without edge partitioning is already very effective in solving these strips planning problems  which indicates that these search problems contain a great deal of the kind of local structure that sdd exploits. this means that these problems actually present a serious challenge for edge partitioning  which must identify additional structure that can be exploited to reduce internal memory requirements even further. for search problems for which sdd without edge partitioning is less effective  sdd with edge partitioning is likely to reduce internal memory requirements by a much larger ratio.
　since edge partitioning is effective even when the abstract graph used by sdd does not capture any local structure  one might wonder whether such local structure is useful anymore. although it is no longer needed to reduce internal memory requirements  it is still useful in reducing time complexity. first of all  if a problem can be solved by sdd without edge partitioning  the time overhead of incremental node expansion can be avoided. if edge partitioning is used  then the more local structure  i.e.  the fewer successor nodes of an abstract node in the abstract graph   the fewer incremental expansions are needed before a node is fully expanded  and the overhead of incremental node expansion is reduced. the results in table 1 show that edge partitioning reduces the amount of internal memory needed in exchange for an increase in average running time  although the actual increase is still fairly modest.
1 application to delayed duplicate detection
so far  we have described how to use edge partitioning in sdd  where it improves scalability by reducing internalmemory requirements. in particular  it reduces the proportion of generated nodes that need to be stored in internal memory at any one time in order to ensure detection of all duplicates. as we now show  edge partitioning can also be used in a form of delayed duplicate detection  ddd  that uses local structure to reduce the delay between generation of nodes and eventual detection and removal of duplicates. this has the advantage of reducing the disk storage requirements of ddd. we begin with a review of ddd and then describe how edge partitioning can enhance its performance.
1 delayed duplicate detection
ddd alternates between two phases; successor generation and duplicate elimination. depending on how duplicates are eliminated  there are two forms of ddd  as follows.
sorting-based ddd
the first algorithms for external-memory graph search used sorting-based ddd  stern and dill  1; munagala and ranade  1; korf  1; edelkamp et al.  1 . sortingbased ddd takes a file of nodes on the search frontier   e.g.  the nodes in the frontier layer of a breadth-first search graph   generates their successors and writes them to another file without checking for duplicates  sorts the file of generated nodes by the state representation so that all duplicate nodes are adjacent to each other  and scans the file to remove duplicates. the i/o complexity of this approach is dominated by the i/o complexity of external sorting  and experiments confirm that external sorting is its most expensive step.
hash-based ddd
hash-based ddd  korf and schultze  1  is a more efficient form of ddd. to avoid the i/o complexity of external sorting in ddd  it uses two orthogonal hash functions. during node expansion  successor nodes are written to different files based on the value of the first hash function  which means all duplicates are mapped to the same file. once a file of successor nodes has been fully generated  duplicates can be removed from it. to avoid the overhead of external sorting in removing duplicates  a second hash function maps all duplicates to the same location of a hash table  which accomplishes by hashing what otherwise would require sorting. since the hash table corresponding to the second hash function must fit in internal memory  hash-based ddd has a minimum internal-memory requirement that corresponds to the largest set of unique nodes in any file. thus  this approach requires some care in designing the first hash function to make sure it does not map too many unique nodes to a single file.
　although hash-based ddd in its original form does not depend on  or leverage  the structure of a graph  an important improvement  called  interleaving expansion and merging   korf and schultze  1   does. it works as follows. the nodes on the search frontier are stored in multiple files  called  parent files   depending on the first hash function  and the successor nodes that are generated when the nodes in the parent files are expanded are also stored in multiple files  called  child files.  instead of waiting until all parent files at a given depth are expanded before merging any child files at the next depth to remove duplicates  a child file is merged as soon as all parent files that could possibly add successor nodes to it have been expanded. in other words  interleaving expansion and merging makes it possible to remove duplicates early. because ddd generates duplicates and stores them on disk before eventually removing them  the technique of  interleaving expansion and merging  reduces the amount of extra disk storage needed by ddd. in fact  in the best case  it can reduce the amount of extra disk storage need by ddd by a factor of b  where b is the average branching factor  although the actual reduction may be less.
　to allow  interleaving expansion and merging   the first hash function must be designed in such a way that it captures local structure in the search graph. in particular  a child file must only contain successor nodes generated from a small number of parent files. a generic hash function cannot be used for this since it will typically hash nodes uniformly across all files. instead  a problem-specific hash function that captures local structure must be designed. in the following  we explain how this enhancement of hash-based ddd exploits and  thus  depends on the local structure of a graph  and how it can further exploit edge partitioning.
1 edge partitioning in ddd
to see how edge partitioning can be used to improve the performance of hash-based ddd  we first consider how the kind of local structure exploited by  interleaving expansion and merging  is related to the kind of local structure exploited by sdd. as previously pointed out  hash-based ddd requires a problem-specific hash function  and the  interleaving expansion and merging  technique is only effective when this hash function maps nodes to files in such a way that the nodes in one file  the child file  are generated from nodes in only a small number of other files  its parent files . in fact  these relationships can be represented by an abstract state-space graph in which abstract nodes correspond to files  and an abstract edge is present when nodes in one file have successor nodes in the other file. this should make it clear that the first hash function used by hash-based ddd is actually a statespace projection function  and  for  interleaving expansion and merging  to be effective  this hash function should capture the same kind of local structure that is exploited by sdd. the following concept will help make this more precise.
definition 1 the predecessor-expansion scope of a child file for an abstract node y under a state-space projection function Π corresponds to the union of nodes in the parent files for abstract nodes  that is 
 
where is the set of predecessors of y in the abstract graph  and Π 1 y  is the set of nodes in the parent file for an abstract node y.
　an important property of the predecessor-expansion scope is that it is guaranteed to contain all stored predecessors of nodes in a child file  which leads to the following theorem.
theorem 1 merging duplicate nodes after expanding all nodes in the predecessor-expansion scope of a child file is guaranteed to eliminate all duplicates that could be generated for the child file.
　the concept of predecessor-expansion scope lets us identify the local graph structure needed by  interleaving expansion and merging in a principled way  and relate it to the kind of local structure exploited by sdd. for undirected graphs  they are exactly the same  since the set of predecessors of an abstract node always coincides with the set of its successors. for directed graphs  they may or may not be the same.
　this analysis also lets us specify a condition under which  interleaving expansion and merging  will not be effective  at least by itself. when the abstract graph is fully connected  the predecessor-expansion scope of any child file is the entire set of parent files. this means the earliest time a child file can be merged is when all nodes at the current depth have been expanded which preventsthe application of interleaving expansion and merging. we are now ready to show how edge partitioning allows  interleaving of expansion and merging  to be effective even in this case.
　the idea is to force nodes within the predecessorexpansion scope of a child file to generate successors only for that child file alone  without generating successor nodes for other child files at the same time. this can be achieved as follows. let y be the abstract node that corresponds to the child file that is the target of merging. to merge duplicate nodes in this file as early as possible  edge partitioning only uses operators to generate the successors of nodes in the parent files for abstract nodes.
　once all nodes in the parent files have generated their successors for this child file  merging can take place as usual. the advantage of edge partitioning is that it saves external memory by not generating  possibly duplicate  successors for any other child files before merging is performed. after merging duplicates in a child file  edge partitioning picks another child file as the next merging target  until all the child files have been merged. it can be shown that by the time the last child file is merged  edge partitioning must have used all the operators to generate all the successor nodes for the current depth. by doing so in an incremental way  it ensures that the local structure needed by the  interleaving expansion and merging  technique is always present.
　although we do not present empirical results for edge partitioning in ddd  our analysis helps to clarify the relationship between sdd and hash-based ddd with interleaving of expansion and merging. both exploit the same local structure of a graph  and thus the performance of both can be enhanced by using edge partitioning in a similar way.
1 conclusion
we have introduced a technique  called edge partitioning  that improves the scalability of structured approaches to externalmemory graph search by using a strategy of incremental node expansion to localize memory references in duplicate detection. results show that it significantly reduces the internal memory requirements of structured duplicate detection  sdd . moreover  it is guaranteed to be effective regardless of the structure of the graph  and this guarantees that sdd can be applied to any search problem. finally  we have shown that it can also be used to reduce the amount of disk storage needed by delayed duplicate detection.
　there are a number of directions for future work. one possibility is to vary the degree of incremental expansion. for example  instead of using one operator group at a time  edge partitioning can use multiple  but not all  operator groups to generate successor nodes at a time. if enough internal memory is available  this can reduce the overall number of  incremental  expansions. with edge partitioning  we now have two options to reduce the internal-memory requirements of sdd. we can either increase the granularity of the state-space projection function  or we can use edge partitioning. which option is better under what circumstances is an interesting question  and the answer is likely to help us understand how to best trade off internal-memory requirements with the number of disk i/o operations needed by sdd.
