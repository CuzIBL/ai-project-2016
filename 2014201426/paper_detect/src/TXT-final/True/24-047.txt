 
this paper discusses a radically new scheme of natural language processing called massively parallel memorybased parsing. most parsing schemes are rule-based or principle-based which involves extensive serial rule application. thus  it is a time consuming task which requires a few seconds or even a few minutes to complete the parsing of one sentence. also  the degree of parallelism attained by mapping such a scheme on parallel computers is at most medium  so that the existing scheme can not take advantage of massively parallel computing. the massively parallel memory-based parsing takes a radical departure from the traditional view. it views parsing as a memory-intensive process which can be sped up by massively parallel computing. although we know of some studies in this direction  we have seen no report regarding implementation strategies on actual massively parallel machines  on performance  or on practicality acecssment based on actual data. thus  this paper focuses on discussion of the feasibility and problems of the approach based on actual massively parallel implementation using real data. the degree of parallelism attained in our model reaches a few thousands  and the performance of a few milliseconds per sentence has been accomplished. in addition  parsing time grows only linearly  or sublincarly  to the length of the input sentences. the experimental results show the approach is promising for real-time parsing and bulk text processing. 
1. 	i n t r o d u c t i o n 
this paper presents a radically new scheme of natural language processing called massively parallel memory-based parsing. we will report the experimental results of our scheme actually implemented on massively parallel machines  and discuss the benefits and problems of the approach. specifically  we will examine its performance and memory requirements - we will show that parsing can be completed in a few milliseconds  and the memory requirement is within practical limits. 
　massively parallel memory-based parsing was inspired from ideas of the memory-based reasoning and case-based reasoning which place memory as the basis of reasoning. 
these paradigms are  by definition  memory-intensive and  
   *this work is supported in part by the pittsburgh supercomputing center under grant tra1p and iri-1p. 
usually assume the use of massivelly parallel machines for the implementation of practical systems. so far  some successful results on massively parallel implementation of memorybased reasoning systems have been published in such areas as word pronunciation  stanfill  1 and classification of census data  waltz  1; surprisingly  no report has been made on the application of the idea to natural language parsing with full syntactic and semantic analysis. the only studies we have today in this direction are the direct memory access parser  dmap   riesbeck and martin  1  dmtrans machine translation system  tomabechi  1   kitano et. al.  1  and the dmdialog speech-to-speech translation system  kitano  1. however  dmap and dmtrans are only implemented on very small scale  less than 1 words  simulated on serial machines  leaving the scalability and practicality open to question. dmdialog is larger  more than 1 words  and more sophisticated system  yet it was only recently that versions of the system was implemented on massively parallel machines  kitano and higuchi  1  kitano  et. al.  1 . also  we have not seen any detailed discussions on the memory-based or case-based parsing regarding implementation strategies and theoretical claims based on actual data. in this paper  we report the memory-based parsing model actually implemented on massively parallel computers  and discuss the viability of the approach using actual data. 
	we use three corpora of spoken utterances. 	these are: 
 1  atr  atr interpreting telephony research laboratories  conference registration task which contains 1 utterances and the vocabulary size of 1 extracted from simulated telephone conversations.  1  the darpa  defense advanced research project agency  resource management task contains 1 sentences and the vocaburaly size of 1.  1  the cnn prime news consists of 1 sentences. the cnn corpus is the real-world data from actual cable broadcasting. some of the sentences are very long  but can be segmented into two or three independent sentences  segmented data is referred as 'cnn segmented'  and the raw data is simply refered as 'cnn prime news' . 
1. 	memory-based parsing 
memory-based parsing was inspired by the memory-based reasoning paradigm proposed in  stanfill and waltz  1  and  stanfill and waltz  1 . the basic idea of memorybased reasoning places memory at the foundation of intelligence. it assumes that large numbers of specific events are stored in memory  and response to new events is handled by first recalling past events which are similar to the new input  and invoking actions associated with these retrieved 

events to handle the new input1. this idea runs counter to most ai approaches which place rules or hueristics as the central thrust of reasoning. for example  traditional parsing has been considered a rule-based or principle-based process which comprises serial rule application procedures. however  the massively parallel memory-based parsing makes a radical departure from the traditional view. it considers parsing 1 be a memory-intensive process which can be sped up by massively parallel computing. in the memory-based parsing model  parsing is viewed as a memory-search process which locates the past occurrence of similar sentences  and the interpretation is built by activating the past occurrence. we believe that memory and their associations plays the central role in many intelligence tasks. this view is similar to the idea of direct memory access parsing  riesbeck and martin  1   except that our model does not contain adaptation and refinement processeses required in case-based reasoning  and there is no consideration of a massively parallel implementation in the dmap model as it presently exists. actually  the main locus of the dmap is case-adaptation for parsing  and it does not pay much attention to parsing with large instances of sentences. however  the basic parsing process is quite similar  so by testing our model we can also examine the practicality of the dmap model for real-world tasks. 
   the first issue is performance. traditional parsing is a time consuming task. a few seconds or even a few minutes is required to complete the parsing of one sentence. the most efficient parsing algorithm known to date is tomiia's generalized lr parser which takes less than 1 n1  for most practical cases  tomita  1. in addition  the earlcy type algorithms degrade its performance as size of grammar increases  1 g1  . furthermore  extensive use of unification  which is a computationally expensive operation  in recent grammar theories substantially undermines the speed of processing. efforts to parallelize these traditional approachs have only a limited contribution to attain turely real-time parsing system. reasons for this include:  1  the level of parallelism attained by implementing a parallel version of the traditional parsing scheme is rather low   1  serial application of piecewise rules causes combinatorial explosion which leads to substantial performance degradation as input length gets longer and/or as size of grammar grow larger  and  1  unification is essentially a sequential operation which docs not gain much from parallelization. 
　in our model  serial rule application is eliminated and replaced by a parallel memory-search process  also  syntactic structure and interpretation are pre-indexed so that expensive unification operation is no longer necessary  or sufficiently minimized. this approach has not been taken in the serial machine because the approach will face a trade-off between improved efficiency due to pre-expansion and degradation due to an increase in search cost. since each instance of memory activation can be processed independently on a massively parallel machine  we expect that the level of parallelism will exceed at least 1 which will lead to a dramatic increase in performance. 
　　one of the major differences of our model from other memorybased reasoning models is that we do not use similarity-based memory matching. this is due to the lack of sound domain theory of similarity matching in parsing. a similarity-based matching is only possible when the syntactic pattern of the input sentence is very close  we do not have measurement of closeness itself  to one of the instances. however  if a syntactic structure is different  we can not make use of past instances. 
　other than the performance aspect  there are several problems which need to be examined in order to claim that memory-based parsing is a viable model. perhaps the most significant issue is the memory requirement. since the memory-based reasoning paradigm requires an extensive number of past instances to solve new problems  the critical issue is whether the number of necessary cases converges with a practical number in the given task domain. in natural language processing  the productivity of language dictates that human beings are capable of producing an infinite number of sentences. thus  if we store all the sentences which we could possibly encounter  we need either an infinite memory space  or finite but astronomical space. obviously  such a naive approach should be rejected. the approach we do take is to use abstract instances of sentences. for example  instead of storing sequences of words  we store sequences of syntactic 
categories such as or semantic grammar templates such as {agent want-to attend event . by using abstract templates  the memory space required to cover the task domain will be significantly reduced 1. although theoretically  an infinite number of syntactic patterns need to be stored to process all possible sentences  a finite number of syntactic patterns can cover a fairly significant percentage  say 1%  of possible input sentences when the length of the sentence has a specific upper-boundary1. still  it is open to question whether or not the necessary number of instances converges within a practical size. observation of convergence of the coverage by a finite number of syntactic patterns is one of the major purposes of the experiments in this paper. 
1. experimental i m p l e m e n t a t i o n 
this section describes the implementation used in the experiments in this paper. it should be understood that the idea of memory-based parsing is new and that it is in the early stages of development. thus the specific implementation described here should be regarded as an example of implementation  not the definitive implementation of the memory-based parser. in fact  we will discuss some enhancements later. the experimental implementation has two major parts: a massively parallel associative processor ixm1 and a memory-based parser implemented on the ixm1. 
1. 	the massively parallel associative processor ixm1 
ixm1 is a massively parallel associative processor designed and developed at the electrotechnical laboratory  higuchi et. al.  1 . it is dedicated to semantic network processing using marker-passing. 
　ixm1 consists of 1 processors  called associative processors  which operate with associative memory  each of which has a memory capacity of 1k words by 1 bits. each associative processor is connected to other associative processors through network processors. the structure of the ixm1 is shown in figure 1. 
　an associative processor consists of an ims t1 transputer  1 associative memory chips  ram  link adapters  and 
   1 an alternative approach is to store a large set of sentences in its surface sequence  and use similarity-based matching to cover unknown inputs. see  sumita and iida  1  for such an approach. 
   1 in the section 1  we will demonstrate that we approximately limit the maximum length of sentences in the spoken dialogues. 


figure 1: structure of the ixm1 associative memory pro-


figure 1: overall architecture of the syntactic recognition part 

cessor 
associated logic. when operated at 1 mhz clock  t1 attains 1 mips  inmos  1 . each associative memory chip is a 1 kbit cam  1 words x 1 bits  manufactured by ntt. the ixm1 has 1 such processors  thus attaining 1k parallelism which is far larger than 1k parallel of the connection machine  hillis  1 . this high level of parallelism allows us to implement practical memory-based systems. the design decision to use associate memory chips driven by 1 bit cpus  instead of having thousands of 1 -bit cpus  is the major contributing factor for performance  processor efficiency  and cost performance. 
1. 	organization and algorithm of the parser 
we describe the organization and algorithm of the memorybased parser on the ixm1. as an experimental implementation designed to test the practicality of the approach  we employed a flat memory structure  i.e. no hierarchy was used to encode syntactic patterns. this is because the flat structure is the most memory-intensive way of implementing the memory-based parsing model. thus  should this implementation be judged to be practically useful  other versions which use a more memory-efficient implementation can also be judged to be practical. 
　the system consists of two parts: a syntactic recognition part on the 1xm1 and a semantic interpretation part on the host computer. 
　for the syntactic recognition part on the ixm1  the overall architecture is shown in figure 1. the memory consists of three layers: a lexical entry layer  a syntactic category layer  and a syntactic pattern layer. 
lexical entry layer: the lexical entry layer is a set of nodes each of which represents a specific lexical entry. most of the information is encoded in lexical entries in accordance with modern linguistic theories such as hpsg pollard and sag  1   and the information is represented as a feature structure. it is a straightforward task to represent huge numbers of lexical entries on the ixm1. 
syntactic category layer: the second layer comprises a group of nodes representing the syntactic features. perhaps the most important feature for parsing is the head major category  generally known as the syntactic cate-

table 1: a part of pre-expanded syntactic structures  simplified  
gory. in the specific implementation examined in this paper  we use the head major category as a single feature to index syntactic structures. however  it is also possible to incorporate other features to index syntactic structures. the choice of features to be incorporated largely depends on the design decision on how much the constraint checks to be conducted on each processor or on the host computer. 
syntactic patterns layer: all possible syntactic structures are directly mapped onto the associative memory as a syntactic patterns layer. as mentioned earler  the syntactic structure is a flat sequence of syntactic categories which can be generated from the given grammar or from a corpus of training sentences. table 1 shows a part of simple syntactic structure loaded on the associative memory. grammatical constraints can be incorporated when expanding grammar rules. it allows for a recursive structure so that the number of actual syntactic structures loaded is less than the actual number of syntactic patterns the system can accept. 
   the degree of constraints which are incorporated in the expanded syntactic structures largely affects the memory requirements and the processing load on the host processor. if only the head major category is incorporated  most constraint checks must be done by the host computer or at the transputer. on the other hand  if all constraints are incorporated in expanding grammar  the number of possible syntactic structures will be explosive and it will require far more associative memory chips. in this experiment  we only used the head major category  such as noun  verb   thus most constraint processing is done at each transputer and at the host processor. it is also possible to use more subdivided symbols at the cost of memory requirements. 
　in the host computer  sun-1   the case-role binding table is pre-compiled which indicates correspondence between case-roles and word positions. table 1 shows a part of a simple case-role binding table. each position in the table is associated with actions to be taken in order to build meaning representation. in building the meaning representation  the program resides on the host computer and carries out 


table 1: case-role table  simplified  
role-bindings and some constraint checks depending on how the constraints are incorporated into the syntactic recognition part. if there are ambiguous parses  more than two items in the table need to be processed. however  it should be noted that all items which are notified from the ixm1 are already known to be accepted parsing hypotheses as far as syntactic structure is concerned. this architecture drastically minimizes the number of operations required for parsing by 
eliminating operations on parses which turn out to be false. 
　the algorithm is simple. two markers  activation markers  a-markers  and prediction markers  p-markcrs  are used to control the parsing process. a-markers are propagated through the memory network from the lexical items which are activated by the input. p-markers are used to mark the next possible elements to be activated. a general algorithm follows: 
1. place p-markers at all first elements of the syntactic patterns. 
1. activate the lexical entry. 
1. pass the a-marker from the lexical entry to the syntactic category node  scn . 
1. pass the a-marker from the scn to the elements in the syntactic patterns. 
1. if the a-marker and a p-marker co-exist at an element in the syntactic pattern  
then the p-marker is moved to the next element of the syntactic pattern. 
1. if there arc no more elements  the syntactic pattern is temporarily accepted  and a pattern id is sent to the host or local processors for semantic interpretation. 
1. repeat 1 thru 1  until the end of the sentence. 
　on the host computer or on the 1 t1 transputers  the semantic interpretation is performed for each hypothesis. the general flow follows: 
1. receive the syntactic pattern id from the syntactic recognition part. 
1. if words remain in the sentence  then ignore the id received. 
1. if no words remain  perform semantic interpretation by executing the functions associated with each hypothesis in the table. most operations are reduced to a bit-marker constraint check and case-role bindings at compile time. 
1. performance 
we carried out several experiments to measure the system's performance. figure 1 shows the syntactic recognition time against sentences of various lengths. syntactic recognition 

figure 1: syntactic recognition time vs. sentence length 

figure 1: comparison of syntactic recognition time 
at milliseconds order is attained. this experiment uses a memory containing 1 syntactic patterns. on average  1 syntactic patterns are loaded into each associative processor. processing speed improves as parsing progresses. this is because the computational costs for a sequential part in the process is reduced as number of hypotheses activated decreases. there is one sequential process which checks active hypothese on each 1 transputer. during this process  the parallelism of the total system is 1. discussion of processor loading factor will be given in section 1. 
   it should be noted that this speed has been attained by extensive use of associative memory in the ixm1 architecture - simple use of 1 parallel processors will not attain this speed. in order to illustrate this point  we measured the performance of the sun-1  cm-1 connection machine  and cray xmp with only 1 syntactic patterns which is equivalent to a single processor of the 1xm1. the program on each machine uses an optimized code for this task in c language. the experimental results are drawn on figure 1. the ixm1 is almost 1 times faster than that of the sun-1 and cray x-mp even with such a small task1 the cm-1 connection 
machine is very slow due to a communication bottleneck between processors. while both the ixm1 and the sun-
　　1 cray x-mp is very slow in this experiment mainly due to its sub-routine call overhead. we have tested this benchmark on a cary x-mp in japan and at the pittsburgh supercomputing center  and obtained the same result. thus this is not hardware trouble or other irregular problem. 


table 1: syntactic recognition time vs. grammar size  milliseconds  

figure 1: distribution by sentence length 
1 use a cpu of comparable speed  the superiority of the ixm1 can be attributed to its intensive use of the associative memory which attains a massively parallel search. 
next we examine the scaling property of both systems. 
figure 1 shows the performance for a sentence of length 1  for syntactic patterns of size 1 and 1. while a single processor of the 1xm1 maintains less-than-linear degradation  the sun1 and cray x-mp degrades more than linearly. it should be noted that 1 syntactic patterns in other machines literally means 1 patterns  but in the single processor in the ixm1  it means 1 patterns when all 1 processors are used. 
   it is expected that the larger task set would demonstrate a dramatic difference in total computation time. the ixm1 can load more than 1 syntactic patterns which is sufficient to cover the large vocabulairy tasks currently available for speech recognition systems. with up-to-date associative memory chips  the number of syntactic patterns which can be loaded on the ixm1 exceeds 1. also  extending the ixm1 architecture to load over one million syntactic patterns is both economically and technically feasible. 
1. 	m e m o r y requirements 
while the high performance of memory-based parsing on a massively parallel machine has been clearly demonstrated  now we look into its memory requirement. it is well acknowledged that the productivity or language dictates that we can produce an infinite number of sentences. even if we consider only vaild syntactic patterns  it will be infinite when no restriction has been applied. obviously  we can not create and encode an infinite number of syntactic patterns. to advocate the memory-based parsing  we need to demonstrate that  in 
practice  the number of syntactic patterns actually used is finite  or it can be approximated by a finite number of syntactic patterns. 
　it should be noted that an infinite number of sentences can be produced when any of three assumptions stands: 

figure 1: coverage by sentence length 
assumption 1: infinite vocabulary 
assumption 1: infinite grammar rules 
assumption 1: infinite sentence length 
   in the other words   1  finite vocabulary   1  finite grammar rules  and  1  finite sentence length are the conditions for finite productivity of language. however  assumption 1 and 1 can be ignored since most of parsers only have finite vocabulary and grammar at run time. also  people do not acquire infinite vocabulary and grammar rules at a point in the dialogue. increase in vocabulary and grammar are more long term effects. what makes our model different from the traditional models depends upon whether the third assumption stands or not. if the third assumption is false  our model is  at least  equivalent to traditional models in its sentence productivity. therefore  we counted the number of sentences of each length. surprisingly  the atr and darpa corpora show very similar characteristics  figure 1 : both have a peak of sentences between 1 to 1 words long. cnn has longer sentences  though it too has peaks in 1 to 1 words length. the atr sentences' maximum length was 1 words and that of the darpa corpus was 1. cnn prime news was 1  and cnn segmented was 1. 1% of sentences from atr  darpa  and cnn segmented corpus are less than 1 words length. from this data  we can expect that most sentences are within manageable length. thus  we conclude that in practice sentence which can be produced at a given time point is finite. 
　next we examine that if  in practice  the number of syntactic structures which appear in the given task domain will saturate at a certain number. empirical observation using a corpus taken from the darpa task shows that it does converge when it is in a restricted domain  figure 1 . however  the number of syntactic patterns necessary to cover the task domain was 1 with the flat structure  and it was reduced to 1 with a simple hierarchical network. since ixm1 is capable of loading over 1 syntactic patterns  the model is capable of covering the task even with the flat memory approach  and much wider domain can be covered with hierarchical model. however  a larger scale of experiment will be necessary to see if the number of syntactic patterns saturates  and where it saturates. we are currently investigating this issue using a large corpus from real world data such as cnn. 
　independently  we have carried out an experiment to cover a given domain based on syntactic patterns pre-expanded from a set of grammar rules. we pre-expanded syntactic patterns from a set of context-free rules to see the memory requirements. a set of 1 basic grammar rules will produce about 1 patterns when the maximum length is 1 words  and 


figure 1: training sentences vs. syntactic patterns 
about 1 patterns when the maximum length is 1 words. however  this has been reduced to 1 by using local networks which handle noun-noun modifications  adjective-noun modifications  etc. thus  by imposing additional constraints  pre-expansion of syntactic patterns from a set of grammar rules is also feasible  and can be loaded on ixm1. in addition  it should be noted that not all syntactic patterns are actually used in the real world  thus the number of syntactic patterns that we really need to load on the machine would be far smaller. psycholinguistic study shows that there is an upper-bound in the complexity of sentences which people can process igibson  1. the hypothesis that the number of syntactic patterns that actually appears in the given task is relatively small can be independently confirmed. nagao  nagao  1  reported that syntactic patterns appeared in the title of over 1 scientific papers were around 1  and it was reduced to just 1 with simple reduction rules. while we can only confirm our hypothesis on the basis of our experiments on the small and medium size domains  increasing availability of large memory space and large number of processors provided by massively parallel machines offers a realistic opportunity that massively parallel memory-based parsing can be deployed practical tasks. 

figure 1: number of active hypotheses per processor 
abstraction and hierarchical networks would undermine performance and quality of translation  because it would be close to rule-based systems. use of abstraction and hierarchical networks should be minimized and should be used only when sufficient memory space was not allocated to cover a wide range of input sentences. 
1. 	enhancement i i : linguistic knowledge 
while we arc currently using a relatively simple syntactic constraint and semantic interpretation mechanism using the caserole table  there is room for improvements. first  the syntactic constraints can be compiled as a large finite-state network so that some additional mechanisms using marker-passing can effectively incorporate syntactic constraints. second  instead of using the case-role table  we can load functional descriptions and partially build f-structurcs so that only lexical insertion will be performed to complete final f-structure  levin and gates; personal communication . most heavy operations such as unification are performed in compile time to create a partial f-structure  and only a minimal operation is left to run time. with this approach  the unification operation will be minimized  if not eliminated  for the number of words in the sentence. 

1. discussions 
1. 	enhancement i: hierarchical memory network 
use of the hierarchical memory network model reduces memory requirements by layering the levels of abstractions incorporated in the memory  but at the cost of building such networks which is not a trivial task. figure 1 shows an example of the memory saving effect of the hierarchical memory network. by incorporating levels of abstractions  such as surface sequences  generalized cases  and syntactic rules  the memory requirements can be reduced significantly without undermining the benefits of the memory-based approach. they also exemplify an extended notion of the phrasal lexicon becker  
1 . actually  this model has been implemented in the dmdialog speech-to-speech dialogue translation system  and has been proven to be useful for spoken language understanding systems. we have implemented a memory-based translation system using hierarchical memory network  and attained the milliseconds performance  kitano and higuchi  1   also   kitano  et. al.  1  reports the similar idea has been implemented on the semantic network array processor  snap   moldovan et. al.  1   and obtained a comparable performance. it should be cautioned  however  that excessive 
1. 	hardware architecture for memory-based parsing 
first  the ixm1-type associative memory machine has the advantage in cost performance. in the ixm1  the associative memory stores syntactic patterns  and it allows for a massively parallel search  the current implementation allows for a parallel search up to 1k   while limiting the number of processors to 1. 
   figure 1 shows the number of active hypotheses per one associative processor. although it starts with a high processing load where a significant percentage of hypotheses arc activated  the number of hypotheses decreases drastically as the processing proceeds. since the ixm1 uses associative memory chips to store syntactic patterns  no processor will be idle unless all the hypotheses assigned to the processor are eliminated. however  in other massively parallel machines that assign processors to all the hypotheses  most of the processors will be idle because most of the hypotheses will be eliminated as the processing progresses. since the use of associative memory chips would be far cheaper than processor chips to store and carry out the operations necessary in the implementation in this paper  the ixm1's architecture would be more cost effective tnan other architectures for this task. 
　although there are cases that performance and functionalities benefit from assigning one processor for one hypothesis   such cases involve complex calculations of probabilistic measures  a source of activation check  and dynamic reconfiguration of the network  the associative memory-based architecture as seen in the ixm1 suffices for many memory-based reasoning tasks. in most memory-based reasoning tasks  similarity matching uses relatively simple similarity measures from numeric computations which can be computed on associative memory. even in cases which require complex computations  the ixm1 is expected to maintain high performance with better cost effectiveness  because a sixty four 1 bit cpu can distributively perform higher-levels of symbolic and numeric operations. thus  the ixm1's architecture which we advocate in this paper is a cost effective architecture not only for the memory-based parser  but also for more general memory-based reasoning systems. 
1. 	conclusion 
in this paper  we proposed a massively parallel memory-based parsing. we have shown  using data obtained from our experiments  that the massively parallel memory-based parsing is a promising approach to implement a high-performance reallime parsing system for certain task domains. major claims and observations made by our experiments are: 
  the massively parallel memory-based parsing attains real-time parsing when implemented on a massively parallel machine. our experiments using the 1xm1 associative memory processor shows that syntactic recognition is completed in less than 1 milli-seconds. the system not only attains milli-second order parsing performance  but also exhibits a desirable scaling property. the parsing time grows only linearly  or sublinearly  to the size of the inputs  also  the parsing time required grows only sublincarly to the number of syntactic patterns loaded. this scaling property is the real benefit of using a massively parallel machine. 
  massively parallel memory-based parsing  even in its simplest form  can be implemented within practical memory and processor requirements  when designed for suitable task domains. our observation from spoken language corpora demonstrates that the length 1 sentences converges within a managablc length  and the number of syntactic patterns also converges into a practical scale. this enables us to implement the memory-based parsing model using massively parallel machines that already exist. with the possible development of larger scale massively parallel machines such as the one targeted by darpa for teraops by 1  waltz  1   the possibility of the large scale massively parallel memory based parsing would be within sight. 
  the ixm1's architecture is cost effective for memory-based reasoning tasks. the use of associative memory to attain a high level of parallelism  1k  in the current implementation  allows us to build a practical memorybased system with possibly lower resource requirements. while only a few parts of memory are actually involved in solving a specific problem  most processors could be idled when a full fine-grained processor architecture is used. the ixm1 is one of the ideal and cost effective architectures for building practical and large-scale memory-based systems. 
acknowledgements 
we would like to thank the members of the center for machine 
translation at carnegie mellon university  particularly jaime 
1 	natural language 
carbonell and masaru tomita  and the members of electrotechnical laboratory for discussions and support. also  we would like to thank hitoshi lida and akira kurematsu at the atr interpreting telephony research laboratories for allowing us to use their corpus  and for their continuous encouragement. 
