panel: artificial intelligence and legal responsibility 
margaret a. boden - university of sussex 
panellists: margaret a. boden  chairman ; yorick wilks; 
marshal s. w i l l i c k ; jay bloombecker  susan nycum  robert kowalski 

　　　president truman's famous remark -  the buck stops here  - was clearly correct. i t ' s much less clear where the buck stops when one of the elements  i almost wrote  people   in the chain of responsibility is an al-program. 
there are two broad questions to be asked. f i r s t   to what extent   i f any  can the making of - and the responsib i l i t y for - a given judgement or decision  or mistake he attributed to a  computer-program  and second  supposing that responsibility cannot be attributed to a program  with whom does it lie  who is legally responsible for what a program does  i t s user  person or i n s t i t u t i o n   professional or c l i e n t     i t s programmers  alive or dead   the domain-experts who provided the knowledge-base . . . who  
　　　in the past  the law has derided the notion that one might apply psychological predicates to a machine. significantly  perhaps  this derision has sometimes resulted in a person's escaping responsibility for a clearly dishonest  and prima facie i l l e g a l   action. for example  on january 1th 1  the london times reported a case in which the defendant was found not g u i l t y on the grounds that  machines cannot be deceived . a motorist had avoided paying a car-park fee  by manually l i f t i n g the e x i t - b a r r i e r instead of putting money in the exit-machine. his defense counsel said  the p l a i n t i f f has to be aware that deception has taken place for this case to be proved. it is impossible to deceive a machine as it has no mind and consequently cannot be aware of the deception as a car park attendant might . this argument was accepted by the bench. in dismissing the case  and seven other similar cases   the chairman said  someone has got to be deceived in a case like t h i s   but here this was not so.  the penny-pinching motorist got o f f   because the magistrates ruled that it was in principle impossible to apply psychological categories to machines. 
     but car-park machines are different from powerful computers  and l i f t i n g a sixpenny barrier is different from giving a medical diagnosis  or advizing where to d r i l l for o i l . is this  no-nonsense  judgment of january 1 a useful precedent for the sorts of legal complications that are l i k e l y to arise with the increasing public use of complex al-systems  
     hackers and laymen alike constantly refer to programs - and a f o r t i o r i to al-programs - in psychological terms. 
we speak of t h e i r reasoning judgments  evidence   knowledge  ignorance and mistakes. we speak of what they are trying to do  and what p r i o r i t i e s are guiding their decisionss is this simply sentimentality  a sloppy way of speaking which can and should be avoided - above a l l   in the law courts  if it is not  if people as a matter of fact do not or cannot avoid using such terms in conceptualizing al-systems  then 
what implications follow  if we are allowed to use some psychological words 
when describing al-programs  why not a l l   if we use the language of knowledge and inference  and even of choice  then 
why not the language of purpose  e f f o r t - and even blame  
     these questions are the focus of the f i r s t two speakers on the panel  yorick wilks and marshal w i l l i c k . the ascription of legal responsibility already varies depending on the  personal  category of the putative offender: states  companies  individuals  the sane  the insane  children  pets  wild animals  servants  and agents. what about computer programs  
     current i n t u i t i o n s about this question may seem absurd in a few years' time  when people are more used to al-applications. some of us may already feel uneasy with the judgment that  machines cannot be deceived . if one wishes to prevent people from w i l f u l l y feeding false 

1 	m.boden 
information to a computerised system should a person or i n s t i t u t i o n be found  or a legal f i c t i o n invented  to suffer  sic  the deception  or should we be willintj to grant that machines can be deceived  though maybe not disappointed  sherry turkle  in her recent book the second self  reports that young children growing up in today's computer-culture spontaneously ascribe cognitive concepts  such as knowledge  intelligence  deciding  and mistake  to computers. they also use some conative concepts  like purpose  goal wanting  t r y i n g   and f a i l i n g     at least in the context of problem-solving  on the computer's part. but they adamantly refuse to use affective concepts  such as feeling and emotion   and they also j i b at such motivational concepts as caring  and the l i k e . indeed  the c h i l d ' s concept of 
what it is to be  alive  is apparently changing  so that affective and conative concepts are stressed at the expense of  mere  cognition. does this imply that the l i t i g a n t s of tomorrow w i l l allow that computers can make mistakes  but cannot t r u l y have intentions  
　　　among the intentions which human beings harbour - and not only in car-parks - are some which are criminal. the t h i r d panelist  jay bloombecker  discusses a range of examples taken from the current case-law on computer crime. he relates these to some r e l a t i v e l y novel issues that may arise  once 
  f i f t h generation  systems are available. when dealing with programs capable of some degree of  autonomous  reasoning  both crime-detection and the ascription of responsibility are l i k e l y to be even more d i f f i c u l t than they are today. 
　　　an enormous amount of l i t i g a t i o n   at least in the usa  concerns medical issues. clearly  legal problems w i l l arise in connection with the use  and misuse  and even non-use  of medical expert systems. various l o c i of responsibility seem prima facie to be possible: the doctor who uses the system; the patient who knows this is happening  caveat emptor  ; the hospital administration; the programmer/s; the specialist physician who supplied the relevant diagnostic or prescriptive rules in the f i r s t place; the author of the textbooks used. many of these individuals may already be dead. but  as norbert wiener pointed out   old programs never d i e   ; could a doctor or hospital be sued for relying on an old out-of-date program  could they be sued for not using any program at a l l   the fourth panellist  susan nycum  considers some of the legal problems l i k e l y to dog applications of ai in the medical domain. 
　　　finally  bob kowalski contributes some thoughts on how  legal  expert systems might be used. his own work includes the building of a system which incorporates the b r i t i s h nationality laws  a prime late-twentieth-century example of baroque a r t   . what implications  if any  does this project have for the individual and society  arguably  it would be an improvement on current practices to have nationality-decisions computerized. for a program cannot be affected by  turkle's subjects would say  it does not care about  anyone's skin colour or physiognomy  or their manner of dress or speech. and arguably  the c l a r i t y of the programmed rules might help make clear any basic injustices in the programmed laws themselves: to change the world one has f i r s t to understand i t . but where would responsibility l i e if misclassification occurred  should kowalski s t a r t saving his pennies  in anticipation of his defense costs in the legal suits of the 'nineties  
　　　and what about the legal implications of other legal or quasi-legal programs  if a program searching for precedents in case-law does not have analogical reasoning powerful enough to find the right one  to whom could the defendant complain  if governmental and other i n s t i t u t i o n s formulate policies based on legal  decisions  made by in-house programs  who is to know  who is to care  and what can be done  
　　　the panel promises many questions. as for answers  those are more elusive. but since the panellists include both specialist ai-practitioners and professional attorneys who have already concerned themselves with these questions we can expect a l i v e l y and informed discussion. 
