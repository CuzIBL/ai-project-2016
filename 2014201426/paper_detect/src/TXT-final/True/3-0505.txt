
this paper investigates the automatic evaluation of text coherence for machine-generated texts. we introduce a fully-automatic  linguistically rich model of local coherence that correlates with human judgments. our modeling approach relies on shallow text properties and is relatively inexpensive. we present experimental results that assess the predictive power of various discourse representations proposed in the linguistic literature. our results demonstrate that certain models capture complementary aspects of coherence and thus can be combined to improve performance.
1 introduction
the use of automatic methods for evaluating machinegenerated text is quickly becoming mainstream in machine translation  text generation  and summarization. these methods target various dimensions of text quality  ranging from sentence grammaticality to content selection  bangalore et al.  1; papineni et al.  1; hovy and lin  1 . however  a fundamental aspect of text structure - its coherence - is still evaluated manually.
　coherence is a property of well-written texts that makes them easier to read and understand than a sequence of randomly strung sentences. although the same information can be organized in multiple ways to create a coherent text  some forms of text organization will be indisputably judged incoherent. the automatically-generated summary in table 1 is an example of a badly organized text: the presence of thematically unrelated sentences and non-canonical time sequences makes it nearly impossible to comprehend. we want to develop an automatic method that can distinguish such a summary from a text with no coherence violations.
　this paper focuses on local coherence  which captures text organization at the level of sentence to sentence transitions  and is undoubtedly necessary for global coherence. this topic has received much attention in the linguistic literature  grosz et al.  1; morris and hirst  1; halliday and hasan  1  and a variety of qualitative models have been proposed. the emphasis of our work is on quantitative models of local coherence that can be efficiently computed from raw text  and readily utilized for automatic evaluation. our approach relies on shallow text properties that can be easily identified and
newspapers reported wednesday that three top libyan officials have been tried and jailed in the lockerbie case. secretary-general kofi annan said wednesday that he may travel to libya next week in hopes of closing a deal. the sanctions were imposed to force libyan leader moammar gadhafi to turn the men over. louis farrakhan  the leader of a u.s. muslim group  met with gadhafi and congratulated him on his recovery from a hip injury.
table 1: a low-coherence summary
quantified without recourse to elaborate world knowledge and handcrafted rules.
　the coherence models described in this paper fall into two broad classes that capture orthogonal dimensions of entity distribution in discourse. the first class incorporates syntactic aspects of text coherence and characterizes how mentions of the same entity in different syntactic positions are spread across adjacent sentences. inspired by centering theory  grosz et al.  1   our algorithm abstracts a collection of coherent texts into a set of entity transition sequences  and defines a probabilistic model over their distribution. given a new text  the model evaluates its coherence by computing the probability of its entity transitions according to the training data.
　the second  semantic  class of models quantifies local coherence as the degree of connectivity across text sentences. this approach is motivated by the findings of halliday and hasan  who emphasized the role of lexical cohesion in text coherence. the key parameter of these models is the definition of semantic relatedness  as a proxy for connectivity  among text entities. we explore a wealth of similarity measures  ranging from distributional to taxonomy-based  to find an optimal lexico-semantic representation for the coherence assessment task.
　we employ our models to evaluate the coherence of multidocument summaries produced by systems that participated in the document understanding conference. we acquire coherence ratings for a large collection of machine-generated summaries by eliciting judgments from human subjects  and examine the extent to which the predictions of various models correlate with human intuitions. our experiments demonstrate that while several models exhibit a statistically significant agreement with human ratings  a model that fuses the syntactic and the semantic views yields substantial improvement over any single model. our contributions are twofold:
modeling we present a fully automatic  linguistically rich model of local coherence that correlates with human judgments. this model is the first attempt  to our knowledge  to automatically evaluate the local coherence of machinegenerated texts.
linguistic analysis we present experimental results that assess the predictive power of various knowledge sources and discourse representations proposed in the linguistic literature. in particular  we show that certain models capture complementary aspects of coherence and thus can be combined to improve performance.
1 related work
most of the work on automatic coherence evaluation has been done in the context of automatic essay scoring. higgins et al.  develop a system that assesses global aspects of coherence in student essays. they use a manually annotated corpus of essays to learn which types of discourse segments can cause breakdowns in coherence. other approaches focus on local coherence. miltsakaki and kukich  manually annotate a corpus of student essays with entity transition information  and show that the distribution of transition types correlates with human grades. foltz et al.  propose a model of local coherence that presupposes no manual coding. a text is considered coherent if it exhibits a high degree of meaning overlap between adjacent sentences. they employ a vector-based representation of lexical meaning and assess semantic relatedness by measuring the distance between sentence pairs. foltz et al. report that the model correlates reliably with human judgments and can be used to analyze discourse structure. the success of this approach motivates our research on semantic association models of coherence.
　previous work has primarily focused on human authored texts and has typically utilized a single source of information for coherence assesment. in contrast  we concentrate on machine generated texts and assess which knowledge sources are appropriate for measuring local coherence. we introduce novel models but also assess whether previously proposed ones  e.g.  foltz et al.   generalize to automatically generated texts. as a byproduct of our main investigation  we also examine whether humans can reliably rate texts in terms of coherence  thus undertaking a large-scale judgment elicitation study.
1 models of local coherence
in this section we introduce two classes of coherence models  and describe how they can be used in automatic evaluation. we motivate their construction  present a corresponding discourse representation  and an inference procedure.
1 the syntactic view
linguistic motivation centering theory  grosz et al.  1; walker et al.  1  is one of the most influential frameworks for modeling local coherence. fundamental in centering's study of discourse is the way entities are introduced and discussed. the theory asserts that discourse segments in which successive utterances mention the same entities are more coherent than discourse segments in which multiple entities are discussed. coherence analysis revolves around patterns of local entity transitions which specify how the focus
1.  former chilean dictator augusto pinochet o  was arrested in  london x on  1 october x 1.
1.  pinochet s  1  was recovering from  surgery x.
1.  the arrest s was in  response x to  an extradition warrant x served by  a spanish judge s.
1.  pinochet o was charged with murdering  thousands o  including many  spaniards o.
1.  pinochet s is awaiting  a hearing o   his fate x in  the balance x.
1.  american scholars s applauded the  arrest o.
table 1: summary augmented with syntactic annotations for entity grid computation.

table 1: an entity grid
of discourse changes from sentence to sentence. the key assumption is that certain types of entity transitions are likely to appear in locally coherent discourse. centering also establishes constraints on the linguistic realization of focus  suggesting that focused entities are likely to occupy prominent syntactic positions such as subject or object.
discourse representation to expose entity transition patterns characteristic of coherent texts  we represent a text by an entity grid. the grid's columns correspond to discourse entities  while the rows correspond to utterances  see table 1 . we follow miltsakaki and kukich  in assuming that an utterance is a traditional sentence  i.e.  a main clause with accompanying subordinate and adjunct clauses .
　grid columns record an entity's presence or absence in a sequence of sentences  s1 ... sn . more specifically  each grid cell represents the role rij of entity ej in a given sentence si. grammatical roles reflect whether an entity is a subject  s   object  o   neither  x  or simply absent  - . table 1 illustrates an entity grid constructed for the text in table 1. since the text contains six sentences  the grid columns are of length six. as an example consider the grid column for the entity arrest   - - s - - o . it records that arrest is present in sentence 1 as a subject and in sentence 1 as an object  but is absent from the rest of the sentences.
　ideally  each entity in the grid should represent an equivalence class of coreferent nouns  e.g.  formerchileandictatoraugustopinochet and pinochet refer to the same entity . however  the automatic construction of such an entity grid requires a robust coreference tool  able to accurately process texts with coherence violations. since coreference tools are typically trained on coherent texts  this requirement is hard to satisfy. instead  we employ a simplified representation: each noun in a text corresponds to a different entity in the grid. the simplification allows us to capture noun-coreference  albeit in a shallow manner - only exact repetitions are considered coreferent. in practice  this means that named entities and compound nouns will be treated as denoting more than one entity. for instance  the np formerchileandictatoraugusto pinochet will be mapped to three entities: dictator  augusto  and pinochet. we further assume that each noun within an np bears the same grammatical role as the np head. thus  all three nouns in the above np will be labeled as objects. when a noun is attested more than once with a different grammatical role in the same sentence  we default to the role with the highest grammatical ranking  i.e.  s   o   x .
　entity grids can be straightforwardly computed provided that an accurate parser is available. in the experiments reported throughout this paper we employed collins'  state-of-the-art statistical parser to identify discourse entities and their grammatical roles. entities involved in passive constructions were identified using a small set of patterns and their corresponding deep grammatical role was entered in the grid  see the grid cell o for pinochet  sentence 1  table 1 .
inference a fundamental assumption underlying our inference mechanism is that the distribution of entities in coherent texts exhibit certain regularities reflected in the topology of grid columns. grids of coherent texts are likely to have some dense columns  i.e.  columns with just a few gaps such as pinochet in table 1  and many sparse columns which will consist mostly of gaps  see london judge in table 1 . one would further expect that entities corresponding to dense columns are more often subjects or objects. these characteristics will be less pronounced in low-coherence texts.
　we define the coherence of a text t  s1 ... sn  with entities e1...em as a joint probability distribution that governs how entities are distributed across document sentences:
	pcoherence t = p e1...em;s1...sn 	 1 
we further assume  somewhat simplistically  that an entity is selected into a document independently of other entities:
m
	pcoherence t  「 ’p ej;s1...sn 	 1 
j=1
to give a concrete example  we will rate the coherence of the text in table 1 by multiplying together the probabilities of the entities dictator  augusto  pinochet  etc.
　we define p ej;s1...sn  as a probability distribution over transition sequences for entity ej across all n sentences of text t. p ej;s1...sn  is estimated from grid columns  as observed in a corpus of coherent texts: p ej;s1...sn  = p r1 j ...rn j  n
= ’ p ri j|r1 j ...r i 1  j 
i=1	 1  n
「 ’ p ri j|r i h  j ...r i 1  j  i=1
where ri j represents the grammatical role for entity ej in sentence i  see the grid columns in table 1 . the estimates for p ri j|r1 j ...r i 1  j  are obtained using the standard markov assumption of independence  where h is the history size. assuming a first-order markov model  p pinochet;s1...s1  will be estimated by multiplying together p o   p s|o   p -|s   p o|-   p s|o   and p -|s   see the column for pinochet in table 1 .
　to compare texts with variable lengths and entities  the probabilities for individual columns are normalized by column length  n  and the probability of the entire text is normalized by the number of columns  m :
	1	m	n
pcoherence t  「 ‘‘logp ri j|r i h  j ...r i 1  j   1  m，n j=1i=1
　once the model is trained on a corpus of coherent texts  it can be used to assess the coherence of unseen texts. texts that are given a high probability pcoherence t  will be deemed more coherent than those given low probability.
　notice that the model is unlexicalized  i.e.  the estimation of p ej;s1...sn  is not entity-specific. in practice  this means that entities with the same column topology  e.g.  london  october in table 1  will be given the same probability.
1 the semantic view
linguistic motivation an important factor in text comprehension is the degree to which sentences and phrases are linked together. the observation dates back to halliday and hasan  who stressed the role of lexical cohesion in text coherence. a number of linguistic devices - entity repetition  synonymy  hyponymy  and meronymy - are considered to contribute to the  continuity of lexical meaning  observed in coherent text. morris and hirst  represent lexical cohesion via lexical chains  i.e.  sequences of related words spanning a topical text unit  thus formalizing the intuition that coherent units will have a high concentration of dense chains. they argue that the distribution of lexical chains is a surface indicator of the structure of coherent discourse.
discourse representation the key premise behind lexical chains is that coherent texts will contain a high number of semantically related words. this allows for a particularly simple representation of discourse that does not take account of syntactic structure or even word order within a sentence. each sentence is thus represented as a bag of words. these can be all words in the document  excluding function words  or selected grammatical categories  e.g.  verbs or nouns . for our models  we assume that each sentence is represented by a set of nouns.
　central to this representation is the ability to measure semantic similarity. different coherence models can be therefore defined according to the chosen similarity measure.
inference measuring local coherence amounts to quantifying the degree of semantic relatedness between sentences. specifically  the coherence of text t is measured by taking the mean of all individual transitions.
n 1
‘ sim si si+1 
	coherence t = i=1	 1 
n 1
where sim si si+1  is a measure of similarity between sentences si and si+1.
　we have experimented with three broad classes of models which employ word-based  distributional  and taxonomybased similarity measures.
　in its simplest form  semantic similarity can be operationalized in terms of word overlap:
1|words s1 ”words s1 |
sim s1 s1 = 	 1   |words s1 |+|words s1 | 
where words si  is the set of words in sentence i. the main drawback of this measure is that it will indicate lowcoherence for sentence pairs that have no words in common  even though they may be semantically related.
　measures of distributional similarity  however  go beyond mere repetition. words are considered similar if they occur within similar contexts. the semantic properties of words are captured in a multi-dimensional space by vectors that are constructed from large bodies of text by observing the distributional patterns of co-occurrence with their neighboring words. for modeling coherence  we want to be able to compare the similarity of sentences rather than words  see  1  . our computation of sentence similarity follows the method described in foltz : each sentence is represented by the mean  centroid  of the vectors of its words  and the similarity between two sentences is determined by the cosine of their means. sim
n

where μis the vector for word w.
　an alternative to inducing word similarity relationships from co-occurrence statistics in a corpus  is to employ a manually crafted resource such as wordnet  fellbaum  1 . wordnet-based similarity measures have been shown to correlate reliably with human similarity judgments and have been used in a variety of applications ranging from the detection of malapropisms to word sense disambiguation  see budanitsky and hirst  for an extensive survey . we employed five measures commonly cited in the literature. two of these measures  hirst and st-onge  1; lesk  1  define similarity solely in terms of the taxonomy  whereas the other three are based on information theory  resnik  1; jiang and conrath  1; lin  1  and combine taxonomic information with corpus counts. by considering a broad range of these measures  we should be able to distill which ones are best suited for coherence assessment.
　all wordnet-based measures compute a similarity score at the sense level. we define the similarity of two sentences s1 and s1 as:
	‘ argmax	sim c1 c1 
w1（s1 c1（senses w1  w1（s1 c1（senses w1 
	sim s1 s1 = 	 1 
|s1||s1|
where |si| is the number of words in sentence i. since the appropriate senses for words w1 and w1 are not known  our measure selects the senses which maximize sim c1 c1 .
1 collecting coherence judgments
to comparatively evaluate the coherence models introduced above  we first needed to establish an independent measure of coherence by eliciting judgments from human participants.
materials and design for optimizing and testing our models  we created a corpus representative of coherent and incoherent texts. more specifically  we elicited coherence judgments for multi-document summaries produced by systems and human writers for the document understanding conference  duc  1 . automatically generated summaries are prone to coherence violations  mani  1  - sentences are often extracted out of context and concatenated to form a text - and are therefore a good starting point for coherence analysis.
　we randomly selected 1 input document clusters1 and included in our materials six summaries corresponding to each cluster. one summary was authored by a human  whereas five were produced by automatic summarization systems that participated in duc. thus the set of materials contained 1，1 = 1 summaries. from these  six summaries were reserved as a development set  whereas the remaining 1 summaries were used for testing  see section 1 .
procedure and subjects participants first saw a set of instructions that explained the task  defined local coherence  and provided several examples. then the summaries were presented; a new random order of presentation was generated for each participant. participants were asked to use a seven point scale to rate how coherent the summaries were without having seen the source texts. the study was conducted remotely over the internet and was completed by 1 unpaid volunteers  approximately 1 per summary   all native speakers of english.
　the judgments were averaged to provide a single rating per summary and all further analyses were conducted on means. we report results on inter-subject agreement in section 1.
1 experiments
all our models were evaluated on the duc summaries for which we elicited coherence judgments. we used correlation analysis to assess the degree of linear relationship between human ratings and coherence as estimated by our models. in this section we provide an overview of the parameters we explored  and present and discuss our results.
1 parameter estimation
since the models we evaluated have different training requirements  e.g.  some must be trained on a large corpus and others are not corpus specific   it was not possible to have a uniform training corpus for all models. here  we give an overview of the data requirements for our models and explain how these were met in our experiments.
　for the entity-based models  we need corpus data to select an appropriate history size and to estimate the probability of various entity transitions. history size was adjusted using the development corpus described above. entity transition probabilities were estimated from a different corpus: we used 1 human summaries that were produced by duc analysts. we assumed that human authored summaries were coherent and therefore appropriate for obtaining reliable estimates. the grid columns were augmented with the start and end symbols  increasing the size of the grid cell categories to six. we estimated the probabilities of individual columns using n-gram models  see  1   of variable length  i.e.  1  smoothed with witten bell discounting.
humansegridoverlaplsahstoleskjconlinegrid.1*overlap.1 .1**lsa.1*.1.1hsto.1**.1.1.1lesk.1.1 .1.1.1**jcon .1** .1**.1**.1.1**.1*lin.1.1 .1.1.1**.1**.1**resnik.1 .1.1 .1.1**.1**.1**.1***p   .1  1-tailed **p   .1  1-tailed table 1: correlation between human ratings and coherence models  measured by pearson coefficient. stars indicate the level ofstatistical significance.
　some of the models based on semantic relatedness provide a coherence score without presupposing access to corpus data. these models are based on the measures proposed by hirst and st-onge   lesk  and on word overlap. since they require no prior training  they were directly computed on a lemmatized version of the test set. other semantic association models rely on corpus data to either automatically construct representations of lexical meaning  foltz et al.  1  or to populate wordnet's representations with frequency counts  resnik  1; jiang and conrath  1; lin  1 . we used a lemmatized version of the north american news text corpus for this purpose. the corpus contains 1 million words of text taken from a variety of news sources and is therefore appropriate for modeling the coherence of news summaries. vector-based representations were created using a term-document matrix. we used singular value decomposition  berry et al.  1  to reduce the vector space to 1 dimensions and thus obtained a semantic space similar to latent semantic analysis  lsa  foltz et al.  .
1 results
the evaluation of our coherence models was driven by two questions:  a  what is the contribution of various linguistic sources to coherence modeling  and  b  are the two modeling frameworks complementary or redundant 
upper bound we first assessed human agreement on coherence judgments. inter-subject agreement gives an upper bound for the task and allows us to interpret model performance in relation to human performance. to measure agreement  we employed leave-one-out resampling  weiss and kulikowski  1 . the technique correlates the ratings of each participant with the mean ratings of all other participants. the average agreement was r = .1.
model performance table 1 displays the results of correlation analysis on 1 summaries1 from our test set. significant correlations are observed for the entity grid model  egrid   the vector-based model  lsa   and two wordnetbased models  employing hirst and st-onge's   hsto  and jiang and conrath's   jcon  similarity measures.
　the significant correlation of the egrid model with human judgments empirically validates centering's claims about the importance of entity transitions in establishing local coherence. the performance of the lsa model confirms previous claims in the literature  foltz et al.  1  that distributional similarity is a significant predictor of text coherence. the fact that jcon correlates1 with human judgments is not surprising either; its performance has been superior to other wordnetbased measures on two tasks that are particularly relevant for the problem considered here: modeling human similarity judgments and the automatic detection of malapropisms  a phenomenon that often results in locally incoherent discourses  see budanitsky and hirst  for details . it is worth considering why hsto performs better than jcon. in quantifying semantic similarity  hsto uses all available relations in wordnet  e.g.  antonymy  meronymy  hyponymy  and is therefore better-suited to capture cohesive ties arising from semantically related words than jcon which exploits solely hyponymy. these results further suggest that the chosen similarity measure plays an important role in modeling coherence.
　the correlations obtained by our models are substantially lower when compared with the inter-subject agreement of .1. our results indicate that there is no single method which captures all aspects of local coherence  although hsto yields the highest correlation coefficient in absolute terms. it is worth noting that egrid is competitive with lsa and the semantic association models based on wordnet  even though it is unlexicalized. we conjecture that egrid makes up for the lack of lexicalization by taking syntactic information into account and having a more global perspective of discourse. this is achieved by explicitly modeling entity transitions spanning more than two consecutive sentences.
　as far as model intercorrelations are concerned  note that egrid is not significantly correlated with the lsa model  thus indicating that the two models capture complementary coherence properties. interestingly  lsa displays no correlation with the wordnet-based models. although both types of models rely on semantic relatedness  they employ distinct representations of lexical meaning  word co-occurrencebased vs. taxonomy-based . expectedly  the wordnet-based coherence models are all intercorrelated  see table 1 .
model combination an obvious question is whether a better fit with the experimental data can be obtained via model combination. a standard way to integrate different predictors
 i.e.  models  is multiple linear regression. since several of the models were intercorrelated  we performed stepwise regression  using forward selection  to determine the best set of predictors for coherence. the best-fitting combined model  obtained on the set of 1 summaries  included five variables: egrid  overlap  lsa  hsto  and lesk.
　next we tested the combined model's performance on 1 unseen summaries  the output of two systems different from those used for assessing the contribution of the individual models. more specifically  we obtained a coherence score for each unseen summary using the combined model and then compared the estimated values and the human ratings. the comparison yielded a correlation coefficient of .1  p   .1  outperforming any single model. this is a linguistically richer model which integrates several interrelated aspects of coherence  both syntactic and semantic  such as repetition  overlap   syntactic prominence  egrid   and semantic association  lsa  hsto  lesk .
1 conclusions
in this paper we have compared and contrasted two main frameworks for representing and measuring text coherence. our syntactic framework operationalizes centering's notion of local coherence using entity grids. we argued that this representation is particularly suited for uncovering entity transition types typical of coherent and incoherent texts and introduced a model that quantitatively delivers this assessment. our semantic framework capitalized on the notion of similarity between sentences. we experimented with a variety of similarity measures employing different representations of lexical meaning: word-based  distributional  and taxonomybased. our experiments revealed that the two modeling approaches are complementary: our best model retains aspects of entity coherence as well as semantic relatedness.
　the modeling approach taken in this paper relies on shallow text properties and is relatively inexpensive  assuming access to a taxonomy and a parser . this makes the proposed models particularly attractive for the automatic evaluation of machine generated summaries. an important future direction lies in the development of a lexicalized version of the entitygrid model that combines the benefits of grid column topology and semantic similarity. further investigations on different languages  text types  and genres will test the generality and portability of our models. we will also examine whether additional linguistic knowledge  e.g.  coreference resolution  causality  will result in improved performance.
acknowledgments
the authors acknowledge the support of epsrc  lapata; grant gr/t1  and the national science foundation  barzilay; career grant iis-1 . thanks to eli barzilay  frank keller  smaranda muresan  kevin simler  caroline sporleder  chao wang  and bonnie webber for helpful comments and suggestions.
