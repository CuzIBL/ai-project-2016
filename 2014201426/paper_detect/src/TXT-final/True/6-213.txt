 stochastic actions which only address the expected outcome of a local policy. it turns out that  while both representations can be used to compute a global policy  the use of abstract stochastic actions may result in an exponential speed-up.- in the next section we 
   *this work was partly supported by the german science foundation  dfg   grant no. la 1-1  and a grant by the nrw ministry of education and research  mswf . 
briefly introduce mdps and dtgolog. section 1 describes options in more detail and how to map them into dtgolog. we end with a discussion of experimental results. 
1 decision theory and golog 
a fully observable mdp is represented as a tuple m = 
 where s is a finite set of states  a is a finite set of actions  pr is a probability distribution 
 1   pr sj|a  si  denotes the probability of the agent ending up in state sj after performing action a in state st  and  is a bounded reward function. the objective is then to construct a policy  that maximizes the expected total reward over some horizon. a simple algorithm for constructing optimal policies is value iteration  cf.  puterman  1  . 
　dtgolog  boutilier et al  1  can be thought of  roughly  as a programming language which allows a user to combine pre-defined primitive actions into complex programs using the usual constructs like sequence  if-then-else  while  and recursive procedures. in addition there are nondeterministic actions to model that an agent can choose among alternatives. the semantics is based on the situation calculus and  in particular  the so-called basic action theories described in  reiter  1   which define when primitive actions are executable and how they change and do not change what is true in the world. as a special form of primitive action dtgolog allows so-called stochastic actions  which have probabilistic outcomes  just as in mdps. for example  it is possible to define an action r  move to the right   after which a robot has moved one step to the right with probability 1  but has moved to the left  up  or down with probability 1. mdp-style reward and cost functions are incorporated as well. given a dtgolog program  the idea is then to compute a policy in the sense that each nondeterministic choice is resolved in a decision theoretic manner  that is  by choosing the action with maximal expected reward. assuming that there are not too many nondeterministic or stochastic actions  a policy can be computed even in cases where an mdp representation seems infeasible due to the size of the state space. 
1 mapping options into dtgolog 
as in  sutton et al.  1   we define options as triples where i is the set of initial states   is a policy  and 
　is the set of terminating states. to illustrate options consider the example in figure 1 from  hauskrecht et al  1 . 

1 	poster papers 

figure 1: mazc1 from  hauskrecht et al  1 . 
the task is to find an optimal policy to get from position s to position g. performing an action has cost 1  the goal position has a high positive reward. the agent can perform the stochastic basic or primitive actions r l u d succeeding with probability 1. with probability 1 it will be in any other ad-
jacent position. for each room options arc defined to leave the room through a certain door  one for each room/door combination . the gray dots correspond to the termination positions of these options. for the left lower room in figure 1 there are two options oe and on  leave through the east or north door  respectively  with ie = and 
 = { 1    1 }  and similarly for on.  in order to mark state  1  in as success and  1  as failure  they are assigned positive and negative reward  respectively.  
　applying standard value iteration techniques  the following can be computed for an option o: 
　1. the optimal policy that is  the most appropriate action ai for each st j; 
　1. for each st i and sj the probability to terminate the option with the outcome sj when starting in si 
　these results can now be translated into a form suitable for dtgolog. given i = {s1 ...  sn   suppose that each st can be uniquely characterized by a logical formula   in our example   pi simply expresses the coordinates of the location of .s .  then the policy  can be translated in a straightforward fashion into the following dtgolog program: 
and sensestate o  is a so-called sens-
ing action. roughly  executing sensestate o  establishes the truth values of the  so that they can be tested in the following while and if conditions.  the sensing actions are necessary to account for the mdp assumption of full observability.  all in all  the program simply prescribes that the optimal action  according to   should be executed as long as the agent is in one of the initial states of o. 
　given  1.  above we can also generate for each st  i an abstract stochastic action o si . for example  for the option oe and state s1 =  1  we would define an abstract stochastic action with nature's choices and for leaving through the east or north door  respectively  together with the probabilities of these actually occurring as computed in  1.  = 1  
　just as options in the mdp framework can be treated like primitive actions in mdps that use these options  the translation into abstract stochastic actions in the dtgolog framework has the same effect  that is  we can now write dtgolog programs treating abstract stochastic actions just like any other primitive actions. a global policy computed by dtgolog for such a program usually mentions abstract actions. these are not readily executable  leaving a room through the east door is not a primitive action . hence  in a final step we need to replace the abstract actions by the programs which we derived above. 
1 	experimental results 
using our running example we conducted a number of experiments  the goal being at different distances from the initial 
position  fig.l shows the special case of distance 1 . the results are given in figure 1. the x-axis depicts the initial distance to the goal  the y-axis the running time. we compared three different approaches:  a  calculating the optimal policy in dtgolog nondeterministically choosing only from the primitive actions   b  using a set of procedures like the one in the previous section for leaving each room towards a certain neighboring room  choosing from primitive actions only in the goal room  and  c  using options in the form of abstract stochastic actions  choosing from primitive actions only in the goal room. 

figure 1: runtimes of the three test programs in the maze. 
　note that the y-axis of the diagram has a logarithmic scale. the speed-up from  a  to  b  shows the benefit using dtgolog to constrain the search space by providing fixed programs for certain subtasks. interestingly   c   that is  using abstract actions  clearly outperforms  b . roughly  this is because each abstract action has only two outcomes  whereas the corresponding program provides a very fine-grained view with a huge number of outcomes that need to be considered. 
　taking the time of calculation of all options into account  here: 1 seconds  the use of options pays off at horizons greater than 1. also  calculating options can be done off-line and can be neglected in case of frequent reuse. 
　we remark that while method  a  guarantees optimality  this is not necessarily so for  b  ana  c   for essentially the same reasons as in  hauskrecht et ai  1 . certainly in the case of  c   this price seems worth the computational gain. finally  while we currently assume options as given  hauskrecht et al.  1  discuss ways of automatically generating options with good solution qualities  an issue we intend to investigate in the future as well. 
