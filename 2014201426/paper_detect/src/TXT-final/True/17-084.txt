 
this paper presents a framework for a theory of granularity  which is seen as a means of constructing simple theories out of more complex ones. a transitive indistinguishability relation can be defined by means of a set of relevant predicates  allowing simplification of a theory of complex phenomena into computationally tractable local theories  or granularities. nontransitive indistinguishability relations can be characterized in terms of relevant partial predicates  and idealization allows simplification into tractable local theories. various local theories must be linked with each other by means of articulation axioms  to allow shifts of perspective. such a treatment of granularity must be built into the very foundation of the reasoning processes of intelligent agents in a complex world. 
abstraction 
we look at the world under various grain sizes and abstract from it only those things that serve our present interests. thus  when we are planning a trip  it is sufficient to think of a road as a one-dimensional curve. when we are crossing a road  we must think of it as a surface  and when we are digging up the pavement  it becomes a volume for us. when we are driving down a road  we alternate among these granularities  sometimes conscious of our progress along the one-dimensional curve  sometimes making adjustments in our position on the surface  sometimes slowing down for bumps or potholes in the volume. 
¡¡our ability to conceptualize the world at different granularities and to switch among these granularities is fundamental to our intelligence and flexibility. it enables us to map the complexities of the world around us into simple theories that are computationally tractable to reason in. if we are to have a machine of even moderate intelligence  it must have a theory of granularity woven into the very foundation of its reasoning processes. the purpose of this paper is to propose the outlines of such a theory. 
¡¡let us assume the world  in whatever complexity the machine is capable of dealing with  is represented in a global theory  which we may take to be a first-order logical theory to- our approach to granularity will be to extract from to smaller  more computationally tractable  local theories. 
let po be the set of predicates of to  and so its domain of interpretation. suppose a subset r of po has been determined to be the predicates relevant to the situation at hand. we can then define an indistinguishability relation ~ on so by means of the following second-order axiom: 

that is  x and y are indistinguishable if no relevant predicate distinguishes between them. 
¡¡in general  of course  it is a very hard problem to determine just which predicates are relevant. we would expect that in the course of planning to achieve a goal or reasoning about a situation  the set of relevant predicates becomes more constrained  and as it does  more entities become indistinguishable  for all practical purposes . in planning a trip from menlo park to los angeles  for example  the only relevant spatial predicates involve distance from los angeles. two points in the roadway which differ only in their coordinates across and into the roadway  not along it  do not differ in their distance from los angeles  and hence are indistinguishable. 

this indistinguishability relation allows us to simplify the volumetric roadway into a one-dimensional curve. 
simplification 
the indistinguishability relation allows us to define a mapping k that will collapse the complex theory to into a simpler  more  coarse-grained* theory t1. let s1 be the set of equivalence classes of so with respect to ~  and let k be the mapping that takes any element of so into its equivalence class in s1. that is  
1 
¡¡
	j. hobbs 	1 

¡¡
1 j. hobbs 
 1   where € is the size of the gaps between the values for which the relevant partial predicates are true and thoee for which they are false. in the temperature example  let the relevant partial predicates be *x is around tmn for all real numbers t  which are true for  say  false for 	and undefined otherwise  where e is between 1 and 1. then any two temperatures that are only 1* apart cannot be distinguished by any of the relevant partial predicates. briefly  if we represent the fuzzy judgments people make by means of partial predicates  then e in definition  1  can be seen as a measure of their undefinedness. 
¡¡the next problem is that the indistinguishability relation defined in  1  is transitive  whereas that defined by partial predicates as in  1  need not be. thus we cannot use indistinguishability to collapse our global theory into a local theory  for the collapsing mapping k as given above is not well-defined. how is simplification to be achieved with this new definition of indistinguishability  
¡¡suppose we are given a set r of relevant partial predicates. consider the set sq of unambiguous elements of so  that is  the elements for which every predicate in r is defined.  this subset may be empty  in which case all the burden falls on the second step.  for so  the indistinguishability relation ~ is transitive  and hence we may take s1 to be the set of equivalence classes imposed on sq by ~  and define k as the function that maps every element of so' into its equivalance class in s|. 
¡¡suppose  for example  we are interested only in a rough characterization of temperatures as being in the 1's  or in the 1's  etc. we have here a set of predicates that are not overlapping  i.e.  if p x  then for every other relevant predicate q in r. then for those temperatures t that are unambiguously in the 1's  that is  the temperatures from 1 to 1  we can define k t  to be  1s . 
¡¡we should extend * to the ambiguous elements so that all circumstances will be covered by the theory. it seems natural for s1 to inherit indistinguishability from so. since and are indistinguishable in to. the 1's and the 1's are indistinguishable in  the residual fuzs never disappears. if we further collapse s1 into a set st consisting of the  the 1's  etc.  then the 1's will be indistinguishable from the 1's since 1* is indistinguishable from 1. however  we realise that inherited indistinguishability becomes less and leas appropriate as we collapse to simpler theories. to eliminate the indistinguishability  we simply stipulate that all elements of s1 are distinguishable. this is the process of idealization. 
¡¡we thus define k arbitrarily on the ambiguous elements  respecting  however  the relevant structure of to  e.g.  the order  which can be defined on s1 in the obvious way. thus we would impose the condition that 

we might map 1 into 1s and 1 into 1s  even though they are indistinguishable in to  and declare by fiat that the 1's and the 1's are distinguishable. we thereby sacrifice the tight connection between our local theory and the overarching global theory  as one always does in idealization  but it may be that the resulting local theory is clean enough to make this sacrifice worthwhile. 
¡¡the idealization should be faithful to the global theory  insofar as possible. one measure of the faithfulness of an idealization is the proportion of the entire set so that it was necessary to be arbitrary on  elements x where 

in the temperature example s1 we have been arbitrary on 1% of the set  so that the idealization is moderately faithful. for s1  the ambiguous areas are only 1% of the entire set  so the idealization is quite a bit more faithful. the aim in defining k is to construct a useful theory while at the same time maximizing the faithfulness of the idealization  by this or more sophisticated measures. 
¡¡this approach to the fuzzy quality of granularity  using partial predicates and idealization  contrasts with a treatment that makes the truth of the relevant predicates a matter of degree. our approach has the disadvantage of forcing nonintuitive sharp distinctions between the areas where a predicate is true  false and undefined. on the other hand  it gives us a discrete  propositional system that is often finite and even small  and that allows both subtlety of expression and tractable computation. 
articulation 
people not only view the world at different granularities. they translate easily among the granularities as needs dictate. therefore  a theory of granularity must say something about how various local theories articulate1 with each other. there has been a certain amount of work in ai on this problem - research on hierarchical problem-solving in expert systems  and on hierarchical planning . when we move from one level of a hierarchy to the level below  we are moving from a coarse-grained local theory to a more fine-grained local theory  and the axioms that specify the decomposition of coarse-grained predicates into finegrained ones constitute the articulation between the two theories. the articulation can often be quite complex. for example  at the granularity appropriate for the commander of a ship  an event might be thought of as an increase on a continuous speed scale. for the officer in charge of the engine room  the same event might have to be conceptualized in terms of the number of boilers that must be operating.1 there are some general things one can say 
1 etymology: from the latin for  joint   from the indo-european  or  to fit together . 
1 i am indebted to bruce roberts for tail example. 
1 
¡¡
about articulation  but it is largely a matter of spelling out the particular cases in the knowledge base. 
¡¡much of our knowledge is grain-dependent. in the knowledge bases we build as we axiomatize commonsense knowledge     grain-size must be an explicit argument of many predications. it is first of all required when we are stating axioms that relate how a phenomenon is seen at two different granularities. we have already seen one example of such a relation - the relation between duration in continuous time and at-time in discrete time in the situation calculus example. another example comes from the naive physics of materials. what appears as the bending of a flexible object at one granularity can be viewed at a finer granularity as a stretching and a compressing. 
¡¡in addition  grain-size must sometimes be mentioned explicitly to prevent us from falling out of the region where the local theory applies. for example  the notion of  substance  is a grain-dependent one. water is water down to the molecular level  but sand is sand only down to the size of a grain of sand  the grain size associated with succotash is somewhat larger than an individual lima bean  and the grain size of traffic is larger than an individual car. the granularity must be explicitly represented for substances in order to avoid paradoxes of infinite divisibility. we can express as follows the axiom that states that if a substance p has an indistinguishability relation - determined by the characteristic granularity of p  then a piece of p has proper parts which are of the same substance  provided it has two distinguishable points: 

the qualification saves us from paradox. 
¡¡finally it should be pointed out that the whole issue in the philosophy of science of the reducibility of one scientific theory to another is an issue of articulation. 
intelligence 
this approach to granularity suggests an intriguing view of the intelligence that people have and that intelligent machines will have to have. it is that our knowledge consists of a global theory together with a large number of relatively simple  idealized  grain-dependent  local theories  interrelated by articulation axioms. in a complex situation  we abstract the crucial features from the environment  determining a granularity  and select the corresponding local theory. this is the only computation done in the global theory. the local theory is then applied in the bulk of the problem-solving process. when shifts in perspective are required  when we must translate the problem from one local theory to another  articulation axioms are used. 
¡¡one consequence of this view is that work done in  neat  domains  even on toy problems in microworlds  is not necessarily wasted when our ultimate concern is the real world 
	j. hobbs 	1 
in all its complexity. it may be that this work results in the discovery of local theories that will be essential elements in the overall reasoning process.1 
¡¡one cause of despair in artificial intelligence has always been the lurking fear that when we scale up to knowledge bases of the size that is clearly required for intelligent behavior in a complex world  all of the methods we have been developing will become computationally intractable. considerations of the uses to which the notion of granularity can be put in knowledge bases and reasoning processes suggest that this need not be the case. 
acknowledgments 
i am indebted to johan van bent ham  henry kautz  pat hayes  marilyn stelzner  sandy pent land  marc dymetman  and mike georgeff for valuable discussions during the course of this research. the research was supported by nih grant lm1 from the national library of medicine  by grant ist-1 from the national science foundation  and by a gift from the system development foundation. 
