
we consider the problem of optimal planning in stochastic domains with resource constraints  where resources are continuous and the choice of action at each step may depend on the current resource level. our principal contribution is the hao* algorithm  a generalization of the ao* algorithm that performs search in a hybrid state space that is modeled using both discrete and continuous state variables. the search algorithm leverages knowledge of the starting state to focus computational effort on the relevant parts of the state space. we claim that this approach is especially effective when resource limitations contribute to reachability constraints. experimental results show its effectiveness in the domain that motivates our research - automated planning for planetary exploration rovers.
1 introduction
control of planetary exploration rovers presents several important challenges for research in automated planning. because of difficulties inherent in communicating with devices on other planets  remote rovers must operate autonomously over substantial periods of time  bresina et al.  1 . the planetary surfaces on which they operate are very uncertain environments: there is a great deal of uncertainty about the duration  energy consumption  and outcome of a rover's actions. currently  instructions sent to planetary rovers are in the form of a simple plan for attaining a single goal  e.g.  photographing some interesting rock . the rover attempts to carry this out  and  when done  remains idle. if it fails early on  it makes no attempt to recover and possibly achieve an alternative goal. this may have a serious impact on missions. for example  it has been estimated that the 1 mars pathfinder rover spent between 1% and 1% of its time doing nothing because plans did not execute as expected. the current mer rovers  aka spirit and opportunity  require an average of 1 days to visit a single rock  but in future missions  multiple rock visits in a single communication cycle will be

 
research institute for advanced computer science.
 
qss group inc.possible  pedersen et al.  1 . as a result  it is expected that space scientists will request a large number of potential tasks for future rovers to perform  more than may be feasible  presenting an oversubscribed planning problem.
　working in this application domain  our goal is to provide a planning algorithm that can generate reliable contingent plans that respond to different events and action outcomes. such plans must optimize the expected value of the experiments conducted by the rover  while being aware of its time  energy  and memory constraints. in particular  we must pay attention to the fact that given any initial state  there are multiple locations the rover could reach  and many experiments the rover could conduct  most combinations of which are infeasible due to resource constraints. to address this problem we need a faithful model of the rover's domain  and an algorithm that can generate optimal or near-optimal plans for such domains. general features of our problem include:  1  a concrete starting state;  1  continuous resources  including time  with stochastic consumption;  1  uncertain action effects;  1  several possible one-time-rewards  only a subset of which are achievable in a single run. this type of problem is of general interest  and includes a large class of  stochastic  logistics problems  among others.
　past work has dealt with some features of this problem. related work on mdps with resource constraints includes the model of constrained mdps developed in the or community  altman  1 . a constrained mdp is solved by a linear program that includes constraints on resource consumption  and finds the best feasible policy  given an initial state and resource allocation. a drawback of the constrained mdp model is that it does not include resources in the state space  and thus  a policy cannot be conditioned on resource availability. moreover  it does not model stochastic resource consumption. in the area of decision-theoretic planning  several techniques have been proposed to handle uncertain continuous variables  e.g. feng et al.  1; younes and simmons  1; guestrin et al.  1  . smith 1 and van den briel et al. 1 consider the problem of over-subscription planning  i.e.  planning with a large set of goals which is not entirely achievable. they provide techniques for selecting a subset of goals for which to plan  but they deal only with deterministic domains. finally  meuleau et al. 1 present preliminary experiments towards scaling up decision-theoretic approaches to planetary rover problems.
　our contribution in this paper is an implemented algorithm  hybrid ao*  hao*   that handles all of these problems together: oversubscription planning  uncertainty  and limited continuous resources. of these  the most essential features of our algorithm are its ability to handle hybrid state-spaces and to utilize the fact that many states are unreachable due to resource constraints.
　in our approach  resources are included in the state description. this allows decisions to be made based on resource availability  and it allows a stochastic resource consumption model  as opposed to constrained mdps . although this increases the size of the state space  we assume that the value functions may be represented compactly. we use the work of feng et al.  1  on piecewise constant and linear approximations of dynamic programming  dp  in our implementation. however  standard dp does not exploit the fact that the reachable state space is much smaller than the complete state space  especially in the presence of resource constraints. our contribution is to show how to use the forward heuristic search algorithm called ao*  pearl  1; hansen and zilberstein  1  to solve mdps with resource constraints and continuous resource variables. unlike dp  forward search keeps track of the trajectory from the start state to each reachable state  and thus it can check whether the trajectory is feasible or violates a resource constraint. this allows heuristic search to prune infeasible trajectories and can dramatically reduce the number of states that must be considered to find an optimal policy. this is particularly important in our domain where the discrete state space is huge  exponential in the number of goals   yet the portion reachable from any initial state is relatively small because of the resource constraints. it is well-known that heuristic search can be more efficient than dp because it leverages a search heuristic and reachability constraints to focus computation on the relevant parts of the state space. we show that for problems with resource constraints  this advantage can be even greater than usual because resource constraints further limit reachability.
　the paper is structured as follows: in section 1 we describe the basic action and goal model. in section 1 we explain our planning algorithm  hao*. initial experimental results are described in section 1  and we conclude in section 1.
1 problem definition and solution approach
1 problem formulation
we consider a markov decision process  mdp  with both continuous and discrete state variables  also called a hybrid mdp  guestrin et al.  1  or generalized state mdp  younes and simmons  1  . each state corresponds to an assignment to a set of state variables. these variables may be discrete or continuous. continuous variables typically represent resources  where one possible type of resource is time. discrete variables model other aspects of the state  including  in our application  the set of goals achieved so far by the rover.  keeping track of already-achieved goals ensures a markovian reward structure  since we reward achievement of a goal only if it was not achieved in the past.  although our models typically contain multiple discrete variables  this plays no role in the description of our algorithm  and so  for notational convenience  we model the discrete component as a single variable n.
　a markov state s （ s is a pair  n x  where n （ n is the discrete variable  and x =  xi  is a vector of continuous variables. the domain of eachn xi is an interval xi of the real line  and x = i xi is the hypercube over which the continuous variables are defined. we assume an explicit initial state  denoted  n1 x1   and one or more absorbing terminal states. one terminal state corresponds to the situation in which all goals have been achieved. others model situations in which resources have been exhausted or an action has resulted in some error condition that requires executing a safe sequence by the rover and terminating plan execution.
　actions can have executability constraints. for example  an action cannot be executed in a state that does not have its minimum resource requirements. an x  denotes the set of actions executable in state  n x .
state transition probabilities are given by the function
 pr s1 | s a   where s =  n x  denotes the state before action  denotes the state after action a  also called the arrival state. following  feng et al.  1   the probabilities are decomposed into:
  the discrete marginalspn1（n	1|n x a  = 1pr n; 1|n x a . for all  n x a  
pr n
  the continuous conditionalsr pr x1|n x a n1 . for all  n x a n1   x1（x pr x1|n x a n1 dx1 = 1.
any transition that results in negative value for some continuous variable is viewed as a transition into a terminal state.
　the reward of a transition is a function of the arrival state only. more complex dependencies are possible  but this is sufficient for our goal-based domain models. we let rn x  − 1 denote the reward associated with a transition to state  n x .
　in our application domain  continuous variables model non-replenishable resources. this translates into the general assumption that the value of the continuous variables is nonincreasing. moreover  we assume that each action has some minimum positive consumption of at least one resource. we do not utilize this assumption directly. however  it has two implications upon which the correctness of our approach depends:  1  the values of the continuous variables are a-priori bounded  and  1  the number of possible steps in any execution of a plan is bounded  which we refer to by saying the problem has a bounded horizon. note that the actual number of steps until termination can vary depending on actual resource consumption.
　given an initial state  n1 x1   the objective is to find a policy that maximizes expected cumulative reward.1 in our application  this is equal to the sum of the rewards for the goals achieved before running out of a resource. note that there is no direct incentive to save resources: an optimal solution would save resources only if this allows achieving more goals. therefore  we stay in a standard decision-theoretic framework. this problem is solved by solving bellman's optimality equation  which takes the following form:
vn1 x  = 1  
 
x
	vnt+1 x  =	max	pr n1 | n x a 
         a（an x 	1	 1  n （n z pr x.
x1
note that the index t represents the iteration or time-step of dp  and does not necessarily correspond to time in the planning problem. the duration of actions is one of the biggest sources of uncertainty in our rover problems  and we typically model time as one of the continuous resources xi.
1 solution approach
feng et al. describe a dynamic programming  dp  algorithm that solves this bellman optimality equation. in particular  they show that the continuous integral over x1 can be computed exactly  as long as the transition function satisfies certain conditions. this algorithm is rather involved  so we will treat it as a black-box in our algorithm. in fact  it can be replaced by any other method for carrying out this computation. this also simplifies the description of our algorithm in the next section and allows us to focus on our contribution. we do explain the ideas and the assumptions behind the algorithm of feng et al. in section 1.
　the difficulty we address in this paper is the potentially huge size of the state space  which makes dp infeasible. one reason for this size is the existence of continuous variables. but even if we only consider the discrete component of the state space  the size of the state space is exponential in the number of propositional variables comprising the discrete component. to address this issue  we use forward heuristic search in the form of a novel variant of the ao* algorithm. recall that ao* is an algorithm for searching and/or graphs  pearl  1; hansen and zilberstein  1 . such graphs arise in problems where there are choices  the or components   and each choice can have multiple consequences  the and component   as is the case in planning under uncertainty. ao* can be very effective in solving such planning problems when there is a large state space. one reason for this is that ao* only considers states that are reachable from an initial state. another reason is that given an informative heuristic function  ao* focuses on states that are reachable in the course of executing a good plan. as a result  ao* often finds an optimal plan by exploring a small fraction of the entire state space.
　the challenge we face in applying ao* to this problem is the challenge of performing state-space search in a continuous state space. our solution is to search in an aggregate state space that is represented by a search graph in which there is a node for each distinct value of the discrete component of the state. in other words  each node of our search graph represents a region of the continuous state space in which the discrete value is the same. in this approach  different actions may be optimal for different markov states in the aggregate state associated with a search node  especially since the best action is likely to depend on how much energy or time is remaining. to address this problem and still find an optimal solution  we associate a value estimate with each of the markov states in an aggregate. that is  we attach to each search node a value function  function of the continuous variables  instead of the simple scalar value used by standard ao*. following the approach of  feng et al.  1   this value function can be represented and computed efficiently due to the continuous nature of these states and the simplifying assumptions made about the transition functions. using these value estimates  we can associate different actions with different markov states within the aggregate state corresponding to a search node.
　in order to select which node on the fringe of the search graph to expand  we also need to associate a scalar value with each search node. thus  we maintain for a search node both a heuristic estimate of the value function  which is used to make action selections   and a heuristic estimate of the priority which is used to decide which search node to expand next. details are given in the following section.
　we note that lao*  a generalization of ao*  allows for policies that contain  loops  in order to specify behavior over an infinite horizon  hansen and zilberstein  1 . we could use similar ideas to extend lao* to our setting. however  we need not consider loops for two reasons:  1  our problems have a bounded horizon;  1  an optimal policy will not contain any intentional loop because returning to the same discrete state with fewer resources cannot buy us anything. our current implementation assumes any loop is intentional and discards actions that create such a loop.
1 hybrid ao*
a simple way of understanding hao* is as an ao* variant where states with identical discrete component are expanded in unison. hao* works with two graphs:
  the explicit graph describes all the states that have been generated so far and the and/or edges that connect them. the nodes of the explicit graph are stored in two lists: open and closed.
  the greedy policy  or partial solution  graph  denoted greedy in the algorithms  is a sub-graph of the explicit graph describing the current optimal policy.
in standard ao*  a single action will be associated with each node in the greedy graph. however  as described before  multiple actions can be associated with each node  because different actions may be optimal for different markov states represented by an aggregate state.
1 data structures
the main data structure represents a search node n. it contains:
  the value of the discrete state. in our application these are the discrete state variables and set of goals achieved.
  pointers to its parents and children in the explicit and greedy policy graphs.
  pn ，  - a probability distribution on the continuous variables in node n. for each x （ x  pn x  is an estimate
of the probability density of passing through state  n x  under the current greedy policy. it is obtained by progressing the initial state forward through the optimal actions of the greedy policy. with each pn  we maintain the probability of passing through n under the greedy policy:	z
m pn  =	pn x dx . x（x
  hn ，  - the heuristic function. for each x （ x  hn x  is a heuristic estimate of the optimal expected reward from state  n x .
  vn ，  - the value function. at the leaf nodes of the explicit graph  vn = hn. at the non-leaf nodes of the explicit graph  vn is obtained by backing up the h functions from the descendant leaves. if the heuristic function hn1 is admissible in all leaf nodes n1  then vn x  is an upper bound on the optimal reward to come from  n x  for all x reachable under the greedy policy.
  gn - a heuristic estimate of the increase in value of the greedy policy that we would get by expanding node n. if hn is admissible then gn represents an upper bound on the gain in expected reward. the gain gn is used to determine the priority of nodes in the open list  gn = 1 if n is in closed   and to bound the error of the greedy solution at each iteration of the algorithm.
　note that some of this information is redundant. nevertheless  it is convenient to maintain all of it so that the algorithm can easily access it. hao* uses the customary open and closed lists maintained by ao*. they encode the explicit graph and the current greedy policy. closed contains expanded nodes  and open contains unexpanded nodes and nodes that need to be re-expanded.
1 the hao* algorithm
algorithm 1 presents the main procedure. the crucial steps are described in detail below.
expanding a node  lines 1 to 1 : at each iteration  hao* expands the open node n with the highest priority gn in the greedy graph. an important distinction between ao* and hao* is that in the latter  nodes are often only partially expanded  i.e.  not all markov states associated with a discrete node are considered . thus  nodes in the closed list are sometimes put back in open  line 1 . the reason for this is that a markov state associated with this node  that was previously considered unreachable  may now be reachable. technically  what happens is that as a result of finding a new path to a node  the probability distribution over it is updated  line 1   possibly increasing the probability of some markov state from 1 to some positive value. this process is illustrated in figure 1. thus  while standard ao* expands only tip nodes  hao* sometimes expands nodes that were moved from closed to open and are  in the middle of 
the greedy policy subgraph.
　next  hao* considers all possible successors  a n1  of n given the state distribution pn. typically  when n is expanded for the first time  we enumerate all actions a possible in  n x   a （ an x    for some reachable x  pn x    1  
1: create the root node n1 which represents the initial state.
1: pn1 = initial distribution on resources.
1: vn1 = 1 everywhere in x.
1: gn1 = 1.
1: open = greedy = {n1}.
1: closed =  .
1: while open ” greedy 1=   do
1:	n = argmaxn1（open”greedy gn1 .
1:	move n from open to closed.
1:	for all  a n1  （ a 〜 n not expanded yet in n and
reachable under pn do
1:	if n1 （/ open “ closed then
1: create the data structure to represent n1 and add the transition  n a n1  to the explicit graph.
1:	get hn1.
1:	vn1 = hn1 everywhere in x.
1:	if n1 is terminal: then
1:	add n1 to closed.
1:	else
1:	add n1 to open.
1:	else if n1 is not an ancestor of n in the explicit graph then
1:	add the transition  n a n1  to the explicit graph.
1:	if some pair  a n1  was expanded at previous step  1  then
1:	update vn for the expanded node n and some of its ancestors in the explicit graph  with algorithm 1.
1: update pn1 and gn1 using algorithm 1 for the nodes n1 that are children of the expanded node or of a node where the optimal decision changed at the previous step  1 . move every node n1 （ closed where p changed back into open.algorithm 1: hybrid ao*
and all arrival states n1 that can result from such a transition  pr n1 | n x a    1 .1 if n was previously expanded  i.e. it has been put back in open   only actions and arrival nodes not yet expanded are considered. in line 1  we check whether a node has already been generated. this is not necessary if the graph is a tree  i.e.  there is only one way to get to each discrete state .1 in line 1  a node n1 is terminal if no action is executable in it  because of lack of resources . in our application domain each goal pays only once  thus the nodes in which all goals of the problem have been achieved are also terminal. finally  the test in line 1 prevents loops in the explicit graph. as discussed earlier  such loops are always suboptimal.

 a  initial greedy graph. actions have multiple possible discrete effects  e.g.  a1 has two possible effects in n1 .
the curves represent the current probability distribution p and value function v over x values for n1. n1 is a fringe node. b  greedy graph with n1 expanded. since the path  n1 n1 n1  is optimal for some resource levels in n1  pn1 has changed. as a consequence  n1 has been reexpanded   showing that node n1 is now reachable from n1 under a1  and action a1 has become do-able in n1.figure 1: node re-expansion.updating the value functions  lines 1 to 1 : as in standard ao*  the value of a newly expanded node must be updated. this consists of recomputing its value function with bellman's equations  eqn.1   based on the value functions of all children of n in the explicit graph. note that these backups involve all continuous states x （ x for each node  not just the reachable values of x. however  they consider only actions and arrival nodes that are reachable according to pn. once the value of a state is updated  its new value must be propagated backward in the explicit graph. the backward propagation stops at nodes where the value function is not modified  and/or at the root node. the whole process is performed by applying algorithm 1 to the newly expanded node.
1: z = {n} // n is the newly expanded node.
1: while z 1=   do
1:	choose a node n1 （ z that has no descendant in z.
1:	remove n1 from z.
1:	update vn1 following eqn.1.
1:	if vn1 was modified at the previous step then 1:	add all parents of n1 in the explicit graph to z.
1: if optimal decision changes for some  n1 x   pn1 x    1 then
1:	update the greedy subgraph  greedy  at n1 if
necessary.
1:	mark n1 for use at line 1 of algorithm1.algorithm 1: updating the value functions vn.
updating the state distributions  line 1 : pn's represent the state distribution under the greedy policy  and they need to be updated after recomputing the greedy policy. more precisely  p needs to be updated in each descendant of a node where the optimal decision changed. to update a node n  we consider all its parents n1 in the greedy policy graph  and all the actions a that can lead from one of the parents to n. the probability of getting to n with a continuous component x is the sum over all  n1 a  and all possible values of x1 of the continuous component over the the probability of arriving from n1 and x1 under a. this can be expressed as:

                         pr x | n1 x1 a n dx1 .  1  here  x1 is the domain of possible values for x1  and  n is the set of pairs  n1 a  where a is the greedy action in n1 for some reachable resource level:
 n = { n1 a  （ n 〜 a :  x （ x 
       pn1 x    1  μ n1 x  = a  pr n | n1 x a    1}   where is the greedy action in  n x . clearly  we can restrict our attention to state-action pairs in  n  only. note that this operation may induce a loss of total probabilityp mass  pn   n1 pn1  because we can run out of a resource during the transition and end up in a sink state.
　when the distribution pn of a node n in the open list is updated  its priority gn is recomputed using the following equation  the priority of nodes in closed is maintained as
1 :	z
	gn =	pn x hn x dx ;	 1 
x（s pn  xnold
where s p  is the support of p: s p  = {x （ x : p x    1}  and xoldn contains all x （ x such that the state  n x  has already been expanded before  xoldn =   if n has never been expanded . the techniques used to represent the continuous probability distributions pn and compute the continuous integrals are discussed in the next sub-section. algorithm 1 presents the state distribution updates. it applies to the set of nodes where the greedy decision changed during value updates  including the newly expanded node  i.e. n in hao* - algorithm 1 .
1 handling continuous variables
computationally  the most challenging aspect of hao* is the handling of continuous state variables  and particularly the
1: z = children of nodes where the optimal decision changed when updating value functions in algorithm 1.
1: while z 1=   do
1:	choose a node n （ z that has no ancestor in z.
1:	remove n from z.
1:	update pn following eqn.1.
1:	if pn was modified at step 1 then
1:	move n from closed to open.
1:	update the greedy subgraph  greedy  at n if nec-
essary.
1:	update gn following eqn.1.algorithm 1: updating the state distributions pn.
computation of the continuous integral in bellman backups and eqns. 1 and 1. we approach this problem using the ideas developed in  feng et al.  1  for the same application domain. however  we note that hao* could also be used with other models of uncertainty and continuous variables  as long as the value functions can be computed exactly in finite time. the approach of  feng et al.  1  exploits the structure in the continuous value functions of the type of problems we are addressing. these value functions typically appear as collections of humps and plateaus  each of which corresponds to a region in the state space where similar goals are pursued by the optimal policy  see fig.1 . the sharpness of the hump or the edge of a plateau reflects uncertainty of achieving these goals. constraints imposing minimal resource levels before attempting risky actions introduce sharp cuts in the regions. such structure is exploited by grouping states that belong to the same plateau  while reserving a fine discretization for the regions of the state space where it is the most useful  such as the edges of plateaus .
　to adapt the approach of  feng et al.  1   we make some assumptions that imply that our value functions can be represented as piece-wise constant or linear. specifically  we assume that the continuous state space induced by every discrete state can be divided into hyper-rectangles in each of which the following holds:  i  the same actions are applicable.  ii  the reward function is piece-wise constant or linear.  iii  the distribution of discrete effects of each action are identical.  iv  the set of arrival values or value variations for the continuous variables is discrete and constant. assumptions  i-iii  follow from the hypotheses made in our domain models. assumption  iv  comes down to discretizing the actions' resource consumptions  which is an approximation. it contrasts with the naive approach that consists of discretizing the state space regardless of the relevance of the partition introduced. instead  we discretize the action outcomes first  and then deduce a partition of the state space from it. the state-space partition is kept as coarse as possible  so that only the relevant distinctions between  continuous  states are taken into account. given the above conditions  it can be shown  see  feng et al.  1   that for any finite horizon  for any discrete state  there exists a partition of the continuous space into hyper-rectangles over which the optimal value function is piece-wise constant or linear. the implementation represents the value functions as kd-trees  using a fast algorithm to intersect kd-trees  friedman et al.  1   and merging adjacent pieces of the value function based on their value. we augmented this approach by representing the continuous state distributions pn as piecewise constant functions of the continuous variables. under the set of hypotheses above  if the initial probability distribution on the continuous variables is piecewise constant  then the probability distribution after any finite number of actions is too  and eqn. 1 may always be computed in finite time.1
1 properties
as for standard ao*  it can be shown that if the heuristic functions hn are admissible  optimistic   the actions have positive resource consumptions  and the continuous backups are computed exactly  then:  i  at each step of hao*  vn x  is an upper-bound on the optimal expected return in  n x   for all  n x  expanded by hao*;  ii  hao* terminates after a finite number of iterations;  iii  after termination  vn x  is equal to the optimal expected return in  n x   for all  n x  reachable under the greedy policy  pn x    1 . moreover  if we assume that  in each state  there is a done action that terminates execution with zero reward  in a rover problem  we would then start a safe sequence   then we can evaluate the greedy policy at each step of the algorithm by assuming that execution ends each time we reach a leaf of the greedy subgraph. under the same hypotheses  the error of the greedy policy at each step of the algorithm is bounded byp n（greedy”open gn. this property allows trading com-
putation time for accuracy by stopping the algorithm early.
1 heuristic functions
the heuristic function hn helps focus the search on truly useful reachable states. it is essential for tackling real-size problems. our heuristic function is obtained by solving a relaxed problem. the relaxation is very simple: we assume deterministic transitions for the continuous variables  i.e.  pr x1|n x a n1  （ {1}. if we assume the actions consume the minimum amount of each resource  we obtain an admissible heuristic function. a non-admissible  but probably more informative heuristic function is obtained by using the mean resource consumption.
　the central idea is to use the same algorithm to solve both the relaxed and the original problem. unlike classical approaches where a relaxed plan is generated for every search state  we generate a  relaxed  search-graph using our hao* algorithm once with a deterministic-consumption model and a trivial heuristic. the value function vn of a node in the relaxed graph represents the heuristic function hn of the associated node in the original problem graph. solving the relaxed problem with hao* is considerably easier  because the structure and the updates of the value functions vn and of the probabilities pn are much simpler than in the original domain. however  we run into the following problem: deterministic consumption implies that the number of reachable states for any given initial state is very small  because only one continuous assignment is possible . this means that in a single expansion  we obtain information about a small number of

figure 1: case study: the rover navigates around five target rocks  t1 to t1 . the number with each rock is the reward received on testing that rock.
states. to address this problem  instead of starting with the initial resource values  we assume a uniform distribution over the possible range of resource values. because it is relatively easy to work with a uniform distribution  the computation is simple relative to the real problem  but we obtain an estimate for many more states. it is still likely that we reach states for which no heuristic estimate was obtained using these initial values. in that case  we simply recompute starting with this initial state.
1 experimental evaluation
we tested our algorithm on a slightly simplified variant of the rover domain model used for nasa ames october 1 intelligent systems demo  pedersen et al.  1 . in this domain  a planetary rover moves in a planar graph made of locations and paths  sets up instruments at different rocks  and performs experiments on the rocks. actions may fail  and their energy and time consumption are uncertain. resource consumptions are drawn from two type of distributions: uniform and normal  and then discretized. the problem instance used in our preliminary experiments is illustrated in figure 1. it contains 1 target rocks  t1 to t1  to be tested. to take a picture of a target rock  this target must be tracked. to track a target  we must register it before doing the first move. later  different targets can be lost and re-acquired when navigating along different paths. these changes are modeled as action effects in the discrete state. overall  the problem contains 1 propositional state variables and 1 actions. therefore  there are 1 different discrete states  which is far beyond the reach of a flat dp algorithm.
　the results presented here were obtained using a preliminary implementation of the piecewise constant dp approximations described in  feng et al.  1  based on a flat
representation of state partitions instead of kd-trees. this is considerably slower than an optimal implementation. to compensate  our domain features a single abstract continuous resource  while the original domain contains two resources
abcdefgh1.111111111.111111111.111111111.111111111.111111111.111111111.1111table 1: performance of the algorithm for different initial resource levels. a: initial resource  abstract unit . b: execution time  s . c: # reachable discrete states. d: # nodes created by ao*. e: # nodes expanded by ao*. f: # nodes in the optimal policy graph. g: # goals achieved in the longest branch of the optimal solution. h: # reachable markov states.
 time and energy . another difference in our implementation is in the number of nodes expanded at each iteration. we adapt the findings of  hansen and zilberstein  1  that overall convergence speeds up if all the nodes in open are expanded at once  instead of prioritizing them based on gn values and changing the value functions after each expansion.1 finally  these preliminary experiments do not use the sophisticated heuristics presented earlier  but the following simple admissible heuristic: hn is the constant function equal to the sum of the utilities of all the goals not achieved in n.
　we varied the initial amount of resource available to the rover. as available resource increases  more nodes are reachable and more reward can be gained. the performance of the algorithm is presented in table 1. we see that the number of reachable discrete states is much smaller than the total number of states  1  and the number of nodes in an optimal policy is surprisingly small. this indicates that ao* is particularly well suited to our rover problems. however  the number of nodes expanded is quite close to the number of reachable discrete states. thus  our current simple heuristic is only slightly effective in reducing the search space  and reachability makes the largest difference. this suggests that much progress can be obtained by using better heuristics. the last column measures the total number of reachable markov states  after discretizing the action consumptions as in  feng et al.  1 . this is the space that a forward search algorithm manipulating markov states  instead of discrete states  would have to tackle. in most cases  it would be impossible to explore such space with poor quality heuristics such as ours. this indicates that our algorithm is quite effective in scaling up to very large problems by exploiting the structure presented by continuous resources.
　figure 1 shows the converged value function of the initial state of the problem. the value function is comprised of several plateaus  where different sets of goals are achieved. the first plateau  until resource level 1  corresponds to the

figure 1: value function of the initial state.
initialexecution# nodes# nodesresourceεtimecreated by ao*expanded by ao*1.1.111111.1.111111.1.111111.1.111111.1.111111.1.111111.1.11table 1: complexity of computing an ε-optimal policy. the optimal return for an initial resource of 1 is 1.
case where the resource level is insufficient for any goal to be achieved. the next plateau  until 1  depicts the region in which the target t1 is tested. the remaining resources are still not enough to move to a new location and generate additional rewards. in the region between 1 and 1 the rover decides to move to l1 and test t1. note that the location l1 is farther from l1 and so the rover does not attempt to move to l1  yet. the next plateau corresponds to the region in which the optimal strategy is to move to l1 and test both t1 and t1.the last region  beyond 1  is in which three goals t1  t1 and t1 are tested and reward of 1 is obtained.
　when hn is admissible  we can bound the error of the current greedy graph by summing gn over fringe nodes. in table 1 we describe the time/value tradeoff we found for this domain. on the one hand  we see that even a large compromise in quality leads to no more than 1% reduction in time. on the other hand  we see that much of this reduction is obtained with a very small price  1 = 1 . additional experiments are required to learn if this is a general phenomenon.
1 conclusions
we presented a variant of the ao* algorithm that  to the best of our knowledge  is the first algorithm to deal with: limited continuous resources  uncertainty  and oversubscription planning. we developed a sophisticated reachability analysis involving continuous variables that could be useful for heuristic search algorithms at large. our preliminary implementation of this algorithm shows very promising results on a domain of practical importance. we are able to handle problems with 1 discrete states  as well as a continuous component.
　in the near future  we hope to report on a more mature version of the algorithm  which we are currently implementing. it includes:  1  a full implementation of the techniques described in  feng et al.  1 ;  1  a rover model with two continuous variables;  1  a more informed heuristic function  as discussed in section 1.
acknowledgements
this work was funded by the nasa intelligent systems program.
eric hansen was supported in part by nsf grant iis-1  nasa grant nag-1 and a nasa summer faculty fellowship.
