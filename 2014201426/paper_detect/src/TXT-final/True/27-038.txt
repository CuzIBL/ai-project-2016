crystal inducing a conceptual dictionary * 
	s t e p h e n 	s o d e r l a n d   	d a v i d f i s h e r   
	j o n a t h a n 	a s e l t i n e   	w e n d y 	l e h n e r t 
d e p a r t m e n t o f c o m p u t e r science 
u n i v e r s i t y o f m a s s a c h u s e t t s   a m h e r s t   m a 1 
{ s o d e r l a n dfisher aseltine l e h n e r t }   c s u m a s s e d u 

a b s t r a c t 
one of the central knowledge sources of an information extraction  ie  system is a dictionary of linguistic patterns that can be used to identify references to relevant information in a text automatic creation of conceptual dictionaries is important for portability and scalability of an ie system this paper describes crystal  a system which automatically induces a dictionary of  concept-node definitions  sufficient to identify relevant information from a training corpus each of these concept-node definitions is generalized as far as possible without producing errors  so that a minimum number of dictionary entries cover the positive training instances because it tests the accuracy of each proposed definition  crystal can often surpass human intuitions in creating reliable extraction rules 
1 	i n f o r m a t i o n e x t r a c t i o n 
an information extraction  ie  system analyzes unrestricted natural language text and produces a representation of the information from the text which is considered relevant to a particular application information extracted from the text is represented as case frames  called  concept nodes   cn's  in the university of massachusetts b a d g e r sentence analyser  which performs selective concept extraction similar to that of the previous circus system  lehnert  1  
lehnert et al  1  
   the domain knowledge needed to identify relevant references is stored in a dictionary of  cn definitions  that describe the local syntactic and semantic context in which relevant information is likely to be found before the cn definition is applied  b a d g e r segments the input text to identify syntactic constituents such as subject  verb phrase  direct and indirect object  and prepositional phrases  and also looks up the semantic class of each word in a domain-specific semantic lexicon a cn 
   ＊this research was supported by nsf grant no eec1  state/ industry/ university cooperative research on intelligent information retrieval thanks also to david 
aronow for help as our medical domain expert 
definition specifies a set of syntactic and semantic constraints that must be satisfied for the definition to apply to a segment of text 
   examples in this paper are from a medical domain  where the task is to analyze hospital discharge reports and identify references to  diagnosis  and to  sign or symptom  these are further classified with subtypes 
diagnosis confirmed ruled out 
suspected pre-existing past s  ign of symptom present 
absent presumed 
unknown 
history    the example shown in figure 1 is a cn definition from this domain that identifies references to absent symptoms this cn definition extracts the phrase in the direct object buffer when the subject buffer has the word  patient   semantic class  patient or disabled group    the verb is  denies  in the active voice  and the direct object has the semantic class  sign or symptom  
cn type sign or symptom 
subtype absent 
extract from direct object 
active voice verb 
subject constraints words include  patient' head class  patient or disabled group  
verb constraints words include  denies' 
direct object constraints 
	head class 	 sign or symptom  
figure 1 a cn definition to identify  sign or symptom  absent  
   this cn definition would extract  any episodes of nausea  from the sentence  the patient denies any episodes of nausea  it would fail to apply to the sentence  patient denies a history of asthma   since asthma is of semantic class  disease or syndrome   which is not a subclass of  sign or symptom  for the hospital discharge report domain  we are using a semantic lexicon 

and semantic hierarchy derived from the unified medical language systems  umls  medical metathesaurus and semantic network  lindberg et al   1   which is currently under development by the national library of 
medicine 
　a dictionary of cn definitions for thus domain is specific to the semantics and writing style of hospital dis charge records and could not be transferred to other applications a new conceptual dictionary must be constructed for each ie application the definitions must be general enongh to cover previously unseen instances  but at the same time constrained tightly enough to avoid over generalising to instances that do not contain the desired information 
　a tool that automatically generates a dictionary of cn definitions is needed to ensure that badger can be easily ported to new domains the crystal dictionary induction tool is one of the first systems to automatically create a conceptual dictionary from a training corpus before presenting crystal  we will describe in more detail the cn definitions used by the badger sentence analyser  which crystal must generate automatically 
1 	concept node definitions 
a concept node  cn  is a case frame instantiated by the badger sentence analyser to represent relevant information identified in the text the cn has two fixed slots  cn type and subtype  as well as slots to hold extracted information  which are usually noun phrases from the text a cn is instantiated from a text segment when the constraints of a cn definition are satisfied 
　these constraints operate on major syntactic con stituents the subject  verb phrase  direct or indirect object  and prepositional phrases any of these constituents may be tested for a sequence of specific words  for specific semantic classes in the head noun of a phrase  or for specific semantic classes in the modifiers of a phrase the verb can be further constrained with respect to active or passive voice 
　figure 1 shows a cn definition that identifies preexisting diagnoses with a set of constraints that could be summarised as   was diagnosed with recurrence of  body part   disease   here the information to be extracted is found in a prepositional phrase which must have the preposition  with  and contain the words  recurrence of  the prepositional phrase must have a head noun whose semantic class is a  disease or syndrome  and a modifying term whose class is a  body part or organ  this cn definition appbes to a sentence such as  the patient was diagnosed with a recurrence of laryngeal cancer  since there are no constraints on the subject  the text segment is free to have any subject  including a relative pronoun or an omitted subject 
　will this cn definition reliably identify only preexisting diagnoses  perhaps in some texts  the recurrence of a disease is actually a principal diagnosis of the current hospitalisation and should be identified as  diagnosis  confirmed  or may be a condition that no longer exists and should be identified as  diagnosis  past  in such cases the cn definition will produce an extraction error on the other hand  this definition might be reh-
cn type diagnosis 
subtype pre existing 
extract from prep phrase with 
passive voice verb 
verb constraints 
words include diagnosed 
prep phrase constraints preposition = 	with words include 	recurrence of 
modifier class  body part or organ  head class  disease or syndrome  
figure 1 a cn definition for  diagnosis  pre-existing  
able  but miss some valid examples that it would cover if the constraints were relaxed slightly 
　judgments about how tightly to constrain a cn definition are difficult to make a prion  and would require careful consideration by someone who combines domain expertise with a deep understanding of the badger sentence analyser an alternate to manually engineering these cn definitions is to induce them automatically from a training corpus of representative texts that have been annotated by a domain expert this is the approach taken by crystal  which we will describe in the following section 
1 	the crystal dictionary induction tool 
crystal derives a domain-specific dictionary of cn definitions from a training corpus  initialising the dictionary with a cn definition for each positive training instance these initial cn definitions are designed to extract the relevant phrase in the training instance that motivated them  but are too specific to apply broadly to previously unseen sentences 
　the main work of crystal is to gradually relax the constraints on these initial definitions to broaden then' coverage  while merging similar definitions to form a more compact dictionary the cn definitions in crystal's final dictionary are generalized as much as possible without producing extraction errors on the training 
corpus 
1 	creating initial cn definitions 
the first step in dictionary creation is the annotation of a set of training texts by a domain expert each phrase that contains information to be extracted is bracketed with sgml-style tags to mark the appropriate cn type and subtype a team of three nurses under the supervision of a physician annotated our training documents for us the annotated texts are then segmented by the badger sentence analyser to create a set of training instances each instance is a text segment  generally a simple clause  some of whose syntactic constituents may be tagged as positive instances of a particular cn type and subtype 
crystal begins its induction with a dictionary of 
cn definitions built from each instance that contains 
	soderland etal 	1 

the cn type and subtype being learned if a training instance has its subject buffer tagged as  diagnosis  with subtype  pre-existing   an initial cn definition is created that extracts the phrase in the subject buffer as a pre-existing diagnosis the constraints on an initial cn definition are derived from the words and classes found in the motivating instance 
   before the induction process begins  crystal cannot predict which characteristics of an instance are essential to the cn definition and which are merely accidental features crystal encodes all the details of the text segment as constraints to the initial cn definition  requiring the exact sequence of words and the exact sets of semantic classes in each syntactic buffer crystal will later learn which constraints should be relaxed 
   figure 1 shows the initial cn definition derived from the sentence fragment  unremarkable with the exception of mild shortness of breath and chronically swollen ankles   the domain expert has marked  shortness of breath  and  swollen ankles  with cn type  sign or symptom  and subtype  present  when b a d g e r analyses this sentence  it assigns the complex noun phrase *the exception of mild shortness of breath and chronically swollen ankles  to a single prepositional phrase buffer when a complex noun phrase has multiple head nouns or multiple modifiers  the class constraint becomes a conjunctive constraint class constraints on words such as  unremarkable  which are of class  root class  are dropped as vacuous 
cn type sign or symptom 
subtype present 
extract from prep phrase 'with 
verb =  null  
subject constraints 
	words include 	unremarkable 
prep phrase constraints preposition =  with  words include 
the exception of mild shortness of 
breath and chronically swollen ankles 
modifier class  sign or symptom  head class 
 sign or symptom   body location or region  
figure 1 an initial cn definition  with exact words from the instance 
   it is highly unlikely that this cn definition will ever apply to a sentence from a different text  but it is guaranteed to operate properly on the sentence that motivated it the initial cn definitions created from a train-
ing corpus are too tightly constrained to be useful until crystal relaxes some of the constraints semantic constraints are relaxed by moving up the semantic hierarchy or by dropping the constraint exact word constraints are relaxed by dropping all but a subsequence of the words or dropping the constraint the combinatorics on ways to relax constraints becomes overwhelm-
ing there are over 1 possible generalizations of the initial cn definition in figure 1 
the c r y s t a l algorithm 
initialize drctioniry and training instances database 
do until no more initial cn definitions in dictionary 
d = an initial cn definition removed from the dictionary 
loop 
d = the most similar cn definition to d 
	if d' = null 	exit loop 
u = the unification of d and d 
test the coverage of u in training instances 
if the error rate of u   tolerance exit loop 
delete all cn definitions covered by u 
set d = u 
add d to the dictionary 
return the dictionary 
   the following section shows how crystal induces generalisations from a set of initial cn definitions  testing that each proposed definition does not over generalise 
1 	i n d u c i n g g e n e r a l i z e d c n d e f i n i t i o n s 
crystal finds useful generalisations of its initial cn definitions by locating and comparing definitions that are highly similar let d be the definition being generalized assume we have found a second definition  d'  which is very similar to d according to a similarity metric that counts the number of relaxations required to unify two cn definitions a new definition u is then created with constraints relaxed just enough to unify d and d' the new definition u is then tested against the training corpus to make sure that it does not extract phrases that were not marked with the cn type and subtype being learned 
   if u is a valid cn definition  crystal deletes from the dictionary all definitions covered by u  thus reducing the sue of the dictionary while still covering all the positive training instances in particular  d and d' will be deleted the definition u becomes the current definition and this process is repeated  using similar cn definitions to guide the further relaxation of constraints eventually a point is reached where further relaxation would produce a definition that exceeds some pre-specified error tolerance at that point  crystal begins the generalisation process on another initial cn definition until all initial definitions have been considered for generalization 
   faced with an exponential number of ways in which the constraints conld be relaxed  crystal relaxes exactly those constraints that allow the current cn definition to be unified with a similar definition an implementation that allows c r y s t a l to locate similar definitions efficiently is discussed in the next section 
   crystal unifies two similar definitions by finding the most restrictive constraints that cover both if word constraints from the two definitions have an intersecting string of words  the unified word constraint is that intersecting string otherwise the word constraint is dropped 

unifying two class constraints may involve moving up the semantic hierarchy to find a common ancestor of claues in the two constraints class constraints are dropped entirely when they reach the root of the semantic hierarchy if a constraint on a particular syntactic buffer is musing from one of the two definitions  that constraint is dropped from the unified constraints 
　as an example of unifying two cn definitions  suppose that one definition has the class constraint  sign or symptom  for the subject buffer and the other has the class constraint  laboratory or test result  these unify to  finding   their common parent in the semantic hierarchy if the direct object of one definition has a class constraint requiring both  disease or syndrome  and  acquired abnormabty  and the other only requires  disease or syndrome   then the unified class constraint on direct object will have only  disease or 
syndrome  
　rather than reprocessing the training texts each time it tests the validity of a proposed cn definition  crys tal uses the badger sentence analyrer to segment the training documents and creates a database of instances this database includes an entry for each segment of the training corpus  not just those with phrases marked as having relevant information if an instance meets all the constraints of a cn definition  but the phrase extracted had not been tagged with the appropriate cn type and subtype  it is counted as an error crystal accommodates a limited amount of noise in the training data by using an error tolerance threshold instead of calling a definition bad from a single extraction error this add1 robustness to the dictionary  which is needed when dealing with unrestricted text 
　the algorithm presented here has glossed over some implementation lssues of how to find the most similar cn definitions or test the coverage and error rate of a proposed cn definition against the training instances efficiently the following section discusses these issues  
which can be critically important in scaling up to a large training corpus 
1 	efficiency issues 	finessing 
intractability 
since each cn definition may have several constraints and a variety of ways to relax each constraint  there are an exponential number of generalizations possible for a given cn definition crystal has the challenge of producing a near optimal dictionary while avoiding intractability and maintaining a rich expressiveness of its 
cn definitions 
　crystal reduces the intractable problem of constraint relaxation to the easier problem of finding a similar cn definition relaxing the constraints to unify a cn definition with a similar definition has the effect of retaining the constraints shared with another valid definition and dropping accidental features of the current definition this is also guaranteed to produce a cn definition with greater coverage that either of the definitions being unified  since the coverage of the two cn definitions is disjoint 
finding similar definitions efficiently is achieved by indexing the cn definitions database by verbs and by extraction buffers in this way  crystal can retneve a list of similar definitions which is small relative to the entire database each of these is tested with a similarity metric that looks for intersecting classes and intersecting strings of words for corresponding syntactic buffers 
　testing a generalised definition's error rate on the training corpus is actually done on the badger instances database  a database of instances which have already been segmented by the badger sentence analyser the primary index is on verbs  including the  null  verb for sentence fragments testing a cn defi mtion that has a constraint on the verb can be done by retrieving a small percent of the instances  with most of the constraint testing done on pointers in memory without the need to retrieve the actual instance crystal drops the constraint on exact verb only after relaxing all other constraints as far as possible  to take full advantage of the efficiency of indexing by verb 
　crystal has been tested on a corpus of 1 hosp-i tal discharge reports  averaging just under one thousand words each  which produced 1 training instances with 1 positive instances of  diagnosis  and 1 positive instances of  sign or symptom  crystal is able to induce a dictionary of all cn types and subtypes from this training set in about 1 minutes of clock time on a dec alpha axp 1 using 1 mb of memory 
1 	experimental results 
experiments were conducted in which 1 annotated hospital discharge reports were partitioned into a training set and a blind test set dictionaries for each cn type and subtype were induced from the training set and then evaluated on the test set performance is measured here in terms of recall and precision  where recall is the percentage of possible phrases that the dictionary extracts and precision is the percentage correct of the extracted phrases for example if there are 1 phrases that could possibly be extracted from the test set by a dictionary  but the dictionary extracts only 1 of them  recall is 1% if the dictionary extracts 1 phrases  only 1 of them correct  precision is 1% 

figure 1 effect of the error tolerance setting on perfor-
　the choice of error tolerance parameter has a signifi cant impact on performance and can be used to manipulate a tradeoff between recall and precision of a dictio-
	soderland etal 	1 

nary figure 1 ihowg performance of a dictionary of cn definitions that identify  sign or symptom  of any subtype  where the error tolerance is varied from 1 to 1 the results shown here are the averages of 1 random partitions of the corpus into 1% training and 1% test documents for each error tolerance 
　there is another parameter that affects recall and precision the results in figure 1 are for generalized cn definitions  those with coverage  = 1 precision can be boosted by raising the minimum coverage threshold at eiror tolerance of 1 the dictionary for  sign or symptom  had precision 1 and recall 1 at minimum coverage of 1  precision 1 and recall 1 at minimum coverage of 1  and precision 1 and recall 1 at minimum coverage of 1 
　the dictionary for all cn types and subtypes had 1 cn definitions that covered 1 or more training instances  1 that covered from 1 to 1  and 1 with coverage of 1 this is for a dictionary induced at error tolerance 1 from all 1 training instances 
　to aaaess the learning curve as training size increases  we set the training partition at 1%  1%  1%  1%  and 1% of the 1 annotated documents this was done 1 times for each training size and results averaged the number of positive training instances in a partition depends on what cn type and subtype is being learned 
　the graph in figure 1 plots recall against the number of positive training instances for the most frequent cn type and subtypes m the corpus recall is over 1 and still increasing at this level of training for  diagnosis  and  sign or symptom  of any subtype with this error tolerance set at 1 and minimum coverage at 1  precision remains fairly constant regardless of training size  at about 1 for most cn types and subtypes 
the difference in coverage for  symptom  absent  and 
 symptom  present  is due to a limitation in negation handling by the current version of crystal cn definitions for absent symptoms can include a constraint requiring the word  no   but crystal has no mechanism to require the absence of  no  for present symptoms the next version of crystal will handle conjunctions  disjunction  and the scoping of negation 
　here are a few representative cn definitions to give a feeling of what crystal is learning symptom  present is extracted from the direct object when the verb is  revealed  and the direct object has the word ''a  and head class  finding  this covers cases where a test reveals  a mass   ''a pleural effusion    a murmur  and so forth if the constraint on  a  is relaxed the cn definition would erroneously identify  no murmurs  or  normal bowel sounds  which are symptoms of subtype absent 
　a similar cn definition that identifies symptom  absent in the direct object requires the verb  revealed  and the word  no  in the direct object this has over 
1% accuracy with no semantic constraints another cn definition with no semantic constraints finds symptom  absent in a prepositional phrase with the preposition  without   covering 1 training instances with 1% error rate if a constraint is added to require the class  finding  in the prepositional phrase  coverage 

drops to 1 with 1% errors 
some cn definitions look for specific words  such as 
 normal    unremarkable    regular   and  negative  to identify symptom  absent  or  enlarged    increased    mild''  and  supple  to identify symptom  present 
　the lack of semantic constraints on most of the high coverage cn definitions may be attributed to gaps in the semantic lexicon we refrained from customising the umls medical metathesauius for our particular task  since our main research interest is designing portable systems with a minimum of knowledge engineering umls  unfortunately  has spotty coverage of clincial terms it's remarkable that we have gotten good performance from a lexicon that lacks words such as  lesions    rate    rhythm    tenderness   and  distention  
1 	related w o r k 
previous research on inductive learning in natural language processing has concentrated on the semantics of isolated words  granger  1  mooney  1  zermk  1  crystal is one of the first systems to automati cally induce a dictionary of information extraction rules only two of the seventeen research sites participating in the arpa-sponsored fifth message understanding conference  muc-1  1  described automatically generated dictionaries  the university of massachusetts and the university of southern california 
　the umass system used the autoslog dictionary construction tool  riloff  1   which generates a proposed cn definition for each phrase to be extracted from a motivating instance in the text autoslog uses heuristics to select certain exact words from the instance as  trigger words   often selecting the head verb the semantic constraints on the extracted buffer are set in advance by the user autoslog made no attempt to relax constraints  merge similar cn definitions  or test proposed definitions on the training corpus each proposed defi mtion had to be reviewed by a human who retained the definitions that looked reasonable  and discarded those that were more dubious  roughly 1% of autoslog's pro-

posed definitions had to be manually discarded  
	the 	p a l k a 	system 	 moldovan 	and 	kim  	1  
moldovan et a l   1  used by ljsc includes an induction itep simdar to crystal palka constructs an initial  frame-phrasal pattern structure   fp-structure  from each example clause that has been marked as having information to be extracted the fpstructure includes a constraint on the root form of the verb and semantic constraints on noun groups except for prepositional phrases with no marked information p a l k a generalises the semantic constraints by moving up the semantic hierarchy and specializes by moving down until the fp-structure coders all the applicable positive training instances and none of the negative p a l k a is not tolerant of noise in the training data  and a single negative instance can block an otherwise g&od generalisation 
   crystal allows more expressive extraction patterns than either autoslog or p a l k a while autoslog requires an exact word constraint on the trigger word or words  which was determined heunstirally from a single instance  crystal allows an exact word constraint on any words it learns to be significant  and it can also learn cn definitions with no word constraints palka's fp structures constrain the root form of the verb but allow no other exact word constraints unlike autoslog and p a l k a   crystal makes no a priori decision on which constituents are to be included in its cn definitions 
   the inductive concept learning in crystal is similar to an inductive learning algorithm described by mitchell   a  specific-to-general  data driven search to find the most specific generalisation that covers all positive and no negative instances crystai has the same goal  but uses a greedy unification of similar instances rather than breadth-first search this does not guarantee that crystal will always find the optimal dictionary  but in practice  crystal dictionaries appear to be near optimal 
	another survey of inductive 	concept learning  	by 
michalski  describes a  star methodology'1 that is quite similar to crystal algorithm this methodology is capable of learning a set of multiple generalizations to cover the positive instances by keeping only the best unification rather than branching on all possible gen eradications michalski sees this methodology in terms of a set covering algorithm  with the goal of merging generalizations to find the minimum set that cover all positive training instances  whde avoiding negative instances this is exactly the goal of crystal 
1 	conclusions 
crystal is one of the first systems to automatically derive a conceptual dictionary from a training corpus  and represents an improvement over previous attempts to derive text analysis rules from training examples the goal of crystal is to find the minimum set of generalised cn definitions that cover all of the positive training instances and to test each proposed definition against the training corpus to ensure that the error rate is within a predefined tolerance crystal's error tolerance parameter also allows a user to manipulate a recallprecision tradeoff 
　the requirements of crystal are a sentence analyzer  a semantic lexicon that maps individual words into classes in a semantic hierarchy  and a set of annotated training texts the approach taken by crystal facilitates a turnkey information extraction system that requires no familiarity with language processing tech nologies on the part of the end-user who creates an annotated training corpus crystal then uses the training to build a fully functional conceptual dictionary that requires no further knowledge engineering 
