. 
	while 	being 	a 	powerful 	paradigm 	for 	solving 	constraint 
satisfaction problems  csps   consistency techniques  cts  have never been taken into account during the design of declarative programming languages. this paper defines a theoretical framework for using cts inside logic programming. three inference rules are introduced and their formal properties are investigated. also  computation rules are defined which are worth considering wrt the inference rules. as practical results  the programmer can write   generate & test  programs while the interpreter/compiler will use cts for solving them  e.g forward checking or arc consistency . this makes logic programming not only a good language for stating csps but also an efficient tool for solving them as confirmed by our first experiences. 
1. motivation 
　our work aims at the integration of cts inside a declarative programming language in order to solve constraint satisfaction problems  csps . 
　the class of csps is of great importance in ai as  for instance  graph colouring  graph isomorphisms and homomorphisms  boolean satisfiability  scene and edge labeling 
and logical pussies can be seen as particular cases of it. a csp can be defined in the following way. assume the existence of a finite set 1 of variables which take respectively their values from finite domains and a set of constraints. a 
constraint between k variables from 1 is a subset of the cartesian product of the respective variable domains which specifies which values of the variables are compatible with each other. a solution to a csp is an assignment of values to all variables which satisfies all the constraints and the task is to find one or all the solutions. 
1 	architectures and languages 　cts are a powerful paradigm for solving csps. they should be contrasted to  generate & test   standard backtracking  depth-first search with chronological backtracking  and dependency-directed backtracking. clearly   generate & test  is an unfeasible search procedure as soon as the sise of a problem makes it- interesting to consider.  generate & test  corresponds to the naive way of writing logic programs for csps. for instance  a  generate & test  for the 1-queens problems generates a possible configuration for the queens and then test the constraints. standard backtracking  st   while being a substantial improvement over  generate & test   leads to a pathological behaviour known as thrashing. in logic programming  st corresponds generally to a  generate & test  program with coroutining informations or to a program written for testing the constraints as soon as possible. dependencydirected backtracking  has been introduced in order to avoid one of the thrashing symptoms but it is more a remedy to a symptom of the malady and not to the malady itself. indeed  it is better to prevent failures than to react . 1 muenchen 1  west germany 
intelligently to them. more generally  the drawback of standard and dependency-directed backtracking lies in the way they reduce the search space  only in an  a posteriori' way after having discovered a failure. contrarily  cts  1  1  1  prevent jauuret and reduce the search space in an  a prion' way before discovering a failure by removing combinations of values which cannot appear together in a solution. this leads to an early detection of failures and reduces both the amount of backtracking and the number of constraint checks. 
　the paradigm behind cts has been the basis for some problem-solvers like hef-arf |1j and alice  1| but it has never been taken into account in the design of  high-level  programming languages although its importance in this context has been stressed elsewhere |llj. however  they are some inherent interest for using cts m the design of declarative languages and not to restrict our attention to problem solvers. programming languages  contrary to problem-solvers  should lead to a greater flexibility with respect to the set of constraints  used to define a csp  and the strategy  used to solve it . among declarative programming languages  logic programming is very appropriate for integrating cts due to its relational form which makes it an adequate tool for expressing csps and its freedom of control which makes it adequate for integrating different paradigms. 
　our work atrm at a declarative logic programming language integrating cts. the objective is to preserve the expressiveness of logic programming while using the efficiency of cts. this allows the programmer to write a  generate & test  program for stating his problem while the interpreter/compiler will use cts for solving it. for this purpose  three new inference rules are introduced  say the forward checking inference rule  the fcir   the lookahead inference rule  the lair  and the partial lookahead inference rule  the pla1r   which are general mechanisms for using cts in logic programming. examples of their uses have been described in |1  1 . in the second paper  performance measures on several problems have been given which show the feasibility and the importance of the approach. its main advantage lies in the duality generality/specialization and the total freedom wrt the strategy. the duality generality/specialization comes from the fact that  on one hand  the inference rules are not restricted to a particular set of predefined constraints but can be used for logic programs and  on the other hand  they can be specialised  i.e. built-in  for some constraints by taking into account their particular properties leading to a very efficient handling of these constraints. the freedom wrt the strategy comes from the fact that these inference rules can be combined inside the same program  i.e different strategies can be used for different kinds of constraints  this is important as some constraints are more appropriate for a forward-checking use while others are bestsuited for a lookahead use. also  they can be combined either with domain-splitting or instantiation. 
　the present paper describes the theoretical framework for our approach. we show how sld-resolution can be extended in 

order to use cts in logic programming. for this purpose  the next section introduces the domain concept and the unification algorithm is extended to handle it. next  the inference rules are defined and their formal properties are investigated. 
 our approach should be related in philosophy to the works on constraints in logic programming  e.g |1  1  . it differs from them as we are not restricted to a set of built-in constraints. genera  mechanisms are provided which can be used for any constraint to solve problems in a well-defined class. 
1. domains in logic programming. 
1. the domain concept. 
 domains provides the basis for using cts in logic programming. it is often the case that variables range over a finite domain but this information cannot be expressed clearly in logic programming languages. domain declarations have been introduced for taking this fact into account  1 . 
definition 1: a domain declaration for predicate symbol p of arity n is an expression of the following form. 
domain p a1. ... a   where a. is either h or d.. 
when a. is equal to h  this means that the i argument of p ranges over the herbrand universe. otherwise  it means that the i argument is a list of variables which ranges over d1 in the following  the domains di are finite and explicit sets of values  i.e constants . 
definition 1: let dl ... dn the domains appearing in the domain declarations of a logic program pr and different from the herbrand universe. we note d pr  the set { d 
 we call it the domain 
set of the logic program. the domain set of a logic program contains all domains we possibly need during the computations. 
 the resulting language is a first-order language with aggregate variables  1 . this means that the domains must be seen as unary relations and an aggregate variable as a variable which ranges over this unary relation. in the following  we refer aggregate variables as d-variables  domain variables   we note x a variable x ranging over d and we use a d to denote d a  where a is a constant. also  we use and vp to denote the existential and universal closures of p. two rules are added to the usual first-order validity rules. we assume an interpretation 1 and a variable assignment a. a x/y  is a with x assigned to y and |d| is the unary relation d defines in i. the terms are constructed as usual except that variables can now be usual variables and d-variables. 
1. if the formula has the form  then the truth value of the formula is true if there exists such that f has truth value true wrt 1 and 
a x |d ; otherwise its truth value is false. 
1. if the formula has the form then the truth value of the formula is true if for all we have that f has truth value true wrt i and 
a x |d ; otherwise its truth value is false. 
the unification algorithm must be extended to take the domains into account. informally  a d-variable and a constant can only been unified if the constant is in the domain of the d-variable. also  when unifying a variable and a d-variable  the variable is bound to the d-variable. finally  two dvariables can only be unified if the intersection of their 
	van hentenryck 	1 

finitely many variables and each application of steps 1 .. 1 decreases by one the number of variables. we now prove that the unification algorithm does indeed find a tngu of a unifiable set of terms or predicates. it is a generalisation of the usual unification theorem  see for instance  1  . 
theorem s:  unification theorem . 
let s a finite set of terms or predicates. if 1 is unifiable  then the unification algorithm terminates and gives an mgu for s. if s is not unifiable  then the unification algorithm terminates and reports this fact. 
proof we have already noted that the algorithm always terminates. it suffices to show that if s is unifiable  then the algorithm finds an mgu. in fact  if s is not unifiable  then the algorithm cannot terminate at step 1 and  since it does terminate  it must terminate at step 1. thus it reports the fact that s is not unifiable. 
assume then that s is unifiable and let 1 be any unifier for s. we prove first that  for  the substitution given at the kth iteration of the algorithm  then there exists a substitution  such that 

suppose first that 	then wt can put since 
     next  suppose for some 	there exists 	such that 	is a singleton  then the algorithm terminates at step 1. hence we can confine attention to the case when  is not a singleton. we want to show that the algorithm will produce a further substitution 	and that there exists a substitution such that 	|vars s |. since  is not a singleton  the algorithm will determine the disagreement set ~ 	of sff. and go to step 1. since and 1 unifies s  it follows that 
unifies 	. thusmust contain either a variable or a d-variab e. 
suppose first that contains a variable v and let t another term of then v cannot occur in t because 
         we can suppose that 	is indeed the substitution chosen at step 1. thus 	we now d	e	f	i	n	e	h	a	s a binding for 
v  then 
docs not contain a variable  it contains a d-
variable. the steps 1 and 1 can be handled in the same way as the step 1. 


now we can complete the proof. if s is unifiable  then we have shown that the algorithm must terminate at step 1 and  if it terminates at the kth iteration  then  = 
　　|vars s |. since ak is a unifier of s  this equality shows that it is indeed an mgu for s. 
we refer sld-resolution with this extended unification algorithm as sldd-resolution. the notions of sldd-refutatlon and sldd-answer substitutions are defined by analogy to the sld case. the reader can verify easily that the mgu and lifting lemmas as well as theorems 1 and 1 in |1  hold for sldd-resolution. thus  sldd-resolution is both sound and complete for definite clauses. this result corresponds to the one of  which has shown that resolution is sound and complete when extended for handling d-varlablee also  the same result can be proved if we switch to a many-sorted logic since the domain set augmented by the herbrand universe and the empty set can be organised as a meet-semilattice . the domain concept is a necessary extension for consistency techniques to be applied but it not in it-self as interesting as other extensions like  for instance  login . before presenting the inference rules  we define the constraints we considered. 
definition 1s let p be a n-ary predicate symbol  p is a conttraint iff for any ground t e r m s e i t h e r  has a successful refutation or   has only finitely failed derivations. 
s. f o r w a r d checking in logic programming. 
 forward checking is often considered as one of the most efficient procedures for solving csps. intuitively  a constraint can be used in forward checking as soon as at most one variable occurs in it. in this case  the set of possible values for the variable is reduced to the set of values which satisfy the constraint. thus  a program based on forward checking 

gives a value to a variable  uses all constraints which contains 
1 	architectures and languages 

at moat one variable  chooses a value for che next variable  uses the constraint! and to on until all variables have received valuet. if  during the search  a constraint cannot be satisfied  it reduces the set of possible values for a variable to the empty set   the search procedure gives another value to the previously assigned variable. this section defines the fcir and its use for a general control mechanism  forward declarations  and for the implementation of some built-in predicates. they enable programs previously based on a st or  generate & test  search to use a forward checking strategy. 
1 . l . t h e inference rule. 

 since a ground instance of a constraint either succeeds or finitely fails  the set dnew in point 1 of the definition can be computed easily  for instance by using sldd-resolution  . the pcir can be seen as a general mechanism for enforcing nodeconsistency . 
 the fcir provides a theoretical foundation  1  for a general control mechanism  forward declarations  that can be used whatever the kind of constraints to be satisfied and  1  for the implementation of some built-in constraints. 
 forward declarations  provide a general method for using forward checking inside logic programming. a forward declaration for a predicate symbol p of arity n is an expression of the following form. 
the main interests of a efficient computation rule wrt the forward declarations are for expressiveness and efficiency. constraints can be stated before the generators and the interpreter/compiler is responsible to select them at an appropriate computation step. thus  forward declarations will act as preconditions to the selection of a predicate and can be selected as soon as possible for reducing the search space. this introduces a 'dolo drteen' computation as  for instance  in the constraint language of  1  and a generalised form of forward checking. 
 example i consider the problem of colouring a map in four colors. a logic program for solving this problem will include a constraint different x y  which holds if x and y are different colors. it can be defined as a finite set of assertions of the form 

 the fcir provides also a theoretical foundation for the implementation of built-in constraints which are the primitives of the logic language  e.g arithmetic constraints . in usual logic languages  these constraints can only be used for testing values and thus only reduce the search space in an  a posteriori way . however  the fcir can be specialised for these constraints which now can not only test values but can also prune the search space when only one d-variable is left uninstantiated. consider a non-equality constraint between integers  which holds if x and y are different integers . in usual logic languages  the non-equality predicate is implemented by mean of the  negation as failure  rule and thus can only be selected when both arguments are ground. by redefining this constraint as specialisation of the fcir  the same pruning as the above different predicate is achieved but in a more efficient way. 
1. 	properties of the f c i r . 
 we now prove the soundness and completeness of the fcir. the first two lemmas allow us to  remove  a value from the domain of a d-variable if this value does not satisfy a constraint. 
	van hentenryck 	1 

a 
1 	architectures and languages 

necessary solved by this treatment and mutt be reconsidered later on. the lair and tookahead declarations are introduced in logic programming in order to use lookahead whatever the kind of constraints used in the program. therefore  programs previously based on a st or  generate & test  search now use walts filtering-like algorithm or lookahead. this is especially important in areas like vision and qualitative reasoning  1|. 
1 . 1 . the inference pule. 

 the lair can be seen as a general mechanism for enforcing a k-consistency between the k lookahead variables. the lair reduces the search space in 'a priori way* and earlier than a forward checking use of the constraint. however  it takes also more computation time to produce this reduction. note also  that when only one variable occurs in p  the lair reduces to the fcir. 
 the lair is the theoretical foundation for a control mechanism called lookahead declarations. a lookahead declaration for a predicate symbol p of arity n is an expression of the following form. lookahead  where a. is either g either d. 
this declaration  which is unique for a particular predicate symbol  specifies that a l i t e r a l i n the resolvent can be selected only when all its arguments corresponding to a 'g' in the declaration are ground and when it is either ground or lookahead checkable. when it is lookahead checkable  the lair must be used to resolve it; otherwise  normal derivation is applied. 
 the computation rule is even more important for the lair than for the fcir. we first define a efficient computation rule wrt lookahead declarations. 
definition 1: a computation rule is efficient wrt the lookahead declarations  if a l i t e r a l i n the 
resolvent submitted to a lookahead declaration is only selected if either it is lookahead checkable or all its arguments are ground. 
a efficient computation rule wrt lookahead declarations gives 
ub few informations about when to select a lookahead constraint. it is clear that selecting it too early can induce some unproductive work  no new informations are inferred  and that a late selection reduces the pruning of the search space. it is not difficult to define efficient computation rules which select only lookahead constraints which are likely to produce new informations. the definition of the lair should not be seen as suggesting a particular implementation. actual implementations should be based  for instance  on generalisation* of ac-1 |ll  or ac-1 |1 . 
 example: consider a vision problem in a three-faced vertex world.  is the set of possible labels for the 
vertex. constraints in the problem are given by the  so-called  fork  l  t and arrow junctions. for instance  the fork junction can be defined as follows. 

 finally  the lair can be specialised for some constraints  e.g an inequality constraint between two integers . this will achieve the pruning defined by the lair in a very efficient way. 
1. properties of the l a i r . 
theorem 1: soundness of the lair. 
 the proof of this theorem is a simple generalisation of the soundness proof of the fcir. 
 there is no equivalent for the lair to theorem 1. thus  a proof procedure using the lair for lookahead checkable predicates and sldd-resolution otherwise will not be complete. the reason is that the lair is used only to remove inconsistent values and not for making choices. a sufficient condition to ensure completeness is to provide generators of values for the variables occurring in predicates submitted to lookahead declarations. this result was expected as the lair can be seen as a general mechanism for enforcing a kconttistency between the k lookahead variables and it is wellknown that enforcing a k-consistency throughout a network of constraints is not generally sufficient for solving arbitrary problems. 
1. partial lookahead in logic programming. 
 this section provides a theoretical basis for building-in some constraints in such a way that there are neither a specialisation of the fcir or a specialisation of the lair. it is motivated by the existence of some constraints for which  on one hand  forward checking is not appropriate  the reduction of the search space occurs too late in the computation  and lookahead use is too costly  in computation time  while  on the other hand  it is possible to use this constraint for reducing drastically the search space with a 
 small amount of computation. examples of such constraints 
	van hentenryck 	1 

are linear equation! and inequations on natural numbers which can be handled by a reasoning about variation intervals |1 . for instance  given 

since it reduces drastically the search space while inducing almost no overhead. since it can happen that not all the inconsistent values are removed  they are a special case of the plair. these constraints have been applied successfully in areas ranging from crypt-arithmetic to integer linear programming. we now define an inference rule we call the partial lookahead inference rule which is of no use per se but provides a theoretical basis for building in some particular class of constraints. the lair can be seen as a particular case of it. it consists in replacing the point 1 and s in the lair definition by the following two points. 

the set dzj is not defined in this inference rule and is dependent of each particular constraint. what we have defined is a theoretical framework for justifying certain kinds of specialisation. the soundness of the plair can easily be proved from the soundness of lair. 
1. conclusion. 
 cts are a powerful paradigm for solving csps. while being the basis for some successful problem-solvers  this paradigm has not been taken into account during the design of programming languages. however  there exists a inherent interest to build a declarative language based on this paradigm as it increases both flexibility for stating and solving the problem. 
 this paper has presented a theoretical framework for integrating cts inside logic programming. several new inference rules have been defined and their formal properties have been proved. also  the interest of some classes of computation rules wrt the expressiveness and the efficiency have been stressed in this context. 
 this makes logic programming not only a good language for slating csps but also an efficient tool for solving them as confirmed by our first experiments. 
acknowledgements. 
 1 gracefully acknowledge many helpful discussions concerning this research with m. dincbas  h. gallaire and h. simonis. in addition  l. vieille provided a careful criticism on a first version of this paper. 
