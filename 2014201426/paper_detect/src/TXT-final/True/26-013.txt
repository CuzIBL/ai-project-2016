 
this paper describes how meta-level theories are used for analytic learning in multi-tac. multi-tac operationalizes generic heuristics for constraint-satisfaction problems  in order to create programs that are tailored to specific problems. for each of its generic heuristics  multi-tac has a meta-theory specifically designed for operationalising that heuristic. we present examples of the specialisation process and discuss how the theories influence the tractability of the learning process. we also describe an empirical study showing that the specialised programs produced by multi-tac compare favorably to hand-coded programs. 
1 	introduction 
multi-tac  multi-tactic analytic compiler  is a learning system for constraint-satisfaction problems  csps . the system operationalises generic heuristics ll   producing problem-specific versions of these heuristics  and then attempts to find the most useful combination of these heuristics on a set of training problems. 
　this paper focuses on the knowledge that multi-tac uses in order to operationalize generic heuristics  and how this approach differs from previous work in  analytic   or knowledge-based  speed-up learning. previous analytic speed-up methods  such as ebl  chunking  and derivational analogy  have been used primarily for caching problem-solving experience. typical ebl systems  for example  learn from problem-solving successes and/or failures by caching a knowledge structure summarising the experience  e.g.  a chunk  and then reusing that knowledge during subsequent problem solving. in retrospect  relatively little attention has been paid to the theories  i.e.  the knowledge  used in the learning pro-
cess. however  this subject deserves more attention since the choice of theories determines what is learned. 
　multi-tac employs meta-level theories in the learning process. these enable the system to reason about the problem solver's base-level theory  as opposed to simply caching the generalised results of the problem solver's search. we argue that this approach is particularly appropriate when solving combinatorial problems  such as scheduling problems. 
　the system employs a rich variety of meta-level theories. this reflects a shift in research focus from  learning as a caching process  to  learning as an inferential process . the key is to find tractable meta-theories for generating useful search control knowledge. we outline two such theories  and discuss how the representation of the meta-level theories and the representation of the underlying constraint-satisfaction task influences the utility of the learning process. we also describe an empirical study in which multi-tac compared favorably with hand-coded programs. 
1 	theories and analytic learning 
informally  analytic learning systems are characterized by a  theory-driven  component that generates hypotheses by analyzing a domain. several analytic approaches have been used for speed-up learning  in which the goal is to improve problem-solving efficiency. for example  one approach is to apply ebl to generate possible search control rules. these rules serve as  hypotheses  which are tested by evaluating their utility on a set of problem instances. we will use ebl to illustrate our argument here  although a variety of analytic approaches have been employed for speed-up learning. 
　the simplest type of ebl normally utilized by a problem solver involves learning from success. in this case  the theory that is used in the explanation process is typically the same theory that the problem solver uses in its search. for example  in macro-operator learning  a simple form of ebl  an explanation is simply an operator sequence which solved a problem. 
　learning from failure is slightly more complex  but again  can often be accomplished by using essentially the same theory that the problem solver employs. for instance  if the base-level theory includes the axiom  p q1 v q1   a system can reason about the failure to derive  p  by using an equivalent form of the axiom  u p *-+ -*q1 a -*q1*. 
　in this paper we explore a more sophisticated approach in which the learning system is given additional  metalevel* knowledge. meta-level theories enable the learning system to reason about the base-level theory that the problem solver employs  so that the learning system can do more than simply generate inferences from the base-level theory. for example  we show how a domainindependent heuristic  such as the csp heuristic  prefer the least-constraining-value  can be automatically specialised for a particular problem by reasoning about what  least-constraining  means in the context of that problem. this process involves incorporating information that is not normally used by the base-level problem solver. although some forms of meta-reasoning have been employed in previous ebl systems  e.g.  learning from goal-interactions   this approach remains largely unexplored. 
　meta-level analysis is particularly appropriate when the base-level theory is intractable and there is no obvious skew in the distribution of solutions. for example  consider an np-hard problem such as scheduling or timetabling. it is unlikely that simply caching generalized solutions will prove very useful unless the distribution of instances is extremely biased so that some special cases arise repeatedly. in contrast  applying heuristics that are specialized to the problem is quite likely to be useful. such heuristics can improve average problem-solving performance on intractable problems without depending on a skewed distribution. 
1 	multi-tac 
multi-tac a speed-up learning system for solving combinatorial problems  such as shop scheduling  timetabling  and bin packing. such problems abound in industry and government. the system is designed for a scenario where instances of some np-hard problem must be solved routinely  such as a factory where a schedule must be devised each week. while all np-complete problems can be reduced to a single problem type  such as satisfiability  and then solved by some heuristic algorithm  this is generally a poor approach since the relative utility of many heuristics is problem-dependent . instead  one typically writes a program specifically for the problem at hand by modifying some  off-the-shelf  algorithm and adding appropriate heuristics. although np-complete problems are intractable in the worst case  relatively efficient programs can often be designed for specific applications. indeed  for certain np-complete problems  such as graph-coloring  simple heuristic methods can produce solutions in polynomial time with high probability even for  random  distributions l1 . 
　in this paper  we adopt the standard terminology of computer science and use the term  problem  to refer to a generic problem class and  instance  to refer to a particular problem instance. for example   graph-1colorability  is a problem that requires that each node in a graph be assigned one of three possible colors  such that no two neighbors have the same color. an instance of graph-1-colorability would consist of a specific graph and a specific set of colors. 
　multi-tag's input consists of a problem specification plus an instance generator for that problem. the instance generator is a  black box  that generates instance specifications according to some distribution. m u l t i tags' output is a lisp program that is tailored to the problem and the instance distribution. our goal is to synthesize programs that are as efficient as those written by competent programmers  not algorithms experts . doing so requires the system to have considerable expertise. however  it primarily requires expertise that would be  commonsense  to a programmer  as opposed to a knowledge base of operations research algorithms.  we are not opposed to the latter approach  it is simply not the focus here . 
　in order to present a problem to multi-tac it must be formalized as an integer csp  that is  as constraints over a set of integer variables. the problem are described using a first-order  sorted logic  a relatively expressive language for csps. a solution exists when all the variables are assigned a value such the constraints on each variable are satisfied. consider the problem  graph-1colorability . the nodes can be represented by variables whose value can range from 1 to 1. the constraints on variables are specified as follows: 
 iff  satisfies var vat  
 forall neigh-var such-that  edge neigh-var var  
 not  assigned neigh-var vat      
that is  a value satisfies the constraints on a variable iff all neighboring variables have not been assigned that value. the constraint language includes two types of relations  problem-specific user-defined relations such as edge and built-in system-defined relations such as satisfies and assigned. user-defined relations are de-
fined by explicitly listing their extension in the instance specification. there are variety of built-in relations  such as sum  greater-than  max  of a set   and union. 
the graph coloring constraint is particularly simple. 
however  a wide variety of problems can be specified in the language. consider another example  bin packing. each object is represented by a variable and each bin by a value. the user-defined relations object-size and bin-capacity relate each variable to a  size  and each bin to a  capacity . the bin-packing constraint is represented as follows  paraphrasing in english : a variable  object  can be assigned a value  bin  iff the bin's capacity minus the object's size is greater than or equal to the sum of the sizes of the other objects assigned to the bin. the program synthesis process employed by multitag is hierarchically organized. at the top level  the system chooses one of a set of generic constraint satisfaction search algorithms  including backtracking and iterative repair . currently only the backtracking strategy is implemented  so the remainder of the paper assumes a backtracking search. as in the standard csp backtracking search  the system selects a variable and then chooses a value for that variable. backtracking occurs when all values for a variable fail to satisfy the constraints. thus  two obvious points for search control knowledge are the choice of which variable to instantiate next and the choice of which value to assign. associated with the backtracking schema are generic heuristics for choosing variables and values  such as: 
  most-constrained-variable-first: this variable ordering heuristic prefers the variable with the fewest possible values left. 
  most-constraining-variable-first: a related variable ordering heuristic  this prefers variables that constrain the most other variables. 
	minton 	1 
  least-constraining-value-first: a value ordering heuristic  this heuristic prefers values that constrain the fewest other variables. 
  dependency-directed backtracking: if a value choice is independent of a failure  backtrack over that choice without trying alternatives. 
　to synthesize a program  multi-tac first operationalizes the generic variable and value-ordering heuristics  producing a set of candidate search control rules. this set may be large  typically between 1 and 1 rules in our experiments   since there are a variety of heuristics and each heuristic may be specialized and/or approximated in several different ways. a system configuration consists of a combination of control rules and a variety of flag settings controlling other heuristic mechanisms  e.g.  a flag indicates whether or not to use forward checking  . the system carries out a utility evaluation process in which it searches for the most effective system configuration using a hill-climbing search. during this search  the system evaluates the utility of a given configuration by compiling a lisp program that implements the configuration  and then  experimenting  with the program by running it on a set of instances.  the compilation process also includes a variety of additional optimization techniques  such a finite differencing l1  and constraint simplification . 
　the learning process impacts the efficiency of the target code in two ways. first  the generic heuristics are re-expressed as problem-specific rules. second  the utility evaluation process searches for a combination of these heuristics that yields the best overall performance. the resulting program may include several heuristics  which can act synergistically. 
　unfortunately  space does not allow a complete discussion of the architecture. in the next section  we describe how the generic variable- and value-selection heuristics are specialized  the main subject of this paper. 
1 	specializing generic heuristics 
multi-tac is based on the supposition that expert problem solving can result from combining a variety of relatively simple  generic heuristics. the key lies in specializing those heuristics to a given problem and then selecting the most useful combination. 
　as in the ebl paradigm   the learning process requires a target concept and a theory describing the target concept. the result is a specialization of the target concept  i.e.  a sufficient condition for the target concept . 
in m u l t i - t a c each generic heuristic  such as least-
constraining-value-first  is a target concept. to specialize the heuristic  we use a meta-theory which describes the heuristic.  for some heuristics there are several meta-theories  each one representing a different tactic for operationalising the heuristic.  the specialization method is very similar to the bbs method used in prodigy/ebl   except that the entire set of possible specializations is generated  as in etzioni's static    rather than using an example to guide the specialization process. as discussed in section 1  the meta-theories are designed so that the set of possible specializations is not prohibitively large. below we present two examples illustrating the specialization process. 
1 	least-constraining-value-first 
our first example illustrates the theory multi-tac employs to operationalize least-constraining-value-first. 
　the backtracking csp search is modeled as a series of state changes. in a given state  some variables have a value assigned  and the rest of the variables are unassigned. formally   holds  assigned var vat  s  if and only if variable var is assigned value val in state 1. in general  for any given state 1 and any statement in the constraint language  holds statement 1  if and only if statement is true in s. 
　assigning a value to variable varl constrains another variable varl iff the number of values that satisfy the constraints on varl is reduced. a value is a leastconstraining value if it would constrain the fewest number of other variables.  this is just one way to formalize the intuitive notion of  least constraining  . the generic control rule which expresses this notion  paraphrased for readability  is: 
if the current variable to be assigned is varl choose a value with the minimum score  where 
the score of val1 is the number of varl  such that 
there exists a val1 such that 
 and  satisfies vars val1 
 is-false-upon-assignment varl val1 
  satisfies var1 val1    
in other words  each candidate value vail for variable varl is scored by counting the number of other variables that are constrained when vail is assigned to varl  where variable varl is constrained if for some value val1   satisfies varl val1  changes from true to false.1 the semantics of  is-false-upon-assignment  can be formally characterized as follows. let  next-state s1 s1 varl val1l  be true if s1 is the state that results from s1 after val1 is assigned to varl. then  is-false-upon-assignment varl val1 statement  is true if: 
v1  
  holds statement s1  a  next-state s1 s1 varl val1 } =   not  holds statement s1    
　multi-tac derives a specialized control rule by operationalizing  is-false-upon-assignment . the system employs a theory designed specifically for this task. since space does not permit us to describe the complete theory  below we list the sequence of specializations carried out for our graph-colorability example.  we have taken a few liberties for readability . 
initial statement to be specialized: 
 is-false-upon-assignment varl val1   satisfies varl val1    
　　1 this is a local definition of  constrains   since we only consider the case where assigning a value to one variable directly constrains another variable. for example  if assigning a value to varl constrains varl  which in turn constrains var1  we do not say that assigning a value to varl constrains vars. 
expands into: 
 is-false-upon~as ignaent varl vail 
  forall neigh-var such-that  edge neigh-var vars  
 not  assigned neigh-var val1      
specializes to: 
 exists neigh-var such-that  edge neigh-var var1  
 is-falsa-upon-assigxunent varl val1 
  not  assigned neigh-var val1      
specializes to: 
 exists neigh-var such-that  edge neigh-var vars  
 and  equal varl neigh-var  equal val1 val1    
simplifying  we have: 
 and  edge varl var1  equal val1 val1   
as the example shows  the analysis proceeds by recursively propagating  is-false-upon-assignment  inward through the constraint. the meta-theory which guides this analysis describes how each type of statement should be specialized. for example  below we show an axiom for specializing a universally quantified formula.  this is used in the first specialization step above . 
 only-if 
 is-false-upon-assignment var vol 
  forall x such-that gen-statement test-statement    
 exists x such-that gen-statement 
 is-false-upon-assignment var val test-statement    
this axiom indicates that a universally quantified statement over a set will become false if the statement becomes false for at least one member of the set. there are similar axioms for conjunctions  disjunctions  simple predicates  etc. 
　incorporating the specialized result in our example back into the generic control rule and then simplifying gives us the following specialized version of leastconstraining- value-first: 
if the current variable to be assigned is varl choose a value with the minimum score  where 
the score of val1 is the number of var1  such that 
 and  edge var1 varl  
 satisfies var1 val1  
　to determine the score for the color red  for example  this rule counts the number of neighbors which themselves can be colored red  i.e.  the number of neighbors which do not themselves have a red neighbor . to understand why this makes sense  consider the extreme case where all neighbors already have a red neighbor. in this case  choosing red does not constrain any neighbor since none could be colored red in any event. thus the score for red will be 1  and so red will be a preferred value. 
　of what value is this specialization process  consider the generic  unspecialized rule that would be the alternative. this rule would simply count  for each value  how many variables would be constrained by this value. for graph colorability the complexity would be 1 k1nd   where k is the number of colors  n is the number of nodes in the graph  and d is the maximum node degree  i.e.  number of edges per node  since for every value  each unassigned node in the graph would be examined to see if any of its k possible values would be eliminated  at a cost of d per constraint check. in comparison  the cost of evaluating the specialized rule is 1 kd1   since for each value  each neighbor is examined and a constraint check of cost d is carried out.  note that d   n  so the savings is at least a factor of k.  
　for another illustration of the utility of this process  consider the bin packing problem described earlier. choosing a value to assign to a variable corresponds to choosing a bin for an object. if we simply try each bin in turn  this gives us the  first-fit  heuristic. specializing least-constraining-value-first gives us  a version of  the well-known  best-fit  heuristic  which prefers the bin with the least remaining capacity that will hold the object. to see why the  best-fit  heuristic is a specialization of least-constraining-value-first  consider that the bin with the least remaining capacity is a  possible bin  for fewer objects than any other bin. thus  putting the object in this bin constrains the fewest other ob-
jects.  note that there is an alternate argument that the least-constraining bin is the one with the  most remaining capacity . in fact  this results from an alternative specialization. the relative utility of these alternatives depends on the instance distribution.  
1 	identifying symmetric values 
we now describe a meta-theory that enables multi-tac to recognize certain types of symmetries and thereby eliminate unnecessary search. the result is a control rule that carries out a specialized form of dependencydirected backtracking. once again we will use graph colorability as an illustration. suppose that we are choosing the color for a node  and the possible values include red and green. if no node in the graph is yet colored either red or green  then these two colors can be considered equivalent. if coloring the node red results in failure  i.e. the remainder of the graph cannot be colored consistently   then it makes no sense to try green since it is guaranteed to fail as well. to see why this is true  consider that any solution where the node is colored green could be transformed into a solution where the node is colored red merely by interchanging the labels green and red on all nodes. 
this is an example of reasoning by symmetry  1; l . 
in the general case  we will consider symmetries where two values val1 and val1 are swapped  such that all the variables that are assigned val1 are re-assigned val1  and vice versa. a generic control rule for utilizing this information is  paraphrased : 
if assigning value val1 to variable var resulted in failure and value val1 is a possible value for variable var and no variable is assigned value val1 and no variable is assigned value vals and interchanging val1 and vals 
preserves the solution property 
then eliminate val1 as a possible value for var 
we can formalize the last antecedent as follows. let 
 interchange s1 s1 val1 vals  be true if s1 is the state that results from interchanging val1 and and val1 in state s1. then  interchanging val1 and val1 preserves the solution property if: 
	minton 	1 

vs1 s1   interchange s1 st val1 valt  
v var1   holds  satisfies vars valt  s1  
  holds  satisfies varx val1  st    
multi-tac's task is to operationally express the condition under which val1 and val1 can be interchanged. as we will show  for graph colorability the analysis reveals that this condition simplifies to true  since interchanging val1 and valt in any solution preserves the solution property. in other problems  val1 and valt can be interchanged only in certain circumstances. for instance  in the bin packing problem  interchanging two values is equivalent to swapping all the objects assigned to one bin with all the objects assigned to another bin. in this case  the specialisation process reveals that the two bins can be interchanged only if they have equal capacities. thus  for bin packing  the resulting search control rule indicates that if putting an object in a bin fails  then putting it in another bin will also fail provided that the bins have the same capacity and both are empty. 
　in order to operationally express the conditions under which two values can be interchanged  multi-tac specialises the statement below  using a theory specifically designed for this task. 
	 interchange-preserves-truth 	 val  
  satisfies var vat    
the meta-predicate  interchange-preserves-truth  is a two-place relation. the second argument  e.g.    satisfies var val   is the statement whose truth must be preserved by the interchange. the first argument  e.g.    val * is a list of the free  logical  variables in the statement that are affected by the interchange. 
 that is  if val = v a l 1 prior to the exchange then val = val1 afterwards  and if val = val1 prior to the interchange then val = v a l 1 afterwards  otherwise val is unaffected by the interchange.  
　for each type of statement in multi-tac's language  the meta-theory determines the conditions under which a value interchange will preserve the truth of the statement. multi-tac recursively analyzes the expression  keeping track of the terms to which the interchange applies. below we show the series of specializations used in the graph coloring example  after having expanded the definition of  satisfies  in the statement above. 
 interchange-preserves-truth  val  
  forall neigh-var such-that  edge neigh-var var  
　　　　 not  assigned neigh-var vol      specializes to: 
 forall neigh-var such-that  edge neigh-var var  
 interchange-preserves-truth  val  
  not  assigned neigh-var vat      
specializes to: 
 forall neigh-var such-that  edge neigh-var var   true   
simplifies to: 
 true  
the key step in this analysis is the last specialisation  where it is determined that the truth of   not  assigned neigh-var vat    will be preserved when val minimum maximal matching 
total 
cpu sec number unsolved project member 1 o multi-tac 1 o 1 subject 1 1 1 simple csp 1 	1 	| 1 
k-closure 
total 
cpu sec number unsolved project member 1 o 1 multi-tac 1 o 1 subject1 1 	1 	| simple csp 1 	1 	| figure 1: performance results on two problems 
is affected by the interchange  e.g.  if  not  assigned neighbor val1   is true prior to the interchange  then  not  assigned neighbor vai1   will be true after the interchange. 
　we are also implementing two related meta-theories to carry out more sophisticated analyses. the first considers interchanging values val1 and valt in a solution  except that those variables already assigned at the decision point are left unchanged. for graph coloring  this analysis reveals that if value val1 fails  value valt will also fail provided no node presently assigned val1 or valt is next to an unassigned node. for bin packing  this analysis reveals that if bin val1 fails  bin valt will fail provided both bins have the same remaining capacity. 
　the second meta-theory considers interchanging value val1 with val1+/  where i is some integer. this is not useful for either graph coloring or bin-packing  but is useful for traveling salesman  hamiltonian circuit  and some types of scheduling problems. consider the traveling salesman problem where the variables correspond to cities  and their values indicate the order in which the cities are visited. the analysis reveals that it does not matter which city is chosen to start the tour. 
1 	i n i t i a l r e s u l t s 
although multi-tac is still under development  the present version  multi-taci.o  is capable of synthesizing very good algorithms on some problems. for example  on graph coloring  multi-taci.o synthesizes the well-known  brelaz  algorithm. however  this is not very surprising since the author's previous familiarity with this algorithm influenced the design of multi-tac. a more interesting question is how the system performs on problems that are unfamiliar to the author and other project members. 
　in order to gauge the system's current effectiveness  we selected two problems from the book  computers and intractability    minimal maximal matching and kclosure. these problems could be easily expressed in 
multi-tac's language and  in addition  they appeared amenable to a backtracking approach  based on a cursory examination . two ph.d.-level computer scientists working on unrelated projects volunteered to write pro-

grams that we could use for comparison. in addition  one of the multi-tac project members was asked to write programs for both problems. the humans were given access to the same instance generators as multi-tac for the purpose of testing their programs.1 the instance generators used simple random techniques to generate instances; we did not attempt to construct  tricky  instances. the humans were asked to write the fastest program they could  given their busy schedules  and they spent between 1 and 1 hours coding their programs. 
　for both problems  figure 1 compares target code generated by multi-tac with the hand-coded programs and with an unoptimized csp engine  with no heuristics . the figures show the cumulative running time on 1 instances of both problems. the programs were allowed to run for a maximum of 1 cpu seconds per instance  so the figures also report for each program the number of instances that were not solved within this time bound. the results clearly indicate that multi-tac's target programs were more efficient than those of our two volunteers  subject 1 and subject1   although the project member was able to produce even faster code on both problems. also  it is clear that the unoptimized csp engine performed poorly. 
　interestingly  the project member's programs were significantly faster than our two subjects' programs. we believe that this illustrates the importance of expertise in writing heuristic algorithms. whereas the project member has become something of an expert on heuristics for combinatorial problems through his work on m u l t i tac  the other two subjects do not work in this area. 
　obviously  the results are extremely promising. few  if any  previous speed-up learning systems have performed favorably against hand-coded programs. however  we note that these results do not address the generality of the system  since the problems were not randomly selected from  computers and intractability . thus  while the results indicate the potential of our approach  a more thorough evaluation must eventually be carried out. of course  multi-tac is still under development and with further work its efficiency and robustness will both improve.  see  for a complete description of these experiments  as well as followup experiments.  
1 d i s c u s s i o n : t r a c t a b i l i t y of a n a l y s i s 
multi-tac's approach to generating control knowledge is motivated in part by the prodigy/ebl system  and etzioni's subsequent work with static . like prodigy/bbl  multi-tac produces control knowledge by specializing meta-level concepts. however  prodigy's ebl module generates explanations from  first principles  in the following sense: the metatheories used to explain the examples are essentially descriptions of the problem solver that are interpreted during the explanation process. as a result  the explanations are long and complex  and the explanation process must be guided by heuristics so that useful explanations 
　　1 unfortunately  due to a misunderstanding  subject1 did not make use of the instance distribution and thus undoubtedly was at a disadvantage. 
can be generated tractably. 
　in contrast  although static is designed to learn the same type of control knowledge as prodigy/bbl  static's meta-theories are much simpler. by  simpler   we mean that the search tree of possible specializations is much smaller. as a result a much higher proportion of the possible specializations can be examined  all of them  in fact . even though the number of possible specializations is considerably smaller  the useful specializations tend to be included. to a large extent  this observation motivated the development of multi-tac. 
　how is it that static's theories can be simpler  yet still include the useful specializations  there are at least two contributing factors that we have identified. the first is that static's theories are written in a more abstract language than prodigy/ebl's. although static' theories describe the same target concepts as 
prodigy/bbl  success  failure  goal-interaction   the theories describe the target concepts directly in terms of the planning operators in the base-level theory  what etzioni called a  problem-space graph  . in comparison  prodigy/ebl's theories have an extra layer of indirection  since the meta-theories describe the target concepts in terms of how the problem solving architecture operates on the base-level theory. the second factor is that the complexity of static's analyses is depth-bounded in accordance with etzioni's hypothesis that  recursive  explanations are unlikely to be useful. 
　in designing multi-tac  we borrowed both of these ideas in an effort to make the analyses as simple as possible. more precisely  our goal is for the search tree of possible specializations to be small enough so that we can tractably generate the entire tree. like static  the metarlevel analyses in multi-tac operate directly on the base-level theory  i.e.  on the constraints in the problem specification. we have not attempted to axiomatize the problem solver  as in prodigy/bbl. instead  the meta-theories used in multi-tac are each designed for specializing a particular generic heuristic  as illustrated by the two theories described in the last section. compared to the  first principles  approach used in prodigy/bbl  multi-tac's approach is more like that of an  expert system . 
   in addition  multi-tac's analyses are depth-limited  reminiscent of the way static's analyses are likewise limited. specifically  the meta-theories in multi-tac are not used to analyze chains of constraints. the system only analyzes direct constraints between variables. in fact  because the specialization process operates by recursively processing the constraint specification  as illustrated in the previous section   the depth of the tree of specializations is limited by the fixed size of the constraint specification. for example  at compile time multi-tac can analyze the conditions under which two variables are arc-consistent  or  by analogy  arc-independent  but cannot analyze the conditions under which two variables are path-consistent  or pathindependent. the only way for the system to make use of these latter concepts is for the problem solver to transitively apply the  arc  properties at runtime. 
the guiding principle  perhaps  hypothesis  would be 
	minton 	1 

more accurate  underlying multi-tac's specialisation process is that relatively simple analyses can produce useful control knowledge. indeed  our previous experience with prodigy/ebl and static indicates that we are much more likely to be successful if the analyses are simple. as discussed by etsioni and minton  as the proofs become more complex  ebl is more likely to generate specialisations that are overspecific in that they include irrelevant conditions. the complexity of finding a  good  specialisation  or what etsioni and minton call a minimal sufficient condition grows with the size of the theory.1 in multi-tac we have reduced the size of the meta-theories using the techniques outlined above  such that a complete static evaluation typically produces between 1 and 1 search control rules. in contrast  prodigy/ebl is capable of producing an unlimited number of search control rules as the size of the instances increases  most of which are useless. 
　unfortunately  designing tractable theories requires considerable expertise. in the future  we hope to address this issue. one possibility is to start with intractable theories and to incrementally refine them. 
　multi-tac bears some resemblance to automatic programming systems that refine a high-level specification by applying correctness-preserving transformations. we were motivated particularly by smith's kids system  and related work on knowledge compilation  e.g.  1; 1  . our approach is primarily distinguished from these systems by the use of machine learning  in that we generate alternative search control rules and then test them on examples. this approach enables the system to be completely autonomous  in contrast to transformational systems that require the user to direct the transformational process. nevertheless  the approach we employ for representing and reasoning about constraints could also be employed in a transformational system. 
　recently  ellman  and yoshikawa and wada  have proposed new methods for improving csp search. in the future we hope to incorporate these into m u l t i tac  giving it a broader range of possible optimizations. 
1 	conclusion 
this paper has advocated the use of meta-level theories for analytic learning. we illustrated this approach with two meta-level theories used for speeding up constraint satisfaction in multi-tac. each met a-theory is designed for operationalizing a specific heuristic in such a way that the number of specializations is limited. in this sense  multi-tac can be considered an expert system for operationalizing the generic heuristics. this approach appears quite promising; in an empirical study  target code produced by multi-tac compared favorably with hand-coded programs. 
1 	acknowledgements 
i am indebted to several colleagues for their significant contributions to multi-tac: jim blythe  gene davis  
1
　　unfortunately  a single example  as used in ebl  does not necessarily help the system discriminate between useful and useless specialisations during the explanation process. 
andy philips  ian underwood and shawn wolfe. peter cheeseman  oren etsioni  rich keller  phil laird  and 
mike lowry commented on drafts of this paper. and last  but not least  bernadette kowalski minton helped to generate the ideas behind multi-tac. 
