 
methods for conceptual clustering may be explicated in two lights. conceptual clustering methods may be viewed as extensions to techniques of numerical taxonomy  a collection of methods developed by social and natural scientists for creating classification schemes over object sets. alternatively  conceptual clustering may be viewed as a form of learning by observation or concept formation  as opposed to methods of learning from examples or concept identification. in this paper we survey and compare 
a number of conceptual clustering methods along dimensions suggested by each of these views. the point we most wish to clarify is that conceptual clustering processes can be explicated as being composed of three distinct but inter-dependent subprocesses: the process of deriving a hierarchical classification scheme; the process of aggregating objects into individual classes; and the process of assigning conceptual descriptions to object classes. each subprocess may be characterized along a number of dimensions related to search  thus facilitating a better understanding of the conceptual clustering process as a whole. 
i. introduction 
　　　classification is a process critical to the success of an intelligent organism. the ability to classify objects  events  states  observations  etc.  as members of object families or concepts  is the basis of all inferential capacity. work in artificial intelligence has concentrated significantly on developing mechanisms for classification  and the conceptual representations necessary to support these mechanisms. machine learning research  specifically work in learning from examples  has facilitated a better understanding of processes of concept identification  that is the derivation of concepts for a teacher imposed classification. learning from examples however  has not addressed the problem of how a learner can originate classes  but only how conceptual descriptions can be assigned to externally provided classes. recently methods of conceptual clustering have been forwarded  which do provide  partial  
solutions to the object class origin problem. 
f this work was supported by contract n1-k-1 from the information sciences division  office of naval research. 
     methods of conceptual clustering are best explicated and compared with respect to two alternative  but complementary views. 
two views of conceptual clustering 
1  methods of conceptual clustering are viewed as extensions or analogs to techniques of numerical taxonomy  a collection of methods developed by natu-
ral and social scientists used to form classification schemes over data sets. 
1  already alluded to is that conceptual clustering is a form of concept formation or learning by observation as opposed to learning from examples. 
　　　each of these views has utility in explicating processes of conceptual clustering  and each view will contribute to a unified set of dimensions along which we may characterize various conceptual clustering techniques. 
n. conceptual clustering and numerical taxonomy 
　　　conceptual clustering is a process abstraction originally motivated and defined by michalski  1  and michalski and stepp  1a  as an extension of processes of numerical taxonomy. any clustering method  whether it be of the conceptual clustering or numerical taxonomy 
variety may be abstracted as follows. 
the abstract clustering task 
given: a set of symbolically described objects  o. 
task: 	distinguish clusters  ie. subsets of o   
c  ...  c n   such that the set of clusters 
 ie. a clustering  is of high quality  perhaps not optimal  with respect to a clustering quality function. 

     methods of numerical taxonomy cluster objects that are symbolically described as sets of variable-value pairs  ie. attribute-feature pairs . in methods of numerical taxonomy  the quality of a clustering is a function only of the clusters of the clustering. that is  numerical taxonomy techniques attempt to find a clustering which maximizes a  numeric  quality function of the following form. 

     despite the usefulness of numerical taxonomy techniques  any such method suffers from a major limitation  in that the resultant clusters may not be well characterized in some human-comprehensible conceptual language. this limitation can be of concern to a data analyst  or learning program  who  which  wishes to abstract the underlying conceptual structure of object groups in order to hypothesize about future observations  or to simply compress the data in an intelligent  easily recoverable way. michalski  1  defines conceptual clustering as an extension over the techniques of numerical taxonomy  which directly addresses the problem of determining conceptual representations. in methods of conceptual clustering  the quality of a clustering is dependent on the quality of concepts which may be used to characterize clusters of the clustering  eg. the 'simplicity' of concepts  and/or the map between concepts and the clusters they cover  eg. the 'fit1 or generality of derived concepts . that is  methods of conceptual clustering seek to obtain clusterings which maximize a quality function of the following form. 

where concepts is a set of concepts which may be used to describe object clusters.1 
     conceptual clustering algorithms which have been framed as extensions to numerical taxonomy techniques include cluster/1 by michalski and stepp  1a  
1b   discon by langley and sage  1   and the rummage program by fisher  1 . a number of other algorithms  although not explicitly labeled conceptual clustering techniques  but which nonetheless can be framed as such  include glauber by langley  zytkow  simon  and bradshaw  1   mk1 by wolff  1   and lebowitz' epp  lebowitz  1  and u n i m e m  lebowitz  1  systems. each of these systems has a rough analog with some methods of numerical taxonomy which we now touch upon. 
     the literature on numerical taxonomy distinguishes three classes of methods  everitt  1 . 
optimization techniques of numerical taxonomy form a 'flat'  ie. unstructured  set of mutually exclusive clusters  ie. a partition over the input object set . optimization techniques make an explicit search for a globally optimal k-partition of an object set  where k is a user supplied parameter. this search for globally optimal partitions make optimization techniques computationally expensive  thus constraining their use to small data sets and/or small values of k. 
hierarchical techniques form classification trees over object sets  where leaves of a tree are individual ob-
jects  and internal nodes represent object clusters. a 'flat' clustering of mutually-exclusive clusters may be obtained from the classification tree by severing the tree at some level. hierarchical techniques are further divided into divisive and agglomerative techniques  which construct the classification tree top-down and bottom-up  respectively. hierarchical techniques depend on 'good' clusterings arising from a series of 'local' decisions. in the case of divisive techniques  a node in a partially constructed tree is divided independent of other  non-ancestrial  nodes of the tree. the use of 'local' decision-making in hierarchical methods make them computationally less expensive than optimization techniques with an associated probable reduction in the quality of constructed clusterings. 
clumping techniques return clusterings where constituent clusters possibly overlap. the possibility of cluster overlap stems from independently treating some number of clusters as possible hosts for an object which must be incorporated into a clustering. 
     we can impose a classification on conceptual clustering methods analogous to the one just discussed for methods of numerical taxonomy. the ptlrtitioning module of cluster/1 by michalski and stepp can be viewed as a conceptual optimization technique which given an ob-
ject set to be partitioned and a parameter  k  specifing the number of desired clusters  ie. the partition size   attempts to construct an optimal k-partition of the object set. the partitioning module is computationally expensive and is prohibitive for large values of k. the hierarchybuilding module of cluster/1 is a conceptual hierarchical technique which builds a classification tree top-down  ie. it is a divisive technique . in dividing each node in the classification tree  the hierarchy-building module calls the partitioning module for small partition sizes  ie. k   and selects the optimal partition from among these possibilities. other divisive hierarchical techniques of conceptual clustering include discon and rummage. both rummage and discon form monothetic classification trees in which any set of siblings in the tree are distinguished by their value along a single variable. in contrast  cluster/1 allows arcs to be labelled by a conjunction of values across several variables  and thus cluster/1 

forms polythctic classifications. discon  unlike both rummage and cluster/1  discovers an optimal classification tree  in terms of the number of nodes in the completed tree   whereas the latter two algorithms seek only to independently optimize the division of each node  in the hopes that the resultant trees will be of 'high quality'. mk1 by wolff represents an agglomerative hierarchical technique. conceptual clumping techniques include dpp and unimem by lebowitz and glauber by langley et.al.. each of these systems builds classification schemes equivalent to reentrant  acyclic graphs  where each node represents a cluster  and objects may be included in multiple clusters. 
     the view of conceptual clustering methods as extensions to methods of numerical taxonomy has served as a vehicle for presenting the input-output behavior of a number of algorithms. for a better understanding the processing characteristics and utility of each of these techniques we turn to the view of conceptual clustering as learning by observation. 
i n . conceptual clustering as learning 
　　　an alternative view of conceptual clustering relates this task to the well-studied problem of learning from examples. both the conceptual clustering task and learning from examples are concerned with formulating some description that summarizes a set of data. in learning from examples  a tutor specifies which objects should be assigned to which class  and the learner must characterize each class. in conceptual clustering the learner has the two-fold task of creating object classes as well as characterizing these classes. thus there are two problems which must be addressed by a conceptual clustering algorithm  one of which is shared by processes of learning from examples. 
the aggregation problem is the problem of distinguishing subsets of an initial object set  that is the formation of a set of classes  each defined as an extensionally enumerated set of objects. the aggregation problem is addressed by tasks of conceptual clustering and not by processes of learning from examples which assume a set of classes has been supplied by an external source  ie. a tutor . 
the characterization problem is the problem of determining characterizations  ie. concepts  for an extensionally represented object class  or each of multiple object classes. this problem has been extensively addressed in work on learning from examples where object classes are presented by a tutor  and the learner is responsible for assigning a conceptual description to each class. in fact  the characterization problem  as defined here  and the problem of learning from examples are the same. conceptual d fisher and p. langley 	1 
clustering processes must address the characterization problem since cluster quality  as we have stated  is dependent on conceptual descriptions which may be used to describe clusters. 
　　　we do not mean to imply that the aggregation and characterization  ie. learning from examples  problems are independent  simply that they may be usefully modularized  thus allowing us to make use of the wealth of information regarding learning from examples in analyzing and formulating methods of conceptual clustering. 
　　　given this view  a natural approach to solving the conceptual clustering problem involves first solving the aggregation problem  and then using traditional methods of learning from examples to solve the characterization problem. in fact  present conceptual clustering algorithms can be framed in this way. for instance  glauber forms classes based on the most commononly occuring relation  defined over an object set  and then characterizes these classes with respect to the remaining relations. mk1 employs a very similar technique  in fact  glauber's method is based on mk1 . unimem and ipp construct a number of alternative classes each of which is based on the predictive features  ie. variable values  shared by all class members  and characterized by a conjunction of all predictable features shared by class members.1 
　　　both rummage and discon use a list of userspecified attributes to form possible partitions over an object set. rummage considers a number of partitions  each implied by the values of a distinct attribute and selects that partition  ie. clustering  which possesses the 'best' conceptual descriptions of objects over the remaining attributes. thus  rummage solves the aggregation problem by using individual attribute values to imply possible clusters  the values of a single attribute collectively imply a clustering   and then utilizes a learning from examples subroutine to characterize clusters in terms of the remaining attributes. rummage applies this method recursively to each of the resulting clusters  thus tracing out a single hierarchical classification scheme. like rummage  discon uses attribute values to imply possible partitions  thus solving the aggregation problem. unlike rummage  discon does not construct an explicit description of the devised clusters over the remaining attributes  but simply calls itself recursively on each of the possible clusters  thus forming a classification tree over the objects of each cluster with respect to the remaining attributes. both rummage and discon are to a greater or lesser extent based on quintan's id1 program for learning from examples  quinlan  1  an abstraction of the aggregation processes of both rummage and discon is given in figure 1. 
1 see lebowits  1  for definitions of predictive and predictable features. 

iv. other dimensions for characterizing conceptual clustering algorithms 
     we have thus far characterized conceptual clustering algorithms in terms of the structuring of the clusterings they produce  and in terms of the ways in which each technique deals with the problems of aggregation and characterization. we now define dimensions relating to search  along which we may describe the subprocesses of conceptual clustering. we begin by discussing dimensions 

     the reasons for this seemingly roundabout means of aggregating and describing object classes are best explicated in michalski  1 . by first formulating maximallygeneral descriptions  any clustering implied by any combination of maximally-general descriptions  one description for each seed  can be shown to contain at least one cluster which covers an arbitrary object. thus by first formulating maximally-general descriptions  cluster/1 guarentees that every observed object can be classified. once all objects are classified  derivation of maximally-specific descriptions serve to reduce the possibility of overlapping clusters with respect to unobserved objects. a 'fix-up' operation is then employed to make all possible clusterings mutually-disjoint. 
of characterization  ie. learning from examples . 
a. searching the space of characterizations 
     as we have seen  the characterization component of the conceptual clustering task is identical to the wellstudied task of learning from examples. thus  we can employ previous results from the machine learning literature in our analysis of this component. for instance  mitchell 
1
  set michalski  1  for definition! of discriminant and characteristic description!.  1   dietterich and michalski  1   and langley and carbonell  1  have proposed various dimensions along which methods for learning from examples may vary. mitchell points out that the space of concept descriptions is ordered according to generality. this ordering leads to three alternative schemes for systematically searching the 
space of hypotheses. first  one may start with a very specific hypothesis  and move toward more general descriptions in search of one that covers the instances; this approach may be called learning by generalization. second  one may start with a very general hypothesis  and move toward more specific descriptions that cover the data; this may be called learning by discrimination. finally  one may search in both directions  hoping to converge on the correct hypothesis; this is mitchell's version space strategy. 
　　　applying this analysis to the characterization components of the existing conceptual clustering systems  we find that unimem/ipp and glauber use generalization in characterizing their groupings. recall that cluster/1 forms characterizations at two points in its processing: the derivation of maximally-general discriminant concepts uses a discrimination approach; the derivation of maximally-specific characteristic concepts uses a generalization approach. rummage and discon use attribute values to form a number of possible partitions  where each attribute value may be viewed as a maximallygeneral discriminant concept of the object group it implies. no discrimination or generalization is employed in this process. rummage does however  use generalization to derive characterizations of object groups over those attributes not used in partitioning the object groups. wolff's mk1 does not form characterizations per  e  though it does generate conjunctive descriptions based on co-occurrences. 
　　　a second dimension involves the method used to direct search through the space of hypotheses. some ai systems that learned from examples have used depth-first search to select hypotheses  others have used breadthfirst search  while still others have non-exhaustive methods such as beam-search and best-first search. the nonexhaustive methods require some evaluation function to order hypotheses  so the same search technique may give different results depending on the evaluation function it employs. because of the limited concept languages employed by each of the conceptual clustering systems discussed  there is exactly one maximally-specific concept description for any given object group  which is to say there is no  or only a degenerate  search occuring in most cases. michalski and stepp's cluster/1 carried out a beam search in deriving maximally-general discriminant concepts  using evaluation functions supplied by the user  such as simplicity of class description . the formation of maximally-specific characteristic descriptions in cluster/1  as with all of the other systems  is deterministic. 
     third  one may distinguish between data-driven and mode-driven learning systems. in data-driven systems  the operators for moving through the space of hypotheses require data as input; thus  these data direct the search 
d. fisher and p. langley 1 
through the problem space. in model-driven systems  some other knowledge is used to generate new hypotheses  and the data are used only in the evaluation stage. cluster/1  unimem  glauber  and mk1 employ data-driven characterization methods  while the remaining systems can be viewed as model-driven systems  to the extent that they form characterizations . however  the  models  used by discon and rummage consisted only of a list of attributes that might be used in constructing a classification scheme. 
     a final dimension concerns whether ail observations are processed together  or whether they are handled one at a time. the first situation may be called non-incremental learning  and is plausible for modeling scientific data analysis. the vast majority of conceptual clustering systems 
 cluster/1  discon  rummage  glauber  and 
mk1  are all non-incremental learning systems. the second situation may be called incremental learning  and is more plausible for modeling concept formation based on continuous interaction with one's environment. of the existing conceptual clustering systems  only unimem and ipp can be viewed as incremental learners. this dimension is associated with the entire conceptual clustering system  not only with the characterization component. 
b. searching the space of aggregations 
　　　as we have seen  conceptual clustering methods solve the aggregation problem as well as the characterization problem  suggesting another set of dimensions along which such methods may differ. in this case  two dimensions present themselves: 
  search control. one can imagine a conceptual clustering system systematically considering all possible groupings  evaluating them  and then selecting the best. however  none of the systems we have considered employ such an inefficient approach. upon inspection  we find that cluster/1 uses a hillclimbing method to home in on an acceptable aggregation  using characterization techniques to evaluate its choices. in contrast  the remaining systems carry out only degenerate searches  of depth one  through the aggregation space  since they select their groupings in a one-step process. 
  nature of the operators. in order to understand why rummage  discon  and most other systems require only one-step searches  we must examine the operators they use to generate candidate groupings. rummage and discon both require a user-specified list of attributes and their values; by selecting an attribute  these systems automatically generate a candidate grouping  one for each value of the attribute   which can then be evaluated. 

glauber  mk1  and unimem/epp all accomplish the same effect in a more data-driven manner. only in cluster/1 do we find a less constrained operator  which selects seed objects that may or may not lead to a useful characterization. 
c. searching the space of hierarchies 
　　　we have seen that unlike systems that learn from examples  conceptual clustering methods must also determine their own aggregations. however  there remains another issue that distinguishes conceptual clustering from the task of learning from examples. in the latter  one is generally concerned with forming concepts at a single level  while conceptual clustering usually focuses on generating hierarchies of concepts. some numerical taxonomy methods  the optimization techniques  generate only single level groupings  but most methods arrive at some tree of groupings. 
     the implication for our analysis of conceptual clustering methods is clear - the search for aggregations and the search for characterizations are embedded within a higher level search through the space of classification trees. moreover  we can classify the existing clustering systems in terms of two additional dimensions. these are: 
  direction of the search. upon examining the existing conceptual clustering systems  we find that divisive  top-down  methods have been used by the majority  including cluster/1  discon  and rummage. these systems start with a single class of observations  and proceed by subdividing the instances into classes  these classes into subclasses  and so forth. however  one can also imagine methods that begin with separate  classes* for each observation  joining these classes together to form larger classes  and joining these classes in turn. such bottom-up  agglomerative  methods have been used by a minority of conceptual clustering systems  including glauber and mk1. other arrangements are also possible; for example  mervis and rosch  1  have suggested an approach where one first forms classes of medium generality  and later forms both more general and more specific classes. unimem/ipp behaves in roughly this manner and at any point in its processing classes of greater or lesser generality than existent classes may be added to the classification. 
  search control. conceptual clustering systems must somehow direct their search through the space of hierarchies. upon examining the existing systems  we find that cluster/1  rummage  glauber  
and mk1 carry out only degenerate searches through this space. the reason is that their operators consist of techniques for finding optimal aggregations and characterizations. search is involved at these lower levels  but the result is an optimal extension to the hierarchical tree. in contrast  discon has degenerate search schemes at these lower levels  but carries out a best-first search through the space of hierarchies. it accomplishes this through an exhaustive look-ahead process  evaluating entire sub-trees and preferring those containing fewer nodes. unimem and ipp also carried out search at this level  entertaining multiple organizations  thus using a form of beam search ; however  these organizations might be revised later in the search  so backup was allowed. 
although these dimensions are similar to those presented for the characterization problem  it is important to note that the current dimensions are separate from those for characterization. for instance  cluster/1 employed beam search to find maximally-general discriminant descriptions  but employed only a degenerate search for determining the best hierarchy. 
v. concluding remarks 
　　　we have discussed the mechanics of a number of conceptual clustering methods and defined dimensions which serve to clarify the differences and similarities between methods. our bias has been that further work in conceptual clustering is best facilitated by first understanding these processes in terms of well-understood concepts. following michalski  1   we have presented conceptual clustering as an extension of numerical taxonomy. further  by framing conceptual clustering as a composition of aggregation and characterization processes  we have shown a relationship between conceptual clustering and methods of learning from examples. this dichotomy has led to a view of conceptual clustering processes as conducting a three-tiered search: a search through a space of hierarchies; a search through a space of possible aggregations; and a search through a space of conceptual descriptions. 
　　　it is our view that explicating conceptual clustering as multi-layered search will not only ease comprehension of existing methods  but facilitate work in a number of still open problem areas.1 one problem concerns the task of clustering structured objects  where object descriptions allow relations to be represented between attribute values of an object. vere's thoth system  vere  1  is currently being investigated as a basis for a conceptual clustering system for structured objects. thoth discovers a minimal set of generalizations which cover a given set of relational production instances  where each production instance is a  before  state -  after  state pair. each state representation is equivalent to a structured object representation. thoth traces out a hierarchical classification 

bottom-up and in many ways resembles an agglomerative approach to conceptual clustering. a second area of interest to us concerns the problem of utilizing information on the functionality of objects to aid the formation of useful clusters. an approach suggested in discussion by nelson  1  involves using domain-specific knowledge of object functionality to guide the search for possible aggregates  and to use perceptual information as the basis of characterization. distinct forms of knowledge may serve to guide the search for hierarchies. by distinguishing levels of search we can more easily motivate and express the rules  heuristics  and descriptive languages  utilized at different levels. 
acknowledgements 
     we would like to thank dan easterlin  whose terminology we partially adopted in describing conceptual clustering processes  and dennis kibler who pointed us to vere's thoth system as a possible basis for future work. 
