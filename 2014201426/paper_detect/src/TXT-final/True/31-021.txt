 
we propose a process model for hierarchical perceptual sound organization  which recognizes perceptual sounds included in incoming sound signals. we consider perceptual sound organization as a scene analysis problem in the auditory domain. our model consists of multiple processing modules and a hypothesis network for quantitative integration of multiple sources of information. when input information for each processing module is available  the module rises to process it and asynchronously writes output information to the hypothesis network. on the hypothesis network  individual information is integrated and an optimal internal model of perceptual sounds is automatically constructed. based on the model  a music scene analysis system has been developed for acoustic signals of ensemble music  which recognizes rhythm  chords  and source-separated musical notes. experimental results show that our method has permitted autonomous  stable and effective information integration to construct the internal model of hierarchical perceptual sounds. 
1 	introduction 
over the past years  a number of approaches have been taken on machine vision: both theoretical and experimental efforts on feature extraction  shape restoration  stereo vision  knowledge-based vision and other techniques have been accumulated. on the other hand  research on machine audition  or computer systems to understand acoustic information  has been so far focused mainly on spoken language understanding. however  one of the requirements to an intelligent system is to possess the ability of recognition of various events in a given environment. specifically  understanding not only visual information or speech but also various acoustic information would play an essential role for an intelligent system which works in the real world. 
　on recognition or understanding of non-speech acoustic signals  several pioneering works can been found  currently at ntt basic research laboratories. 
1 	action and perception 
in the literature. for example  environmental sound recognition systems and auditory stream segregation systems have been developed  oppenheim and nawab  1; lesser et a/.  1; nakatani et a/.  1   as well as music transcription systems and music sound source separation systems  roads  1; mellinger  1; kashino and tanaka  1; brown and cooke  1 . here we consider two aspects: flexibility of processing and hierarchy of perceptual sounds. 
　first  we note that the flexibility of existing systems has been rather limited when compared with human auditory abilities. for example  automatic music transcription systems which can deal with given ensemble music played by multiple music instruments have not yet realized  although several studies have been conducted  mont-reynaud  1; chafe et a/.  1 . 
　regarding flexibility of auditory functions in humans  recent progress in physiological and psychological acoustics has offered significant information. especially  the property of information integration in the human auditory system has been highlighted  as demonstrated in the  auditory restoration'1 phenomena  handel  1 . to achieve flexibility  machine audition systems must have this property  since sound source separation  a sub problem of sound understanding  is an inverse problem in general formalization and cannot be properly solved without such information as memories of sound or models of the external world  as well as given sensory data. using the blackboard architecture  information integration for sound understanding has already been realized  oppenheim and nawab  1; lesser et a/.  1; cooke et a/.  1 . however  it is still necessary to consider a quantitative and theoretical background in information integration. 
　second  we should consider the basic problem of sound understanding   what is a single sound   noting the distinction between a perceptual sound and a physical sound. a perceptual sound in our terminology is a cluster of acoustic energy which humans hear as one sound  while a physical sound means an actual vibration of media. for example  when one listens to ensemble music of several instruments through one loudspeaker  there is a single physical sound source while we hear multiple perceptual sounds. as discussed in the following sections  an essential property of perceptual sound is its hierarchical structure. 

　with these points as background  we provide a novel process model of hierarchical perceptual sound organization with a quantitative information integration mechanism. our model is based on probability theory and characterized by its autonomous behavior and theoretically proved stability. 
1 problem description 
1 	perceptual sound organization 
an essential problem of perceptual sound organization is a clustering of acoustic energy to create such clusters that humans hear as one sound entity. here it is important to note that humans recognize various sounds in a hierarchical structure in order to properly grasp and understand the external world. that is  a perceptual sound is structured in both spatial and temporal hierarchy. for example  when one waits for a person to meet standing in a busy street  the waiting person sometimes hears a whole traffic noise as one entity  while sometimes hears a noise of one specific car as one entity. if he or she directs attention to the specific car's sound  an engine noise of the car or a frictional sound from the road surface and the tires of the car can be heard separately as one entity. 
　figure 1 shows an example of snapshot of perceptual sounds for music. note that there is not only spatial structure as shown in this figure but also temporal clusters of perceptual sounds  typically melodies or chord progression  though the temporal structure of perceptual sounds has not been depicted in figure 1 for simplicity of the figure. 

　the problem of perceptual sound organization can be decomposed into the following sub problems: 
1. extraction of frequency components with an acoustic energy representation. 
1. clustering of frequency components into perceptual sounds. 
1. recognition of relations between the clustered perceptual sounds and building a hierarchical and symbolic representation of acoustic entities. 
　note that we consider the problem as extraction of symbolic representation from flat energy data  while most approaches toward  auditory scene analysis  have so far considered their problem as restoration of target sound signals nakatani et a/.  1; brown and cooke  1 . in the computer vision field  the scene analysis problem has been considered as extration of symbolic representation from bitmap images and clearly distinguished from the image restoration problem which addresses recovery of target images from noise or intru-
1 	music scene analysis 
here we have chosen music as an example of applicable domain of perceptual sound organization. we use the term music scene analysis in the sense of perceptual sound organization in music. specifically  music scene analysis refers to recognition of frequency components  notes  chords and rhythm of performed music. 
　in the following sections  we first introduce general configuration of the music scene analysis system. we then focus our discussion on hierarchical integration of multiple sources of information  which is an essential problem in perceptual sound organization. then behavior of the system and results of the performance evaluation are provided  followed by discussions and conclusions. 
1 	system description 
figure 1 illustrates our process model optima  organized processing toward intelligent music scene analysis . input of the model is assumed to be monaural music signals. the model creates hypotheses of frequency components  musical notes  chords  and rhythm. as a consequence of probability propagation of hypotheses  the optimal  here we use the term  optimal  in the sense of  maximum likelihood   set of hypotheses is obtained and outputted as a score-like display  midi  musical instrument digital interface  data  or re-synthesized sourceseparated sound signals. 
　optima consists of three blocks:  a  preprocessing block   b  main processing block  and  c  knowledge sources. in the preprocessing block  first the frequency analysis is performed and a sound spectrogram is obtained. an example of sound spectrograms is shown in figure 1. 
　with this acoustic energy representation  frequency components are extracted. this process corresponds to the first sub problem discussed in the previous section. in the case of complicated spectrum patterns  it is difficult to recognize onset time and offset time solely by bottom-up information. thus the system creates several terminal point candidates for each extracted component  which are displayed in figure 1 as white circles. 
　with rosenthal's rhythm recognition method  rosenthal  1  and desain's quantization method  desain and honing  1   rhythm information is extracted for precise extraction of frequency components and recognition of onset/offset time. based on the integration of beat probabilities and termination probabilities of terminal point candidates  the candidates were fixed their status: continuous or terminated  and consequently processing scopes are formed. here a processing scope is a group of frequency components whose onset times are 
kashino etal 
: extracted frequency components  displayed as lines  with terminal point candidates  white circles . radius of each circle corresponds to the estimated probability close. the processing scope is utilized as a basic time clock for succeeding main processes of optima  as discussed later. examples of the formed processing scopes are shown in figure 1  bottom panel . 
　when each processing scope is created in the preprocessing block  it is passed to the main processing block  as shown in figure 1. the main block has a hypothesis network with three layers corresponding to levels of abstraction:  1  frequency components   1  musical notes and  1  chords. each layer encodes multiple hypotheses. that is  optima holds an internal model of the external acoustic entities as a probability distribution in the hierarchical hypothesis space. 
　multiple processing modules are arranged around the hypothesis network. the modules are categorized into three blocks:  a  bottom-up processing modules to transfer information from a lower level to a higher level   b  top-down processing modules to transfer information from a higher level to a lower level  and  c  temporal processing modules to transfer information along the time axis. the processing modules consult knowledge sources 
1 	action and perception 
of termination. ordinate: frequency  abscissa: time. 
middle : terminal point candidates for the component  1  in the top panel with timepower plane display  showing the difficulty of finding where a component terminates or starts only by bottom-up information. 
ordinate: power  abscissa: time. 
bottom : processing scopes with the label  scopeid :component-id   formed with rhythm information. vertical dotted lines show rhythm information extracted by the system. as an example  scope no.1 is highlighted. ordinate: frequency  abscissa: time. 
 source: the beginning of a two part chamber ensemble  auld lang syne   performed by a piano and a flute  
figure 1: examples of frequency components and processing scopes 


kashin1 etal 

in our implementation  the frequency analysis module and the frequency component prediction module have been installed on a parallel computer  fujitsu ap1  to achieve high processing speed  while the other part of the system was developed on workstations. this section discusses configuration of knowledge sources and the behavior of processing modules in the main processing block in figure 1. 
1 	knowledge sources 
six types of knowledge sources are utilized in optima. the chord transition dictionary holds statistical information of chord progression  under the n-gram assumption  typically we use n=1 ; that is  we currently assume that the length of markov chain of chords is three  for simplicity. since each s-level node has ingram hypotheses  one can note that the independence condition stated by equation  1  is satisfied even in slevel nodes. we have constructed this dictionary based on statistical analysis of 1 traditional songs  all western tonal music   which are popular in japan and other countries. 
　in the chord-note relation database  probabilities of notes which can be played under a given chord are stored. this information is also obtained by statistical analysis of the 1 chords. a part of the stored data is shown in table 1. 
　the chord naming rules  based on a music theory  are used to recognize chord when hypotheses of played notes are given. 

　the tone memory is a repository of frequency components data of a single note played by various musical instruments. currently it maintains notes played by five instruments  clarinet  flute  piano  trumpet  and violin  at different expressions  forte  medium  piano   fre-
1 	action and perception 
quency range  and durations. we recorded those sound samples at a professional music studio. 
　the timbre models are formed in the feature space of the timbre. we first selected 1 parameters for musical timbre  such as onset gradient of the frequency components and deviations of frequency modulations  and then reduced the number of parameters to eleven by the principal component analysis. this eleven-dimension feature space  where at least timbres of above mentioned five instruments are completely separated with each other  is used as a timbre model information. 
　finally  the perceptual rules describes the human auditory characteristics of sound separation bregman  1 . currently  the harmonicity rules and the onset timing rules are employed kashino and tanaka  1 . 
1 	bottom-up processing modules 
there are two bottom-up processing modules in optima: nhc  note hypothesis creator  and chc 
 chord hypothesis creator . nhc is a h-creator for the note layer  and performs the clustering for sound formation and the clustering for source identification to create note hypotheses. it uses the perceptual rules for the clustering for sound formation  and the timbre models for discrimination analysis of timbres to identify the sound source of each note. chc is a h-creator for the chord layer  which creates chord hypotheses when note hypotheses are given. it refers to chord naming rules in the knowledge sources. 
1 top-down processing modules fcp  frequency component predictor  and np  note predictor  are the top-down processing modules. fcp is a h-correlator between the note layer and the frequency component layer  and evaluates conditional probabilities between hypotheses of the two layers  consulting tone memories. np is a h-correlator between the chord layer and the note layer  to provide a matrix of conditional probabilities between those two layers. np uses the stored knowledge of chord-note relations. 
1 	temporal processing modules there are also temporal processing modules: ctp 
 chord transition predictor  and cgc  chord group creator . ctp is a h-correlator between the two adjacent chord layers  which estimates the transition probability of two n-grams  not the transition probability of two chords   using the chord transition knowledge source. cgc decides the m-link between the chord layers and the note layers. in each processing scope  cgc receives chord hypotheses and note hypotheses. based on rhythm information extracted in the preprocessing stage  it tries to find how many successive scopes correspond to one node in the chord layer  to create mlink instances. thus the m-link structure is formed dynamically as the processing progresses. 
1 evaluation 
we have performed a series of evaluation tests on the system: frequency component level tests  note level tests  

chord level tests  and tests using sample song performances. in this section  a part of the results will be presented. 
1 	note level benchmark tests 
an example of the experimental results for the n-level evaluation is displayed in figure 1  which shows the effect of information integration to the note recognition rates. in figure 1  tests have been performed in two ways: perceptual sound organization  1  without any information integration and  1  with information integration at the n-level. in the former case  the best note hypothesis produced by the bottom-up processing  nhc  is just viewed as the answer on the system  while in the latter case the tone memory information given by fcp is integrated. in both cases  we used two kinds of random note patterns: a two simultaneous note pattern and a three simultaneous note pattern. both patterns were composed by a computer and performed by a midi sampler using digitized acoustic signals  1bit  1khz  of natural musical instruments  clarinet  flute  piano  trumpet  and violin . the recognition rate was defined as 
		 1  
where right is the number of correctly identified and correctly source-separated notes  wrong is the number of spuriously recognized  surplus  notes and incorrectly identified notes  and total is the number of notes in the input. since it is sometimes difficult to distinguish surplus notes from incorrectly identified notes  both are included together in wrong. scale factor 1 is for normalizing r: when the number of output notes is the same as the number of input notes  r becomes 1  %  if all the notes are incorrectly identified and 1  %  if all the notes are correctly identified by this normalization. the results in figure 1 indicate that integration of tone memory information has significantly improved the note recognition rates of the system. 

figure 1: results of benchmark tests for note recognition 
1 	chord level benchmark tests 
another example of the experimental results shows the efficacy of s-level information integration for the chord recognition rates  figure 1 . in this test  we chose a sample song with chord transition of 1 chords. based on this chord transition pattern  test note groups were composed. to these 1 test note groups  noise  random addition or removal of the note  was added in four ways: 
 exp.l  one noise note in one chord among 1 chords   exp.1  two noise notes in one chord among 1 chords   exp.1  one noise note in each of 1 chords   exp.1  two noise notes in each of 1 chords. figure 1 displays significant improvement of chord recognition rates by our information integration scheme. 

error bar: 1% confidence interval 
figure 1: results of benchmark tests for chord recognition 
1 	evaluation using a sample music 
in addition to the benchmark tests by artificial test data  we have evaluated the system using music sound signals. figure 1 shows the note and chord recognition rates for a sample song: a three part chamber ensemble of  auld lang syne  performed by a sampler using acoustic signals of a flute  clarinet and piano. figure 1 clearly shows that information integration is effective not only in a test data but also in a music performance. 
1 	related work 
based on the physiological and psychological findings such as the ones bregman has summarized  bregman  1   brown and cooke developed a computational auditory scene analysis system  brown and cooke  1 . however  it was basically a bottom-up based system  and effective integration of information was not considered. from a viewpoint of information integration  lesser et al. proposed ipus  an acoustic signal understanding system based on the blackboard architecture lesser et al  1   and recently cooke et al. have also considered a blackboard-based auditory scene analysis system  cooke et a/.  1 . the blackboard architecture used in those systems requires global control knowledge and tends to 
kashino.etal 

result in a system with complex control rules. by contrast  our model only needs the local computations and consequently supports a simple control strategy with theoretically proved stability. recently nakatani et al. reported their studies based on a multi-agent scheme  nakatani et al  1 . our model can be viewed as a quantitative version of a multi-agent approach which uses probability theory. 
1 	conclusion 
we have proposed a method of hierarchical organization of perceptual sound  and described a configuration and behavior of the process model. based on the model  a music scene analysis system has been developed. specifically  our employment of a hypothesis network has permitted autonomous  stable and efficient integration of multiple sources of information. 
　the experimental results show that the integration of chord information and tone memory information significantly improves the recognition accuracy for perceptual sounds  in comparison with a conventional bottom-up based processing. here we have focused on the mechanism of information integration and left out detailed discussions on optimality of the output of each processing module. we are planning to clarify theoretical limits of the accuracy of each processing module  and to conduct further experiments to evaluate systematically the advantages and disadvantages of information integration mechanism of the proposed model. 
