 
most conceptual clustering systems rely solely on data to form concepts without supervision; the few that exploit causalities in the background knowledge do so only after the completion of a similarity-based learning phase. in this paper  we describe a multistrategy misconception discovery system  mmd  that utilizes data and theory in a more tightly coupled way. the integration of similarity- and causality-based learning in mmd is shown to be essential for the automatic construction of accurate and meaningful misconceptions that account for errors in novice behavior. 
1 	introduction 
the primary method for unsupervised concept formation in ai is conceptual clustering  which is the grouping of unlabeled objects into categories for which conceptual descriptions  i.e.  concepts  are formed. although conceptual clusterers differ in the way they address six key dimensions 1 they largely share the characteristic of relying solely on data to form concepts  a situation that likewise characterizes concept learning research in cognitive psychology  komatsu  1 . recent works  e.g.  
 barsalou  1; rips and collins  1; wisniewski and medin  1    however  reveal an increasing dissatisfaction over similarity-based models with their almost exclusive reliance on data and show an increasing interest in the role of theories and goals in concept formation. 
　while there may be a couple of ai systems that use data  similarity-based learning  sbl   and theory  explanation-based learning  ebl   to form concepts without supervision  these systems treat sbl and ebl as phases that are performed one after the other. the unsupervised learning systems of  lebowitz  1  and  pazzani  1   for example  first apply sbl on objects to form clusters which are then fed into an ebl or ebl-like component for further processing. the supervised learners of  flann and dietterich  1    mooney and ourston  1  and  yoo and fisher  1   on the other hand  operate in the reverse fashion: the output of the ebl phase is sent to an sbl component. however   wisniewski and medin  1  argue cogently that such loosely coupled approaches to using data and theory  while undoubtedly useful  remain inadequate as models of concept formation. 
　in this paper  we describe mmd  a multistrategy concept  or  more specifically  misconception  discovery system that utilizes data and theory in a more tightly coupled way than previous systems have. as a system that incrementally constructs and revises a hierarchy of possibly overlapping categories of relational descriptions  mmd is also unique in the manner in which it addresses the key dimensions of conceptual clustering earlier mentioned. 
misconception discovery as a special form of 
concept formation errors in novice behavior  such as bugs in a program written by a novice programmer  can be represented as logic formulas that describe specific relations  i.e.  discrepancies  between the incorrect behavior and an ideal  table 1 illustrates . such sets of discrepancies are analyzed in order to uncover the underlying misconceptions that cause them. knowledge of the causes of bugs will enable a tutor to both present a lesson and remediate a student more effectively. 
　in misconception discovery  therefore  the usual problem of concept formation is further complicated by the fact that conceptual descriptions of clusters of discrepancies can hardly be considered misconceptions - unless causal explanations for them are found. 
　in what follows  we first present a basic similaritybased algorithm for clustering relational descriptions and then describe how causal relationships in the background knowledge can be exploited to construct or correct descriptions of concepts/misconceptions while they are being formed. we then report experimental results showing that the approach to concept discovery embodied in 

table 1: discrepancies in behavior 	table 1: basic clustering algorithm 


mmd enables the automatic construction of meaningful misconceptions from theory and data. 
1 incremental clustering of relational descriptions 
1 	basic similarity measure 
basically an object o is classified into a category with concept description c with which it has more similarities than differences. to measure this degree of similarity/dissimilarity we use as our basis tversky's contrast model  tversky  1 : 

which expresses the similarity between two sets of features  in our case  discrepancies   c and o  as a function of the weighted measures of their common  and distinctive  c - 1 - c  features. 
　we compute the commonalities between two sets of relational descriptions c and o using: 

where lgg x  y  is the least general generalization  plotkin  1; muggleton and feng  1  of atomic formulas x and y in the function-free first-order logic  and m and n are the number of atoms in c and o  respectively. 1 basic r e l a t i o n a l clustering a l g o r i t h m 
the basic similarity-based clustering algorithm classifies a sequence of objects into a hierarchy which it inductively revises in the process. the algorithm is incremental  so it takes one object at a time and classifies this object recursively into the nodes that match it to a certain degree. each node in the hierarchy denotes a concept  which is either  a  a generalization  intersection or variableization  of the subconcepts below it  or  b  a record of an instance  or both. a counter is used to store the number of instances associated with a node. table 1 describes the basic algorithm. 
1. from the children n1 ...  nm of a given node n of the concept hierarchy  determine those that match the set of input discrepancies  o. the match function computes  for every child node ni 
  the set of commonalities  com ni 1   between a node  tv   and o  and 
  the degree of similarity  sim ni 1   between ni and o 
and determines whether sim exceeds a system threshold  
1. if no match is found  place o under n. otherwise  for every ni that matches o  perform one of the following depending on the value of com: 
  increase the weight counter of ni ; 
  replace ni with o and insert  ni - o  under o; 
  cluster  o - ni  under ni  i.e.  repeat the procedure  this time matching  o - ni  with the children of ni ; 
  create a new node  com  under tv  representing the commonalities of o and ni  and place their differences under this node. 
1. nodes whose  weight   height  values fall below a system parameter may be discarded on a regular or demand basis. 
　the above algorithm is similar to unimem  lebowitz  1  and cobweb  fisher  1   which are both incremental conceptual clusterers.1 unimem's similarity measure  however  considers only the differences between two sets of features. furthermore  unimem retrieves only a set of  potentially relevant  nodes to compare against the new object  rather than examining every child of a given node   and maintains a total of 1 different parameters. 
　cobweb  on the other hand  uses a probabilistic concept representation  rather than set theoretic  and a corresponding probabilistic similarity measure  category utility  gluck and corter  1; corter and gluck  1    and can only produce disjoint clusters  but see the probabilistic clusterer in  martin and billman  1  . in terms of explaining errors in novice behavior  disjoint clusters mean that a set of discrepancies can only be classified under one  misconception   though it may well be symptomatic of several. 
　both unimem and cobweb deal only with attributional descriptions  though one of cobweb's variants  labyrinth  thompson and langley  1   extends cobweb to handle structured objects. like its predecessor  
1
　　the above algorithm and the com c 1  function are presented in greater detail in  sison and shimura  1a   where the algorithm is called rc. said report also provides a more in-depth discussion of the similarities and differences among unimem  cobweb  and rc. 
	sison  numao  & shimura 	1 

however  labyrinth can only produce disjoint clusters. we also mention here cluster/s  stepp and michalski  1   which is an early algorithm that handles structured descriptions by first transforming these into attribute-value form  then feeding these into an attributional clusterer  cluster/1  michalski and stepp  1   that is nonincremental. 
1 using causal relations to strengthen coherence of concept descriptions 
1 	causality in background knowledge 
similarity-based clusterers like the ones described or mentioned above form categories on the basis of regularities  e.g.  co-occurrence  frequency  among features in the data  but ignore qualitative relationships among these same features. we argue that the presence of a qualitative  particularly causal relationship between features of a concept serves at least two purposes: 
  first  causal relationships strengthen the coherence of a conceptual description  and can warrant the splitting of a concept or an object when some regularities are coincidental. 
  second  and more important for the problem of mis-conception discovery  causal relationships explain the regularities in the data. by examining causal relationships it is possible to gain a better understanding of the causes of raw discrepancies. 
1 	causality heuristics 
causal relationships between features can be induced or deduced in a variety of ways. lebowitz   for example  suggests first using the frequency of occurrence of a feature in other concepts as a heuristic indicator of whether the feature is a cause or an effect  and then forward-chaining from the causative features to the other features using heuristic  low-level  causal domain rules. in  pazzani  1   there are only two  kinds  of features  namely  actions and state changes  and actions are always the causative features. determining which state changes are caused by which actions is achieved by instantiating general causal patterns. 
　in our case  we use causal relationships among components of the ideal behavior  together with the following heuristics: 
  component-level causality: causal  or enabling or determination  relationships among the components of the ideal behavior that are present in a set of discrepancies suggest causal relationships among these discrepancies. 
  concept-level causality: a causal relationship between two discrepancies in a generalization node  where one is an intersection generalization and the other a variableization  suggests that the former causes the latter. 
  subconcept-level causality: causal relationships between a parent node and its child suggests that the latter causes the former. 
cognitive modeling 

figure 1: causal relationships 
the second and third causality heuristics assume the existence of component-level causal relationships  and are used to determine the direction of casuality. 
　to illustrate the first heuristic  recall the ideal program for reverse/1 in table 1  reproduced in figure 1 . the ideal program states that the reverse of a list is the concatenation of the reverse of its tail and its head. note that this can be viewed as describing relationships among four objects  namely  the head h and the tail t of the list to be reversed  the reversed list r  and a temporary entity 1 representing the reverse of t; and the relations reverse/1 and append/1. these relationships are illustrated graphically in figure 1. 
　the component-level causality heuristic suggests that if two discrepancies dl and  1 involve two features f1 and /1  respectively  both of which involve a common object c  i.e.  c in f1 causes  enables or determines c in f1  or vice versa   then dl and d1 are causally related. thus  since both discrepancies in figure 1 involve the object r  r in the relation in the second discrepancy causes  enables  the r in the first   then the two discrepancies are  according to this heuristic  causally related; that is  the student's use of the construct   i   is related to the conspicuous absence of the append/1 subgoal in his program. the drawing in figure 1 illustrates. 
the concept-level causality heuristic suggests that if 


figure 1: direction of causality between discrepancies in a node 
two discrepancies d1 and dl are causally related and belong to the same concept/node  and if dl is an intersection generalization and d1  a variableization generalization  then dl causes  d1. this is because the intersection would be the discrepancy that is present together with all the various possible instantiations of the variableization. thus  for example  students who are not confident in introducing variables in the body of a clause would omit the variable tl in the recursive subgoal of reverse/1  and  as a result  use possibly different variables from the head  e.g.  r or h  in place of tl in the append/1 subgoal. figure 1 illustrates. 
　finally  the sub concept-level causality heuristic suggests that if two discrepancies dl and dl are causally related  and dl is a parent of d1  then dl is a probable cause of dl  i.e.  based on empirical data  the child implies the parent. this is because a child is more likely to be encountered or seen with its parent  than the parent with a particular child. for instance  it is possible that the student in the example in table 1  and in figure 1  put  tl |h  in the head because he/she purposely omitted the append/1 subgoal in the body of his/her clause. this suggests that the student knew about the append/1 relation but decided that using  |  was better  at least in this case. however  it is more likely that the student omitted the append/1 subgoal as a result of putting  t1ih  in the head  figure 1 . this means that the student thought  incorrectly  that the   i   construct could be used to prepend a list to an object  and having dealt with the necessary concatenation  had no further need for a concatenation subgoal in the body of the clause. 
1 a similarity- and causality-based clustering algorithm 
existing approaches  e.g.   lebowitz  1; pazzani  1   to using data and theory  causality  in concept formation use separate sbl and ebl components one 

figure 1: direction of causality between discrepancies along a link 
table 1: incorporating causality into concept formation 
1. same as in table 1  with the addition that causality relationships among discrepancies are to be determined using the component-level heuristic. 
1. same as in table 1. 
1. for every new node created in  1   
 a  if concept-level causality exists among discrepancies in this node  record the direction of causality. if no such causality exists  retain the node nevertheless.  if the node needs to be split  it will be split by step  1  on another occasion.  
 b  if subconcept-level causality exists between this node and its parent  record the direction of causality. if no such causality exists  sever the link between this child and its parent  linking it instead with its grandparent  then recheck for subconceptlevel causality  i.e.  step 1b . 
1. same as step  1  in table 1. 
after the other. in mmd  sbl and ebl are tightly coupled in the concept formation process. this entails two revisions to the basic algorithm presented in the previous section  rather than a separate algorithm altogether . 
  causal relationships are to be determined using the component-level causality heuristic. 
  the directions of causalities are to be deter-mined whenever possible using the concept and subconcept-level heuristics. this may lead to the severing of ties between a parent node and its child when the two are in fact unrelated. 
these revisions are found in table 1. note that step  1b  effectively functions as a reorganization operator that is causality-based. step  1  likewise reorganizes  prunes  the hierarchy  though it does this based on frequencies. reorganization operators are especially important for incremental learners to mitigate ordering effects. 
1 	evaluation 
for a preliminary empirical evaluation of the ability of 
mmd to discover actual misconceptions in real-world 
	sison  numao & shimura 	1 

behavior  a sufficiently large corpus of incorrect novice behavior  here in the form of buggy prolog programs  had to be compiled. a total of 1 buggy reverse/1 and 1 buggy sumlist/1 programs  for the naive reversal of lists and for summing up the elements of a list of numbers  respectively  were obtained from third-year undergraduate students who have learned basic prolog concepts  and then submitted for expert  teacher  analysis of the underlying misconceptions. the discrepancies between the buggy programs and their associated ideal programs were also computed and then fed into mmd. a point-by-point comparison of the misconception hierarchies generated by the expert and by mmd is presented elsewhere  see  sison  numao and shimura  1  ; here we compare instead the accuracies of the misconception/classification hierarchies generated by: 
a  the basic unimem-like similarity-based algorithm 
in table 1  called smd in figure 1 ; and 
b  mmd 
given worst-case orderings of the objects in the reverse/1 and sumlist/1 datasets. a misconception or classification generated by mmd or smd is considered accurate if it matches that of the expert. 
　the results obtained are very encouraging  figure 1 . mmd was able to correctly classify most of the bugs in the student programs. the lower accuracy of the hierarchies generated by smd were mainly due to incoherent groupings and multiple bugs  which smd is insensitive to. the bugs which mmd  and of course smd  was not able to classify correctly were primarily due to discrepancies which could be transformed to other   more meaningful  discrepancies. for mmd to classify these bugs correctly  two options are possible. one option would be to give mmd the ability to recognize discrepancies between discrepancies  i.e.  to transform one discrepancy to another . alternatively  this task could be given to the preprocessor which computes discrepancies between buggy programs and an ideal. the second option is preferable since mmd's primary task is clustering discrepancies rather than transforming them. 
1 	concluding remarks 
a similarity-based approach to misconception discovery is important because it reveals regularities in the data  which in turn may indicate the existence of underlying causalities. moreover  in the absence of feature relar tionships deducible from the background knowledge  an sbl-generated node with high confidence can be learned as a new  though yet unexplainable  misconception. on the other hand  an explanation causality -based approach is necessary because concepts based solely on regularities might not be coherent and because some features in concept descriptions can be irrelevant. furthermore  a similarity-based learner can only roughly classify an erroneous program but not specify the cause s  of its errors. 

figure 1: accuracy of hierachies generated by smd and mmd given worst-case orderings of objects in the reverse/1 and sumlist/1 datasets 
　the tight integration of similarity- and causality-based learning in the multistrategy unsupervised concept discovery system mmd has been shown to be useful  if not essential  for the the automatic construction of meaningful misconceptions that can be used to account for discrepant behavior in student programs. the applicability of mmd's approach should extend naturally to similar domains  i.e.  domains in which causal relationships exist among components of behavior in the background knowledge. future work will involve investigating mechanisms for generalizing misconceptions across problems  for using domain semantics to articulate causal relationships  and for exploiting and dynamically choosing other qualitative relationships  e.g.  goals  that might exist in the background knowledge. mmd is a step toward the automatic discovery of  prolog programming  misconceptions  sison  1  and their use in multistrategic student modeling  sison and shimura  1b . 
acknowledgment 
the first author thanks ethel chua joy and philip chan for their assistance in compiling and analyzing the programs in the reverse/1 and sumlist/1 datasets. 
