
we look at the problem in belief revision of trying to make inferences about what an agent believed - or will believe - at a given moment  based on an observation of how the agent has responded to some sequence of previous belief revision inputs over time. we adopt a  reverse engineering  approach to this problem. assuming a framework for iterated belief revision which is based on sequences  we construct a model of the agent that  best explains  the observation. further considerations on this best-explaining model then allow inferences about the agent's epistemic behaviour to be made. we also provide an algorithm which computes this best explanation.
1 introduction
the problem of belief revision  i.e.  of how an agent should modify its beliefs about the world given some new information which possibly contradicts its current beliefs  is by now a well-established research area in ai  gardenfors  1“  . traditionally  the work in this area is done from the agent's perspective  being usually pre-occupied with constructing actual revision operators which the agent might use and with rationality postulates which constrain how these operators should behave. in this paper we change viewpoint and instead cast ourselves in the role of an observer of the agent. imagine the following scenario. suppose we are given some sequence  耳1 ... 耳n  of revision inputs which a particular agent  hereafter a  has received over a certain length of time and suppose we are also given a sequence  牟1 ... 牟n  with the interpretation that following the ith input 耳i  a believed  at least  牟i. throughout the paper  we make the assumptions that a received no input between 耳1 and 耳n other than those listed  and that the 牟i are correct  but possibly partial  descriptions of a's beliefs after each input. a couple of questions now suggest themselves:
  what will a believe following a further revision input 耳n+1 
  what did a believe immediately before it received the first input 耳1  what  apart from 牟i did a believe after the ith input 耳i 
one area in which such questions might arise is in humanmachine dialogues  del cerro et al.  1   where the 耳i correspond to inputs given to a - the user - by a machine  and the 牟i are the user's responses. here  it can be useful for the machine to keep a model of the evolution of a user's beliefs during a dialogue. another setting where reconstruction of a prior belief set is needed is in law cases  where guilt and innocence often depend on who knew what  when. inquiry is a possible method of approaching this type of problem. the above mentioned sequences can be seen as the information the inquiry yields and which is now used for drawing conclusions. our aim in this paper is to answer these questions.
﹛our strategy for dealing with these questions will be to adopt a  reverse engineering  approach - constructing a model of the agent.  similar approaches have already been tried in the context of trying to infer an agent's goals from observable actions  e.g.   brafman and tennenholtz  1; lang  1 .  having no access to the agent's internals  we assume a belief revision framework a uses for determining its beliefs and for incorporating new information  and construct a model of a that explains the observation about it. by considering this model  we will then be able to make extra inferences or predictions about a's epistemic behaviour. of course this raises the problem of which belief revision framework to choose. such a framework will obviously need to support iterated revision  darwiche and pearl  1; lehmann  1b; nayak et al.  1   and preferably also non-prioritised revision  hansson et al.  1; makinson  1   i.e.  revision in which new inputs are allowed to be rejected. in this paper  we restrict the investigation to one such framework that has been studied in  booth  1 . the idea behind it is that an agent's epistemic state is made up of two components:  i  a sequence 老 of sentences representing the sequence of revision inputs the agent has received thus far  and  ii  a single sentence n standing for the agent's set of core beliefs  which intuitively are those beliefs of the agent it considers  untouchable . the agent's full set of beliefs in the state  老 n  is then determined by a particular calculation on 老 and n  while new revision inputs are incorporated by simply appending them to the end of 老. note that our choice of this framework does not imply that others are less worthy of investigation. the challenge now becomes to find that particular model of this form which best explains the observation o = h 耳1 ... 耳n   牟1 ... 牟n i we have made of a.
﹛the plan of the paper is as follows. in sect. 1 we describe in more detail the model of epistemic state we will be assuming. this will enable us to pose more precisely the problem we want to solve. we will see that the problem essentially reduces to trying to guess what a's initial epistemic state  老 n   i.e.  before it received 耳1  was. in sect. 1  inspired by work done on reasoning with conditional beliefs  we propose a way of finding the best initial sequence - or prefix - 老 n  for any given fixed n. then  in sect. 1 we focus on finding the best n. this will amount to equating best with logically weakest. the epistemic state  老 n  n  obtained by combining our answers will be our proposed best explanation for o  which we will call the rational explanation. in sect. 1 we present an algorithm which constructs the rational explanation for any given o  before giving some examples to show the type of inferences this explanation leads to in sect. 1. in sect. 1 we briefly mention a related piece of work by dupin de saintcyr and lang  before concluding and giving some pointers for future research.
1 modelling the agent
we assume sentences 耳i 牟i n  etc. are elements of some finitely-generated propositional language l. in our examples  p q r denote distinct propositional variables. the classical logical entailment relation between sentences is denoted by `  while √ denotes classical logical equivalence. wherever we use a sentence to describe a belief set the intention is that it represents all its logical consequences. the set of all possible observations o = h牝 而i which can be made of a  where 牝 =  耳1 ... 耳n  and 而 =  牟1 ... 牟n  are two finite sequences of sentences of the same length  is denoted by o. the operation ﹞ on sequences denotes sequence concatenation.
﹛as indicated in the introduction  we follow  booth  1  by assuming that  at any given moment in time  an agent's epistemic state is represented by a pair  老 n .   konieczny and perez  1; lehmann  1b∩   also use sequences to represent epistemic states  but without core beliefs . in order to fully specify the agent's epistemic processes  we also need to formally specify  i  how the agent determines its set of beliefs bel  老 n   in any given state  老 n   and  ii  how it incorporates new revision inputs into its epistemic state. turning first to  i   we can describe bel  老 n   neatly with the help of a function f  which takes as argument a non-empty sequence 考 =  汐m ... 汐1  of sentences  and returns a sentence. f is defined by induction on the length m of 考: if m = 1 then f 考  = 汐1. if m   1 then

in other words f 考  is determined by first taking 汐1 and then going backwards through 考  adding each sentence as we go  provided that sentence is consistent with what has been collected so far  cf. the  linear base-revision operation  of  nebel  1  and the  basic memory operator  of  konieczny and perez  1∩  .  the belief set associated to the state  老 n  is then given by bel  老 n   = f 老 ﹞ n . hence when calculating its beliefs from the sentences appearing in its epistemic state  an agent gives highest priority to n. after that  it prioritises more recent information received. note that n is always believed  and that bel  老 n   is inconsistent if and only if n is inconsistent.
example 1. consider n =  p and 老 =  q q ↙ p . bel  老 n   = f q q ↙ p  p . in order to determine f q q ↙ p  p  we need to know if q is consistent with f q ↙ p  p . as f  p  =  p and q ↙ p is consistent with  p  f q ↙ p  p  =  q ↙ p  ＿  p √  q ＿  p. so q is inconsistent with f q ↙ p  p . consequently we get f q q ↙ p  p  = f q ↙ p  p  and bel  老 n   = f q ↙ p  p  √  q ＿  p.
﹛an agent incorporates a new revision input 竹 into its epistemic state  老 n  by simply appending 竹 to 老  i.e.  the agent's revision function   is specified by setting  for every 竹 ﹋ l   老 n    竹 =  老 ﹞ 竹 n .
given this  we see that a new input 竹 will not always be believed in the new state. indeed  when n is consistent  it will be so only if it is consistent with n. if it contradicts n then it will not be accepted  and in fact in this case the agent's belief set will remain unchanged  c.f. screened revision  makinson  1  . note also that n remains unaffected by a revision input  i.e.    is a core-invariant revision operator  booth  1 .1 core beliefs are needed to ensure that revision inputs can be rejected. if they were not allowed  which corresponds to demanding n =   in the above definitions  any consistent revision input would belong to the agent's beliefs.
﹛as is shown in  booth  1   the above revision method satisfies several natural properties. in particular  it stays largely faithful to the agm postulates  gardenfors  1“    leaving aside the  success  postulate  which forces all new inputs to be accepted   and satisfies slight  non-prioritised  variants of several postulates for iterated revision which have been proposed  including those of  darwiche and pearl  1 . one characteristic property of this method is the following variant of the rule  recalcitrance  from  nayak et al.  1 :
if n 1`  竹1 ↙  竹1  then bel  老 n    竹1   竹1  ` 竹1
this entails if the agent accepts an input 竹1  then it does so wholeheartedly  in that the only way it can be dislodged from the belief set by a succeeding input 竹1 is if that input contradicts it given the core beliefs n.
﹛returning to our agent a from the introduction  from now on we assume a's epistemic state is always of the form  老 n   and that a determines its belief set and incorporates new inputs into its epistemic state as described above. then  suppose we make the observation o = h 耳1 ... 耳n   牟1 ... 牟n i about a. then after receiving the ith input 耳i  a's epistemic state must be  老 ﹞  耳1 ... 耳i  n  and its belief set f 老 ﹞  耳1 ... 耳i  ﹞ n   where  老 n  is a's unknown initial  i.e.  before 耳1  epistemic state. observation o now amounts to the following:
	f 老 ﹞  耳1 ... 耳i  ﹞ n  ` 牟i	i = 1 ... n	 1 
we make the following definitions:
definition 1. let o = h 耳1 ... 耳n   牟1 ... 牟n i ﹋ o. then  老 n  explains o  or is an explanation for o  iff  1  above holds. we say n is an o-acceptable core iff  老 n  explains o for some 老.
example 1.  i   老 n  =   p ↙ q  r  explains h p q   q r i because f p ↙ q p r  √ p ＿ q ＿ r ` q and f p ↙ q p q r  √ p ＿ q ＿ r ` r.
 ii    p ↙ q     does not explain h p q   q r i because f p ↙ q p q    √ p ＿ q 1` r.
﹛if we had some explanation  老 n  for o then we would be able to answer the questions in the introduction: following a new input 耳n+1 a will believe f 老 ﹞  耳1 ... 耳n 耳n+1  ﹞ n   before receiving the first input a believes f 老 ﹞ n   and the beliefs after the ith input are f 老 ﹞  耳1 ... 耳i  ﹞ n .
﹛note for any o ﹋ o there always exists some explanation  老 n  for o  since the contradiction ﹠ is an o-acceptable core using any 老. but this would be a most unsatisfactory explanation  since it means we just infer a believes everything at every step.
﹛our job now is to choose  from the space of possible explanations for o  the best one. as a guideline  we consider an explanation good if it only makes necessary  or minimal  assumptions about what a believes. but how do we find this best one  our strategy is to split the problem into two parts  handling 老 and n separately. first   i  given a fixed o-acceptable core n  find a best sequence 老 o n  such that  老 n  explains o  then   ii  find a best o-acceptable core n o . our best explanation for o will then be  老 o n o   n o  .
1 finding 老
given o = h 耳1 ... 耳n   牟1 ... 牟n i  let us assume a fixed core n. to find that sequence 老 o n  such that  老 o n  n  is the best explanation for o  given n  we will take inspiration from work done in the area of non-monotonic reasoning on reasoning with conditional information.
﹛let's say a pair  竹 聿  of sentences is a conditional belief in the state  老 n  iff 聿 would be believed after revising  老 n  by 竹  i.e.  bel  老 n    竹  ` 聿. in this case we will write 竹   老 n  聿.1 this relation plays an important role  because it turns out a's beliefs following any sequence of revision inputs starting from  老 n  is determined entirely by the set   老 n  of conditional beliefs in  老 n . this is because  for any sequence of revision inputs 耳1 ... 耳m  our revision method satisfies
bel  老 n  耳1 ﹞﹞﹞ 耳m  = bel  老 n  f 耳1 ... 耳m n  .
thus  as far as their effects on the belief set go  a sequence of revision inputs starting from  老 n  can always be reduced to a single input.  but note the set of conditional beliefs   老 n  竹 in the state  老 n    竹 following revision by 竹 will generally not be the same as   老 n . 
﹛all this means observation o may be translated into a partial description of the set of conditional beliefs that a has in its initial epistemic state:
cn o  = {f 耳1 ... 耳i n    牟i | i = 1 ... n}.
clearly  if we had access to the complete set of a's conditional beliefs in its initial state  this would give another way to answer the questions of the introduction. now  the problem of determining which conditional beliefs follow from a given set c of such beliefs has been well-studied and several solutions have been proposed  e.g.   geffner and pearl  1; lehmann  1a . one particularly elegant and well-motivated solution is to take the rational closure of c  lehmann and magidor  1 . furthermore  as is shown in  e.g.   freund  1   this construction is amenable to a relatively simple representation as a sequence of sentences! our idea is essentially to take 老 o n  to be this sequence corresponding to the rational closure of cn o . first let us describe the general construction.
1 the rational closure of a set of conditionals
given a set of conditionals c = {竹i   聿i | i = 1 ... l} we denote by c  the set of material counterparts of all the conditionals in c  i.e.  c  = {竹i ↙ 聿i | i = 1 ... l}. then a sentence 糸 is exceptional for c iff c `    糸  and a conditional 糸   米 is exceptional for c iff its antecedent 糸 is. to find the  sequence corresponding to the  rational closure 老r c  of c  we first define a decreasing sequence of sets of conditionals c1   c1   ﹞﹞﹞   cm by setting  i  c1 = c   ii  ci+1 equals the set of conditionals in ci which are exceptional for ci  and
 iii  m is minimal such that cm = cm+1. then we set 老r c  =   c m  c m 1 ...  c 1 .
writing 汐i for vc i  the rational closure of c is then the relation  r given by 竹  r 聿 iff either
where j is minimal such that. since 汐m a ﹞﹞﹞ a 汐1 it easy to check that in fact this second disjunct is equivalent to f 汐m ... 汐1 竹  ` 聿.
we now make the following definition:
definition 1. let o ﹋ o and n ﹋ l. we call 老r cn o   the rational prefix of o with respect to n  and will denote it by 老r o n .
example 1. let o = h p q   r  p i and n =  p. then cn o 	=	{f p  p    r f p q  p     p} =	{ p   r  q ＿  p     p}.
since neither of the individual conditionals are exceptional for cn o  we get c1 = cn o  and c1 =  . clearly then also c1 =   = c1 so we obtain 老r o n  =  v  vc n o  . rewriting the sequence using logically equivalent sentences we get 老r o n  =     p ↙ r .
﹛now  an interesting thing to note about the rational prefix construction is that it actually goes through independently of whether n is o-acceptable. in fact a useful sideeffect of the construction is that it actually reveals whether n is o-acceptable. given we have constructed 老r o n  =
 汐m ... 汐1   all we have to do is to look at sentence 汐m and check if it is a tautology:
proposition 1. let o ﹋ o and n ﹋ l  and let 老r o n  =
 汐m ... 汐1  be the rational prefix of o w.r.t. n. then
 i  if 汐m √   then  老r o n  n  is an explanation for o.  ii  if 汐m √1	  then n is not an o-acceptable core.
﹛thus this proposition gives us a necessary and sufficient condition for n to be an o-acceptable core. this will be used
in the algorithm of sect. 1.
in example 1 老r o n  =     p ↙ r  was cal-
culated. the above proposition implies      p ↙ r   p  is an explanation for o = h p  q    r    p i. this is verified by f    p ↙ r p  p  √  p ＿ r ` r and f    p ↙ r p q  p  √  p ＿ q ＿ r `  p.
1 justification for using the rational prefix
in the rest of this section we assume n to be some fixed oacceptable core. as we just saw   老r o n  n  then provides an explanation for o given this n. in this section we want to show in precisely what sense it could be regarded as a best explanation given n. let 曳 = {考 |  考 n  explains o}.
﹛one way to compare sequences in 曳 is by focusing on the trace of belief sets they  in combination with n  induce through o  i.e.  for each 考 ﹋ 曳 we can consider the sequence
  whereis defined to be the be-
liefs after the ith input in o  under the explanation  考 n  . in other words gives
the initial belief set. 
example 1. let o  n and 老r o n  be as in example 1. then the belief trace is   p ＿ r  p ＿ r  p ＿ q ＿ r .
﹛the idea would then be to define a preference relation 1 over the sequences in 曳  with more preferred sequences corresponding to those  lower  in the ordering  via some preference relation over their set of associated belief traces. given any two possible belief traces  汕1 ... 汕n  and  污1 ... 污n   let us write  汕1 ... 汕n  ≒lex  污1 ... 污n  iff  for all i =
1 ... n   for all j   i implies. then we define  for any 老 考 ﹋ 曳:  iff  .
 is a pre-order  i.e.  reflexive and transitive  on 曳.  thus  given two sequences in 曳  we prefer that one which leads to a having fewer  i.e.  weaker  beliefs before any of the inputs 耳i were received. if the two sequences lead to equivalent beliefs at this initial stage  then we prefer that which leads to a having fewer beliefs after 耳1 was received. if they lead to equivalent beliefs also after this stage  then we prefer that which leads to a having fewer beliefs after 耳1 was received  and so on. thus  under this ordering  we prefer sequences which induce a to have fewer beliefs  earlier in o. the next result shows 老r o n  is a best element in 曳 under this ordering.
proposition 1. for all 考 ﹋ 曳.
﹛another way to compare sequences is to look at their consequences for predicting what will happen at the next step after o.
	iff
for all 竹
thus  according to this preference criterion we prefer 老 to 考 if it always leads to fewer beliefs being predicted after the next revision input. it turns out 老r o n  is a most preferred element under 1 amongst all minimal elements under 1.
proposition 1. for all then
.
﹛thus if we take a lexicographic combination of 1 and 1  with 1 being considered as more important   老r o n  emerges overall as a best  most preferred  member of 曳. having provided a method for finding the best explanation  老 n  given n  we now turn our attention to finding the best n itself.
1 minimising n
as argued earlier  core beliefs are needed  but at the same time we try to minimise the assumptions about the agent's beliefs. this includes minimising n. the first idea would be to simply take the disjunction of all possible o-acceptable cores  i.e.  to take nˍ o   defined by
nˍ o  √  {n | n is an o-acceptable core}.
﹛but is nˍ o  itself o-acceptable  thankfully the answer is yes  a result which follows  in our finite setting  from the following proposition which says that the family of o-acceptable cores is closed under disjunctions.
proposition 1. if n1 and n1 are o-acceptable then so is n1 ˍ n1.
so as a corollary nˍ o  does indeed satisfy:
 acceptability  n o  is an o-acceptable core
what other properties does nˍ o  satisfy  clearly  nˍ o  will always be consistent provided at least one consistent oacceptable core exists:
 consistency  if n o  √ ﹠ then n1 √ ﹠ for every o-acceptable core n1.
acceptability and consistency would appear to be absolute rock-bottom properties which we would expect of any method for finding a good o-acceptable core. however for we can say more. given two observations o = h牝 而i and o = h牝  而 i  let us denote by o﹞o1 the concatenation of o and o1  i.e.  o ﹞ o1 = h牝 ﹞ 牝1 而 ﹞ 而1i. we shall use o vright o1 to denote that o1 right extends o  i.e.  o1 = o﹞o1 for some  possibly empty  o1 ﹋ o  and o vleft o1 to denote o1 left extends o  i.e.  o1 = o1 ﹞ o for some  possibly empty  o1 ﹋ o.
proposition 1. suppose o vright o1 or o vleft o1. then every o1-acceptable core is an o-acceptable core.
﹛as a result of this we see nˍ satisfies the following 1 properties  which say extending the observation into the future or past leads only to a logically stronger core being returned.
 right monotony 	if o vright o1 then n o1  ` n o   left monotony 	if o vleft o1 then n o1  ` n o .
right- and left monotony provide ways of expressing that n o  leads only to safe conclusions that something is a core belief of a - conclusions that cannot be  defeated  by additional information about a that might come along in the form of observations prior to  or after o.
﹛we should point out  though  that it is not the case that by inserting any observation anywhere in o  nˍ will always lead to a logically stronger core. consider o1 = h p q   p  p i and o1 = h p  p q   p  p  p i  i.e.  h  p    p i was inserted in the middle of o1. nˍ o1  √ q ↙  p whereas nˍ o1  √  . so although o1 extends o1 in a sense  the corresponding nˍ is actually weaker. looking at o1  assuming as we do that a received no inputs between p and q  the only way to explain the end belief in  p is to ascribe core belief q ↙  p to a  cf. the  recalcitrance  rule in sect. 1 . however  looking at o1  the information that a received  and accepted  intermediate input  p is enough to  explain away  this end belief without recourse to core beliefs. our assumption that a received no other inputs between 耳1 and 耳n during an observation o = h 耳1 ... 耳n   牟1 ... 牟n i is rather strong. it amounts to saying that  during o  we kept our eye on a the whole time. the above example shows that relaxing this assumption gives us an extra degree of freedom with which to explain o  via the inference of intermediate inputs. this will be a topic for future work.
﹛it turns out the above four properties are enough to actually characterise nˍ. in fact  given the first two  just one of rightand left monotony is sufficient for this task: proposition 1. let n : o ↙ l be any function which returns a sentence given any o ﹋ o. then the following are equivalent:
 i nsatisfies acceptability  consistency and right monotony.  ii  n satisfies acceptability  consistency and left monotony.  iii  n o  √ nˍ o  for all o ﹋ o.
﹛note that as a corollary to this proposition we get the surprising result that  in the presence of acceptability and consistency  right- and left monotony are in fact equivalent.
﹛combining the findings of the last two sections  we are now ready to announce our candidate for the best explanation for o. by analogy with  rational closure   we make the following definition:
definition 1. let o ﹋ o be an observation. then we call  老r o nˍ o   nˍ o   the rational explanation for o.
﹛in sect. 1 we will give some examples of what we can infer about a under the rational explanation. but how might we find it in practice  the next section gives an algorithm for just that.
1 constructing the rational explanation
the idea behind the algorithm is as follows. given an observation o  we start with the weakest possible core n1 =   and construct the rational prefix  汐m ... 汐1 =老1 of o w.r.t. n1. we then check whether 汐m is a tautology. if it is then we know by prop. 1 that  老1 n1  is an explanation for o and so we stop and return this as output. if it isn't then prop. 1 tells us n1 cannot be o-acceptable. in this case  we modify n1 by conjoining 汐m to it  i.e.  by setting n1 = n1 ＿ 汐m. constructing the rational prefix of o w.r.t. the new core then leads to a different prefix  which can be dealt with the same way.

algorithm 1 calculation of the rational explanation
input: observation o
output: the rational expl
n     repeat
老   老r o n 
﹛n   n ＿ 汐 until
return  老 n anation for o
{老 =  汐m ... 汐1 }﹛before showing that the output of this algorithm matches the rational explanation  we need to be sure it always terminates. this is a consequence of the following:
lemma 1. let n and 汐m be as after the calculation of 老r o n . if 汐m 1√   then n 1√ n ＿ 汐m.
﹛this result assures us that if the termination condition of the algorithm does not hold  the new core will be strictly logically stronger than the previous one. thus the cores generated by the algorithm become progressively strictly stronger.
in our setting  in which we assumed a finite propositional language  this means  in the worst case  the process will continue until n √ ﹠. however in this case it can be shown the rational prefix of o w.r.t. ﹠ is just      and so the termination condition will be satisfied at the very next step.
﹛now  to show the output matches the rational explanation  consider the sequence  老1 n1  ...  老k nk  of epistemic states generated by the algorithm. we need to show nk √ nˍ o . the direction nk ` nˍ o  follows from the fact that  老k nk  is an explanation for o and so nk is an oacceptable core. the converse nˍ o  ` nk is proved by showing inductively that nˍ o  ` ni for each i = 1 ... k: the case i = 1 clearly holds since n1 √  . the inductive step uses the following property:
lemma 1. let 1	 	i	≒	k and suppose 老i 1	=
 汐m ... 汐1 . then  for any o-acceptable core n1  if n1 ` ni 1 then n1 ` 汐m.
﹛this enables us to prove that  given nˍ o  ` ni 1  we must also have nˍ o  ` ni. thus nˍ o  ` nk as required. since obviously 老k is the rational prefix of o w.r.t. nk by construction  we have:
proposition 1. given input observation o  the algorithm outputs the rational explanation for o.
example 1. let o = h p q   r  p i. starting with n=   in the first run c 1 = {f p    ↙ r f p q    ↙  p} = {p ↙ r p ＿ q ↙  p}. only the second conditional is exceptional  so c 1 = {p ＿ q ↙  p}. now the remaining conditional is exceptional for itself  so c 1 = c 1. n is updated to
n √ p ↙  q because 老 =  p ↙  q p ↙  r ＿  q  .
﹛the next calculation yields c 1 = {f p p ↙  q  ↙ r  f p q p ↙  q  ↙  p} = {p ＿  q ↙ r q ＿  p ↙  p}.
this time none of the conditionals are exceptional  so c 1 =  . as this means 汐1 =    no further run is necessary and the result is 老 =     p ＿  q  ↙ r   n = p ↙  q.
     p＿ q  ↙ r  p ↙  q  is the rational explanation for o.
1 some examples
in this section we want to give a few simple examples to illustrate the rational explanation.
﹛for o = h p   q i  the rational explanation is     p ↙ q     . so we infer a's initial belief set is p ↙ q. indeed to explain a's belief in q following receipt of p it is clear a must initially believe at least p ↙ q since p itself does not entail q. it seems fair to say we are not justified in ascribing to a any initial beliefs beyond this. after a receives p we assume a accepts this input - we have no reason to expect otherwise - and so has belief set p＿q. if a is given a further input   p＿q  we predict a will also accept this input  but will hold on to its belief in p. the reason being we assume a  having only just been told p  now has stronger reasons to believe p than q. if  instead  a is given further input  p we predict its belief set will be just  p  i.e.  we do not assume a's belief in q persists. essentially the rational explanation assumes the prior input p must have been responsible for a's prior belief in q. and with this input now being  overruled  by the succeeding input  a
﹛can no longer draw any conclusions about the truth of q.
﹛another illustrative example is o = h p    p i  for which the rational explanation is       p . indeed  p must be a core belief  as that is the only possibility for p to be rejected. and if p was not rejected  the agent could not consistently believe  p.
﹛in some cases the rational explanation gives only the trivial explanation  i.e.  nˍ o  √ ﹠. one of the simplest examples extends the prior one: o = h p  p    p p i. the first part of the observation tells us that  p must be a core belief  but when confirmed of that belief a changes its opinion. this behaviour of always believing the opposite of what one is told can be called rational only in very specific circumstances that are not in the scope of this investigation. hence  failing to provide a satisfactory explanation for this example is not to be seen as a failure of the method.
1 conclusion
before concluding  one paper which deserves special mention as having similarities with the present one is  de saintcyr and lang  1  on belief extrapolation  itself an instance of the general framework of  friedman and halpern  1  . a belief extrapolation operator takes as input a sequence of sentences representing a sequence of partial observations of a possibly changing world  and outputs another sequence which  completes  the input. these operators proceed by trying to determine some history of the world which  best fits   according to various criteria  the observations. a fundamental difference between that work and ours is that belief extrapolation is  like traditional operators of revision and update  an  agent's perspective  operator - it is concerned with how an agent should form a picture of how the external world is evolving  whereas we are interested in forming a picture of how an observed agent's beliefs are evolving. nevertheless the precise connections between these two works seems worthy of further study.
﹛to conclude  in this paper we made an attempt at reconstructing an agent's initial epistemic state in order to explain a given observation of the agent and make predictions about its future beliefs. we did so by assuming a simple yet powerful model for epistemic states allowing for iterated nonprioritised revision. the algorithm we provided constructs a best explanation based on the rational closure of conditional beliefs. this answer should be applicable to problems requiring the modelling of agents' beliefs  for example in the area of user modelling.
﹛generalisations of this approach which are object of future work include  i  relaxing the assumption that we are given an unbroken sequence of revision inputs  i.e.  allowing also for intermediate inputs as an explanation for what the core accounts for now   ii  allowing our observations to incorporate information about what the agent did not believe after a given revision step  and  iii  allowing the core beliefs to be revised. further  it is of interest to compare our results with what other models of epistemic states would yield as explanation.
acknowledgements
thanks are due to the reviewers for helpful comments. a.n. acknowledges support by the ec  ist-1  wasp .
references
 booth  1  r. booth. on the logic of iterated nonprioritised revision. in conditionals  information and inference - selected papers from the workshop on conditionals  information and inference  1  pages 1. springer's lnai 1  1.
 brafman and tennenholtz  1  r. i. brafman and m. tennenholtz. modeling agents as qualitative decision makers. artificial intelligence  1-1 :1  1.
 darwiche and pearl  1  a. darwiche and j. pearl. on the logic of iterated belief revision. artificial intelligence  1-1  1.
 de saint-cyr and lang  1  f. dupin de saint-cyr and j. lang. belief extrapolation  or how to reason about observations and unpredicted change . in proceedings of kr'1  pages 1  1.
 del cerro et al.  1  l. farinas del cerro  a. herzig   d. longin  and o. rifi. belief reconstruction in cooperative dialogues. in proceedings of aimsa'1  pages 1- 1. springer's lncs 1  1.
 freund  1  m. freund. on the revision of p