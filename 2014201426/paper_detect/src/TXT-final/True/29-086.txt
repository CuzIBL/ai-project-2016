 
many search trees are impractically large to explore exhaustively. recently  techniques like limited discrepancy search have been proposed for improving the chance of finding a goal in a limited amount of search. depth-bounded discrepancy search offers such a hope. the motivation behind depth-bounded discrepancy search is that branching heuristics are more likely to be wrong at the top of the tree than at the bottom. we therefore combine one of the best features of limited discrepancy search - the ability to undo early mistakes - with the completeness of iterative deepening search. we show theoretically and experimentally that this novel combination outperforms existing techniques. 
1 introduction 
on backtracking  depth-first search explores decisions made against the branching heuristic  or  discrepancies    starting with decisions made deep in the search tree. however  branching heuristics are more likely to be wrong at the top of the tree than at the bottom. we would like therefore to direct search to discrepancies against the heuristic high in the tree - precisely the opposite of depth-first search. to achieve this aim  depthbounded discrepancy search  or dds  combines together limited discrepancy search  harvey and ginsberg  1  and iterative deepening search  korf  1 . dds appears to perform better than both limited discrepancy search  lds and the improved version  ilds  korf  1 . unlike ilds  dds does not need an upper limit on the maximum depth of the tree  that is  the maximum number of branching points down any branch . this will be an advantage in many domains as we often only have a loose limit on the maximum depth because of constraint propagation and pruning. 
1 discrepancies 
a discrepancy is any decision point in a search tree where we go against the heuristic. for convenience  we assume that left branches follow the heuristic. any other branch 
   *the author is supported by epsrc award gr/k/1. i wish to thank members of the apes group for their help. 
1 	search 
breaks the heuristic and is a discrepancy. for convenience  we call this a right branch. limited discrepancy search  lds  explores the leaf nodes in increasing order of the number of discrepancies taken to reach them. on the ith iteration  lds visits all leaf nodes with up to i discrepancies. the motivation is that our branching heuristic has hopefully made only a few mistakes  and lds allows a small number of mistakes to be corrected at little cost. by comparison  depth-first search   d f s   has to explore a significant fraction of the tree before it undoes an early mistake. korf has proposed an improved 

figure 1: ilds on a binary tree of depth 1. 
version of lds called ilds that uses  an upper limit on  the maximum depth of the tree. on the ith iteration  ilds visits leaf nodes at the depth limit with exactly i discrepancies. this avoids re-visiting leaf nodes at the depth limit with fewer discrepancies. when the tree is not balanced or the limit on the maximum depth is loose  ilds re-visits all those leaf nodes above the limit. 
　both lds and llds treat all discrepancies alike  irrespective of their depth. however  heuristics tend to be less informed and make more mistakes at the top of the search tree. for instance  the karmarkar-karp heuristic for number partitioning  korf  1  makes a fixed  and possibly incorrect  decision at the root of the tree. near to the bottom of the tree  when there are 1 or less numbers left to partition  the heuristic always makes the optimal choice. given a limited amount of search  it may pay to explore discrepancies at the top of the tree before those at the bottom. this is the motivation behind depth-bounded discrepancy search. 
1 depth-bounded discrepancy search 
depth-bounded discrepancy search  dds  biases search to discrepancies high in the tree by means of an iteratively increasing depth bound. discrepancies below this bound are prohibited. on the oth iteration  dds explores the leftmost branch. on the i+ hh iteration  dds explores those branches on which discrepancies occur at a depth of i or less. as with ilds  we are careful not to re-visit leaf nodes visited on earlier iterations. this is surprisingly easy to enforce. at depth i on the i -f 1th iteration  we take only right branches since left branches would take us to leaf nodes visited on an earlier iteration. at lesser depths  we can take both left and right branches. and at greater depths  we always follow the heuristic left. search is terminated when the increasing bound is greater than the depth of the deepest leaf node. as the z 1- 1th iteration of dds on a balanced tree with branching factor b explores  branches  the cost of each iteration grows by approximately a constant factor. 

figure 1: dds on a binary tree of depth 1. 
   as an example  consider a binary tree of depth 1. on the oth iteration  we branch left with the heuristic and visit the leftmost leaf node. on the 1st iteration  we branch right at the root and then follow the heuristic left. on the 1nd iteration  we branch either way at the root  just right at depth 1  and left thereafter. branching left at depth 1 re-visits leaf nodes visited on earlier iterations. on the 1rd iteration  we branch either way at the root and depth 1  just right at depth 1  then left thereafter. on the 1th and final iteration  we branch either way up to and including depth 1  and just right at depth 1. this takes us to the remaining leaf nodes and completes the search of the tree. note that we did not re-visit any leaf nodes. the following pseudo-code describes dds for binary trees. note that  unlike ilds  the maximum depth does not need to be given as it is computed during search as the second argument returned by probe. 


　dds visits all the leaf nodes of a search tree  but never re-visits leaf nodes at the maximum depth. to prove this  consider the depth j of the deepest right branch on the path to any leaf node at the maximum depth. only the jth iteration of dds will visit this leaf node. 
dds re-visits leaf nodes above the maximum depth of the tree when the depth bound k exceeds the depth of the shallowest leaf node. this typically occurs late in search when much of the tree has been explored. indeed  as search is usually limited to the first few iterations  we often do not re-visit any leaf nodes. ilds also re-visits leaf nodes that occur above  the upper limit on  the maximum depth of the tree. however  ilds can re-visit leaf nodes from the first iteration onwards. for example  if the leftmost leaf node is above the maximum depth of the tree  it is re-visited on every iteration. 
　given a limited amount of search  dds explores more discrepancies at the top of search tree than lds or ilds. at the end of the first iteration of ilds on a binary tree of height d  we have explored branches with at most one discrepancy. by comparison  dds is already on approximately its log1 d -th iteration  exploring branches with up to  og1 d  discrepancies. and at the end of the second iteration of ilds  we have explored branches with at most two discrepancies. dds is now on approximately its 1og1 d -th iteration  exploring branches with up to 1og1 d  discrepancies. 
　dds is a close cousin of iterative-deepening search  korf  1 . there are  however  two significant differences. first  when the depth bound is reached  we do not immediately backtrack but follow the heuristic to a leaf node. second  by always going right immediately above the depth bound  we avoid re-visiting any nodes at the depth bound. 
1 asymptotic complexity 
for simplicity  we consider just balanced binary trees of depth d. the analysis would generalize with little difficulty to trees with larger branching rates. korf has shown that in searching the tree completely  lds visits  leaf nodes  korf  1 . by comparison  
dds  dfs and ilds all visit the 1d leaf nodes just once. 
ilds and dds are less efficient than dfs as they re-visit interior nodes multiple times. however  they have a similar overhead. dfs visits   nodes in searching the tree exhaustively. by comparison  ilds visits approxim-
	walsh 	1 

ately 1d+1 nodes  korf  1   as does dds. let dos rf  be the number of nodes visited by dds in searching a tree of depth d completely. on the oth iteration  dds explores the d nodes below the root ending at the leaf node on the leftmost branch. on the ith iteration  dds explores completely a subtree of depth i- 1. in addition  dds branches right at the 1i-1 nodes at depth t- 1  and then left down the  d - i  nodes to a leaf node. thus  for d large  

　expanding only right nodes at the depth bound contributes significantly to this result. on the last and most  expensive iteration  the depth bound reaches the bottom of the tree  yet dds explores just the out of the 1d leaf nodes that have not yet been visited. without this efficiency  dds would explore approximately 1d+1 nodes. 
1 a small improvement 
by increasing the space complexity of dds  we can avoid most of the interior node overhead. for instance  we could search to the depth bound using breadth-first search instead of iterative-deepening as present. we can then start successive iterations from the frontier at the depth bound  saving the need to re-visit all the interior nodes above the depth boumd. the only overhead now is that of re-visiting the approximately 1d interior nodes that lie along the left paths followed beneath the depth bound. the dominating cost is that of breadth-first search which visits approximately 1 d + 1 nodes. unfortunately  this improvement increases the space complexity at the kth iteration from o k  to 1 k  . provided search is limited to the first few iterations  this may be acceptable and offer improvements in runtimes. we can achieve lesser savings but at linear cost in the space complexity using recursive best-first search with the depth as the cost function  korf  1 . this will perform breadthfirst search using space that is only of size 1 k  but will re-visit some interior nodes. we might also consider searching up to depth bound using ilds. such a strategy would not explore the tree completely. for example  it would not explore the branch which goes left at the root  right at depth 1  and left thereafter. for completeness  we would need to search to the depth bound using lds. but this search strategy would re-visit many more nodes than necessary. 
1 a simple theoretical model 
to analyse iterative broadening and limited discrepancy search theoretically  harvey and ginsberg introduce a 
1 	search 
simple model of heuristic search with a fixed probability of a branching mistake  ginsberg and harvey  1; harvey and ginsberg  1 . nodes in a search tree are labelled  good  and  bad . a good node has a goal beneath it. a bad node does not. the mistake probability  m is the probability that a randomly selected child of a good node is bad. this is assumed to be constant across the search tree. see  harvey  1  for some experimental 
justification of this assumption. 
　the heuristic probability  p is the probability that at a good node  the heuristic choses a good child first. if the heuristic choses a bad child  then since the node is good  the other child must be good. if the heuristic makes choices at random then p = 1 - m. if the heuristic does better than random then p   1 - m. and if the heuristic always makes the wrong choice  p = 1 - 1m. figure 1  adapted slightly from figure 1 in  harvey  1   lists what can happen when we branch at a good node  and gives the probabilities with which each situation occurs. 

figure 1: branching at a good node. as the situations are disjoint and exhaustive  the probabilities sum to 1. 
　in order to simplify the analysis  harvey and ginsberg assume that p is constant throughout the search tree  though they note that it tends to increase with depth in practice. at the top of the tree  heuristics are less informed and have to guess more than towards the bottom of the tree. this simplifying assumption will bias results against dds. harvey and ginsberg also restrict their analysis to the first iteration of lds. the combinatorics involved in computing exactly the probability of success of later iterations are very complex and have defeated analysis. to study later iterations  we simply estimate probabilities by searching an ensemble of trees constructed to have a fixed depth and a given heuristic and mistake probability. since search will be limited to the first few iterations of the algorithms  the nodes in the trees can be generated lazily on demand  'frees with billions of nodes can easily be analysed by this method. 
as in figure 1 of  harvey and ginsberg  1   figure 
1 gives the probability of finding a goal when searching trees of height 1 with a mistake probability of 1 computed from an ensemble of 1 trees. such trees have approximately a billion leaf nodes  of which just over a million are goals. with about 1 in 1 leaf nodes being goals  such problems are relatively easy. nevertheless  the probability of success for dfs rises only slightly above pd  the probability that the first branch ends in a goal. as in  harvey and ginsberg  1   dfs is given the highest heuristic probability in this and all 
subsequent experiments but still performs poorly. ilds  which is omitted from the graphs for clarity  performs very similarly to lds. on these small and easy problems  dds offers a small advantage over lds at each value of p. the discontinuities in the gradients of the graphs for dds correspond to increases in the depth bound. they do not seem to disappear with even larger samples. 

figure 1: probability of success on trees of height 1 and mistake probability  m = 1. 
we next consider more difficult search problems. as in 
figure 1 of  harvey and ginsberg  1   figure 1 gives the probability of finding a goal when searching trees of height 1 with a mistake probability of 1 computed from an ensemble of 1 trees. such trees have approximately 1 leaf nodes  of which about 1 in 1 are goals. this graph covers the 1th  1st and part of the 1nd iteration of lds. previously  harvey and ginsberg restricted their theoretical analysis to the 1st iteration of lds as it is intractable to compute the probabilities exactly for later iterations. as predicted in  harvey and ginsberg  1   at the start of the 1nd iteration  the probability of success for lds  rise s  steadily once again  as we explore fresh paths at the top of the search tree. however  dds appears to be more effective at exploring these paths and offers performance advantages over lds at every value of heuristic probability. 

figure 1: probability of success on trees of height 1 and mistake probability  m = 1. 
finally  we vary the heuristic probability with depth. 
in figure 1  p varies linearly from 1 - m at depth 1  i.e. random choice  to 1 at depth d  i.e. always correct . this may provide a better model for the behavior of heuristics in practice. dds appears to offer greater performance advantages over lds in such situations. 

figure 1: probability of success on trees of height 1 and m = 1 with heuristic probability  p varying linearly from 1 - m  random choice  at the root to 1  correct choice  at leaf nodes. at depth 
1 early mistakes 
we attribute the success of dds to the ease with which it can undo early mistakes. such mistakes can be very costly for dfs. for instance  crawford and baker identified early mistakes as the cause of poor performance for a davis-putnam procedure using dfs on sadeh's scheduling benchmarks  crawford and baker  1   on some problems  a solution was found almost immediately. on other problems  an initial bad guess would lead to searching a subtree with the order of 1 nodes. recent phase transition experiments have also identified early mistakes as a cause of occasional exceptionally hard problems  or ehps  in under-constrained regions. for example  gent and walsh found ehps in soluble propositional satisfiability problems  gent and walsh  1 . they report that ehps often occur after an early mistake when the heuristic branches into an insoluble and difficult subproblem. similarly  smith and grant found ehps in soluble constraint satisfaction problems with sparse constraint graphs  smith and grant  1 . again they report that they often occur after an early branching mistake into an insoluble and difficult subproblem. 
dds may undo early mistake at much less cost than 
dfs. to test this hypothesis  we generated satisfiable problems from the constant probability model using the same parameters as  gent and walsh  1 . each problem has 1 variables and a ratio of clauses to variables of 1. literals are included in a clause with probability 1  giving a mean clause length of approximately 1. unit and empty clauses are discarded as they tend to make problems easier. this problem class is typically much easier than the random 1-sat model 
	walsh 	1 

but generates a greater frequency of ehps  gent and walsh  1 . to solve these problems  we use the davisputnam procedure  branching on the first literal in the shortest clause. table 1 demonstrates that dds reduces the severity of ehps compared to dfs. other methods to tackle ehps include sophisticated backtracking procedures like conflict-directed backjumping  smith and grant  1  and learning techniques like dependencydirected backtracking  bayardo and schrag  1  which identify quickly the insoluble subproblems after an early mistake. 
dds dfs mean 
1% 
1% 
1% 
1% 
1% 1 
1 
1 
1 
1 1   1 
1 
1 
1 
1 
  1 1 table 1: mean and percentiles in branches explored by the davis-putnam procedure on 1 satisfiable problems from the constant probability model with 1 variables. at the time of submission  dfs had searched over 1 1 branches on 1 problems without success. we hope to report the final result at the conference. 
1 adding bounded backtracking 
whilst early mistakes are not necessarily costly for dds  mistakes deep in the tree will be very costly. bounded backtrack search   b b s   allows quick recovery from mistakes deep in the tree for little additional cost  harvey  1 . a backtrack bound of depth / adds at most a factor of 1l to the search cost. for small /  this is likely to be cheaper than the cost of additional iterations of dds. an alternative view is to consider bounded backtracking as strengthening the branching heuristic to avoid choices which fail using a fixed lookahead. 
   d d s combines very naturally with bbs to give the algorithm d d s - b b s . dds explores early mistakes made at the top of the tree whilst bbs identifies failures rapidly at the bottom of the tree. the two search strategies therefore join together without overlap. harvey has also combined lds with bbs to give lds-bbs. see the appendix for details and for a correction of a small error in the specifications of the algorithm given in  harvey  1; harvey and ginsberg  1 . on the ith iteration  lds-
bbs re-visits all those leaf nodes with less than i discrepancies. one improvement is to combine ilds with bbs so that the discrepancy search does not re-visit leaf nodes. however  even with such a modification  bounded backtracking still re-visits leaf nodes visited on earlier iterations. by comparison  the bounded backtracking in d d s - b b s only visits leaf nodes with a greater discrepancy count that have not yet been visited. the following pseudo-code describes d d s - b b s for binary trees. 
1 	search 

1 experiments 
a phase transition is often seen when we vary the constrainedness of randomly generated problems and they go from being under-constrained and soluble to over-constrained and insoluble  cheeseman et ai  1 . discrepancy search strategies like dds  lds and ilds are designed for under-constrained and soluble problems  where we expect to explore just a small fraction of the search tree. harvey and ginsberg report that such problems are common in areas like planning and scheduling  harvey and ginsberg  1 . discrepancy search strategies offer no advantage on insoluble problems where the tree must be traversed completely. indeed  for a balanced binary tree  our asymptotic results  along with those of  korf  1   show that we explore approximately double the number of nodes of dfs. when the tree is unbalanced  as a result of constraint propagation and pruning  the overhead can be even greater. we therefore restricted the experiments in this section to soluble and under-constrained problems  on which discrepancy search strategies can offer advantages. 
　as in  harvey  1   we use the random 1-sat model. a phase transition occurs for this model at a clause to variable ratio  l/n of approximately 1  mitchell et a/.  1 . we generated 1 satisfiable problems from the soluble region with l/n = 1 and from 1 to 1 variables. each problem is solved using the davis-putnam procedure  branching as before on the first literal in the shortest clause. results are given in table 1. as expected  on such under-constrained and soluble problems  dds and ilds are superior to dfs  with dds offering an advantage over ilds especially on larger and harder problems. by comparison  on  critically constrained  problems at l/n = 1  dfs gave the best performance  with little to chose between ilds and dds. 

n      dfs mean 1%      ild mean s 1% dds mean 1% 1 
1 
1 
1 
1 1 
1 
1 
 1.1 1 1 
1 
1 
　1 1 1 1 
1 
1 
1 
1 1 
1 
1 
1 
1 1 
1 1 
1 
1 1 1  
1 
1 
1 
1 table 1: branches explored by the davis-putnam procedure on 1 satish'able random 1-sat problems at l/n = 1. best results in each row are underlined. 
1 related work 
lds was proposed by harvey and ginsberg in  harvey  
1; harvey and ginsberg  1 . they showed that it outperformed existing strategies like dfs and iterative sampling on large  under-constrained scheduling problems. ilds  korf's improved version of lds appeared in  korf  1 . ari jonsson  personal communication reported on page 1 of  harvey  1   has proposed a related search strategy to dds in which we perform iterative-deepening search followed by bounded backtracking from nodes at the depth bound. like lds  such a strategy re-visits leaf nodes multiple times. for example  the leftmost leaf node is visited on every iteration of the search. by comparison  dds visits only half the nodes at the depth bound  and never re-visits any leaf node at the maximum search depth. 
1 conclusions 
for many problems  the search tree is too large to explore completely. as a consequence  we often want to increase the chance of finding a solution in a limited amount of search. we have shown both theoretically and experimentally that depth-bounded discrepancy search is an effective means of exploring large and under-constrained search trees. by focusing on branching decisions at the top of the search tree  where heuristics are most likely to be wrong  depth-bounded discrepancy search outperforms depth-first and limited discrepancy search. 
