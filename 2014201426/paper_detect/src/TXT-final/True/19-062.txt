 
bundy. silver and plummet  1  provide an analysis of the focussing algorithm and the classification algorithm in the case where the description space consists of a set of relation trees. this paper discusses an extension to their analysis in which the description space is construed as a geometric space. under this construal the behaviour of both the focussing algorithm and the classification algorithm is analysed in terms of the construction of hypercuboids. this analysis leads to a number of observations:  i  that a distinction can be made between a strong and a weak version of the disjunctive-concept problem;  ii  that certain solutions to the disjunctive-concept problem can be shown to exploit what are  in effect  distance functions over the description space and  iii . that the classification algorithm is only capable of learning a subset of the possible disjunctive concepts in any given domain. 
1. 	introduction 
in the interpretation of the focussing algorithm presented by bundy et al.  ibid.  the description space consists of a set of relation trees and execution of the algorithm involves the manipulation of upper markers  which collectively define the most general form of the concept being learned  and lower markers  which collectively define the most specific form of the concept being learned . the form of this marker-manipulation is reasonably simple: following the presentation of a new positive instance  one or more lower markers may be raised so that the most specific form of the concept covers the new instance: following the presentation of a new negative instance  an upper marker may be lowered so that the most general form of the concept no longer covers the new instance. 
in an alternative interpretation  thornton 1  the description space is a geometric space and execution of the algorithm involves the shrinking of an outer hypercuboid  which defines the most general form of the concept being learned  and the expansion of an inner hypercuboid  which defines the most specific form of the concept being learned . 
hypercuboid-manipulations correspond to markermanipulations in the obvious way: following presentation of a new positive instance  the inner hypercuboid may be expanded so as to enclose the point which corresponds to the new instance; following presentation of a new negative instance  the outer hypercuboid may be shrunk so as to exclude the point which corresponds to the new instance. 
to help clarify this alternative interpretation two descriptions of the behaviour of the focussing algorithm will be contrasted. the first of these descriptions will invoke the standard  marker-oriented interpretation: the second will 

suppose that a description space consists of two trees called  a and  b  whose structure is as depicted in fig. 1.  instances in this description space are simply conjunctions of tip nodes taken from trees a and b; e.g. a1 & b1.  the focussing algorithm is presented with a sequence of four instances from this description space:  a1 & b1. a1 & b1. a1 & b l . al & b1 . the first two elements are positive instances of the concept to be learnt and the second two are negative instances. presentation of the first positive instance  i.e. initialisation  causes the placing of upper markers at the root nodes of both trees and lower markers at the appropriate tip nodes  a1 & b1 . presentation of the second positive instance  a1 & b1  causes the lower marker in tree a to be raised to the parent of a1. presentation of the first negative instance  a1 & bl  causes the upper marker in tree b to be lowered and presentation of the second negative instance   a l & b1  causes the upper marker in tree a to be lowered. the positions of markers at this point are as depicted in fig. 1. 

the hypercuboid-oriented interpretation of the behaviour of the algorithm in this case is as follows. we say that the description space has two dimensions called  a  and  b  which range over the values   a l . a1. a1} and {bl. b1. b1  respectively. presentation of the first positive instance  initialisation  triggers the construction of an inner hypercuboid enclosing the a1/b1 cell and an outer hypercuboid enclosing the entire space. presentation of the second positive instance causes the inner hypercuboid to be expanded so as to include the a1/b1 cell. presentation of the two negative instances cause the outer hypercuboid to be shrunk to exclude both the a1/b1 and a1/b1 cells. the structure of 
	thornton 	1 
the inner and outer hypercuboids at this point is as shown in figure 1.  the outer hypcrcuboid is drawn using  *  
characters: the inner hypercuboid is drawn using v characters.  
note that the sequence of instances described does not enable the focussing algorithm to construct a fully-specified concept: the most general form of the defined concept is not identical to the most specific form. under the standard interpretation the evidence for this is the fact that one set of markers do not coincide  i.e. one of the trees is not firmed-up . in the hypercuboid-oriented interpretation the evidence is the fact that the hypercuboids are not identical. 
1. the disjunctive-concept problem 
bundy et al. show that there are sequences of positive and negative instances which  when presented to the focussing algorithm  will cause upper markers to be moved below lower markers or lower markers to be moved above upper markers. such an event corresponds to the emergence of an inconsistency and the failure of the algorithm. this problem is frequently referred to as the disjunctive-concept problem on the grounds that such sequences may be associated with disjunctive concepts. 
an inconsistency  i.e. the inversion of an upper and a lower marker  is described  under the hypercuboid-oriented interpretation  as the situation which arises when an inner hypercuboid boundary extends beyond an outer hypercuboid boundary. viewing inconsistencies in this way leads to the observation that  i  there are precisely two situations which will cause inconsistencies to arise and  ii  that these situations will always lead to inconsistencies regardless of the order in which instances are presented. 

fig. 1 
consider fig. 1 which shows the geometric form of the previously defined description space. positive instances are depicted as  ps : negative instances are depicted as  ns . two positive instances are depicted  pi and p1  and one negative instance  nl . 
note that as things stand it is impossible to construct a hypercuboid which encloses all the ps and excludes all the ns: clearly  any hypercuboid which encloses all the ps must extend beyond a hypercuboid which excludes all the ns. translated into the standard interpretation  this observation says that  in any set of  upper and lower  markers which both cover all the positive instances and exclude all the negative instances  there will be at least one lower marker which is above an upper marker. presentation of the corresponding set of instances will therefore inevitably lead to an inconsistency regardless of the presentation schedule. 
1 	knowledge acquisition 
if the only positive instance was p1  a valid hypercuboid  i.e. one which encloses all the ps and excludes all the ns  could be constructed  but not by the focussing algorithm. the reason is very simple. the focussing algorithm actually manipulates markers not hypercuboid boundaries and its boundary-manipulation capabilities are therefore constrained. it can locate boundaries at positions x and y in some dimension d if and only if the values of d falling between x and y are descended from a unique node in a tree of the underlying tree-based description space. the values in the singleton set {b1  are not descended from a unique node in the tree-based description space  therefore hypercuboid boundaries cannot be positioned so as to enclose them. since any valid hypercuboid for {p1. nl} must have such boundaries  we can infer that the focussing algorithm cannot construct a valid hypercuboid for this set of instances. 
the focussing algorithm then can fail either because  i  the valid hypercuboid does not exist or because  ii  it cannot be constructed due to constraints on boundary manipulations. in either of these situations failure will occur regardless of presentation schedules. 
a useful distinction can therefore be made between two different versions of the disjunctive-concept problem: a strong version corresponding to situation  i  and a weak version corresponding to situation  ii . drawing this distinction leads to the observation that techniques such as the 
 least-disjunction  procedure  utgoff 1  and  tree-hacking   bundy et al. 1  which  in effect  alter the set of possible boundary positions by introducing novel disjunctions into the underlying description space  can alleviate the weak version of the disjunctive-concept problem but not the strong version. 
1. assignment heuristics 
a modification of the focussing algorithm which is capable of learning disjunctive-concepts is associated with mitchell et al.  1 . this technique  called shell creation by bundy et al. 1  involves the construction of what are. in effect  multiple sets of markers  i.e.  rule-shells  . under the hypercuboid-oriented interpretation this technique corresponds to the construction of sets of inner and outer hypercuboids  thornton 1 . 
note that  in the case where multiple inner and outer hypercuboids have been constructed  the presentation of a new instance does not have deterministic consequences. if the new instance is a positive instance  then an inner hypercuboid may have to be expanded to enclose it. if the new instance is a negative instance  then an outer hypercuboid may have to be shrunk to exclude it. the question is  in both cases : which particular hypercuboid should be chosen  
bundy et al. have noted that there appears to be no infallible way of choosing how to assign new instances to existing hypercuboids/rule-shells  1  p. 1 . the conclusion seems to be that assignments must be carried out using a heuristic. 

there are obviously many possible assignment heuristics that might be used; however we can imagine a completely general heuristic which computes preferences for assignments on the basis of explicit features of the data  i.e. which does not employ domain-knowledge . it can be shown that the preferences computed by this heuristic must satisfy certain criteria.  we consider the case where there are two denned  points  in the description space named p1 and p1.  
we can assume that the completely general heuristic  like any other heuristic  must behave systematically. therefore  i  the computed preference for assigning an instance occupying p1 to a hypercuboid enclosing p1 must be identically equal to the computed preference for assigning an instance occupying p1 to a hypercuboid enclosing p1:  ii  the computed preference for assigning an instance occupying p1 to a hypercuboid enclosing p1 must have a maximum value  since the limiting case is the assignment of an instance to a single-cell hypercuboid which already encloses it. 
in addition  we can infer that preferences generated by the heuristic must be intrinsically coherent. this means that the computed preference for assigning an instance occupying p1 to a hypercuboid enclosing p1 must not be outweighed by the computed preference for assigning p1 first to some intermediate hypercuboid. and then on to the one enclosing p1. if computed preferences were of this sort  then we would be able to increase our level of preference for any particular assignment simply by devising ever more elaborate assignment schedules. 
any heuristic which satisfies these criteria must exploit what is. in effect  a valid distance function over the description space. presented with any two points in the description space  the heuristic must be capable of generating a distinct level of preference for assigning one of the two points to a hypercuboid enclosing the other. this level of preference  interpreted as a measure of distance  satisfies all the axioms for a mathematical distance function  including triangle inequality  reflexivity and symmetry. 
1. the classification algorithm 
bundy et al.  ibid.  note that the classification algorithm  associated with quinlan  1  and hunt  1 . is capable of dealing flawlessly with disjunctive concepts. in their presentation of this algorithm  the description space is an attribute-value space and the action of the algorithm involves  an iteration of  the selection of an attribute a followed by the splitting of the instances  which are all given in advance  into sets  such that all the instances in a given set have the same value of a. this process terminates when every set of instances contains either uniformly positive or uniformly negative instances. the structure of attribute choices forms a decision tree  i.e. disjunctive rule  for the concept underlying the instances. 
in the hypercuboid-oriented interpretation of this algorithm 
 thornton 1 . the description space is a geometric space consisting of k dimensions each of which ranges over the possible values of a single attribute. instances correspond to points in the normal way. initialisation of the algorithm corresponds to the construction of a single hypercuboid which encloses all the points in the description space. this hypercuboid is then subdivided into k hypercuboids. where k is the number of values of the chosen attribute. any of the k hypercuboids which do not contain uniform instances are then further subdivided. the division process continues until all hypercuboids contain instances which are either uniformly positive or uniformly negative. 
note that a hypercuboid constructed by the classification algorithm will have boundaries which  for each dimension d. enclose either a single value of d or the complete range of d. in other words  the boundaries in any one dimension of a hypercuboid formed by the classification algorithm must  by definition  be either maximally close or maximally distant. 
if we consider the entire set of possible hypercuboids which can be constructed in any given geometric space of a reasonable size  and compare it with the set of hypercuboids which can be constructed in the same space using boundaries which are of the described form  we will conclude that the size of the former set is overwhelmingly greater than the size of the latter set. the implication is that the classification algorithm can only construct a small proportion of the possible hypercuboids in any given space and must therefore be assumed to be only capable  in general  of constructing a small proportion of the possible disjunctive concepts for a given domain. 
1. summary 
we have shown that under the hypercuboid-based interpretations of the focussing algorithm and the classification algorithm a number of possibilities emerge:  i  a simple account of the disjunctive-concept problem can be presented;  ii  a distinction can be made between a strong and a weak version of the problem;  iii  techniques such as tree-hacking can be shown to alleviate only the weak version;  iv  solutions to the problem which involve the construction of multiple hypercuboids and the exploitation of completely general assignment heuristics can be shown to exploit what are. in effect  valid distance functions over the description space;  v  the classification algorithm can be shown to be capable of forming only a subset of the possible disjunctive concepts for a given domain. 
acknowledgements 
i would like to thank allan ramsay. guy scott. steve draper. claire o'malley and andrew law for providing extremely helpful comments on this paper. 
