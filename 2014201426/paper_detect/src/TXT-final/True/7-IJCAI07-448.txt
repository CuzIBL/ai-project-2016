
distance measures like the euclidean distance have been the most widely used to measure similarities between feature vectors in the content-based image retrieval  cbir  systems. however  in these similarity measures no assumption is made about the probability distributions and the local relevances of the feature vectors. therefore  irrelevant features might hurt retrieval performance. probabilistic approaches have proven to be an effective solution to this cbir problem. in this paper  we use a bayesian logistic regression model  in order to compute the weights of a pseudo-metric to improve its discriminatory capacity and then to increase image retrieval accuracy. the pseudo-metric weights were adjusted by the classical logistic regression model in  ksantini et al.  1 . the bayesian logistic regression model was shown to be a significantly better tool than the classical logistic regression one to improve the retrieval performance. the retrieval method is fast and is based on feature selection. experimental results are reported on the zubud and wang color image databases proposed by  deselaers et al.  1 .
1 introduction
the rapid expansion of the internet and the wide use of digital data in many real world applications in the field of medecine  security  communications  commerce and academia  increased the need for both efficient image database creation and retrieval procedures. for this reason  contentbased image retrieval  cbir  approach was proposed. in this approach  each image from the database is associated with a feature vector capturing certain visual features of the image such as color  texture and shape. then  a similarity measure is used to compare these feature vectors and to find similarities between images with the assumption that images that are close to each other in the feature space are also visually similar. distance measures like the euclidean distance have been the most widely used for feature vector comparison in the cbir systems. however  these similarity measures are only based on the distances between feature vectors in the feature space. therefore  because of the lack of information about the relative relevances of the featurebase feature vectors and because of the noise in these vectors  distance measures can fail and irrelevant features might hurt retrieval performance. probabilistic approaches are a promising solution to this cbir problem  that when compared to the standard cbir methods based on the distance measures  can lead to a significant gain in retrieval accuracy. in fact  these approaches are capable of generating probabilistic similarity measures and highly customized metrics for computing image similarity based on the consideration and distinction of the relative feature vector relevances. as to previous works based on these probabilistic approaches   peng et al.  1  used a binary classification to classify the database color image feature vectors as relevant or irrelevant   caenen and pauwels  1  used the classical quadratic logistic regression model  in order to classify database image feature vectors as relevant or irrelevant   aksoy et al.  1  used weighted l1 and l1 distances  in order to measure the similarity degree between two images and  aksoy and haralick  1  measure the similarity degree between a query image and a database image using a likelihood ratio derived from a bayesian classifier.
﹛in this paper  we investigate the effectiveness of a bayesian logistic regression model based on a variational method  in order to adjust the weights of a pseudo-metric used in  ksantini et al.  1   and then to improve its discriminatory capacity and to increase image retrieval accuracy. this pseudometric makes use of the compressed and quantized versions of the daubechies-1 wavelet decomposed feature vectors  and its weights were adjusted by the classical logistic regression. we will show that thanks to the variational method  the used bayesian logistic regression model is a significantly better tool than the classical logistic regression model to compute the pseudo-metric weights and to improve the querying results. the retrieval method is fast  efficient and based on feature selection. the evaluation of the retrieval method using both models  separately  is performed using precision and scope curves as defined in  kherfi and ziou  1 .
﹛in the next section  we briefly define the pseudo-metric. in section 1  we briefly describe the pseudo-metric weight computation using the classical logistic regression model  while showing the limitations of this latter and that the bayesian logistic regression model is more appropriate for the pseudometric weight computation. then  we detail the bayesian logistic regression model. moreover  we will describe the data training performed for both models. the feature selection based image retrieval method and the feature vectors used to represent the database images are presented in section 1. finally  in section 1  we will perform some experiments to validate the bayesian logistic regression model and we will use the precision and scope  in order to show the advantage of the bayesian logistic regression model over the classical logistic regression one  in terms of querying results.
1 the pseudo-metric
given a query feature vector q and a featurebase of |db| feature vectors tk  k = 1 ... |db|  having 1j components each  our aim is to retrieve in the featurebase the most similar feature vectors to q. to achieve this  q and the |db| feature vectors are daubechies-1 wavelets decomposed  compressed to m coefficients each and quantized. then  to measure the similarity degree between q and a target feature vector tk of the featurebase  we use the one-dimensional version of the pseudo-metric used in  ksantini et al.  1  and given by the following expression
 
 1 
where
t c  i 
 1 
q  and t k are the scaling factors of q and tk  q cq i  and t kqc  i  represent the i-th coefficients of their daubechies1 wavelets decomposed  compressed to m coefficients and quantized versions  w 1 and the wbin i 's are the weights to compute  and the bucketing function bin   groups these latters according to the j resolution levels  such as
		with	i = 1 ... 1j   1.	 1 
1 the weight computation
in order to improve the discriminatory power of the pseudometric  we compute its weights using a classical logistic regression model and a bayesian logistic regression model  separately. we define two classes  the relevance class denoted by 次1 and the irrelevance class denoted by 次1  in order to classify the feature vector pairs as similar or dissimilar. the basic principle of using the bayesian logistic regression model and the classical logistic regression one is to allow a good linear separation between 次1 and 次1  and then to compute the weights which represent the local relevances of the pseudo-metric components.
1 the classical logistic regression model
in this model  each feature vector pair is represented by an explanatory vector and a binary target variable. specifically  for the i-th feature vector pair  we associate an explanatory vector xi =  x 1 i x1 i ... xj 1 i 1  ﹋ rj ℅{1} and a binary target si which is either 1 or 1  depending on whether or not the two feature vectors are intended to be similar. x 1 i is the absolute value of the difference between the scaling factors of the daubechies-1 wavelets decomposed  compressed and quantized versions of the two feature vectors and are the numbers of mismatches between the j resolution level coefficients of these latter. we suppose that we have n1 pairs of similar feature vectors and n1 pairs of dissimilar ones. thus  the class 次1 contains n1 explanatory vectors and their associated binary target variables to represent the pairs of the similar feature vectors  and the class 次1 contains n1 explanatory vectors and their associated binary target variables to represent the pairs of the dissimilar feature vectors. the pseudo-metric weights w 1 and and an intercept v are chosen to optimize the following conditional log-likelihood.

where pri and pirj are the relevance and irrelevance probabilities  respectively  and given by
 
where is the logistic function. for this reason  standard optimization algorithms such as fisher scoring and gradient ascent algorithms  clogg et al.  1   can be invoked. however  in several cases  especially because of the exponential in the likelihood function or because of the existence of many zero explanatory vectors  the maximum likelihood can fail and estimates of the parameters of interest  weights and intercept  may not be optimal or may not exist or may be on the boundary of the parameter space. also  as there is complete or quasicomplete separation between 次1 and 次1  the function l is made arbitrarily large and standard optimization algorithms diverge  krishnapuramet al.  1 . moreover  as 次1 and 次1 are large and high-dimensional  these standard optimization algorithms have high computational complexity and take long time to converge. the first two problems can be solved by smoothing the parameter of interest estimates  assuming a certain prior distribution for the parameters  thereby reducing the parameter space  and the third problem can be solved by using variational transformations which simplify the computation of the parameter of interest estimates  jaakkola and jordan  1 . this motivates the adoption of a bayesian logistic regression model based on variational methods.
1 the bayesian logistic regression model
in the bayesian logistic regression framework  there are three main components which are a chosen prior distribution over the parametersof interest  the likelihoodfunction and the posterior distribution. these three componentsare formallycombined by bayes' rule. the posterior distribution contains all the available knowledge about the parameters of interest in the model. among many priors having differentdistributional forms  gaussian prior has the advantage of having low computational intensity and of smoothing the parameter estimates toward a fixed mean and away from unreasonable extremes. however  when the likelihood function is not conjugate of the gaussian prior  the posterior distribution has no tractable form and its mean computation involves high-dimensional integration which has high computational cost. according to  jaakkola and jordan  1   it's possible to use accurate variational transformations in order to approximate the likelihood function with a simpler tractable exponential form. in this case  thanks to the conjugacy  with a gaussian prior distribution over the parameters of interest combined with the likelihood approximation  we obtain a closed gaussian form approximation to the posterior distribution. however  as the number of observations is large  the number of variational parameters updated to optimize the posterior distribution approximation is also large  thereby the computational cost is high. in the bayesian logistic regression model that we propose  we use variational transformations and the jensen's inequality in order to approximate the likelihood function with tractable exponential form. the explanatory vectors are not observed but instead are distributed according to two specific distributions. the posterior distribution is also approximated with a gaussian which depends only on two variational parameters. the computation of the posterior distribution approximation mean is fast and has low computational complexity. in this model  we denote the random vectors whose realizations represent the explanatory vectors of the relevance class 次1 and the explanatory vectors of
the irrelevance class 
and x  respectively. we suppose that x1 ‵ q1 x1  and x1 ‵ q1 x1   where q1 and q1 are two chosen distributions. for x1 we associate a binary random variable s1 whose realizations are the target variables  and for x1 we associate a binary random variable s1 whose realizations are the target variables
. we set s1 equal to 1 for similarity and we set s1 equal to 1 for dissimilarity. parameters of interest  weights and intercept  are considered as random variables and are denoted by the random vector w . we assume that w ‵ 羽 w   where 羽 is a gaussian prior with prior mean 米 and covariance matrix 曳. using bayes' rule  the posterior distribution over w is given by
p w|s1 = 1 s1 = 1  =
 px1﹋次1 x1﹋次1 qi1 p si = i|xi = xi w qi xi = xi  羽 w 
 
p s1 = 1 s1 = 1 
 for each
i ﹋ {1}. using a variational approximation  jaakkola and jordan  1  and the jensen's inequality  the posterior distribution is approximated as follows
 

where

 
where eq1 and eq1 are the expectations with respect to the distributions q1 and q1  respectively  and  are the variational parameters. therefore  the approximation of the posterior distribution is considered as an adjustable lower bound and as a proper gaussian distribution with a posterior mean 米post and covariance matrix 曳post which are estimated by the following bayesian update equations

the weight and intercept computation algorithm is in two phases. the first phase is the initialization of q1  q1 and the gaussian prior 羽 w   and the second phase is iterative and allows the computation of 曳post and 米post through the bayesian update equations  1  and  1   respectively  while using an em type algorithm  jaakkola and jordan  1   in order to find the variational parameters at each iteration to have an optimal approximation to the posterior distribution. in the initialization phase  q1 and q1 are chosen to model 次1 and 次1  respectively  and because of the absence of prior knowledge about the weights and the intercept  羽 w  is chosen univariate with zero mean and large variances  congdon  1 . the values of 米post components are the desired estimates of the pseudo-metric weights and the intercept v. once the parameters of the posterior distribution approximation are computed  its magnitude is given by the term. this latter becomes very close to 1 as 次1 and 次1 are linearly separated or quasi separated and tends towards 1 as 次1 and 次1 become more and more overlapped. analogically  in the classical logistic regression model  the term e1l has almost the same characteristics as  caenen and pauwels  1 . these two terms will be used to perform feature selection in the retrieval method.
1 training
let us consider a color image database which consists of several color image sets such that each set contains color images which are perceptually close to each other in terms of object shapes and colors. in order to compute the pseudo-metric weights and the intercept by the classical logistic regression model  we have to create the relevance class 次1 and the irrelevance class 次1. to create 次1  we draw all possible pairs of feature vectors representing color images belonging to the same database color image sets  and for each pair we compute an explanatory vector and we associate to this latter a binary target variable equal to 1. similarly  to create 次1  we draw all possible pairs of feature vectors representing color images belonging to different database color image sets  and for each pair we compute an explanatory vector and we associate to this latter a binary target variable equal to 1. for the bayesian logistic regression model  we create the 次1 and 次1 with the same way  but instead of associating a binary target variable value to each explanatory vector of 次1 and 次1  we associate a binary target variable s1 equal to 1 to all 次1 explanatory vectors and we associate a binary target variable s1 equal to 1 to all 次1 explanatory vectors.
1 color image retrieval method
the querying method is in two phases. the first phase is a preprocessing phase done once for the entire database containing |db| color images. the second phase is the querying phase.
1 color image database preprocessing
we detail the preprocessing phase done once for all the database color images before the querying in a general case by the following steps.
1. choose n feature vectors for comparison.
1. compute the n feature vectors tli  l ﹋ {1 ... n}  for each i-th color image of the database  where i ﹋ {1 ... |db|}.
1. the feature vectors representing the database colorimages are daubechies-1 wavelets decomposed  compressed to m coefficients each and quantized.
1. organize the decomposed  compressed and quantizedfeature vectors into search arrays  and  which are used to optimize the pseud-metric
computation process  ksantini et al.  1 .
1. adjustment of the metric weights for each featurebase tli  i = 1 ... |db|  representing the database color images  where l ﹋ {1 ... n}.
1 the querying algorithm
we detail the querying algorithm in a general case by the following steps.
1. given a query color image  we denote the feature vectorsrepresenting the query image by.
1. the feature vectors representing the query image aredaubechies-1 wavelets decomposed  compressed to m coefficients each and quantized.
1. the similarity degrees between and the database color image feature vectors  are represented by the arrays
 such that
for each i ﹋ {1 ... |db|}. these arrays are returned by the procedure retrieval  respectively. the procedureretrieval is used to optimize the querying process  ksantini et al.  1 .
1. the similarity degrees between the query color imageand the database color images are represented by a resulted array totalscore  such as  totalscore i  =  for each i ﹋ {1 ... |db|}  where  are weightfactors used to down-weight the fea-
ture which has low discriminatory power. 污l = e1ll when the weights are computed by the classical logistic regression model  and when the weights are computed by the bayesian logistic regression model.
1. organize the database color images in order of increasing resulted similarity degrees of the array totalscore. the most negative resulted similarity degrees correspond to the closest target images to the query image. finally  return to the user the closest target color images to the query color image and whose number is denoted by ri and chosen by the user.
1 used feature vectors
in order to describe the luminance  colors and the edges of a color image  we use luminance histogram and weighted histograms. the image texture description is performed by kurtosis and skewness histograms. given an m ℅ n pixel lab color image  its luminance histogram hl contains the number of pixels of the luminance l  and can be written as follows
	 	 1 
for each c ﹋ {1 ... 1}  where il is the luminance image and 汛 is the kronecker symbol at 1. the weighted histograms are the color histogram constructed after edge region elimination and the multispectral gradient module mean histogram. the former is given by

and the latter is given by
	 	 1 
where np k c  is the number of the edge region pixels and is defined as

and

 
for each c ﹋ {1 ... 1}and k = a b  where 竹max represents the multispectral gradient module  ksantini et al.  1   灰 is a threshold defined by the mean of the multispectral gradient modules computed over all image pixels  ia and ib are the images of the chrominances a red/green and b yellow/blue  respectively  and 聿 is the characteristic function. the multispectral gradient module mean histogram provides information about the overall contrast in the chrominance and the edge region elimination allows the avoidance of overlappings or noises between the color histogram populations caused by the edge pixels. the lab color image kurtosis and skewness histograms are given by
	 	 1 
and
	 	 1 
respectively  for each c ﹋ {1 ... 1} and k = l a b  where il百  ia百 and ib百 are the kurtosis images of the luminance l and the chrominances a and b  respectively  and  and ibs are the skewness images of these latter. they are obtained by local computations of the kurtosis and skewness values at the luminance and chrominance image pixels. then  a linear interpolation is used to represent the kurtosis and skewness values between 1 and 1. since each used feature vector is a histogram having 1 components  we set j equal to 1 in the following section.
1 experimental results
in this section  we will discuss the choices of the distributions q1 and q1  in order to validate the bayesian logistic regression model in the image retrieval context. finally  we will use the precision and scope as defined in  kherfi and ziou  1   to evaluate the querying method using both models separately. the choices of the distributions q1 and q1 and the querying evaluation will be conducted on the wang and zubud color image databases proposed by  deselaers et al.  1 . the wang database contains |db| = 1 color images which were selected manually to form 1 sets  e.g. africa  beach  ruins  food  of 1 images each. the zurich building image database  zubud  contains a training part of |db| = 1 color images and query part of 1 color images. the training part consists of 1 building image sets  where each set contains 1 color images of the same building taken from different positions. before the feature vector extractions  we represent the wang and zubud database color images in the perceptually uniform lab color space. since from each color image of the zubud and wang databases we extract n = 1 histograms which are given by  1    1    1    1  and  1  respectively  each database is represented by eleven featurebases. the choices of q1 and q1 will be separately performed for each featurebase. for each featurebase  we assume that  and  x1 ... xj 1  are independent. we make the same assumption for  and
 x1 ... xj 1 . moreover  we suppose that the random vector  x1 ... xj 1  random variables whose realizations are positive integers  are independent and each one of them follows a truncated poisson distribution at its greatest realization  to have a best fit. analogically  we make the same choice for  x1 ... xj 1 . also  we assume that the random variable  whose realizations are positive reals  follows a gaussian mixture distribution  which is the same choice for . generally  to carry out an evaluation in the image retrieval field  two principal issues are required: the acquisition of ground truth and the definition of performance criteria. for ground truth  we use human observations. in fact  three external persons participate in the below evaluation. concerning performance criteria  we represent the evaluation results by the precision-scope curve pr = f ri   where the scope ri is the number of images returned to the user. in each querying performed in the evaluation experiment  each human subject is asked to give a goodness score to each retrieved image. the goodness score is 1 if the retrieved image is almost similar to the query  1 if the retrieved image is fairly similar to the query and 1 if there is no similarity between the retrieved image and the query. the precision is computed as follows: pr = the sum of goodness scores for retrieved images/ri. therefore  the curve pr = f ri  gives the precision for different values of ri which lie between 1 and 1 when we perform the querying evaluation on the wang database  and lie between 1 and 1 when we perform the querying evaluation on the zubud database. when the human subjects perform different queryings in the evaluation experiment  we compute an average precision for each value of ri  and then we construct the precision-scope curve. in our evaluation experiment  each color image of the wang and zubud databases is represented by n = 1 histograms which are hl  hha  hhb   h‘ea  h‘eb  hl百  h百a  hb百   hsl 
. in order to evaluate the querying in the wang database  each human subject is asked to formulate a query from the database and to execute a querying  using weights computed by the classical logistic regression model  and to give a goodness score to each retrieved image  then to reformulate a query from the database and to execute the querying  using weights computed by the bayesian logistic regression model  and to give a goodness score to each retrieved image. each human subject performs the querying fifty times by choosing a new query from the database each time. we repeat this experience for different orders of compression m ﹋ {1 1}. to evaluate the querying in the zubud database  each human subject is asked to follow the preceding steps  while formulating the queries from the database query part. for the wang and zubud databases  the resulted precision-scope curves are given in figure 1 for compression orders m ﹋ {1 1}. the figure 1 illustrates two retrieval examples in the zubud database comparing the performances of the regression models for m = 1. in each example the query is located at the top-left of the dialog box.

 a 

 b 
figure 1: evaluation   a  zubud database and  b  wang database : precision-scope curves for retrieval using weights computed by the classical logistic regression model and weights computed by the bayesian logistic regression model.

	 a 	 b 
figure 1: comparison  zubud database : a  first 1 color images retrieved using weights computed by the classical logistic regression model  b  first 1 color images retrieved using weights computed by the bayesian logistic regression model.
1 conclusion
we presented a simple  fast and effective color image querying method based on feature selection. in order to measure the similarity degree between two color images both quickly and effectively  we used a weighted pseudo-metric which makes use of the one-dimensionaldaubechies decomposition and compression of the extracted feature vectors. a bayesian logistic regression model and a classical logistic regression one were used to improve the discriminatory capacity of the pseudo-metric and to allow feature selection. evaluations of the querying method showed that the bayesian logistic regression model is a better tool than the classical logistic regression one to compute the pseudo-metric weights and to improve the querying results.
