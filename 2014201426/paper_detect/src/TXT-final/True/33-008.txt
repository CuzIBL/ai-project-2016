 
boris is an integrated natural language understanding system for narratives. in an integrated system  processes of event assimilation  inference  and episodic memory search occur on a word-by-word basis as parsing proceeds.  parsing  here refers to the task of building a conceptual representation for each natural language expression. in addition to being integrated  the boris parser is also a unified parser. the same parser is used both at story understanding time and question answering time. this paper explores some of the consequences which arise when the same parser serves both tasks. for instance  one such consequence is that boris often knows the answer to a question before it has completely understood the question. 
l background 
　　early natural language programs at yale used a single parser which operated in isolation from other conceptual processes. 
i he parser would first map each sentence into its conceptual 
dependency  cd  representation   which a script applier  sam    plan applier  pam    or question answerer  qualm   would then interpret. for a time  it was hoped that a single parser could be constructed to serve as a  front-end  for every new project taking natural language as input. 
　　but eventually it became clear that this type of modularity had its drawbacks. with an isolated parser  other process knowledge arising from the parsing task could never be made 
available as an aid in the natural language understanding task itself. modularity was abandoned with the creation of frump   which relied heavily on scriptal knowledge to direct its parsing processes. since then every parser at yale has been integrated to a greater degree - each trying to make use of task and domain specific knowledge to aid in parsing. 
　　to date  boris    an experimental program designed to understand complicated narratives  approximately 1 words in length   is the most integrated parsing project at yale. boris runs as a single module: all tasks such as event assimilation  inference  memory search and question answering  occur as an integral part of the parsing process. this paper discusses some of the issues and consequences that arise from an integrated approach to text processing. 
　　this work supported in part by advanced research projects agency contract nooi1-c-1 and national science foundation contract ist1. 
1. integrating parsing with memory search 
　　our interest in making boris more integrated first arose within the task domain of question answering  qa . an informal analysis of question answering protocols revealed that people were making use not only of general syntactic  semantic and lexical knowledge to parse a question  they were also making use of episodic knowledge -- i.e. the information specifically contained within the story they had read. this made perfectly good sense. if people understand in context  then of course they would make use of the story itself as a context for understanding a question about the story. here was all of this episodic information available. why not use it to help parse questions  
for instance  suppose we read the following story: 
richard hadn't heard from his friend paul in years. richard owed paul money which he had never returned because he didn't know where paul lived. then he got a letter from paul asking him to represent paul in a divorce case. 
now  if we are asked: 
q: why hadn't richard paid paul back  
we will think of the following answer: 
a: richard didn't know where paul lived. 
suppose  however  the story had been: 
richard hadn't heard from his friend paul in years. paul had killed richard's wife and richard vowed that he would get back at him if it was the last thing he ever did  but paul had 
escaped to argentina. 
now if asked: 
q: why hadn't richard paid paul back  
we do not interpret the question to mean:  why hadn't richard returned money to paul   now we understand  pay back  in terms of revenge. 
　　the expression  pay back  cannot be disambiguated by means of general semantic  syntactic or lexical knowledge. the only way  pay back  can be understood in the question above is by searching episodic memory while the question is being parsed. this view of the question understanding process has four major consequences: 
　　 i  integrated parsing changes our perspective on what it means to understand a question. it no longer means building a representation of the question in isolation of the narrative context. understanding a question now involves finding a 
　　referent for the question in episodic memory. in the terminology of a prior question answering model   the parser is now also involved in memory search for an answer key. this means that the representation of a question is no longer just a conceptual 
1 

dependency  diagram but includes pointers into episodic memory - i.e. references to episodic instantiations or higher level knowledge structures. 

　　 1  by viewing parsing and episodic search as an integral process  specific knowledge contained within the narrative becomes available to aid in the parsing process itself. as we have already seen with the example of  pay back   this integrated approach makes 'disambiguation'easier. 

　　 1  since an integrated parser starts searching episodic memory immediately  as soon as the first word of the question is parsed   it is now possible to retrieve the answer to the question before its parse i$ finished. this may sound magical  but in fact people do it all the time. people search memory to find an answer as soon as possible  and then they monitor the rest of the question to make sure that no presuppositions have been violated. for example  given the following story: 
　　on a night when there was a full moon. john took mary for a walk along the river. there he kissed her and asked her to marry him. she was so surprised that she was left speechless. when asked the question: 
q: who kissed mary by the river when the moon was full  
most people have the answer  john  by the time they have processed the first three words in the question. the rest of the question is simply monitored to make sure that any presuppositions in the question do not violate knowledge of the story. earlier yale systems would have first parsed the question in its entirety before attempting any search for an answer 

　　 1  in those cases when an answer is not found before the question has been fully processed  subsequent answer retrieval will be faster in an integrated system. this effect is the result of having searched episodic memory during question understanding. since the parser has already searched episodic memory  the answer retrieval phase is already partially accomplished by the time the parse phase is completed. 

1. parser unification 
　　boris did not start out as a single module. largely for pragmatic reasons  early versions of boris had two parsers. one was called dypar  and was used to parse natural language questions about narratives. the other was a version of ca  i . the ca parser was used to analyze sentences occurring within narratives and produce a conceptual dependency representation for each. at story understanding time  a separate event assimilation module accepted these conceptual dependencies as input and constructed an episodic memory of the events in the narrative. 
　　the integrated approach to parsing questions was sufficiently successful in boris for us to consider this approach for parsing narrative sentences at story understanding time. an additional motivation rose from our belief that two distinct parsers are not psychologically valid. it seems most unlikely that people would use one parser for answering questions which is fundamentally different from the parser used for understanding stories. 
     because earlier parsers had operated in isolation  parser unification had never been an issue. when a parser does not interact with any other processes during sentence analysis  it is trivial to use the same parser to produce conceptual dependency representations for both questions and narrative sentences. parser unification only becomes tricky when parsing procedures are integrated with other memory manipulations. 
recall that  as the ca parser produced a conceptual 
dependency representation as output  the event assimilator then had the task of building episodic memory from each conceptual dependency input. since the parser worked in isolation  it was up to the the event assimilator to apply top-down expectations across sentence boundaries in order to 'knit'events together into a cohesive episodic memory for the narrative. for example  whenever the parser failed to fill a role binding  because of ellisions   or whenever there was a need to interpret a conceptual dependency structure in terms of a higher level of knowledge  the event assimilator would step in. however  this 
approach posed a fundamental problem for parser unification. 
     a unified parser cannot rely upon an event assimilation module to aid in the parse for the following simple reason: once the narrative has been completely processed  event assimilation expectations no longer exist. these expectations arose as each sentence in the narrative was being read. when the comprehension task is complete and the entire narrative has been processed  there are no more active expectations. but the description of any event which occurred in a narrative may appear within a subsequent question about that narrative. thus  the qa parser has to be able to parse event descriptions which were parsed at story-understanding time  but without the help of an event assimilation module. it is important to have a qa parser which can work after all active story expectations have terminated. 
     in fact  the question answering parser  dypar  had been parsing event descriptions successfully without relying on an event assimilation module. as we have already stated  it accomplished this by searching episodic memory during the parse. thus  the solution seemed clear: unify the two parsers both by eliminating the event assimilator and by augmenting the qa parser to build episodic memory during its parsing process. this was accomplished  yielding a completely integrated system in which there is a single parser scheduling all processing for both story understanding and question answering. 
　　specifically  this complete integration was achieved in the following way: unlike frump and other hop-down'processors  boris operates in a very 'bottom-up' manner. active processes called  demons  are associated with lexical and phrasal entities. in addition to syntactic and semantic sentence analysis  these demons also search and construct episodic memory. 
although these demons can not be expected to know about 
1 
the specific content of episodic memory  they do know enough about the structural principles of episodic memory to search it and determine how each lexical entity should be interpreted. this is done by finding an appropriate knowledge structure in memory  and activating processes associated with that knowledge structure. it is memory search demons associated with lexical items that ultimately determine which knowledge structure to apply. 

　　to get an idea of how parsing can entail access to episodic memory  consider the following short story  with two alternative endings: 
bill was very jealous of his wife mary. he decided to hire 
george  a private detective  to keep an eye on her. george took his miniature camera with him to the school where mary worked. 
fi: there  george found mary with another teacher. 
　　e1. there  george found mary with another man. in el  the inference is that mary is also a teacher. however  the inference in e1 is not that mary is also a man. instead  the inference to be made in e1 is that mary is with a man other than her husband bill  versus 'other than' george . 
　　in addition  if e1 had been read in isolation  it would not be clear whether it is george or mary who is also a teacher. interpretive inferences in these cases depend on what immediately precedes and follows  another   and the relationship between george and mary. the moment  another  finds the object.it is modifying  it activates demons which search episodic memory for the needed interpersonal information.1 
1. 	event explanation 
　　integration in boris has resulted in a system which is more explanation-based than expectation-based. that is  each input must search memory in order to explain itself  instead of relying upon some  top-down  process to grab it and interpret it. in boris  most expectations   1  are encoded implicitly within the structure of episodic memory  rather than being active processes which poll the input. because expectations in boris fall into a fixed number of classes  each class can be represented declaratively in terms of the kinds of memories which are being constructed. 
for example  when boris reads: 
     s1: the teacher examined the student   examined  is represented as: 
ev-1:  d-know actor x 
object  state of y  
instru  attend to y   
boris tries to 'explain' ev-1 in the following way: 
to explain an event e  use whatever bottom-up knowledge is associated with the event to decide where in memory to search. once a knowledge structure  ks  is found  apply whatever 
　the word  another  appears in two narratives boris has processed  in one  boris infers that a character is a teacher. in the other boris infers marital infidelity. for the text of these narratives  see    
processing is associated with that ks to e. this may result in building new structures  or connections  in episodic memory. 
for instance  boris knows that the object bound to the 
actor slot in ev-1 could tell it where to search. since x is bound to rt-teacher  role theme information about teachers   boris searches knowledge structures associated with rt-teacher. within rt-teacher are pointers to other memory structures  give-test in m-education   which will match the description of ev-i. thus  ev-1 will be 'explained* in terms of m-education. this particular search heuristic will also work correctly in other cases  such as: 
s1: the doctor examined the student. 
which refers to a rather different situation than the one described bys1. 
　　when the semantics of the input sentence fails to constrain the memory search  episodic memory is searched according to recency and the first structure which matches the input is then used to interpret the input. this is similar in spirit to having 
explicit expectations each poll the input according to recency. thus  the explanation-based approach compares favorably with having expectation-driven techniques. 
　　when is an event fully 'explained  this is a difficult issue which ultimately depends on the beliefs and interests that a reader brings to the subject matter being read. a satisfactory explanation for one person may seem superficial to another. the intelligence of a reader depends on the depth of explanation sought and what is appropriate given the surrounding circumstances. 
　　in earlier versions of boris  two different approaches were tried: 
　　 a  once one knowledge structure has been found which matches the input  then the input has been explained. 
　　 b  exhaustively apply every knowledge structure to the input. 
　　but there are weaknesses with each of these strategies. the first approach is inadequate for an understanding system which supports multiple perspectives  because the explanation process will terminate after only one perspective is found. for example  one narrative boris reads is divorce! in it  two characters  richard and paul  agree to meet at lunch for the following reasons: a  they haven't seen each other for years  and b  richard is a lawyer and paul wants richard to represent him in the divorce case. their meeting  therefore  has more than one perspective: 
scriptal: restaurant meal occurring. 
thematic: suspended friendship being renewed. 
role: lawyer and client meeting to discuss legal case. 
the second approach solves the multiple perspective problem by examining every active knowledge structure in order to explain a given input. however  this creates a very unfocussed system  which will always be considering lots of irrelevant knowledge structures. this approach  therefore  is inherently inefficient. 
　　the current approach used in boris is to analyze an event at several specified levels - scriptal  goal/plan  thematic  and 
1 

role. once an event has been analyzed in terms of each level  the processing for that event terminates. 
1. parsing in different modes 
　　although both narrative sentences and questions are parsed by the same program  there are several situations in which the  mode  of understanding  question answering versus story understanding  requires that the parser behave differently: 
1.1. tokentzotkm 
　　whenever the parser encounters a reference to an object primitive  a setting  or a character  memory is searched to find out if the object already exists in episodic memory. in story-understanding mode  the failure to find a referent results in the creation of a new token in memory. however  during question answering  a failure to find a referent results in abandoning any attempt to find an answer. instead  a 
'complaint' is generated to point out that a referent could not be found. for example  given a story only about two characters. 
richard and paul  boris would respond as follows: 
q: what does george do for a living  
a: i don't recall any mention of a person named george in the story. 
1.1. presupposition cheeking 
　　during story understanding  if a knowledge structure in episodic memory is referenced and the role bindings created during the parse fail to match the bindings already in memory  this indicates that a new instantiation must be built. for example  in divorce-!  richard receives a letter from paul. if a letter were subsequently mentioned as having been written by another character  say sarah   then boris would create a new instance of a letter knowledge structure in memory. 
　　during question answering  the parser also checks the role bindings presumed by the question against the actual bindings maintained in episodic memory. in this case  however  a failure of roles to match indicates a false presupposition. so boris rejects the question and corrects the false presupposition. for instance: 
q: why did sarah write to richard  
a: it was paul  not sarah  who wrote to richard. 
　　people sometimes fail to notice false presuppositions. in such cases the presupposition can become incorporated into their memory model of the narrative  see section 1 . 
1.1. question words 
　　in any narrative or question  words like  who    what    why     where   etc. must be handled in special ways. if such a word is encountered during question answering  it is assumed to be initiating a request for information. during story understanding however  different demons process these words when they occur at clause boundaries. so if a question is encountered while the story is being comprehended  it will be treated as a rhetorical question  and no attempt will be made to answer it. 
1.1. loco! contexts 
　　another difference between story understanding and question answering modes arises from the use of local contexts in story understanding and question answering tasks. local contexts arc represented by the memory created for events immediately prior to some event currently being explained. 
consider the following sentence   from  
richard had borrowed money which was never puid back  but now he didn't know where to find his old friend. 
the parse of the first clause causes the knowledge structure 
m-borrow to be built  within which the event return-obj is marked as violated. the second clause  however  is understood with the help of a local m-borrow context. as the parser produces a representation of richard's goal failure  not knowing where to find paul   the following 'negative event* rule is 
activated: 
if a negative event e1 occurs  and local context contains a violation of a prior event e1 
then see if e1 enables ei. and if so  create link:  e 1 blocked-by e1  
as an explanation for the violation. 
thus  'not knowing where to find paul' is understood as the explanation for 'never having paid paul back' by using the above rule  along with enablement information supplied by m-borrow 
　　a different kind of local context exists during question answering. in this case  the local context contains the last question asked  along with its answer.1 consider the following question and its alternative answers: 
q: did richard know where to find his old friend  
ai: yes. they met at a restaurant. 
a1: yes. paul had written to richard. 
a1: no. richard hadn't heard from paul in years. 
although this question contains information similar to its narrative counterpart. boris will find answers al and a1 before a1 because the local context provide constraints differing from those during story understanding. 
　　more specifically  there is no direct access to all of richard's knowledge states in boris. instead. boris is forced to search a scenario map  in order to see if paul and richard ever interacted with one another. however  understanding the question: 
q: why hadn't richard paid paul back  
accesses m-borrow directly  as it was accessed at story understanding time . next the  blocked-by  link  which was constructed during the comprehension phase  is retrieved by an 
   one use of local context during question answering is to handle such situations as: 
ql: who wrote richard a letter  al: paul. 1: why  
where both the previous question and answer are needed to understand a subsequent question. 

interference search . this results in the answer: a: richard didn't know where paul lived. 
　　one natural consequence of the different uses of local context in story understanding and question answering is that it is difficult to understand a question about an event which required a local context when being understood at story understanding time. 
1. memory modification during qa 
　　as parsing became unified and integrated in boris  an unanticipated situation arose during question answering: asking 
boris certain questions created episodic memory modifications. at first we thought this effect was a 'bug' in the program  and the difference in parsing modes  see section 1.  was set up in an attempt to eliminate it. 
　　but on further reflection  there is no reason to believe that people only construct memories during narrative comprehension  any more than they only search memories during question answering. in fact  psychological experiments by elizabeth loftus  ii  have demonstrated that the process of asking questions can modify the memory of eyewitnesses. preliminary experiments performed at yale  show that the loftus effect occurs in the case of narratives as well. it follows that memory modification during question answering is a natural consequence of having a unified and integrated parser. 
　　how do memory modifications during question answering actually occur  if we assume that the mode of processing  as described in section 1  determines when new memories can be built  then memory modification depends on  fooling  an understanding system in question answering mode to think that it is operating in story understanding mode. this naturally occurs if the understander assumes that whoever is asking a question knows something about the narrative and may impart this information within the question. questions normally convey information by means of presuppositions. for example  suppose we read a story about john going to a store  and then we are asked: 
q: when did john go to buy groceries  
here  the questioner presupposes that john went specifically to a grocery store. if we believe that the questioner knows something about john's activities  then we will accept being told new information in the question. this is similar to having heard originally that john went grocery shopping. so modification of old information during question answering depends upon: 
　　 1  an acceptance  by the answerer  that the questioner may know something about the situation under examination. 
　　 1  new information supplied in the presupposition of the question. 
　　in boris  memory modification can occur because the processes which add information to an event description are the same processes which check presuppositions. for example  if a narrative sentence states  john got a letter from mary  then a letter-event must be instantiated with the bindings:  sendcr = mary  and  receiver = john . in order to determine that this is not a reference to a new letter-event  memory search processes must search to see if another letter-event exists with different bindings. now suppose that boris is asked:  why did john get a letter from mary  . again we must search memory for a 
　　letter-event which shares these bindings. so presupposition checking and role-binding instantiations are performed at the same point during both story understanding and question answering. 
　　however  if a question presupposition clearly violates our memory  we will complain. so the question:  why did john kill mary   - in a story about john and mary getting married - will not be accepted. 
　　the experiments in  indicate that memory modifications during question answering are more likely to occur when the presuppositions in the question do not violate any important script  plan or goal information in the narrative. modifications during question answering depend on what information was built in memory during narrative comprehension  and how strongly presuppositions conflict with it. if absolutely everything stated  or inferable  in a narrative were explicitly instantiated  then presupposition checking would always notice new information assumed in a question. however  if memory is largely reconstructed from more general information  then it is difficult for a presupposition checker to distinguish new information from old - as long as the new does not violate anything on purely reconstructive grounds. 
　　there is good evidence that human memory is largely reconstructive. for example. spiro et al.   1  used the following experimental materials in a series of reading experiments:  a  the karate champion hit the block. 
 b  the block broke. 
 c  he had had a fight with his wife earlier. it was impairing his concentration. 
　　the following claims were made about reconstructive memory: 
　　 1  if only  a  and  b  were first presented  then  b  could be reconstructed from  a   so  b  would not be instantiated in memory. therefore  a later presentation of  c  would cause people to falsely assume that  b  had not occurred  and memory of  b  would be degraded. 
　　 1  if  c  were presented before  a  and  b   then  b  could not be easily reconstructed. therefore   b  would be explicitly instantiated  so memory of  b  would not be degraded. 
　　these claims were supported by the experimental data. whenever people could infer the consequence of an antecedent they did not have to instantiate it  since it was reconstructable. this meant that false information about reconstructable events is more likely to be integrated into an existing memory representation. 
1 

1. memory reconstruction in boris 
　　in keeping with the reconstructive claims espoused in spiro et al.   the boris system tries to follow a simple principle: 

　　whenever a knowledge structure is successfully applit in interpreting an input  a memory instantiation is created  or updated   in which the following information is stored: 
　　1. a pointer to the knowledge structure which was activated  if we dont instantiate this  boris wouldn't know what had happened versus what had not happened.  
　　1. any role bindings  so that boris can recall who did what to whom . 
　　1. scenario information  so that an event can be placed in a spatio-temporal relationship with other events  
　　1.  how far  along in the knowledge structure boris has progressed. 
　　1. any higher level goals which this knowledge structure has either achieved  or intends to achieve. 
　　1. any violations or deviations of this knowledge structure. 
　　therefore  any event central to a script  scenario  or goal is instantiated. by definition  violations and deviations can not be reconstructed  so they are instantiated also. for instance  when boris reads that a waitress spills coffee on someone  then the spill-event must be explicitly instantiated both as a violation of 
bring-food 	in 	restaurant 	and 	a 	violation 	of 
do-service in service. for more discussion of violation handling in boris  see  and . 
1. conclusions 
　　in a completely integrated system  both parsing and memory processes require a fresh examination. our experience suggests that one way to proceed is to build fully integrated systems in which a single parsing process invokes all comprehension tasks on an on-going basis. 
　　this article has discussed some consequences arising from unifying parsing for both question answering and narrative comprehension in an integrated system such as boris. two major consequences are: 
  answers to questions may be known before questions have been completely understood. 
  asking questions may modify episodic memory by augmenting it with new information contained within the questions. 
　　these phenomena resemble the type of behavior people exhibit when reading and answering questions about narratives  and deserve further experimentation in both the computational environment and the psychology laboratory. 
acknowledgements 
　　special acknowledgements go both to margot flowers  and to my thesis advisor  wendy lehnert  for their valuable suggestions and edits on earlier drafts of this paper. special  hanks go also to tom wolf  who has been central in the current design and implementation of boris. finally  my appreciation goes to pete johnson. mark burstein  marty korsin. cj yang and steve harley for their past contributions to the boris project. 
