
we present a new architecture for description logic implementations  a range of new optimisation techniques and an empirical analysis of their effectiveness.
1 introduction
description logics  dls  are a family of logic based knowledge representation formalisms. although they have a range of applications  e.g.  configuration  mcguinness& wright  1   and reasoning with database schemas and queries  calvanese et al.  1b; 1a    they are perhaps best known as the basis for widely used ontology languages such as oil  daml+oil and owl  horrocks et al.  1 . as well as dls providing the formal underpinnings for these languages  i.e.  a declarative semantics   dl systems are also used to provide computational services for ontology tools and applications  knublauch et al.  1; rectore  1 .
　most modern dl systems are based on tableaux algorithms. such algorithms were first introduced by schmidtschau  and smolka  schmidt-schau &smolka  1   and subsequently extended to deal with ever more expressive logics  baader et al.  1 . many systems now implement the shiq dl  a tableaux algorithm for which was first presented in  horrocks et al.  1 ; this logic is very expressive  and corresponds closely to the owl ontology language. in spite of the high worst case complexity of the satisfiability/subsumption problem for this logic  exptime-complete   highly optimised implementations have been shown to work well in many realistic  ontology  applications  horrocks  1 .
　optimisation is crucial to the viability of tableaux based systems: in experiments using both artificial test data and application ontologies   relatively  unoptimised systems performed very badly  often being  at least  several orders of magnitude slower than optimised systems; in many cases  hours of processing time  in some cases even hundreds of hours  proved insufficient for unoptimised systems to solve problems that took only a few milliseconds for an optimised system  massacci  1; horrocks&patel-schneider  1 . modern systems typically employ a wide range of optimisations  including  at least  those described in  baader et al.  1; horrocks&patel-schneider  1 .
　tableaux algorithms try to construct a graph  usually a tree  representation of a model of a concept  the structure of which is determined by syntactic decomposition of the concept. most implementations employ a space saving optimisation known as the trace technique that uses a top-down construction requiring  for pspace logics  only polynomial space in order to delineate a tree structure that may be exponential in size  with respect to the size of the input concept . for the exptime logics implemented in modern systems  however  guaranteeing polynomial space usage is no longer an option. moreover  for logics that support inverse roles  such as shiq   a strictly top down approach is no longer possible as constraints may be propagated both  up  and  down  the edges in the tree.
　we describe an alternative architecture for tableaux implementations that uses a  set of  queue s  instead of  an adaption of  the standard top-down approach. this architecture  which we have implemented in our new fact++ system  has a number of advantages when compared to the top-down approach. firstly  it is applicable to a much wider range of logics  including the expressive logics implemented in modern systems  because it makes no assumptions about the structure of the graph  in particular  whether tree shaped or not   or the order in which the graph will be constructed. secondly  it allows for the use of more powerful heuristics that try to improve typical case performance by varying the global order in which different syntactic structures are decomposed; in a top-down construction  such heuristics can only operate on a
　local region of the graph-typically a single vertex.
1 preliminaries
we present here a brief introduction to dl  in particular shiq  syntax  semantics and reasoning; for further details the reader is referred to  baader et al.  1 .
1 description logics
syntax let r be a set of role names with both transitive and normal role names r+“rp = r  where r+”rp =  . the set of shiq-roles  or roles for short  is r“{r  | r （ r}. let nc be a set of concept names. the set of shiqconcepts  or concepts for short  is the smallest set such that every concept name c （ nc is a concept  and if c and d are concepts  r is a role  s is a simple role1 and n （ in  then  c u d    c t d     c     r.c     r.c    1nr.c  and
  nr.c  are also concepts; the last four are called  respectively  value  exists  atmost and atleast restrictions.
　for r and s  possibly inverse  roles  r v s is called a role inclusion axiom  and a finite set of role inclusion axioms is called a role hierarchy. for c and d  possibly complex  concepts  c v d is called a general concept inclusion  gci   and a finite set of gcis is called a tbox.
semantics an interpretation i =   i ，i  consists of a nonempty set  i  the domain of i  and a function ，i which maps every role to a subset of  i 〜  i such that  for p （ r and
r （ r+  hx yi （ pi iff hy xi （ p i  and if hx yi （ ri and hy zi （ ri then hx zi （ ri. the interpretation function ，i of an interpretation maps  additionally  every concept to a subset of  i such that
 c u d i = ci ” di   c t d i = ci “ di 
 ci =  i   ci 
  r.c i = {x （  i | ri x c  1=  } 
 
and
where  m is the cardinality of a set m and ri x c  is defined as {y | hx yi （ ri and y （ ci}.
an interpretation i satisfies a role hierarchy r iff ri   si for each r v s （ r  and it satisfies a tbox t iff ci   di for each c v d （ t ; such an interpretation is called a model of r and t .
a concept c is satisfiable w.r.t. a role hierarchy r and a
tbox t iff there is a model i of r and t with ci 1=  .
such an interpretation is called a model of c w.r.t. r and t . as usual for expressive dls  subsumption can be reduced to satisfiability  and reasoning w.r.t. a tbox and role hierarchy can be reduced to reasoning w.r.t. a role hierarchy only  horrocks et al.  1 .
1 tableaux algorithms
the basic idea behind a tableau algorithm is to take an input concept c and role hierarchy r  and to try to prove the satisfiability of c w.r.t. r by constructing a model i of c
w.r.t. r. this is done by syntactically decomposing c so as to derive constraints on the structure of such a model. for example  any model of c must  by definition  contain some individual x such that x is an element of ci  and if c is of the form  r.d  then the model must also contain an individual y such that hx yi （ ri and y is an element of di; if d
is non-atomic  then continuing with the decomposition of d would lead to additional constraints. the construction fails if the constraints include a clash  an obvious contradiction   e.g.  if some individual z must be an element of both c and  c for some concept c. algorithms are normally designed so that they are guaranteed to terminate  and guaranteed to construct a model if one exists; such an algorithm is clearly a decision procedure for concept satisfiability.
　in practice  algorithms often work on a tree shaped graph that has a close correspondence to a model; this may be because  e.g.  models could be non-finite  although obviously finitely representable   or non-trees  although usually treelike . typically this will be a labelled graph  usually a tree or collection of trees  where nodes represent individuals in the model  and are labelled with a set of concepts of which they are instances  and edges represent role relationships between pairs of individuals  and are labelled with a set of role names.
　the decomposition and construction is usually carried out by applying so called tableaux expansion rules to the concepts in node labels  with one rule being defined for each of the syntactic constructs in the language  with the exception of negation  which is pushed inwards using re-writings such as de morgan's laws  until it applies only to atomic concepts . for example  the expansion rule for conjunction causes c and d to be added to any node label already containing c ud  in order to guarantee termination  side conditions prevent rules from being applied if they do not change either the graph or its labelling .
　there are two forms of non-determinism in the expansion procedure. in the first place  many rules may be simultaneously applicable  and some order of rule applications must be chosen. from a correctness perspective  this choice is usually irrelevant1  because  if there is a model  then it will be found by any expansion ordering   but as we will see later  the order of expansion can have a big effect on efficiency. in the second place  some rules expand the graph non-deterministically; e.g.  the expansion rule for disjunction causes either c or d to be added to any node label already containing c t d. from a correctness perspective  this choice is relevant  because one choice may lead to the successful construction of a model  while another one does not   and is usually dealt with by backtracking search. although such search must  in the worst case  consider all possible expansions  the order in which they are considered can still have a big effect on efficiency.
　two kinds of rule will be of particular interest in the following discussion: non-deterministic rules  such as the t-rule mentioned above  and generating rules  such as the  -rule  that add new nodes to the graph. applying these rules is likely to be more  costly   as they either increase the size of the graph or increase the size of the search space  and they are typically applied with lower priority than other rules.
1 fact++ system architecture
as discussed above  many implementations use a top-down expansion based on the trace technique. the idea of the topdown expansion is to apply the  -rule with the lowest priority  i.e.  only apply this rule when no other rule is applicable ; the added refinement of the trace technique is to discard fully expanded sub-trees  so that only a single  trace   i.e.  a branch of the tree  is kept in memory at any one time.
　this technique has the advantage of being very simple and easy to implement-a procedure that exhaustively expands a node label can be applied to the current node and then  recursively  to each of its successors. it does  however  have some serious drawbacks. in the first place  for logics with inverse roles  the top-down method simply breaks down as it relies on the fact that rules only ever add concepts to the label of the node to which they are applied or to the label of one of its successor nodes. the result is that  once the rules have been exhaustively applied to a given node label  no further expansion of that label will be possible. in the presence of inverse roles  expansion rules may also add concepts to the labels of predecessor nodes  which could then require further expansion. moreover  discarding fully expanded sub-trees may no longer be possible  as the expansion of a concept added to the label of a predecessor may cause concepts to be added to the label of a sibling node that had previously been fully expanded.
　in the second place  the top down method forces nondeterministic rules to be applied with a higher priority than generating rules. as the size of the search space caused by non-deterministic rule expansions is  in practice  by far the most serious problem for tableaux based systems  horrocks  1   it may be advantageous to apply nondeterministic rules with the lowest priority  giunchiglia& sebastiani  1 . in fact  top-down implementations typically apply non-deterministic rules with a priority that is lower than that of all of the other rules except the generating rules  horrocks&patel-schneider  1 .
todo list architecture the fact++ system was designed with the intention of implementing dls that include inverse roles  and of investigating new optimisation techniques  including new ordering heuristics. currently  fact++ implements shif  a slightly less expressive variant of shiq where the values in atleast and atmost restrictions can only be zero or one.1
　instead of the top-down approach  fact++ uses a todo list to control the application of the expansion rules. the basic idea behind this approach is that rules may become applicable whenever a concept is added to a node label. when this happens  a note of the node/concept pair is added to the todo list. the todo list sorts all entries according to some order  and gives access to the  first  element in the list.
　a given tableaux algorithm takes an entry from the todo list and processes it according to the expansion rule s  relevant to the entry  if any . during the expansion process  new concepts may be added to node labels  and hence entries may be added to the todo list. the process continues until either a clash occurs or the todo list become empty.
　in fact++ we implement the todo list architecture as a set of queues  fifo buffers . it is possible to set a priority for each rule type  e.g.  u and     and a separate queue is implemented for each unique priority. whenever the expansion algorithm asks for a new entry  it is taken from the non-empty queue with the highest priority  and the algorithm terminates when all the queues are empty. this means that if the  -rule has a low priority  say 1   and all other rules have the same priority  say 1   then the expansion will be  modulo inverse roles  top-down and breadth first; if stacks  lifo buffers  were used instead of queues with the same priorities  then the expansion would simulate the standard top-down method.
1 heuristics
when implementing reasoning algorithms  heuristics can be used to try to find a  good  order in which to apply inference rules  we will call these rule-ordering heuristics  and  for non-deterministic rules  the order in which to explore the different expansion choices offered by rule applications  we will call these expansion-ordering heuristics . the aim is to choose an order that leads rapidly to the discovery of a model  in case the input is satisfiable  or to a proof that no model exists  in case the input is unsatisfiable . the usual technique is to compute a weighting for each available option  and to choose the option with the highest  or lowest  weight. much of the  art  in devising useful heuristics is in finding a suitable compromise between the cost of computing the weightings and their accuracy in predicting good orderings.
　such heuristics can be very effective in improving the performance of propositional satisfiability  sat  reasoners  freeman  1   but finding useful heuristics for description and modal logics has proved to be more difficult. choosing a good heuristic  or at least not choosing a bad one  is very important: an inappropriate heuristic may not simply fail to improve performance  it may seriously degrade it. even more problematical is  given a range of possible heuristics  choosing the best one to use for a given  type of  problem.
　so far  the heuristics tried with dl reasoners have mainly been adaptions of those already developed for sat reasoners  such as the well known moms heuristic  freeman  1  and jeroslow and wang's weighted occurrences heuristic  jeroslow&wang  1 . these proved to be largely ineffective  and even to degrade performance due to an adverse interaction with backjumping  baader et al.  1 . an alternative heuristic  first presented in  horrocks  1   tries to maximise the effect of dependency directed backtracking  backjumping  by preferentially choosing expansions that introduce concept with  old  dependencies. even this heuristic  however  has relatively little effect on performance with realistic problems  e.g.  problems encountered when reasoning with application ontologies.
　we conjecture that the standard top-down architecture has contributed to the difficulty in finding useful heuristics as it rules out many possible choices of rule-ordering; in particular  the top-down technique may require generating rules to be applied with a low priority  and so lead to non-deterministic rules being applied before deterministic generating rules. in contrast  the todo list architecture gives a much wider range of possible rule orderings  and so has allowed us to investigate a range of new rule-ordering heuristics  in particular heuristics that give non-deterministic rules the lowest priority.
　another factor that has contributed to the weakness of sat derived heuristics is that they treat concepts as though they were atoms. this is obviously appropriate in the case of propositional satisfiability  but not in the case of concept satisfiability where sub-concepts may have a complex structure. we have also investigated expansion-ordering heuristics that take into account this structure  in particular a concept's size  maximum quantifier depth  and frequency of usage in the knowledge base.
implementation in fact++ the fact++ reasoner uses the standard backtracking search technique to explore the different possible expansions offered by non-deterministic rules  such as the t-rule . before applying a non-deterministic rule  the current state is saved  and when backtracking  the state is restored before re-applying the same rule  with a different expansion choice . when inverse roles are supported  it is possible for a sequence of deterministic rule applications to propagate changes throughout the graph  and it may  therefore  be necessary to save and restore the whole graph structure  in addition to other data structures such as the todo list . fact++ trys to minimise the potentially high cost of these operations by lazily saving the graph   i.e.  saving parts of the graph only as necessitated by the expansion   but the cost of saving the state still makes it expensive to apply a non-deterministic rule  even if the state is never restored during backtracking.
　as discussed in section 1  fact++ uses a todo list architecture with separate queues for each priority level. different rule-ordering heuristics can  therefore  be tried simply by varying the priorities assigned to different rule types. low priorities are typically given to generating and nondeterministic rules  but the todo list architecture allows different priority ordering of these rule types; in contrast  the top-down architecture forces a lower priority to be given to generating rules.
　fact++ also includes a range of different expansionordering heuristics that can be used to choose the order in which to explore the different expansion choices offered by the non-deterministic t-rule. this ordering can be on the basis of the size  maximum quantifier depth  or frequency of usage of each of the concepts in the disjunction  and the order can be either ascending  smallest size  minimum depth and lowest frequency first  or descending. in order to avoid the cost of repeatedly computing such values  fact++ gathers all the relevant statistics for each concept as the knowledge base is loaded  and caches them for later use.
1 empirical analysis
in order to evaluate the usefulness of the heuristics implemented in fact++  we have carried out an empirical analysis using both real-life ontologies and artificial tests from the dl'1 test suite  horrocks&patel-schneider  1 .
　ontologies can vary widely in terms of size and complexity  e.g.  structure of concepts  and types of axiom used . we used three ontologies with different characteristics in order to see how the heuristics would perform in each case:
winefood a sample ontology that makes up part of the
owl test suit1  carroll&de roo  1 ; it is small  but has a complex structure and includes 1 gcis.
dolce a foundational  top-level  ontology  developed in the wonderweb project  gangemi et al.  1 ; it is of medium size and medium complexity.
galen the anatomical part of the well-known medical terminology ontology  rogers et al.  1 ; it is large  1 concepts  and has a relatively simple structure  but includes over 1 gcis.
　fact++ separates the classification process into satisfiability testing  sat  and subsumption testing  sub  phases; the results from the sat phase are cached and used to speed up subsequent tests via a standard  model-merging  optimisation  horrocks&patel-schneider  1 . fact++ allows different heuristics to be used in the two phases of the process; this is because the tests have different characteristics: in the sat phase  nearly all of the tests are satisfiable  ontologies typically do not give names to unsatisfiable concepts   while in the sub phase  up to one in four of the tests are unsatisfiable. we measured the time  in cpu seconds  taken by fact++ to complete each phase.
　in addition to the ontologies  we used artificially generated test data from the dl'1 test suite. artificial tests are in some sense corner cases for a dl reasoner designed primarily for ontology reasoning  and these tests are mainly intended to investigate the effect of hard problems with very artificial structures on the behaviour of our heuristics. for this purpose we selected from the test suite several of the tests that proved to be hard for fact++.
　each of these tests consists of a set of 1 satisfiability testing problems of similar structure  but  supposedly exponentially  increasing difficulty; the idea of the test is to determine the number of the largest problem that can be solved within a fixed amount of processing time  1 seconds of cpu time in our case . the names of the tests are of the form  testp  or  testn   where  test  refers to the kind of problem  e.g.  the  ph  tests are derived from encodings of pigeon hole sorting problems   and  p/n  refers to whether the problems in the test set are satisfiable  n  or unsatisfiable  p . for these tests we have reported the number of the largest problem solved in less than 1 seconds  1 means that all the problems were solved   along with the time  in cpu seconds  taken for the hardest problem that was successfully solved.
　for all the tests  fact++ v.1.1 was used on pentium 1.1 ghz machine with 1mb of memory  running linux. times were averaged over 1 test runs.
1 rule-ordering heuristics
in these tests we tried a range of different rule-ordering strategies. each  strategy  is shown as a sequence of letters specifying the priorities  highest first  of the different rule types  where  o  refers to the t-rule   e  to the  -rule  and  a  to any other rule type. e.g.   ao  describes the strategy where the t-rule has the lowest priority  and all other rules have an equal higher priority.
ontology tests the results of using different rule-ordering strategies with the various ontologies are shown in table 1. all ontologies were tested with the best disjunction-ordering heuristic  as determined in separate tests  see below .
kbdolcewinefoodgalensatsubsatsubsatsuba111111ao111111aeo111111ae111111aoe111111table 1: ontology tests with different rule-orderings
　the first thing to note is that rule-orderings have relatively little effect on the dolce and winefood ontologies; in contrast  the performance of the best and worst strategies differs by a factor of almost 1 in the galen tests. even in the galen case  however  the difference between the  -o  strategies  i.e.  those that assign the lowest priority to the trule  and  -e  strategies  i.e.  those that assign the lowest priority to the  -rule  is relatively small. in most cases the best result is given by the  aeo  strategy  i.e.  by assigning the lowest priority to the t-rule and the next lowest priority to the  -rule  and even when  aeo  is not the best strategy  the difference between it and the best strategy is very small. moreover  the difference between the  aeo  and  aoe  strategies is small in most cases  and never more than a factor of 1.
dl1 tests the results of using different rule-ordering strategies with the dl1 tests are shown in table 1. the first thing to note from these results is that rule-ordering heuristics can have a much more significant effect than in the ontology tests: in some cases the performance of the best and worst strategies differs by a factor of more than 1. in most tests  the  -e  strategies give the best results  with the difference between  -o  and  -e  strategies being much more marked than in the case of the ontology tests. in the case of the d1n test  however  performance is dramatically improved  by a factor of 1  when an  -o  strategy is used.
testbrnbrpd1nphnphplasttimelasttimelasttimelasttimelasttimea1.111.111.1ao1.111.111.1aeo1.111.111.1ae1.111.111.1aoe1.111.111.1table 1: dl-1 tests with different rule-ordering strategies
1 expansion-ordering heuristics
in these tests we tried a range of different expansion-ordering heuristics. each heuristic is denoted by two letters  the first of which indicates whether the ordering is based on concept size   s    maximum depth   d   or frequency of usage   f    and the second of which indicates ascending   a   or descending   d   order. in each group of tests we used the best ruleordering heuristic as determined by the tests in section 1.
ontology tests for the ontology tests  we tried different orderings for the sat and sub phases of classification. the results are presented in tables 1  1 and 1; the first figure in each column is the time taken by the sat phase using the given ordering  and the remaining figures are the subsequent times taken using different sub phase orderings.
　for dolce  table 1   the difference between the best and worst orderings was a factor of about 1  and many possible orderings were near optimal. for winefood  table 1   the difference between the best and worst orderings was a factor of about 1  and using sd for sat tests and dd for sub tests gave the best result  although several other orderings gave similar results. for galen  table 1   the difference between the best and worst orderings was so large that we were only the orderings given allowed tests to be completed in a reasonable time. the best result was given by using da for both phases.
satsadafasdddfdsub111111sa111111da111111fa111111sd111111dd111111fd111111table 1: dolce test with different expansion-orderings
satsadafasdddfdsub111111sa111111da111111fa111111sd111111dd111111fd111111table 1: winefood test with different expansion-orderings
satsadasub11sa11da11fd11table 1: galen test with different expansion-orderings
dl1 tests table 1 presents the results for the dl1 tests. each column shows the times taken using different expansion orderings to solve the hardest problem that was solvable within the stipulated time limit using any ordering.
　in almost every test  the difference between the best and worst strategies is large: a factor of more than 1 in the d1n test. moreover  strategies that are good in one test can be very bad in another  the sd and dd strategies are the best ones in the branch tests  brn and brp   but  by far  the worst in the d1n test   and this is not strongly dependent on the satisfiability result  in the br tests  all strategies perform similarly in both satisfiable and unsatisfiable cases . the fd strategy is  however  either optimal or near optimal in all cases.
orderbrnbrpd1nphnphptest 1test 1test 1test 1test 1sa11111da111 1.1fa 1 1.1.1.1sd11 1.1.1dd11 1.1.1fd11111table 1: dl1 tests with different or strategies
1 analysis
the different rule-ordering heuristics we tried had relatively little effect on the performance of the reasoner when classifying the dolce and winefood ontologies. with the galen ontology  any strategy that gave a lower priority to the  - and t-rules worked reasonably well  and the aeo strategy was optimal or near-optimal in all cases. the crucial factor with galen is giving low priority to the  -rule. this is due to the fact that galen is large  contains many gcis and also contains existential cycles in concept inclusion axioms  e.g.  c v  r.d and d v  r .c ; as a result  the graph can grow very large  and this increases both the size of the search space  because gci related non-determinism may apply on a pernode basis  and the cost of saving and restoring the state during backtracking search. giving a low priority to the  -rule minimises the size of the graph and hence can reduce both the size of the search space and the cost of saving and restoring. this effect is less noticeable with the other ontologies because their smaller size and/or lower number of gcis greatly reduces the maximum size of graphs and/or search space. in view of these results  fact++'s default rule-ordering strategy has been set to aeo.1
　the picture is quite different in the case of the dl'1 tests. here  different strategies can make a large difference  and no one strategy is universally near optimal. this is to be expected  given that some of the tests include very little nondeterminism  but are designed to force the construction of very large models  and hence graphs   while others are highly non-deterministic  but have only very small models. given that these extreme cases are not representative of typical reallife ontologies  the test results may not be directly relevant to a system designed to deal with such ontologies. it is interesting  however  to see how badly the heuristics can behave in such cases: in fact the standard aeo strategy is near optimal in two of the tests  and is never worse than the optimal strategy by a factor of more than 1.
　the expansion-ordering heuristics had a much bigger effect on ontology reasoning performance  than the ruleordering heuristics . in the case of dolce and winefood  almost any strategy that uses sd or dd in the sub phase is near optimal. for galen  however  using da in both phases gives by far the best results. this is again due to the characteristic structure of this ontology  and the fact that preferentially choosing concepts with low modal depth tends to reduce the size of the graph. unfortunately  no one strategy is universally good  da/da is best for galen but worst for dolce and winefood ; currently  sd/dd is the default setting  as the majority of real life ontologies resemble dolce and winefood more than galen   but this can of course be changed by the user if it is known that the ontology to be reasoned with will have a galen-like structure.
　for the dl'1 tests  the picture is again quite confused: the sd strategy  the default in the sat phase  is optimal in some tests  but bad in others-disastrously so in the case of the d1n test. as in the ontology case  the only  solution  offered at present is to allow users to tune these settings according to the problem type or empirical results.
1 discussion and future work
we have described the todo list architecture used in the fact++ system along with a range of heuristics that can be used for rule and expansion ordering. we have also presented an empirical analysis of these heuristics and shown how these have led us to select the default setting currently used by fact++.
　these default settings reflect the current predominance of relatively small and simply structured ontologies. this may not  however  be a realistic picture of the kinds of ontology that we can expect in the future: many existing ontologies  including  e.g.  winefood  pre-date the development of owl  and have been translated from less expressive formalisms. with more widespread use of owl  and the increasing availability of sophisticated ontology development tools  it may be reasonable to expect the emergence of larger and more complex ontologies. as we have seen in section 1  heuristics can be very effective in helping us to deal efficiently with such ontologies  but choosing a suitable heuristic becomes of critical importance.
　in our existing implementation  changing heuristics requires the user to set the appropriate parameters when using the reasoner. this is clearly undesirable at best  and unrealistic for non-expert users. we are  therefore  working on techniques that will allow us to guess the most appropriate heuristics for a given ontology. the idea is to make an initial guess based on an analysis of the syntactic structure of the ontology  it should be quite easy to distinguish galen-like ontologies from dolce and winefood-like ontologies simply by examining the statistics that have already been gathered for use in expansion-ordering heuristics   with subsequent adjustments being made based on the behaviour of the algorithm  e.g.  the size of graphs being constructed .
　another limitation of the existing implementation is that a single strategy is used for all the tests performed in the classification process. in practice  the characteristics of different tests  e.g.  w.r.t. concept size and/or satisfiability  may vary considerable  and it may make sense to dynamically switch heuristics depending on the kind of test being performed. this again depends on having an effective  and cheap  method for analysing the likely characteristics of a given test  and syntactic and behavioural analyses will also be investigated in this context.
