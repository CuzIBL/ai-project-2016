 
many inductive knowledge acquisition algorithms generate classifiers in the form of decision trees. this paper describes a technique for transforming such trees to small sets of production rules  a common formalism for expressing knowledge in expert systems. the method makes use of the training set of cases from which the decision tree was generated  first to generalize and assess the reliability of individual rules extracted from the tree  and subsequently to refine the collection of rules as a whole. the final set of production rules is usually both simpler than the decision tree from which it was obtained  and more accurate when classifying unseen cases. transformation to production rules also provides a way of combining different decision trees for the same classification domain. 
introduction 
a decision tree is a simple recursive structure for expressing a sequential classification process in which a case  described by a set of attributes  is assigned to one of a disjoint set of classes. each leaf of the tree denotes a class. an interior node denotes a 
test on one or more of the attributes with a subsidiary decision tree for each possible outcome of the test. to classify a case we start at the root of the tree. if this is a leaf  the case is assigned to the nominated class; if it is a test  the outcome for this case is determined and the process continued with the subsidiary tree appropriate to that outcome. 
figure 1 shows a non-trivial decision tree for one aspect of the diagnosis of thyroid disease  quinlan  compton  horn and lazarus  1 . to simplify printing  the tree has been turned on its side. leaves are shown in bold font  and the possible outcomes at an interior node are represented by logical expressions with equal indentation. the interpretation of the attributes and decision 
* permanent address: school of computing sciences  new south wales institute of technology  sydney 1  australia. ** much of the research described here was carried out at the artificial intelligence laboratory  massachusetts institute of technology  whose artificial intelligence research is supported in part by the advanced research projects agency of the department of defense under office of naval research contract n1k-1. this research has also received support from the australian research grants scheme and the westinghouse electric corporation. 
1 	knowledge acquisition 
classes is not important here  but notice that the root of this tree is a test on attribute t1. when classifying a case  we will be directed to the subtree starting with fti   1 or that headed fti   1 depending on whether the value of tz is less than  or greater than or equal to  1. 
research that commenced in the late 1s with hunt's concept learning system  hunt  marin and stone  1  has led to several reliable methods for developing decision trees from training sets of cases with known classes. modern systems of this type  such as those described in  breiman  friedman  olshen and stone  1; kononenko  bratko and roskar  1; quinlan  1  can deal effectively with large training sets affected by noise and incompleteness  and can classify new cases even when the outcome of crucial tests is unknown. 
the starting point for this paper is a decision tree developed by some means from a training set of cases. we examine methods for re-expressing the decision tree as a succinct collection of production rules of the form 
if left-hand side then class  certainty factor  
there are three reasons for such a transformation. first  production rules are a widely-used and well-understood vehicle for representing knowledge in expert systems  winston  1 . secondly  a decision tree such as that in figure 1 can be difficult for a human expert to understand and modify  whereas the extreme modularity of production rules makes them relatively transparent. finally  and most importantly  this transformation can improve classification performance by eliminating tests in the decision tree attributable to peculiarities of the training set  and by making it possible to combine different decision trees for the same task. 
the transformation takes place in two stages addressed in the following sections. individual rules are first developed from the decision tree  and the collection of rules so derived is then processed as an entity to yield the final ruleset. 
extracting individual rules 
recall that classifying a case using a decision tree is effected by following a path through the tree to one of the leaves. this path from the root of the tree to a leaf establishes conditions  in terms of specified outcomes for the tests along the path  that must be 


satisfied by any case classified by that leaf. for example  any case that is classified as negative by the asterisked leaf near the middle of figure 1 must satisfy all the conditions 

every leaf of a decision tree thus corresponds to a primitive production rule of the form 
then class c 
where the xi's are conditions and c is the class of the leaf. 
at this point we make use of the training set t of cases from which the decision tree was generated in order to improve this prototype rule. let xi be one of these conditions and let s c t be the set of cases that satisfy all the other conditions in the lefthand side of our rule. with respect only to 1  the relevance of xi to determining whether a case belongs to class c  given that the other conditions are satisfied  can be summarized by the 1 contingency table 

where se is the number of these cases that satisfy xt and belong to class e  se is the number that satisfy xi but belong to some class other than c  and so on. 
note that sc-f sc is the number of cases in the training set t that satisfy the entire left-hand side of the rule and that ac of them belong to the class nominated by the rule. these two numbers provide a means of estimating the accuracy or certainty factor of the rule. the obvious choice of setting 

can be rather optimistic  especially when the numbers are small. since for any reasonable rule sc will be larger than ac  the use of yates' correction for continuity  snedecor and cochran  1  p1  gives a more reasonable estimate as 

there are at least two sets of circumstances under which this condition xi should be deleted from the left-hand side of the rule. the first typically arises with disjunctive concepts  bundy  silver and plummer  1  in which a case belongs to a particular class whenever a disjunctive logical expression of the form y v z is satisfied. a decision tree for such a classification task might commence with a test that is relevant to y but not to z  so the leaves associated with the disjunct z will generate prototype rules that contain irrelevant conditions. if xi is such a condition  eliminating it will produce a more general rule without any decrease in accuracy  i.e. 
secondly  the presence of xi in the left-hand side of the rule may give greater apparent accuracy  but this accuracy may derive from chance characteristics of the training set that cannot be expected to hold for unseen cases. the algorithm used to construct the decision tree from the training set t has probably attempted to 'fit' the data  even when it is noisy or inconclusive. under these circumstances  retaining xi can be dangerous because the seeming reliability of the rule can lend false confidence to a classification. there are several statistical tests that can be used to signal this state of affairs. following a suggestion of donald michie  i use fisher's exact test  finney  latscha  bennett and hsu  1  to determine the significance level at which we can reject the hypothesis that xi is irrelevant to whether a case satisfying all the other conditions belongs to class c. if this level is not very small  the condition xi is deleted. 
	quinlan 	1 

the algorithm for dropping conditions from the left-hand tide of a rule can now be stated succinctly. condition xi is a candidate for elimination either if its removal will not decrease the certainty factor of the rule  or if the hypothesis that xi is irrelevant cannot be rejected at the 1% level or better. so long as there are candidates for elimination  we discard the one whose removal has the least detrimental effect on the accuracy of the rule  and continue. of course  after any xi has been removed  the contingency tables for the remaining conditions must be recalculated. 
as an illustration of the process  consider the rule above  extracted from figure 1 which was in turn generated from a training set of 1 cases. we focus first on the condition tt1   1. the contingency table over all cases satisfying the remaining conditions is 

so that removing this condition will increase the value of the certainty factor. the same holds for the condition fti   1. in the reduced rule the contingency table for the condition tsh   
1 is 

even though the rule without this condition is apparently less accurate  the condition is removed because the hypothesis that it is irrelevant to whether a case in 1 is class negative can only be rejected at the 1% level. the remaining condition is significant at better than the 1% level  so the final rule from this path becomes if t1   1 then class negative  1%  
the number of rules generated in this way is almost always smaller than the number of leaves in the decision tree. some paths generate no rules  either because all conditions are eliminated or because the rule replicates another from a different path. in this example  although the decision tree of figure 1 has 1 leaves  the process above produces just 1 rules. 
 aside: the reader may wonder why we use the decision tree at all  instead of developing rules directly from the training set of cases. working from the tree has two major advantages. most interesting classification tasks involve attributes with continuous values which must be formed into tests by the development of appropriate threshholds  e.g. tz   1 from before . the divide-and-conquer approach commonly employed by algorithms for constructing decision trees provides a powerful and contextsensitive means of coping with this otherwise complex problem. secondly  even a long path in a decision tree typically involves only a small proportion of the possible attributes. the training set of figure 1 uses 1 attributes to describe each case  but no path in the decision tree uses more than nine tests; the space of potential rules is thus shrunk from 1  to 1  with a corresponding reduction in computational load.  
1 	knowledge acquisition 
processing collections of rules 
having reduced the given decision tree to a set of plausible rules  we might judge the transformation task to have been accomplished. it seems relevant to wonder  though  how well the rules classify unseen cases  and whether some subset of the rules might be as useful as the whole set. these questions presume that there is some target production rule interpreter in the wings. the following uses an extremely simple interpreter: 
to classify a case  find a rule that applies to it. if there is more than one  choose the rule with the higher certainty factor. if no rule applies  take the class by default to be the most frequent class in the training set. 
alternative and equally sensible interpreters  e.g. those that choose the most specialized applicable rule  should produce similar results. 
let r be the set of production rules and t the training set of cases from which the decision tree was generated. we would like to find that subset of r which misclassified the fewest cases in t but  by analogy with the set-covering task  this is an np problem. instead  a heuristic algorithm is used to find a  good  subset by successively discarding single rules. 
for any case in t and any single rule r  we look at the class to which this case would be assigned by the entire set r and the reduced set r - {r}. the advantage of r is the number of cases in t for which the correct class is given by r but not by r - {r}  less the number of cases vice versa. if the advantage of r is negative or zero  removing r from the set of production rules will not increase the number of cases in t that are misclassified. this suggests a straightforward procedure: at each step  delete from r the rule with least advantage  so long as this advantage is less than or equal to zero. the set of rules remaining at the end of this process is locally optimal to the extent that deleting any further rule will increase the number of misclassifications over t.  this may overlook  however  situations in which deleting a subset of the rules would improve performance.  the procedure usually finds a good subset of r  but is weak when the initial set of rules contains many pairs of very similar rules: in this situation  most rules will have advantage 1 and so advantage is a poor basis on which to choose the rule to delete. 
we saw previously that the decision tree of figure 1 with 1 leaves gave rise to 1 production rules. the winnowing process described above reduces this set to just four rules with an average of 1 conditions per rule. 
accuracy 
we now turn to the classification accuracy of the reduced set of rules. since each rule was formed by eliminating conditions from a path in the tree  it tends to be over-generalized with respect to the training set. however  the relevant test of any classification mechanism is its performance on unseen cases. 


when presented with 1 unseen cases  the decision tree of figure 1 misclassifies 1 of them as compared to 1 errors from the final set of four production rules above. results from other experiments reported in detail in  quinlan  1  are summarized in table 1. in each of six domains  ten decision trees were generated from a training set and their performance measured on unseen cases. each decision tree was then transformed to a set of production rules whose accuracy was assessed on the same unseen cases. the average sizes and error rates for each domain shown in table 1 bring out the point that the production rules are generally much simpler and sometimes more accurate than the decision tree from which they were generated. 
another advantage of transforming decision trees to production rules is their resulting modularity. there is no obvious way to combine two decision trees for the same classification task so as to generate a super-tree that is more accurate than either of its parents. if each decision tree is converted to a set of rules  though  a composite reduced set can be produced simply by merging rules from all trees before applying the final winnowing process outlined above. this approach has been found to give encouraging results. for example  ten decision trees derived from the same training set as the tree of figure 1  when combined in this way  yield a set of five production rules that correctly classify all but 1 of the 1 unseen cases  even though the best of the trees gives 1 errors on these same cases. 
conclusion 
the conclusion of this work is that it is possible to re-express complex decision trees as small sets of production rules that outperform the original trees when asked to classify unseen cases. the methods outlined here also provide a way to merge different decision trees for the same task  thereby obtaining another increase in accuracy. 
this method for reducing the number of rules can be contrasted with the trunc algorithm employed in aq1  michalski  mozetic hong and lavrac  1 . the analog of a rule in that system is a complex or conjunction of conditions associated with a class. unlike rules  complexes are exact in the sense that any case in the training set satisfying all the conditions is guaranteed to belong to the designated class. at each iteration  trunc discards the complex satisfied by the fewest cases in the training set until some stopping criterion is met. aq1 uses a powerful form of partial or analogical matching to allow a case which satisfies no complex to be deemed to match the most similar complex. as a result  even though deleting a complex cannot decrease the number of misclassified training cases  it may not necessarily cause an increase in this number. interestingly  michalski et al also report that removal of little-used complexes has been found to lead to improved classification performance on unseen cases. 
although the algorithms presented here work well  they should be capable of further improvement. both the condition-dropping and rule-dropping processes use a hill-climbing approach which can often get stuck on a local optimum. more sophisticated search strategies should generate better individual rules and better rule sets at the cost of some increase in computation. 
acknowledgements 
i thank many colleagues for comments and suggestions  particularly wray buntine  jason catlett  bill leech  john mcdermott  donald michie and ron rivest. i am grateful to the garvan institute of medical research  sydney  for providing access to the thyroid data. 
