 
   learning systems which engage in sequential activity face the problem of properly allocating credit to steps or actions which make possible later steps that result in environmental payoff. in the classifier systems studied by holland and others  credit is allocated by means of a  bucketbrigade  algorithm through which  over time  environ-
mental payoff in effect bows back to classifiers which take early  stage-setting actions. the algorithm has advantages of simplicity and locality  but may not adequately reinforce long action sequences. we suggest an alternative form for the algorithm and the system's operating principles designed to induce behavioral hierarchies in which modularity of the hierarchy would keep all bucket-brigade chains short  thus more reinforceable and more rapidly learned  but overall action sequences could be long. 
	i 	introduction 
   many learning systems face the problem of temporal credit allocation: the proper reinforcement of activities which do not directly result in need satisfaction or external reward but are nevertheless essential precursors to such outcomes. animals learn extensive hunting  stalking  or foraging behaviors aimed at the ultimate payoff of something to eat. a person who values others' cooperation must discover and reinforce effective precursor strategies. in the message-passing  rule-based classifier systems  holland  1   credit is allocated by means of a 
 bucket-brigade  algorithm to earlier-acting rules which  set the stage  for later actions that bring external payoff. the essential idea is that classifiers which match messages and become active on a given time step  pay  a fraction e of their  strengths  to the strengths of the classifiers which posted the messages and were active on the previous time step. when finally external payoff enters the system  it is added to the strengths of the then currently active classifiers. if over time a given payoff-achieving sequence gets repeated  strength increments will in effect flow back to reinforce its early-acting classifiers. consequently  early-acting classifiers that indeed participate in sequences that make possible later payoff will  by the algorithm  receive due credit. 
   in certain other ai systems which learn to perform multiple-step tasks  e.g.  mitchell's lex system for symbolic integration  mitchell  utgoff  and banerji  1   and the act  cognitive architecture of anderson  1    credit is assigned to early steps by keeping and analysing a record of all pre-payoff actions  both considered and taken  and the associated reasoning. in contrast  holland's bucket brigade technique does not depend on retrospective analysis but operates locally  during performance  in the strength transaction between steps  with the better classifiers at each step being selected statistically over time. the bucket-brigade principle would consequently appear appropriate for systems such as animals and autonomous robots in on-going interaction with uncertain environments-where storage and analysis of raw experience is expensive or impractical. 
   in this paper  however  we suggest that the bucketbrigade may lose effectiveness as action sequences grow long. as a remedy  we propose a modification of the algorithm that makes it more directly reflect the hierarchical nature of behavior. 
	ii 	long chains 
   the mechanics of the bucket brigade suggest that a classifier whose early action indeed contributes to later payoff may still have difficulty getting reinforced if the number of message-posting cycles from its activation to payoff is large. as a simple example  suppose that classifier c posts a message which  by triggering an effector  causes an action  e.g.  application of hand pressure to a restaurant door  that leads eventually  n time-steps  message-postings  later to payoff in the form of satisfaction at the taste of food. tv will at least equal the number of intervening elementary actions  which may be very large. if over time c is to be properly reinforced as a member of the sequence  the sequence will have to be repeated as many times as it takes the strength increment due to the food payoff to  reach  c. 
   to estimate the number of repetitions required  we used a simple simulation in which a list of n strengths represented a bucket-brigade chain of n classifiers. setting the strengths initially at zero  we provided external payoff r at one end and ran the chain repeatedly  according to the bucket-brigade rule of the previous section  until the nth strength reached 1% of r/e  where  as can be shown  r/e is the assymptotic value approached by all strengths in the chain. the number of repetitions required  m1%  was  to a close approximation  
m1% =  1+ 1n /c  
for values of c in the range from 1 to 1. e should be kept small so that classifier strengths average over a 
	wilson 	1 
number of payoff events; typically  c is chosen to be no greater than 1. given that value  our equation says that a  stage-setting  classifier just 1 steps from environmental payoff will require no fewer than 1 repetitions of the sequence to be properly reinforced. 
	hi 	behavioral modules 
   clearly  something is wrong if the reinforcement algorithm must feed strength increments back through the enormous number of elementary steps between  say  the push on a restaurant door and the enjoyment of food. intuitively  that sequence consists of just a few big steps:   enter restaurant     get a table    get food    eat . if the algorithm treated these as the bucket-brigade units  reinforcement would be faster since the chain would be short. somehow we must also reinforce the smaller steps which compose the big ones. but we note that  enter restaurant  can be broken into the sequence:  find door    open door    go through   and that  open door  expands  in turn  into a short sequence one of whose components is  push . albus  1   among others  shows how any complex activity can be decomposed into a hierarchy of behavioral modules each consisting of just a few  steps .. if the bucket brigade could apply hierarchically to module steps  we might be able to reinforce quite extended activities without encountering the  long chain  problem. 
   our approach to this objective is to modify the standard classifier system's performance and reinforcement algorithms so as explicitly to encourage behavioral modules and short bucket-brigade chains  see holland  1  for a different suggested approach . the basic change is to use a hierarchical message list instead of the standard homogeneous one in which all messages have equal status  and  for the moment  to allow at most one message at a time on a given level. our plan of exposition is first to take the reader through an example  then to present the new algorithm  and finally to discuss questions which the algorithm raises. 
	iv 	an example 
   figure 1 illustrates the operation of the hierarchical performance and reinforcement algorithm over a certain interval of 1 time-steps. the figure shows principally the contents of the system's message list  but also indicates environment changes  actions  and bucket-brigade flows. 
   at time t1 we imagine that the message m1 spontaneously appears on a previously empty message list. m1 is special in that it represents an internal system need  e.g.   get food   in which case we could say that the system has just felt  renewed  hunger. we note that m1 stays on the list until the very end of the epoch  when food  r  is received. m1 is in effect the name of a behavioral module  intent  plan  subprogram  with the purpose  get food . 
   at t1 the message from the environment was e1  top of figure . since the system took no external action on that time step  the same environment holds  we assume  at t1 but the overall situation is different at t1 since the message list contains m1 the system now forms a match set consisting of all classifiers which match both e1 
1 	knowledge acquisition 
and m1. from the match set  a single classifier is picked  through a competition based on classifier strengths  and that classifier's message  m1  is posted on the list on the next level down. the interpretation is that m1 names a 
module of m1 that applies when the environment is e1. 
   still the system has not made an external action. at t1  a match set is again formed with the proviso that its members must match e1  still unchanged  and m1  illustrating the matching rule:  if no external action occurred in the previous time-step  compare only against the lowest level message on the list in forming the match set.  the rationale is that the lowest level message represents the system's most immediate intent  which should have priority. again  the figure shows the posting of message m1 and thus a deepening of the hierarchy. 
   at t1 something new happens. following the same rules as above  the system picks a winning classifier whose message specifies an external action a1. in this case the action is taken and no new message is posted  action messages cause only actions . we have reached the level of a module  m1  whose components are not intents or submodules but external activity. 
   at t1  a new matching rule applies:  if an external action occurred during the previous time-step  compare against all messages on the list; if the  again  single winning classifier matched a message on level k of the list  erase all lower-level messages  if any  and post the winner's message one level below k.  in the current case  we see from the figure that m1 must have been the highest message matched  since no messages got erased  and that the winner's message was the action a1. the interpretation is that the system simply executes another action belonging to m1. 
   at t1  bigger changes occur. the second matching rule  the  ascent  rule  again applies  and this time the winning classifier matcxied m1  and e1   resulting in erasure of m1 and the posting of m1. here the interpretation is that  given environment e1  module m1 moves on to its second submodule m1; i.e.  its first submodule  m1  has been successfully carried out. 
   we now have enough information to understand the rest of the figure. from t1 through t1  the system executes the actions of m1  but this also completes m1. at t1 the system enters  descent   the first matching rule applies  and begins execution of the module m1  which lasts until t1. note that the first three steps of m1 are actions but the fourth is a submodule. finally  the fifth step  the action a1  results in external reward entering the system  which causes erasure of the entire message list. 
   the  ascent  matching rule  which acts most dramatically at t1 and t1  is designed to identify the highest-level module to which the environment resulting from the current action is relevant  and to terminate all lower level modules. this corresponds to the observation that completion of a high-level subplan usually means completion of all subplans which immediately underlie it. for example  completion of the subplan  get a table  under the plan  get food  also completes  take a seat  and  under that   pull the chair back up to the table   etc. 
   the operation of the bucket brigade in figure 1 is illustrated by the small arrows  which indicate strength 
   
flows. an arrow from one message to another  as between m1 and m1  means a payment from the classifier which posted m1 to the classifier which posted m1. as usual  the amount involved is a fraction of the strength of the source classifier  and it is added to the strength of the recipient. similarly  an arrow from one action to another  or from a message to an action  or vice versa  means a payment between the two corresponding classifiers. the special case of an arrow leaving the first step of a module  as with m1  means a fraction of the strength of the posting classifier is simply removed and  thrown away . 
   at time-steps 1  1  1  and 1  a more complicated payment pattern occurs. for example  at t1  the standard strength fraction is deducted from the classifier which sent m1  but the resulting quantity is paid to each of the three recipients indicated by the arrows. that is  if an amount q is deducted from the source classifier  each recipient has its strength incremented by q. similarly  at t1  the payoff quantity j   and not one-third of r  is paid to each of the three recipients shown. the intent of this  nonsplitting  of payoff is to encourage hierarchical deepening where appropriate; a different rule may of course turn out to be better. 
   we may note in figure 1 how the bucket-brigade pattern causes strength flows along the constituent steps of each module  thus reinforcing the steps in the spirit of the original bucket-brigade principle. but this  hierarchical bucket brigade  also achieves our objective of reducing the length of any individual chain. note that the overall activity of figure 1 consists of ten action steps  and 1 time-steps  yet no classifier is more than five payment steps from the external reward. more generally  hierarchy means that the average payment sequence length will be of the order of log n  where n is the number of steps in the overall activity. 
	v 	hierarchical algorithm 
   we now state the hierarchical performance and reinforcement algorithm. 
1  obtain the current message e from the environmental input interface. 
1  if phase=  descent   form the match set  m  of all classifiers which match both e and the lowest-level message on the message list  else 
if phase =  ascent   form the match set  m  of all classifiers which match both e and any message on the message list. 
1  compute the bid b of each classifier c in  m  by taking the product of c's strength and a small constant  say 1 . 
1  select a classifier c* from im  using a procedure in which higher-bidding classifiers are more likely to be selected. 
1  reduce c*'s strength by the amount of its bid; then 
if phase =  ascent   pay an amount equal to b to each of the classifiers  if any  which sent messages lower on the list than the message matched by c*  erase those lower messages  and pay an amount b to the classifier whose action was carried out on the previous time-step; 
1  if c*'s message is an external action  set phases  ascent . 
else post the message on the next lower empty level of the message list and set phase=  descent . 
	wilson 	1 
   
1  if the message of step 1 was an action  take it. 
1  if payoff r is received from the environment  pay amounts equal to r to each of the classifiers which sent messages now on the list  erase all messages  and pay an amount r to the classifier whose action was 
just taken. 
set phase= descent . 
1  return to step 1. 
   the new algorithm leaves some operational questions unanswered. for instance  we are not told what to do in step 1 if the match set  m  is null  this is also not covered in the standard algorithm . in  descent   the sensible thing would seem to be to assume that the most recent posting  lowest message  was a  mistake   erase it  and retry the match against the next higher message. in  ascent  the situation is more complicated  but failure to match is less likely since the whole list is matched against. a possible response would be to  reverse  the last action  if possible  and retry the match. in both cases  the stochastic element in the selection of c*  step 1  would permit alternative outcomes. if the system became truly stuck in a certain state or loop  a  fatigue  process could come into play  causing messages gradually to drop from the list. all these questions are more properly addressed at the level of the system routine of which the hierarchical algorithm is a component. 
	vi 	discussion 
   an important difference between the hierarchical classifier system outlined here and the standard system is that parallelism appears to be greatly reduced. the standard classifier system permits numerous messages to be posted in each cycle  whereas the hierarchical system permits the addition  to those already on the list  of no more than one message per cycle. parallelism in the standard system is intended to serve several functions  holland  1 . having numerous classifiers active on each cycle should allow the system simultaneously to test numerous hypotheses about the best way to get to payoff. over time  those that profit in the bucket brigade should win out  and  under the discovery algorithm  become progenitors of new  possibly even better  classifier hypotheses . secondly  parallelism is intended to give the system gracefulness in the sense that when control is divided among a cluster of rules  the failure or absence of one rule can be expected to have only a marginal effect on performance. finally  complex situations may be more flexibly represented internally by a set of numerous activated rules   each responding to an element of the situation  than by a few  or just one  rule which would have to encompass all relevant aspects in its condition. in short  multiple activation is intended to give the system a more powerful mental model of the world  holland et .  1 . 
   this is clearly an important objective for any learning system. we suggest  however  that the hierarchical system is not so narrow as it may appear. at any moment  in general  the message list contains a number of messages  which could be taken to represent a mental model  but in this case an hierarchical one. the higher level messages 
1 	knowledge acquisition 
represent broader  more general  aspects of the situation than the lower level messages. selection of a message for posting on a given level is the result of a competition which on another occasion could well pick a different classifier's message for testing. furthermore  the hierarchical system can be modified to permit more than one message on each level  essentially  one lets c* be a set instead of a single classifier  but there is not space here to go into detail . the single place where the hierarchical system is clearly  narrower  than the standard one is in  descent   where our rule is that only the lowest-level message gets matched against  corresponding to the principle that a plan cannot be achieved before its subplans.  to prevent insensitivity to environmental surprises  a multi-level interrupt can be provided by adding  if e differs from the previous e  set phase ='ascent'  to step 1.  
   further research is needed to determine the hierarchical classifier system's merit. we intend to apply it in an extension of our previous  animat   or artificial animal  simulations  wilson  1  in press . in concluding  we would stress the hierarchical system's two apparent plusses: short bucket-brigade chains and explicit modularity. 
