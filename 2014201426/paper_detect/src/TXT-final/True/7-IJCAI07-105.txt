
a novel training algorithm for nonlinear discriminants for classification and regression in reproducing kernel hilbert spaces  rkhss  is presented. it is shown how the overdetermined linear leastsquares-problem in the corresponding rkhs may be solved within a greedy forward selection scheme by updating the pseudoinversein an order-recursive way. the described construction of the pseudoinverse gives rise to an update of the orthogonal decomposition of the reduced gram matrix in linear time. regularization in the spirit of ridge regression may then easily be applied in the orthogonal space. various experiments for both classification and regression are performed to show the competitiveness of the proposed method.
1 introduction
models for regression and classification that enforce square loss functions are closely related to fisher discriminants  duda and hart  1 . fisher discriminants are bayesoptimal in case of classification with normally distributed classes and equally structured covariance matrices  duda and hart  1  mika  1 . however  in contrast to svms  least squares models  lsms  are not sparse in general and hence may cause overfitting in a supervised learning scenario. one way to circumvent this problem is to incorporateregularization controlled by a continuous parameter into the model. for instance  ridge regression  rifkin et al.  1  penalizes the norm of the solution yielding flat directions in the rkhs  which are robust against outliers caused by e. g. noise. in  suykens and vandewalle  1  least-squares svms  lssvms  are introduced which are closely related to gaussian processes and fisher discriminants. a linear set of equations in the dual space is solved using e. g. the conjugate gradient methods for large data sets or a direct method for a small number of data. the solution is pruned  de kruif and de vries  1  hoegaerts et al.  1  in a second stage. the close relation between the ls-svm and the kernel fisher discriminant  kfd  was shown in  van gestel et al.  1 . it follows from the equivalence between the kfd and a least squares regression onto the labels  duda and hart  1  mika  1  that the proposed method is closely related to the kfd and the ls-svm. however  the proposed method imposes sparsity on the solution in a greedy fashion using subset selection like in  billings and lee  1  nair et al.  1 . for the svm case similar greedy approaches exist  g. cauwenberghs  1  ma et al.  1 .
¡¡especially in case of large data sets subset selection is a practical method. it aims to eliminate the most irrelevant or redundant samples. however  finding the best subset of fixed size is an np-hard combinatorial search problem. hence  one is restricted to suboptimal search strategies. forward selection starts with an empty training set and adds sequentially one sample that is most relevant according to a certain criterion  e. g. the mean square error . in  nair et al.  1  an external algorithm which is based on elementary givens rotations is used to update the qr-decomposition of the reduced gram matrix in order to construct sparse models. the gram schmidt orthogonalization is used in  billings and lee  1  and  chen et al.  1  for the orthogonal decomposition of the gram matrix. they also apply forward selection in a second step to obtain sparse models. this method is known as orthogonal least squares  ols . however  the ols algorithm requires the computation and the storage of the full gram matrix which is prohibitive for large datasets.
¡¡in this paper a very simple and efficient way for constructing lsms in a rkhs within a forward selection rule with much lower memory requirements is presented. the proposed method exploits the positive definiteness of the gram matrix for an order-recursive thin update of the pseudoinverse  which reveals to the best of our knowledge a novel kind of update rule for the orthogonaldecomposition. the solution is regularized in a second stage using the generalized cross validation to re-estimate the regularization parameter.
¡¡the remainder of this paper is organized as follows. in section 1  computationally efficient update rules for the pseudoinverse and the orthogonal decomposition are derived. in section 1  it is shown how the solution may be regularized. in section 1 some experimental results on regression and classification datasets are presented. finally  a conclusion is given in section 1.
1 update of the orthogonal decomposition
in a supervised learning problem one is faced with a training data set d = {xi yi} i = 1...m. here  xi denotes an input vector of fixed size and yi is the corresponding target value which is contained in r for regression or in {1  1} for binary classification. it is assumed that .
we focus on sparse approximations of models of the form y  = k¦Á.	 1 
the use of mercer kernels k ¡¤ x   mercer  1  gives rise to a symmetric positive definite gram matrix k with elements kij = k xi xj  defining the subspace of the rkhs in which learning takes place. the weight vector ¦Á = {b ¦Á1 ... ¦Ám} contains a bias term b with a corresponding column 1 = {1 ... 1} in the gram matrix.
consider the overdetermined least-squares-problem
	¦Á m = argmin	 1 
¦Ám
in the m-th forward selection iteration with the reduced gram matrix km =  1 k1 ...km  ¡Ê rm¡Á m+1  where ki =
 k ¡¤ x1  ... k ¡¤ xm  t  i ¡Ê {1 ... m} denotes one previously unselected column of the full gram matrix. we denote the reduced weight vector as ¦Ám = {b ¦Á1 ... ¦Ám} ¡Ê rm+1 and the target vector as y =  y1 ... ym t. among all generalized inverses of km the pseudoinverse
               k	 1  is the one that has the lowest frobenius norm  ben-israel and greville  1 . thus  the corresponding solution
	¦Á m = k my	 1 
has the lowest euclidean norm.
partitioning km and ¦Ám in the form
	km	=	 km 1km 	 1 
t
	¦Ám	=	 ¦Ám 1¦Ám 	 1 
and setting ¦Ám = ¦Ám1 = const  the square loss becomes

the minimum of  1  in the least-squares-sense is given by
	¦Á m 1 = k m 1 y   km¦Ám1 .	 1 
inserting  1  into  1  yields

 1 
with i denoting the identity matrix of appropriate size.
note that the vector
¡¡¡¡¡¡¡¡¡¡¡¡q	 1  is the residual corresponding to the least-squares regression onto km. hence  qm is a nullvector if and only if km is a nullvector unless k is not strictly positive definite. to ensure strictly positive definiteness of k  it is mandatory to add a small positive constant ¦Å to the main diagonalof the full gram matrix in the form k ¡ú k+¦Åi. forward selection may then be performed using this strictly positive definite gram matrix.
in the following k is assumed. the minimum of  1  is met at
	¦Á m1 = q m i   km 1k m 1 y	 1 
noting that the pseudoinverse of a vector is given by
qt
	q	 1 
equation  1  may be written as
y
		 1 
ktm i   km 1k m 1 t i   km 1k m 1 y
	=	.
the matrix
¡¡¡¡¡¡¡¡¡¡¡¡¡¡pm = i   km 1k m 1  1  is an orthogonal projection matrix which implies being symmetric and idempotent and thus equation  1  simplifies to
	¦Á m1 = q my.	 1 
combining  1  with  1  the current weight vector ¦Á m may be updated as
	¦Á m y	 1 
revealing the update
	k m 	 1 
for the current pseudoinverse.
¡¡since every projection qm = pmkm lies in a subspace which is orthogonal to km 1 it follows immediately that qti qj = 1  for . hence  an orthogonal decomposition
	km = qmum	 1 
of the reduced gram matrix is given by the orthogonal matrix
	qm =  qm 1qm 	 1 
and the upper triangular matrix
	um .	 1 
in the m-th iteration o mm  operations are required for all these updates. note that the inversion of the matrix qtmqm is trivial since this matrix is diagonal. however  the condition number of the matrix qm increases as the number of selected columns m grows. thus  to ensure numerical stability it is important to monitor the condition number of this matrix and to terminate the iteration if the condition number exceeds a predefined value unless another stopping criterion is reached earlier.
1 regularization and selection of basis centers
the goal of every forward selection scheme is to select the columns of the gram matrix that provide the greatest reduction of the residual. methods like basis matching pursuit  mallat and zhang  1   order-recursive matching pursuit  natarajan  1  or probabilistic approaches smola and scho¡§lkopf  1  are several contributions to this issue. in  nair et al.  1   forward selection is performed by simply choosing the column that corresponds to the entry with the highest absolute value in the current residual. the reasoning is that the residual provides the direction of the maximum decrease in the cost function 1¦Átk¦Á   ¦Áty  since the gram matrix is strictly positive definite. the latter method is used in the following experiments. but note that the derived algorithm may be applied within any of the above forward selection rules. in the following we will refer to the proposed method as order-recursive orthogonal least squares  orols .
consider the residual
¡¡¡¡¡¡¡¡¡¡¡¡e m = y   y m = y   qm¦Á m  1  in the m-th iteration. the vector ¦Á m contains the orthogonal weights.
the regularized square residual is given by
	e m	= e tme m + ¦Ë¦Á tm¦Á m	 1 
= ytp my
where ¦Ë denotes a regularization paramter. the minimum of  1  is given by
	p m	= i   qm qtmqm + ¦Ëim  1qtm	 1 
.
thus  the current residual corresponding to the regularized least squares problem may be updated as
q
e my
ytqm
	= e 	  q	.	 1 
the orthogonal weights
ytqi
	  m i =	 	1 ¡Ü i ¡Ü m.	 1 
can be computed when the forward selection is stopped. the original weights can then be recovered by
	¦Á m = u m1¦Á m	 1 
which is an easy inversion since um is upper triangular.
¡¡in each iteration one chooses the qi which corresponds to the highest absolute value in the current residual and adds it to qm 1. it is possible to determine the number of basis functions using crossvalidation or one may use for instance the bayesian information criterion or the minimum description length as alternative stopping criteria. following  gu and wahba  1  and  orr  1  it is possible to use the generalized cross validation  gcv   as a stopping criterion. we will now summarize the results. for details see  orr  1 . the gcv is given by
	.	 1 
trace
minimizing the gcv with respect to ¦Ë gives rise to a reestimation formula for ¦Ë. an alternative way to obtain a re-estimation of ¦Ë is to maximize the bayesian evidence  mackay  1 .
¡¡differentiating  1  with respect to ¦Ë and setting the result to zero gives a minimum when
	y	trace p m  = ytp 1my	m .
 ¦Ë
noting that 1 ytp m p my = ¦Ë¦Á tm qtmqm + ¦Ëim  1¦Á m 1 	 p  y	 trace p 	 
 ¦Ë
equation  1  can be rearranged to obtain the re-estimation formula
	¦Ë :=	m	m
trace p m ¦Á tm qtmqm + ¦Ëim  1¦Á m
where 1 	  trace p 	 / ¦Ë ytp 1 y
	.	 1 
the forward selection is stopped when ¦Ë stops changing significantly.
¡¡the computationalcost for this updateis o m . the orolos algortihm is summarized in pseudocode in algorithm 1.
1 experiments
to show the usefulness of the proposed method empirically  some experiments for regression and classification are performed. in all experiments the gaussian kernel
		 1 
is used. the kernel parameter ¦Ò is optimized using a 1-fold crossvalidation in all experiments. for the classification experiments the one-vs-rest approach is used to obtain a multiclass classification hypothesis.
1 classification
for classification  1 well-known benchmark datasets were chosen. the usps dataset contains 1 pixel values of handwritten digits as training and testing instances.
¡¡the letter dataset contains 1 labeled samples. the character images were based on 1 different fonts and each letter within these fonts was randomly distorted to produce a dataset of unique stimuli. for this dataset no predefined split for training and testing exist. we used the first 1 instances for training and the remaining 1 instances for testing.
¡¡optdigits is a database of digits handwritten by turkish writers. it contains digits written by 1 writers. the training set is generated from the first 1 writers and digits written by the remaining independent writers serve as testing instances. the database was generated by scanning and processing forms to obtain 1 ¡Á 1 matrices which were then reduced to 1 ¡Á 1.
algorithm 1 order-recursive orthogonal least squares
 orols 

require: training data x  labels y  kernel
initializations:  
q1 = 1  u1 =   i = {1 ... m}  iopt = {}
while ¦Ë changes significantly and qm is not illconditioned
do update e m
find the index iopt of the entry of e m with the highest
absolute value
iopt ¡û {iopt iopt}
i ¡û i   {iopt}
compute kiopt compute qiopt
km ¡û  km 1kopt  qm ¡û  qm 1qopt 
update k m and um using kopt and qopt update ¦Ë
m ¡û m + 1
end while return ¦Á m iopt

¡¡pendigits contains pen-basedhandwritten digits. the digits were written down on a touch-sensitive tablet and were then resampled and normalized to a temporal sequence of eight pairs of  x y  coordinates. the predefined test set is formed entirely from written digits produced by independent writers.
¡¡the satimage dataset was generated from landsat multispectral scanner image data. each pattern contains 1 pixel values and a number indicating one of the six classes of the central pixel.
¡¡the caracteristics of the datasets are summarized in table 1. the results can be seen in table 1. especially for the optdigits and pendigits datasets orols appears to be significantly superior compared with svms. the performance on the remaining 1 datasets is comparable with svms.
data set# classes# training# testingusps11letter11optdigits11pendigits11satimage11table 1: datasets used for the classification experiments.
data setsvmorolsusps11 1 letter11 1 optdigits11 1 pendigits11 1 satimage11 1 table 1: test errors in % on 1 benchmark datasets. the onevs-rest approach is used. average fraction of selected basis centers in % within parantheses.
1 regression
for regression  we first perform experiments on a synthetic dataset based on the function sinc x  = sin x /x  x ¡Ê   1  which is corrupted by gaussian noise. all training and testing instances are chosen randomly using a uniform distribution on the same interval. the results are illustrated in figures 1 and table 1.
¡¡additionally  the two real world datasets boston and abalone  which are available from the uci machine learning repository  are chosen. the hyperparameters are optimized in a 1-fold crossvalidation procedure. for both datasets  random partitions of the mother data for training and testing are generated  1  1  partitions with 1  1  instances for training and 1  1  for testing for the boston and abalone dataset  respectively . all continuous features are rescaled to zero mean and unit variance for both abalone and boston. the gender encoding  male / female /infant  for the abalone dataset is mapped into { 1 1   1 1   1 1 }. the mean squared error  mse  of orols is compared with a forward selection algorithm based on a qr-decomposition of the gram matrix  nair et al.  1 . the results in table 1 show that the mse is improved significantly by orols. in contrast to orols the qr method uses an external algorithm with reorthogonalization for the update of the orthogonal decomposition. we observed that our update scheme which is to the best of our knowledge a novel update needs not to be reorthogonalized as long as the gram matrix has full rank. this improvement of accuracy could be one reason for the good performance of orols. furthermore  it should be noted that the best performance of orols for the boston dataset is quite favourablecomparedwith the best performance of svms  mse 1 ¡À 1   scho¡§lkopf and smola  1 .
methodrmsesvm1rvm1orols1table 1: average rmse for the sinc experiment. 1 / 1 randomly generated points are used for training / testing. the standard deviation of the gaussian noise is 1 in all runs. the results are avereged over 1 runs.

figure 1: example fit to a noisy sinc function using 1 / 1 randomly generated points for training / testing. the standard deviation of the gaussian noise is 1. the root mean square error  rmse  is 1 in this case. 1 points are selected as basis centers.

figure 1: rmse of fits to a noisy sinc function w. r. t. different training set sizes. 1 randomly generated points are used for testing. the standard deviation of the gaussian noise is 1 in all runs. the results are avereged over 1 runs for each size.
datasetqrorolsboston1¡À11¡À1 1 abalone1¡À11¡À1 1 table 1: mean square error  mse  with standard deviations for the boston and abalone dataset using different methods. average fraction of selected basis centers in % within parantheses.
1 conclusion
a computationally efficient training algorithm for orthogonal least squares models using mercer kernels is presented.

figure 1: rmse of fits to a noisy sinc function w. r. t. different noise levels. 1 / 1 randomly generated points are used for training / testing. the results are avereged over 1 runs for each noise level.
the pseudoinverse is updated order-recursively and reveals the current orthogonal decomposition of the reduced gram matrix within a forward selection scheme. the generalized cross validation serves as an effective stopping criterion and allows to adapt the regularization parameter in each iteration. extensive empirical studies using synthetic and real-world benchmark datasets for classification and regression suggest that the proposed method is able to construct models with a very competitive generalization ability. the advantage of the proposed method compared with e. g. svms is its simplicity. sparsity is achieved in a computationally efficient way by constuction and can hence better be controlled than in the svm case where a optimization problem is to be solved. furthermore  in contrast to svms orols allows an easy incorporation of multiple kernels  i e. the kernel parameters may be varied for different training instances in order to obtain more flexible learning machines. this possibility is not examined here and may be an interesting direction for future work. a further step for future work could be the development of the proposed algorithm for tasks like dimensionality reduction or online-learning.
