 
statistics-based classifiers in natural language are developed typically by assuming a generative model for the data  estimating its parameters from training data and then using bayes rule to obtain a classifier. for many problems the assumptions made by the generative models are evidently wrong  leaving open the question of why these approaches work. this paper presents a learning theory account of the major statistical approaches to learning in natural language. a class of linear statistical queries  lsq  hypotheses is defined and 
learning with it is shown to exhibit some robustness properties. many statistical learners used in natural language  including naive bayes  markov models and maximum entropy models are shown to be lsq hypotheses  explaining the robustness of these predictors even when the underlying probabilistic assumptions do not hold. this coherent view of when and why learning approaches work in this context may help to develop better learning methods and an understanding of the role of learning in natural language inferences. 
1 	introduction 
generative probability models provide a principled way to the study of statistical classification in complex domains such as natural language. it is common to assume a generative model for such data  estimate its parameters from training data and then use bayes rule to obtain a classifier for this model. in the context of natural language most classifiers are derived from probabilistic language models which estimate the probability of a sentence s  say  using bayes rule  and then decompose this probability into a product of conditional probabilities according to the generative model assumptions. 

    research supported by nsf grants iis-1 and sbr-1. 
1 	natural language processing 
where hi is the relevant history when predicting wi. note that in this representation s can be any sequence of tokens  words  part-of-speech  pos  tags. etc. 
　this general scheme has been used to derive classifiers for a variety of natural language applications including speech applications  rabiner  1   part of speech tagging  kupiec  1; schiitze  1   word-sense disambiguation  gale et al.  1   context-sensitive spelling correction  golding  1  and others. while the use of bayes rule is harmless  most of the work in statistical language modeling and ambiguity resolution is devoted to estimating terms of the form pr w h . the generative models used to estimate these terms typically make markov or other independence assumptions. it is evident from looking at language data that these assumptions are often patently false and that there are significant global dependencies both within and across sentences. for example  when using  hidden  markov model  hmm  as a generative model for the problem of part-of-speech tagging  estimating the probability of a sequence of tags involves assuming that the part of speech tag ti of the word wi is independent of other words in the sentence  given the preceding tag ti-1. it is not surprising therefore that making these assumptions results in a poor estimate of the probability distribution density function1. however  classifiers built based on these false assumptions nevertheless seem to behave quite robustly in many cases. 
　in this paper we develop a learning theory account for this phenomenon. we show that a variety of models used for learning in natural language make their prediction using linear statistical queries  lsq  hypotheses. this is a family of linear predictors over a set of fairly simple features which are directly related to the independence assumptions of the probabilistic model assumed. we claim that the success of classification methods which are derived from incorrect probabilistic density estimation is due to the combination of two factors: 
  the low expressive power of the derived classifier. 
  robustness properties shared by all linear statistical queries hypotheses. 
　　1  experimental evidence to that effect will not be included in this extended abstract for lack of space  but see sec. 1 . 

we define the class of lsq hypotheses and prove these claims. namely  we show that since the hypotheses are computed over a feature space chosen so that they perform well on training data  learning theory implies that they perform well on previously unseen data  irrespective of whether the underlying probabilistic assumptions hold. we then show how different models used in the literature can be cast as lsq hypotheses by selecting the statistical queries  i.e.  set of features  appropriately and how this affects the robustness of the derived hypothesis. 
　the main contribution of the paper is in providing a unified learning theory account for a variety of methods that are widely used in natural language applications. the hope is that providing a coherent explanation for when and why learning approaches work in this context could help in developing better learning methods and a better understanding of the role of learning in natural language inferences. 
　we first present learning theory preliminaries and then define the class of linear statistical queries hypotheses and discuss their properties. in sec. 1 we show how to cast known probabilistic predictors in this framework  and in sec. 1 we discuss the difference between lsq learning and other learning algorithms. 
1 preliminaries 
in an instance of pac learning  valiant  1   a learner needs to determine a close approximation of an unknown target function from labeled examples of that 
function. the unknown  function 
sumed to be an element of a known function class 
over the instance space x =  and the examples are distributed according to some unknown probability distribution d on x. a learning algorithm draws a sample of labeled examples according to d and eventually outputs a hypothesis h from some hypothesis class 
 the error rate of the hypothesis h is defined to be 
err or  h  = the learner goal is to output  with probability at least a hypothesis  whose error rate is at most e  for the given error parameter e and confidence parameter  using sample size that is polynomial in the relevant parameters. the learner is then called a pac learner. the algorithm is an efficient pac learning algorithm if this is done in time polynomial in the relevant parameters. 
　a more realistic variant of the pac-learning model  the agnostic learning model  haussler  1; kearns et al  1   applies when we do not want to assume that the labeled training examples arise from a target concept of an a-priori simple structure in this model one assumes that data elements are sampled according to some arbitrary distribution d on d may simply reflect the distribution of the data as it occurs  in nature   including contradictions  without assuming that the labels are generated according to some  rule . the true error of the hypothesis is defined to be 	and the goal of the learner is to compute  with high probability  a hypothesis 	with true error not larger than 
a learner that can carry out this 
task for any distribution d is called an efficient  agnostic  pac learner for hypothesis class if the sample size  and the time required to produce the hypothesis 
are polynomial in the relevant parameters. 
　in practice  one cannot compute the true error  and in many cases it may be hard to guaran-
tee that the computed hypothesis h is close to optimal in 
   instead  the input to the learning algorithm is a sample of m labeled examples  the learner tries to find a hypothesis h with a small empirical error 

and hopes that it behaves well on future examples. 
　the hope that a classifier learned from a training set will perform well on previously unseen examples is based on the basic theorem of learning theory  valiant  1; vapnik  1  which  stated informally  guarantees that if the training data and the test data are sampled from the same distribution  good performance on large enough training sample guarantees good performance on the test data  i.e.  good  true  error . this is quantified in the following uniform convergence result: 
where k is some constant and 	is the vc-
dimension of the class  vapnik  1   a combinatorial parameter which measures the richness of  
　in practice the sample size  s  may be fixed  and the result simply indicates how much can one count on the true accuracy of a hypothesis selected according to its performance on s. also  the computational problem of choosing a hypothesis which minimizes the empirical error on s  or even approximates a minimum  may be hard  kearns et al  1; hoffgen and simon  1 . finally  notice that the main assumption in the theorem above is that the training data and the test data are sampled from the same distribution; golding and roth  discuss this assumption in the nlp context. 
1 robust learning 
this section defines a learning algorithm and a class of hypotheses with some generalization properties  that we later show to capture many probabilistic learning methods used in nlp. the learning algorithm discussed here is a statistical queries  sq  algorithm  kearns  1 . an sq algorithm can be viewed as a learning algorithm that interacts with its environment in a restricted way. rather than viewing examples  the algorithm only requests the values of various statistics on the distribution of the labeled examples to construct its hypothesis.  e.g.  what is the probability that a randomly chosen labeled example  x  i  has = 1 and l =  a feature is an indicator function {1} which defines a subset of 
	both 	1 

the instance space - all those elements which are mapped to 1 by denotes a class of such indicator functions. 
   can be viewed as a transformation of the instance space x; each example is mapped to an example in the new space. to simplify notation  we sometimes view a feature as an indicator function over the labeled instance space and say that for e x a m p l e s w i t h label/. 
　a statistical query has the form where  is a feature  is a further  optional  restriction imposed on the query and is an error parameter. a call to the oracle returns an estimate of 
 1  
which satisfies  when clear from the context we omit from this notation.  a statistical queries algorithm is a learning algorithm that constructs its hypothesis only using information received from an 
sq oracle. an algorithm is said to use a query space if it only makes queries of the form  where 
       as usual  an sq algorithm is said to be a good learning algorithm if  with high probability  it outputs a hypothesis with small error  using sample size that is polynomial in the relevant parameters. 
　in order to simulate the behavior of the sq oracle on a query on the distribution d  we simply draw a large sample 1 of labeled examples according to d 
　and evaluate 

　chernoff bounds guarantee that the number of examples required to achieve tolerance with probability at least is polynomial i n a n d log  
　the sq model of learning was introduced by kearns  and studied in  decatur  1; aslam and decatur  1 . it was viewed as a tool for demonstrating that a pac learning algorithm is noise-tolerant. in particular  it was shown that learning with an sq algorithm allows the learner to tolerate examples with noisy labels - labels which  with some probability   1  are inconsistent with the target function  classification noise   as well as various forms of corrupted examples  malicious noise  attribute noise . a typical noise-tolerant learning result in the pac model states that if an sq algorithm learns a concept class in the sense that it produces a hypothesis with small true error  then this algorithm is noise tolerant in that it would have produces a hypothesis with small true error even when trained with noisy data. the running time of the algorithm and the number of data samples required depend on the tolerance required from the statistical queries. this  in turn  is determined by the noise level the algorithm is required to tolerate  up to some bounds; see the references for details . in the next section we consider a special sq algorithm and prove a robustness result that may be more satisfying from a practical point of view. 
1 	natural language processing 
1 	linear statistical queries hypotheses 
let be a class of features and a function that depends only on the values 
linear statistical queries  lsq  hypothesis predicts  given when 
clearly  the lsq is a linear discriminator over the feature space with coefficients that are computed given  potentially all  the values the definition generalizes naturally to non-binary classifiers by allowing to range over a larger set of labels; in this case  the discriminator between predicting l and other values is linear. a learning algorithm that outputs an lsq hypothesis is called an lsq algorithm. 
example 1 the naive bayes predictor  duda and 
hart  1  is derived using the assumption that given the label the features' values are statistically inde-
pendent. consequently  the bayes optimal prediction is given by: 

where denotes the prior probability of i  the fraction of examples labeled i  and are the condi-
tional feature probabilities  the fraction of the examples labeled i in which the ith feature has value xi . the lsq definition implies: 
claim 1 the naive bayes algorithm is an lsq alfeatures: 
　we note that formalized this way  as is commonly done in nlp  e.g.   golding  1    features which are not active in the example are assumed unobserved and are not taken into account in the naive bayes estimation. it is possible to assume instead that features which are not 
active are false  this assumption yields an lsq algorithm over the same features  with different coefficients  see  e.g.   roth  1  for the derivation . 
　the observation that the lsq hypothesis is linear over yiedls the first robustness result. vc dimension theory implies  via a variation of  e.g.   anthony and holden  1   and due to the restriction lsq imposes on the coefficients  that: 
claim 1 the vc dimension of the class of lsq hypotheses is bounded above by if all the features used are polynomials of the form  which are all linearly independent   the vc dimension of the lsq hypotheses is exactly  
this  together with the basic theorem 1 implies: 

corollary 1 for any lsq learning algorithm and given a performance guarantee on previously unseen examples  sampled from the same distribution   the number of training examples required in order to maintain this performance scales linearly with the number of features used. 
1 	distributional robustness 
lsq hypotheses axe computed by an sq algorithm; it calls the sq oracle to estimate  for all elements in and applies  then  given an example 
evaluates the hypothesis to make the prediction. 
　as discussed above  the sq model was introduced as a tool for demonstrating that a pac learning algorithm is noise-tolerant. in a practical learning situation we are given a fixed sample of data to use in order to generate a hypothesis  which will be tested on a different sample  hopefully  similar  to the training sample. our goal is to show that an algorithm that is able to learn under these restrictions  namely  interacting only via sqs  is guaranteed to produce a robust hypothesis. intuitively  since the sq predictor does not really depend on specific examples  but only on some statistical properties of the data  and only within some tolerance  it should still perform well in the presence of data sampled according to a different distribution  as long as it has the same statistical properties within this tolerance. 
　next we formalize this intuition and show that the basic results proved for the sq model can be adapted to this situation. formally  we assume that there is a test sample  stest  generated according to some distribution d1'  and that we care about our hypothesis' performance on this sample. however  training is performed on a different sample  strain  generated according to a different distribution d. we show that an sq hypothesis trained over strain can still perform well on stest  if d and d' are not too different. a common distance measure between distributions is the variation = 
it is easy to see that this mea-
sure is equivalent to 
which is more convenient for our purposes. a more biased measure  strictly smaller than the standard measures may also be used in this case: 

yielding better robustness guarantees for the algorithm. 
theorem 1 let a be an learning algorithm for a function class over the distribution d and assume that inversely polynomial in  
then 	is also an 	learning algorithm for  
over  
proof: since 	is 	an 	algorithm  a hypoth-
esis generated with estimates 	that are close to behaves well on d. in order for the hypothesis to behave 
　1  yamanishi  1  shows that this measure is equivalent to other well known measures such as the kl divergence. well on d1 we can simply run 	with oracle calls of tol-
erance to get 	that is within 
by the definition of the distance measure  the difference between the probability of events of interest occurring under d and d' is no more than 
thus  this procedure simulates estimates that are within  on d'  implying a well behaved hypothesis on df. 
to simulate the more accurate estimates  with tolerance there is a need to use more samples  but only 
polynomially more due to the relation between 	and 	 

　for the final point regarding robustness  observe that the proof shows the importance of the tolerance. in practice  the sample is given  and the robustness of the algorithm to different distributions depends on the sample's size. the richness of the feature class plays an important role here. to ensure tolerance of for all features one needs to use samples  more generally  samples . for a given size sample  therefore  the use of simpler features in the lsq representation provides better robustness. this  in turn  can be traded off with the ability to express the learned function with an lsq over a simpler set of features. 
1 applications: probabilistic classifiers 
in this section we cast a few widely used probabilistic classifiers as lsq hypotheses  implying that they are subject to the properties discussed in sec. 1. 
1 naive bayes 
as shown in example 1  the naive bayes predictor is an lsq hypothesis. the implication is that this method  which has been widely used for- natural language tasks  gale et a/.  1; golding  1  satisfies: 
corollary 1 the naive bayes algorithm is a robust learning method in the sense of cor. 1 and thm 1-
1 	general bayesian classifier 
the naive bayes predictor can be generalized in several ways. first  one can consider a generalization of the naive model  in which hidden variables are allowed. the simplest generalization in this direction would assume that the distribution is generated by postulating a  hidden  random variable z. having randomly chosen a value z for the hidden variable  we choose the value for each observable xi independently of each other. 
　as a motivating example consider the case of disambiguation or tagging in natural language tasks  where the  context  or a tag may be viewed as hidden variables. 
　the prediction problem here is to predict the value of one variable  given known values for the remaining variables. one can show that the predictor in this setting is again an lsq that uses essentially the same features as in the simple  observable  naive bayes case discussed above  e.g.   grove and roth  1  . 
	roth 	1 

　a second generalization of naive bayes is to assume that the independence structure around the predicted variable x is more complicated and is represented  for example  using a bayesian network that is not as trivial as the naive bayes network. in this case too  since we care only about predicting a value for a single variable having observed the other variables  it is not hard to verify that the bayes optimal predictor is an lsq hypothesis. the features in this case are polynomials of degree that depends on the number of neighbors of x in the network. the details of this analysis are not included for lack of space. instead  we concentrate on a special case that is of great interest in natural language. 
1 	markov models 
markov models and hidden markov models  hmms  are a standard model used in language modeling in general and in disambiguation tasks such as part-of-speech  pos  tagging in particular.  see  e.g.  rabiner  1 ; we will assume familiarity with them here . for a bi-gram pos tagger  the states of the hmm are pos tags. transition probabilities are probabilities of a tag given the previous tag  and emission probabilities are probabilities of a word given a tag. the standard algorithms first estimate the transition and emission probabilities from training data; then they use the markov model assumption in order to evaluate the probability of a particular part of speech sequence given a sentence. for example  for the part-ofspeech sequence of the sentence this can will rust one computes 

of course  this is correct only under the markov assumption; namely  one assumes that given a tag for the ith word in the sentence  the value of the word is independent of other words or tags  yielding the term p{can nn  i and the tag of the i -f 1 word is independent of preceding tags  yielding p md nn  . in order to tag a sentence for pos one needs to maximize this term over all possible assignments of pos tags. this is usually done using a dynamic programming technique  e.g.  a variation of the viterbi algorithm. 
　here we will be interested in the main computational step in this process - predicting the pos of a single word  given the sentence and assuming the pos of all the neighboring words as input. we will disregard the global optimization of finding the best simultaneous pos assignment to all words in the sentence1. we call this predictor the markov predictor. for this prediction  it is easy to see that given an example 
　　1 interestingly  experimental studies have shown that sometimes the global optimization is not required and the local predictions actually produce better results  delcher et 
al.  1; roth and zelenko  1 . 
1 	natural language processing 
we predict  
or  equivalently  

where t is the set of all possible pos tags. in the above example  for the word 'can' we will select the tag t that maximizes  
　let t  w denote the sets of all tags t and all words resp.  which occur in the examples. in order to represent the markov predictor as an lsq algorithm we use  =  the set of all singletons and pairs 
of words and tags. with this set of features and the notation introduced in eq. 1 it is clear that the markov predictor above can be written as an lsq hypothesis: 

where and otherwise   
　notice that in this formalization each word in the sentence yields a single training example and features are computed with respect to the word for which the tag is to be predicted. we get that: 
claim 1 the markov predictor is an lsq algorithm over a set  of singletons and pairs of words and tags. therefore  the markov predictor is a robust learning method in the sense of cor. 1 and thm 1- this fact naturally generalizes to higher order markov predictors by increasing the expressivity of the features. 
1 	maximum entropy models 
maximum entropy  me  models  jaynes  1  are exponential distributions which satisfy given constraints. constraints are what we have called features and the distribution is defined in terms of the expected value of the feature over the training set. these models have been used successfully to generate predictors for several disambiguation tasks  ratnaparkhi et a/.  1; ratnaparkhi  1   typically by estimating the me probability model for the conditional probability  as in sec. 1 . 
　it can be shown that the distribution that maximizes the entropy and is consistent with a set of constraints  features  must have the form  where c is a normalization constant and 
the ctj are the model parameters. each parameter corresponds to exactly one feature  and can be viewed as the weight of that feature. 
　predictions using the me model are performed as follows. given that the probability distribution pme has 

been estimated as above  and given the observed sentence s we would predict 

equivalently  by taking logarithms  we get a linear decision surface over the feature space. 
　however  while the optimal coefficients for an me predictor are also a function of the features' statistics over the training data  they cannot be written explicitly. instead  an iterative algorithm  e.g.   darroch and ratcliff  1   that approximates the optimal coefficients  but is not known to converge efficiently  is required. the me predictor is thus a linear predictor but  strictly speaking  in its optimal form is not an sq algorithm. 
1 	lsq vs. consistent learning 
in this section we discuss an important difference between an lsq algorithm and  standard  learning algorithms  using the naive bayes classifier as a running example. in most cases  algorithms studied in machine learning search for a hypothesis that is  almost  consistent with the training data and then rely on thm 1 to guarantee good performance in the future. an lsq algorithm  on the other hand  need not seek consistency. it simply computes a hypothesis from the responses of the sq oracle and uses that hypothesis to make future predictions. indeed  it is easily verified that the hypothesis produced by  say  a naive bayes algorithm may not be consistent with the data that was used to compute it. nevertheless  when the data satisfies the probabilistic assumptions used to derive the predictor  namely features' values are independent given the label  this hypothesis is the optimal predictor for future examples  although it may still make many mistakes . however  when the probabilistic assumptions do not hold  the success of the predictor is directly related to its success on the training data. 
　consistency  or almost consistency  is not a trivial requirement. in fact  it is hard to come up with structural properties of classes for which naive bayes is an optimal predictor. even when the target concept is very simple - the class of boolean conjunctions - consistency is not achieved in general  and the naive bayes algorithm is not a good predictor for this class1. 
example 1 consider the boolean conjunction f =  over over  assume that examples are generated according to the following distribution: exactly half of the examples are positive. over the positive examples necessarily = =1 and we define and that this value is 1 with probability are 1 with probability 1  independently 
　　1 domingo&pazzani claim in a 1 mlj paper that naive bayes is optimal for boolean conjunctions and disjunctions. their proof  however  holds only for product distributions  for which the naive bayes assumptions hold. the following example may be viewed as a counterexample to a more general interpretation of their claim. 
of anything else. overthe negative examples x1 is l with probability 1 and x1 = x1 =x1 and this value is 1 with probability 1. x1        xk are 1 with probability 1  independently of anything else. 
　in this way we have thatp x1|l = 1  = p x1|l = 1  = 1 and  = 1|z = 1  = 1 for i = 1 ...k. for the negative examples we have that = 1 
for i = 1 1 and 	= 1 for  
　it is now easy to see that while the naive bayes algorithm does not make mistakes on negative examples of a conjunction  it will predict incorrectly on half of the 
positive examples  all those for which x1 = x1 = x1 = 1. 
clearly  this behavior is indicative of the performance of this predictor on any data presented to it which is sampled according to this distribution. 
　practitioners  however  do not simply compute the lsq hypothesis and use it  confident that it performs well since  when the assumptions of the underlying model hold  it represents the maximum likelihood hypothesis. instead  they spend time choosing a feature set with which the predictor performs well on the training set. once this is achieved  according to the results in sec. 1  the predictor behaves well on previously unseen examples  but this happens regardless of whether the underlying probabilistic assumptions hold1. the only limitation on growing the feature set is the relation between the expressiveness of these features and the sample size required to achieve small tolerance  which we have alluded to. the role of the probabilistic assumptions in this case may be viewed as supplying guidance for selecting good features. 
　the question of why it is that even in seemingly hard nlp tasks  the search for good features is fairly simple  making it unnecessarily to resort to very expressive features  is an orthogonal question that we address in a companion paper. for the sake of the argument developed here  it is sufficient to realize that all the algorithms presented rely on their good behavior on the training set  and thus behave well even if the probabilistic assumptions that were used to derive the predictor do not hold. 
1 	conclusion 
in the last few years we have seen a surge of empirical work in natural language. a significant part of this work is done by using statistical machine learning techniques. roth  has investigated the relations among some of the commonly used methods and taken preliminary steps towards developing a better theoretical understanding for why and when different methods work. this work continues to develop the theoretical foundations of learning in natural language  focusing on the study of 
　　1  experimental evidence to this effect is presented in  golding and roth  1 . two sets of features are compared. the one which provides a better conditional probability estimation  by virtue of better satisfying the independence assumption  nevertheless supports significantly worse performance on the prediction task. 
	roth 	1 

probabilistic density estimation models and their use in performing natural language predictions. 
　in addition to providing better learning techniques  developing an understanding for when and why learning works in this context is a necessary step in studying the role of learning in higher-level natural language inferences. 
acknowledgments 
i am grateful to jerry dejong  lenny pitt and anonymous referees for their useful comments on an earlier version of this paper. 
