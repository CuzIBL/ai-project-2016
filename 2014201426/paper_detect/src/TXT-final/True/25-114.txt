 
artificial intelligence has been the field of study for exploring the principles underlying thought  and utilizing their discovery to develop useful computers. traditional ai models have been  consciously or subconsciously  optimized for available computing resources which has led ai in certain directions. the emergence of massively parallel computers liberates the way intelligence may be modeled. although the ai community has yet to make a quantum leap  there are attempts to make use of the opportunities offered by massively parallel computers  such as memory-based reasoning  genetic algorithms  and other novel models. even within the traditional ai approach  researchers have begun to realize that the needs for high performance computing and very large knowledge bases to develop intelligent systems requires massively parallel ai techniques. in this computers and thought award lecture  i will argue that massively parallel artificial intelligence will add new dimensions to the ways that the ai goals are pursued  and demonstrate that massively parallel artificial intelligence is where ai meets the real world. 
1. 	introduction 
massively parallel artificial intelligence is a field of study exploring methodologies for building intelligent systems and investigating computational models of thought and behavior that use massively parallel computing models  waltz  1  kitano et al.  1a . traditionally  artificial intelligence research has two goals: as an engineering discipline  artificial intelligence pursues methods to build useful and intelligent systems. as a scientific discipline  artificial intelligence aims at understanding the computational mechanisms of thought and behavior. i believe that massively parallel artificial intelligence will be the central pillar in the ultimate success of artificial intelligence in both engineering and scientific goals. 
	massively 	parallel 	artificial 	intelligence 	does 	not 
software engineering laboratory 
nec corporation 
1-1 shibaura  minato 
     tokyo 1 japan kitano ccs.mt.nec.co.jp 
merely speed up traditional ai models. in fact  many traditional models are not appropriate for massively parallel implementation. vast majorities of the ai models proposed so far are strongly influenced by the performance of existing von neumann architectures. in a narrow sense  a von neumann architecture can be represented by a single processor architecture with a memory to store instructions and data. even in the early 1s  available computing power for most ai researchers was far less than that for personal computers in 1  and hopelessly slower than advanced risc-based workstations. additionally  memory space has increased drastically in 1 years. 
   within the hardware constraints to date  sequential rule application  for example  has been an optimal implementation strategy. while i agree that this idea is not merely an implementation strategy  in fact there are numbers of cognitive bases for sequential rule application   hardware constraints have prevented ai researchers from seriously investigating and experimenting on other  more computationally demanding approaches. for example  the memory-based reasoning model could not have been experimentally supported  using only the computing power and memory space available in the early 1s. genetic algorithms and neural networks could not be seriously investigated without a major leap in computing power. 
   one might argue that if a program were run for  say  a month  experimental results could have been obtained even in the early 1s. however  in practice  dominating computational resources for such a substantial period of time would be impossible for the vast majority of researchers  and painfully long turn around time for experiments can be a major discouraging factor in promoting research activities based on computationally demanding paradigms. 
   this hardware factor inevitably guided  consciously or subconsciously  ai in a particular direction. although the issues and approach may vary  several researchers have been expressing concern over the influence of existing computer architectures on our models of thought. david waltz stated: 
the methods and perspective of ai have been dramatically skewed by the existence of the common digital computer  sometime called the 
von neumann machine  and ultimately  ai will have to be based on ideas and hardware quite different from what is currently central to it. waltz  1  
and proposed memory-based reasoning. rodney brooks noted in the his 1 computers and thought lecture 
the state of computer architecture has been a strong influence on our models of thought. the von neumann model of computation 
　　has lead artificial intelligence in particular directionspbrooks  1l  and proposed the subsumption architecture. 
　therefore  it is critically important to understand the nature of the computational resources which are available today and will be available in the near future. i argue that massively parallel artificial intelligence will add a new dimension to our models of thought and to the approaches used in building intelligent systems. in the next section  the state-of-the-art in computing technology will be reviewed and the inevitability of massive parallelism will be discussed. 
1. 	massively parallel computers 
advancements in hardware technologies have always been a powerful driving force in promoting new challenges. at the same time  available hardware performance and architectures have been constraints for models of intelligence. this section reviews progress in device technologies and architecture  in order to give an idea of the computing power and memory space which could be available to ai researchers today. 
1. 	devices 
device technology progress is fast and steady. memory chips  dram  capacity increases at a 1% a year rate. 1m dram will be available shortly  and successful experimental results have been reported for a  design rule. figure 1 shows how this trend would continue into the future. 
　microprocessor performance increases at the rate of 1 to 1% a year. in 1  engineers at intel corporation predicted that processors in the year 1  micro1  would contain over 1 million transistors per square inch and run with a 1mhz clock gelsinger et al.  1 . the micro1 would contain four 1 mips cpus run in parallel and achieves 1 mips. in 1  they have commented that their prediction was too modest! this implies the possibility of achieving low cost  high performance computing devices. thus  some of the research on current massively parallel computers may be conducted on workstations in the future. 
　on the other hand  uniprocessor supercomputer performance is improving at a substantially lower rate 
1 	awards 

figure 1: memory capacity for mass production memory chips 

figure 1: peak performance for supercomputers 
than the microprocessor improvement rate. all newly announced vector supercomputers use multiprocessing technologies  so that the slow processor performance improvement rate can be compensated for  figure 1 . in addition  massively parallel machines are already competitive with current supercomputers in some areas  myczkowski and steele  1  sabot et al.  1  hord  1 . it is expected that  in the near future  the majority of high-end supercomputers will be massively parallel machines. 
　this implies that the progress in microprocessors will face a slow down stage in the future  and that an architecture for a higher level of parallelism will be introduced. this increasing availability of high performance computers  from personal computers to massively parallel supercomputers  will allow experiments on simulated massive parallelism to be made at numerous places. thus  computational constraints should no longer impose restrictions on how thought is modeled. 
　special purpose devices provide dramatic speed up for certain types of processing. for example  associative memory is a powerful device for many ai applications. it provides a compact and highly parallel pattern matcher  which can be used for various applications such as memory-based reasoning  knowledge base search  and genetic algorithms  stormon et al.  1  twardowski  1  kitano et al.  1c . the benefit of associative memory is that it can attain very high parallelism in a single chip. although only a few simple operations  such as bit pattern matching  can be accomplished  these are central operations for al. 
　there are also efforts to build high performance and large scale neural network device chips  graf et al.  1  ae and aibara  1  mead  1 . although current devices have not attained implementation of sufficiently large networks  which can cope with many real applications  there are some promising research results. for example  the use of wafer scale integration for neural networks may provide for scaling up to a reasonable size  yasunaga et al.  1 . 
1. 	architecture 
the history of massively parallel computers can be traced back to illiac-iv  bouknight et al.  1   dap  bowler and pawley  1   and mpp  batcher  1 . illiac-iv was the first simd supercomputer with 1 processors operating at a rate of 1 mips. mpp consists of 1 1-bit processors. dap ranges from 1 to 1 processors. these early machines were built particularly for scientific supercomputing. 
　however  the real turning point came with the development of the connection machine  hillis  1 . the connection machine was motivated by netl  fahlman  1  and other work on semantic networks  and was originally designed with al applications in mind. hillis noted: 
this particular application  retrieving commonsense knowledge from a semantic network  was one of the primary motivations for the design of the connection machine hillis  1 . 
the cm-1 connection machine  which is the second commercial version of the connection machine  consists of large numbers of 1-bit processors and floating point processing units  tmc  1 . various systems has been implemented  such as rule-based inference systems  blelloch  1  blelloch  1   a massively parallel assumption-based truth maintenance system  dixon and de kleer  1   a text retrieval system  stanfillet al.  1   stereo vision programs  drumheller  1   a frame-based knowledge representation system  evett et al.. 1a   heuristic search programs  evett et al.  1b   parallel search techniques  geller  1   a classifier system  robertson  1   case-base retrievers  kolodner  1  cook  1  kettler et al.  1   a motion control system  atkeson and reinkensymeyer  1   and others  see  kitano et al.  1a  for a partial list of systems developed . the newest version  cm-1  tmc  1   uses 1-bit sparc chips interconnected via a tree structured network. companies  such as maspar computer  blank  1  nickolls  1   intel  cray  ibm  ncr  fujitsu  nec and others  have started development projects for massively parallel computers  and some of these are already on the market. 

the block diagram of ixm1 associative memory processor 
figure 1: ixm-1 
　although most massively parallel machines developed so far employ distributed memory architectures  there have been some attempts to build a shared memory machine. ksr-1  ksr  1  by kendall square research is an attempt to overcome the difficulties in software development which often arise on distributed memory machines. ksr-1 employs the allcache architecture  which is a virtual shared memory  vsm  model. in a vsm model  a physical architecture resembles that of a distributed memory architecture  but local memories are virtually shared by other processors  so that programmers do not have to worry about the physical location of data. 
　independently from these commercial machines  several research machines have been developed at universities and research institutions. these projects include the j-machine  dally et al.  1  at mit  snap  moldovan et al.  1  at the university of southern california  usc   and ixm-1  hieuchi et al.  1  at electrotechnical laboratory  etl . i have been involved in some of these projects  as will be described below. 
　the ixm-1 massively parallel associative memory processor was developed at etl by higuchi and his colleagues  higuchi et al  1   dcm-1 consists of 1 associative processors and 1 communication processors  figure 1 . the associative processor is organized using a t1 transputer  inmos  1   1k words associative memory  sram  and other circuitry. due to an intensive use of associative memory  ixm-1 exhibits 1k parallelism for bit pattern matching. ixm-1 is now being used for various research projects at etl  carnegie mellon university  and the atr interpreting telephony 
research laboratory. the application domains range through genetic algorithms  knowledge-base search  and example-based machine translation. 

figure 1: snap-1 
　the semantic network array processor  snap  is now being developed by professor dan moldovan's team at the university of southern california. a snap-1 prototype has been developed and has been operational since the summer of 1. snap-1 consists of 1 clusters interconnected via a modified hypercube  figure 1 . five tms1 processors form a cluster  which store up to 1 semantic network nodes. processors within a cluster communicate through a multi-port memory. several systems have been implemented on snap  such as the dmsnap machine translation system  kitano et al.  1b   the pass parallel speech processing system  chung et al.  1   and some classification algorithms  kim and moldovan  1 . 
　these trends in hardware and architectures will enable massive computing power and memory space to be available for ai research. 
1. wafer-scale integration for memory-based reasoning 
wsi-mbr is a wafer-scale integration  wsi  project designed for memory-based reasoning. wsi is the stateof-the-art in vlsi fabrication technology  and has been applied to various domains such as neural networks  yasunaga et al.  1 . wsi fabricates one large vlsibased system on a wafer as opposed to conventional vlsi production which fabricates over 1 chips from one wafer. the advantage of wsi is in its size  high integration level   performance  cost  and reliability: 
size: wsi is compact because nearly all circuits necessary for the system are fabricated on a single wafer. 
performance: wsi has a substantial performance advantage because it minimizes wiring length. 
cost: wsi is cost effective because it minimizes the requirement for using expensive assembly lines. 
reliability: wsi is reliable because it eliminates the bonding process which is the major cause of circuit malfunctions. 
　however  there is one big problem in wsi fabrication: defects. in conventional vlsi fabrication  one wafer consists of over 1 chips. typically  there are certain percentages of defective chips. traditionally  chips with defects have been simply discarded and the chips without defects have been used. to estimate the faults in 
1 	awards 
a wsi chip  we can use the seeds model seeds  1 :  where y is a yield of the wafer  d is the fault density which is  in the fabrication process being used  about 1 fault per cm1  and a is the chip area. this is a reasonable rate for the current fabrication process. however  even this level of fault would cause fatal problems for an attempt to build  for example  an entire ibm 1 on one wafer. unless sophisticated defect-control mechanisms and robust circuits are used  a single defect could collapse an entire operation. but  redundant circuits diminish the benefits of the wsi. this trade-off has not been solved. 
　mbr is ideal for wsi  and avoids the defect problem because it does not rely upon any single data unit. wsi-mbr is a digital/analog hybrid wsi specialized for memory-based reasoning. the digital/analog hybrid approach has been used in order to increase parallelism and improve performance. in the digital computing circuit  a floating point processor part takes up the most of chip area. on the other hand  the analog circuit requires only a fraction of the area for implementation of equivalent floating point operation circuits and drastic speed up can be attained. for detailed arguments on the relative advantages of analog and digital circuits  see  kitano and yasunaga  1 . use of the less area-demanding analog approach provides two major advantages over the digital approach:  1  increased parallelism  and  1  speed up due to relaxed wiring constraints  critical paths and wire width . expected performance with the current design is 1tflops on a single 1 inch wafer. using wafer stack and micro-pin technologies  peta-flops on a few hundred million record system would be technically feasible! 
1. 	grand challenge ai applications 
from the engineering point of view  the ultimate success in artificial intelligence can be measured by the economic and social impact of the systems deployed in the real-world. applications with significant economic and social impacts provide us with some grand challenge ai applications. 
　grand challenge ai applications are applications with significant social  economic  engineering  and scientific impacts. the term grand challenge was defined as 
a fundamental problem in science or engineering  with broad economic and scientific impact  whose solution will require the application of high-performance computing resources. 
in the u.s. high performance computing act of 1. thus grand challenge ai applications are ai applications with significant social  scientific  and engineering impacts. typical examples are human genome project  nrc  1  and speech-to-speech translation system projects. two workshops have been organized to discuss grand challenge ai applications. 
in february 1  nsf sponsored the workshop on 
high performance computing and communications for grand challenge applications: computer natural language and speech processing  and artificial intelligence. in october 1  i organized the workshop on grand challenge ai applications in tokyo - participated in by a group of leading japanese researchers kitano  1b . details on these workshops and some of the on-going grand challenge ai applications can be found in  kitano et al.  1 . 
　typical applications discussed include: a speechto-speech translation system  human genome analysis  global architecture for information access  gaia  - a highly intelligent information access system  shogi and go systems which beat meijin  grand master   intelligent robots and programs which undertake daily tasks  and intelligent vehicles. there have also been discussions as to wehat technologies are needed to support grand challenge ai applications. the conclusions from both workshops are surprisingly similar. the common thread was the need for massive computing power  massive memory storage  massive data resource  and ultra high band-width communication networks. 
　for a grand challenge to succeed  our computer systems must be able to be scaled up. for example  natural language systems must be scaled up to the point where a dictionary of the system contains all common words and many domain-specific terms  and to where grammar rules cover most syntactic structures. for any serious machine translation system  this means that the dictionary contains from half a million to a few million word entries  and the grammar rules amount to over 1. 
　in addition  for systems to be scaled up  and to be deployed in the real world  they must be able to cope with a noisy environment and incomplete data resources. 
1. 	the bottlenecks 
this section discusses limitations of  the traditional ai approach.  the traditional ai approach can be characterized by several salient features: 
formal representations: most traditional models use rigid and formal knowledge representation schemes. thus  all knowledge must be explicitly represented in order for the system to use that knowledge. there is no implicit knowledge in the system. 
rule driven inferencing: reasoning is generally driven by rules or principles  which are abstract and generalize knowledge on how to manipulate specific knowledge. 
strong methods: since the system depends on explicit knowledge and rules  domain theory must be understood in order to build any system based on the traditional approach. 
hand-crafted knowledge bases: knowledge 
and rules have been hand-coded at extensive labor costs. in many cases  coding must be carried out by experts in the field. 

figure 1: approaches for building intelligent systems 
　in essence  these are derived from the physical symbol system hypothesis and the heuristic search. as dave waltz stated: 
for thirty years  virtually all ai paradigms were based on variants of what herbert simon and allen newell have presented as physical symbol systems and heuristic search hypotheses. 
the fundamental assumption in the traditional approach is that experts know the necessary knowledge regarding the problem domain  and that expert knowledge can be explicitly written using formal representations. toy problems  such as the blocks world and the tower of hanoi  meet this condition. and thus  historically  many ai research efforts have been carried out on domains such as the blocks world and the tower of hanoi. the intention of such work is that by proving the effectiveness of the method using small and tractable tasks the method can be applied to real-world problems. to rephrase  most research has been carried out in such a way that researchers develop a highly intelligent system in a very restricted domain. researchers hoped that such systems could be scaled up with larger funding and increased effort  figure 1: independently  brooks and kanade uses similar figures . 
　however  experiences in expert systems  machine translation systems  and other knowledge-based systems indicate that scaling up is extremely difficult for many of the prototypes. for example  it is relatively easy to build a machine translation system  which translates a few very complex sentences. but  it would be far more difficult to build a machine translation system which correctly translates thousands of sentences of medium complexity. japanese companies have invested a decade of time and a serious amount of labor inputs to developing commercial machine translation systems  based on traditional models of language and intelligence. so far  a report on high quality machine translation systems has not been published. 
　there are several reasons why the traditional ai approach fails in the real-world. the basic assumption is that knowledge exists somewhere in the mind of experts  so that  if it can be written down in operational form - the expertise can be transferred to the system. however  this assumption does not stand in the real-world. the following three factors prevent the traditional ai approach from real-world deployment. 
incompleteness: it is almost impossible to obtain a complete set of knowledge for a given problem domain. experts themselves may not have the knowledge  or the knowledge may be tacit so that it can not be expressed in a formal manner. thus  a certain portion of the knowledge is always absent. 
incorrectness: there is no guarantee that expert knowledge is always correct and that encoding is perfect. a large knowledge base  which contains over 1 frames  must include a few errors. if there were a 1% error rate  there would be over 1 incorrect frames in a million frame knowledgebase! 
inconsistenet: the set of knowledge represented may be inconsistent  because  1  contextual factors to maintain consistency were ignored  or  1  expert knowledge is inconsistent. 
　these realities in regard to real world data are devastating to the traditional ai approach. moreover  in addition to these problems  there are other problems  such as: 
human bias: the way knowledge is expressed is inevitably influenced by available domain theory. when the domain theory is false  the whole effort can fail. 
tractability: the system will be increasingly intractable  as it scales up  due to complex interaction between piece wise rules. 
economics: when rules are extracted from experts  system development is a labor intensive task. for example  even if mcc's cyc  lenat and guha  1  system provides a certain level of robustness for knowledge-based systems  proliferation of such systems would be limited due to high development cost. 
　empirically  efforts to eradicate these problems have not been successful. in essence  ai theories for the realworld must assume data resources to be inconsistent  incomplete  and inaccurate. 
　lenat and feigenbaum pointed out the scaling up problem for expert systems  and proposed the breadth hypothesis  bh   lenat and feigenbaum  1  and the 
cyc project  lenat and guha  1 . while there is some truth in the bh  whether or not the robustness can be attained by a pure symbolic approach is open to a question. a series of very large knowledge based 
1 	awards 
systems projects  such as cyc  lenat and guha  1   knowledge-based machine translation  kbmt   goodman and nirenberg  1l   corporate-wide cbr system  kitano et al.  1   and large-scale cbr systems  kettler et al.  1  will be important test-beds for this approach. 
　for these systems to succeed  i believe that incorporation of mechanisms to handle messy real world data resources will be necessary. 
1. 	computing  memory  and modeling 
one promising approach for building real world ai applications is to exploit the maximum use of massively parallel computing power and data resources. in essence  i argue that an approach emphasizing massive computing power  massive data resources  and sophisticated modeling will play a central role in building grand challenge ai applications. 
computing: the importance of computing power can be represented by the deep-thought chess machine hsu  1  hsu et al.  1  hsu  1 . deep-thought demonstrates the power of computing for playing chess. it was once believed that a strong heuristic approach was the way to build the grand master level chess machine. however  the history of chess machines indicates that computing power and chess machine strength have almost direct co-relation  figure 1: reproduced based on  hsu et al.  1  . deep-thought-ii consists of 1 processors computing over a billion moves per second. it is expected to beat a grand master. deep-thought exemplifies the significance of the massive computing. similarly  real-time processing  using a very large knowledge source requires massive computing power  evett et al.  1a  evett et al.  1b  geller  1 . 
memory: the need for memory can be argued from a series of successes achieved in memory-based reasoning. starting from the initial success of mbrtalk stanfill and waltz  1   memory-based reasoning has been applied to various domains  such as protein structure prediction  zhang et al.  
1   machine translation  sato and nagao  1  sumita and iida  1  furuse and iida  1  kitano and higuchi  1a  kitano. 1   census classificationfcreecy et al.  1   parsing kitano and higuchi  1b  kitano et al.  1b   and weather forecasting. pace  a census classification system  attained 1% classification accuracy for occupation codes and 1% classification accuracy for industry codes  creecy et al  1 . the aicos expert system attained only 1% for the occupation codes and 1% for industry codes. these successes can be attributed to the superiority of the approach which places memory as the basis for intelligence  rather than fragile hand-crafted rules. in a memorybased reasoning system  the quality of the solution depends upon the amount of data collected. figure 1 shows the general relationship between the 


figure 1: progress of chess machines 

figure 1: memory and accuracy 
amount of data and solution quality. the success of memory-based reasoning demonstrates the significance of a massive memory or data-stream. 
modeling: the importance of modeling can be discussed using the sphinx speech recognition system lee  1 . using massive computing and a massive data-stream is not sufficient to build such artificial intelligence systems. the critical issue is how the application domain is modeled. figure 1 shows the improvement in recognition rate  with modeling sophistication. even if massively parallel machines and large data resources are used  if the modeling is not appropriate  only a poor result can be expected. sphinx exemplifies the significance of modeling. 
　there are several reasons that these factors are important  the following analysis may help in understanding the effectiveness of the approach. 
distribution many of the phenomena that ai tries to deal with are artifacts determined by human beings  such as natural languages  society  and engineering systems. however  it is acknowledged that even these phenomena follow basic statistical principles  as applied in nature. normal distribution  also called gaussian distribution  and poisson distribution are important distributions. in physics  quantum statistics  such as bose-einstein statistics and fermi-dirac statistics  and a classical statistics  maxwell-boltzmann statistics  are used. ai  however  is 

figure 1: modeling and accuracy 
not mature enough to establish statistics systems for various phenomena. however  using statistical ideas  even in a primitive manner  can greatly help to understand the nature of many phenomena. 
　zipf's law  for example  describes distributions of types of events in various activities. in zipf's law  the multiplication of the rank of an event  r  and the frequency of the event  f  is kept constant  c = rf . for example  when c = 1  an event of rank 1 should have a 1% share of all events  a rank 1 event should occupy 1%  and so on  figure 1 . the sum of the top 1 events occupies only 1%. despite the fact that the occurance probability for an individual event  greater than the 1-th rank  has only a fraction of a percent  e.g. 1% for the 1-th rank   the sum of these events amounts to 1%. this law can be observed in various aspects of the world. ai models using heuristics rules will be able to cover events of high frequency relatively easily  but extending the coverage to capture irregular and less frequent events will be major disturbing factors. however  the sum of these less frequent events will occupy a significant proportion of the time  so that the system must handle these events. in fact  about 1% to 1% of grammar rules in typical commercial machine translation systems are written for specific linguistic phenomena  whose frequencies are very low. this empirical data confirms zipfs law in the natural language. memory-based reasoning is a straightforward method to cover both high frequency and low frequency events. in fact  the relationship between data size and accuracy of a mbr system and the accumulated frequency in zipf's law is surprisingly similar  figure 1 . 
the central limit theorem inaccuracy  or noise  inherent in large data resources can be mitigated using the very nature of these large data resources. assuming that inaccuracy on data is distributed following a certain distribution  not necessary gaussian   the central limit theorem indicates that the distribution will be gaussian over large data sets. a simple analysis illustrates the central limit theorem. assuming the linear model  y = where y is an observed value  is a true value  and 
is noise  the central limit theorem indicates that the e will follow a normal distribution  regardless of the original 


figure 1: zipf's law 

figure 1: zipf's law and m b r 
distribution which produces  when the expected value of is 1  the observer will get the true value. 
　in fact  adding various types of noise to memorybased reasoning systems does not cause major accuracy degradation. figure 1 shows accuracy degradation for mbrtalk  a m b r version of nettalk  sejnowski and rosenberg  1    when noise is added to the weights. 
the noise follows a uniform distribution. n% noise means that a weight can be randomly changed in the  range  where w is a weight value. the degra-
dation rate is much smaller  when larger data sets are used. 

figure 1: accracy degradation by noisy data 
1 	awards 

figure 1: overlapping noisy data 

figure 1: accracy degradation and precision 
t h e l a w of large n u m b e r s according to the law of large numbers  the peak in a data distribution will be narrower with a large number of data sets. assume that there are two close true values. data distributions may overlap due to noise. however  the law of large numbers indicates that  by collecting larger data sets  overlap can be mitigated  figure 1 .  i have not yet confirmed whether these effects can be observed in memory-based reasoning. however  the relationship between accuracy and data size implies that the law is in effect  figure 
1 .  
n o n l i n e a r b o u n d a r y a vast majority of real world phenomena are non-linear. any attempt to approximate non-linear boundaries  using unear or discrete methods  will create certain levels of inaccuracy. a simple illustration is shown in figure 1. the real boundary is shown by the  non-linear  solid line. available data points are shown by o and x. assuming that a categorization has been made on both the y and x axis as a1  a1  a1 and b1  b1  b1  the region can be expressed as  a1 and  b1 or b1  . however  there are areas which do not fit the rectangular area described by this symbolic representation. the proponents of the symbolic approach may argue that the problem can be circumvented by using fine-grained symbol systems. however  there is always an area where symbol systems and the real boundary do not fit. using an infinite number of symbols would eliminate the error - but this would no longer be a symbol system! therefore  for the non-linear boundary problems  symbol systems are inherently incomplete. 
   the use of a large number of data points and statistical computing methods  however  can better mitigate the error with significantly lower human effort. this observation has two major implications: first  it implies 

figure 1: nonlinear boundary of the problems 
that a massively parallel memory-based approach may be able to attain a certain level of competence. this will be discussed in the next section. second  it has major implications as to how to build very large knowledge-based systems. my recommendation is to use redundancy and population encoding even when the system is based on the traditional ai approach. 
1. 	speech-to-speech translation 
the idea discussed above is also effective in speech-tospeech translation systems. speech-to-speech translation is one of the major topics of the grand challenge ai applications  and its success will have major social and scientific impacts. speech-to-speech translation systems are generally composed of a speech recognition module  a translation module  and a speech synthesis module. the first group of speech-to-speech translation systems appeared in the late 1s. these include speech trans  saito and tomita  1  and dm-dialog  kitano  1a  developed at carnegie mellon university  and sl-trans  morimoto et al.  1  developed at the atr interpreting telephony research laboratories. these systems were immediately followed by a second group of systems  which included janus  waibel et al.  1  at cmu  and asura  kikui et al.  1  at atr. this section discusses limitations with the traditional ai approach for natural language processing and how massively parallel artificial intelligence can mitigate these problems. 
1. 	traditional view of natural language 
the traditional approach to natural language processing has been to rely on extensive rule application. the basic direction for the approach is to build up an internal representation  such as a parse-tree or case-frame  using a set of rules and principles. in essence  it follows the traditional approach to artificial intelligence. 
　in the early 1s  there were efforts to develop natural language systems for small and closed domains. woods developed lunar  which could answer questions about moon rocks woods et al.  1 . in the lunar system  an atn represented possible sequences of syntactic categories in the form of a transition network woods  1 . winograd developed the famous shrdlu program  which involved simple questions about the blocks world  winograd  1 . these efforts heavily involved manipulation of the world model and a procedural model for sentence analysis. the world models  grammar  and inputs were assumed to be complete. these systems are typical examples of the traditional ai approach. 
　in the mid 1s  schank proposed conceptual dependency theory  schank  1j and the conceptual information processing paradigm. his claim was that there are sets of primitives  such as atrans  ptrans  and propel  which represent individual actions  and that language understanding is a heavily semantic-driven process  where syntax plays a very limited role. schank called his approach a cognitive simulation. from the late 1s to the early 1s  there was a burst of ideas regarding conceptual information processing  mostly advocated by schank's research group at yale. these ideas are well documented in inside computer understandinglschank  1  and in dynamic memory  schank  1 . although the emphasis changed from syntax to semantics  the basic framework followed the traditional ai approach - the systems represented knowledge using primitives and used heuristics to derive a cd representation from an input sentence. again  this requires that the world model and the knowledge must be complete and hand-crafted. 
　in the mid 1s  the syntactic approach gained renewed interest. it was motivated by unification-based grammar formalisms  such as lexical-functional grammar  lfg: kaplan and bresnan  1   generalized phrase structure grammar  gpsg: gazdar et al.  1    and head-driven phrase structure grammar  hpsg: pollard and sag  1  . introduction of a powerful information manipulation operation  such as unification and well-formalized theories  resulted in some success in developing experimental systems. once again  however  these approaches were within the traditional ai approach. the frustrating fact is that the major emphasis of these theories was to determine whether or not a given sentence was grammatical. this is a bit of exaggerating  but the point is that modern linguistic theories ignore many phenomena important for building natural language systems. for example  linguistic theories typically do not explain how people understand ungrammatical sentences. most linguistic theories merely put * in front of the ungrammatical sentence. many linguistically interesting sentences  such as extremely deep center-embedded sentences  almost never appear in reality. in addition  these theories do not entail a theory on how to disambiguate interpretations  which is a major problem in natural language processing. 
　therefore  although there have been progress and changes in emphasis  these approaches are all within the traditional approach towards artificial intelligence. as i have argued in the previous section  systems based on the traditional approach inevitably face a number of problems. for example  developers of commercial machine translation consider that the following problems are inherent in their approach  which is grounded in traditional ai and nlp: 
performance: performance of most existing machine translation systems is not sufficient for real-time tasks such as speech-to-speech translation. it takes 
a few seconds to a few minutes to translate one sentence. 
scalability: current machine translation systems are difficult to scale up because their processing complexity makes the systems' behavior almost intractable. 
quality: intractability of a system's behavior combined with other factors lowers the quality of translations. 
grammar writing: by the same token  grammar writing is very difficult since a complex sentence has to be described by piecewise rules. it is a hard and time consuming task  partly due to the intractability of the system's behavior  when these rules are added into the whole system. 
　notice that the capability to cope with ungrammatical sentences is not even listed  because such a feature is not listed in the initial specification of the system. obviously  i do not intend to claim that massively parallel artificial intelligence will immediately open an avenue for high performance and robust natural language systems. the accomplishment of such specifications seems to be far in the future. however  i do argue that reliance on models which assume complete knowledge will never accomplish the goal! 
1. 	memory-based model of natural language 
the alternative to the traditional approach to natural language processing is a memory-based model. the memory-based approach to intelligence has been explicitly discussed since the early 1s. in 1  nagao proposed a model of translation based on analogy at a nato conference  later published in  nagao  1  . this model is the precursor for recent research on memory-based and example-based models of translation. nagao argued that humans translate sentences  by using similar past examples of translation. in 1  stanfill and waltz proposed a memory-based reasoning paradigm stanfill and waltz  1  stanfill and waltz  1 . the basic idea of memory-based reasoning places memory at the foundation of intelligence. it assumes that large numbers of specific events are stored in memory  and response to new events is handled by first recalling past events which are similar to the new input  and invoking actions associated with these retrieved events to handle the new input. in the same year  riesbech and martin proposed the direct memory access parsing model rie1beck and martin  1   they argued that the build-and-store approach toward parsing is incorrect  and proposed the recognize-and-record approach. the main thrust of the dmap was to view language understanding as retrieval of episodic memory. dmap can be viewed as an application of case-based reasoning to parsing. 
　despite certain differences among these models  the common thread is to view memory as the foundation of intelligence. this idea runs counter to most ai approaches which place rules or heuristics as the central thrust of reasoning. at the same time  differences be-
1 	awards 
tween models later became very important. for example  dmap  in essence  uses complex indexing and heuristics which follows the tradition of ai. thus  the weaknesses of traditional ai were also exposed when scaling up the dmap system. 
　massively parallel memory-based natural language processing directly inherits these ideas. for example  in the memory-based parsing model  parsing is viewed as a memory-search process which locates the past occurrence of similar sentences. the interpretation is built by activating past occurrences. rationales for memorybased natural language processing include: 
very large finite space: the very large finite space  vlfs  concept is critical to the memorybased approach. the most frequently raised concern with memory-based approaches to natural language processing is how these models account for the productivity of language - the ability to generate an infinite set of sentences from a finite grammar. opponents to the memory-based approach would claim that due to the productivity of language  this approach cannot cover the space of all possible sentences. however  the productivity of language is incorrect. the productivity of language ignores resource boundedness. it should be noted that only a finite number of sentences can be produced when the following conditions stand: 
condition 1: finite vocabulary 
condition 1: finite grammar rules 
condition 1: finite sentence length 
conditions 1 and 1 stand  since people only have finite vocabulary and grammar at a given time. obviously  condition 1 stands  as there is no infinite length sentence. for example  the longest sentence in one recent cnn prime news report was 1 words long. 1% of sentences were within 1 words of length. logically  natural language is a set of sentences within a very large finite space  vlfs . 
similarity: in memory-based natural language processing  input sentences are matched against all relevant data to find similar sentences in the data-base. the assumption is that similar problems have a similar solution. in fact  a series of experimental data 
on example-based machine translation indicate reasonable accuracy can be maintained for a relatively broad space. figure 1 illustrates the relationship between accuracy and normalized distance in ebmt.  this figure is reproduced based on  sumita and iida  1 .  therefore  a memory-based approach may be able to cover a real solution space with implementable numbers of examples. 
collective decision: the memory-based approach can take advantage of the redundancy of information implicit in a large data resource. an advantages of this collective decision making is the increased robustness against the inaccuracy of individual data. as has been argued previously  the central limit theorem and the law of large numbers ensure the robustness of the system. 


figure 1: normalized distance and accuracy 

figure 1: solution spaces 
real space covering: assuming that there is a set of grammar rules which covers a certain portion of natural language sentences  the grammar not only covers sentences which actually appear in the real world  but it also covers sentences which are grammatical  but also never produced in reality. an empirical study revealed that only 1% of possible sentences considered to be grammatical are actually produced. figure 1 shows how solution spaces overlap. the memory-based approach never overgenerates because the memory contains only examples of actual sentences used in the past. 
   in addition to these rationales  it should be noted that the memory-based approach keeps examples  whereas neural networks and statistical approaches do not. this difference is important because the memory-based approach will be able to capture singular events and higher order nonlinearities  while neural networks and statistical approaches often fail to capture these. for neural networks and statistical methods  the ability to capture singular and nonlinear curvature is determined by their network structure or the model. in memory-based reasoning  there are chances that dense lower order interpolation may approximate higher order nonlinear curvature. 
1. 	systems 
since 1  i have been building several massively parallel natural language systems.  dmdlalog  or d m dlalog for short  is a representative system resulting from the early work.  dmdlalog is a speech-to-speech dialog translation system between japanese and english which accepts speaker-independent  continuous speech. it has been publicly demonstrated since march 1. 
the first version of   d m d i a l o g had a vocabulary of only 1 words. it was extended to 1 words in the second version. major characteristics of  dmdlalog are: 
massively parallel c o m p u t i n g m o d e l : knowledge representation and algorithms are designed to exploit the maximum level of parallelism. 
m e m o r y - b a s e d a p p r o a c h : the knowledge-base consists of a large collection of translation examples and templates. parsing is viewed as a recall of past sentences in the memory network. 
parallel constraint marker-passing: the basic computational mechanism is marker-passing in which markers carry various information among nodes in the memory network  charniak  1  charniak  1 . this is a useful mechanism which has been studied in various applications  hendler  
1a  hendler  1b  hendler  1  hirst  1  norvig  1 . markers are not bit markers  but are structured markers which carry data structures. in addition  propagation paths for each type of marker are restricted by the types and orders of links to be traversed. 
i n t e g r a t i o n of speech and n l p : the architecture enables interaction between speech processing and natural language processing. the bottom up process provides the likelihood that a certain interpretation could be correct. the top-down processing imposes constraints and a priori probability of phoneme and word hypotheses. 
   the first version of   d m d i a l o g was strongly influenced by the idea of dmap  but substantial extensions has been made. it was a memory-based approach to natural language processing since the parsing process is to recall past sentences in the memory network. however  the focus of the system was on accomplishing speechto-speech translation. thus  the system design has been rather conservative by today's standards in the memorybased translation community. in fact    d m d i a l o g used a complicated indexing mechanism for the memory network  and markers carried feature structures when markers were propagating to a part of the network used for abstract memory. it used three abstraction levels - specific cases  generalized cases  and unification grammar  figure 1 . therefore    d m d i a l o g can be considered as a mixture of the traditional approach and a memory-based approach. 
   despite the fact that the system was not a full implementation of memory-based natural language processing paradigm    d m d i a l o g demonstrated various interesting and promising characteristics such as a high level of parallelism  simultaneous interpretation capability  and robustness against inconsistent knowledge. one of the salient features of the model is the integration of speech processing and linguistic processing. in   d m d i a l o g   activation of the network starts bottom up triggered by external inputs. the activation propagates upward in the network to the word  sentence  and discourse levels. at each level  predictions have been made which are rep-


figure 1: translation paths 

figure 1: bottom-up activation and top-down prediction 
resented by downward propagation of prediction markers with a priori probability measures. there are multiple levels of loops in the system  figure 1 . 
　however  the performance of the system on a serial machine was far from satisfactory. the parallelism was simulated on software in the first version. in addition  increasing emphasis on robustness against ill-formed inputs and the inconsistency of the knowledge-base further degraded the total performance. fortunately  the high level of parallelism in  dmdlalog enabled reimplementing the system on various massively parallel computers. 
　massively parallel implementations of versions of  dmdlalog started in the fall of 1. two different approaches have been examined. one approach was to emphasize the dmap approach using complex indexing. d m s n a p was implemented on the semantic network 
array processor  snap  to examine this approach. one other approach was to focus on memory-based parsing using simpler indexing and a large number of examples. astral was implemented on ixm-1 based on this approach. 
implementations of $ d m d i a l 1 g on snap began in 
october 1 when i started a joint research project with dan moldovan's group at the university of southern california. at that time  moldovan's group had been designing snap-1. i worked together with moldovan's to implement d m s n a p   a snap version of   d m d i a l o g . the first version was completed by the end of 1. 
d m s n a p emphasized complex indexing and dynamic 
1 	awards 
memory network modification to create discourse entities. this follows an approach taken in the case-based reasoning community  hammond  1  kolodner  1  riesbeck and schank  1 . in a sense  d m s n a p is much closer to the original dmap idea  rather than to the memory-based reasoning approach. 
　an independent research program to implement a version of $ d m d i a l 1 g on the ixm1 massively parallel associative memory processor began in march 1 with tetsuya higuchi at electrotechnical laboratory  etl . 
ixm1 is an associative memory processor designed and developed by higuchi and his colleagues at etl. it is a faithful hardware implementation of netl  fahlman  1   but using associative memory chips. astral  the ixm1 version of  dmdlalog was completed in the summer of 1. in astral  complex indexing was eliminated  and a large set of sentence templates were used as a central source of knowledge kitano and higuchi  1b . 
　these differences in emphasis have been employed to make maximum use of architectural strengths for each kind of massively parallel computer. comparisons between different implementations of   d m d i a l o g revealed contributing factors and bottlenecks for robust speech-to-speech translation systems. d m s n a p faced a scaling problem due to reliance on complex indexing  and a performance bottleneck due to node instantiation which inevitably involves an array controller - a serial process. in addition  it is difficult to maintain multiple contexts before an interpretation is uniquely determined. on the other hand  astral exhibited desirable scaling properties and single millisecond order parsing performance. the d m s n a p project has been re-directed to place more emphasis on the memory-based approach without complex indexing and node instantiation. 
   while i have been working on massively parallel implementations and performance aspects of memory-based approach to natural language  a series of reports has been made on the quality of the translation attained by memory-based models. sato-san developed mbt-l sato  1  for word selection in japanese-english translation  and attained an accuracy of 1%. he extended the idea to the transfer phase of translation in his mbtii  sato and nagao  1 . a group at the atr interpreting telephony research laboratory developed example-based machine translation  ebmt: sumita and iida  1   and transfer-driven machine translation  tdmt: furuse and iida  1  . ebmt translates japanese phrase of type ' 'a no b  with an accuracy of 1%. tdmt applied the memory-based translation model to translate whole sentence  and attained an accuracy of 1%. since the architecture of tdmt is almost identical to the baseline architecture of  dmdlalog  the high translation accuracy of tdmt and the high performance of massively parallel versions of $ d m d i a l 1 g indicates a promising future for the approach. 
　since 1  a new joint research project has begun to implement ebmt and tdmt on various massively parallel computers. ebmt was already implemented on the ixm1  cm-1  and snap-1 machines  sumita et al.  

figure 1: asymptotic convergence 
1 . early results shows that the e b m t approach fits well with massively parallel computers and scales well. implementation of t d m t is in progress. independently  
sato has implemented m b t - i i i  sato  1  on cm-1 and ncube machines. 
1. 	l i m i t a t i o n s a n d solutions 
although i strongly believe that the memory-based approach will be a powerful approach in many application domains  there are limitations to this approach. 
	first  there is the problem of data collection. 	here  
zipf s law which was a strong supporting rationale plays a killer role. although most problems can be defined as having vlfs as their solution space  it is economically and practically infeasible to collect all solutions in a memory-base. thus  a certain part of the solution space must be left out. if solutions  which are not covered  can be derived from similar examples in the memory-base  the entire solution space can be covered. however  it is most likely that rare examples are truly irregular so that any similar examples cannot derive correct solution. for this type of problems  the only solution at this moment is to keep adding rare events to the memory-base. it should be noted  however  that this problem is not unique to a memory-based approach. in rule-based systems  rules which are specific to rare events must keep being added to a rule-base. 
　second  the memory-based approach is not free from the human bias problem. just like the traditional ai approach  the representations of example and modeling are created by a system designer. for example  current memory-based translation systems use a hand-crafted thesaurus to determine a distance between words. a series of experiments indicated that inaccuracy of thesaurus-based distance calculations is the major source of translation errors. methods to refine domain theories need to be incorporated  shavlik and towell  1  towell et al.  1 . if inappropriate representation and modeling have been adopted  the system will exhibit poor performance. even if an appropriate representation and modeling is used  it is implausible that the representation and model would be complete so that 1% accuracy could be attained. some bias in representation and modeling would be inevitable. when there is some bias  in figure 1 will not be zero. this means that even if the memory-base contains data to cover the entire solution space  certain errors     still remain. a hybrid approach has been proposed and tested in  zhang et al.  1  to overcome the problem of biased internal representations. their experiment shows certain improvements can be gained by using multiple strategies. 
　third  pure memory-based approach models form only a part of the cognitive process. although the memorybased process may plays an important roles in intelligent activities  use of rules in the human cognitive process cannot be ignored. i believe that there is a place for rules. however  it is likely to be a very different role from what has been proposed in the past. a rule should be created as the result of autonomous generalization over a large set of examples - it is not given a priori. resource bounds are the major reason for using rules in this context. there must be a cost for btoring in memory. thus  a trade-off exists between memory and rules. pinker  pinker  1  made an interesting observation on the relationship between memory and rules for english verbs. in english  the 1 most frequent verbs are irregular verbs: be  have  do  say  make  go  take  come  see  get  know  give  find. most other verbs are regular. low frequency irregular verbs are reshaped  and become regular over the centuries. pinker proposed the rule-associative-memory theory  which considers that irregular verbs are stored in the memory and that regular verbs are handled by rules. in this model  generalization takes place over a set of regular verbs. 
　finally  from the engineering viewpoint  explicit rules given a priori work as a monitor. for example  human translators use rules which define how specific terms should be translated. even for human experts  such rules are given in an explicit form. a memory-based model should be able to incorporate such reality. thus  i recently proposed a model of memory-based machine translation which combines a memory-based approach and a rule-based approach in a novel fashion  kitano  1 . 
1. 	grand breakthrough 
massively parallel artificial intelligence can be an ideal platform for the scientific and engineering research for next generation computing systems and models of thought. although discussions have been focused on the memory-based approach  even this approach involves several trails in traditional a i   which are potentially undesirable as a model of thought. massively parallel memory-based reasoning and massively parallel v l k b search use explicit symbols and do not involve strong learning and adaptation mechanisms by themselves. thus  they are inherently biased by the features and representation schemes defined by the system designer. although the problems of representation  domain knowledge  and knowledge-base building can be greatly mitigated by memory-based paradigm  they are not totally free from the problem. therefore  while there are many places where the present form of massively parallel ai can be useful  we must go beyond this for a next 
	kitano 	1 
generation paradigm to come into play. at least  it will be a significant scientific challenge. i argue that massively parallel artificial intelligence itself will provide an ideal experimental basis for the genesis of new generation technologies. in this section  therefore  i focus on scientific aspects  rather than engineering. i believe that the following properties will characterizes the realistic theory for intelligence: 
emergence: intelligence is an emergent property. it emerges from as a result of interactions between subcomponents. emergence can take place in various levels such as emergence of intelligence  emergence of structure  and emergence of rules for structure development. in this section  the term emergence is used to indicate emergence of intelligence. 
evolution: intelligence is one of the by-products of evolution. intelligence is the memory of matters since the birth of this universe. thus  a true theory of intelligence must be justified from an evolutional context. 
symbiosis: intelligence emerges as a result of symbiotic computing. the essence of intelligence is diversity and heterogeneity. symbiosis of diverse and heterogeneous components is the key to the intelligence. 
diversity: diversity is the essence of intelligence. no meaningful artifacts can be created without substantial diversity  the reality of our brain  body  and the eco-system demonstrates the significance of the diversity. 
motivation: intelligence is driven by motivation. obviously  learning is a key factor in intelligence. for learning theories to be legitimate from evolutional and psychological contexts  they must involve theories on the motivation for learning. essentially  learning and other intelligent behavior are driven to the direction which maximizes survivability of the gene. 
physics: intelligence is governed by physics. ultimately  intelligent system must be grounded on physics. nano-technology and device technology provide direct grounding  and digital physics may be able to ground the intelligence on hypothetical physics. 
1. 	emergence 
intelligence is an emergent property. for example  natural language emerged from ethological and biological interaction under a given environment. our linguistic capability is an emergent property based on our brain structure  and evolved under selectional pressure where individuals with better communication capacity survived better. 
　brooks proposed the subsumption architecture as an alternative to a sense-model-plan-action  smpa  architecture  brooks  1 . the subsumption architecture is a layered architecture in which each layer is defined in a behavior-based manner. the network consists of simple augmented finite state machines  afsm   and has no central control mechanism. a series of mobile robots based on the subsumption architecture demonstrated that a certain level of behaviors which would appear to be intelligence can emerge. 
　currently  the structure of the system is given a priori in the subsumption architecture. human bias and the problems of the physical symbol systems take place again because definitions for the units of behavior and internal circuitry for each layer must be defined by a designer. in addition  manual designing of the system which contains large numbers of layers for higher levels of intelligence is be extremely difficult. 
　the research front now has to go into the next level of emergence - the emergence of structure. structure could emerge using the principles of self-organization and evolution  jantsch  1  nicolis and prigogine  1 . self-organization is a vague notion which can be interpreted in many ways. however  since the a priori definition for a structure is infeasible  self-organization of a system's structure through dynamic interaction with an environment needs to take place. darwinm developed by edelman's group  edelman  1  edelman  1  shows some interesting properties for a self-organizing network without explicit teaching signals. however  darwin-iii is limited to synaptic weight modifications  and has no structure emergence property. 
　independently  recent studies by hasida and his colleagues propose the emergence of information flow as a result of dynamic interaction between the environment and constraints imposed orthogonally  hasida et al.  1 . although this work is still at a preliminary stage  it has an interesting idea that the system has no defined functional unit. functionality emerges through dynamic interaction with the environment. 
the concept of a holonic computer was proposed by 
shimizu  shimizu  et al.  1 . a holonic computer consists of devices which have non-linear oscillation capability  van del pol oscillator   which are claimed to be self-organizing. shimizu emphasizes an importance of holonic loop  which is a dynamic interaction of elements and top-down constraints emerged from bottom up interactions. figure 1 is a conceptual image of next generation emergent computers  needs for the limbic system and the endogenous system will be discussed later . 
　although these models have not captured the emergence of structure by themselves  combining these ideas with evolutional computing may enable the emergence of structures and the emergence of structure generating mechanisms. 
1. 	evolution 
intelligence is a by-product of evolution. species with various forms of intelligence find their niches for survival. different species could have different forms of intelligence as they have been evolved under different environment  and hence different selectional pressures. if the environment for human beings had been different from what has 


figure 1: an architecture for next generation computers  


figure 1: development using l-system 
are known to have the same origin  and to have differentiated in the course of evolution. projections from the limbic system  which is an old part of the brain  controls awake/sleep rhythms and other basic moods of the brain. these systems play a central role in building a motivation for the system. 

been imposed as the selectional pressure for mankind  the form of human intelligence could have been very different. dolphins  for example  are considered to have a comparable numbers of neurons  however  human beings and dolphins have evolved to have different kinds of brains. physiological differences of the brain inevitably affects the form of intelligence. 
   the structure of our brain is also affected by our evolutional path. obviously  brain is not a tabula rasa  not only a global structure such as the existence of a brain stem  neo-cortex  and other brain components  but also with numbers of local circuits which are genetically defined such as the hypercolumn  papez circuits  and hippocampal loops. investigation of these existing structures of brain and how the brain has evolved may provide substantial insights. 
   creating structural systems using evolutional computing  such as genetic algorithms  generally requires use of a developmental phase. while the biological theory for development has not been established  there are some useful mathematical tools to describe development - the lyndenmayer system  lindenmayer  1  lindenmayer  1  and the cellular automaton. for example  the l-system can be augmented to handle graph rewriting so that a wide range of development processes can be described  figure 1 . wilson proposed using the l-system in the context of artificial life  wilson  1   
however  i believe i was the first to actually implement and experiment with this evolution of development  kitano  1b . since the l-system is a descriptive model  and not a model which describes mechanisms for development  new and more biologically grounded models should be introduced. nevertheless  the encouraging results achieved in a series of experiments  kitano  1b  kitano  1a  gruau  1  demonstrate that the evolution of development is an important issue. 
　in addition  the evolutional perspective leads us to investigate the mechanism of immune systems  tonegawa  1   projections from the limbic system  and the endogenous system  mcgaugh  1 . the central nervous system  immune system  and endogenous system 
1. 	symbiosis 
symbiotic computing is an idea whereby we view an intelligent system as a symbiosis between various different kinds of subcomponents  each of which is differentiated through evolution. symbiosis has three major types: host-parasite  fusion  and network. the idea of symbiosis itself is originated in the field of biology. one of the strong proponents of symbiosis is lynn margulis. margulis considered symbiosis as an essential principle for the creation of eukaryotic cells. she claims 
some parts of eukaryotic cells resulted directly from the formation of permanent associations between organisms of different species  the evolution of symbiosis  margulis  1 . 
while the serial endosymbiosis theory  set   taylor  1  is a symbiosis theory for eukaryotic cells  which is a fusion  there are symbioses at various levels. 
   i claim that symbiosis is one of the central principles for models of thought and life. particularly  symbiosis is the key for evolution of intelligence. although genetic algorithms and other genetically-inspired models provide powerful adaptation capabilities  it would be extremely difficult and time-consuming to create highly complex structures from scratch. it may be difficult for genetic algorithms alone to create complex structures with high multimodality. symbiosis can take place at various level from cells to global multi-agent systems. at the neural level  neural symbiosis may be a useful idea to consider. 
   in the society of mind  marvin minsky postulates intelligence to emerge from the society of large numbers of agents  minsky  1 . the society of mind idea is one instance of symbiosis. it is a weak form of symbiosis  which can be categorized as a network  because individual agents retain their independency. symbiotic computing offers a more dynamic picture. in symbiotic computing  subcomponents can be merged to form a tight symbiosis or loosely coupled to form a weak symbiosis. a component can be differentiated to form several kinds of components  but could be merged again later. in the light of symbiotic computing  the society of minds and the subsumption architecture are instances  

or snapshots  of symbiotic computing. 
1. 	d i v e r s i t y 
diversity is an essential factor for intelligence. for example  in order for the symbiosis to be effective  or even for symbiosis to take place  components involved in symbiosis must have different functions. thus  the assumption of symbiosis is diversity. evolution often drives species into niches so that differentiation between species takes place. 
　intelligence is a manifestation of a highly complex and dynamic system whose components maintain a high level of diversity. it is analogous to big artifacts such as big buildings and bridges. in physics  particle physicists try to discover the unified theory. numbers of theories such as the superstring theory and the supersymmetry theor have been proposed. although these theories comes very close to the unified theory  or even if one of them is the unified theory  they never explain how a building can be built. 
　buildings are built using various different components interrelated to each other just like symbiosis. it is impossible to build any meaningful architecture using a single type of component. theories which claim that one particular mechanism can create intelligence fall into this fallacy. thus  it is wrong to claim that rules alone can be the mechanism behind intelligence. it is wrong to claim that memory alone can be the mechanism behind intelligence. 
　many models of neural networks make the same mistake. most of models are too simplified as they usually assume a single neuron type. the real brain consists of numbers of different types of neurons interconnected in a specific and perhaps genetically specified manner. in addition  most neural networks assumes electric impulses to be the sole source of inter-neuron communication. recent biological studies revealed that hormonal effects also affect the state of neurons. 
　even if mechanisms at microscopic levels can be described by relatively small numbers of principles  it is analogous to a theory of particle physics. in order for us to formulate the model of thought  we must find how these components create diverse substructures and how they interact. 
1. 	m o t i v a t i o n - genetic supervision 
no learning takes place without motivation. this is the factor which is critically lacking in much current ai research. the term motivation is used as a drive for learning. although  both conscious and subconscious drives are important  the following discussion focuses on a subconscious drive. 
　one example would illustrate the point of my discussion. infants learn to avoid dangerous things without any teaching signal from the environment. when an infant touches a very hot object  the infant reflexively removes its hand and screams. after several such experiences  the infant would learns to avoid the hot object. the question is why this infant learns to avoid hot objects rather than learning to prefer them. there must be some innate drive  which guides learning in a particular direction. learning is genetically supervised. we learn to avoid dangerous things because individuals whose learning is driven to that direction have survived better than a group of individuals whose learning is driven to prefer dangerous things. therefore  learning is genetically supervised in a particular direction which improves the reproduction rate. in addition  studies on child language acquisition seems to support the existence of an innate mechanism for selective learning. not just direction and focus  but also the learning mechanisms are determined genetically. 
　some species of bird demonstrate a form of learning called imprinting. for these birds  the first moving ob-
ject which they see after hatching will be imprinted on their brain as being their mother. imprinting is an innate learning which allows birds to quickly recognize their mother  so that they can be fed and protected well. while there are certain risks that an individual might imprint something different  evolutional choice has been made to use the particular mechanism under the given environment. 
　how knowledge should be organized and represented largely depends upon the purpose of learning. if the learning is to avoid dangerous things  features of ob-
jects such as temperature  speed  distance  and other factors could be important. in addition  reasoning mechanisms to provide quick real-time response will be used rather than accurate but slow mechanisms. the subsumption architecture captured a mechanism which is largely primitive but important for survival. thus  these mechanisms must be composed of relatively simple circuitry and provide real-time and autonomous reactions without central control. 
　the idea of reinforcement learning  watkin   1  provides a more realistic framework of learning than most traditional models of learning which assume explicit teaching signals. reinforcement learning assumes that the action module received a scalar value feedback called reward  according to their action under a certain environment. i feel this is closer to reality. the shortcoming of reinforcement learning is that the evaluation function to provide the reward signal to the action module  is given a priori. ackely and littman proposed evolutional reinforcement learning  erl: ackley and littman  1   to remedy this problem. in erl  the evaluation function co-evolves with the action module so that the individual witha better evaluation function learns more effectively. independently  edelman uses the value system to drive learning of darwin-iii to a desired direction. i call this class of models genetic supervision. a general architecture for genetic supervision should look like figure 1. 
　for genetic supervision to be grounded in the biological and evolutional context  several new factors must be considered such as the influence of hormones over mem-

figure 1: learning and evolution 
ory modulation  mcgaugh  1  and a1 and a1 neural systems. these systems have an older origin than neocortex  and control deep emotional and motivational aspects of our behavior. due to the fact these systems were established at an early stage of evolution  their signal processing-transmission capabilities are limited. generally  signals from these systems act as an analog global modulator. nevertheless  i believe that these factors play critically important roles in the next generation of intelligence theories. 
1. 	physics 
since intelligence is a result of evolution and emergence  it is governed by physics. as long as models of intelligence deal with stationary models  the significance of physics does not come into play. however  once the research is directed to more basic properties such as emergence  development  and evolution  physics cannot be ignored. for example  morphogenesis involves cell movements and cell divisions which are largely influenced by physics. morphogenesis is not necessary  or it can be significantly simplified  if there is no physics involved. however  at the same time  theories without physics inevitably assume a priori external constraints. basic configurations of gene and reproduction mechanisms could have been different if a different physics has been applied. recent discoveries from the u.s. space shuttle missions  particularly by japanese experiments in endeavor  uncovered the importance of gravitational influence in cell division and the gene repair. the semantics of the world could have been very different if a spontaneous breakdown of symmetry took place in a very different way. this argument largely assumes artificial life research to be the basis for artificial intelligence. i agree with belew that artificial life is  a constructive lower bound for artificial intelligence   belew  1 . brooks made a good argument for the embodiment of robots in the real world at a macroscopic level. the argument in this section is at a more microscopic level. my argument is that physics will be necessary to make emergence  development  and evolution to take place. it should be noted that this is one extreme  and much ai research will be carried out without physics. but  as research delves into basic mechanisms  physics will have to be considered. there are two promising directions  both approaches whcih should be pursued for physical grounding of ai and alife - nanotechnology and digital physics. 
　nano-technology can be a driving force grounding ai and alife research to actual physics. the same physics which has been applied to ourselves for billions of years can be applied to ai. this approach will result in enormous impact to society because the very definition of natural life and intelligence will be contested in the light of the state-of-the-art technologies. 
　to the contrary  digital physics will be able to create a new physics. massively parallel machines in the near future may be able to produce physics in themselves. this was one of the motivations for building the connection machine  hillis  1 . just as langton termed artificial life life-as-it-could-be  langton  1   digital physics may allow us to pursue the universe-as-it-could-
be. 
1. 	conclusion 
the arguments developed in this paper can be expressed in a single statement: massively parallel artificial intelligence is where ai meets the real world. the phrase ai meets the real world has been used in various contexts  
such as robotics is where ai meets the real world. nevertheless  the phrase has its own significance and that is why it has been repeatedly used. 
　artificial intelligence is the field of studying models of intelligence and engineering methods for building intelligent systems. the model of intelligence must face the biological reality of the brain  and intelligent systems must be deployed in the real world. the real world is where we live  and embodies vastness  complexity  noise  and other factors which are not easy to overcome. in order for the ai to succeed  powerful tools to tackle complexity and vastness of the real world phenomena must be prepared. in physics  there are powerful tools such as mathematics  particle accelerators  the hubble telescope  and other experimental and conceptual devices. biology made a leap when dna sequencing and gene recombination methods were discovered. 
　artificial intelligence  however  has been constrained by available computing resources for the last thirty years. the conceptual devices have been influenced by hardware architectures to date. this is represented by the traditional ai approach. fortunately  however  the progress of vlsi technology liberates us from the computational constraints. 
　computationally demanding and data-driven approaches  such as memory-based reasoning and genetic algorithms emerge as realistic alternatives to the traditional approach. this does not mean that the traditional approach will be eliminated. there are suitable applications for traditional models  and there are places where massively parallel approach will be suitable. thus  there will be co-habitation of different approaches. however  the emergent new approach will be a powerful method to challenge the goals of ai  which has not been accomplished so far. 
　massive computing power and memory space  along with new ideas for ai  will allow us to challenge the realities in our engineering and scientific discipline. for ai to be a hard-core science and engineering discipline  real world ai must be pursued and our arsenals for attacking the problems must be enriched. i hope that discussions developed in this paper contribute to the community making a quantum leap into the future. 
acknowledgements 
there are just too many people to thank: makoto nagao  jaime carbonell  masaru tomita  jay mcclleland  david waltz  terry sejnowski  alex waibel  kai-pu lee  
jim gellar  david goldberg  dan moldovan  tetsuya higuchi  satoshi sato  hideto tomabechi  moritoshi yasunaga  ron demara  toyoaki nishida  hitoshi iida  akira kurematsu  hiroshi okuno  koiti hasida  katashi nagao  toshio yokoi  mario tokoro  lori levin  rodney brooks  chris langton  katsu shimohara  scott fahlman  david touretzky  hideo shimazu  akihiro shibata  keiji kojima  hitoshi matsubara  eiichi osawa  eiichirou sumita  kozo oi... i cannot list them all. 
sorry if you are not in the list. jim hendler gave me various advices for the final manuscript of this paper. i owe him a lot. 
　this research is supported  in part  by national science foundation grant mip-1 and by pittsburgh supercomputing center iri-1p. 
