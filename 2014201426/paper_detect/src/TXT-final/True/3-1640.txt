
temporal-difference  td  networks are a formalism for expressing and learning grounded world knowledge in a predictive form  sutton and tanner  1 . however  not all partially observable markov decision processes can be efficiently learned with td networks. in this paper  we extend td networks by allowing the network-update process  answer network  to depend on the recent history of previous actions and observations rather than only on the most recent action and observation. we show that this extension enables the solution of a larger class of problems than can be solved by the original td networks or by historybased methods alone. in addition  we apply td networks to a problem that  while still simple  is significantly larger than has previously been considered. we show that history-extended td networks can learn much of the common-sense knowledge of an egocentric gridworld domain with a single bit of perception.
temporal-difference  td  networks are a formalism for expressing and learning grounded knowledge about dynamical systems  sutton and tanner  1 . td networks represent the state of the dynamical system as a vector of predictions about future action-observation sequences. each prediction is an estimate of the probability or expected value of some future event. for example  a prediction might estimate the probability of seeing a particular observation at the next time step. the predictions generated at each time step are thought of as  answers  to a set of  questions  asked by the td network.
　representations that encode the state of a dynamical system as a vector of predictions are known as predictive representations  littman et al.  1; jaeger  1; rosencrantz et al.  1 . predictive representations are a relatively new area of research; as a community we are answering fundamental questions about their possibilities and limitations. so far  the results have been encouraging. singh et al. have shown that one particular representation known as linear predictive state representations  or linear psrs  can represent any nth-order markov model or partially observable markov decision process  pomdp   singh et al.  1; littman et al.  1 . further  they showed that the size of the psr scales at least as well as the existing approaches; a linear psr model is at least as compact as the equivalent pomdp or nth-order markov model. td networks are a generalization of linear psrs and therefore inherit their representational power  singh  1 .
　td networks have been applied successfully to simple environments  both fully and partially observable. although td networks have the expressive power to accurately model complex partially-observable environments  we show that the existing learning algorithm is insufficient for learning some of these models.
　in this paper  we explore the classes of environment whose model could not previously be learned and we improve td networks so that they can learn models of these environments.
　in section 1 we review the td networks specification. in sections 1 and 1 we examine the class of problems that we have been unable to learn with the existing specification of td networks  augment the specification  and present new results. we present the results of applying these augmented td networks to a more complex grid-world domain in section 1. finally we conclude and discuss the direction of our future research in section 1.
1 td networks without history
in the following text we describe a specific instantiation of the original td networks specification that is instructive for understanding the details of our work. for information on the general specification of td networks we direct the reader to the original work  sutton and tanner  1 .
　the problem addressed by td networks is a general one of learning to predict aspects of the interaction between a decision making agent and its environment  a dynamical system . at each of a series of discrete time steps t  and agent takes an action at （ a and the environment responds by generating an observation ot （ o. in this work  we will consider td networks with two observations  1 and 1. the action and observation events occur in sequence  at 1 ot at ot+1 at+1 ot+1. this sequence will be called experience. we are interested in predicting not just each next observation  but more general  action-conditional functions of future experience.
　the focus of this work current is on partially observable environments-environments where the observation ot is not a sufficient statistic to make optimal predictions about future experience  ot does not uniquely identify the state of the environment . we will be using td networks to learn a model of the environment that is accurate and can be maintained over time.
　a td network is a network of nodes  each representing a single scalar prediction. the nodes are interconnected by links representing target relationships between predictions  observations  and actions. these nodes and links determine a set of questions being asked about the data and predictions  and accordingly are called the question network.
　each node on the td network is a function approximator that outputs a prediction using inputs such as the current observation  the previous action  and the predictions made at the previous time step. this computation part of the td network is thought of as providing the answers to the questions  and accordingly is called the answer network.
　figure 1 shows a typical question network. the question of node y1 at time t is 'if the next action is a1  what is the probability that the next observation ot+1 will be 1 '. similarly  node y1 asks 'if the next action is a1  what will node y1 predict at time t + 1 '. this is a desired relationship between predictions  but also a question about the data. we can unroll this interpredictive  or td  relationship to look at the extensive relationship between y1 and the data  which yields the question 'if the next two actions are a1  what is the probability that ot+1 will be 1 '.
　in a fully observable  markov  environment  it is natural for the question network to be a set of questions that are in some way interesting to the experimenter. in partiallyobservable environments the structure of the question network has additional constraints; the answers should be a sufficient statistic that accurately represents the state and can be updated as new data becomes available. although the question of how to discover a question network that expresses a minimal sufficient statistic is important  it is tangental to the focus of this work. the question networks we use are not minimal  and a network like that in figure 1 likely contains

figure 1: symmetric action-conditional question network. the network forms a symmetric tree  with a branching factor of |a|. this example has depth d = 1. some of the labels have been left out of this diagram for clarity  each of these nodes should have a label yi and each is conditioned on some action.
extraneous questions.
　formally  denotes the prediction for node i at time step t. the column vector of predictions yt =  yt1 ... ytn 1 t is updated according to a vectorvalued prediction function with modifiable parameter w:
	yt = σ wtxt 	 1 
　this prediction function corresponds to the answer network  where xt （  m is a feature vector  wt is a n 〜 m matrix of weights  and σ is the s-shaped logistic function
. the feature vector is a function of the preceding action  observation  and node values.
　in previous work on td networks  xt had a binary component for each unique combination of the current observation and previous action  one of which is 1  the rest 1   and n more for the previous node values yt 1. additionally  xt has a bias term  constant value of 1 . in our experiments we also included real valued features with the complement of the predictions from the last time step  1   yt 1   although we later found this was not necessary. the learning algorithm for each component wtij of wt is of the form
	 	 1 
where α is a step-size parameter  zti is a target for yi defined by a question network  and corresponds to whether the action condition for yi was met at time t.
1 cycle-world counterexamples
in our experimentation we found that td networks are able to solve certain  but not all of our small testing problems. the success of learning does not seem to be directly correlated with the complexity of the environment. in previous work  td networks were able to learn an accurate model of a partially-observable seven-state deterministic random walk as well as the probabilities in a fully observable stochastic random walk  sutton and tanner  1 . since that time  we have discovered that td networks are unable to learn an accurate model of certain small partially-observable deterministic environments. careful analysis has determined that the question networks that were used were sufficient to represent the appropriate model  so the issue must lie somewhere in the td network specification.
　figure 1 presents a simple example of a task and question network for which the solution is representable but not learnable by td networks without history. the cycle world consists of the four states shown on the left. the current state of the system cycles clockwise through the states. there is a single observation bit that is 1 at the top of the cycle  and 1 at all other times. on the right is a question network which asks what the observation bit will be one  two  and three time steps in the future. recall that at time t  yt is calculated as a function of  yt 1 at 1 ot wt . if we assume that the yt 1 is correct  there is a solution for the weights that will keep yt correct at each successive time step. unfortunately  yt 1 will never be correct; the solution exists but will not be found.
　to illustrate this  we must look at the question network  and remember that it specifies target values for the answer network. at the start of training  yt 1 will be likely be incorrect. there are no actions in this environment  so the current observation ot is the only useful input feature in xt. for the network to become correct  it is necessary that some sequence of questions can eventually be answered  starting or anchored only with knowledge of ot. also note that when training begins  the only node with a valid target is y1  because its target is not a prediction  but rather the grounded observable value of ot+1. as training progresses  the agent interacts with the environment and some answers will be learned  anchored only on the grounded observations. eventually  the environment will reach a point where ot = 1 and yt1 should be 1. the information that distinguishes this case from the case where ot = 1 and yt1 should be 1 lies in a correct answer for. unfortunately  the target for. in this case  the cyclic dependency between the question network and the temporal flow of information eliminates the possibility of the td network learning a correct solution.
　after considering this problem  it became apparent that it is necessary to find an augmentation to the td network specification to eliminate cyclic information dependencies. such a dependency occurs when the target zi for node i relies on yi making accurate predictions. this dependency can be eliminated by providing additional input features to the td network. this additional information acts as an anchor for the td network predictions.
　incidentally  good approximate solutions to the cycle world can be learned by td networks consisting of a single node. clearly  there is no way that a single predictive node can solve this problem perfectly  but it can achieve very low error in an unusual way. when ot = 1  the network predicts approximately .1. on the next time step  the network will multiply the previous prediction so as to predict approximately .1. on the next step it will predict .1  and then finally .1 for the step where ot will again be 1. unfortunately this solution is not stable  and continued training leads to oscillation between this sort of approximate solution and strictly predicting 1 at each step.
cycle world question network

figure 1: a counterexample for td-network learning without history. on the left is a representation of the cycle world. this environment has four states that are cycled through deterministically. on the right is the associated question network. there are no actions in this world.
	features	initial	final
　　1 1 history = 1 1 history = 1 1 history = 1 1 history = 1 1 history = 1 1 history = 1 1 history = 1 1 history = 1 1
1
	y t-1	.1
	1	.1
y t-1
	y1t-1	.1
figure 1: input vector for cycle world with 1-step history and 1 levels of predictive nodes. on the left is the definition of each feature. the first feature is the bias term. the next 1 features correspond to the 1 distinct 1-step histories
{ot 1ot 1ot}  not all are possible in this world . the final 1 features are the predictions from the previous time step. the middle vector is a sample input vector for the third state from the top of the cycle world at the start of learning. at this point  all of the predictions are at their initial value  .1. finally  the rightmost vector is the input vector for the third state when learning is complete  all of the predictions are accurate.
1 td networks with history
the cycle world is a problem in which there is a simple relationship between the observations and recent experience. methods that try to directly learn such relationships are called history-based methods. we will consider history-based methods which predict ot+1 using a different variable for each unique k-length window of history where a k-length window of history is defined as at kot k+1...at 1ot. in this case  a window of length 1 would be sufficient to uniquely identify each state of the system and thus would be able to make accurate predictions. incorporating short history into the feature vector xt of a td network should allow the td network to learn a correct solution to this problem. figure 1 shows an example of a hybrid input vector that uses 1 time steps of history and 1 predictive nodes.
　in order to test the hypothesis that incorporating history into the input vector xt of a td network would allow it to solve a greater class of problem  we used a cycle world like that in figure 1  except with six states instead of four. this size was chosen to clearly illustrate the effectiveness of different configurations of history and predictive nodes. on this problem  we tested three different methods:  1  td networks as previously specified without history   1  a simple historybased approach   1  a combination of td networks and history together. for each method we used several values for the step size parameter; the best of these was used as the performance measure for that method. for each method and step size  the network was trained for at least one million time steps. the 1-step prediction errors were averaged over the final 1 steps to produce an overall performance measure for each method.
the results  shown in figure 1  show that the simple

length of history
figure 1: performance on the 1-state cycle world of td networks extend to incorporate various lengths of history. the different lines correspond to different depths of the question network  as indicated by the numeric label.
history-based method only performed well when it had enough history to solve the problem exactly. td networks without history correspond to the data points with history length one. these performances are better than history of length one  but not as good as the td networks with history. it is also interesting to notice that the td network is able to solve the problem with a much shorter window than the history-based method alone. this illustrates that our combined algorithm is not simply using history instead of the predictive representation  but rather is leveraging the history to learn a predictive representation. it is interesting that the performance of the various combinations of history and predictive nodes do not follow a clear pattern. for example  when there are 1 predictive nodes  it appears that 1 or 1 steps of history is better than having 1 steps. we have verified that the minimum length of history required to solve the 1-state cycle world exactly with 1 predictive nodes is a 1-step history. this means that the low error seen with 1 steps of history is a case of the td network stumbling on a good approximate solution when it could not represent an exact solution  as discussed at the end of section 1 .
1 indefinite-memory problems
in the previous section  we introduced history to the td network specification in order to eliminate cyclic dependencies and increase the class of problems where solutions can be learned with td networks. there appeared to be a tradeoff between predictive levels in the question network and lengths of history. from this example alone  it may not be clear that the combined approach is superior to a history-only approach. there is a potentially large class of problems that cannot be represented with a history-only approach  but can be represented and solved by td networks. environments in this class are such that there is no finite length of history that

figure 1: an indefinite-memory problem  the four-state deterministic ring world. there are two actions in this world  next and previous. next advances in clockwise rotation while previous advances in counter-clockwise rotation. prediction methods using a finite length history will lose localization after some number of transitions back and forth between states b and c or c and d.
can uniquely identify the current state of the environment. we will refer to problems in this class as indefinite-memory problems.
　one simple example of an indefinite-memory problem is the ring world shown in figure 1. because states b  c  and d are indistinguishable  there are sequences of actions that keep the environment in that subset of states and will eventually fill a fixed-length memory with useless information. in contrast  a td network can easily represent this environment  and can never be made to forget its location in the environment.
　we applied td networks with various depths of question network and lengths of history to the 1-state ring world problem. the performance measure used was the same as in the previous experiments  except in this case averaged over 1 independent runs of 1 million time steps. the results are shown in figure 1. as the history window increases  the history-only method more closely approximates the correct solution. this improvement seems to diminish as the history window gets larger  and is further hampered by the fact that the number of unique histories grows exponentially with the length of the window. with the predictive approach  the problem is solved correctly with only 1 level of history and a predictive question network of depth 1.
　it is interesting that  provided enough time  the td network can learn a correct model of this environment without history  something which it could not do for the cycle world. especially puzzling was that these two problems seemed highly related  the cycle world seemingly even less complex than the ring world. the fact that actions have inverses in the ring world eliminates the information flow dependencies that existed in the cycle world. in the ring world  the agent can incrementally learn more and more about the environment. in early training  the agent can anchor itself when ot = 1 because this observation uniquely identifies this state. as time passes  the agent can learn accurate 1-step predictions from that anchored location. it can also learn 1-step predictions that involve leaving this position and then returning imme-

1           1           1            1            1
levels of history
figure 1: performance on the 1-state ring world as a function of length of history and depth of question network. the history method suffers from diminishing returns as size of the history window increases. learning also slows considerably because the number of unique histories that can be observed grows exponentially.
diately. this process can continue until this chaining effect has allowed the agent to make accurate predictions from all positions in the network.
1 gridworld experiments
in previous work  and so far in this paper  td networks have been applied only to abstract problems with fewer than 1 states. in this final section of the paper we present suggestive results for a gridworld environment that is an order of magnitude more complex than those previously considered. the gridworld in figure 1 shows the environment that we chose. in this environment  the agent has a single perceptual input  a single bit indicating whether there is a wall directly in front of it. the agent also has a very limited action space  it can either attempt to move forward or it can rotate 1 degrees clock-wise. we encourage the reader to consider two analogous tasks suitable for a human. first  consider sitting at a table with two buttons and a light bulb. you are told that the light bulb will turn on and off based on the buttons pressed according to some unobservable process. if that unobservable process were the map in figure 1  could a human learn this problem 
　a second interpretation of the problem which should be more familiar is navigating around a room with the lights off. at all times you can feel if you are touching a wall  otherwise nothing. what sorts of common-sense knowledge would you apply to this type of scenario  the simplest knowledge is that if you are touching a wall and you attempt to walk forward  you will still be touching a wall. you would also know that 1 consecutive 1 degree turns ends with the same observation that it started with. these are the types of knowledge that can be learned easily with a history-based approach.

figure 1: the gridworld used in the final experiment. the agent's location is represented by the triangle. the agent has a very limited perceptual space  it observes 1 if the arrow is pointing at a block  and 1 if the arrow is pointing at an open space. this agent has only two actions  forward and turn. forward will move the agent 1 space in the direction that the arrow points  if it is not blocked   while turn will rotate the arrow clock-wise 1 degrees. there are 1 unique environmental states in this world.
　there are other types of common-sense knowledge that are harder to learn with history. if you are facing wall  then you turn around  1 degrees   observe no wall and walk forward; what common-sense predictions could we make  first  we would know that turning 1 degrees again will be clear  and walking forward from there will take us back to the wall. we also know that if we rotate 1 degrees any number of times at any step of this process  the entire process remains intact  nothing changes. this concept is impossible to learn with a fixed length history. this scenario exemplifies the conceptual and practical difference between predictive representations and history-based representations. after facing a wall  turning 1 degrees and going forward  a 1-level history-based approach knows 'i am in the state described as {wall  turn  clear  turn  clear  forward  clear}'. the predictive agent has a different representation  more like 'i am in the state where{pr clear|turn turn  = 1 pr wall|turn turn forward  = 1 etc.}'.
　we ran a td network of depth 1 with a history of length 1 in this environment for more than 1 million time steps to give it the opportunity to learn about the causal structure of its environment. at the end of this time we took control of its decisions to explore particular places in the world as shown in figure 1. on the left  this figure shows the state of the environment at various points in time as the agent is controlled from state to state. on the right a representation of the agents predictions at each step is shown. these predictions show that the agent has learned a great deal about its environment.
　at t = 1 the agent is in a corner  and we can see that it knows there is a wall on its left side and a wall behind it. notice just to the right of the 'a' is a dark bar. this bar represents the prediction for turning right  going forward twice  and then turning right three more times. the fact that this bar is dark indicates that the agent correctly knows something about the nearest interior blocked cell. above the 'a'  the agent also predicts a wall and below the 'a' it predicts gray. this shows that the agent does not completely understand that

figure 1: a sample trajectory in the gridworld and a representation of the associated predictions at several time steps. the representation of the agent's predictions is laid out as a subjective action-conditional map of the area near the agent. all of the colored bars correspond to some prediction that the agent is making. for example  if the agent believes that it will observe a wall by going forward  the bar in front of the triangle of it is black. if the agent believes it will be clear  the bar is white  and if the agent is uncertain  the bar is gray.
it will be blocked if it chooses forward  turn forward  but it surely knows it will be blocked if it went forward again at the end of that sequence.
　the white bars in front of the agent indicate that it believes it will see clear if it goes forward for the next four steps  and then it will see a wall.
　at t = 1  the agent has moved along the wall and we can see that its expectation of seeing a wall ahead of it has moved closer  while to its left are all the predictions appropriate to their being an extended wall.
　at t = 1  the agent has moved along another wall. note that it knows how many steps it is from the wall ahead.
　at t = 1 the agent has turned to the right and observed that its way is blocked. this agent does not know all of the details of its world  at t = 1 we can see that the agents predictions do not reflect many of the details of the environment.
　by inspection  we can determine that much knowledge of this gridworld environment has been obtained by the agent. for example  the agent knows that if it is facing a wall and goes forward  it will always see a wall. further  when facing a wall  the agent correctly does not change its other predictions if told to go forward  it seems to know that the state is not changing. the agent appears to know that four consecutive turns should leave the set of predictions unchanged. the agent also seems to have a sense of rotational persistence  if it sees a wall  and then walks away and does a few rotations  it remembers where the wall is if navigated back to it.
　td networks can learn what we would consider to be much of the common-sense knowledge in a complex  perceptually deprived grid-world. in this case the agent learned concepts related to rotation and persistence that were grounded in primitive actions and observations.
1 conclusions and future work
we have presented a straightforward extension of td networks to incorporate the strengths of history-based methods. the combination of history-based learning and td network learning is more than putting two algorithms into one box and using the appropriate approach for a particular problem; the combined algorithm is stronger than either of its parts on their own.
　there are still many questions about td networks that deserve further attention. one question is whether there are other classes of problems that cannot be solved with the augmented td network specification. are there some types of common-sense knowledge that cannot be learned with a td network  can we learn optimal stucture for the question network instead of specifying it manually  we expect to explore these and other questions questions in future work.
acknowledgments
the authors gratefully acknowledge the ideas and encouragement they have received in this work from satinder singh  doina precup  michael littman  mark ring  eddie rafols  vadim bulitko  anna koop  and all the members of the rlai.net group. this work was supported in part by nserc and icore.
