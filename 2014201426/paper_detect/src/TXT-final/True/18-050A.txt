 
   we report on initial experiments with an implemented learning system whose inputs are images of two-dimensional shapes. the system first builds semantic network shape descriptions based on brady's smoothed local symmetry representation. it learns shape models from them using a modified version of winston's analogy program. the learning program uses only positive examples  and is capable of learning disjunctive concepts. we discuss the lcarnability of shape descriptions. 
1. introduction 
   we report on initial experiments with an implemented system that learns two-dimensional shapes from images. the system first builds semantic network descriptions of the imaged shape based on brady's smoothed local symmetry representation  brady and asada 1  ileide 1 . it learns shape models from the descriptions using a modified version of winston's ana log y program  winston 1  1  j1; winston  binford  katz  and lowry 1 . the inputs to the program are grey-scale images of real objects  such as tools  model airplanes  and model animals. the outputs of the program are production rules that constitute a procedure for recognising subsequent instances of a taught concept. 
   figure la shows the gray-scale image of  a model of  a boeing 1  figure lb shows the results of brady's smoothed local symmetries program  and figure lc shows a portion of the semantic network that is computed from them by our program. the semantic network is transformed into a set of associative triples  doyle and katz 1  and input to our learning program. the 1 generates 1 associative triples. similarly  figure 1a shows the subshapes found from the smoothed local symmetries of a tack hammer and figure 1b shows the full semantic net for this image. the tack hammer generates 1 associative triples. 
   the learning program is a modification of winston's analogy  council 1 . it is capable of learning concepts containing disjunctions. the program learns shape models using positive examples only. figure 1b shows the concept hammer that is learned from the three positive instances shown in figure 1a. 

figure 1. a. the input image  b. the smoothed local symmetries of the plane c. a portion of the hierarchical semantic network that is computed from the information m b. the full network generates 1 associative triples. 
   the novelty of our work is the ability to learn visual shape representations from real visual data. previous work has not been based on real data because such data was unavailable or too complex and unstructured for existing learning algorithms. however  recent developments 
in edge-detection  canny 1  and middle-level vision  brady and asada 1  have provided a solid base on which to build a robust vision system. using this system we can generate shape descriptions in a form amenable to learning. furthermore  although the descriptions typically comprise between fifty and three hundred assertions  various forms of abstraction keep this volume of data manageable. 

j. connell and m. brady 1 


figure 1. a. the main nmoothed local symmetries computed from the results of brady's program  h. the semantic network that is computed from the information in a. 
1. representing shape 
to describe an object it is necessary to first segment it into separate subshapes. in terms of the mathematical analysis in brady and asada 1 i  a subshape is defined as maximal with respect to smooth variations in the defining parameters. for example  the portions of fuselage in front of and behind the wings of the b1 in figure 1 are joined  but the handle and blade of a screwdriver arc perceived as separate pieces. once a part has been found  its shape is specified by three numbers: the aspect ratio  the curvature of the axis  and the change in width along the axis. 
   joins between subshapes are determined by examining the spines of the regions and the adjacency of the contour segments. a join is specified by the relative angle and sizes of the pieces  and the location of join with respect to each piece. few previous representations of shape have described subshape joins. for example  acronym  brooks 1  brooks and binford 1  specified the coordinate transformation between two joined pieces  but did not explicitly describe the join. 
   once we break the image into pieces and find the joins we must somehow represent this information. images are noisy  so it is necessary to develop representations that are stable  in the sense of being invariant under localized changes such as image noise. however  tasks involving visual representations  for example inspection  often require that programs be sensitive to fine detail. a variety of techniques for simultaneously achieving stability and sensitivity have been proposed  each expressing some aspect of hierarchical description. the underlying idea is that gross levels of a hierarchy provide a stable base for the representation  while finer levels increase sensitivity. 
figure 1. the concept hammer that is learned from the three positive instances shown above. 
a vision program needs to maintain several different representational hierarchies  including the following:   numeric values and symbolic descriptors 
   specifying a shape parameter of interest  say a measure of the elongation of a shape  by a numerical value is sensitive  but highly unstable. symbolic names that correspond to an interval of numeric values are  usually  more stable but less sensitive. our representation employs symbolic descriptors that have overlapping ranges. for example  an end which is determined to be on the borderline between blunt and sharp is declared to be both blunt and sharp. overlaps like this help to combat the quantization error introduced by encoding a continuous range as a set of discrete symbolic values. a small change in value leads to a small change in the representation. 
  structural approximations to shapes 
   marr and nishihara  proposed summarizing the lesser subparts of an object  leaving them unspecified until they are needed. for example  all airplanes have a fuselage  writh pairs of symmetrically attached wings and elevators. upon closer examination  a wing of a b1 has two attached engine pods  a dc1 has one  and an l1 none. suppressing mention of the engine subshapes  as well as summarizing the parameters that describe the shapes of the wings and fuselage  enables the descriptions of the three airplanes to closely match each other. 
   in general  larger subshapes tend to determine gross categorization  and so they tend to appear higher in the structural hierarchy. conversely  smaller subshapes tend to allow finer discrimination and occur lower in the hierarchy. the smaller subparts of a tool typically determine the specific function of the tool. for example  deciding whether a tool is an awl  a gimlet  or a phillips screw 

1 j. conneil and m. brady 
driver involves looking closely at the end of the blade; the relatively localized context of the business end of the blade is established by the grosser levels of the hierarchy  where it is recognized  for example  that the tool is not a hammer or wrench. in this way  the marr-nishihara proposal tends  hcuristically  to relate large scale geometric structure to gross functional use. 
  a-kind-of hierarchies 
   family hierarchies are ubiquitous  and apply as much to visual shape representations as to the more cognitive situations in which they were developed in artificial intelligence. acronym represents the fact that the sets of b1-sps  b1s  wide-bodied jets  jets  and aircraft  are ordered by subset inclusion. similarly  a claw hammer is a-kind-of framing hammer  which is a-kind-of hammer. in general  a subset hierarchy is a partially-ordered set  but not a tree. from the domain of tools  for example  a shingle ax is both a-kind-of ax  and a-kind-of hammer. 
1. learning 
the commonest form of inductive generalization used to learn concepts from positive examples is the drop condition heuristic idietterich and michalski 1  winston 1  page 1 . this is the method used in our program. through careful design of the representation the method has been extended to allow generalizations of intervals and structural graphs. 
   the idea behind the heuristic is that if two things belong to the same class then the differences between them must be irrelevant. accordingly  when we have a partial model of a concept and receive a new example  we modify the model by deleting all the differences between it and the example. this can be seen by comparing figure 1b with figure 1b. notice that the network in figure 1 puts very little constraint on the size or shape of the head. this is because the shapes of the heads in the examples vary widely. for instance  the heads of the first and third hammer are straight while the head of the second hammer is curved. note also that the manner in which the handle joins the head.is only loosely specified. this is because the handle is joined to the side of the head in the first two examples but to the end of the head in the third example. 
   this is a simplified explanation of the learning algorithm. the matching involved is not graph isomorphism nor is it  merely counting the number of required features an object has. rather it is a complex local matching scheme. consider using the semantic net shown in figure 1 as the model for the airplane concept. for an object to match this model  at the top level it must have three pieces which look similar to the three in the model. a piece of the example is similar to the wing model if  first of all  it has the shape specified in the network and  second  it has two things which look like engines attached to it. suppose that a certain piece has the right shape for a wing but has only one engine attached to it. at the level of the wing model the program notices that there is a discrepancy yet judges that the piece is still close enough to the description to be called a wing. when the top level of the matcher asks if the piece in question looks like a wing the answer is  yes . no mention is made of the fact that the wing is missing an engine. the difference only matters locally and is isolated from the higher levels of matching. 
   another important concern is limiting the scope of generalizations made. imagine that the program is shown a positive example that is substantially different from its current model. altering the model by the usual induction heuristics typically leads to gross over-generalization. this  in turn  runs counter to what winston  1  page 
1  has dubbed martin's law  namely: learning should proceed in small steps. therefore our program creates a new  separate model based on the new example  splitting the concept being taught into a disjunction. 
   in some cases  the disjunction will be replaced by a single model as positive examples are taught that are intermediate to the disjuncts. for example  suppose that the first example of a hammer shown to the program is a claw hammer  and that the second is a sledge hammer. the program will create a disjunction as its concept of hammer  but it will be consolidated into a single model once it has seen such examples as a mallet and ballpein hammer. 
   even though the program only generalizes a concept using an example that is structurally similar  it is sometimes deceived and must recover from over-generalization. we follow winston  and provide censors that override the offending rule. censors can be generalized and there can be disjunctive censors; in fact this is the usual case. since censors can be generalized they also have the possibility of being over-generalized. this is countered by putting censors on the censors. in general  a concept is not represented by a single model but by a group of models. there can be several positive models corresponding to the disjuncts as well as several negative non-models summarizing the exceptions to the other models. 
1. current work 
the goals of our research are not limited to learning. the work reported here forms part of the mechanic's mate project  brady  agre  braunegg  and conneil 1   which is intended to assist a handyman in generic assembly and construction tasks. the primary goal of that project is to understand the interplay between reasoning that involves tools and fasteners and representations of their shape. 
   for example  instead of learning that a certain geometric structure is called a hammer  we learn that something which has a graspable portion and a striking surface can be used as a hammer. these two functional concepts are then defined geometrically in terms of the shape representation. reasoning from function as well as from form 

allows more flexibility. for instance  faced with a hammering task  but no hammer  one might try mapping the hammer structure onto that of any available tool. a screw driver provides a good match  identifying the blade of a screw driver with the handle of the hammer  and the  assumed flat  side of the screw driver handle with the striking surface of the head of the hammer. in this way  the mechanic's mate can suggest improvisations  like using a screw driver as a hammer. 
   our initial goal was to learn shape models cast in the representation described previously. eventually  the mechanic's mate will have to learn about the non-geometric 
properties of objects: weight  material type  and the processes that use them. currently we are using katz's english interface |katz and winston 1  to tell our program such things. this is not satisfactory. instead  we hope to teach dynamic information using a robot arm and hand. 
   another area of interest is inducing structural subclasses from examples. since the subclasses that form the a-kind-of hierarchy are an important part of the shape representation  they should be learnable. however  in learning subclasses there is a danger of combinatorial explosion. learning subclasses requires a suitable similarity metric. feature-based pattern recognition systems learn subclasses as clusters in feature space  and clusters are sets that are dense with respect to the euclidean metric. part of our research in learning shape descriptions has been to determine what makes objects look similar. this suggests using the metric employed in the learning procedure to form subclasses through a process analogous to feature space clustering. this is the focus of our current work. 
1. acknowledgements 
this report describes research done at the artificial intelligence laboratory of the massachusetts institute of technology. support for the laboratory's artificial intelligence research is provided in part by the the system development foundation  the advanced research projects agency of the department of defense under office of naval research contract n1-o-1  and the office of naval research under contract number n1-c1. we thank the people who have commented on the ideas presented in this paper  particularly phil agre  steve bagley  hob berwick  hen duboulay  alan bundy  
margaret fleck  scott heide  boris katz  tomas lozanoperez  john mallery  tom mitchell  sharon salveter  dan weld  and patrick winston. 
