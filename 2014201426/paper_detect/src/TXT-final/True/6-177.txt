 
sequence alignment is an important problem in computational biology. we compare two different approaches to the problem of optimally aligning two or more character strings: bounded dynamic programming  bdp   and divide-and-conquer frontier search  dcfs . the approaches are compared in terms of time and space requirements in 1 through 1 dimensions with sequences of varying similarity and length. while bdp performs better in two and three dimensions  it consumes more time and memory than dcfs for higher-dimensional problems. 
1 	introduction and overview 
aligning multiple dna or protein sequences  known as sequence alignment  is an important problem in computational biology. sequence alignment is useful for comparing genomes and finding genes  for determining evolutionary linkage of different biological sequences  and for predicting protein sequence secondary and tertiary structure  durbin et al  1    waterman  1 . we consider two algorithms for finding an optimal alignment of two or more sequences: bounded dynamic programming  bdp   and divideand-conquer frontier search  dcfs   a best-first search algorithm. 
　dynamic programming  dp  is the traditional approach for solving sequence alignment problems. dcfs is a more recent approach  and the two methods have been compared for the problem of aligning two and three sequences  korf and zhang  1 . we extend this work by comparing the two algorithms for simultaneously aligning larger numbers of sequences  and find that dcfs is both faster and can solve larger problems  suggesting that the more traditional dp approach is not the best choice for aligning more than three sequences. 
　we begin by introducing to the multiple sequence alignment problem  and show how it can be mapped to the problem of finding a lowest-cost corner-to-corner path in a ddimensional grid. we describe the dynamic programming approach in section 1  and the dcfs algorithm in section 1. we discuss the heuristic evaluation functions we use for sequence alignment in section 1. in section 1  we explain the methodology used in comparing the two approaches  and present our results. future work is discussed in section 1  and we conclude in section 1. 
1 	sequence alignment and the grid problem 
1 pairwise sequence alignment and cost function 
two sequences are aligned by inserting gaps into each of the sequences so that each character in one sequence corresponds to a character or gap in the other. each pair of corresponding symbols  gaps or characters  in the sequences can be characterized as a match  two identical characters   substitution  two different characters   or gap  gap in one sequence but not the other . for example  given the sequences acgtacgtacgt and atgtcgtcacgt  one alignment is as follows: 
acgtacgt-acgt 
atgt-cgtcacgt 
the cost of an alignment is calculated by assigning a cost to each position of the two sequences  and then summing the costs over all positions. for example  a cost function might charge 1 units for matching characters  1 unit for a substitution  and 1 units for a gap. if this cost function is applied to the alignment above  the result is a total cost of 1  since we have two gaps  marked by a '-'   and one substitution  t for c in the second position . an optimal alignment is one that minimizes a given cost function. for these sequences and this cost function  this alignment is optimal. 
1 	multiple sequence alignment and cost function 
the problem of optimally aligning multiple sequences is a natural extension of pairwise alignment: insert gaps in the sequences such that a given cost function is minimized. for multiple alignments  we use the sum-of-pairs  sp  cost function  i.e. the cost of an alignment of multiple sequences is the sum of the cost of all induced pairwise alignments. the cost of each position is still determined by a given cost function such as that described in the previous section. as an example  consider the following alignment of three sequences: 

1  agtta1  agct-g 
1  	-gacag 
using the same cost function above  the cost of this alignment is 1  obtained by summing the costs of the pairwise alignments of sequences  1   1    1   1   and  1   1 . 
1 	the grid problem 
the sequence alignment problem can be mapped to the problem of finding a lowest-cost corner-to-corner path in a directed grid  where each dimension corresponds to one of the sequences  needleman and wunsch  1 . we associate a cost with each edge in the grid  and the cost of an alignment is the sum of the edge costs along the corresponding path. in the directed-grid problem  we are only allowed to move toward the goal  either orthogonally or diagonally. 
　in two dimensions  the directed-grid problem is to find the lowest-cost path in an x x y grid  where x and y are the lengths of the two strings. the path goes from the upper-lefthand corner to the lower-righthand corner  and the legal moves are to the right  down  or diagonally right and down. for example  for the two sequences in subsection 1  the corresponding grid and the optimal solution path are shown in figure 1. the horizontal move corresponds to the gap in the vertical sequence  while the vertical move corresponds to the gap in the horizontal sequence. diagonal moves correspond to matches or substitutions. 
　in three dimensions  we have a three-dimensional grid of size x x y x z and can move in one of seven directions from each node in the grid: to the right  along the x axis   down  along the y axis   back  along the z axis   right and down  right and back  down and back  and finally right  down  and back. in general  the k-dimensional directed-grid problem seeks a lowest-cost path in a k-dimensional grid  with 1k - 1 possible moves from each node. aligning k sequences of length / requires a grid of size lk. 

figure 1: optimal alignment of the two sequences from subsection 1. the solid line represents the solution path  and the dashed line represents a substitution. 
1 dynamic programming 
1 standard dynamic programming 
dynamic programming  dp  is a general technique that can be used to find a lowest-cost path in a directed grid. note that any optimal path passing through a node n must include an optimal path passing through one of its predecessors. to determine the lowest-cost path to a node n  we do not need to consider all possible paths to the node; rather  we only need to know the costs to reach each of n's immediate predecessors. 
　this observation yields an efficient algorithm for searching a directed grid  which we describe for the case of an x x y grid. we scan the grid from left to right and from the top to the bottom  and at each node n we store the cost of a lowestcost path from the start node to n. this cost is the minimum obtained by adding the cost of the edge from the left to the cost of the node to the left  the cost of the edge from above to the cost of the node above  and the cost of the edge from diagonally above and left to the cost of the node diagonally above and to the left. when we reach the goal node  we have the cost of an optimal path and can trace back through the grid to find the associated path. dp has both 1 x x y  time and space complexity  since we store costs for all nodes and compute the cost at each node in constant time. dp can be generalized to k dimensions  where the time and space complexity is 1 lk  for a hypercube with length /. 
1 hirschberg's divide-and-conquer method 
using a divide-and-conquer approach proposed by  hirschberg  1   the space complexity of dp is reduced from  this reduction allows much larger problems to be solved. hirschbcrg's method is best illustrated in the two-dimensional grid problem. we find a node n in the middle row of the grid that is guaranteed to be on an optimal path and then recursively solve two subproblems: finding an optimal path from the start node to n  and finding an optimal path from n to the goal node. in order to find such a node n  we first calculate the costs from left to right in each row from the top to the middle row  and then calculate costs from the right to left in each row from the bottom to the middle row. when we have forward and backward costs to reach the middle row  we add the corresponding entries for each node and take the node of minimum total cost to be node n  a node on the optimal corner-to-corner path. storing these costs requires two rows in memory: one for the top-down computation and one for the bottom-up computation  thus reducing the space requirement from  the same idea can be applied in k dimensions  reducing the space complexity from 

1 eppstein's divide-and-conquer method 
while hirschberg's algorithm is bidirectional  he also mentioned a unidirectional version of the algorithm that he attributed to eppstein  hirschberg  1   hereafter called eppstein's divide-and-conquer method. the algorithm is best ex-

plained in two dimensions. instead of maintaining two rows of costs  one for each direction of search  we proceed row by 
1 	search 

row from the start row to goal row and maintain only a single row of costs. after we pass the middle row  we maintain with each node a pointer back to its ancestor on the middle row along a lowest-cost path from the initial node. when we reach the goal state  we have a pointer to a node on the middle row that is on a lowest-cost corner-to-corner path. as in hirschberg's method  we then have two smaller path-finding problems that we solve recursively. this algorithm also has  space complexity. 
1 	bounded dynamic programming 
one limitation of both these algorithms is that they evaluate every node in the grid. bounded dynamic programming  bdp  is an extension of dp that introduces an upper bound on the solution cost. it was presented by  spouge  1  as a method of finding optimal lattice paths. given an upper bound on the total solution cost  we can prune nodes of equal or greater cost since they cannot lie on a solution path of lower cost. for each node n we compute where g n  is the cost to reach n from the initial node  and is a lower bound on the cost of reaching the goal node from node n. if this sum is equals or exceeds the upper bound  we know that n does not lie on a lower-cost solution path. when aligning sequences of equal length  an initial upper bound can be computed by directly aligning the sequences without any gaps. 
1 	iterative upper bounds 
in general  a better upper bound on the solution cost will prune more nodes. an initial bound obtained from directly aligning the sequences is usually much greater than the true cost. a better approach is to use iterative deepening  korf  1 . starting with an upper bound that is less than the optimal solution cost  the algorithm will fail to reach the goal node by pruning too many nodes. the bound is then iteratively increased  and search repeated  until the goal is reached. this strategy has been applied to bounded dynamic programming by  ukkonen  1 . 
　normally  the upper bound is increased to the lowest cost among all nodes pruned in the current iteration. we can do better by quickly running a few shallow searches with bounds likely to be below the optimal solution cost. by regressing the depths reached on the bounds used  we can estimate the solution cost  since we know the depth of the goal node. we use this predicted bound as the starting point for traditional iterative deepening  thus eliminating many iterations with bounds that are too low. this method of using bdp with iterative upper bounds is referred to as bdp-1ub  korf and zhang  1 . the principle of iterative upper bounds can be applied in conjunction with either hirschberg's or eppstein's algorithm  but in practice  eppstein's algorithm is faster and is used in the results that follow. 
　when using iterative deepening  no iterations of the algorithm that use an upper bound less than the optimal solution cost can reach the goal. on the other hand  using an upper bound greater than the optimal solution cost examines more nodes than necessary. usually we don't know the solution cost in advance  and have no choice but to use iterative deepening. however  if we knew the optimal solution cost  this would represent a perfect bound  pb . using this optimal bound with bdp is the best case scenario in terms of performance and does not depend on any particular iterative deepening scheme. we refer to this scenario as bdp-pb  and use this algorithm below when comparing performance with dcfs. 
1 	disadvantages of dynamic programming 
while bdp-1ub is easy to implement in two dimensions  extending it to three or more dimensions is much more difficult. other researchers have also noted this difficulty  zhang  1 . while we were able to implement a bdp-1ub algorithm written explicitly for three dimensions  we were not able to extend the implementation to four or more dimensions using the same approach. in higher dimensions  we used a generalized implementation that explicitly checks for legal operators at each node. this introduces a constant factor to the time complexity of dp since processing each node takes longer than it would in an implementation tailored to a specific dimension. 
　another drawback of dp is that it can be used to find lowest-cost paths in directed grids  but not in more general grids where we allow moves in all directions. the reason is that dynamic programming must know a priori which nodes are the parents of a given node. for the general shortestpath problem  we must use a best-first search algorithm  such dcfs  which is a generalization of eppstein's algorithm. 
1 	divide-and-conquer frontier search 
divide-and-conquer frontier search  dcfs  is a recent general heuristic search algorithm  korf  1 . best-first-search algorithms  such as dijkstra's algorithm  dijkstra  1  or a*  hart et al  1   normally store both a closed list of nodes which have been expanded  and an open list of nodes which have been generated but not yet expanded. dcfs stores only the open list  which corresponds to the frontier of the searched area. the size of the closed list is often much larger than the size of the open list  and dcfs reduces the space requirement of aligning k strings of length / from 1{lh  to   {lk'x   korf  1 . 
　since dcfs does not store the closed list  it is necessary to prevent the generation of nodes that have already been expanded. to do this  every node stores a list of operators to its neighbors. when a node n is expanded  the operator from each neighbor n' back to n is marked as used. therefore  when n' is expanded  used operators are not applied  and n is not regenerated. 
　unlike standard best-first search  when dcfs reaches the goal node  it cannot retrace pointers to discover the solution path  since the closed list is not saved. alternatively  storing a path with each open node would require space linear in the path length for each node. in order to recover the solution path  before starting the search  a hyperplane is chosen that divides the search space in half. every node n on the open list beyond this hyperplane stores a pointer to the node in the hyperplane that is on the optimal path to n. in two dimensions  this hyperplane is simply a row or column halfway along one of the dimensions. we choose the hyperplane to split the longest dimension of the space. when the algorithm encounters the goal node  it has a pointer to a node on the middle plane that is on the optimal solution path. the algorithm then recurses to find an optimal path from the start node to the optimal middle node  and also from the middle node to the goal node. 
　dcfs is a general heuristic search algorithm. we use divide-and-conquer frontier a* search  dcfa*  with the cost function where g n  is the cost of a path 
from the start node to node n  and h n  is a heuristic estimate of reaching the goal from node n. 
1 	heuristic evaluation functions 
an accurate heuristic evaluation function is important to limit the number of nodes visited during search. in two dimensions  we use the distance of a node from the corner-to-corner diagonal times the gap cost as a heuristic  since any solution path must return to the diagonal in order to arrive at the goal node. in general  the heuristic cost of a node at is 
times the gap cost  where 	are the 
coordinates of the goal node. 
in three dimensions  we can compute a better lower bound. 
since we are using the sum-of-pairs cost function  we can use the sum of the optimal pairwise alignments as a lowerbound heuristic. obviously  the cost of an optimal pairwise alignment is always less than or equal to the cost of any other alignment of the two strings as part of a multiple alignment. for example  an optimal alignment of three sequences is: 

the cost of this alignment is 1  since the costs of the alignment shown for the pairs  1    1    1  are 1  1  and 1 respectively. the heuristic estimate for the cost of aligning these strings is only 1  however  since the optimal pairwise alignment of the pair  1  is only 1  as shown below. 

　the heuristic evaluation of a particular node in the grid is a lower bound on the lowest-cost path from that node to the goal. this corresponds to the alignment of the sequence suffixes that correspond to that node. for a three-dimensional alignment  we precompute and store three two-dimensional matrices  one for each pair of strings. each entry of each matrix contains the cost of optimally aligning the corresponding remaining suffixes of the pair of strings. to compute the overall heuristic for any node in the cube  we sum the corresponding elements from each of the three matrices. 
　in the general case of aligning k sequences using the sumof-pairs cost  the alignment of any d sequences induced by the multiple alignment will cost at least as much as the optimal alignment of those d sequences. therefore in four or more dimensions we can also include optimal alignments of three sequences in the heuristic. while the three-dimensional alignments are more expensive to compute and occupy more space than two-dimensional alignments  including them gives 

figure 1: the five ways to partition the edges of a complete graph of four nodes  only four of which include a triangle. 
a more accurate heuristic function  resulting in fewer nodes visited during the search. 
　to guarantee that the heuristic be a lower bound  we cannot include the cost of aligning the same pair of sequences more than once. for example  the cost of a pair of sequences that are part of a three-way alignment cannot be included in another three-way alignment  or as a pairwise alignment. consider a complete graph with a node for each sequence  and an edge between every pair of nodes  representing that pair of sequences. in this graph a triangle represents a three-way alignment. we need to partition the edges of this graph into groups of single edges and triangles  without including any edge in more than one group  ideally in a way that maximizes the resulting heuristic value. 
　for example  we can partition the edges of a complete graph of four nodes  k1  into six single edges  or one triangle and three single edges. in general  we want to include as many three-way alignments as possible  in order to make the heuristic function more accurate. there are four different ways to partition the edges of k1 into one triplet and three pairs  see figure 1 . for the complete graph on five nodes  k1  there are 1 different ways to partition the nodes into two triplets and four pairs. one way of selecting which partitioning to use is to evaluate all of them and choose the one which gives the largest heuristic evaluation for the original sequences. the hope is that a larger heuristic evaluation of the start node is an indicator of larger heuristic values throughout the search. we found that evaluating all possible partitions  instead of randomly choosing one  used more time than it saved and did not yield significant savings. 
　we represent the three-dimensional heuristics with an octree  which stores only certain parts of a cube  along with enough information to generate the other parts of the cube if needed.  mcnaughton et al.  1  computing parts of this octree on demand saves a significant amount of space. we found that storing three-dimensional heuristics in an octree leads to an overall savings on hard problems because the time used to calculate the heuristics is offset by time saved during the search. 

1 	search 

time  s  nodes time  s  nodes time  s  nodes time  s  nodes time  s  nodes bdp-pb 
dcfa* 1 1 1 
1 1 1 1 1 1 
1 1 1 1 
1 1 1 1 1 
1 1 1 
1 1 table 1: average results over 1 generated problems of generalized versions of the two algorithms on 1 dimensional problems  length 1. the table displays average time in seconds  the average number of nodes expanded for dcfa*  and the average number of nodes visited for bdp-pb. 
time  s  	nodes time  s  	nodes time  s  	nodes time  s  	nodes time  s  	nodes bdp-pb 
dcfa* 	1 	1 
	1 	1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1.1 1 	= 1 	= 1 		= 1 	= 1 
table 1: average results over 1 generated problems of the generalized versions of the two algorithms on 1 dimensional problems  length 1. the table displays average time in seconds  the average number of nodes expanded for dcfa*  and the 

average number of nodes visited for bdp-pb. 
1 comparison of the approaches 
1 methodology 
we compared divide-and-conquer frontier a*  dcfa*  and bounded dynamic programming with perfect bounds  bdppb  on problems of aligning 1  1  1 and 1 sequences. our goal was to determine which algorithm could solve harder problems within our space limits. the difficulty of a problem is determined by the number  length  and similarity of the sequences. we ran the tests on a 1 ghz intel pentium 1 with 1 gb of ram  1 kb li cache  and 1 kb l1 cache. for dcfa*  we always used a fixed-size hash table of 1 million nodes which occupies 1 mb of memory. we tested both algorithms on both randomly-generated sequences and on real sequences from balibase  thomson et al  1 . 
　for the randomly-generated sequences  we varied sequence similarity to see how it affected the performance of the algorithms. we first generated a reference sequence uniformly from an alphabet of four letters  simulating dna sequences  and then generated the actual sequences from the reference sequence. each character in the actual sequences was generated independently  with a probability of matching the corresponding character in the reference sequence  and probability of being randomly generated  including the matching character. we varied from 1 in increments of 1. problem instances with = 1 are identical sequences  which are easy to align  and were run in order to get an upper bound on solvable problem size. problem instances with  = 1 are the hardest problems  since the sequences are completely random. we ran both algorithms on 1 problems for each similarity  dimension  and length  and averaged the results. we found little variability between different random trials of the same experiment. 
　for actual sequences  we used real protein sequences from the balibase database of benchmark alignments  thomson et al.1 . proteins are represented by strings from an alphabet of 1 amino acids. the pairwise cost function we used charges 1 units for a match  1 unit for a substitution  and 1 units for a gap. for multiple sequences we used the sumof-pairs cost function. for problems in four and five dimensions  there is a choice of using two-dimensional or threedimensional heuristics. we ran the algorithms using both heuristics and reported the faster of the two times. 
1 experimental results 
our previous work  korf and zhang  1  showed that dp outperforms dcfa* in two and three dimensions  with random strings. our new results corroborate these findings; in two dimensions  bdp-1ub aligns completely random sequences much faster than dcfa*. for example  for length 1 it is an average of almost 1 times faster. in three dimensions  the disparity is somewhat smaller  with dp aligning completely random sequences of length 1 an average of about 1 times faster. dcfa* exceeds our one hour time limit for larger problems. 
　in four and five dimensions  however  dcfa* was able to solve much larger problems than bdp  due to memory constraints. dp allocates memory based on problem size. in four dimensions  it was able to align sequences up to length 1  independent of the similarity of the sequences. in contrast  dcfa* was able to align four completely random sequences of length 1 in about 1 minutes. 
　furthermore  for problems that both algorithms could solve  dcfa* was significantly faster than bdp. in these experiments we compared dcfa* with bdp-pb  where we use the optimal solution cost as our initial upper bound. in a realistic setting where this bound is not known a priori  bdp would fare even worse in comparison. the average results of aligning four sequences of length 1 arc shown in table 1. as the sequences become more similar  the performance advantage of dcfa* increases. 
　in five dimensions  bdp-pb was able to solve problems of length 1  whereas dcfa* was able to align completely random sequences of length 1 in about 1 minutes  and could align some random sequences of length 1. both algorithms are compared on problems of aligning five sequences of length 1 in table 1. comparing both tables  we see the relative performance of bdp-pb on random problems decreases when moving from four to five sequences. while bdp-pb takes about 1% longer than dcfa* to align four completely random sequences of length 1  it takes about 

| sequence set #seq. min. seq. length max. seq. length sol. cost bdp-pb dcfa luky refl  1 1 1 1 1 1 1grs refl  1 1 1 1 1 1 1 lzin refl  1 1 1 1 1 1 1hsda refl  1 1 1 1 1 1 lajsa ref1  1 1 1 1 1 1 lpama ref1  1 1 1 1 * 1 1c refl  1 1 1 1 1 1 lplc refl  1 1 1 1 1 1 1rnt refl  1 1 1 1 * 1 1mhr refl  1 1 1 1 * 1 1ptp refl  1 1 1 1 * 1 lton refl  1 1 1 1 * 1 1cba  ref1  1 1 1 1 * 1 kinase  ref 1  1 1 1 1 * 1 glg  ref1  1 1 1 1 * 1 table 1: results of the generalized versions of dcfa* and bdp-pb for 1 and 1 dimensional real protein sequence sets  with minimum and maximum sequence lengths for each set indicated. times are in seconds. problems unsolvable by bdp-pb  due 

to memory limitations  are denoted by a '*'. 
1 times longer when aligning five completely random sequences of length 1. 
　our results in four and five dimensions were obtained using dimension-independent implementations of both algorithms in four and five dimensions. we were not able to test performance of hand-tailored versions for four and five dimensions due to the difficulties discussed in subsection 1. while the overhead of these dimension-independent implementations contributes to the disparity in speed between the two algorithms  the space requirement of bdp-pb is the more serious drawback. we believe bdp-pb runs slower in four and five dimensions because bounds checking becomes more complicated and cache performance decreases. the bookkeeping necessary to maintain bounded regions becomes significant  as reflected by the nearly uniform performance for five-dimensional problems. 
　dcfa* can run larger problems in higher dimensions because the amount of memory required by bdp-pb depends on problem size and dimension. much of this memory will not be used since many nodes lie outside the bounds  and will never be visited. the problem gets worse as the sequences become more similar  and as the dimensionality increases. dcfa*  on the other hand  allocates a fixed-size hash table  allowing it to align longer sequences as they become more similar. while dcfa* uses memory more efficiently than bdp-pb  both algorithms are limited by space  with the programs filling the available memory in less than an hour. 
　performance of the algorithms on real sequence sets is presented in table 1. as for randomly-generated sequences  dcfa* is faster than bdp-pb for four and five dimensional problems. a number of the real sequence sets were solvable by dcfa* but not bdp-pb because of memory constraints. in addition  there were some sequence sets not solvable by either algorithm. we used groups of 1 or fewer sequences from the first and third sets of balibase  refl and ref1 . neither algorithm could solve problems from the other sets  which involved very large numbers of sequences. 
1 	future work 
the most significant drawback of bdp in higher dimensions is its space requirements. one possible solution is to dynamically allocate space only for the nodes actually visited  allowing larger problems to fit in memory. combining bdp with dynamic node allocation can be viewed as a hybrid algorithm between dynamic programming and best first search. the main drawback to this hybrid method is that node processing becomes significantly more complicated to implement since there is no longer a direct mapping between a node's location in memory and its position in the grid as in traditional dynamic programming. the mapping of memory to position is an integral component of dp  and thus this hybrid algorithm is fundamentally different than standard dp. cache performance  one of the main advantages of dp  would also decrease. 
1 	conclusions 
we compared two classes of algorithms to optimally solve sequence alignment problems: bounded dynamic programming and divide-and-conquer frontier search. we determined the effects of including three-dimensional alignments in the heuristic function. we then compared divide-and-conquer frontier a*  dcfa*  to bounded dynamic programming with perfect bounds  bdp-pb  on problems of aligning two  three  four and five sequences with varying degrees of similarity. we also compared performance on real protein sequence sets. our results indicate that bdp's inefficient use of memory does not allow it to solve problems which dcfa* can solve  and in practice is significantly slower in 1 and 1 dimensions. this surprising result contrasts with findings in lower dimensions  where bdp is the method of choice. 
1 	acknowledgments 
thanks to weixiong zhang for his invaluable help in proofreading a draft of this paper  supplying us with three di-

1 	search 

mensional bdp code  and for helpful feedback throughout. 
the research was supported by nsf under grant no. e1. 
