a recognition system using probabilistic decisions 
based on extracted features 
　　　
p. murden 
e . m . i . e l e c t r o n i c s l i m i t e d 
summary 
       the paper describes a p a t t e r n r e c o g n i t i o n system t h a t has been simulated using a computer w i t h an o n - l i n e camera input. the system is a d a p t i v e   using a t r a i n i n g set of p i c t u r e s together w i t h the names or classes to which each p i c t u r e 
       belongs. the system uses an edge f o l l o w i n g t e c h nique f o r e x t r a c t i n g features from the m u l t i l e v e l i n p u t s . during the t r a i n i n g mode  some of the d e s c r i p t o r s derived from the e x t r a c t e d features are s t o r e d . also  the system b u i l d s up s t a t i s t i c s of the l i k e l i h o o d of a p i c t u r e belonging to a given class given the presence of each incoming f e a t u r e . 
       during the t e s t mode  a previously unseen set of p i c t u r e s is used and features are extracted and compared w i t h the stored d e s c r i p t o r s . a sequent i a l d e c i s i o n mechanism uses these comparisons and the l i k e l i h o o d s t a t i s t i c s to produce responses corresponding to the assessed class of the i n p u t . some p r e l i m i n a r y experimental r e s u l t s are g i v e n . 
	1 * 	i n t r o d u c t i o n 
       the paper discusses the organisation of a p a t t e r n recognition system using feature d e s c r i p t o r s s t o r e d from a t r a i n i n g set of i n p u t s . the system is adaptive when working w i t h the t r a i n i n g set and is subsequently tested w i t h a d i f f e r e n t set o f i n p u t s f o r r e c o g n i t i o n . 
       although most of the experimental t e s t s have been performed on numerical characters   f i x e d font and handwritten  the system was designed as a f a i r l y general device in order to cover a wider range o f v i s u a l p a t t e r n problems. i n i t i a l l y   t h e r e f o r e   it w i l l give a poorer performance than 
a s p e c i a l purpose character recogniser; however it should have greater p o t e n t i a l as a f a i r l y gene r a l p a t t e r n recogniser. 
       it was proposed that the system should d e t e r mine the best f e a t u r e sizes and shapes f o r the p a t t e r n r e c o g n i t i o n task being performed and would be capable of adapting i t s h i e r a r c h i c a l s t r u c t u r e . the aim of the present research is to gain an understanding of how to design such systems. if t h i s can be achieved the knowledge could be appl i e d t o designing s p e c i f i c systems f o r s p e c i f i c t a s k s . 
       the system chooses i t s own f e a t u r e s   t h a t is to say u s e f u l e x t r a c t s from the i n p u t image  since man does not always know which p a r t s are l i k e l y to prove u s e f u l in r e c o g n i t i o n . thus  features are not e x t r a c t e d from f i x e d p o s i t i o n s on a p i c t u r e but are determined by the system according to the requirements of the t a s k . 
m. symons 
  hayes  middlesex  	u.k. 
the system 
 1  evaluates i t s own features from the t r a i n i n g set of p i c t u r e s   
  i i   b u i l d s up i t s own hierarchy by l i s t i n g sequences of these features i n t o groups  
  i i i   b u i l d s up a s s o c i a t i o n s between features and between groups of features to p r e d i c t usef u l areas of search w i t h i n an input p i c t u r e   
  i v   adapts parameters used by the decision mechanism to improve the performance of the dec i s i o n mechanism. 
　　　　the system being studied uses edge f o l l o w i n g techniques as one means of processing the input p i c t u r e f o r the purpose of e x t r a c t i n g f e a t u r e s . the term  edge f o l l o w i n g   is used d e l i b e r a t e l y r a t h e r than   c o n t o u r f o l l o w i n g   . contour f o l l o w i n g traces p o i n t s o f equal l i g h t i n t e n s i t y a s i n the techniques of ledley and ruddle   and zahn1  whereas edge f o l l o w i n g traces a path of p o i n t s of high i n t e n s i t y g r a d i e n t . i f a locus o f high i n t e n s i t y d i f f e r e n t i a l passes from a b r i g h t part of 
a p i c t u r e to a dark p a r t   the edge f o l l o w e r w i l l continue to t r a c k along the l o c u s . edge f o l l o w i n g is a u s e f u l and f l e x i b l e technique f o r data g a t h e r i n g and is w e l l s u i t e d to the more f l e x i b l e dec i s i o n system to be s t u d i e d . although edge f o l lowing on i t s own has some severe l i m i t a t i o n s   the usual problems of gaps or m u l t l l a t l o n of the edges of the input shape are overcome in a s s o c i a t i o n w i t h the proposed l e a r n i n g system by d i r e c t i n g the edge f o l l o w e r to new areas of search. s t o r i n g 
features from the edges of an object does i n c u r loss o f i n f o r m a t i o n but i t i s expected that f o r many a p p l i c a t i o n s the most u s e f u l i n f o r m a t i o n l i e s at the edges. 
　　　　an a l t e r n a t i v e method of processing the i n put p i c t u r e by a c l u s t e r i n g technique has also been considered. b r i e f l y   t h i s method used an i t e r a t i v e process w i t h a metric t h a t was a funct i o n of both euclidean distance and i n t e n s i t y d i f f e r e n c e between p i c t u r e p o i n t s   to c l u s t e r the p o i n t s i n t o a number of subsets such that the subsets were maximally dense w i t h respect to the metr i c . however  d e t a i l s o f t h i s a l t e r n a t i v e method w i l l not be presented in t h i s paper. 
　　　　the basic philosophy behind the system being developed is described.together with a more det a i l e d explanation of a s i m p l i f i e d n o n - h i e r a r c h i c a l model t h a t has been simulated on a computer as p a r t of the r e s e a r c h . there a r e   in f a c t   v a r i a n t s of t h i s simple model and the model its e l f i s being c o n t i n u a l l y modified during the course of the work. 
　　　
- 1 1 -
　　　
1. description of the non-hierarchical or single level model 
　　　as part of the development of the system  a simplified non-hierarchical version has been simulated on a d i g i t a l computer  using fortran as the programming language. this model extracts features from the input pattern and l i s t s these features. the model then issues a name response from analysis of the combination of features in this l i s t . this model can be regarded as the basis of the f i r s t level of the f u l l hierarchical system. 
　　　the input patterns are 1ox'1o matrices of picture points  each point taking one of 1k i n tensity levels  integral values of 1 to 1 . the sources of the input patterns are objects or photographs viewed by a television camera and converted into coded form by a special device   the pictures can be input by the computer either d i r ectly to the simple model or to magnetic tape. 
　　　fig.1 shows a block schematic of the simple model with an edge following technique incorporated for the purpose of extracting features. the model is adaptive  using a training set and a different recognition test set of input pictures. connections which are energised only during the learning phase are shown in fig.1 by the letter l in parenthesis. other connections may be made during both learning and recognition. 
　　　the matrix of intensity levels is input and the edge detector scans the matrix to find an edge 
of high intensity gradient. 	the edge follower then traces the edge u n t i l a feature is detected by the feature extractor. 
　　　the coded feature information from the feature extractor is transferred to the comparator where it is compared with previously stored information from the feature store. the comparator issues a measure of the degree of f i t   known as the score  and the feature store produces the name of the stored feature having the largest score. these feature names are purely arbitrary and are a l l o cated in sequence down the feature store. in the current version of the simple model  the feature store ia f i l l e d during an i n i t i a l period of the learning phase. during this period  when the largest score ia below a certain threshold  then the extracted feature ia added to the feature store as a further feature type and the next sequential feature name la allocated to i t . 
　　　during the learning phase the true class name is input from tape  in the case of tape converted pictures  or from manually operated buttona in the caae of direct camera input. 	in this phase  the true claaa name ia fed together with the issued feature name to a block which builda up atatiatlca of likelihoods of each claaa for each given feature. 
　　　during both learning and recognition phasea the feature name ia added to a feature list   i n i t i a l l y clear when a fresh picture is input  to form a current l i s t of features extracted from the input picture. 
　　　the decision mechanism uses the likelihood statistics to produce a sequence of claaa name responses from the growing number of features in the feature l i s t . 
　　　during the learning phase  the features in the feature list are fed together with their positions in the input picture to a positional matrix within the next feature predicator. the positional matrix computes the relative distances between the various combinations of features. over a number of input pictures  the positional matrix produces a number of mean values of the relative distances. 
　　　during recognition  when a sequential claea name response is produced  the decision mechanism uses information from the likelihood block to predict the most likely occurring feature  not yet found  for the expected class. 	the name of this predicted feature is fed to the next feature 
predictor  which examines the current feature list and the positional matrix  in order to direct the feature extractor to a new area of search. if the predicted feature is found  it is added to the current feature list and further predictions may take place. however  if the predicted feature is not found  the system returns to the edge follower and the next adjacent feature is extracted. 
　　　in addition to producing a class name response  the decision mechanism produces a measure of confidence of i t s response and in general this confidence level increases as the number of featured in the feature list increases. the system can stop i t s examination of the input picture either when the confidence level exceeds a certain threshold or when a l l edges in the picture have been traced. a final class name response is then produced and the system is ready for the next picture. 
　　　during both learning and recognition phases  more than one edge can be followed. furthermore  it ia not necessary to lineariae the input pattern; thus solid objects can be used aa data as well as line objects. 
　　　the following aectiona 1 to 1 explain the operation of the various mechanisms in further d e t a i l . 
　　　1. edge tracing and feature extraction 1 edge detection 
　　　the f i r s t operation ia to scan the 1 x 1o picture point matrix for an intensity gradient which might correspond to the edge of some shape in the picture. i n i t i a l l y 1 random picture 
　　　
-1-
　　　
points out of the total population of 1 are sampled to obtain an estimate of the standard deviation of the intensity of the points. 
　　　the picture is then examined by a window which consists of a horizontal s l i t   six elements long and one element wide  which scans the picture in horizontal sweeps similar to a t.v. l i n e scan. successive positions of the window overlap so that the central cell of the window moves through the entire set of picture points except those at the extreme edge. at each position of the s l i t the right-moot element is tested to determine whether it differs by a significant amount from the local mean of the six. this amount is set by the value of a quantity c.s  where s is the standard deviation and c is a constant; the latter value determined by t r i a l and error in this instance  but ideally adjustable in the complete system as a function of the performance. when a significant element is found an additional test is made. one of the three elements immediately below this f i r s t point must also be found significant by the same test before the point may be accepted. figure 1 illustrates this process with s l i t a failing to find an edge and s l i t b finding an edge at element with value 
　　　as soon as such a significant point is found  i t s co-ordinates are stored together with an i n dication as to whether the point is brighter or darker than the local mean of the elements. 
1 	sdge following 
　　　when the edge detector finds a significant point a 1 x 1 window and a 1 x 1 window are centred on the point  fig.1 . the mean of the 1 points of the large window is calculated and then an auxiliary 1 x 1 matrix is formed  each cell of which has the value 1 or 1  fig.1a . if the significant point found by the edge detector had been brighter than the local mean then vs are placed in those cells of the auxiliary matrix corresponding to cells in the small window whose intensities are greater than the mean of the 1. the remaining cells are f i l l e d with 1's. if the significant point had been dark the vs are placed in positions corresponding to intensities less than the mean. 
　　　the next stage is to move the small window by one c e l l in one of the eight possible horizontal  vertical or diagonal directions along the edge of the shape and to repeat this process of binary quantisation. the movement consists of centralising the window on the f i r s t ' 1 ' encountered by an imaginary anti-clockwise rotating vector  centred on the auxiliary matrix and looking at the outer cells of this matrix in turn. the vector starts from that c e l l   in the auxil i a r y matrix  corresponding to the picture element that just preceded  in the anticlockwise sense  the one selected previously by the vector as containing the f i r s t  !   
　　　a new 1 x 1 window and a new 1 x 1 auxiliary matrix are formed around the new centralised posi t i o n and the process is repeated as shown in fig. km a new localised mean is derived from the 1 x 1 window to quantise the picture points of the auxiliary matrix into 1s and 1s. 
1 	feature extraction 
　　　the edge follower outputs a string of coordinates representing successive points on the edge of the shape being traced. the feature extractor measures the mean curvature of a l l strings of y consecutive picture points  t is related to the resolution of the picture  . the curvature is signed according to the direction of movement of the edge follower. hence  a l i s t of curvatures of overlapping strings is produced. whenever a string is found whose curvature differs from that of the previous feature by an amount greater than another parameter d  a feature is said to have been completed. this feature is output in terms of the co-ordinates of the f i r s t point in the f i r s t string of the feature  the last point in the last string  and the average value of the centre points of a l l the strings in the feature. 
　　　two methods of encoding the extracted feature are being 	investigated. 	the encoded descriptor in one case takes the form of a sub-array or  snapshot  of k x k elements  each element taking one of the gk possible intensity values  centred on the average value of the centre points of the feature found while edge following. 	in the second case  line segment form   the encoded descriptor contains the start and finish picture point co-ordinates of the feature found by edge following together with the mean curvature of the edge. 
1. comparison of features 
　　　when features are extracted from the input image  each feature in encoded descriptor form is fed to the comparator  where the extracted descriptor is compared with each of a number of previously stored feature descriptors. the comparator produces a measure of the degree of f i t   known as the score  between an incoming descriptor and a stored descriptor. 
　　　during an i n i t i a l part  mode 1  of the learning phase  if a l l the scores s are below a certain value  either a preset amount or an adaptable quantity   then the incoming descriptor is added to the feature store as a new descriptor and a sequential feature name is allocated to i t . 
　　　during the subsequent part  mode 1  of the learning phase and during the recognition phase  mode 1   the scoring mechanism issues the associated feature name of the stored descriptor having the largest score value s. the issued feature name is fed to a l i s t l   i n i t i a l l y empty at the start of examination of each input picture  during modes 1 and 1. 
　　　
-1-
　　　

　　　
next feature predictor to extract non-adjacent features. at present  prediction has been used only with the  snapshot ' type feature descriptor. 
　　　during the learning phase  features are extracted by edge following and no prediction takes place. nevertheless  statistical information is gathered for use by prediction in the later recognition phase. this information consists of the names of the extracted features together with the positions of their feature centres. at the end of processing each input picture  the relative distances between a l l combinations of features extracted from the picture are computed  and in a number of positional arrays  one for each class  the mean distances and the standard deviations of the distances between combinations of features occurring in each class are built up. 
　　　during the recognition phase  i n i t i a l l y three features are extracted by the edge following technique. the quantity three may be increased by changing a parameter on a data card associated with the computer program. the i n i t i a l three features are used to produce a class decision as previously explained. 
　　　now the likelihood array does not store the values of vex directly but stores the values ncx  and the vex are computed when required. thus the ncx can be used to indicate the frequency of occurrence of each feature for any given class. therefore  when the class decision is produced from the f i r s t three features  the likelihood 
array is used to predict the most likely feature not already extracted for the decided class of the current input picture. 
　　　the positional array is then inspected for the mean relative distances  and stored deviations of these distances  between the predicted feature and the three extracted features for the decided class. the expected position of the predicted feature is computed by a standard geometrical triangulation method from the three relative distances. a search area is produced about the expected position in both horizontal and vertical directions of the input picture; the size of the search area being a function of the stored deviations. the search area is further restricted to be contained within the 1o x 1 frame. a series of over-lapping sub-arrays or  snapshots1 is then extracted centred on a l l possible positions within the search area. 
　　　the extracted  snapshots1' are compared in turn with the descriptors in the feature store. if the predicted feature is not found  the system returns to the edge follower to find the next adjacent feature by the method of section 1. if the predicted feature is found  it is added to the l i s t l and using the decision mechanism of section 1  the confidence level of the class decision i n creases. 
　　　as the l i s t l increases using the edge followed or predicted features  further predictions are made using the last three features in the l i s t for locating the search area. 
　　　the predicted sequence is equivalent in some ways to the formal syntax  e.g. sharp clockwise curve followed by f a i r l y straight segment followed by  etc.  as used by ledley and ruddle1 and others. however  allowing the system to generate i t s own sequences  of not necessarily adjacent features  preserves generality and requires no foreknowledge of the syntactic formation. 
1. the stopping rules 
　　　during the learning phases  when no prediction is taking place  the process of feature extraction and decision making continues u n t i l a l l edges have been followed and then a new picture is read i n . there i s   however  a deliberate rest r i c t i o n that prevents more than 1 features being extracted from any one picture. in the experiments this l i m i t has never yet been reached. 
　　　during recognition  when prediction of features may take place  the process of feature extraction and decision making continues until either a l l edges have been followed  or 1 features have been extracted  or the confidence level of the decision mechanism exceeds 1% .whichever is the sooner. 
　　　in general  using printed or handwritten numerals as input  the 1% confidence level comes into play only when a correct decision is being made in recognition whereas  in general  incorrect decisions rarely exceed 1% confidence and are often considerably lower. however  a significant number of correct decisions l i e in the 1% to 1% confidence range. 
　　　thus if the system were allowed only to make decisions if a 1% confidence had been reached then wrong decisions would be rarely made. however  decisions would be made on a minority of occasions only. ideally  in generalised pattern recognition  a cost function should be introduced and the use of sequential decision theory  wald1&  could be used. 
　　　during the computer simulations  for each picture  the f u l l l i s t of sequential decisions was printed out together with the corresponding confidence levels. if non-decision is counted as being equally as wrong as incorrect decision  then roughly 1% confidence as a decision threshold appears to give the best overall results. improved stopping rules are being examined. 
1. experimental studies 
　　　experiments have been performed mainly with numerals 1 to 1 as test patterns. these were extracted from two sources. the f i r s t source was from 1mm negative photographs of telephone exchange meter dials. although these were fixed font  the mutilation was often very high  as shown in fig.1 which shows a  computer's eye view  of an input. the  view  was produced on a graph plotter peripheral of the computer by varying the amount 
　　　
-1-
　　　
of ink in each of the 1 x 1 picture point positions   although the computer receives intensities of one of 1 levels  for simplification the graph plotter output has been requantised more coarsely into one of 1 levels. 
　　　the other source of numerals was derived from handwritten samples from 1 people. numerals were written in ink  biro  pencil or red crayon. 
　　　in a l l tests the recognition set was made up from examples that had never been seen during the 
learning period. 
　　　with the sub-array or  snapshot'' type feature descriptor it was found that enlarging the array dimensions improved the 	results. 	however  computer storage limitations restricted the array to 1 x 1 in size and thus the largest feature size used in the present tests was 1 x   . 
　　　the best preliminary results achieved with the single level model using the numerals from the telephone meter photographs were 1% correct. the simple model then had a stored total of 1 sub-array features which it had extracted from one example of each of the ten classes of numerals. four examples of each class were used for updating the likelihood statistics and four examples of each class were used for updating the likelihood statistics and four examples of each class were used in the recognition set. 
　　　the recognition set included the example of fig.1 which the system did not identify correctly.  but since the  correct  answer had been ruled to be a 1  it is debatable whether this can be counted as a true 	failure . 	no attempt  however  was made to select or remove any of the scanned examples from the input tape and thus the results may appear to be unfairly low. 
　　　with handwritten numerals  again one example of each numeral was used for feature storage giving a t o t a l of 1 stored features. ten examples of each numeral were used for updating statistics. using a recognition set of numerals from people who had not had examples of their handwriting used during the learning phase gave only a 1% success rate. however  when examples not previously used by the model  but nevertheless from people who had supplied examples for the learning sett were then used as the recognition set of the success rate rose to 1%. these results are not unexpected and indicate the requirement for much further teaching of the complete system to provide the increased generalising a b i l i t y needed for more d i f f i c u l t tasks. 
　　　with the line segment type feature  instead of the sub-array or  snapshot  type descriptor  and using the telephone meter photographs  the best results achieved were 1% correct recognitions. 
1* the hierarchical system 
　　　a block schematic is shown in fig.1 which indicates how the simplified model may be extended into an hierarchical system. 
　　　at the f i r s t level  features are extracted from the input 	picture. 	the coded representation of the input feature is compared with a l l representations from the feature store which have been previously stored during a training phase* 	a feature name is issued as in the simplified model. during the training mode the next feature predictor builds up the probabilities of any particular feature being present in an input picture when 
another feature is known to be present. the overall decision mechanism can use the information from the next feature predictor in order to direct the edge detector  with the feature extractor  to new useful areas of search. 
　　　also during the training mode  the feature decision mechanism counts the number of times each feature occurs for each class name of the input pictures and from this builds up probabilities of each class given a particular input feature. thus  when a feature name is issued  the feature decision mechanism predicts the most l i k e ly class name of the input picture* this prediction is fed in turn to the overall decision mechanism. alternatively  a number of class names is issued  with the feature decision mechanism predicting a l l names with probabilities above some preset  or adaptable  value. in this latter case  the overall decision mechanism receives the values of the probabilities as well as the predicted names. 
　　　as the system examines various edges of the input shape  a sequence of features is produced. this causes a sequence of feature names to be issued to the group detector where a current l i s t of features from the input picture is built up together with the absolute or relative positions of these features. the hierarchical system uses the coded group in a manner similar to that used with the coded input feature. the coded group is compared with codes from a group decision mechanism in order to locate new areas of search in the i n put picture  in order to find further group shapes. this examination of group information is the second level of the hierarchy. 
　　　similarly  there can be a third level of the hierarchy l i s t i n g the group names for an association store. a decision mechanism  as before  predicts a class name when the association is i d e n t i f i e d . 
　　　thus as the input picture is examined  a sequence of class name predictions is made at various levels in the hierarchy together with a l i s t of feature names  group names and association names. when the decision mechanism has received sufficient information to produce a decision above a certain level of confidence a name response is produced. during learning  the name response can be compared with the true class name of the input picture and the threshold of confidence may be adapted. the overall decision mechanism can either build and use a sequential decision tree or use the information from the three hierarchical 
　　　
-1-
l e v e l s as a weighted v o t i n g process. 
acknowledgements 
       acknowledgement is made to the m i n i s t r y of technology  u.k.  and e.m.i. e l e c t r o n i c s l t d . who j o i n t l y supported the work. 	the authors also wish to thank m.k. i n g l i s f o r h i s work on the edge detector/follower and messrs. j . a . wiseman and r.w. moth f o r programming a s s i s t a n c e . 
