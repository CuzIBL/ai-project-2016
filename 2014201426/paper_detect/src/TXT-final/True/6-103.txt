 
the problem of deriving joint policies for a group of agents that maximize some joint reward function can be modeled as a decentralized partially observable markov decision process  pomdp . yet  despite the growing importance and applications of decentralized pomdp models in the multiagents arena  few algorithms have been developed for efficiently deriving joint policies for these models. this paper presents a new class of locally optimal algorithms called  joint equilibriumbased search for policies  jesp  . we first describe an exhaustive version of jesp and subsequently a novel dynamic programming approach to jesp. our complexity analysis reveals the potential for exponential speedups due to the dynamic programming approach. these theoretical results are verified via empirical comparisons of the two jesp versions with each other and with a globally optimal brute-force search algorithm. finally  we prove piece-wise linear and convexity  pwlc  properties  thus taking steps towards developing algorithms for continuous belief states. 
1 introduction 
as multiagent systems move out of the research lab into critical applications such as multi-satellite control  researchers need to provide high-performing  robust multiagent designs that are as nearly optimal as feasible. to this end  researchers have increasingly resorted to decision-theoretic models as a framework in which to formulate and evaluate multiagent designs. given a group of agents  the problem of deriving separate policies for them that maximize some joint reward can be modeled as a decentralized pomdp  partially observable markov decision process . in particular  the dec-pomdp  decentralized pomdp   bernstein et al.  1  and mtdp  markov team decision problem  pynadath and tambe  1   are generalizations of a pomdp to the case where there are multiple  distributed agents basing their actions on their separate observations. these frameworks allow a variety of multiagent analysis. of particular interest here  they allow us to formulate what constitutes an optimal policy for a multiagent system and in principle derive that policy. 
multiagent systems 
   however  with a few exceptions  effective algorithms for deriving policies for decentralized pomdps have not been developed. significant progress has been achieved in efficient single-agent pomdp policy generation algorithms  monahan  1; cassandra et al  1; kaelbling et al  1 . however  it is unlikely such research can be directly carried over to the decentralized case. finding optimal policies for decentralized pomdps is nexp-complete  bernstein et al  1 . in contrast  solving a pomdp is pspacecomplete  papadimitriou and tsitsiklis  1 . as bernstein et al.  note  this suggests a fundamental difference in the nature of the problems. the decentralized problem cannot be treated as one of separate pomdps in which individual policies can be generated for individual agents because of possible cross-agent interactions in the reward  transition or observation functions.  for any one action of one agent  there may be many different rewards possible  based on the actions that other agents may take.  in some domains  one possibility is to simplify the nature of the policies considered for each of the agents. for example  chades et al.  restrict the agent policies to be memoryless  reactive  policies. further  as an approximation  they define the reward function and the transition function over observations instead of over states thereby simplifying the problem to solving a multi-agent mdp  boutilier  1 . xuan et al.  describe how to derive decentralized mdp  not pomdp  policies from a centralized mdp policy. their algorithm  which starts with an assumption of full communication that is gradually relaxed  relies on instantaneous and noise free communication. such simplifications reduce the applicability of the approach and essentially side-step the question of solving decentralized pomdps. peshkin et al.  take a different approach by using gradient descent search to find local optimum finite-controllers with bounded memory. their algorithm finds locally optimal policies from a limited subset of policies  with an infinite planning horizon  while our algorithm finds locally optimal policies from an unrestricted set of possible policies  with a finite planning horizon. 
　thus  there remains a critical need for new efficient algorithms for generating optimal policies in distributed pomdps. in this paper  we present a new class of algorithms for solving decentralized pomdps  which we refer to as joint equilibrium-based search for policies  jesp . jesp iterates through the agents  finding an optimal policy for each agent 
1 

assuming the policies of the other agents are fixed. the iteration continues until no improvements to the joint reward is achieved. thus jesp achieves a local optimum similar to a nash equilibrium. we discuss exhaustive-jesp which uses exhaustive search to find the best policy for each agent. since this exhaustive search for even a single agent's policy can be very expensive  we also present dp-jesp which improves on exhaustive-jesp by using dynamic programming to incrementally derive the policy. we conclude with several empirical evaluation that contrast jesp against a globally optimal algorithm that derives the globally optimal policy via a full search of the space of policies. finally  we prove piece-wise linear and convexity  pwlc  properties  thus taking steps towards developing algorithms for continuous initial 
belief states. 
1 model 
we describe the markov team decision problem  mtdp   pynadath and tambe  1  framework in detail here to provide a concrete illustration of a decentralized pomdp model. however  other decentralized pomdp models could potentially also serve as a basis  bernstein et ai  1; xuan et al. 1 . 
　given a team of n agents  an mtdp  pynadath and tambe  1  is defined as a tuple: 
1 is a finite set of world states 
  where a1 ...  an  are the sets of action for agents 1 to n. a joint action is represented as  a  ...  a  . p s i   a 1  ... a n    s f    the transition function  represents the probability of the current state is .sf  if the previous state is si and the previous joint action is  o 1  ... a  .  are the set of observations for agents 1 to n. 1 s   a 1  ...  an   u    the observation function  represents the probability of joint observation  if the current state is $ and the previous joint action is  a 1  ...  an . the agents receive a single  immediate joint reward r s a 1  .. .  an  which is shared equally. 
　practical analysis using models like mtdp often assume that observations of each agent is independent of each other's observations. thus the observation function can be expressed as 1 s   ai ...  a n    = o1 s   au...  a      ...  
　each agent i chooses its actions based on its local policy  1tj  which is a mapping of its observation history to actions. 
thus  at time t  agent i will perform action  where  refers to the joint policy 
of the team of agents. the important thing to note is that in this model  execution is distributed but planning is centralized. thus agents don't know each other's observations and actions at run-time but they know each other's policies. 
1 	example scenario 
for illustrative purposes it is useful to consider a familiar and simple example  yet one that is capable of bringing out key difficulties in creating optimal policies for mtdps. to that end  we consider a multiagent version of the classic 
1 
tiger problem used in illustrating single agent pomdps kaelbling et ai  1  and create an mtdp   s  a  p  o  r   for this example. in our modified version  two agents are in a corridor facing two doors: left  and  right . behind one door lies a hungry tiger and behind the other lies untold riches but the agents do not know the position of either. thus  1 = {sl sr}  indicating behind which door the tiger is present. the agents can jointly or individually open either door. in addition  the agents can independently listen for the presence of the tiger. thus  a1 = a1 - {'openleft'  lopenright'  'listen'}. the transition function p  specifics that every time either agent opens one of the doors  the state is reset to sl or sr with equal probability  regardless of the action of the other agent  as shown in table 1. however  if both agents listen  the state remains unchanged. after every action each agent receives an observation about the new state. the observation function  1 or 1  shown in table 1  will return either hl or hr with different probabilities depending on the joint action taken and the resulting world state. for example  if both agents listen and the tiger is behind the left door  state is sl   each agent receives the observation hl with probability 1 and hr with probability 1. 
table 1: transition function 

table 1: observation function for each agent 
　if either of them opens the door behind which the tiger is present  they are both attacked  equally  by the tiger  sec table 1 . however  the injury sustained if they opened the door to the tiger is less severe if they open that door jointly than if they open the door alone. similarly  they receive wealth which they share equally when they open the door to the riches in proportion to the number of agents that opened that door. the agents incur a small cost for performing the 'listen' action. 
　clearly  acting jointly is beneficial  e.g.  a1 - a1 = 'openleft'  because the agents receive more riches and sustain less damage by acting together. however  because the agents receive independent observations  they do not share observations   they need to consider the observation histories of the other agent and what action they are likely to perform. 
multiagent systems 

table 1: reward function a 
we also consider consider another case of the reward function  where we vary the penalty for jointly opening the door to the tiger  see table 1 . 

table 1: reward function b 
1 	optimal joint policy 
when agents do not share all of their observations  they must instead coordinate by selecting policies that are sensitive to their teammates' possible beliefs  of which each agent's entire history of observations provides some information. the problem facing the team is to find the optimal joint policy  i.e. a combination of individual agent policies that produces behavior that maximizes the team's expected reward. 
　one sure-fire method for finding the optimal joint policy is to simply search the entire space of possible joint policies  evaluate the expected reward of each  and select the policy with the highest such value. to perform such a search  we must first be able to determine the expected reward of a joint policy. we compute this expectation by projecting the team's execution over all possible branches on different world states and different observations. we present here the 1-agent version of this computation  but the results easily generalize to arbitrary team sizes. at each time step  we can compute the expected value of a joint policy  for a team 
starting in a given state  with a given set of past observations  and as follows: multiagent systems 
at each time step  the computation of a summation over all possible world states and agent observations  so the time complexity of this algorithm is  the overall search performs 
this computation for each and every possible joint policy. since each policy specifies different actions over possible histories of observations  the number of possible policies for an individual agent is o the number of possible joint policies for n agents is thus 
o   where  correspond to 
the largest individual action and observation spaces  respectively  among the agents. the time complexity for finding the optimal joint policy by searching this space is thus: 
o  
1 	joint equilibrium-based search for policies 
given the complexity of exhaustively searching for the optimal joint policy  it is clear that such methods will not be successful when the amount of time to generate the policy is restricted. in this section  we will present algorithms that are guaranteed to find a locally optimal joint policy. we refer to this category of algorithms as  jesp   joint equilibriumbased search for policies . just like the solution in section 1  the solution obtained using jesp is a nash equilibrium. in particular it is a locally optimal solution to a partially observable identical payoff stochastic game poipsg   peshkin et al.  1 . the key idea is to find the policy that maximizes the joint expected reward for one agent at a time  keeping the policies of all the other agents fixed. this process is repeated until an equilibrium is reached  local optimum is found . the problem of which optimum the agents should select when there are multiple local optima is not encountered since planning is centralized. 
1 	exhaustive approach exhaustive-jesp  
the algorithm below describes an exhaustive approach for jesp. here we consider that there are n cooperative agents. we modify the policy of one agent at a time keeping the policies of the other n - 1 agents fixed. the function b e s t policy  returns the joint policy that maximizes the expected joint reward  obtained by keeping n - 1 agents' policies fixed and exhaustively searching in the entire policy space of the agent whose policy is free. therefore at each iteration  the value of the modified joint policy will always either 
1 

increase or remain unchanged. this is repeated until an equilibrium is reached  i.e. the policies of all n agents remains unchanged. this policy is guaranteed to be a local maximum since the value of the new joint policy at each iteration is nondecreasing. 

algorithm 1 exhaustive-jespq 
random joint policy  conv 
1: 
1: 	fix policy of all agents except i 
1: 	policy space list of all policies for i 
1: 	new 	bestpolicy i policyspace prev  
1: 
1: 1: 
1: 
1: 
1: if new.value = prev.value then conv conv + 1 
else 
prev new  conv 1 
if conv = n - 1 then break 1: 
　the best policy cannot remain unchanged for more than n - 1 iterations without convergence being reached and in the worst case  each and every joint policy is the best policy for at least one iteration. hence  this algorithm has the same worst case complexity as the exhaustive search for a globally optimal policy. however  it could do much better in practice as illustrated in section 1. although the solution found by this algorithm is a local optimum  it may be adequate for some applications. techniques like random restarts or simulated annealing can be applied to perturb the solution found to see if it settles on a different higher value. 
the exhaustive approach to steps 1 and 1 of the 
exhaustive-jesp algorithm enumerates and searches the entire policy space of a single agent  i. there are 
o
thus  using the ex-
haustive approach incurs an overall time complexity in steps 
1 and 1 of: o . since we incur this 
complexity cost in each and every pass through the jesp algorithm  a faster means of performing the b e s t p o l i c y function call of step 1 would produce a big payoff in overall efficiency. we describe a dynamic programming alternative to this exhaustive approach for doing jesp next. 
1 dynamic programming  dp-jesp  
if we examine the single-agent pomdp literature for inspiration  we find algorithms that exploit dynamic programming to incrementally construct the best policy  rather than simply search the entire policy space  monahan  1; cassandra et al.  1; kaelbling et a/.  1 . these algorithms rely on a principle of optimality that states that each sub-policy of an overall optimal policy must also be optimal. in other words  if we have a t-step optimal policy  then  given the history over the first t steps  the portion of that policy that covers the last t - t steps must also be optimal over the remaining t -t 
1 
steps. in this section  we show how we can exploit an analogous optimality property in the multiagent case to perform more efficient construction of the optimal policy within our jesp algorithm. 
　to support such a dynamic-programming algorithm  we must define belief states that summarize an agent's history of past observations  so that they allow the agents to ignore the actual history of past observations  while still supporting construction of the optimal policy over the possible future. in the single-agent case  a belief state that stores the distribution  =   i s a sufficient statistic  because the agent can compute an optimal policy based on without having to consider the actual observation sequence  
  sondik  1 . 
　in the multiagent case  an agent faces a complex but normal single-agent pomdp if the policies of all other agents are fixed. however   is not sufficient  because the agent must also reason about the action selection of the other agents and hence on the observation histories of the other agents. thus  at each time t  the agent i reasons about the tuple - 
is 
the joint observation histories of all the agents except i. by treating e  to be the state of the agent i at time t  we can define the transition function and observation function for the single agent pomdp for agent i as follows: 

　we now define the novel multiagent belief state for an agent i given the distribution over the initial state  b s  = pr s1 = s : 
		 1  
　in other words  when reasoning about an agent's policy in the context of other agents  we maintain a distribution over rather than simply the current state. figure 1 shows different belief states b1   b1 and  for agent 1 in the tiger domain. 
for instance  b1 shows probability distributions over . in 
 =  sl   hr  1  hr  is the history of agent 1's observations while sl is the current state. section 1 demonstrates how we can use this multiagent belief state to construct a dynamic program that incrementally constructs the optimal policy for agent i. 
1 	the dynamic programming algorithm 
following the model of the single-agent value-iteration algorithm  our dynamic program centers around a value function over a t-step finite horizon. for readability  this section presents the derivation for the dynamic program in the 
multiagent systems 
is figure 1: trace of tiger scenario 
two-agent case; the results easily generalize to the n-agent case. having fixed the policy of agent 1  our value function  vt{bt   represents the expected reward that the team will receive when agent 1 follows its optimal policy from the t-th step onwards when starting with a current belief state  we start at the end of the time horizon  i.e.  t = t   and then work our way back to the beginning. along the way  we construct the optimal policy by maximizing our value function over possible action choices: 
we can define the action value function  	recursively: 
the first term in equation 1 refers to the expected immediate reward  while the second term refers to the expected future reward  is the belief state updated after performing action a1 and observing . in the base case  t - t  the future reward is 1  leaving us with: 
the calculation of expected immediate reward breaks down as follows: 

thus  we can compute the immediate reward using only the agent's current belief state and the primitive elements of our given mtdp model  see section 1 . 
　computation of the expected future reward  the second term in equation 1  depends on our ability to update agent 1 's belief state from b1t to  in light of the new observation  for example  in figure 1  the belief state b1 is updated to on performing action a1 and receiving observation . we now derive an algorithm for performing such an update  as well as computing the remaining term from equation 1. the initial belief state based on the distribution over initial state  1  is: 
		 1  
multiagent systems 
	we 	treat the 	denominator of equation 	1  i.e.  
pr as a normalizing constant to bring the sum of the numerator over all to be 1. this result also enters into our computation of future expected reward in the second term of equation 1. thus  we can compute the agent's new belief state  and the future expected reward and the overall value function  in turn  using only the agent's current belief state and the primitive elements of our given mtdp model. having computed the overall value function  vt  we can also extract a form of the optimal policy  that maps observation histories into actions  as required by equations 1 and 1. 
　algorithm 1 presents the pseudo-code for our overall dynamic programming algorithm. lines 1 generate all of the belief states reachable from a given initial belief state  
. since there is a possibly unique belief 
state for every seauence of actions and observations by agent 
1  there are o 	reachable belief states. this 
reachability analysis uses our belief update procedure  algorithm 1   which itself has time complexity when invoked on a belief state at time t. thus  the overall reachability analysis phase has a time complexity of 
o 	lines 1 perform the heart 
of our dynamic programming algorithm  which also has a 
time complexity of 1  	. lines 1-
1 translate the resulting value function into an agent policy defined over observation sequences  as required by our algorithm  i.e.  the argument . this last phase has a lower time and space complexity  o   than our other two phases  since it considers only optimal actions for agent 1. thus  the overall time complexity of our algorithm is o . the space complexity of the resulting value function and policy is essentially the product of the number of reachable belief states and the size of our belief state representation: o  
1 	piecewise linearity and convexity of value function 
algorithm 1 computes a value function over only those belief states that are reachable from a given initial belief state  which is a subset of all possible probability distributions over st and  to use dynamic programming over the entire set  we must show that our chosen value function is piecewise linear and convex  pwlc . each agent is faced with solving a single agent pomdp is the policies of all other agents is fixed as shown in section 1. sondik  showed that the value function for a single agent pomdp is pwlc. hence the value function in equation 1 is pwlc. thus  in addition 1 

to supporting the more efficient dynamic programming of algorithm 1  our novel choice of belief state space and value function can potentially support a dynamic programming al-
gorithm over the entire continuous space of possible belief states. 
1 experimental results 
in this section  we perform an empirical comparison of the algorithms described in sections 1 and 1 using the tiger scenario  sec section 1  in terms of time and performance. figure 1  shows the results of running the globally optimal al-
gorithm and the exhaustive jesp algorithm for two different reward functions  tables 1 and 1. finding the globally optimal policy is extremely slow and is doubly exponential in the finite horizon  t and so we evaluate the algorithms only for finite horizons of 1 and 1. we ran the jesp algorithm for 1 different randomly selected initial policy settings and compared the performance of the algorithms in terms of the number of policy evaluations  on y-axis using log scale  that were necessary. as can be seen from this figure  for t - 1 the jesp algorithm requires much fewer evaluations to arrive at an equilibrium. the difference in the run times of the globally optimal algorithm and the jesp algorithm is even more apparent when t = 1. here the globally optimal algorithm 
performed 1 million policy evaluations while the jesp algorithm did 1 evaluations. for the reward function a  jesp succeeded in finding the globally optimal policies for both t = 1  expected reward = -1  and 1  expected reward = -1 . however  this is not always the case. using reward function b for t = 1  the jesp algorithm sometimes settles on a locally optimal policy  expected reward = -1  that is different from the globally optimal policy  expected reward = 1 . however  when random restarts are used  the globally optimal reward can be obtained. 
　based on figure 1  we can conclude that the exhaustive jesp algorithm performs better than an exhaustive search for the globally optimal policy but can some times settle on a policy that is only locally optimal. this could be sufficient for 
problems where the difference between the locally optimal policy's value and the globally optimal policy's value is small and it is imperative that a policy be found quickly. alternatively  the jesp algorithm could be altered so that it doesn't get stuck in a local optimum via random restarts. 
　table 1 compares presents experimental results from comparison of exhaustive jesp with our dynamic programming approach  dp-jesp . these results  also from the tiger domain  show run-time in milliseconds  ms  for the two algorithms with increasing horizon. dp-jesp is seen to obtain significant speedups over exhaustive-jesp. for time horizon of 1 and 1 dp-jesp run time is essentially 1 ms  compared to the significant run times of exhaustive-jesp. as we increased the horizon to  1  we could not run exhaustive-jesp at all; while dp-jesp could be easily run up to horizon of 1. 
1 summary and conclusion 
with the growing importance of decentralized pomdps in the multiagents arena  for both design and analysis  it is critical to develop efficient algorithms for generating joint poli-
1 	multiagent systems 


figure 1: evaluation results 

table 1: run time ms  for various t with pentium 1  1ghz  1gb memory  linux redhat 1  allegro common lisp 1 
cies. yet  there is a significant lack of such efficient algorithms. there are three novel contributions in this paper to address this shortcoming. first  given the complexity of the exhaustive policy search algorithm - doubly exponential in the number of agents and time - we describe a class of algorithms called  joint equilibrium-based search for policies   jesp  that search for a local optimum rather than a global optimum. in particular  we provide detailed algorithms for exhaustive jesp and dynamic programming jesp dpjesp . second  we provide complexity analysis for dp-jesp  which illustrates a potential for exponential speedups over exhaustive jesp. we have implemented all of our algorithms  and empirically verified the significant speedups they provide. third  we provide a proof that the value function for individual agents is piece-wise linear and convex  pwlc  in their belief states. this key result could pave the way to a 
new family of algorithms that operate over continuous belief states  increasing the range of applications that can be attacked via decentralized pomdps  and is now a major issue for our future work. 
acknowledgments 
we thank piotr gmytrasiewicz for discussions related to the paper. this research was supported by nsf grant 1 and darpa award no. f1-1. 
