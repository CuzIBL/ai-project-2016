 
discretization refers to splitting the range of continuous values into intervals so as to provide useful information about classes. this is usually done by minimizing a goodness measure  subject to constraints such as the maximal number of intervals  the minimal number of examples per interval  or some stopping criterion for splitting. we take a different approach by searching for minimum splits that minimize the number of intervals with respect to a threshold of impurity  i.e.  badness . we propose a  total entropy  motivated selection of the  best  split from minimum splits  without requiring additional constraints. experiments show that the proposed method produces better decision trees. 
1 	introduction 
continuous values refer to linearly ordered values  mainly numeric values. while continuous values are common in real applications  many learning algorithms focus on unordered discrete values. a common practice is to discretize continuous values into intervals so as to provide useful information with respect to classes. discretization can be performed either on the whole dataset prior to induction  i.e  global discretization  or on local regions during induction  i.e.  local discretization. we focus on local discretization as it takes into account the context sensitivity of the nature. one example of local discretization is the entropy-based c1  quinlan  1   in which continuous values are split into two intervals  i.e.  binary splitting  for consideration at a node. however  as pointed out in  fayyad and irani  1   an interesting range is usually an internal interval within the feature's range  and to get to such an interval a binarysplit-at-a-time leads to unnecessary and excessive partitioning of the examples. we now provide further reasons for multi-way splits. 
　one frequent argument against multi-way splitting is that a multi-way split can be  simulated  by a series of binary splittings. though theoretically true  this argument is false in the process of generating decision trees where there is no guarantee that all  simulating  binary splittings will be finished up before considering other features because the splitting at each level is performed independently. as a result  a structured multi-way split is hardly simulated by binary splits in practice. in addition  by restricting to only binary splits  an unstructured feature could be selected instead of a structured but never explored multi-way split of a continuous feature  making the simple structure disappear. consider the following two decision trees built in one of the 1fold cross validation on iris dataset. the first tree is produced by the multi-way split proposed in this paper  and the second by c1. though both trees have the same size and same error rate on test data  the first tree classifies most examples at the first level using simple rules and thus is preferred. the reason why c1 didn't select petal length at the first level is because the optimal binary split of petal length loses to that of petal width. as a result  the simple one-level test for most examples  as in the first tree  is not discovered. this example also reveals the bias of the tree size measure: it does not take the frequency  i.e.  importance  of rules into consideration. 


the brute-force solution is exponential in k - 1  where k is the number of intervals considered. since the number of intervals in a multi-way split is unknown and not fixed a prior  goodness measures such as the gain that are always improved by further splitting do not work directly  and have to be coupled with other constraints or criterions. in this paper  we address the search problem in two steps. first  we define the notion of minimum splits as a necessary condition for a good split. a minimum split wrt a threshold of impurity is a split that minimizes the number of intervals subject to the threshold. we propose a dynamic programming algorithm for finding a minimum split for any impurity measure that is additive in the sense that the impurity of several intervals is the sum of the impurities of each interval. this includes most standard impurity measures  such as entropy  quinlan  
1   twoing rule and gini index  breiman et al.  1   sum minority  inconsistency rate  and others. the dynamic programming algorithm runs in a quadratic time in the number of starting intervals. we describe two approximation algorithms that can compute nearly optimal solutions efficiently for large datasets. 
　we then propose a method of determining the  best  split from a collection of minimum splits called candidate splits  found in a single run of the algorithm for finding minimum splits. the  best  split of continuous values is the candidate split that has the smallest product of entropy and number of intervals. intuitively  this product measures the total information  rather than the average information like the standard entropy  of all intervals. we compared the proposed minimum split method with release 1 of c1  a substantial improvement of early releases on handling continuous values. the study shows that multi-way splits usually build decision trees that are shallow and classify more examples at upper levels of the tree  compared to binary splits. we propose the notion of testing depth to capture this aspect of simplicity of decision trees  which is not addressed by the tree size. 
　several recent papers have examined discretization of continuous values. one approach  e.g.   kerber  1; richeldi and rossotto  1   starts with one interval per value and repeatedly merges adjacent intervals based on some  similarity  measure. it could be diffi-
cult to specify a good threshold of similarity so that not too many intervals are constructed. another approach aims at finding a split that optimizes some goodness criterion. examples are  quinlan  1; catlett  1; 
holte  1; chiu et a/.  1; fulton et a/.  1; auer et a/.  1 . see  dougherty et a/.  1   for more on these work. in these methods  additional constraints  such as the maximum number of intervals  the minimum number of examples in each interval  a penalty function on the number of intervals  are needed to control the number of intervals.  catlett  1; 
	wang 1c goh 	1 


	wang & goh 	1 

1 is reduced to k'1. in general  the switching point k' represents a trade-off between optimality and speed. 
1 	the optimal split 
we now address the central question of how to determine the optimal split for a continuous feature at a node of decision trees. once the optimal split is determined for every continuous feature  any existing selection criterion for discrete features is applied to select the best feature at the node. we use the entropy  quinlan  1  as the impurity measure. 
　as motivated in section 1  a good split must be a minimum split  therefore we consider only minimum splits when searching for the optimal split. since any threshold larger than eb gives a minimum split having at most two intervals  where eb is the entropy of optimal binary splits  we need only to consider thresholds not more than eb. in other words  the search space of the optimal split is the set of minimum splits of x1 ...  xk wrt x  where x =  which have at least two intervals. 
from section 1  these minimum splits can be found in a single run of the dynamic programming algorithm wrt eb. we call these splits candidate splits for feature a. the set of candidate splits is empty only if all examples belong to the same class  in which case there is no need of splitting at the node. if the greedy algorithm is run instead  splits after each iteration form an approximation of candidate splits. 
　let m x k  and m x k  denote the number of intervals and entropy of a minimum split of x  ...  xk wrt 
x. in the search for the goodness measure of a split  we observed that the product  usually gives a reasonable quality measure of the minimum split. on one hand  a  good  split usually has both a small number of intervals and a small entropy  thus yielding a small product. on the other hand  a small product but a  not-so-small  interval number entails a very small entropy  thus a nearly pure classification with no unnecessary splitting  due to minimum splits . it is possible  however  that some of these intervals are very small. we avoid such splits by requiring a minimal number of examples in an interval of a minimum split. with these said  we have: 
　the optimal split for a continuous feature: choose the candidate split that has the smallest product of entropy and number of intervals. 
　there is a natural interpretation for the above selection. suppose that feature a is split into d intervals  with n  examples and ei entropy for the ith interval. let n be the total number of examples. the entropy of the split is then given by 

this is exactly the weighed average of entropies for all intervals. therefore  the above minimum product selection aims at minimizing the total entropy of a split. the following corollary says that the optimal split is well behaved by being actually a strong minimum split. 
corollary 1 the optimal split selected above is a strong minimum split of x1 ...  xk wrt some x. 
　in fact  if the optimal split is not strong  there must be another minimum split  also in the set of candidate splits  having the same number of intervals but a smaller entropy  which yields a smaller product and thus it is preferred to the other one. 
1 	empirical evaluation 
we compared three algorithms: release 1 of c1  the latest release  with the default setting  the multi-way splitting based on dynamic programming  the multiway splitting based on the greedy algorithm. these algorithms are denoted by c1 r1   dynamic  and greedy  respectively. the hybrid algorithm is not included because its performance is expected to lie between dynamic and greedy. in dynamic  the bins to start with correspond to examples having the continuous value. unlike early releases  c1 r1  improves the performance on continuous values by employing an mdl-inspired penalty to adjust the gain of a binary split of continuous values. as shown in  quinlan  1   c1 r1  compares favorably with the multi-way split method t1  auer et al.  1  and the discretization method  fayyad and irani  1 . therefore  we choose c1 r1  as a benchmark. dynamic and greedy determine the optimal split of continuous values as in section 1 and select a feature for branching as in c1 using the optimal splits for continuous features. all three algorithms are applied to 1 datasets from the uci repository  murphy and aha  1   all involving some continuous features and some involving many. all experiments are performed using 1-fold cross validation. the size and error rate of pruned decision trees are collected on test data. a summary is given in table 1. the numbers following ＼ are standard errors. 
　in table 1  the testing depth of a decision tree is defined as the average length of root-to-leaf paths weighed by the numbers of examples covered by leaves. thus  the testing depth measures the average number of tests needed to classify an example  thus  the average complexity of rules used. a decision tree with a small testing depth is likely to classify examples by simple rules. this aspect of complexity is not reflected by the simple tree size. we highlight a few results shown in table 1:  a  except for a few datasets  dynamic wins over greedy in all three measurements   b  on tree size dynamic wins over c1 r1  in 1 out of 1 datasets  with 1 tie  and on error rate dynamic wins over c1 r1  in 1 out of 1  


table 1: 1-fold cross validation results 

with 1 ties   c  on tree size  c1 r1  performs better than greedy in general  and on error rate  about halfhalf   d  on testing depth  both dynamic and greedy win over c1 r1  for all 1 datasets. decision trees produced by dynamic and greedy usually are not as deep as those produced by c1 r1 ; they tend to have more  parallel  branches at a node  instead of  nested  ones into the tree. after comparing actual trees  we feel that  parallel  branches are easier to understand than  nested  ones. in running time  dynamic is slowest and the other two are comparable. in this regard  the hybrid algorithm could be more promising to offer both quality trees and fast speed. in conclusion  the proposed multi-way splitting of continuous values shows some advantages over c1 r1 . we believe that for many application domains a multi-way splitting coupled with a careful control on over-splitting is a powerful technique for handling continuous values. 
