 
we introduce  1  a novel stochavic inversion trans duction grammar formalism for bilingual language modeling of sentence-pairs and  1  the concept of bilingual parsing with potential application to a variety of parallel corpus analysis problems the formalism combines three tactics against the con straints that render finite-state transducers less useful it skips directly to a context-free rather than finite-state base it permits a minimal extra degree of ordering flexibility and its probabilistic formula tion admits an efficient maximum-likelihood bilin gual parsing algorithm a convenient normal form is shown to exist and we discuss a number of exam pies ot how stochastic inversion transduction grammars bring bilingual constraints to bear upon prob 
lemalic corpus analysis tasks 
1 	introduction 
we introduce a general formalism for modeling of bilingual sentence pairs known as an inversion transduction grammar with potential application in a variety of corpus analysis areas transducer models especially of the finite state family have long been known however finite-stale transducers impose identical ordering constraints upon both streams confining their applicability to nlp tasks to narrowly restricted do mams outside of which transduction has received relatively little attention the inversion transduction grammar formalism skips directly to a context free rather than finite state base and permits one extra degree ot ordering flexibility while retainingproperties necessary for efficient computation thereby sidestepping the limitations of traditional transducers 
¡¡in tandem with the concept of bilingual language model ing we propose the concept of bilingual parsing whete the input is a sentence pair rather than a sentence though inversion transduction grammars remain inadequate as full fledged translation models bilingual parsing with simple inversion transduction grammars turns out to be very useful for parallel corpus analysis when the true grammar is not fully known parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis  e g brown et al 
1 gale & church 1 gale et al 1 church 1 
brown et at 1 dagan et al 1 fung & church 1  wu & xia l1 fung & mckeown 1  the primary pur-
1b 	natural language 
pose of bilingual parsing with inversion transduction gram mars is not to flag ungrammalical inputs rather the aim is to extract structure from the input data which is assumed lo be grammatical in kindred spirit with robust parsing sample applications lo segmenialion bracketing phrasal alignment and parsing are surveyed later in this paper the formalism s uniform integration of various types of bracketing and alignment constraints is one of its cruel strengths 
   we begin below by laying out the basic lormalism then show that reduction to a normal form is possible afterwards we introduce a siochaslic version and give an algonlhm tor finding the optimal bilingual parse ot a sentence-pair the formalism is independent of the languages we give exam pies and applications using chinese and english because ian guages from different families provide a more rigorous testing ground 
1 	inversion transduction grammars 
as a stepping stone to inversion transduction grammarss we first consider what a symmetric context free transduction grammar  cftg  would look like the utility of finite-state transducers is well-known for narrow tasks such as nominal number and temporal phrase normalization text to-speech conversion and analysis ot inflectional morphology  gazdar & mellish 1  but tor general corpus analysis fsts are inadequate 
   by transduction we mean that two output streams aregener ated one for each language  transducers arc often presented as having one input and one output stream but this view works better tor deterministic finite-state machines than for the non-deterministic models we are using here moreover for our application the two languages role is symmetric   in a cftg every terminal symbol is marked for a particular output stream thus each rewrite rule emits not one but two streams for example a rewrite rule ot the torm 1 - bx  v j t z  means that the terminal symbols x and z are symbols of the language l  emitted on stream 1 while y is a symbol of the language l  emitted on stream 1 it follows that every nonterminal stands for a class of derivable substring 
pairs 
   we can use a cftg to model the generation of bilingual sentence pairs as a mnemonic convention we usually use the alternative notation 1 -  b x/y c z/t to associate matching output tokens though this additional information has no formal generative effect it reminds us that x/y must be a 


	wu 	1 

 1  
1
1 	natural language 

	wu 	1 

1 	natural language 

   in our experience this method has proven extremely effective for avoiding missegmentation pitfalls essentially erring only in pathological cases involving coordination construe tions or lexicon coverage inadequacies the method is also straightforward to employ in tandem with other applications such as those below 
1 	applications bracketing 
bracketing is another intermediate corpus annotation useful especially when a full coverage grammar with which to parse a corpus is unavailable  tor chinese an even more common situation than with english  aside from purely linguistic interest bracket structure has been empirical!  shown to be highly effective at constraining subsequent training of for ex ample stochastic context-tree grammars  pereira & schabes 1 black el al 1  previous algorithms for automatic bracketing operate on monolingual texts and hence require more grammatical constraints forexample 'actics employing mutual information have been applied to lagged text  mager man& marcus 1  
   our method based on sitgs operates on the novel prin ciple that lexical correspondences between parallel sentences yields information from which partial bracketings for both sentences can be extracted the assumption that no grammar is available means that constituent categories are not differ entiated instead a generic brackering imersion maniducation grammer is employed containing only one nonterminal sym 
bol a which rewrites either recursively as a pair of a s or as a single terminal pair 

longer productions with fanout   1 are not needed we show in  wu  1a  that this minimal transduction grammar in normal form is gencratively equivalent lo any reasonable ito lor bracketing moreover we also show how postprocessing 
using rotation and flattening operations restores the fanout flexibility so that an output bracketing can hold more than two immediate constituents as shown in figure 1 
   the b j distribution actually encodes the english chinese translation lexicon we have been using a lexicon that was automatically learned from the hkust english c hinese par allel bilingual corpus via statistical sentence alignment  wu 
1  and statistical chinese word and collocation extraction  fung & wu 1 wu & fung 1  followed b  an em word-translation learning procedure  wu &. xia 1  the latter stage gives us the  probabilities directly for the two singleton productions which permit any word in either sentence to be unmatched a small  -constant can be chosen tor the probabilities and so that the optimal bracketing resorts to these productions only when it is otherwise impossible lo match words 
   an experiment was earned out as follows approximately 1 sentence-pairs with both english and chinese lengths of 1 words or less were extracted from our corpus and brack eted using the algorithm described several additional criteria were used lo filter out unsuitable sentence pairs it the lengths of the pair of sentences differed by more than a 1 ratio the pair was rejected such a difference usually arises as the result of an earlier error in automatic sentence alignment sentences containing more man one word absent from the translation lexicon were also rejected the bracketing method is not intended to be robust against lexicon inadequacies we also rejected sentence pairs with fewer than two matching words since this gives the bracketing algorithm no discriminative leverage such pairs accounted for less than 1% of the input data a random sample of the brackeied sentence pairs was then drawn and the brackel precision was computed under each criterion for correctness examples are shown in figure 1 
¡¡the bracket precision was 1% for the english sentences and 1% tor the chinese sentences as |udged against manual bracketings inspection showed the errors to be due largely to imperfections of our translation lexicon which contains approximately 1 s1 english words and    1 chinese words with about 1¡ã/  translation accuracy  wu & xia 1  so a better lexicon should yield substantial pertorrmnce improve ment moreover if the resources tor a good monolingual part of speech or grammar-based brackeler such as that of magerman & marcus  1  are available its output cjn read ilv be incorporated in complementary fashion as discussed in section k 
1 	applications phrasal alignment 
phrasal alignment the parsing algorithm can be used lo identity phrasal translations within sentence pairs this is useful especially where the phrases in the two languages are not compositionally derivable solely from obvious word translations such as  have acquireda in figure 1 this principle applies lo nested structures also 
such as 	have acquired/; 
	on up to the sentence level 	these examples 
were found using the minimal bracketing iransduction gram mar a relatively weak strategv we evaluated the precision at 1 s /c which is useful bul rather low higher precision could be achieved without great effort by employing a small number of broad nonterminal categories 
word alignment under the itg model word alignment becomes simpk the special case of phrasal alignment at the pirse tree leaves however this gives us an interesting al ternative perspective from the standpoint ol algorithms thai match the words between parallel sentences bv themselves word alignments are ol in tic use but thev provide potential an chor points tor other applications or for subsequent learning stages lo acquire more interesting structures 
   word alignment is difficult because correct matchings are not usually linearly ordered i e there are crossings without some additional constraints any word position in the source sentence can be matched to any position i n the target senlenee an assumption which leads lo high error rales more sophisti cated word alignment algorithms therefore attempt lo model the intuition that proximate constituents in close relationships in one language remain proximate in the other the later ibm models are formulated to prefer collocations  brown et al 1   in the case of word-ah%r   dagan el al 1  dagan & church 1  a penalty is imposed according to the deviation from an ideal matching as constructed by linear 
	wu 	1 * 


	1 	naturai language 


	wu 	1 
