
question answering  qa  is a highly complex task that brings together classification  clustering  retrieval  and extraction. question answering systems include various statistical and rule-basedcomponents that combine and form multiple strategies for finding answers. however  in real-life scenarios efficiency constraints make it infeasible to simultaneously use all available strategies in a qa system. to address this issue  we present an approach for carefully selecting answering strategies that are likely to benefit individual questions  without significantly reducing performance. we evaluate the impact of strategy selection on question answering performance at several important qa stages: document retrieval  answer extraction  and answer merging. we present strategy selection experiments using a statistical question answering system  and we show significant efficiency improvements. by selecting 1% of the available answering strategies  we obtained similar performance when compared to using all of the strategies combined.
1 introduction and related work
in the past few years  an increasing number of question answering systems have started employing multi-strategy approaches that attempt to complement one another when searching for answers to questions. these approaches often include multiple question classifications  several retrieval approaches  multiple answer extractors  and different data sources. question answering performance is often presented within the context of official evaluations where systems are processing batches of questions with no time constraints. however  in real-life scenarios  only a limited number of these strategies  component combinations  parameter settings  etc  can be fully explored. in these cases  the tradeoff between performance and problem complexity  and indirectly response time  require careful selection of answering strategies such that performance is optimized according to realistic constraints.
　we present an answering strategy selection approach that directly addresses the performance-complexity trade-off and we apply it to a statistical  instance-based question answering system. in this paper we investigate the benefits of a principled strategy selection method when applied to the main components of a qa system: document retrieval  answer extraction  and answer merging  i.e. overall qa performance . we present experiments which show that by carefully selecting less than 1% of the available answering strategies  no significant performance degradation is observed. moreover  we integrate a cluster-based confidence scoring method with an answer merging component and observe significant question answering performance improvements.
　experiments  collins-thompson et al.  1  using the cmu javelin  nyberg et al.  1  and waterloo's multitext  clarke et al.  1  question answering systems corroborate the expected correlation between improved document retrieval performanceand qa accuracy across systems. results suggest that retrieval methods adapted for question answering which include question analysis performed better than ad-hoc ir methods  supporting previous findings  monz  1 .
　several practical approaches have been developed to deal with the complexity of the question answering process. the smu system  harabagiu et al.  1  and later the lcc system  moldovan et al.  1  incorporate feedback loops between components of their question answering system. the cmu system treats the qa process as planning problem  formalizing the notion of feedback. several other qa systems using statistical components  chu-carroll et al.  1; nyberg et al.  1; lita and carbonell  1  introduced multiple answering strategies that can be used simultaneously and their results can be combined. furthermore  when answering complex questions   harabagiu and lacatusu  1  argue for a multi-strategy approach for question processing  extraction  and selection.
　the strategy selection problem is closely related to active learning  which explores the trade-off between performance and cost. while active learning algorithms suggest data for labeling by minimizing the expected error  roy and mccallum  1   in the problem of strategy selection  the goal is to reduce qa complexity by limiting the number of answering strategies while not increasing the error of the qa process.
1 answering strategies
most question answering systems are implemented as a pipeline where different stages successively process data.
however  for each stage in the qa pipeline there is a variety of methods that can be employed. each method typically has different parameters  needs different resources  and may produce answers with different confidences  which may not be comparable across methods. we will refer to a complete combination of components at each stage in the pipeline as an answering strategy. in most of today's qa systems  an answering strategy consists of the following components:
1. question analysis - produces an expected answer type  extracts question keywords  and analyzes the question. part of speech tagging  parsing  semantic analysis and additionalprocessing are often used in question analysis.
1. retrieval - specifies what query types and what query content yield high expected performance. very often qa systems pre-specify the query type and additional content according to the question and answer types identified earlier in the strategy.
1. answer extraction - specifies how answers are identified from relevant documents. answer extraction methods range from rule and pattern-based extractors to hidden markov models  hmm   maximum entropy  and support vector machine-based extractors.
stagestrategy sastrategy sb1  answer typetemporalyear1  querieswhen mozart diemozart die biography mozart died death1  extractionrule-basedhmmtable 1: answering strategies sa and sb use different answer types  different queries  and extraction methods.
　when applied to a new question  an answering strategy processes the question text  retrieves documents and extracts a set of possible answers. in the case when multiple strategies are simultaneously applied to a new question  an answer merging component is employed to combine answers and confidences into a final answer set. table 1 shows two simplistic strategies for the question  when did mozart die  . in realistic scenarios the question analysis component produces more information than just an expected answer type  several queries are generated according to pre-specified types  and various processing is performed before answer extraction.
1 cluster-based strategies
as the first stage in answering strategies  most question answering systems employ question ontologies. these ontologies combine expected answer types  date  location etc  and question types  birthday x   nickname x   construction date x  etc . consider again the question  when did mozart die  . depending on the desired answer type granularity  this question can be classified as a temporal question  a temporal::year question  or more specifically as a temporal::year::death year question. each classification may lead to an entirely different answering strategy. existing systems consider answer types ranging from simple answer type sets and qa specific ontologies to semantic networks such as wordnet which provide better coverage and more specificity. however  these ontologies are very restrictive and only take into account the answer type  disregarding question structure  or domain knowledge.
　an approach that is similar to using ontologies is question clustering  lita and carbonell  1   in which training questions are clustered according to different similarity criteria such as shared number of n-grams  contiguous sequences of words   semantic similarity  and same answer type. compared to fixed ontologies  this approach is adaptive to training data  is language and domain independent  and allows overlapping types  clusters  that do not have a hierarchical relationship. figure 1 shows the relationship between an ontology and clustering as it is used in question analysis  stage 1  of a qa process. if clustering is performed at different granularities  each cluster corresponds to an ontology node. thus  individual answering strategies are built for different clusters  rather than different ontology nodes.
　this clustering approach1 allows each component in an answering strategy to be learned only from i  training questions and ii  their known correctanswers. thereforestrategies are learned for individual clusters  using corresponding questions as training data. the retrieval component learns which queries and query types have high performance when run on in-cluster training questions. the answer extraction component is trained on correct answers for all in-cluster questions. finally  the answer merging component considers cluster statistics  retrieval performance  extraction performance and merges answer sets produced by answering strategies.
　if there is sufficient data for learning  i.e. sufficient number of questions   the more clusters of training questions a qa system generates  the more answering strategies will be applied to new questions. however  while qa performance may increase with additional answering strategies  so will the noise  e.g. from irrelevant clusters  and the time it takes to actually run the strategies. our goal is to allow the existence of multiple cluster-based strategies  but only select a set of clusters associated to the strategies most likely to lead to high performance. for document retrieval  high performance translates into high recall of relevant documents. for answer extraction high performance corresponds to a large number of correct answers being extracted.
　queries learned by differentstrategies often lead to some of the same relevant documents - e.g. the queries  the first aria composed mozart  vs.  aria mozart  may lead to an overlap in their retrieved document sets. if a strategy already leads to the retrieval of a document di  subsequent strategies will not benefit if they retrieve di again. therefore  each strategy selection depends on the n-1 previously selected strategies
1 experiments & results

figure 1:  a  classification according to a question ontology versus classification according to a set of clusters in the training data  b  answering a new question using multiple strategies learned from training questions in each cluster  or ontology node . the answer sets produced by individual cluster-based strategy are merged into a single ranked answer set.for our experiments we have chosen to use a statistical question answering system for several reasons. statistical qa systems are usually faster to implement than rule-based systems  they require less knowledge resources and limited manual input  and their results are easier to replicate on standard datasets. in this paper we have implemented an instancebased question answering  ibqa  system  lita and carbonell  1 . the question analysis component relies on clustering training questions at different levels of granularity and then classifying new questions into these clusters. for each cluster  an answering strategy is learned which equivalent to a set of retrieval  extraction  and answer type models. in the remainder of the paper we shall refer to these strategies as cluster-based since a single answering strategy is learned from the training questions of an individual cluster.
　when new questions are presented to a qa system  they are classified according to the training clusters and the corresponding answering strategies are activated  each generating a set of answers. the strategy-specific answers are then merged into a final ranked answer list. details on learning and implementation of the answer type  retrieval  and extraction models can be found in  lita and carbonell  1 .
　the question datasets used in the experiments presented in this paper consist of all temporal questions from previous trec evaluations trec 1  voorhees  1 to 1 . temporal questions have the advantage of having a relatively high density necessary in training statistical qa components. they are also distributed such that they cover simple questions such as  when did beethoven die    but also structurally more complex questions such as  what year did general montgomery lead the allies to a victory over the axis troops in north africa  . each question in this collection has a set of correspondinganswer keys in the formof regularexpressions.
　the questions were processed with a part of speech tagger  brill  1   a morphologicalanalyzer  minnen et al.  1   and a sentence splitter. synonyms and hypernyms were extracted and used to enhance keyword-based queries in document retrieval. several queries are generated for each answering strategy and at most one hundred documents were retrieved for each of these query through the google api  www.google.com/api . documents containing any reference to trec or the actual question  and other problematic content were filtered out.
　two desired qualities in question answering systems are 1  correctness - correct answers should have higher rank than incorrect answers  and 1  redundancy - correct answers should occur more than once in documents. more specifically  the retrieval component should produce multiple relevant documents; the answer extraction component should be able to extract multiple instances of a correct answer and at the same time assign them higher confidence scores compared to incorrect answers. towards this end  we evaluate question answering performance using several metrics:
  mean reciprocal rank  mrr  is computed as the inverse of the rank of the first correct answer returned by a system. the higher the first correct answer can be found in the ranked answer list  the higher the mrr score.
  the second metric tests the existence of at least one correct answer within the top five answers  top1  and is less strict compared to mrr.
  towards the goal of redundancy  for document retrieval we evaluate the number of relevant documents obtained  i.e. documents containing at least a correct answer  and for answer extraction we evaluate the number of correct answers extracted.
questionsstrategiesmrrtop1mrrtop1all1111extracted1111table 1: statistical qa system results on temporal questions for i  all questions and ii  questions for which at least an answer was extracted. we show average score over all questions  but also the average performance over all strategies. however  for question performance  only a small number of strategies need to be successful.
　for comparison purposes  we have evaluated the instancebased qa system using mrr and top1. table 1 shows the mrr and the top1 scores for the question dataset used in this paper. we present the qa system performance over all questions  all  in the dataset as well as the performance the questions with at least one proposed answer  extracted . intuitively  the latter measure can be viewed as precision and the former measure as recall. at the question level  which is what trec evaluations  voorhees  1 to 1  use  we show the performance by averaging over all questions - for each question we combine results from all strategies. at the strategy  cluster  level  we compute the performanceby averaging over all strategies - for each strategy  we compute the performance over all questions it can be applied to. note that not all answering strategies are successful  hence the lower mrr average. at the same time  questions benefit from multiple strategies and therefore their average mrr is higher. performance in the above experiments was computed through leave-one-out cross-validation.
　in these experiments  each iteration corresponds to an answering strategy being selected. the newly selected answering strategy includes specific query types that are known to perform well on training questions in the their respective clusters. in addition  a cluster-specific svm extractor finds and scores potential answers. in some of these experiments  the computation of a greedy-optimal oracle  cluster selection method is tractable. this is not to be confused with a global optimal classifier that finds the absolute best strategy selection sequence - when tractable  we present its performance.
1 selection for document retrieval
we assume a document to be relevant in the context of question answering if it contains a correct answer in a correct context. since it is very difficult to automatically evaluate the correctness of context  the notion of relevance is sometimes relaxed to whether a document contains the correct answer  regardless of context. through cluster-specific data  the retrieval component of the qa system learns n-grams and features that improve retrieval when added to queries. the improvement is measured when these queries are used to retrieve documents for the questions in the same cluster. the learned features become part of the cluster-based answering strategy which can be applied to new similar questions.
strategy selection for document retrieval

figure 1: smart selection based on strategy confidence allows the qa system to employ only 1% of its available strategies to retrieve 1% of the accessible relevant documents.
　when trying to answer the question  when did mozart die   it may be beneficial to create queries that contain features such as  biography    cemetery    spent his life    sacrificed himself   etc. in many qa systems the retrieval component contains rules for building better queries for specific types of questions - in our example: time ofdeath. in the cluster-based approach  these features are learned from other similar questions in the training data  and get added to clusterspecific answering strategies. we measure the retrieval confidence conf air cj |q  of an answering strategy a derived from cluster cj given a new test question q:
 conf air cj |q  = p d+|air cj   ， p cj|q  ， s j   1  where p d+|air cj   is the probability of retrieving a relevant document d+ using strategy air cj  and is measured by testing its effectiveness on a held-out set of questions from the cluster. p cj|q  is the probability of a cluster containing questions similar to q and is given by the average similarity between q and qj  i （ cj  normalized over all clusters. s j  is a minimal cardinality condition for clusters.
　figure 1 shows the effect of using confidence-based selection in order to iteratively add appropriate answering strategies  i.e. beneficial query content . the more strategies are employed to create queries and retrieve new documents  the less time will be available for answer extraction and answer merging. the iterative process offers a good trade-off between performance and number of strategies used  as well as a good basis for user-defined utility functions. in our experiments  if the qa system selects only 1% of the available strategies  the retrieval performance is approximately 1% of the maximum achievable using the existing current strategies.
1 selection for answer extraction
for a particular cluster  the svm answer extractor is trained on the documents obtained by running the retrieval component of the answering strategy  i.e. using the learned queries to retrieve documents . the basic features include proximity features  sentence statistics  and patterns  n-grams and paraphrases  that discriminate best between sentences that contain correct answers and sentences that do not. for the classifier  the value of these features is given by information gain.
　the answer extractor used in these experiments consists of a support vector machine  svm  classifier  joachims  1  producing probabilities  and whose task is to decide whether or not sentences contain at least one correct answer. the svm was trained on features extracted from one-sentence passages containing at least one keyword from the original question. the features consist of: distance between keywords and potential answers  keyword density in a passage  simple statistics such as document and sentence length  query type  lexical n-grams  up to six words in length   and paraphrases.
　under the cluster-based approach  it is not sufficient to train an answer extractor for each answering strategy. these strategies are trained on different number of questions  i.e. cluster size   they are sensitive to the notion of cluster relevance  and they are based on different query types and different relevant document distributions. therefore  extractor confidence has to be taken within the context of its history - i.e. the rest of the answering strategy. we measure the answer extraction confidence conf aae cj |q  of a strategy a derived from cluster cj given a new test question q:
conf aae cj |q  = p a+|aae cj   ， conf air cj |q 
 1 
where p a+|aae cj  is the probability of extracting a correct answer a+ using the answering strategy aae cj  -

figure 1: selection based on confidence yields the best performance after carefully selecting a limited number of strategies  cluster  queries  and extractor  to answer a question. however  for better redundancy it is better use additional answering strategies if further correct answers are required. no answer merging method has been used here - answers preserve their individual strategy-generated scores.more specifically  using the cluster-trained svm extractor. p a+|aae cj  is measured by testing its effectiveness on a held-out set of training questions from the cluster.
　in figure 1 we evaluate the effectiveness of our selection method  confidence selection  according to mrr  top1  and the fraction of correct answers extracted out of the total number of correct answers that would be extracted if all strategies were used. the random selection consists of random sampling from the available strategies and using them to extract more answers. the cluster size selection is an intuitive baseline which gives priority to more specific  focused strategies that correspond to clusters with higher similarity to the test question: p cj|q . however  it does not perform well  since cluster similarity is a necessary property  but it is not sufficient in the selection process. finally  the greedy oracle optimizes at each iteration the strategy that yields the most additional correct answers. in many cases  our confidence selection method performs virtually indistinguishable from the oracle sequence.
　while there is a benefit in using this selection method to quickly obtain a larger number of correct answers  if high answer redundancy is required  then further strategies must be used. however  in terms of mrr and top1 scores  a very small number of carefully selected strategies can be as effective as utilizing all of the available answering strategies. a very important observation is that performance does not degrade with subsequent iterations and increased number of strategies. this can be explained by the fact that the best strategies provide the highest confidence values and corresponding answer scores  while unsuccessful strategies do not introduce additional noise. in these experiments no answer merging method was used. each instance of an answer was treated separately  with its original confidence score given by a specific strategy. this approach does not provide any boost in confidence if the same answer has been seen more than once. however  it provides a measure of relative answering strategy noise and is informative to the performance of the answer extraction stage in a qa system.
1 selection for answer merging
at this stage in the qa pipeline  all answering strategies that were activated produce a strategy-specific answer set. the task of the answer merging component is to make use of redundancy and re-score the answers such that the correct answers are ranked higher than incorrect answers. the answering merging method we implemented consists of a weighted sum of the individual answer confidences for all answer instances with the same surface form. the answer confidence conf ak|q  of an answer ak at the end of the question answering process is aggregated across all clusters cj and is given by:

 1 
where p ak|aae cj  is the probability of extracting a correct answer ak using the answering strategy aae cj .
　both in terms of mrr and top1 scores  as seen from figure 1 and figure 1  the weighted answer merging method gains approximately 1 mrr points  1%  and also 1 top1 points  1%  in performance. the gap trade-off between using the confidence selection scores and using all strategies also improved. as in the case of answer extraction  it is encouraging that the confidence selection approach closely follows the greedy optimal selection.
1 conclusions & future work
an increasing number of question answering systems are relying on multi-strategy approaches in order to find answers to questions. they rely on multiple question classifications  answer extractors  multiple retrieval methods using several data sources  and different web-based services. while qa performance is often presented on batch processing of questions with no time constraints  in real-life scenarios  only a

figure 1: final qa performance - for answer merging  confidence based selection performance allows the qa system to select less than
1% of the strategies with nearly maximum performance. the trade-off is marginally better for the mrr metric  since it requires betterrelative scoring from answer merging.
limited number of these strategies can be fully explored. under these scenarios response time and performance trade-offs require careful selection of answering strategies such that performance is optimized subject to constraints.
　in this paper we presented a strategy selection approach that directly addresses these issues and we apply it to a statistical instance-based question answering system. through experiments we have shown the significant benefits of a principled strategy selection method on document retrieval  answer extraction  and answer merging  i.e. overall qa performance  using several metrics. by carefully selecting 1% of the available answering strategies  we obtained similar performance to the scenario in which we employ all strategies. moreover  the cluster-based confidence scoring method was also incorporated into answer merging which improved performance in terms of mrr and top1 significantly.
　we are currently experimenting with multiple answer extractors that are inherently different from svm-based extractors in order to increase the potential extraction coverage. we also intend to investigateadditional selection methods and explore active learning approaches. in particular  we are interested in the effect of incrementaltraining data on the selection of answering strategies.
