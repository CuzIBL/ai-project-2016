 
the multi-stream dependency detection algorithm finds rules that capture statistical dependencies between patterns in multivariate time series of categorical data  oates and cohen  1c . rule strength is measured by the g statistic  wickens  1   and an upper bound on the value of g for the descendants of a node allows msdd's search space to be pruned. however  in the worst case  the algorithm will explore exponentially many rules. this paper presents and empirically evaluates two ways of addressing this problem. the first is a set of three methods for reducing the size of msdd's search space based on information collected during the search process. second  we discuss an implementation of msdd that distributes its computations over multiple machines on a network. 
1 	introduction 
multi-stream dependency detection  msdd  is an algorithm for finding rules that capture statistical dependencies in databases. past applications of the algorithm include finding dependencies in multi-variate time series  oates and cohen  1c   learning probabilistic planning operators  oates and cohen  1b   and acquiring rules for correlating and predicting asynchronous events  oates et a/.  1 . in this paper  we describe three methods for reducing the size of the search space that msdd considers and empirically evaluate their utility. in addition  we present a version of the algorithm  called dmsdd  that distributes the search for rules over multiple machines on a network. the remainder of this section discusses the core msdd algorithm. section 1 describes the three search space reduction methods and section 1 summarizes our empirical work with them. section 1 presents d-msdd. finally  section 1 summarizes. 
	let d be a database containing t records: 	d = 
 each record is a set of unique tokens taken from the alphabet and the number of tokens may vary from record to record: 
 . let a pattern be defined in exactly the 
1 	machine learning 
same manner as a record. we say that pattern p occurs in record r if  = p. a rule  also called a dependency  consists of a pair of patterns  p and s  which are called the precursor and successor respectively. 
　for any given rule  we can construct a 1 contingency table that describes the frequency of co-occurrence of the corresponding patterns in a database. let count p  s  denote the number of records in d that contain both p and    i.e. for which  - i f either of the arguments to count is negated  then that argument must not appear in the records. for example  count p s  denotes the number of records in d that contain p but do not contain s  i.e. for which 
           . the following contingency table describes the frequency of co-occurrence of p and s: 

　rule strength is measured by the g statistic. g is a statistical measure of association  with large values indicating that p and s co-occur more or less frequently than one would expect by random chance  wickens  1 . g is computed for the table above as follows: 

ni is the expected value of ni under the assumption of independence  and is computed from the row margins and the table total. for example  n1 is the probability that p and s will co-occur in a database record  given that they are independent  times the number of records in the database. the probability of an occurrence of p is   the probability of an occurrence of s is c 1 /t  and the probability of the joint event given independence is therefore   = 
strong dependencies  as indicated by large g 

values  capture structure in the database because they tell us that there is a relationship between their constituent patterns  that occurrences of those patterns are not independent. 
　msdd performs a general-to-specific search in the space of all possible pairs of patterns defined over and returns the k strongest dependencies found  where k is supplied by the user. it is perhaps more correct to say that msdd returns all of the rules associated with the top k values of g found in the search space. the current version of msdd  in contrast to all other implementations reported previously  may return a number of rules much larger than k. the reason is that if multiple rules have exactly the same g value they are all retained  regardless of how different the rules themselves are. 
　the root of msdd's search tree contains two empty patterns  and all rules at depth n have the property that = n. the children of a node are generated by adding a token from to either the pre* cursor or the successor of that node. despite the fact that the current implementation of msdd is highly optimized c code which is capable of processing more than 1 rules per second  on some databases   the size of the search space is exponential in  and simply cannot be explored exhaustively. however  because msdd returns a list of the k strongest dependencies  it is possible to use an upper bound on the value of g for a rule's descendants to prune the search. if none of the descendants of a rule can have a g value higher than that of any of the current k best rules  then that rule can be pruned. in previous work we derived such an upper bound on g  making it possible for msdd to find the k strongest dependencies in an exponential space  oates and cohen  1c . 
　to ensure that good rules are found early in the search  and thus that pruning becomes effective early as well  msdd performs iterative deepening. this also causes the algorithm's memory requirements to be relatively meager. 
1 	improving search efficiency 
the first method for reducing the number of rules that 
msdd searches is based on the observation that the size of the fringe as a function of search depth often has the shape shown in figure 1. for this dataset  the vote dataset taken from the uc irvine collection   the number of fringe nodes initially rises sharply  reaching a peak of 1 1 nodes at depth nine  and then falls off just as sharply. there are two competing forces at work causing this behavior: the size of the search space and msdd's ability to prune. as search depth increases  the size of the search space grows dramatically. at shallow depths few good rules are found and msdd's pruning cannot stop this growth. eventually  though  a depth is reached at which most of the k best rules have been found  and the vast majority of the rules at subsequent depths have low g values due to the presence of extraneous tokens and can be pruned. to take advantage of this phenomenon  msdd stops performing iterative deepening when the size of the fringe falls below a user-specified fraction of its maximum size and instead performs a single depth-first search with no depth limitation. for the search shown in figure 1  the fact that the size of the fringe has peaked can be determined at the end of the search to depth ten; resulting in one final iteration of depth first search rather than the five iterations required by iterative deepening to terminate at depth fifteen. 

figure 1: the number of fringe nodes as a function of search depth on the vote dataset. 
　the second method for reducing the number of rules that msdd considers involves reordering the elements of  at each new depth. as with most implementations of depth-first search  search operators are applied in a fixed order to generate the children of a node. in the case of msdd  search operators add an element of to a rule to generate a child  and operators are ordered so that the operator that adds o% is applied before the one that adds cij for i   j. the result is that children for which the last token added has a lower index in  are expanded before children for which that index is higher. if the tokens in  are ordered so that the ones with low indices are the ones that appear in rules with high g values  then those rules will be generated early and pruning becomes more effective early. under the assumption that tokens that appeared in the k best rules at depth d will continue to be the most useful tokens at depth   we reorder the tokens in after each search depth so that the most frequently used tokens have the lowest indices in 
　the final performance enhancement takes advantage of the fact that the size of msdd's search space is exponential in . let # be the set of tokens that appear at least once in the final set of the k best rules. if  could be determined a priori  then the other tokens could be eliminated from leading to a potentially large reduction in the size of the search space. although it is impossible to determine prior to searching  it is often possible to identify that set by iteratively looking at subsets of even when  is very large  exhaustive search to a shallow depth is feasible. the tokens that appear in the k best rules found during that search serve as an initial approximation to   which we will denote . hereafter  all searches are performed using as the token set rather 
	oates  schmill. and cohen 	1 

than after the initial shallow search  the following procedure is repeated some fixed number of times: 
1. perform a depth-unlimited run of the standard iterative deepening search  using  . 
1. remove all variables in   that do not appear in any of the k best rules. 
1. add some small number of randomly selected variables in 
the search in step 1 is very fast because  is typically much smaller than   making it possible to perform many iterations of this procedure in the time normally required to perform one search cm the space defined by 
 unless the true k best rules individually contain very large numbers of tokens and rules containing subsets of those tokens all have low g scores  the procedure outlined above will eventually converge on $. there is no guarantee of such convergence in practice  especially when the number of iterations through the procedure is small. however  empirical results show that it performs quite well both in terms of cpu requirements and ability to find a good approximation to the set of rules found when searching over 
1 	experiments 
to evaluate the utility of the performance enhancements described above  we ran msdd on several datasets taken from the uc irvine collection. for each dataset  we ran msdd five times in each of the conditions below  with each iteration using a different random ordering of the variables: 
standard - no performance enhancements turned 
on 
fringe - switching from iterative deepening to depth-unlimited dfs when the fringe falls below 
1% of its maximum size 
order - reordering  after each search sample - five iterations of random sampling of five tokens in 
a l l - applying each of fringe  order and sample at the same time 
all experiments were run on a 1mhz dec alpha  and in each case k = 1. 
　results for six different datasets taken from the uc irvine repository are shown below. table 1 lists those datasets along with the number of records and the number of unique tokens  they contain. for each dataset  table 1 shows the mean number of cpu seconds and search nodes required to find the k best rules. in addition  the number of rules returned and their mean length  as measured by the sum of the number of tokens in their constituent patterns  is reported. finally  the disparity between the true k best rules and the actual rules found is shown  where disparity is the mean number of tokens by which each rule in the true rule set differs from its best match in the actual rule set returned. note that the standard  order and fringe conditions are all guaranteed to find the optimal rule set  and thus always have a disparity of zero. 
1 	machine learning 

table 1: datasets and their features that are relevant to msdd's run time. 
　on the vote dataset  standard expanded nearly one quarter of a billion nodes on average  requiring a little over one hour of cpu time. interestingly  the amount of search required by order and standard were virtually identical on this dataset. however  compared to standard  fringe was more efficient by a factor of two  and both sample and all were more efficient by an order of magnitude  t tests comparing mean cpu seconds and mean nodes expanded confirm these results  with all differences being highly significant. despite the fact that sample and all are not guaranteed to find the same rule set as the other conditions  they did so unfailingly  i.e. had a disparity of zero . 
　on the promoters dataset  standard expanded almost 1 million nodes on average  requiring a little under 1 minutes of cpu time. each of order  fringe  sample and all were significantly more efficient  as indicated by t tests  than standard. again  all was the least expensive condition  requiring 1 the amount of search required by standard. although the rules found in the sample and all conditions differed from the rules found in the other conditions on this dataset  there was substantial overlap. the mean g of the rules returned in the former two conditions was 1 and 1 respectively  whereas the mean g of the true k best rules was 
1. on average  the rules returned in the sample and all conditions differed from the optimal rule set by 1 and 1 tokens respectively. 
　results for the lymphography dataset show similar results  with significant speedups and low disparity  less than one token per rule . however  the results are rather different for flare  mushroom and tic-tac-toe. in each of these datasets  the order condition was virtually identical to the standard condition  with fringe performing significantly better. in each case  though  both sample and all performed significantly worse. the reason for this is made obvious by inspecting the final rule sets. almost every token in appears in the list of rules returned. therefore  rapidly approaches   causing each iteration of the sampling procedure to be almost as costly as a single run of standard. future work will include attempting to detect this condition as the search proceeds so as to stop iterating through the sampling procedure. 
1 	distributed search 
expansion of a node by msdd requires knowledge of the node itself  to determine which search operators are 


table 1: performance of msdd and its various enhancements on several datasets. 

valid    the list of all search operators   the dataset  to build contingency tables   and the current list of the k best nodes  to prune . only the list of the k best nodes changes dynamically during the search  making it possible to distribute the search space over multiple machines on a network as long as those machines have access to the same k best list. however  an out-of-date k best list will only result in underestimates of the priming threshold  so the algorithm will not suffer a loss of admissibility if local copies of the k-best list are updated lazily. 
　d-msdd uses a centralized model of communication to coordinate its distributed search. a single server acts as a hub for communication and user control  with one or more clients connecting via tcp/ip to offer their computational resources to the search. 
　the distributed search begins with the server initiating the distribution of data. once complete  the server expands the root of the search space to generate a single ply of the search space  and distributes it among the connected clients. work can begin at a searching machine as soon as there are nodes to be evaluated  and continues until all participating searchers report that they have processed their entire workloads. during the search  should a participant decide to add a rule to it's k-best list  the rule and its rating are broadcast to all of the other participants. 
　the major advantage of distributing the search for dependencies across multiple computing resources is obvious: in the ideal case  a computation requiring milliseconds of computing time would take milliseconds to complete on n machines. due to message passing and other overhead  this idealized speedup is difficult to obtain  but it is the goal of parallel and distributed computation to come as close to it as possible. the key to realizing this goal is to keep all of the distributed resources as busy as possible while reducing message passing and other overhead to a minimum. 
1 	load balancing 
some studies have been made of provably optimal loadbalancing policies. most  if not all such studies  such as  gao et a/.  1   require a priori knowledge about the structure of the search space. the msdd search space can indeed be enumerated and reasoned about  but due to pruning  the effective search space  that space which is actually searched  cannot be determined a priori. for this reason  optimality results based on tree sweep procedures do not directly apply to d-msdd. 
　many solutions to the load balancing problem have been proposed for problems for which optimality results do not apply  such as ida* search  cook  1 . in general  these load balancing algorithms can be distinguished in two ways. the first distinction can be made based on what is partitioned  and subsequently distributed : the computational space  or the data. in functioned decomposition  distinct portions of the computational space are distributed among processing dements. in data decomposition  the data is distributed. with msdd  the systematic nature of the search space allows disjoint sets of nodes to be evaluated independently. the same process if the data were partitioned would not allow the searchers to operate independently; every result generated by a host would need to be synchronized with every other host. as such  the logical 
oates  schmill  and cohen 1 

approach to partitioning  and the one we take  with dmsdd is functional decomposition. 
　the second distinction among distributed algorithms is made between static load .balancing and dynamic load balancing. static load balancing attempts to divide the 
data prior to the beginning of the distributed computation. for d-msdd  static load balancing equates to dividing the first ply of the search space among the distributed processing elements. dynamic load balancing takes place while the search is in progress. an example of dynamic load balancing in d-msdd would be a processor with a large agenda offloading some of its work to a processor with few nodes on its agenda. good static analysis can make dynamic load balancing unnecessary  reducing communication overhead and idle cpu cycles. 
　we explored the utility of two static load balancing policies in d-msdd. a capacity sensitive policy ensures that each searcher receives a number of nodes proportional to its processing capacity by consulting a database of known clients and architectures containing estimates of their processing capacity. the capacity estimates in the database reflect the mean number of nodes expanded per second over a fixed reference trial. a potential improvement on this policy is to take into account the number of children a search node can parent. we will call the number of search operators that apply at a rule its rank. rank can be used to compute the size of the un-
pruned search space parented by rule r. let spacesize be the maximum number of nodes a searcher will have to expand if it is given rule r as its workload. certainly  the maximum amount of work a searcher could have to do is much greater than the work a search will do on most datasets. rank and spacesize  however  are statistics that can be computed a priori  while effective spacesize is not. d~msdd's rank-based policy attempts to balance  in a capacity sensitive manner  the total spacesize it allocates to different searchers. 
　dynamic load balancing schemes are a class of algorithms that perform load balancing after work begins. for d-msdd  dynamic load balancing is initiated when a client detects that its agenda is about to become empty. in such a situation  the client sends a message to the server indicating that it can take on more work. this is referred to as receiver initiated load balancing  as the eventual recipient initiates the transfer of work. 
　when the server receives the work request  it first checks its own agenda to see if there is enough work there to offload some minimum number of nodes. if there is  the server invokes a static load balancing policy to rebalance its load with respect to the client. if the server does not have enough nodes to offload to a waiting client  or its own agenda becomes empty  it broadcasts a request for work to all connected clients  who themselves invoke the static load balancing algorithm when possible. 
　the message passing associated with dynamic rebalancing also provides an opportunity to obtain more upto-date information for use in load balancing. in particular  by the time a searcher has expended its agenda  it will have more recent estimates of its own processing 
1 	machine learning 

figure 1: results for the solar flares data after adding dynamic load balancing to d-msdd.  a  the effect on the time to completion for unloaded machines  b  the effect on mean cpu utilization. 

figure 1: results for the chess data after adding dynamic load balancing to d-msdd.  a  the effect on the time to completion for unloaded machines  b  the effect on mean cpu utilization. 
capacity. updates to the capacity lookup table are sent to the server along with each request for more work. 
1 	evaluation 
the basic msdd algorithm has been shown to be effective in terms of the quality of the rules it discovers  oates and cohen  1a  and efficient in its search of very large spaces. our goal in evaluating d-msdd is to test the hypothesis that efficiency increases proportionally to the computing resources that are added to the search. 
　we measure performance gain  or loss  through four variables: the total number of nodes expanded  cpu time  cpu utilization  and the number of messages generated. the number of nodes considered in the search is a raw measure of computational expense. cpu time is measured in milliseconds as the sum of system and user time spent on behalf of msdd . all results reported here are for machines in which d-msdd is the primary load. cpu utilization measures the percentage of real time that the open list of a machine is non-empty. in our experiments  we record the mean cpu utilization across the nodes in a search as well as the minimum utilization. finally  the number of messages generated is simply a tally of the tcp/ip messages sent from any searcher to another during the search. 
　the datasets used for evaluation of d-msdd ware the solar flare dataset and the chess endgame dataset  both from the uc irvine repository. in all cases  k was set to 1. the number of machines involved in the search varied from one to five  and included one 1mhz alpha  three 1mhz alphas  and one 1mhz sparclo. 


figure 1: the number of network messages generated during search with dynamic load balancing turned on.  a  the solar flare dataset  b  the chess dataset. 
　it should be noted that d-msdd was built on an old version of msdd that was implemented in lisp. therefore  direct comparisons of running times and numbers of nodes expanded between d-msdd and the current implementation of msdd are impossible. however  we believe that the results in this section will be qualitatively the same when d-msdd is re-implemented on the c version of msdd. 
　performance results with dynamic load balancing working in conjunction with both the capacity and rank static load balancing algorithms are shown in figures 1 and 1. the graphs of mean cpu utilization show the effect that we had hoped for. for both the solar flares and chess datasets  mean cpu utilization show only slight decreases as processors are added  and are generally within the 1% range. the minimum cpu utilization  not shown  exhibited similar behavior  in most cases between 1%. as a result  the mean completion time decreases in an apparently linear fashion as processors are added to the search. recall that the machine in the single processor case is an alpha approximately 1 times faster than the machines added in the 1 and 1 processor cases. in the ideal case  then  the performance increase would be around 1%. with the rank based load policy  the mean speedup in our trials was 1% for the solar flares and 1% on the chess data. the dynamic load balancing scheme achieves high levels of cpu utilization despite the relatively poor scheduling information available to the static policies. 
　figure 1 shows the number of network messages generated under the dynamic load balancing scheme. the number of messages generated while searching the solar flare and chess datasets appears linear with respect to the number of searchers and in the thousands. 
1 	discussion 
although the core msdd algorithm is capable of finding complex dependencies in exponential search spaces  we demonstrated the utility of three methods for further reducing the number of rules that msdd considers. the impact of the methods depends to a large extent on the nature of the database  but in ail cases at least one of the methods resulted in significant reductions in running time. if one is willing to forgo msdd's optimaiity guarantees  with respect to the g values of the final rule set   then reductions in cpu time and nodes expanded of up to an order of magnitude are possible while still finding high quality rules. in addition  we demonstrated that it is possible to distribute msdd's search for structure over multiple networked machines and achieve an almost linear speedup in the number of machines used. 
acknowledgements 
this research is supported by darpa/afosr under contract number f1-1. the u.s. government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation hereon. the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements  either expressed or implied  of the defense advanced research projects agency  air force office of scientific research or the u.s. government. 
