
we describe a new sequential learning scheme called  stacked sequential learning . stacked sequential learning is a meta-learning algorithm  in which an arbitrary base learner is augmented so as to make it aware of the labels of nearby examples. we evaluate the method on several  sequential partitioning problems   which are characterized by long runs of identical labels. we demonstrate that on these problems  sequential stacking consistently improves the performance of non-sequential base learners; that sequential stacking often improves performance of learners  such as crfs  that are designed specifically for sequential tasks; and that a sequentially stacked maximum-entropy learner generally outperforms crfs.
1	introduction
in this paper  we will consider the application of sequential probabilistic learners to sequential partitioning tasks. sequential partitioning tasks are sequential classification tasks characterized by long runs of identical labels: examples of these tasks include document analysis  video segmentation  and gene finding.
　motivated by some anomalous behavior observed for one sequential learning method on a particular partitioning task  we will derive a new learning scheme called stacked sequential learning. like boosting  stacked sequential learning is a meta-learning method  in which an arbitrary base learner is augmented-in this case  by making the learner aware of the labels of nearby examples. sequential stacking is simple to implement  can be applied to virtually any base learner  and imposes only a constant overhead in training time: in our implementation  the sequentially stacked version of the base learner a trains about seven times more slowly than a.
　in experiments on several partitioning tasks  sequential stacking consistently improves the performance of nonsequential base learners. more surprisingly  sequential stacking also often improves performance of learners specifically designed for sequential tasks  such as conditional random fields and discriminatively trained hmms. finally  on our set of benchmark problems  a sequentially stacked maximumentropy learner generally outperforms conditional random fields.
1	motivation: a hard task for memms
to motivate the novel learning method that we will describe below  we will first analyze the behavior of one well-known sequential learner on a particular real-world problem. in a recent paper  carvalho and cohen  1   we evaluated a number of sequential learning methods on the problem of recognizing the  signature  section of an email message. each line of an email message was represented with a set of handcrafted features  such as  line contains a possible phone number    line is blank   etc. each email message was represented as a vector x of feature-vectors x1 ... xn  where xi is the feature-vector representation of the i-th line of the message. a line was labeled as positive if it was part of a signature section  and negative otherwise. the labels for a message were represented as another vector y  where yi is the label for line i.
　the dataset contains 1 labeled lines from 1 email messages. about 1% of the lines are labeled  positive . signature sections always fall at the end of a message  usually in the last 1 lines. in the experiments below  the data was split into a training set  of 1 sequences/emails   and a test set with the remaining sequences  and we used the  basic  feature set from carvalho & cohen.
　the complete dataset is represented as a set s of examples s = { x1 y1  ...  xt yt  ...  xm ym }. sequential learning is the problem of learning  from such a dataset  a sequential classifier-i.e.  a function f such that f x  produces a vector of class labels y. clearly  any ordinary non-sequential learning algorithm can be used for sequential learning  by ignoring the sequential nature of the data1.
　in the previous paper  carvalho and cohen  1   we reported results for several non-sequential and sequential learners on the signature-detection problem  including a nonsequential maximum entropy learner  berger et al.  1 
methodnoiseerrormin errorme11memm11crf11memm1%11crf1%11table 1: performance of several sequential learners on the signature-detection problem.
 henceforth me  and conditional random fields  lafferty et al.  1   henceforth crfs . another plausible sequential learning method to apply to this task are maximumentropy markov models  memms   mccallum et al.  1   also called maximum-entropy taggers  ratnaparkhi  1   conditional markov models  klein and manning  1   and recurrent sliding windows  dietterich  1 . in this model  the conditional probability of a label sequence y given an instance sequenceq x is defined to be pr y|x  = i pr yi|yi 1 xi . the local model pr yi|yi 1 xi  is learned as follows. first one constructs an extended dataset  which is a collection of non-sequential examples of the form   xi yi 1  yi   where  xi yi 1  denotes an instance in which the original feature vector for xi is augmented by adding a feature for yi 1. we will call  xi yi 1  an extended instance  and call yi 1 a history feature. note that yi is the class label for the extended example   xi yi 1  yi .
　after constructing extended instances  one trains a maximum-entropy conditional model from the extended dataset. inference is done by using a viterbi search to find the best label sequence y.
　memms have a number of nice properties. relative to the more recently-proposed crf model  memms are easy to implement  and  since no inference is done at learning time  relatively quick to train. memms can also be easily generalized by replacing the local model with one that uses a longer  history  of k previous labels-i.e.  a model of the form pr yi|yi 1 ... yi k xi -and replacing the viterbi search with a beam search. such a learner scales well with the history size and number of possible classes y.
　unfortunately  as table 1 shows  memms perform extremely badly on the signature-detection problem  with an error rate many times the error rate of crfs. in fact  on this problem  memms perform much worse than the nonsequential maximum-entropy learner me.1
　the memm's performance is better if one changes the threshold used to classify examples. letting p i be the probability pr yi = +|xi yi 1  as computed by memm  we found  for each learner  the threshold θ such the rule   yi = +     p i   θ   gives the lowest test error rate. the column labeled  min error  in table 1 gives this  best possible  result. the  min error  for memms is much improved  but still higher than non-sequential me.
the high error occurs because on many test email messages  the learned memm makes a false positive classification somewhere before the signature starts  and then  gets stuck  and marks every subsequent line as part of a signature. this behavior is not consistent with previously-described limitations of memms. it is known that memms can represent only a proper subset of the distributions that can be represented by crfs  lafferty et al.  1 ; however  this  label bias problem  does not explain why memms perform worse than non-sequential me  since memms clearly can represent strictly more distributions that me.
　klein and manning  also describe an  observation bias problem   in which memms give too little weight to the history features. however  in this case  relative to the weights assigned by a crf  memm is seems to give too much weight to the history features. to verify this  we encouraged the memm to downweight the history features by adding noise to the training  not test  data: for each training email/sequence x  we consider each feature-vector xi （ x in turn  and with probability 1  we swap line xi  together with its label yi  with some other xj yj chosen uniformly from the sequence. adding this  sequence noise  almost doubles the error rate for crfs  but reduces the error rate for memms.  of course  this type of noise does not affect nonsequential me.  this experiment supports the hypothesis that memm is overweighting history features.
1	stacked sequential learning
1	description
the poor results for memm described above can be intuitively explained as a mismatch between the data used to train the local models pr yi|yi 1 x  of the memm  and the data used to test these models. with noise-free training data  it is always the case that a signature line is followed by more signature lines  so the memm's local model tends to weight the history feature yi 1 heavily. however  this strong regularity does not hold when the model is applied to test data in the course of executing the viterbi algorithm  because then the local model is applied to extended examples  xi y i 1   where y i 1 is a prediction made by the local model.
　in theory  of course  this training/test mismatch is compensated for by the viterbi search  which is in turn driven by the confidence estimates produced by the local model. however  if the assumptions of the theory are violated  for instance  if there are high-order interactions not accounted for by the maximum-entropy model   the local model's confidence estimates may be incorrect.
　one way to correct the training/test mismatch is to modify the extended dataset that is used to train the local model so that the true previous class yi 1 in an extended instance
 xi yi 1  is replaced by a predicted previous class y i 1. below we will outline a way to do this.
　assume that one is given a sample s = { xt yt } of size m  and a sequential learning algorithm a. previous work on a meta-learning method called stacking  wolpert  1  suggests the following scheme for constructing a sample of
 x y   pairs in which y  is a vector of  predicted  class-labels for x. first  partition s into k equal-sized disjoint subsets s1 ... sk  and learn k functions f1 ... fk  where
stacked sequential learning.
parameters: a history size wh  a future size wf  and a crossvalidation parameter k.
learning algorithm: given a sample s = { xt yt }  and a sequential learning algorithm a:
1. construct a sample of predictions y t for each xt （ s as follows:
 a  split s into k equal-sized disjoint subsets s1 ... sk
 b  for j = 1 ... k  let fj = a s   sj 
 c  let s  = { xt y t  : y t = fj xt  and xt （ sj}
1. construct an extended dataset s1 of instances  by converting each xt to x1t as follows: xwhere x1i =  xi y i wh ... y i+wf  and y i is the i-th component of y t  the label vector paired with xt in s .
1. return two functions: f = a s  and f1 = a s1 .
inference algorithm: given an instance vector x:
1. let y  = f x 
1. carry out step 1 above to produce an extended instance x1  using y  in place of y t .
1. return f1 x1 .
table 1: the sequential stacking meta-learning algorithm.

fj = a s   sj . then  construct the set
s  = { xt y t  : y  = fj xt  and xt （ sj}
in other words  s  pairs each xt with the y t associated with performing a k-fold cross-validation on s. the intent of this method is that y  is similar to the prediction produced by an f learned by a on a size-m sample that does not include x.
　this procedure is the basis of the meta-learning algorithm of table 1. this method begins with a sample s and a sequential learning method a. in the discussion below we will assume that a is me  used for sequential data.
　using s  a  and cross-validation techniques  one first pairs with each xt （ s the vector y t associated with performing cross-validation with me. these predictions are then used to create a dataset s1 of extended instances x1  which in the simplest case  are simply vectors composed of instances of the form  xi y i 1   where y i 1 is the  i   1 -th label in y .
　the extended examples s1 are then used to train a model f1 = a s1 . if a is the non-sequential maximum-entropy learner  this step is similar to the process of building a  local model  for an memm: the difference is that the history features added to xi are derived not from the true history of xi  but are  approximations of  the off-sample predictions of an me classifier.
　at inference time  f1 must be run on examples that have been extended by adding prediction features y . to keep the  test  distribution similar to the  training  distribution  f will not be used as the inner loop of a viterbi or beam-search
process; instead  the predictions y  are produced using a nonsequential maximum-entropy model f that is learned from s. the algorithm of table 1 simply generalizes this idea from me to an arbitrary sequential learner  and from a specific history feature to a parameterized set of features.
　in our experiments  we introduced one small but important refinement: each  history feature  y  added to an extended example is not simply a predicted class  but a numeric value indicating the log-odds of that class. this makes accessible to f1 the confidences previously used by the viterbi search.
1	initial results
we applied stacked sequential learning with me as the base learner  henceforth s-me  to the signature-detection dataset.
we used k = 1  wh = 1  and wf = 1. the s-me method obtains an error rate of 1% on the signature-detection task-less than the baseline me method  1%  but still higher than crfs  1% . however  certain extensions dramatically improve performance.
　for s-me  the only impact of more  history  features is to add new features to the extended instances; hence like memms  s-me can efficiently handle large histories. on the signature-detection task  increasing the history size to 1 reduces error  slightly  to 1%.
　for s-me  the extended instance for xi can include predicted classes not only of previous instances  but also of  future  instances-instances that follow xi in the sequence x. we explored different  window sizes  for s-me  where a  window size  of w means that wh = wf = w  i.e.  the w previous and w following predicted labels are added to each extended instance. a value of w = 1 reduces error rates to only 1%  a 1% reduction from crf's error rate of 1%. this improvement is statistically significant.1
　finally  stacked sequential learning can be applied to any learner-in particular  since the extended examples are sequential  it can be applied any sequential learner. we evaluated stacked sequential crfs  henceforth s-crfs  with varying window sizes on this problem. a value of w = 1 reduces error rates to 1%  a statistically significant improvement over crfs. however  for moderately large window values  there was little performance difference between s-crf and s-me.
1	discussion
a graphical view of a memms is shown in figure 1 a . we use the usual convention in which nodes for known values are shaded. each node is associated with a maximum-entropy conditional model which defines a probability distribution given its input values.
　figure 1 b  presents a similar graphical view of the classifier learned by sequential stacking with wh = 1 and wf = 1. inference in this model is done in two stages: first the middle layer is inferred from the bottom later  then the top layer is inferred from the middle layer. the nodes in the middle layer are partly shaded to indicate that their hybrid status-they are considered outputs by the model f  and inputs by the model f1.

	 a  memm	 b  sequential stacking  wh =1  wf =1	 c  sequential stacking  w =1
figure 1: graphical views of alternative sequential-stacking schemes.
　one way to interpret the hybrid layer is as a means of making the inference more robust. if the middle-layer nodes were treated as ordinary unobserved variables  the top-layer conditional model  f1  would rely heavily on the confidence assessments of the lower-layer model  f . forcing f1 treat these variables as observed quantities allows f1 to develop its own model of how the y  predictions made by f correlate with the actual outputs y. this allows f1 to accept or downweight f's predictions  as appropriate. as suggested by the dotted line in the figure  stacking conceptually creates a  firewall  between f and f1  insulating f1 from possible errors in confidence made by f.
　figure 1 c  shows a sequential stacking model with a window of wh = wf =1. to simplify the figure  only the edges that eventually lead to the node yi are shown.
　to conclude our discussion  we note that as described  sequential stacking increases run-time of the base learning method by approximately a constant factor of k +1.  to see this  note sequential stacking requires training k +1 classifiers: the classifiers f1 ... fk used in cross-validation  and the final classifiers f and f1.  when data is plentiful but training time is limited  it is also possible to simply split the original dataset s into two disjoint halves s1 and s1  and train two classifiers f and f1 from s1 and respectively  where
  extended with the predictions produced by f . this scheme leaves training time approximately unchanged for a linear-time base learner  and decreases training time for any base learner that requires superlinear time.
1 further experimental results
1	additional segmentation tasks
we also evaluated non-sequential me  memms  crfs  sme  and s-crfs on several other sequential partitioning tasks. for stacking  we used k = 1 and a window size of wh = wf =1 on all problems. these were the only parameter values explored in this section  and no changes were made to the sequential stacking algorithm  which was developed based on observations made from the signature-detection task only.
　one set of tasks involved classifying lines from faq documents with labels like  header    question    answer   and  trailer . we used the features adopted by mccallum et al
taskmemmmecrfs-mes-crfa/aigen11111a/ainn11111a/aix11111t/aigen11111t/ainn11111t/aix111111/video111111/video11111mailsig11111table 1: error rate comparison of different sequential algorithms on a set of nine benchmark tasks.

figure 1: comparison of the error rates for s-me with the error rates of me  memm  and crfs.
 and the three tasks  ai-general  ai-neural-nets  and aix  adopted by dietterich et al . the data consists of 1 long sequences  each sequence corresponding to a single faq document; in total  each task contains between 1 and 1 labeled lines. our current implementation of sequential stacking only supports binary labels  so we considered the two labels  trailer   t  and  answer   a  as separate tasks for each faq  leading to a total of six new benchmarks.
　another set of tasks were video segmentation tasks  in which the goal is to take a sequence of video  shots   a sequence of adjacent frames taken from one camera  and classify them into categories such as  anchor    news  and  weather . this dataset contains 1 sequences  each corresponding to a single video clip. there are a total of 1 shots  and about 1 features  which are produced by applying lda to a 1  1-bin rgb color histogram of the central frame of the shot.  this data was provided by yik-cheung tam and ming-yu chen.  we constructed two separate video partitioning tasks  corresponding to the two most common labels.
　all eight of these additional tasks are similar to the signature-detection task in that they contain long runs of identical labels  leading to strong regularities in constructed history features. error rates for the learning methods on these eight tasks  in addition to the previous signature-detection task  are shown in table 1. in each case a single train/test split was used to evaluate error rates. the bold-faced entries are the lowest error rate on a row.
　we observe that memms suffer extremely high error rates on two of the new tasks  finding  answer  lines for ai-general and ai-neural-nets   suggesting that the  anomalous  behavior shown in signature-detection may not be uncommon  at least in sequential partitioning tasks.
　also  comparing s-me to me  we see that s-me improves the error rate in 1 of 1 tasks  and leaves it unchanged once. furthermore  s-me has a lower error rate than crfs 1 of 1 times  and has the same error rate once. there is only one case in which memms have a lower error rate than s-me.
　overall  s-me seems to be preferable to either of three older approaches  me  memms  and crfs . this is made somewhat more apparent by the scatter plot of figure 1. on this plot  each point is placed so the y-axis position is the error of s-me  and the x-axis position is the error of an earlier learner; thus points below the line y = x are cases where sme outperforms another learner.  for readability  the range of the x axis is truncated-it does not include the highest error rates of memm. 
　stacking also improves crf on some problems  but the effect is not as consistent: s-crf improves the error rate on 1 of 1 tasks  leaves it unchanged twice  and increases the error rate twice. in the table  one of the two stacked learners has the lowest error rate on 1 of the 1 tasks.
　applying a one-tailed sign test  the improvement of s-me relative to me is statistically significant  p   1   but the improvement of s-crf relative to crf is statistically not  p   1 . the sign test does not consider the amounts by which error rates are changed  however. from the figures and tables  it is clear that error rates are often lowered substantially  and only once raised by more than a very small proportion  for the  a/aix  benchmark with crfs .
1	a sequence classification task
as a final test  we explored one additional non-trivial task: classifying popular songs by emotion. this task was considered after all code development was complete  and hence is a completely prospective test of sequential stacking. a collection of 1 popular songs was annotated by two students on a five-point scale:  very happy  1    happy  1    neu-
taskmemmmecrfs-mes-crfmusic1.1.1.1.1.1music1.1.1.1.1.1table 1: comparison of different sequential algorithms on a music classification task.
tral  1    sad  1  and  very sad  1 . the pearson's correlation  walpole et al.  1  was used as a measure of interannotator agreement. the pearson's correlation coefficient ranges from  1  perfect disagreement  to +1  perfect agreement  and the inter-annotator agreement between the two students was 1.
　to learn a song classifier  we represented each song as a sequence of 1-second long  frames   each frame being labeled with the emotion for the song that contains it. we then learned sequence classifiers from these labeled sequences. to classify an unknown song  we used the sequence classifier to label the frames for the song  and finally labeled the song with the most frequent predicted frame label.
　the  frames  are produced by extracting certain numerical properties from a waveform representation of the song every 1ms  and then averaging over 1-second intervals. each 1second frame has as features the mean and standard deviation of each property. the numerical properties were computed using the marsyas toolkit  tzanetakis and cook  1   and are based on the short-time fourier transform  tonality  and cepstral coefficients.
　the music dataset contains 1 frames from the 1 songs  with 1 features for each frame. we looked at two versions of the problem: predicting all of the five labels  music1   and predicting only  happy  versus  sad  labels  music1 . preliminary experiments suggested that large windows were effective  thus we used the following parameters in the experiments: k = 1 and a window size of wh = wf = 1 on music1 problem  and k = 1 and wh = wf = 1 on music1.
　the results are summarized in table 1. both s-crf and s-me outperform their non-stacking counterparts  and s-me outperforms crfs. furthermore  s-crf improves substantially over unstacked crfs on the two-class problem  reducing the error rate by more than 1%.
1	conclusions
sequential partitioning tasks are sequential classification tasks characterized by long runs of identical labels: examples of these tasks include document analysis  video segmentation  and gene finding. in this paper  we have evaluated the performance of certain well-studied sequential probabilistic learners to sequential partitioning tasks. it was observed that memms sometimes obtain extremely high error rates. error analysis suggests that this problem is neither due to  label bias   lafferty et al.  1  nor  observation bias   klein and manning  1   but to a mismatch between the data used to train the memm's local model  and the data on which the memm's local model is tested. in particular  since memms are trained on  true  labels and tested on  predicted  labels  the strong correlations between adjacent labels associated sequential partitioning tasks can be misleading to the memm's learning method.
　motivated by these issues  we derived a novel method in which cross-validation is used correct this mismatch. the end result is a meta-learning scheme called stacked sequential learning. sequential stacking is simple to implement  can be applied to virtually any base learner  and imposes an constant overhead in learning time  the constant being the number of cross-validation folds plus two . in experiments on several partitioning tasks  sequential stacking consistently improves the performance of two non-sequential base learners  often dramatically. on our set of benchmark problems  sequential stacking with a maximum-entropy learner as the base learner outperforms crfs 1 of 1 times  and ties once. also  on a prospective test conducted on a completely new task  sequential stacking improves the performance of both crfs and maximum-entropy learners  leading in one case to a 1% reduction in error over conventional crfs.
　one of the more surprising results  for us  is that sequential stacking also often improves performance of conditional random fields  a learner specifically designed for sequential tasks. in a longer version of this paper  we conducted similar with two margin-based base learners: the non-sequential voted perceptron algorithm  vp  freund and schapire  1  and a voted-perceptron based training scheme for hmms proposed by collins  vphmms   collins  1   with qualitatively similar results.
　some initial experiments on a named entity recognition problem suggest that sequential stacking does not improve performance on non-partitioning problems; however  in future work  we plan to explore this issue with more detailed experimentation.
acknowledgements
the authors wish to thank many friends and colleagues for input  in particular david mcallister. we are also grateful to yik-cheung tam and ming-yu chen for providing the videosegmentation data  and to chih-yu chao for the popular music dataset labels. this material is based upon work supported by the defense advanced research projects agency  darpa . any opinions  findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the defense advanced research projects agency  or the department of interior-national business center.
