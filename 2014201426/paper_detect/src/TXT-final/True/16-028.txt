 
　　this paper is a progress report on a scries of three significant extensions to the original parsing design of  marcus j1 .* the extensions are: ihe range of syntactic phenomena handled has been enlarged  encompassing sentences with verb phrase deletion  gapping  and rightward movement  and an additional output representation of anaphor-antcccdcnt relationships has been added  including pronoun and quantifier interpretation . a complete analysis of the parsing design has been carried out  clarifying the parser's relationship to the extended i r k t  parsing method as originally defined by  knuth 1  and explored by  szymanski and williams 1 . the formal model has led directly to the design of a  stripped down  parser that uses standard lr k  technology and to results about the class of languages that can be handled by marcus-style parsers  briefly  the class of languages is defined by those that can be handled by a deterministic  two-stack push-down automaton with severe restrictions on the transfer of material between the two sucks  and includes some strictly 
context-sensitive languages . 
1 extending the marcus parser 
　　while the marcus parser handled a wide range of everyday syntactic constructions  there are many common english sentences that it could not analyze. one gap in its abilities arises because it did not have a way to represent the possibility of rightward movement - that is  cases where a constituent is displaced to the right: 
　　　a book  about nuclear disarmament  appeared yesterday. --  a book appeared yesterday  about nuclear disarmament . further  the only way that the marcus parser could handle leftward movement was via the device of linking a  dummy variable   a trace  to an antecedent occurring somewhere earlier in the sentence. for instance  the sentence   who did mary kiss   is parsed as  who did mary kiss trace!  where trace is a variable bound to its  value  of who  indicating the intuitive meaning of the sentence   for which x  did mary kiss x . 
　　jn the original parser design  a trace was of the category np  so that only noun phrases could be linked to traces. but this meant that sentences where other than nps are displaced or deleted cannot be analyzed. this includes the following kinds of sentences  where deleted material is indicated in square brackets. 
gapping: 
max gave sally a nickel  and harvey  gave sally  a dime. 
  this paper reports work done at the artificial intelligence laboratory of the massachuselts institute of technology support for ihe laboratory's artificial intelligence research is supported in pan by the advanced research projects agency of the department of defense under office of naval research contract noo1-c-1. vp deletion: 
john kissed mary  and i think that frank said that mary thought that marry would have  kissed mary  too. 
   the new parser is also designed so as to account for the ill-formedness of certain sentences related to these  such as the 
 gapped  sentence  
     john hit mary and i don't believe bill  hit  sue. the new design actually explains why gapping is bounded in a way that vp deletion is not. finally  since languages such as german arguably contain rules that move verbs  the german  verb second  rule; see  thiersch 1 . an extension to the trace system is demanded here as well. 
　　the last class of new cases to be handled includes the encoding of information not dealt with in the original marcus parser  that of oven anteccdent-anaphor binding. such sentences include: 
reciprocals: 
they think that feeding each other would be dangerous. 
 they-each other} 
pronouns: 
john thinks that he is wonderful. 
{john = he. under one interpretation} his mother likes john. 
　　how can we begin to attack these cases  let us consider rightward movement first. it is a simple descriptive fact about such sentences that the moved constituent has been displaced from its  normal  location. by analogy with the case of leftward movements  the obvious first tack is to place a variable in the position from which the constituent has been moved  and then bind that variable to the appropriate constituent appearing to its right: 
　　　a book  t  appeared yesterday about nuclear disarmament. this solution is not quite correct  however  since it fails to explain a key property about rightward movement  in fnglish : unlike leftward movement  it is restricted to the domain of a single s cntcncc . but if right- and leftward movements were determined by the same mechanism  there should be no such asymmetry  all other things being equal. however  there is solution within the framework of the deterministic marcus parser that accounts for this asymmetry. since the constituents  a book  and  a book about nuclear disarmament  arc both completely well formed  there is no way for a deterministic parser to be able to record the possibility that  a book  may be lacking a complement   about nuclear disarmament   unless there is explicit evidence in the input stream within a bounded distance of the np a book . this is because all rules in the parser must be stated in a finite  if-then  format that can make reference to at most three constituents of look-ahead information  a bound that prohibits one from moving  about nuclear disarmament  too far to the right. ihe idea  then  is that instead of adding a variable  t  to the np  a book   we simply flag 

the np with the diacritic  +rightmoved  if there is explicit evidence in the look-a head buffer that this is correct. this flag indicates that the argument structure of the np is not yet complete  it lacks a complement . observe that in the case above the lookahcad buffer can contain a proper triggering pattern  since at the time  a book  must be marked  the first cell in the buffer will contain  appeared   the second   yesterday   and the third  
 about .  more generally  a complete np as discussed in  marcus 1  could reside in the second lookahcad cell  and the a pp in the third. note that the type of the pp will determine whether it is a possible complement of the subject np.  then  when the pp complement is encountered and parsed  it is literally attached to the np that is so flagged.  note that this is in contrast to the  trace  solution  where variable binding is used to effect the  restoration  of initial argument structure. here  the np is  reconstructed .  
　　what of sentences with deleted verb phrases or  gapped  constituents  again  the key distinction is that  gapping  is bounded in a way that vp deletion is not  as illustrated by the vp deletion sentence above where the deleted vp  kissed mary  can be arbitrarily far from its copy . a  gapped  constituent  in contrast  is locally detectable  and so we can attempt to formulate local if-then grammar rules to handle it. importantly  there appear to be constraints on natural grammars that prohibit identical gapped constructions that could have been derived from two different underlying sources -- just what is needed in order to accommodate these examples in a deterministic parser. further  it seems that gapped constituents must lie at the left or right periphery of complete sub-trees. for a discussion of these constraints  see  hankamer 1 . lack of space prevents a complete description of this rule system here  but to take just one example  consider the following sentence  along with its underlying form : max gave sally a nickel yesterday  and  max gave sally  a dime today. 
the parser will analyze the first part of the conjunct  up to the 
 and   as a complete s. next   and  will be pushed onto the parser's stack  delaying the decision as to what to do until we look at material in the input buffer. at this point  the lexical items  a dime  in the input buffer triggers the analysis of an np  note that 
 today  unambiguously marks the end of the np . following the marcus parser design  this np is returned to the input buffer  which now contains np in its first cell   today  in its second  and an end of sentence marker in its third cell.  recall that in general the buffer is limited to contain just three items.  but the coordination structure  s and s  demands that a full s  np vp  be found. the periphery constraint  in turn  requires that the deleted material form lie at the left or the right of  np today   forming a complete subtree. this means that the only possible choices arc  ...  np today or np today  ...  where  ...  together with  a dime today  forms an s. but since the sequence np today  ...  is known not to form an s  the only remaining choice is  ...  np today. that is  the parser inserts a  e  constituent into the first position of the buffer and takes  e  np today as forming an s. the interpretation of the  c  is left for a second stage of processing. 
　　it is interesting to contrast this example with comparable vp deletion sentences. the deleted vp can be arbitrarily far away from its copy  but not the gapped constituent: 
  max gave sally a nickel yesterday  and john thought that mary believed a dime today. 
max gave sally a nickel yesterday  and john thought that mary believed that bill did too. 
the point is that by abiding by observable linguistic constraints  we can actually design a parser to efficiently handle the sentences of a 
	r. berwick 	1 
natural language.  conversely  the demand that sentences be efficiently parsable at least suggests a  functional  explanation for dicsc kinds of constraints and evidence for deterministic parsing. however  this possibility must remain a suggestion  since the constraints are not necessary for efficient payability.  
   observable differences in behavior indicate then mat a parser should use different techniques to handle gapping and vp deletion. vp deletion cases are always unambiguous  in the sense that it is always clear when a vp is to be inserted into a sentence - every  fnglish  sentence requires a vp. for instance  this means that given the sentence  
john kissed mary  and 1 think that frank said that mary diought that harry would have too. 
a vp may be inserted by the parser after the words  would have  because  using its lookahcad buffer  the parser can see that there is no vp present  as indicated by the triggering word  too  . thus one can write an ll-tpikn grammar rule based on just local context to handle the surface parsing of dicsc cases. it is precisely this property that pennies vp deletion to be unbounded.  in contrast  the parse of a  gapped  construction demands that one examine the left-context of the parse to determine how to proceed.  this grammar rule inserts a vp into the sentence after  would have   without specifying any relationship between that vpand any previous vp. the actual interpretation of tje deleted vp is carried out in a second stage of analysis  described immediately below.  the approach follows that described by  williams 1 .  
   finally  the new parser is designed to compute a representation of general anaphoric relationships. it docs this by implementing a restricted version of co-indexing  adding a pointer from an anaphor to a representation of an antecedent np. the implementation is a restricted one because the pointer is directed: if there is a link from x to y. then x is dependent on y  in the sense of referential dependency. for example  given the sentence  john thought that he liked nixon. 
one desired output representation  in co-indexed format  is: 
　　　john-i diought that he-1 liked nixon  in its dependency implementation  the notation  he-i  means simply that a pointer is set up linking  he  to the representation of the np  john .  he  is dependent on  john  in the sense that any properties attributed of  john  are automatically inherited by  he   but not necessarily vice versa.  for discussion of some of the advantages of a  dependency  representation over the more conventional scheme of simply selling the  values  of the np antecedent and its anaphor to be equal  see  higgenbotham 1 .  observe that dependency links give only possible connections between items  not necessary connections;  he  can always be dependent on some discourse np not mentioned in the sentence. 
   'ihe constraints that dependency  linking  obeys have been explored in depth in recent years in syntactic theory and will not be covered here. briefly  an antecedent can be linked to a pronoun if the pronoun is  free  in a certain local constituent domain. so for example   he  is free in the domain  that he liked nixon   and hence can be dependent on  john  outside this domain. in contrast   him  and  bill  must be distinct  i.e.   him  cannot depend on  bill   in  bill knows him . note that the constraint cannot be that a pronoun must appear after an antecedent  since we can have  him  and  john  dependent in  the guy who knows him likes john. 
the local constituent domain definition works here though  since 
 he  is free in  who knows him  and thus is available for dependency linking to  john . 
　　how are potential dependents calculated  briefly  the procedure is to maintain an unordered set of previously 

1 r. berwick 
encountered nps  partitioned according to the agreement features of person  number  and gender. the set is updated as local constituent domains are constructed by the  syntactic  parser. potential antecedence is encoded by adding a pointer from the pronoun to the set. after checking for agreement. for example  consider the sentence: 
john gave the bill to him  and then he left. 
the first  local domain  relevant to pronoun linking is the sentence   john gave the bill to him . the list of antecedent nps is empty. on encountering  him   the program links  him  to it.  no compatibility violations arc found since the set is empty.  thus  him  has an emptv antecedent with respect to this sentence  the correct result. if previous sentences has established discourse nps  then  him  could be linked to a set of these. next  on encountering 
 and then  in its input buffer  the parser notes mat the local domain  the s  has been completely built. therefore  the two nps  john  and  him  arc added to the appropriate candidate antecedent set  john  and  he  remain in the same partition  since they agree in terms of defined features. finally  during the analysis of the sentence   he left  the parser again links a pronoun   he  to the only antecedent set  correctly encoding the possibility that cither  he  = john or  he  =  him   - some unspecified discourse np . note that these alternatives are left implicit in the set description  thus avoiding the computational complexity of  writing out  all the possibilities. given this approach  it can be shown that the full syntactic analysis and linking algorithm runs in polynomial time  see  berwick and weinberg  1  forthcoming .  
　　finally  in passing it should be noted that the revised parsing machinery permits one to extend the parser to handle other languages  e.g.  german  using the  move verb  analysis proposed by  thiersch 1 . the details of this work arc reported in  lester 1 . 
1 formal characterization 
　　the marcus parser incorporated a number of interesting design features: the use of a lookahead buffer based on constituents  whole phrases  and not just words: the interweaving of top-down prediction of syntactic categories  as in an atn  with bottom-up recognition: the use of  attention shifting  to delay the immediate analysis of some constituents; and panern-action grammar rules grouped into packets. however  its informal characterization  as a set of grammar rules plus interpreter  has made it difficult to see just what class of languages could be handled by such a machine and what its computational complexity is. it is possible to take these program design features and formally model them  reducing the machine to a more well-known class of devices. to summarize this reduction here:  1  constituent lookahead follows precisely the notion of lr k.t  parsing  as defined by  knuth 1  where t - the number of left-inost complete subtrees that can be used in the lookahead.  1  'the use of top-down prediction in such a parser docs not change its basic bottom-up completion of phrases  as established by  hammer 1 . this means that top-down prediction docs not really affect the class of languages that this type of parser can handle.  1  i.r k.t  parsers can be modeled as two-stack automata  i.e.  an input buffer and push down stack  with constituents moved between input buffer and stack and vice-versa. if the input buffer can be of unlimited size  such a machine can be shown to be able to handle some strictly context-sensitive languages  in time n1 where n=the length of input sentences .  1  in this framework   attention-shifting  corresponds exactly to shifting an item onto the push-down stack from the input buffer  without reducing it to a higher category nonterminal symbol. thus we can determine what class of languages marcus-style parsers can handle and the computational complexity of these devices.  1  rule packets record part of the left-hand context of the parse  corresponding to the item sets of an l.r k   hence also lr k t   parser. all the ingredients of marcus-style parsers  then  have been formally studied in the context of programming language parsing  suggesting that known techniques for reducing the size of l.r k  parsers  see  e.g.  anderson  fve  and  horning 1  may prove applicable to 
 compiled  versions of marcus-style parsers. this possibility is currently under investigation. 
