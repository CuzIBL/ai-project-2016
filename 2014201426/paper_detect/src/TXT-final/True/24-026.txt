s the notion of a compressed representation. the question we address is  how much additional processing time is required for methods with limited storage  processing time for learning algorithms is equated in this paper with the number of passes necessary through a data set to obtain a correct generalization. for instance  neural nets require many passes through a data set before converging. decision trees require fewer passes  but precise bounds are unknown. 
　　we consider limited storage algorithms for a particular concept class  nested hyperrectangles. we prove bounds that illustrate the fundamental trade-off between storage requirements and processing time required to learn an optimal structure. it turns out that our lower bounds apply to other algorithms and concept classes  e.g.  decision trees  as well. notably  imposing storage limitations on the learning task forces one to devise a completely different algorithm to reduce the number of passes. we also briefly discuss parallel learning algorithms. 
1 	introduction 
many existing learning methods attempt to create concise generalizations from a set of examples. besides saving storage  small generalizations are easier to summarize and communicate to others. a common learning method will construct a generalization after one pass through a set of training data  and modify it in subsequent passes to make it smaller or more accurate. per-
   *this research supported in part by air force office of scientific research under grant afosr-1  national science foundation under grant iri-1 and nsf/darpa under grant ccr-1. 
1
　　 supported by nsf under grant ccr-1 and nsf/darpa under grant ccr-1. 
ceptron methods  neural nets  and decision tree techniques all fit this paradigm. most of these methods do not store the entire set of training data. when processing is completed  the only thing they store is a generalized data structure such as a tree  a matrix of weights  or a set of geometric clusters. the issue of limiting the storage of a learning algorithm is one abstraction of the notion of a compressed representation. the question we address is how many passes through a data set arc required to obtain a  correct   i.e.  accurate but minimum in size  generalization if we are only allowed to store the generalization. this issue is equivalent to analyzing an algorithm that has a limited storage requirement. 
   fixed storage is an important consideration for several reasons. first of all  some learning models always use fixed storage. neural net learning algorithms  for example  have a fixed number of nodes and edges  and only change the weights on the edges. decision tree algorithms can in principle grow without bound  but in practice researchers have devised many techniques for restricting their growth  quinlan 1  utgoff 1 . instance-based techniques such as those of salzberg  1ab  and aha and kibler  attempt to store as few examples as possible in order to minimize storage. second  fixed storage is a realistic constraint from the perspective of cognitive modelling - human learning behavior clearly must adhere to some storage limitations. finally  there is experimental evidence that restricting storage actually leads to better performance  especially if the input data is noisy  aha and kibler 1  quinlan 1 . the intuition behind this result is that by throwing away noisy data  an algorithm can construct a more accurate generalization. 
   our results show that if a fixed-storage algorithm attempts to create a simple concept structure  then it cannot generalize on-line without losing accuracy by  simple  we mean a generalization that is both minimum in size and an accurate model of the data; i.e.  it classifies all the training examples correctly. we also show that by making a number of additional passes  depending on the number of concepts  through the data set  an algorithm can create an optimal structure. our main goal is to demonstrate that there exists a fundamental trade-off between the storage available and the number of passes required for learning an optimal structure. there are few results comparable to ours on the number of passes re-
	heath  et al. 	1 

quired to learn a concept. typical theory results  rather  give estimates of the size of the input data set  not of the number of presentations required. this work is an important step towards formalizing the capabilities of incremental vs. non-incremental learning algorithms. any algorithm that does not save all training examples is to some extent incremental  since upon presentation of new inputs the algorithm cannot re-compute a generalization using all previous examples. 
   the learning framework considered in this paper is computing generalizations in the form of geometric concept classes. many important learning algorithms fall in this category  e.g.  perceptron learning  rosenblatt 1   instance-based learning  aha and kibler 1   decision tree models  quinlan 1  and hyperrectangles  salzberg 1 . we focus on the concept class defined by nested hyperrectangles  although many of our results are applicable to other concept classes. in fact  our impossibility results apply to any convex partitioning of feature space  such as decision trees and perceptrons. 
   the learning model we consider has fixed number of storage locations; i.e.  it is not permitted to store and process all training examples at once. the limited storage requirement is applicable only during the learning  or training  phase. that is  our algorithms must operate incrementally  storing a limited number of intermediate results during each pass through a data set and modifying the partially constructed generalization in subsequent passes. for both cognitive and practical reasons  much experimental learning research has focused on the development of incremental models  utgoff 1 . 
1 	nested hyperrectangles 
recent experimental research on learning from examples has shown that concepts in the shape of hyperrectangles are a useful generalization in a variety of realworld domains  salzberg 1ab  1 . in this work  rectangular-shaped generalizations are created from the training examples  and are then used for classification. rectangles may be nested inside one another to arbitrary depth  and new examples are classified by the innermost rectangle containing them. thus nested rectangles may be thought of as exceptions to the surrounding rectangles. this learning model is called the nested generalized exemplar  nge  model. experimental results with this model thus far have shown that it compares very favorably with several other models  including decision trees  rule-based methods  statistical techniques  and neural nets  salzberg 1ab . 
independently of the experimental work cited above  
helmbold  sloan  and warmuth  have produced very promising theoretical results for nested rectangular 
concepts. in particular  they have developed a learning algorithm for binary classification problems that creates strictly nested rectangles  and that makes predictions about new examples on-line. they have proven that their algorithm is optimal with respect to several criteria  including the probability that the algorithm produces a hypothesis with small error  valiant 1  and the expected total number of mistakes for classification of the first t examples. the algorithm applies to all in-
1 	learning and knowledge acquisition 

figure 1: categorizing points using rectangles 
tersection closed classes  which include orthogonal rectangles in rn  monomials  and other concepts. the main assumption behind their model is that it must be possible to classify the training examples with strictly nested rectangles. 
   given that the learning community has found nested rectangles a useful concept class  and that the theoretical community has proven some further results about this same concept class  we have been led to investigate the ability of an algorithm to construct an optimal number of nested rectangles given only limited storage. we will argue below that our results apply to other well-known concept classes  including the partitionings induced by decision trees and perceptrons. 
1 	preliminaries 
an example is defined simply as a vector of real-valued numbers  plus a category label. for instance  we may be considering a problem where medical patients are represented by a set of real numbers including heart rate  blood pressure  etc.  and our task is to categorize the patients as  in-patient  or  out-patient.  for our purposes  an example is just a point in euclidean space  where the dimensionality of the space is determined by the number of attributes measured for each example. we will categorize points by using axis-parallel hyperrectangles  where each rectangle ri is labeled with a category c r{  such as  out-patient.  a point is categorized by the innermost rectangle containing it. figure 1 illustrates how categories are assigned. in the figure  lowercase letters indicate points belonging to categories a and b  and uppercase letter indicate the category labels of the rectangles. notice that points not contained by any rectangle are assigned to category a  which corresponds to a default category. only two rectangles are required to classify all the points in figure 1. 
   the general problem definition is as follows: we are given n points in a d-dimensional space  and we are asked to construct a minimum set of strictly nested hyperrectangles that will correctly classify the set. we will assume that each point belongs to one of two classes  i.e.  we have a binary classification problem . 

   our algorithms learn by processing examples one at a time  in a random order. the algorithm is allowed to store no more than a fixed number s of the examples. in addition  the algorithm may have some additional constant amount of storage. on each pass through the data  the algorithm sees all of the examples exactly once. in most cases  the order of the examples on each pass is independent of the order on other passes. 
   given these definitions  we would like to answer the following general question: how many passes p through the data are required to construct a minimum set of nested hyperrectangles  we will present several algorithms  and show how the number of passes required changes as a function of the amount of storage 1 and of the minimum number of rectangles r. 
1 	learnability 
there are some input sets which cannot be learned with nested isothetic rectangles. we say a set of examples is learnable if the nested rectangle problem for this set has a solution. here we present a necessary and sufficient condition for the learnability of a set of examples. 
d e f i n i t i o n 1: in a binary classification problem  an isothetic hypeerectangle with nonzero area is called a blocking rectangle if every edge of the hyperrectangle intersects points of both classes. 
t h e o r e m 1 . 1 : a set of points from two classes is learnable if and only if there does not exist a blocking rectangle for the set. 
a proof may be found in heath et al.  . 
1 	static algorithm 
when all of the examples can be stored in the memory 
 1 n   the nested rectangle problem can be solved by a modified plane sweep. we make one pass through the examples  during which we store all examples. 
   we sweep 1d orthonormal hyperplanes  two along each axis. each hyperplane defines two halfspaces: inside and outside. the intersection of the 1d inside halfspaces is a hyperrectangle. initially we position the hyperplanes so that the hyperrectangle  ro  is the smallest that contains all of input points. let the category of ro  which is not a partitioning rectangle  be  
   the computation proceeds in r steps. in each step  we find the next hyperrectangle  r i +i  nested inside ri  by sweeping each hyperplane inward  towards the inside halfspace it defines  until it intersects a point not belonging to category  in some steps  some hyperplanes may not move at all. the new positions of the hyperplanes define the hyperrectangle  which is output. 
   note that this algorithm requires that the points are sorted along all dimensions  which requires o dn log n  time. each pair of hyperplanes sweeps over n points  so the sweep takes  time. thus  the total time needed  including that for sorting  is  
t h e o r e m 1   1 : 	given 	n points in 	d-dimensional space  the 	nested 	rectangle 	problem 	can 	be 	solved 	in  time 	and 	o dn  	space. 
1 	limited memory algorithms 
1 	s i m p l e l i m i t e d m e m o r y a l g o r i t h m 
the static algorithm can easily be converted to run with limited memory. first  suppose we have fixed memory sufficient to store all rectangles  but not all examples. we show that r passes will suffice to find r rectangles. 
   intuitively  to find the outermost rectangle  our algorithm finds the maximum and the minimum point in each dimension. these 1d points define the edges of the first hyperrectangle. inductively  assume i rectangles have been constructed and the category of ri is c ri . let inside ri  be the set of points inside the iih rectangle. we now find the maximum and minimum point in each dimension for category c   r i +    that belong to inside ri . 
   this can be done in one pass by storing  for each dimension  the smallest and largest point of category c ri +1  in inside ri  seen so far. we revise our current estimate of the smallest and largest points  if necessary  and continue as each new point is processed. 
   since we can find the next partitioning hyperrectangle in one pass  we can solve the partitioning problem with 1d+1 storage locations in r passes. 
t h e o r e m 1 . 1 : given 1d+1 storage locations  it is possible to solve the d-dimensional nested rectangle problem in r passes  where r is the number of rectangles. 
   this algorithm can be seen as a line adjustment algorithm similar to perceptrons. unlike the standard perception algorithm  it has the following characteristics. 
  it is guaranteed to converge in r passes. 
  hyperplanes always move in the same direction. 
  a hyperplane makes large incremental adjustments towards its final location. 
  the network can classify input data that is not clas-sifiable using the standard perceptron algorithm 
  a hyperplane defining rectangle ri+1will not be adjusted until ri has reached its final location. 
   our major results  described below  illustrate that in many cases we can improve the performance of our algorithm to learn the concept structure correctly in far fewer passes. additionally  if r is small with respect to n  then r passes are required to define the rectangles. 
1 	s p e e d i n g u p t h e l i m i t e d m e m o r y a l g o r i t h m 
in this section we show that with a simple modification of the static algorithm  we can design an algorithm for the nested rectangle problem that runs in fewer than r passes  where r is the number of rectangles a more efficient scheme will be described in the next section. 
   as above  the outermost rectangle can be found in one pass with only 1d+1 storage locations. assume inductively that i rectangles have been found using storage s = 1d 1 + s  + 1. we use 1d storage locations for maintaining a hyperrectangle w  which is a window outside of which we have found all rectangles {r1 . ..  ri}  where ri is innermost. initially  w contains all the examples  no rectangles have been found . all points outside w can be ignored while the algorithm positions one or more 
	heath  et al. 	1 

rectangles within r{. for each of the 1d hyperplanes that define w  we allot s memory locations  which will be used to find the s closest points  of any color  to the hyperplane that are inside w. all of these points can be found in one pass. once they are in memory  the set of s points associated with each line is sorted. 
   we define an alternation to be a pair of points that belong to different categories and are adjacent in a sorted list. the alternations in each list can be found  and then matched up to find partitioning rectangles. if each list has at least one alternation  the outermost alternations in each list define a partitioning rectangle ri + 1- every point stored that is not inside ri + 1 is removed from the lists  since these points cannot define rectangles nested within r i + 1 . now  the outermost alternations in each list define ri + 1- we continue placing rectangles this way until at least one list has no alternations. note that some list could have no alternations at the beginning of the pass. in this case  we simply find no rectangles in that pass. in any case  we redefine w as follows: for each list that still contains alternations  we move its associated hyperplane to the last alternation that was deleted  if any . we move every other hyperplane to the innermost point in its list. because these lists have no alternations  they cannot generate partitioning rectangles. since at least one list has no alternations  at least one hyperplane has moved in by s points  i.e.  at least s points have been excluded from w . since no point that leaves w can reenter  w will be empty in no more than n/s passes. 
t h e o r e m 1: given s storage locations  it is possible to solve the d-dimensional nested rectangle problem in 1 nd/s  passes. 
   in particular  when the number of rectangles r is larger than  the number of passes is smaller than r. in other words  we can always solve the problem in passes irrespective of the number of rectangles. 
1 	l i m i t e d m e m o r y w i t h s a m p l i n g 
all of the preceding algorithms work by finding alternations from the outside  working inwards. in this section we present a different algorithm that works in one dimension. this algorithm finds alternations on the real line  so is applicable to any convex partitioning of the line  such as decision trees and intervals. the algorithm is fairly complex and demonstrates the difficulty of learning efficiently with limited storage even in one dimension. 
   as before  we define an alternation to be a pair of adjacent points which belong to different categories. note that once we detect all the alternations it is easy to find the concept structure. clearly  if the concept being acquired is classifiable with r rectangles  then there are no more than 1r alternations. our algorithm will detect and store all the alternations. the intuitive explanation is as follows. we use one pass to find the size of the input space  and then we partition the input space into intervals of approximately equal size. the algorithm maintains markers for approximately / intervals  where i is chosen appropriately  see below . once these intervals are established  we can find the outermost two 1 learning and knowledge acquisition 

table 1: choosing the number of intervals i 
alternations  if they exist  in each interval in one pass. if an interval contains no alternations  the input points in the interval are ignored in subsequent passes. our algorithm keeps track of only the active intervals  namely intervals that have alternations in them. 
   the following schematic  code fragment is executed repeatedly until all alternations have been found. 
	1. while 	and some nt is splittable 
1. 
1. check each interval for an alternation 
1. discard intervals not containing alternations 
1. end while 
1. detect one or two alternations from each i e int 
　int. is the current set of intervals  initially the entire real line  containing all n of the input points. the method we use to split the intervals in step 1 is guaranteed to split each interval into a small constant number of subintervals  each of which contains no more than a constant fraction of the interval's points. since the intervals will be shrinking in size by a constant fraction each time step 1 is executed  the while loop will be iterated no more than      og n  times. let storage available per interval  where  
   the while loop guarantees that there are at least i intervals  and each one contains an alternation. step  1  will find i alternations in one pass  so will require at most 1r/i passes. o i  storage locations will be used to maintain the intervals. 
　in step 1  we make use of a limited storage splitting technique developed by munro and paterson . it splits each interval into three subintervals  each of which is no larger than a constant fraction of the size of the interval. to split / intervals  it takes passes  using si storage locations  where o log n   the examples must appear in the same order for the 1   l o g 1 n/s  passes . since step 1 is executed at most log n times  a total of passes will be 
needed for step 1. the algorithm uses a total of log n/s  passes with . memory locations. 
　the choice o f / depends on r and the memory/speed trade-off desired  see table 1 . in practice  we always have enough space for the rectangles 1 and therefore we can find all the alternations in passes. 
   there are many situations in which the number of rectangles r is not known beforehand. in this case  a binary search technique can be applied to the above algorithm. initially  we run the algorithm  assuming that 1 rectan-

gles are sufficient to completely learn the structure. in one pass  it is possible to verify that the solution generated does indeed correctly classify the input. if not  we can double the number of rectangles and try again. we continue doubling the number of rectangles until the algorithm successfully finds the nested rectangles  using o r  storage. because the number of rectangles is doubled in each step  and successful classification is guaranteed when the number of rectangles is at least r  the partitioning algorithm will run at most  log k  times. thus  the total number of passes needed to determine r and find the rectangles is  
　a generalization of the algorithm to higher dimensions is not obvious and is being investigated. we are considering a randomized variant of this algorithm for multiple dimensions. it partitions the input set into regions  each containing a fixed fraction of the points. then the algorithm finds a rectangle in each region in each pass. while such an algorithm may work well in practice  it may not find the minimum set of rectangles  and may fail to find a correct generalization when one exists. 
1 	lower bound 
in this section we show that when the concept structure is fairly simple and the amount of storage necessary to represent the concept is therefore small  the number of passes required is proportional to the number of alternations. intuitively  the alternations represent the concept boundaries  so the size of the representation constructed by many learning algorithms is proportional to the number of alternations in the input set. for example  decision tree learning algorithms must construct one branch for every alternation. the theorem below is stated in terms of rectangles but our lower bound technique also applies to the number of passes necessary to find alternations. the same result holds for decision trees or any other method of partitioning into convex regions. 
   we will use a comparison based model which is sufficiently strong to support our previous algorithm. the following theorem shows that any comparison-based algorithm that has kr storage  where k is some constant needs at least r passes of the examples when r is sufficiently small compared to n i  
theorem 1: any comparison-based algorithm for solving the nested rectangle problem with k r storage requires r passes to find r partitioning rectangles  when there are n points in the input  
　a full proof of this theorem may be found in heath et al.  . we give an intuitive explanation for the case when r = 1  where two passes are needed to solve the nested rectangle problem when  we will find  for any algorithm  two distinct sets of inputs indistinguishable by the algorithm that cannot be categorized with the same set of two nested rectangles. suppose we have two categories   blue  and  green.  consider a trainer that presents points in the blue category first. because the learner has limited storage  it will eventually forget at least one blue point. after all n/1 blue points are presented  the trainer presents three points from the green category  surrounding the forgotten blue case 1: 

case 1: 

figure 1: two possible input patterns 
point. it can choose either of two ways to position the green points  see figure 1 . the algorithm will have the opportunity to find the outermost  green  partitioning rectangle. to correctly place the inner  blue  rectangle  the program must surround the forgotten blue point. however  the program will not be able to tell if the input is given by case 1 or case 1 of figure 1. if the program were to decide one way  the trainer could have chosen to present the other case. because no comparisons between the forgotten blue points and the three green points are performed  the two cases are indistinguishable. 
1 	parallel algorithms 
connectionist architectures have natural parallel implementations. this has motivated us to examine whether our algorithms also have natural parallel implementations. given n points on the line  we can solve the nested rectangle problem in time using p processors  where for details  see heath et a/.  . 
　parallel algorithms can be developed to solve the nested rectangle problem in any fixed number of dimensions. one would hope that our parallel algorithm could be generalized to work in any number of dimensions. however  we show that when the dimensionality is not fixed  the nested rectangle problem becomes log-space complete for p  or p-complete. this suggests that when the dimensionality of the problem is not fixed  there is no efficient  i.e. polylog time with polynomial number of processors  parallel algorithm. this condition is usually considered as a strong indication of inherent sequentiality  since there are no known methods to achieve significant speed-ups of such problems on parallel architectures. see heath et al.    for a proof of this result. 
1 	conclusion 
　intuitively  a learning program with fixed storage cannot create an optimal set of nested rectangles to classify a set of points  examples . the problem is that  since memory is limited  the program must forget some of the examples  and these examples may be misclassified as a result. it is clear  however  that a partial generalization constructed in p passes can be further refined using more passes. this paper makes precise the trade-offs between storage  number of passes  and classification accuracy. 
	heath  et al. 	1 


table 1: number of passes required as r increases 

　when an algorithm only has enough memory to store the r rectangles themselves  it requires  in the worst case  no more than r passes through the data. in addition  when r is large with respect to the number of examples n  we can do much better. for instance  when for some constant r  we only need passes through the data. for most learning problems  we do not know the size of r in advance  i.e.  we do not know how compact the generalization might be . however  our algorithm can still learn both r and the target concept accurately in  passes. 
table 1 summarizes our results for different ranges of r. 
   from a practical standpoint  these results allow one to make some statements about the trade-off between processing time and storage for algorithms that learn nested hyperrectangles. as long as storage is not limited  we can use an algorithm that  with one pass through the data  runs in  time  where d is the number of features for each example  to find an optimal  i.e.  minimum  set of rectangles. if storage is fixed and the number of examples is large  then min passes are sufficient to find the optimal set of nested rectangles  depending on the size of r with respect to n. if fewer passes are allowed  then the rectangular concepts learned by the algorithm may misclassify some of the examples. 
   we have studied the complexity of non-incremental learning algorithms for parallel models of computation. in heath et al.   we present an optimal parallel algorithm for learning in one dimension  and show that learning nested hyperrectangles when the dimensionality is not fixed is p-complete  inherently sequential . 
   the major open problems we are currently considering include developing efficient limited memory algorithms for data sets with multiple dimensions. we plan to implement these algorithms for experimental tests on real data. we also are working on improvements to our currently crude parallel algorithms for multiple dimensions. additionally  we are considering the problem of efficiently constructing near-optimal concept structures. 
