 
coordination of agent activities is a key problem in multiagent systems. set in a larger decision theoretic context  the existence of coordination problems leads to difficulty in evaluating the utility of a situation. this in turn makes defining optimal policies for sequential decision processes problematic. we propose a method for solving sequential multiagent decision problems by allowing agents to reason explicitly about specific coordination mechanisms. we define an extension of value iteration in which the system's state space is augmented with the state of the coordination mechanism adopted  allowing agents to reason about the short and long term prospects for coordination  the long term consequences of  mis coordination  and make decisions to engage or avoid coordination problems based on expected value. we also illustrate the benefits of mechanism generalization. 
	1 	introduction 
the problem of coordination in multiagent systems  mass  is of crucial importance in ai and game theory. given a collection of agents charged with the achievement of various objectives  often the optimal course of action for one agent depends on that selected by another. if the agents fail to coordinate the outcome could be disastrous. consider  for instance  two agents that each want to cross a bridge that can support the weight of only one of them. if they both start to cross  the bridge will collapse; coordination requires that they each  agree  which one of them should go first. 
　coordination problems often arise in fully cooperative mass  in which each agent shares the same utility function or common interests. this type of system is appropriate for modeling a team of agents acting on behalf of a single individual  each tries to maximize that individual's utility . in the bridge example above  it may be that neither agent cares whether it crosses first  so long as they both cross and pursue their objectives. in such a setting  coordination problems generally arise in situations where there is some flexibility regarding the  roles  into which agents fall. if the abilities of the agents are such that it makes little difference if agent a  pursues objective o  and a 1 pursues o1  or vice versa  the agents run the risk of both pursuing the same objective-with consequences ranging from simple delay in goal achievement to more drastic outcomes-unless they coordinate. this issue arises in many team activities ranging from logistics planning to robotic soccer. 
1 	distributed ai 
　an obvious way to ensure coordination is to have the agents' decision policies constructed by a central controller  thus defining each agent's role  and imparted to the agents. this is often infeasible. approaches to dealing with  independent  decision makers include:  a  the design of conventions or social laws that restrict agents to selecting coordinated actions  1  1 ;  b  allowing communication among agents before action selection ; and  c  the use of learning methods  whereby agents learn to coordinate through repeated interaction  1  1  1  1 . 
　unfortunately  none of these approaches explicitly considers the impact of coordination problems in the context of larger sequential decision problems. if the agents run the risk of miscoordination at a certain state in a decision problem  how should this impact their policy decisions at other states'! specifically  what is the long-term  or sequential  value of being in a state at which coordination is a potential problem  such a valuation is needed in order for agents to make rational decisions about whether to even put themselves in the position to face a coordination problem. 
　unfortunately  there are no clear-cut definitions of sequential optimality for multiagent sequential decision processes in the general case. most theoretical work on coordination problems assumes that a simple repeated game is being played and studies methods for attaining equilibrium in the stage game. in this paper  we argue that optimal sequential decision making requires that agents be able to reason about the specific coordination mechanisms they adopt to resolve coordination problems. with this ability  they can make optimal decisions by considering the tradeoffs involving probability of  eventual  coordination  the consequences of miscoordination  the benefits of coordination  the alternative courses of action available  and so on. we develop a dynamic programming algorithm for computing optimal policies that accounts not only for the underlying system state  but also the state of the coordination mechanism being adopted. specifically  we show how the underlying state space can be expanded minimally and dynamically to account for specific coordination protocol being used. 
　with this definition of state value given a coordination mechanism  one can tackle the problem of defining good coordination mechanisms for specific decision problems that offer good expected value  we but will make a few remarks 

near the end of the paper on this point . our framework therefore provides a useful tool for the design of conventional  communication and learning protocols  1j. 
　we focus on fully cooperative mass  assuming that a common coordination mechanism can be put in place  and that agents have no reason to deliberate strategically. however  we expect most of our conclusions to apply mutatis mutandis to more general settings. we introduce markov decision processes  mdps  and multiagent mdps  mmdps  in section 1. we define coordination problems and discuss several coordination mechanisms in section 1. in section 1 we describe the impact of coordination problems on sequential optimality criteria  show how to expand the state space of the mmdp to reason about the state of the specific mechanisms or protocols used by the agents to coordinate  and develop a version of value iteration that incorporates such considerations. we illustrate the ability of generalization techniques to enhance the power of coordination protocols in section 1  and conclude with some remarks on future research directions in section 1. 
	1 	multiagent mdps 
1 markov decision processes 
we begin by presenting standard  single-agent  markov decision processes  mdps  and describe their multiagent extensions below  see  1  1  for further details on mdps . a fully observable mdp m =  s  a  pr  r  comprises the following components. s is a finite set of states of the system being controlled. the agent has a finite set of actions a with which to influence the system state. dynamics are given by 
pr :  here denotes the probability that action a  when executed at state induces a transition to is a real-valued  bounded reward 
function. the process is fully observable: though agents cannot predict with certainty the state that will be reached when an action is taken  they can observe the state precisely once it is reached. 
　an agent finding itself in state  at time must choose an action  the expected value of a course of action depends on the specific objectives. a finite horizon decision problem with horizon t measures the value of as 
 where expectation is taken w.r.t. pr . a discounted  infinite horizon problem measures value as 
here  is a discount factor that ensures the infinite sum is bounded. 
　　for a finite horizon problem with horizon  a nonstationary policy a associates with each state s and stage-to-go t an action to be executed at s with t stages remaining. an optimal nonstationary policy is one with maximum expected value at each state-stage pair. a stationary policy for an infinite horizon problem associates actions with states alone. 
　a simple algorithm for constructing optimal policies  in both the finite and infinite horizon cases  is value iteration f 1 . define the  -stage-to-go value function  by setting 

figure 1: a simple mmdp with a coordination problem. 
for a finite horizon problem with horizon t  we set discounting  and during these calculations set action a maximizing the right-hand term  terminating the iteration at t = t. for infinite horizon problems  the sequence of value functions  produced by value iteration converges to the optimal value function v*. for some finite t  the actions a that maximize the right-hand side of equation 1 form an optimal policy  and  approximates its value. 
1 	the multiagent extension 
we now assume that a collection of agents is controlling the process. the individual actions of agents interact in that the effect of one agent's actions may depend on the actions taken by others. we take the agents to be acting on behalf of some individual; therefore  each has the same utility or reward function r. the system is fully observable to each agent. 
　we model this formally as a multiagent markov decision process  mmdp . mmdps are much like mdps with the exception that actions  and possibly decisions  are  distributed  among multiple agents. an mmdp m = 
 consists of five components. the set 
a is a finite collection of n agents  with each agent i  having at its disposal a finite set a  of individual actions. an 
element  of the joint action space   = 	 
represents the concurrent execution of the actions a  by each agent i. the components s  pr and r are as in an mdp  except that pr now refers to joint actions  
   taking the joint action space to be the set of basic actions  an mmdp can be viewed as a standard  single-agent  mdp. specifically  since there is a single reward function  the agents do not have competing interests; so any course of action is equally good  or bad  for all. we define optimal joint policies to be optimal policies over the joint action space: these can be computed by solving the  standard  mdp  using an algorithm like value iteration. 
example an example mmdp is illustrated in figure 1. the mmdp consists of two agents a 1 and a1  each with two actions a and 1 that can be performed at any of the six states. all transitions are deterministic and are labeled by the joint 
	boutilier 	1 

actions that induce that transition. the joint action  a  b  refers to al performing a and al performing 1  and others similarly  with * referring to any action taken by the corresponding agent . at the  source  state s1 a1 alone decides whether the system moves to s1  using a  or s1  using 1 . at s1  the agents are guaranteed a move to s1 and a reward of 1 no matter what joint action is executed. at s1 both agents must choose action a or both must choose b in order to move to s1 and gain a reward of 1; choosing opposite actions results in a transition to s1 and a reward of -1. the set of optimal joint policies are those where a1 chooses a at s1  a1 can choose a or 1   and a1 and a 1 choose either  a  a  or  b b  at s1. 
　　the value function determined by solving the mmdp for the optimal joint policy is the optimal joint value function and is denoted  in the example above  an infinite horizon problem with a discount rate of 1 has = 1  while for a finite horizon problem  is given by  
　mmdps  while a natural extension of mdps to cooperative multiagent settings  can also be viewed as a type of stochastic game as formulated by shapley . stochastic games were originally formulated for zero-sum games only  and as we will see  the zero-sum assumption alleviates certain difficulties   whereas we focus on the  equally special  case of cooperative games. 
1 	coordination problems and coordination mechanisms 
the example mmdp above has an obvious optimal joint policy. unfortunately  if agents a1 and a1 make their decisions independently  this policy may not be implementable. there are two optimal joint action choices at s1:  a a  and  1  1 . if  say  al decides to implement the former and a1 the latter  the resulting joint action  a  1  is far from optimal. this is a classic coordination problem: there is more than one optimal joint action from which to choose  but the optimal choices of at least two agents are mutually dependent  we define this formally below . notice that the uncertainty about how the agents will  play s1  makes al's decision at s1 rather difficult: without having a good prediction of the expected value at s1. agent al is unable to determine the relative values of performing a or 1 at s1  more in this in section 1 . 
　in the absence of a central controller that selects a single joint policy to be provided to each agent  ensuring coordinated action choice among independent decision makers requires some coordination mechanism. such a mechanism restricts an agent's choices among the potentially individually optimal actions  perhaps based on the agent's history. we describe some of these below  including learning  conventional and communication techniques. 
　in the remainder of this section  we focus on repeated games  returning to general mmdps in the next section. an identical-interest repeated game can be viewed as an mmdp with only one state-joint actions are played at that state repeatedly. an immediate reward r a  is associated with each joint action. our aim is to have the individual actions selected by each agent constitute an optimal joint action. formally  a 
1 	distributed al 
stage game g comprises action sets a  for each agent i  joint action space a  and reward function r. the stage game is played repeatedly. 
definition joint action a 	a is optimal in stage game g if 
r a  r a'  for all a. action  is potentially individually optimal  pio  for agent i if some optimal joint action contains we denote by  the set of such actions for agent  
definition stage game g =  r  induces a coordination problem  cp  iff there exist actions  such that  is not optimal. 
intuitively  a cp arises if there is a chance that each agent selects a plo-action  yet the resulting joint action is suboptimal. 
　cps in repeated games can often be  reduced  by eliminating certain plo-actions due to considerations such as dominance  risk  e.g.  see the notions of risk-dominance and tracing used by harsanyi and selten to select equilibria    or focusing on certain plo-actions due to certain asymmetries. these reductions  if embodied in protocols commonly known by all agents  can limit choices making the cp  smaller   thus potentially more easily solved   and sometimes result in a single  obvious  action for each agent. we do not consider such reductions here  but these can easily be incorporated into the model presented below. 
　a coordination mechanism is a protocol by which agents restrict their attention to a subset of their plo-actions in a cp. a mechanism has a state  which summarizes relevant aspects of the agent's history and a decision rule for selecting actions as a function of the mechanism state. while such rules often select actions  perhaps randomly  from among plo-actions  there are circumstances where non-pio-actions may be selected  e.g.  if the consequences of uncoordinated action are severe . mechanisms may guarantee immediate coordination  eventual coordination  or provide no such assurances. to illustrate  we list some simple  and commonly used  coordination methods below. in section 1  we will focus primarily on randomization techniques with learning. however  communication and conventional methods can be understood within the framework developed below as well. 
randomization with learning this is a learning mechanism requiring that agents select a plo-action randomly until coordination is achieved  i.e.  an optimal joint action is selected by the group . at that point  the agents play that optimal joint action forever. we assume that actions are selected according to a uniform distribution.1 the mechanism has k -f 1 states  where k is the number of optimal joint actions: k states each denote coordination on one of the optimal actions  and one denotes lack of coordination. the state changes from the uncoordinated state to a coordinated state as soon as an optimal action is played. this requires that agents be able to observe actions or action outcomes. 
we can model this protocol as a finite-state machine 
 fsm . the fsm for the cp at s1 in figure 1 is illustrated in 
　　1  in this and other mechanisms  reduction methods can be used to reduce the number of actions considered by each agent. 


figure 1: a simple fsm for the randomization mechanism: solid arrows denote state transitions  labeled by inputs  observed joint actions ; dashed arrows indicate outputs  action choices . 
figure 1. when the agents are uncoordinated  state u   they each choose action a and 1 randomly. if the observed joint action is not coordinated  the remain in state  /; but if they coordinate  they move to the appropriate state  a or b  and stay there  executing the corresponding action . 
　for many problems  we can view the mechanism as having only two states: coordinated  c  and uncoordinated  u . if c  we simply require that the agent memorize the action on which the group coordinated. for the purposes of computing expected value below  we often need only distinguish between c and u states  without regard to the actual action chosen . we note that randomization works quite well if there is a small group of agents with few actions to choose from; but as these sets grow larger  the probability of transitioning from u to c gets exponentially smaller. randomization ensures eventual coordination  at a rate dictated by the number of agents and number of choices available to them. 
　fictitious play  fp  is a related learning technique commonly studied in game theory  1  1  where each agent i observes the actions played in the past by other agents and plays a best response given the empirical distribution observed. we refer to  for details  but note that the state of the mechanism consists of  counts  of the plo-actions played by other agents; thus fp has an infinite number of states. for fully cooperative games  fp converges to an optimal joint action if attention is restricted to plo-actions and agents randomize over tied best responses  1  1 .1 it also has the property that once a coordinated action is played  it is played forever. unlike randomization  fp tends to lead to faster coordination as the number of agents and actions increase . 
lexicographic conventions 	conventions or social laws 
 e.g.  driving on the right-hand side of the road  are often used to ensure coordination  1  1 . lexicographic conventions can be applied to virtually any cp. given some commonlyknown total ordering of both agents and individual actions  the set of optimal actions can be totally ordered in several different ways. lexicographic conventions ensure immediate coordination  but can have substantial overhead due to the requirement that each agent have knowledge of these orderings 
　　1  hence  it might best be described as a learning technique with randomization  rather than a randomization technique with learning. of both agents and actions. this may be reasonable in a fixed setting  but may be harder to ensure over a variety of decision problems  e.g.  involving different collections of agents . in contrast  the learning models described above can be viewed as  meta-protocols  that can be embodied in an agent once and applied across multiple decision problems. 
communication finally  a natural means of ensuring coordination is through some form of communication. for example  one agent may convey its intention to perform a specific plo-action to another  allowing the other agent to select a matching plo-action. there are a number of wellknown difficulties with devising communication and negotiation protocols  involving issues as varied as synchronization and noisy channels. we do not delve into such issues here. we assume that some agreed upon negotiation protocol is in place. realistically  we must assume that communication has some cost  some risk of failure or misinterpretation  and delays the achievement of goals. as such  we model communication as actions in an mmdp which have effects not on the underlying system state  but on the  mental state  of the agents involved. rather abstractly  we can say that the state of a communicative coordination mechanism for an agent i is its estimate of the  mental state  of other agents. for example  after negotiation  agent a1 may believe that a 1 is committed to performing action 1. the  mental state  of other agents will generally only be partially observable  and the state of the mechanism will be estimated by each agent. 
1 	dynamic programming with coordination 
1 sequential optimally and state value 
cps arise at specific states of the mmdp  but must be considered in the context of the sequential decision problem as a whole. it is not hard to see that cps like the one at s1 in figure 1 make the joint value function misleading. for example  
 = 1 and  = 1  suggesting that a1 should take action a at s1 with 1 stages-to-go. but  assumes that the agents will select an optimal  coordinated joint action at s1. as discussed above  this policy may not be implementable. generally  the optimal joint value function will overestimate the value of states at which coordination is required  and thus overestimate the value of actions and states that lead to them. 
　a more realistic estimate  of this value would account for the means available for coordination. for instance  if a lexicographic convention were in place  the agents are assured of optimal action choice  whereas if they randomly choose plo-actions  they have a 1% chance of acting optimally  with value 1  and a 1% chance of miscoordinating 
 with value -1 . under the randomization protocol  we have 
 s1  = 1 and  s1  = 1  making the optimal decision at s1  with two stages to go   opting out of the cp:  al should choose action 1 and move to s1. 
　　unfortunately  pursuing this line of reasoning  assuming a randomization mechanism for coordination  will lead the a1 to always choose b at s1  no matter how many stages remain. if we categorically assert that = 1  we must have that  for any stage t 1. this ignores the 
	b1utilier 	1 

fact that the coordination mechanism in question does not require the agents to randomize at each interaction: once they have coordinated at s1  they can choose the same  optimal  
joint action at all future encounters at s1. clearly  the 
depends on the state of the coordination mechanism. if the agents have coordinated in the past  then  = 1  since they are assured coordination at this final stage; otherwise 
 = 1. by the same token  depends on the state of the mechanism for arbitrary t 1  as does the value of other states. 
　the optimal value function v is not a function of the system state alone  but also depends on the state of the mechanism. by expanding the state space of the original mmdp to account for this  we recover the usual value function definition. in this example  we define the expanded mmdp to have states of the form  s  c   where s is some system state and c is the state of the randomization mechanism. we use c and u to refer to coordinated and uncoordinated states of the mechanism  respectively  with c standing for either a or b in the fsm of figure 1 . transitions induced by actions are clear: each action causes a system state transition as in the mmdp  while the coordination state changes from u to c only if the agents choose action  a  a  or  b  b  at s1  and never reverts to u . the coordination protocol also restricts the policies the agents are allowed to use at s1. if they find themselves at  expanded  state  s1  u   they must randomize over actions a and 1. as such  the transition probabilities can be computed easily:  s1 u  moves to both  s1  c  and  s1  u  with probability 1.1 
　the expanded mmdp can be viewed a combination of the original mmdp and the partially specified controller shown in figure 1. the state space of the expanded mmdp is given by the cross-product of the mmdp and fsm state spaces  while the fsm restricts the choices that can be made when the agents are at state s1  for each state a  b or u of the fsm . generally speaking  the protocol restricts action choices at the state where the cp arose  while optimal choices should be made at all other states. notice that these choices are optimal subject to the constraints imposed by the protocol  or 
finite-state controller . 
　with this expanded state space  we can trace value iteration on our running example to illustrate how the agents reason about sequential optimality in a way that accounts for the cp and the coordination mechanism. we assume a finite horizon problem without discounting. 
example for all stages t  1  obviously 	  
so if the agents are in a state of coordination  a 1 
should choose action a at s1 and  opt in  to the cp by moving to s1. matters are more complex if the agents are uncoordinated. for all stages 
so with 1 or fewer stages remaining  a1 should choose to  opt out   choose 1  at for all stages t 1  however  = 1 while thus  a1 should  opt in  to the 
1
    	more 	precisely  	transitions 	to 	states 	and 	 with probability 1 each. 
1
	the values 	and u  are equal for 1  1. 
1 	distributed al 
cp at  if there are 1 or more stages remaining. 
　this example shows how knowledge of the state of the coordination mechanism allows the agents to make informed judgments about the  long term  benefits of coordination  the costs of miscoordination  and the odds of  immediate or eventual  coordination. because of the cost of miscoordination  and its 1% chance of occurrence   the agents avoid s1 with fewer than eight stages to go. the safe course of action is deemed correct. however  with eight or more stages remaining  they move from  s1  u  to  s1  u : the 1% chance of coordination not only provides the agents with a 1% chance at the reward of 1  but also with a 1% chance at least two more passes through s1. the long term benefits of coordination  with a sufficient horizon  make the risk worthwhile when compared to the safe alternative. 
　it is important to note that the state of the coordination mechanism must be taken into account at each  system  state of the mmdp. for instance  though the state of the mechanism can have no influence on what the agents do at state s1  there is only one  choice    it is relevant to determining the value of being at state s1. 
　in general  reasoning with coordination mechanisms allows one to account for the factors mentioned above. naturally  the tradeoffs involving long term consequences depend on the decision problem horizon or discount factor. the key factor allowing computation of value in this case is an understanding of the coordination mechanism used to  stochastically  select joint actions in the presence of multiple equilibria  and the ability to associate a value with any state of the mmdp  given the state of the mechanism . shapley's stochastic games  provide a related sequential multiagent decision model with a a well-defined value for game states. this value  however  is a consequence of the zero-sum assumption  which removes the reliance of state value on the selection of a  stage game  equilibrium. in particular  it does not apply to fully cooperative settings where cps arise. 
1 	value iteration with state expansion 
value iteration can be revised to construct an optimal value function and policy based on any given coordination mechanism. a straightforward version is specified in figure 1. we discuss several optimizations below. 
　a list cp of state-game cps and associated mechanisms is kept as they are discovered. a cp exists if the set of optimal joint actions at a state/stage pair  the q-values in step 
1 a i  induces a cp in the sense defined earlier. notice that cps are defined using the value function vt not immediate reward. we assume that each cp is associated with a state and the collection of actions involved in the optimal joint actions. any state s  with a cp will have the availability of actions involved in the cp restricted by the state of the mechanism. the set  is the set of actions permitted at sgiven the mechanism state-this may include randomization actions as well  if  has no cp  this set is just  and agents can only use permitted actions  step 1 a i . if a cp is discovered among the maximizing  permitted  actions at si  a new mechanism c is introduced and the state is split and replaced by all pairs of states   where c is some state of c . 


figure 1: value iteration with state expansion 
　to illustrate  suppose the value function v1 induces the following choices at s : 
a 1 c a 1 1 1 b 1 1 1 c 1 1 1 if randomization is used to coordinate on a/1  expected value is 1  and the mechanism requires agents to randomize over their plo-actions . in contrast  the q-value of  c  c  is better than that of attempting to coordinate  thus the value of s  is defined as 1 if the agents are uncoordinated  and 1 if they are coordinated . notice that new cps may be introduced at the same state and the process can be iterated.1 in this problem  each state s is split into three states:  s  a   agents have coordinated on joint action  a  a  at   s  b   coordinated on  1    and  s  u   have not coordinated w.r.t. a and 1 . 
　if a mechanism has been introduced for the same state and actions at an earlier stage  a new mechanism is not generated. value  and policy choice  is defined by comparing the value of actions not involved in the cp and the value of behaving according to the rules of the mechanism  step 1 a iv . at the next iteration all states are split according all mechanisms introduced  since this may be required to predict the value of reaching state s . if multiple cps exist  each underlying system state is expanded many times in this  naive  algorithm. 
　implicit in this discussion is that assumption that the transitions induced by a coordination protocol over the expanded state space are well defined: this will generally involve extending the underlying system dynamics by rules involving mechanism state evolution. the mechanism designer must provide such rules  as discussed in section 1 . 
　an important optimization is to have the algorithm only expand states with mechanisms whose state is required to predict value. this can be effected rather easily. if system state si transitions to state sj  and sj has been split in st to involve some mechanism in cp  s  must be split in state st+1. but if si moves only to states that are unaffected by some  or all  cps  si need not be split using the state of those cps. this 
1  however  the splitting must eventually terminate. 

figure 1: a more complex coordination problem. 
allows one to only refer to the state of a mechanism when it is necessary for predicting value: the state space need not be split uniformly. 
　other optimizations of the algorithm are possible. for example  one can  cluster** together states of the coordination mechanism together that provide for the same optimal action and value at a given state. for instance  though fp has an infinite number of distinct states  for any finite number of stagesto-go  only a finite number of distinction are relevant  much like state abstraction methods used in mdps and reinforcement learning  1  1  . finally  we note that modeling communcation protocols requires introducing communication actions  in addition to the state-splitting mechanism above. 
1 examples 
we describe the results of applying the algorithm to several small test problems in this section. we focus here on the use of the simple randomization mechanism described above. 
　testing a finite horizon version of the problem in figure 1 shows that a single cp exists  at state s1 . the state space is eventually expanded so that each state is split into two  referring to coordination or lack of it at s1 . the optimal decision at  s   u  is to  opt out  with fewer than eight stages to go and  opt in  with eight or more stages remaining. the infinite horizon version of this problem gives rise to stationary policies. when the discount rate  = 1  or higher   a1  opts in  at  but for  = 1  or lower   a   opts out  and avoids the cp-because of discounting  the delay in expected payoff of coordination ensures that  opting in  is not worth the cost. with  = 1  the value of opting in is 1 and opting out is 1  assuming the agents act optimally thereafter   while with  = 1  the value of opting in is 1 and opting out is 1  within tolerance 1 . 
　a more complex example is illustrated in figure 1. two agents have independent tasks. agent al moves box 1 and a1 moves 1 to the goal state repeatedly. once a box is dropped at the goal  a reward is received and a new box appears in the original location  so the problem is a continuous  infinite horizon mmdp . while the objectives are independent  both agents are rewarded with the same constant reward whenever either of their boxes is delivered. the optimal policies are not independent however. the dark shaded region at the bottom is  risky:  if both agents are in the region  a large  variable  penalty is given. they must coordinate their moves to ensure that no more than one agent is in the risky 
	b1utilier 	1 

region at any time. the agents* actions are stochastic: they can move in any  feasible  compass direction but with probability 1 they fail to move  they can also stay in place intentionally . complicating the problem is the fact that the light shaded region is  sticky:  the agents' moves are more prone to failure  with varying probability . if stickiness is low  the optimal policy is for both agents to traverse the top of the grid repeatedly. but if stickiness is relatively high  or the problem is heavily discounted  making speedy delivery more important   one or both agents will want to traverse the risky area  in which case coordination is needed. the problem has 1 nominal states  though a number of these are not reachable  and 1 joint actions. 
　we give a brief summary of the results in this domain with the following specific parameter settings: a reward of 1 is given for each box delivered; a penalty of -1 is given whenever both agents are in the risky area; stickiness  the probability of not moving  is 1 in the sticky region; and  = 1. with these settings  the optimal joint policy  roughly  requires that one agent move across the top of the grid and one move across the bottom.1 generally  if an agent is closer to the top it will move across the top; but if both agents are close  and equally close  to the bottom  they must coordinate  since either could move to the top . 
　cps arise at eight states of the mmdp thus there are eight coordination mechanisms needed to solve this problem  expanding the state space by a factor of 1  no distinctions need be made among coordinated choices  so each mechanism has only two states . we focus on two mmdp states where cps arise and their interaction: =  
where both agents are located at grid cell 1 each holding boxes  and  =  which is similar  but with 
both agents at location 1. the optimal joint policy at requires one agent to move up  to traverse the sticky region  and the other to move down  to traverse the risky region  on the way to the goal. the optimal policy at  is similar: one agent should move up  the other right. the optimal joint value function has 
　if the agents have coordinated at all other states where cps arise  we have the following optimal values for the four states of the expanded mmdp corresponding to each of  and 
        here we use to denote that the agents have not coordinated at s1  and c1 to denote that they have coordinated at 
if the agents are uncoordinated  
the optimal policy requires them to randomize  regardless of the state of the other coordination mechanism. notice that the values for most of the expanded states where the agents are uncoordinated are less than the corresponding values for the optimal joint policy  which is identical to the expected values at the states where c1 holds   as expected. the one 
　　1  if the penalty is negligible or if the stickiness is even higher  the agents will both tend to move across the bottom  perhaps with one waiting for the other. if the stickiness is negligible  then both agents will traverse the top of the grid. 
1 	distributed al 
exception is at  when  holds  expected value is identical whether or not c1 holds  since the optimal policy will never take the agents from  in contrast  when u1 holds  the status of c1 has a dramatic impact on expected value: if the agents are uncoordinated at  they will randomize and with probability 1 both choose to move down  hence to  their state of coordination at s1 is thus important to predicting expected value. being uncoordinated at  has very low value  since randomization has a good chance of moving both agents to the risky area-the risk is worthwhile  however  so randomization is the optimal choice at  also when the agents are coordinated at the status of c1 has a rather small effect on value. because coordination at  ensures that one agent takes the  sticky  route to the goal region  the agents get  out of synch'* and the odds of them both reaching the pickup location  cell 1  at the same time  within a reasonable time frame  is quite small. hence  whether or not the agents are coordinated at  has little impact on expected value at  
　randomization is an important aspect of this problem. if the agents were to choose from among their p1 actions independently  but deterministically  without reasoning about the consequences of miscoordination  they can end up in cycles that never reach the goal state. 
1 	generalization of coordination decisions 
one difficulty with the algorithm above is the potential for uninhibited state expansion  and the corresponding computational cost. in the simple experimental domain with two agents collecting boxes in a grid world  eight cps occurred across the 1 problem states  requiring the state space to be increased by a factor of 1  to 1 states . fortunately  in many circumstances we can introduce a single coordination mechanism to deal with multiple  related cps. in the grid problem  for example  once the agents coordinate at a state by one agent moving up and the other down  they can maintain these  roles  at other states exhibiting similar cps. 
　　we do not propose a method for constructing such generalizations automatically-this could use  say  generalization techniques from reinforcement learning -but we illustrate potential benefits with the simple example shown in figure 1. it is similar to the mmdp in figure 1 except that miscoordination at  has a larger penalty  and an analogous  low cost** cp has been added. if a single mechanism is used for both cps  at  and    once coordination is attained at  it is automatic at as in the original mmdp  with fewer than 1 stages-to-go  the optimal action at  is to  opt out** and take the sure reward 1. with 1 or more stages remaining  the optimal action at  the agents move to the low risk cp and try to coordinate there. never do the agents move to  in an uncoordinated state. 
even though there is no immediate benefit to moving to  it gives the agents an opportunity to  train ** or learn to coordinate with minimal risk. once they coordinate  they immediately exploit this learned protocol and choose at 
 thereby moving to  reasoning about the long term 
1 though with higher penalties  it is not. 


figure 1: an mmdp with similar coordination problems 
prospects of coordination and its costs  the agents realize that risk-free training is worthwhile. 
　if we retain the original penalty of -1 at s1  this reasoning fails: there is essentially less risk involved in training at the high stakes cp  so the agents will never move to s1 to train. 
　the infinite horizon problem is similar. with a discount rate of 1  the optimal policy requires the agents to move to s1 until they coordinate  at which point they repeatedly move to s1. interestingly  adding the  training states'' increases the expected reward accrued by the agents. without the training states   1 since the agents accept the risk of getting several -1 rewards to ensure coordination. with the training states  they can learn to coordinate without the severe penalties  and 1. 
	1 	concluding remarks 
we have introduced a novel method of defining value functions  and consequently  optimal policies  for multiagent decision problems that accounts for specific means of coordination. we also defined a value iteration algorithm for computing optimal policies that recognizes and reasons about cps. 
　further experimentation is needed with other coordination mechanisms and their impact on policy value. we have described experiments in this paper using randomization  and have begun to investigate communication methods  and hope to explore other models like fp we intend to introduce economic models  such as auctions  so that agents may integrate reasoning about their activity in markets into their decision processes. we must explore automated generalization methods further; it has the potential to substantially reduce the required number of mechanisms  alleviate computational difficulties  and increase objective policy value. 
　we would also like address the problem of designing robust  computationally effective and value-increasing coordination protocols in the framework. in a certain sense  such an undertaking can be viewed as one of designing social laws . it is also related to the issues faced in the design of protocols for distributed systems and the distributed control of discrete-event systems . but rather than designing protocols for specific situations  metaprotocols that increase value over a wide variety of cps would be the target. the framework developed here can also help decide whether sophisticated protocols are worthwhile. for instance  a lexicographic protocol induces immediate coordination with a measurable  in our model  increase in expected value over  say  a randomization method. this increase can then be used to decide whether the overhead of incorporating a lexicographic convention  e.g.  ensuring agents have common orderings  is worthwhile. similar remarks can be applied to the design of agents  e.g.  is communicative ability worthwile given the class of decision problems they will face . 
acknowledgements 
this research was supported by the darpa co-abs program  through stanford university contract f1-c1   nserc research grant ogp1  and iris phase-ill project bac. thanks to ronen brafman for discussion of these issues. 
