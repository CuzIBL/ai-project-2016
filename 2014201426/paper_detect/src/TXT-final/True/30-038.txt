 
case-based reasoning  cbr  is concerned with solving new problems by adapting solutions that worked for similar problems in the past. years of experience in building and fielding cbr systems have shown that the  rase approach  is not free from problems. it has been realized that the knowledge engineering effort required for designing many real-world easebases can be prohibitively high. based on the wide-spread use of databases and powerful machine learning methods  some cbr researchers have been investigating the possibility of designing casebases automatically. this paper proposes a flexible model for the automatic discovery of abstract cases from data.bases based on the lattice machine. it also proposes an efficient and effective algorithm for retrieving such cases. besides the known benefits associated with abstract cases  the main advantages of this approach are that the discovery process is fully automated  no knowledge engineering costs . 
k e y w o r d s : case-based reasoning  machine learning  knowledge acquisition  automated modeling 
1 	introduction 
case-based reasoning  cbr  is concerned with solving new problems by adapting solutions that worked for similar problems in the past. cbr research is motivated by the desire to establish cognitive models to understand human thinking and behavior  psychology  cognitive science   and to build more effective  efficient  and robust computer systems that solve real-world problems  artificial intelligence . since the early nineties  cbr has been successfully applied in a wide variety of areas. however  with almost ten years of both theoretical and applied experience in building and fielding case-based systems  it has been realized that the  case approach  is not free from problems  leake  .1 . two of the more important issues that need to be addressed are outlined below: 
	  the 	implementation 	of 	casebase 	maintenance. 
1 	case-based 	reasoning 
policies for revising the organization of casebases in order to facilitate effective and efficient future reasoning  leake and wilson  1 ; and 
  the automation of the case  engineering process  that is. the automatic generation of case- knowledge components from existing information  i.e.  databases  
 patterson et. al.  1 ; the work presented in this paper will focus on this issue. 
   the term case engineering refers to task of designing a casebase  that is  the processes of generating those components that represent the application-specific knowledge contained in a cbr system. in general  such knowledge structures describe the content- structure of cases  criteria for organizing cases within and retrieving cases from the case library  indexing scheme  similarity measures   rules for adapting case solutions  and case revision and retention schemes  lenz et a/.  1 . in addition to determining the fundamental knowledge elements  the generation process also entails the selection or extraction of those cases that will be used to populate the initial casebase. 
　the work presented in this paper proposes lattice machine a formal framework for learning from relations  and its application to automatically construct  case content description  and extract  case selection  abstract cases from databases. furthermore  an efficient algorithm for the retrieval of such abstract  cases is also proposed. 
　the remainder of the paper is organized as follows. section 1 briefly discusses the role of abstract cases in 
cbr. section .1 introduces the definitions and notational conventions  followed in section 1 by a formal description of lattice machine  the pivotal part of the work in this paper. sections 1 and 1 present efficient algorithms for extracting abstract cases and retrieving cases  based on the theoretical results in lattice machine. experimental results are reported in section 1. section 1 concludes the paper. 
1 	abstract cases in cbr 
traditional cbr systems retrieve  reuse  and retain cases in a representation reflecting concrete problemsolving episodes. recently  researchers have investigated 

the role and use of abstract cases  e.g.   bergmann and wilke  1 . abstract cases represent cases at a higher level of abstraction. through abstract cases  the cbr process can be supported in several ways  bergmann and wilke  1 ; those ways pertinent to the discussion in this paper are outlined below: 
  abstract cases can reduce the complexity of a case-base by substituting sets of concrete cases and thereby significantly reducing the size of the casebase. a drastically reduced caseba.se can improve retrieval efficiency  reduce maintenance costs  and eliminate or alleviate the notorious swamping problem  smyth and keane  1l 
  cases at higher levels of abstraction can serve as prototypes for indexing larger sets of more detailed cases. this can have profound effects on reducing retrieval times  maintenance costs  and it can promote a better user understanding of the easebase content  and facilitate explanations for the system's reasoning process. 
   in most situations  abstract eases are not readily available  they must be generated - manually or automatically from concrete cases. to manually construct abstract cases will require a very high knowledge engineering effort for most applications. whereas an automatic generation procedure requires general domain knowledge about ways of mapping concrete cases onto higher levels of abstraction. 
   the abstract case construct proposed in this paper is based on the concept of hypertuples. hypertuples  i.e.  abstract cases  are automatically generated through a so-called domain lattice  which is implied in a problem domain. the extraction and retrieval of abstract cases are achieved in domain lattice using the lattice machine. 
1 	definitions and notation 
to present our findings concisely and within the given page limit  we briefly introduce some notational conventions and definitions that are used throughout the paper. 
1 	decision systems 
an information system is a tuple i t-- is a nonempty finite set and 
is a nonemptv finite; set of mappings 
　we interpret u as a set of objects and  as a set of attributes or features  each of which assigns to an object a its value under the respective attribute. let 
for  we 
　　　　　　　　　　　i is called a tuple  and the collection of all tuples is denoted by d. thus  for each   there is at least one  such that 
1
　　note can be finite or infinite. for the latter ease  the domain lattice is infinite. however we are only interested in the finite sublattice generated from a finite casebase. 
   a decision system v is a pair  j  d   where 1 is an information system as above  and d :  -  is an onto mapping  called a labeling of d; 
the value d t  is called the label of t. 
	the mapping d induces a partition 	of d with the 
classes    where 
　in this paper we consider a dataset represented as a decision system d  which can be regarded as an initial easebase consisting of concrete cases. then d is the set 
of  descriptions of  concrete cases  d is the case solution  and v is the set of all possible concrete cases in a problem domain. therefore each  d is associated with a 
solution in the form of a class label d{t . 
1 	order and lattices 
let 	. we 
let  	. i f no confusion can 
arise1  we shall identify singleton sets with the element they contain. 
   let l be a  lattice  partially ordered by for x  y the least upper bound  or sum  is written by x + y and the greatest lower bounel  or product  by 
  its least upper bound and greatest lower bound are denoted by lub a  and glb a  respectively. an element a  a is called maximal in a  if for all  x implies x = a. 
　　　　　　　　　  we say that. b covers a or a is covered by b  written as if for each  there is some a c b such that    . 
   the1 sublattice of ♀ generated from   written by  such that  t - lub x }. the greatest elememt in |a/  is lub m . if m is finite  m  is also finite. 
　a com found in 
1 	the lattice machine 
this section introduces the lattice machine  a construct which facilitates the discovery of abstract ea.ses from a given dataset. the discussion should also make apparent how the lattice machine' is linked to machine learning concepts. 
1 	domain lattice 
we have found that  given a dataset expressed as a decision system  an elegant mathematical structure  lattice*  is implied. this structure makes it possible to investigate cbr  machine learning  as well the relationship between the two from an algebraic  perspective. in the sequel  we shall use d as described above as a generic decision system representing a dataset. 
	let 	. 	t h e n i s a vector 
where1 are sets of values 1. the elements of l are called hypertuples; the elements t of c with = are called simple tuples. 
wang  dubitzky  duntsch  and bell 

hypertuples is called a hyperrelation 1. note that v is a set of all simple tuples for a given problem domain  and d is the set of simple tuples described in the dataset v. c is a lattice under the ordering 
 1  	
with the sum and product operations  and the maximal element  i.e.  1  given by 
 1   1   1  
c is called domain lattice for v. 
there is a natural embedding of d into ♀ by assigning 

and we shall identify d with the image of this embedding. then 
   in the context of cbr  simple tuples are concrete cases whereas hypertuples are abstract cases since hypertuples cover multiple simple tuples hence they are  abstractions  of simple tuples. 
   table 1 a  is a dataset  decision system  consisting of three simple tuples  where  = 
{1  1}  and d is the labeling. table 1 b  and  c  are sets of hypertuples  which are the least and greatest e-scts respectively for the dataset  to be defined later. 

table 1: 	 a  a set of simple tuples in a decision system  
 b  a set of hypertuples as the least e-set.  c  a set of hypertuples as the greatest e-set. 
1
the concept of hyperrelation has been used before in e.g. 
 orlowska  1; wang et a/.  1 . 
1 	case-based reasoning 
1 	equilabelledness and generalization 
a dataset imposes a labeling d of d on the domain lattice c. thus all elements in d are labeled  and those in  are unlabeled. this labeling can be generalized to elements in the lattice which cover d. this generalization must  be consistent with d in the sense that the generalized labeling must be the same as d for t  d. this renders only those generalizations acceptable which generalize d to equilabeled elements. intuitively  an equilabeled element is t  c which covers at least one labeled element and all labeled elements covered by t have the same label. 

figure 1: a running example. 
   for an illustrative example  consider the diagram in figure 1  which depicts a small part of a. domain lattice. the bottom elements  thin-lined circles  in the diagram represent simple tuples. all other elements  bold-lined circles  represent hypertuples. the original labeling d is defined only for the elements { h   /   j  a'  a/}. here  b is equilabeled as h and i are all labeled positive by d while g is unlabelled. in fact  all elements in figure 1 are equilabeled except a g and l: a covers elements with different labels whereas g and l cover no labeled element. 
　　formally  we call an element with respect to 	. in other words  r is equilabeled if | r intersects d  and every element in this intersection is labeled  for some recall that k is the number of classes. in this case  we say that r g-belongs to . we denote the set of all equilabeled elements g-belonging to be the set of all equilabeled elements. note that and that  .          implies 
we will now extend d over all of ♀ by setting 

   now e  along with the extended labeling  can be regarded as a casebase of abstract cases  hypertuples . this is clearly too large. since the elements in ♀ are partially ordered - some are covered by some others 

we need only look at those which are not covered by any; they are maximal our wish to find maximal elements in some context leads to the following notions. 
def. 1. a  generalization  context  for learning  is a set p such that 
c is said to be 
in context p if t 
we observe the following lemma 1: 
l e m m a 1. 	implies   and 
e a  . e b . 
this lemma implies e d   e p   e v  for d 
p v. we then call e d  least e-set and e v  greatest 
e-set  which are denoted by e and e respectively. it is not hard to see that e is the set of all maximal elements in 1. 
　consider figure 1 again. e   { p          k m}  - {c m/} ande {g ---   m }   = { b   c   d . m } . 
1 	i n t e r p r e t a t i o n of a d o m a i n lattice 
given a hypertuple t in a. context p  i.e.  t  m p    we need a calculus to get the remaining hypertuples in the same context  see theorem 1 below . we therefore need to interpret the domain lattice in a suitable way. 
	formally  let t = 	be a hypertuple  with 
. we think of t as an  undetermin-
istic  description of some object a in the following way: 

each e1 . stands as a hypertuple on its own  which is 
interpreted as ; and is interpreted as glb ej  ek . 
consider a hypertuple t - . the relative 
complement or  simply  r-complement of t with respect 
to the greatest element in and  is interpreted as 
given a hyperrelation r = 
  we wish to calculate its r-complement r in such a way that  and . for this 
 purpose we interpret r as to v.... v tn. using the propositional calculus we can calculate the r-complement. of r as follows. let 
clearly the calculation of r-complement is exponential  which makes this not practical for large r. 
　the above r-complement calculus can be generalized as follows. 
if t h then the r-cornplement of t with respect to h  denoted by is calculated similarly except that 
1
　　due to lack of space  we omit all proofs throughout the paper. 
lemma 1. for r c c  
1. 
1. 
is the 
set of all maximal elements in w. 
　the first two properties guarantee that r is exclusively complementary to r with respect to the maximal element of the domain lattice. for an example  consider 
table 1 a . the table is interpreted as   is interpreted as 	= 
then 	  which clearly covers and u1  as well as an unseen tuple  b  1 . 
　the third property says that r is the set of maximal elements not covered by r. consider figure 1. let r = 

1 	h y p o t h e s i s space a n d casebase 
given a dataset we wish to have a hypothesis to replace the entire dataset. the hypothesis should not only cover the dataset but also generalize it. as discussed earlier  a dataset v imposes a labeling on the underlying domain lattice. the labeling can then be generalized to elements in m   p   for a given context p. however we do not need to use the whole m   p   as the hypothesis; a proper subset of it will suffice. then a hypothesis is just a set of hypertuples  each of which is more informative than the simple tuples in the original dataset. therefore  it is possible to consider a casebase as a hypothesis for a dataset. in this section we will introduce and justify some concepts through which we can precisely describe what kind of hypothesis we are aiming for. 
　　note that  by lemma 1  m   d   m   p   m   v   for d v. therefore h  m   p   h m   v   . 
then we have 
def. 1. a hypothesis for d is a // m  v  such that d  h. we use g e n   d   to denote the set of all hypotheses for d 
　　similarly  we define a hypothesis for  note that for a hypothesis h for d   is a hypothesis for is a hypothesis for  for each then is a hypothesis for d. 
　since m   p   contains only equilabeled elements  h is consistent with the dataset. since d  h  h covers all 
simple tuples in the dataset. 
def. 1. let  and  be two hypotheses for d. is more general than hk if and only if 
 strictly  more general than h k   written 
  if and only if 
　a hypothesis // for d is maximally general if and only if h  gen d  and there is no hf  g e n   d   such that h  h'. we denote by g the set of all maximally general hypotheses for d. in  mitchell  1  g is called the general boundary for d. 
　the following lemma establishes the equivalence between g and the greatest e-set  e . 
wang  dubitzky  duntsch  and bell 

lemma 1. g = {e}. 
   this lemma says that  although there are many possible hypotheses for a given dataset  there is only one maximally general hypothesis the greatest e-set. this hypothesis is consistent with the dataset but has the maximal coverage of unseen simple tuples  because it has the maximal context v   . general boundary is a well established concept in the field of concept learning  and it has been used as an inductive bias in some concept learning algorithms  mitchell  1 . the equivalence of the greatest e-set and the general boundary enables us to use the same inductive bias in automatic casebase design. therefore our objective is to find the greatest e-set for a dataset. our casebase design and retrieval are both associated with the greatest e-set. 
   however  as shown in  haussler  1   the size of the general boundary can grow exponentially in the number of training examples. in the context of domain lattice  calculating the greatest e-set needs v  which is not explicitly available; instead it has to be calculated from d using the r-complement calculus discussed above. this involves calculating d  which has been shown exponential in |d|. it is then not practical to directly use the greatest e-set. as the casebase. the following theorem guarantees that we can use a much smaller hypothesis as the casebase  but we can still use the general boundary as inductive bias. in effect it establishes the relationship between the casebase and the greatest e-set. 
t h e o r e m 1 . 1 . let 	h 	be 	hypothesis 	for 	v  	as and 
　this theorem says that if a case  simple tuple  t is covered by e  then there must be h such that  h or 
with g being cither equilabeled or 
unknown. note that is the r-complement of h with respect to h+t   which is the set of all elements covered by h  t but not covered by h. the c a s e r e trieve algorithm in section 1 exploits this theorem to retrieve cases to classify new cases. 
　theorem 1 says that any hypothesis can be used as a. caseba.se that serves as an intermediary between a new case and the greatest e-set. classification of a new case can then be made based on its relationship to the greatest e-set  which employs the general boundary inductive bias. 
1 	case extraction 
as indicated in theorem 1  an expected casebase can be any hypothesis as defined in def. 1. the simplest one is the dataset itself. however  checking whether the conditions are satisfied requires computing r-complemcnts of the tuples. it is usually the case that datasets are large hence the computation cost is high. therefore we need an algorithm to efficiently find a hypothesis  other than the dataset itself  satisfying the con-
1 	case-based reasoning 
ditions. the least e-set seems ideal since it is the e-set in the minimal context  d . however calculating the least e-set is computationally expensive. the following algorithm  c a s e e x t r a c t   finds  given the minimal context d  the set of elements in m   d   which have disjoint coverage of d. 
given  and  as defined above. 
  initialization: 
  repeat until x is empty: 

this algorithm bi-partitions x into a set of elements the sum of which is an equilabeled element  and a new x consisting of the rest of the elements. the new x is similarly bi-partitioned until x becomes empty. this process leads to a binary tree  the depth of which is a measure of the time complexity of the algorithm. in the worst case the time complexity for building the casebase for class  i s i n the order . therefore the worst case complexity for building the whole casebase is   where k is the number of classes. 
　consider figure 1. c a s e e x t r a c t gives the hypothesis  which has disjoint coverage of the labeled elements. 
1 	case retrieval 
the retrieval of relevant cases from the casebase is arguably the most important process in cbr. in this section we discuss how to retrieve cases from a casebase to classify new instances. having a casebase ii as discovered by the c a s e e x t r a c t algorithm  we can associate a new instance /  v with a case h  ii by checking whether t is covered by e through h. then t is regarded as being in the same class as h. the c a s e r e t r i e v e algorithm is as follows. 
  sort the elements in h in decreasing order of |a'| 
             h and h = lub x   which results in if - with ho having the largest coverage of d elements. 
  t is classified by the first  h  in the sorted // such that the conditions in theorem 1 are satisfied. 
  if there is no such i. 
the time complexity of this algorithm is dominated by calculating the r-complement of h h  as needed in theorem 1. this is in the order of o t   where t is the number of attributes. in the worst case we need to do so for all hi  in h  i -      ＊   n. therefore the overall time complexity of the algorithm is  in the worst case. 
　to illustrate the caseretrieve algorithm  consider figure 1. the casebase is now   as discovered by c a s e e x t r a c t . consider a new case g. the sum of g and e is b  which is equilabeled. then e is retrieved 

and g is labeled as positive. 	clearly g  b  hence 
g  e. 
1 	experiment 
c a s e e x t r a c t and c a s e r e t r i e v e are implemented in our cbr system  called l m . we compared lm with c1 vising public datasets. the datasets are described in table 1. datasets in the upper half are from uc irvine machine learning repository; and those in the lower half are collections of documents which are used as benchmark for text mining study  cohen and hirsh  1 . the results are shown in table 1. 

table 1: prediction accuracy of 1 and lm 
1 	summary and conclusion 
the paper proposed a promising model for automating the design of cbr. systems. revolving around the notion of hypertuples  abstract cases   the proposed model presents a successful attempt at combining powerful eager techniques from machine learning with the flexible  defer-processing  philosophy characteristic for lazy methods  aha  1 . on the basis of concise formal argument and empirical evaluation  it has been demonstrated that  the lattice machine approach constitutes an effective; and efficient mechanism to discover abstract cases in a given dataset. abstract cases have been shown to be an effective alternative to representing the knowledge held in cbr. systems  bergmann and wilke  1 . 
they can provide answers to issues such as casebase complexity  maintenance costs  retrieval efficiency  and user acceptance. in addition to the discovery of abstract cases  an algorithm was presented  which employs the general boundary inductive bias and ensures that the retrieval of relevant abstract cases is within the limits of reasonable time constraints. the main contribution of this work lies in the lattice machine's ability to discover abstract cases within a given dataset without requiring difticult-to-obtain domain knowledge. 
