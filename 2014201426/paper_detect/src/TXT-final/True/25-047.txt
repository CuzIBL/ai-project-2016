 
we present a data-driven protocol and a supporting architecture for communication among cooperating intelligent agents in real-time diagnostic systems. the system architecture and the exchange of information among agents are based on simplicity of agents  hierarchical organization of agents  and modular non-overlapping division of the problem domain. these features combine to enable efficient diagnosis of complex system failures in real-time environments with high data volumes and moderate failure rates. preliminary results of the realworld application of this work to the monitoring and diagnosis of complex systems are discussed in the context of nasa's interplanetary mission operations. 
1 introduction 
the interaction and coordination of multiple agents in distributed problem-solving systems have been of interest for a variety of domains whose complexity exceeds the practical capability of monolithic solutions. examples of domains in which distributed systems have been explored include monitoring  lesser and corkill  1   planning  howe et ai  
1    bratman et al  1   and diagnosis  d'ambrosio  1 . these and other approaches  durfee et ai  1    gasser et al  1    hayes-roth et ai  1  emphasize some form of iterative exchange of partial information among nodes for the purpose of eventual convergence on complete solutions. 
　　recently  the need for mechanisms of cooperation that are sufficiently robust for real-world applications has been addressed  jennings and mamdani  1  as part of 
the research described in this paper was carried out by the jet propulsion 
laboratory  california institute of technology under a contract with the national aeronautics and space administration. the authors wish to acknowledge support from jpl's director's discretionary fund. 
distributed ai grate*  an implementation effort targeted at monitoring. grate* makes contributions toward a clearer and more easily implementable interaction of agents during collaborative problem solving. grate* addresses a problem domain in which events occur unpredictably and decisions may be based on incomplete or imprecise data. toward this end  the notion of joint responsibility is proposed as an alternative to the more conventional notion of agents acting in self-interest. the potential for large communication overhead is a possible disadvantage of the grate* system  particularly for applications with time critical analysis. the protocol and architecture described in this paper builds on the notion of joint responsibility and uses modular problem decomposition and data-driven reasoning in order to minimize communication between agents. this approach has been applied to the marvel system  schwuttke et ai  1  for automated monitoring and diagnosis of voyager spacecraft telemetry and has been shown to achieve robust and coherent behavior for complex  real-time diagnostic agents embedded in a conventional  algorithmic  monitoring system. 
1 the characteristics of agents in marvel 
agents are embedded diagnosticians. 
　　rule-based diagnostic modules are embedded in efficient algorithmic code. the algorithmic code performs all functions that do not explicitly require reasoning capability  so that the use of the less efficient reasoning modules is limited to those functions for which it is essential. 
diagnosis is data-driven. 
　　forward-chaining demons are used to represent domain knowledge. reasoning is activated by the appearance of data that requires diagnosis. the initial determination that diagnosis is required is made by algorithmic monitoring code  which detects potential anomalies algorithmically and passes the anomalous data to an appropriate diagnostician. in the absence of anomalous data within its domain  an agent is idle. 
the domain of individual agents is constrained. 
　　an agent is responsible for a small  clearly partitionable domain of expertise. partitioning is governed by the natural decomposition of the system being diagnosed. this helps overcome disadvantages associated with rule-based systems for which  typically  implementation can be intractable  execution is non-deterministic and relatively slow  and verification can be difficult. small  modular knowledge-bases enable developers to handle more easily definable subproblems. smaller knowledge bases execute more efficiently  because less time is spent in search. finally  smaller knowledge-bases are easier to verify. 
the domain of individual agents is nonoverlapping. 
　　a particular domain of expertise is assigned only to one agent to avoid redundant reasoning. 
agents carry individual responsibility for problems entirely within their domain. 
　　agents have sufficient knowledge to be fully accountable for diagnoses within their areas and have no knowledge of other domains. this requires that accountability for locally detectable failures must be local. 
failure domains may not map directly to agent domains. 
　　diagnosis requires more than one agent when the symptoms manifest themselves in more than one domain. 
meta-knowledge enables agents to instigate cooperation for diagnosis beyond their domain. 
　　agents have meta-knowledge to identify symptoms of failures that could possibly extend beyond their domain. meta-knowledge is contained in a set of rules in each knowledge-base  and is associated with the occurrence of events whose analysis may require the cooperation of other agents. 
agents report all problems that extend beyond their domain. 
　　meta-knowledge enables an agent to determine which symptoms from its domain may portend problems beyond its domain. the meta-knowledge also includes the specific agent s  to which the information should be forwarded. 
a hierarchy of agents provides coordination. 
　　an agent forwards all known information pertaining to failures beyond its domain to another agent at the next higher level in the hierarchy. the underlying assumption on forwarded messages is  better safe than sorry ; it is up to the agent receiving the information to determine whether a fault requiring a diagnostic message and an alarm has occurred or whether the anomalous data has some other explanation. this agent may also receive messages from other lower-level agents. agents at the higher level are also implemented according to the principles outlined here; thus reasoning at the higher level of the hierarchy is also data driven. the agents at the higher level are activated by messages from lower-level agents  just as the lowest level agents were activated by messages of symptoms detected by algorithmic code. messages are directed with meta-knowledge to the relevant agent s  in order to complete the final analysis of the anomalous data and provide diagnosis of any associated failures. 
agents share responsibility for diagnosis of problems that overlap domains. 
　　joint responsibility exists in that the lower-level agents are responsible for reporting appropriate symptoms upward in the hierarchy and the higher-level agent s  are responsible for correctly determining whether failures have occurred and providing appropriate diagnosis. this differs from the  self interest  model of communication  durfee  1 and is similar to the joint responsibility model  jennings and mamdani  1  in which agents must temper their self-interest with consideration to a group. these models have parallels in social organizations  with the first being more representative of an unstructured society and the second paralleling the actions of individuals who are dedicated  perhaps for reasons of self-interest  to fulfilling a successful role in a structured organization such as a corporation. in the latter case  independent agents work together with appropriate  and hierarchical  division of responsibility towards fulfilling a common goal. real-world applications can be sufficiently complex that only this second type of organization may enable timely  robust  and coherent behavior. 
1 cooperating agents in a distributed architecture 
the distributed architecture shown in figure 1 is based on a central message routing scheme that is not shown in the figure. the various agents are allocated among a configuration of unix workstations. the data management module receives data from a source  in the case of our current application  the data is spacecraft telemetry received from jpl's ground data system  and allocates it to the appropriate subsystem monitor based on identification of data type.  our system is partitioned according to the partitioning of the spacecraft itself  with one subsystem monitor for every spacecraft subsystem covered by marvel. spacecraft subsystems include command and data  attitude and articulation control  propulsion  telecommunications  thermal  and power. such a partitioning reflects the natural partitioning of the system being monitored  which is desirable for real-time diagnostic architectures.  each of the subsystem monitors provides algorithmic functions such as validation of telemetry  detection of anomalies  trend analysis and automatic reporting. these functions  while not in themselves of interest in ai or computer science research  are vital components of a real-world diagnostic system. they are implemented here in conventional c code for performance reasons. in addition  each subsystem process provides diagnosis of failures based on anomalous data  and recommendation of corrective 
	schwuttke and quan 	1 


figure 1. the distributed architecture on the left can currently be configured to run on one to four unix workstations  with the most common operational configuration involving two workstations  for compatibility with analyst responsibilities . the hybrid subsystem processes on the left are composed of conventional and knowledge processes  as shown in the figure on the right. knowledge processes are used only when a reasoning capability is explicitly required. 

 actions. the latter two functions are provided by knowledgebased agents that are embedded within each of the individual subsystem monitors. the remaining modules include the graphical user interface and display processes for each of the subsystem monitors  and the system-level diagnostic agent for handling failures that manifest themselves across multiple subsystems  and therefore cannot be completely analyzed by any one subsystem alone .the interconnectivity of the distributed system is provided by a tcp/ip central router program and a set of messaging routines that are linked into the subsystem processes. all processes are connected to the central router by unix sockets. the basic measurement of performance for the distributed system is the speed-up s n   defined as the sequential execution time divided by the execution time on n processors. the current implementation of marvel has six basic agents: a data management module  four subsystem modules  and a system-level diagnostician. it has not been possible to measure a unique value s n  because of the heterogeneous nature of the agents. this heterogeneity arises because the processing loads of the six agents are not identical. our alternative to this measurement is the lowest speedup of the individual modules. with a fourprocessor implementation  a speedup of 1  or 1n was observed. this result indicates that marvel is a highly efficient distributed system. two factors contribute to these results. the first is the modularity inherent in the application  and in many other complex applications . the second factor is a distributed design that effectively minimizes the need for interprocess communication. 
distributed al 
1 application to monitoring and diagnosis of a real problem 
in this section we provide an example of cooperation between multiple hierarchical agents in an actual real-time system  as shown in figure 1. this figure depicts four knowledge-based agents  shown in black   each of which has expertise in a different domain of the engineering subsystems of the voyager spacecraft. two of these agents are responsible for diagnosing anomalies in specific spacecraft subsystems: computer command subsystem  ccs   and attitude and articulation control subsystem  aacs . a third agent  the system level knowledge agent  is at a higher level in the agent hierarchy and is responsible for diagnosing anomalies that cannot be fully analyzed in any single subsystem domain. a fourth agent provides data quality information to the other agents based on data from the telecommunications subsystem  telecom   so that when data quality is poor  alarms resulting from the diagnostic activity of the other agents  based upon the faulty data  can be suppressed. any diagnostic communication between agents is coordinated by the system level agent. all data quality message communication is handled by the data quality management process  an algorithmic module that also communicates with the graphical user interface. there is no direct communication between subsystem agents. as explained in the previous section  each agent has an algorithmic telemetry monitor process associated with it. 

　　the telecom agent differs from the other two subsystem agents in that its purpose is to determine the quality of the telemetry data being received from the spacecraft  rather than diagnose subsystem anomalies that occur on the spacecraft. the data quality level is passed to the data quality management process  which in turn sends this information to the various telemetry monitor processes. if the data quality is determined to be very poor  the reporting of anomalies is partially suppressed  as explained below  since the telemetry that led to the anomaly diagnosis is probably not reliable. 
　　our example begins with the arrival of telemetry from the ccs subsystem which indicates an abnormally high computer event count. the ccs event count is incremented each time an event is initiated by the spacecraft computer. one type of event is fault-protection  which attempts to automatically correct a fault that has been detected  or protect against harmful consequences of such a fault. thus  an abnormally high event count could indicate entry into fault protection sequences. the ccs telemetry monitor compares the telemetry event count to the predicted event count and finds that they are not equal. since this is an anomaly  the monitor passes the event count to the ccs knowledge agent for further analysis. the ccs agent finds that the telemetry event count exceeds the expected event count by 1. a difference of 1 in the event count may indicate that a 'heartbeat failure  has occurred on the spacecraft. the ccs   heartbeat'' is a signal  called a  power code   sent every ten seconds from the aacs subsystem to the ccs subsystem on board the spacecraft. if the signal is received at the end of the expected time interval  the ccs spacecraft subsystem assumes that the aacs subsystem is functioning normally. if on the other hand the ccs fails to receive the heartbeat signal twice in a single hour   heartbeat failure    the ccs assumes that the aacs has failed in some way  and it issues a series of commands to switch to redundant back-up components in the aacs  in an effort to correct the problem. 
　　however  a difference of 1 between the actual event count and expected event count is not enough evidence in itself to conclude that a heartbeat failure has occurred. there may have been other events not related to the heartbeat that happened to increase the event count by 1. furthermore  there is no way to confirm the occurrence of a heartbeat failure from any of the ccs telemetry. the ccs agent knows that a complete diagnosis of the problem is beyond its domain and so in this case it passes on the heartbeat failure evidence to the higher level system level agent for further analysis and possible confirmation by other agents. 
　　like the subsystem agents  the system level agent is data-driven. upon receipt of the message from the ccs agent  the system level agent asserts a fact into its local knowledge base indicating that a possible heartbeat failure was detected by the ccs. this fact matches half of the antecedent of a data-driven rule in the system level kb  but this is not sufficient to fire the rule. the heartbeat failure anomaly can be confirmed by diagnostic rules in the aacs agent  but at this point no other messages have been received 
	schwuttke and quan 	1 

at the system level  so nothing is reported to the user. the system level returns control to the telemetry monitor process. 
　　the next telemetry to arrive is a status word from the aacs subsystem. the aacs telemetry monitor compares the telemetry status word to the expected status word value and finds that they are not equal. it then passes the status word to the aacs knowledge agent. the agent analyzes the status word bits and determines that several aacs components have been swapped off-line and their redundant backup units have been activated. based on this pattern of events it concludes that a possible heartbeat failure has occurred. but this information by itself is not enough to be certain that a heartbeat failure has actually taken place. the aacs agent knows that a complete diagnosis of the problem is beyond its domain and will require information from one or more other agents. so it sends a message to the system level agent notifying it of the possible heartbeat failure. 
　　when the system level agent receives the heartbeat failure message from the aacs agent  it asserts a fact into its local knowledge base indicating that a possible heartbeat failure was detected by the aacs. at this point the previously asserted fact from the ccs agent combined with the new fact from the aacs agent match the complete antecedent of a data-driven rule in the system level knowledge base  and the rule fires. the consequent of the rule causes an anomaly message to be sent to the subsystem monitors that were involved in detecting the anomaly  for display to the user. however  before this message is displayed  the subsystem monitor checks the current data quality as determined by the telecom agent. if the data quality is in the range of marginal to error free  the monitor displays both the anomaly message and the data quality level in the output window  and turns the output window color to red. if on the other hand the data quality is poor  meaning excessively noisy  then the telemetry that led to the anomaly diagnosis was probably corrupted during transmission  and the resulting conclusion is probably incorrect. in the latter case the anomaly message is still output to the user  but the alarm is not sounded and the output window color is not changed. in addition  the data quality is displayed along with the anomaly message so that the user is informed that the anomaly diagnosis was probably due to data corruption. 
　　this example illustrates the cooperation and communication between four different knowledge agents in a hierarchical organization. information from all the agents is required in order to provide a complete diagnosis of the anomaly condition. these agents illustrate the principles outlined in section 1. each agent is a data-driven diagnostician responsible for a constrained non-overlapping domain. each of the subsystem agents has meta-knowledge that allows it to identify symptoms that may indicate problems beyond its domain  and it reports these symptoms to a higher level agent for cooperative multi-agent analysis. 
distributed ai 
1. preliminary results 
the distributed architecture described in this paper has been applied to the marvel system for real-time spacecraft diagnostics. it has been recently developed  as a follow-on to a uniprocessor version that could accommodate only one of the three subsystem agents on any one installation. preliminary tests have demonstrated that the distributed system can process up to 1 telemetry values per minute. individual subsystem agents can successfully diagnose anywhere from 1 to 1 anomalies per minute  depending on the complexity of reasoning that is required. the system level agent can process up to 1 anomalies per minute. for anomalies that require analysis from multiple agents  e.g.  heartbeat failure   the maximum number of anomalies that can be processed in a given period of time is equal to the speed of the slowest agent involved in the analysis  assuming all agents execute concurrently   plus approximately 1 second for system level inferencing. this is well within acceptable limits for real life mission operations demands. 
1. conclusions 
the marvel distributed architecture demonstrates the successful implementation of multiple cooperating agents in a complex real-time diagnostic system. we have designed an architecture that facilitates concurrent and cooperative processing by multiple agents in a hierarchical organization. these agents adhere to the concepts of data-driven embedded diagnosis  constrained but complete non-overlapping domains  meta-knowledge of global consequences of anomalous data  hierarchical reporting of problems that extend beyond an agent's domain  and shared responsibility for problems that overlap domains. 
　　the marvel architecture is simple and well suited for real-time telemetry analysis. conventional processing is used wherever possible in order to facilitate performance. the knowledge-based agents are embedded within the algorithmic code  and are invoked only when necessary for diagnostic reasoning. distribution of telemetry monitoring and agent processes across workstations provides significant improvement in performance. these qualities allow for efficient real-time diagnosis of anomalies occurring in a complex system. 
1. 