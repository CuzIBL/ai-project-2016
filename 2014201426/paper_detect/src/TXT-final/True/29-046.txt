 
　we take a new look at one of the fundamental properties of discrete time associative memory and show how it can be adapted for natural language processing  nlp . many tasks in nlp could benefit from such associative functionality particularly those which are traditionally regarded as being context driven such as word sense disambiguation. 
　the results describe the typical time to convergence of a hopfield network when trained on patterns representing sentences from a large corpus. through numerical simulation we estimate the time order of convergence and compare this to previous findings for randomly generated  unbiased and uncorrected patterns. 
1 	introduction 
linear associative memories have been well studied in fields such as statistical physics  bruce et a/.  1  amit  1  tanaka and yamada  1   and biophysics  hopfield  1; 1  for their ability to store and recall a set of patterns robustly  even when the patterns to be recalled can only be presented to the networks in a highly corrupted form. it is this robustness in the face of noise and an ability to recall stored patterns based on associations that makes these memories so potentially valuable for natural language processing  nlp   in particular tasks which have traditionally been regarded as context driven such as word sense disambiguation. 
   in this paper we take a new look at one of the fundamental properties of a discrete time  discrete state hopfield network. we explore its time to convergence which has previously been investigated for the storage and recall of randomly generated patterns. clearly  any patterns derived from natural language sentences will not be random  but rather will reflect a complex underlying linguistic distribution. evidence from using random  correlated and biased patterns suggests that the network's 
    currently visiting fellow at research & development center  toshiba corporation  1 komukai toshiba-cho  kawasaki 1 japan. e-mail: nigel eel.rdc.toshiba.co.jp 
1 	neural networks 
storage limit will change. we want to investigate how other properties of the network might also be effected. 
　this will be a worthwhile undertaking if we can adapt associative memory for nlp and decouple the time order of complexity from the complexity of the linguistic structures in the stored data we have gone some way to forming a more efficient device for language processing than is currently available. 
1 	the model 
the model we are looking at in this paper is a simple onelayered discrete state hopfield network  hopfield  1 . the network is essentially autoassociative in nature  but can be coupled into layers  e.g.  tanaka and yamada  1   to simulate more complex functions  or adapted into a boltzmann machine for better generalisation performance. the results presented in this paper should act as a basis for trying to understand these more complex systems when presented with non-random biased pattern sets. 
the hopfield network consists of a set of n units vi 
 i = 1 ..  n  each of which may take either of two values  1 or 1. the activation of each unit is defined by a simple threshold function 
		 1  
　where u represents a set of thresholds and h is defined as 
		 1  
　inputs to unit i come internally from all the other units in the network and externally from /  which is a constant input in our network set at the start of processing. 
　processing takes place by randomly and asynchronously updating units according to eqn.  1  until eqn.  1  converges  indicating that a stable state has been reached. 

　this is one of koiran et a/'s  koiran  1  'interesting questions' for future investigation  and any evidence for an upper bound on the convergence time should be interesting across a number of fields. 
1 	previous work 
given the importance of the class of associative network models it is surprising to find that most emphasis has been placed on the storage capacity and less notice has been taken of the convergence time. consequently there 
seem to be many unanswered questions which merit investigation. 
　previous analysis of the time order complexity of the discrete hopfield network for randomly generated patterns has most recently   tanaka and yamada  
1   given us an upper bound for successful recall of 1 log n    where n is the number of units in the network. there are at least two remaining unknowns: what is the time order complexity for a failed recall  and is recall time the same for correlated  biased  non-random patterns  a secondary question relates to the factor of scale in tanaka and yamada's simulations which were carried out using a range of tv from 1 to 1. it would be nice to confirm their results over a range of networks to see if there are any finite size effects  as well as for those effects which come from different pattern types. 
　let us first define what we mean by recti// time. a recall will be called successful if the network converges to a stable attractor which has a vector representation which is the same as a pattern in the training set for all bits in the pattern. a partial recall or a failed recall will be recorded otherwise. although other researchers have used measures such as mean cpu. time to convergence  we have defined recall time to be the number of bit flips required for the network to settle into a stable state. this should provide a hardware independent measure. 
　in the case of the single layer hopfield network convergence to a single stable state is guaranteed  whereas in the case of multi-layered networks we would expect the network to settle into a limit cycle of length 1 or 1. in this paper we will confine our investigation to the single layered network  but we are interested in measuring what may be termed the 'transient period' from initial network state to stability. 
　the great advantage of knowing that a failed recall will take much longer than a successful recall is of course a saving in processing time and a reduction in pattern recognition error. we can simply abandon a recall attempt as a failure if its processing time exceeds our expectation for a network of a given size. 
　in an early paper  bruce et a/.  1  report that flow times to convergence were found to depend on a number of factors the most important of which was the storage ratio a. of secondary importance was the system size n and the particular updating schedule. 
1 	training 
we train the network using the localist hebb rule of 
eqn.  1  to obtain the weight matrix t which encodes the knowledge about the patterns to be stored. since artificial pattern construction would not give us the rich characteristics of natural language  the training patterns are generated from a corpus of 'real' texts. in our simulations this was a corpus of english and japanese texts 
	collier 	1 
taken from newspaper editorials. each of the english content words has been sense tagged with a unique label denoting its sense. this was undertaken by finding the japanese lexical translation of the english words in the corresponding japanese sentence in the corpus. the resulting corpus can be considered to be sense tagged. 
   from this sense tagged corpus of 1 sentences we extracted a number of subcorpora and used these as our training sets. sentence-to-pattern construction takes place as follows: a lexicon of length n is generated for each corpus from the constituent words giving each word sense a unique index. for each sentence we generate a bit vector of length n where an occurrence of a word in the sentence is shown by the bit being set to 1 at the position the word occupies in the lexicon. the set of n patterns thus generated    is given to eqn.  1  to generate the weight matrix t. we see that each node in the network therefore corresponds to one word in the corpus lexicon a localist representation. looked at another way we are generating a high dimensional symbolic feature space in which the words which constitute the sentences are the features. 
   this encoding  while not optimal in terms of storage capacity  does allow us to compare our results with a wealth of results from fields such as statistical physics. in terms of representational adequacy for nlp  we see that the encoding does not capture either constituency or sequential relations between words. what it does capture is simple cooccurrence in sentences between word pairs which is a minimum required in our view for the modelling of contextual knowledge of language. if this is found to be inadequate the representation could be bought closer to the ideal language model by  for example  having a non-symmetrical weight matrix in which tij would be interpreted as  i followed by j  and hence capturing sequence knowledge. 
　all of the basic parameters which define the matrices generated from each of the subcorpora tr1 to tr1 are shown in table 1. we see that n ranges from 1 to 1  which should show us any finite size effects and covers substantially the same range used in the simulations of  tanaka and yamada  1 . we also see that the minimum value for the storage ratio a in t r l is above the critical level of 1 predicted for unbiased random patterns. it was seen in  collier  1  that this is not necessarily a problem because n is finite  thus giving us continuous degradation in storage rather than the catastrophic discontinuous effect predicted for infinite n. moreover  we also know that for biased systems of training patterns the value of the critical storage ratio ac has been observed to shift from 1 upwards to 1. 
　the bias constant a for each corpus of patterns is calculated from the probability that a bit in a pattern will 
be set to 1 as follows 
1 	neural networks 

in our simulations we follow the work of  amit  1  and try to relate the mean perfect recall of patterns from the training set to the mean convergence rate of the network. to make the results more interesting we randomly corrupt the test patterns using a level of noise m1 which ranges from 1 to 1 in increments of 1  so that the probability of a bit in the test pattern being flipped from 1 to 1 is equal to the noise level. noise was applied only to the 1 bits as most of the information is contained here. we can expect convergence times to increase with mo as the number of bits which need to be reset increase. 
　to calculate the fraction of error in recall we have looked at the distance between the actual stable state chosen by the network for a pattern    and the nominated stable state in autoassociation tests. we can measure the fractional hamming distance between 
	 1  
　in order to calculate fj we therefore measure the fraction of error free recalls for an ensamble of test patterns taken from the training set  over a large number of trials. 
　convergence times were estimated from the mean number of bit flips required for the network to reach a stable state. the number of bit flips is the number of network updates which result in an output unit changing state from 1 to 1 or from 1 to 1. this quantity was also calculated over the same trials as noise m1 was increased from 1 to 1 in 1 increments. 
　f1 and mean convergence times were estimated numerically for 1 test patterns taken from the training figure 1: mean fraction of patterns recalled with no error fj against initial pattern noise mo for 1 bits. 
sets tr1 to tr1  shown in table 1  over 1 trials. this was then repeated for each level of noise mo- unlike previous simulations we cannot easily 'manufacture' training patterns  so we are limited to the values of a which the corpora give us. 
results for the mean error free fraction fj given in 
figure 1 show us that pattern sets with storage ratios less than 1 have good recall while those above this level show continuous degrading recall. as  collier  1  reported  this leads us to put the critical storage ratio ac somewhere around 1. in this respect tr1 to tr1 have similar perforance and tr1 to tr1 show degrading performance as a increases. 
　looking now at mean spin flip to convergence results in figure 1 we see that the training sets which performed well in recall  trl to tr1  have similar convergence times. clearly tr1 to tr1 take much longer on average to converge to a stable state. if we take a closer look at trl to tr1 in figure 1 we see that in fact trl to tr1 have almost the same convergence times and tr1 is slightly greater  confirming again that tr1 has degraded performance. 
　this result seems to agree with  bruce et a/.  1 's conclusion that a is a factor in convergence times. we can say that our simulations have shown that where  then convergence times will be governed by the amount of induced noise  mo  and the number of 1 bits 
	collier 	1 


figure 1: mean number of bit flips to network convergence flips against initial pattern noise m1. t r l : x   


figure 1: mean number of bit flips to network convergence flips against initial pattern noise m 1 . t r l : x   

in the pattern  shown by w h e n c o n vergence will be much greater than this  as observed by  tanaka and yamada  1   and is related to both the size of the system n  the difference of the storage ratio from the critical value  and the amount of induced noise mo. 
1 	conclusion 
estimating the convergence time order for successful and failed recall using single spin flips is an approach which is clearly limited in scope. the results need confirming analytically and also for other natural language training sets. nevertheless our results do have something important to say about convergence times for linear associative networks of the hopfield type. 
　we have seen that the predicted convergence rate of 1 log n   was not observed in our training sets  possibly because of correlations between patterns and bias  despite using a similar range of system sizes for n as 
 tanaka and yamada  1 . the earlier observation by  bruce et a/.  1  of the convergence rate being linked to the storage ratio a was observed and appears to be the major factor. when recall fails we have more complex behaviour with convergence times being governed by a number of factors of which system size and the storage ratio are clearly dominant. 
acknowledgements 
i would like to thank professor j. tsujii for his many helpful and critical suggestions on the computational linguistic aspects of this work. i also gratefully acknowledge the kind permission of asahi newspapers of japan to use their editorial corpus. funding was provided by the economics and social research council in the uk award no. r1. 
