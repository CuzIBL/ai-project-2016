 
this poster describes methods to enable intelligent access to multimodal information streams. we illustrate these methods in two integrated systems: the broadcast news editor  bne  which incorporates image  speech  and language processing and the broadcast news navigator  bnn  which provides search  visualization and personalized access to broadcast news video. bnn enables users to perform keyword and named entity search  temporally and geospatially visualize entities and stories  cluster stories  discover entity relations  and obtain personalized multimedia summaries. by transforming access from sequential to direct search and providing hierarchical hyperlinked summaries  bne and bnn enable users to access topics and entity news clusters nearly three times as fast as direct search of video. 
1. intelligent news on demand 
figure 1 illustrates the bne and bnn systems that integrate components from mitre  oracle  carnegie mellon university and lincoln laboratory to process imagery  audio  and text to enable news on demand. key elements of bne include scene change detection and classification from imagery  silence and speaker change detection from audio  and named entity extraction from speech or closed caption transcripts. a correlation component extracts cues from across these streams to detect the start and end of broadcasts  commercials  and stories. subsequently  stories are classified and summarized  including selection of key frames  named entities  and representative sentences. finally  the user can search and explore video stories  provide relevancy feedback  specify media preferences  and obtain personalized displays using the bnn web browser as shown in figure 1 below. as detailed in  maybury 1   bne and bnn transform video access from sequential to direct search  providing novel navigation and discovery mechanisms such as topical and entity-specific news clusters. empirical evaluations have shown that users can find stories and answer questions nearly three times as fast as searching digital video with no loss in precision and recall. 

figure 1. multistream broadcast news understanding: system architecture 
1. intelligent multimodal segmentation 
bnn exploits text  audio  and imagery streams and associated cues to detect story and commercial segment boundaries and to select media elements to use for summaries and multimodal displays. underlying bnn is a set of machinelearned  time-enhanced finite state automata modeling news structure that take into account the above cues and probabilistic  temporal models of event occurrence  boykin and merlino 1 . program start/end  anchor/report shots  commercials  and/or story shifts are inferred from cross media cues in text  e.g.  frequent weather or sports terms  funding and/or copyright notices   discourse cues  e.g.   coming up next    music  e.g.  characteristic jingles   silence  indicating breaks to commercial   and visual cues  e.g.  logos  anchor vs. reporter shots . for example  as shown in figure 1  our frequency analysis of news anchor sign on terms  e.g.   hello    welcome   or sign off terms  e.g.   that's all    thanks for watching   occurring minutes from program start enables the creation of pattern detectors for such events. we have discovered analyzing months worth of news  for example  that sign off* terms occur in 1% of news programs. 

poster papers 	1 

figure 1. text and named entity search menus  
 story skim  results and  story detail  results 
figure 1 illustrates a user visualizing named entity frequencies and animating story occurrences associated with geospatial regions over time enabling information discovery. 

1. search  retrieval  and visualization 
after bne segments stories with multimodal cues  bnn supports the retrieval of stories by source  date range  keyword  named entity  or topic query. as exemplified in the upper left of figure 1  after selecting news programs  e.g.  cnn newsnight  abc world news tonight  and indicating date ranges  e.g.  februrary 1  1   a user types in keywords in the text box. if a user is unfamiliar with retrieval terms  they can select from an alphabetic listing of all the named entities extracted for the time period and from the programs of interest  such as shown in the person  organization  and location lists in the upper left of figure 1  note  hans blix  is highlighted . this retrieves 1 matching stories and displays them as a  story skim   the source and date  the top 1 named entities in each story  and a representative key frame from each selected segment. finally  by selecting a story  the user is presented with  story details   including all named entities  a text summary  and access to video and transcript sources. in empirical studies this mix of extracted media and hypertext organization enables analysts to perform searches three times faster than with video source. 

figure 1. news visualization 
acknowledgments 
this research involved collaboration among stanley 
boykin  andy merlino  warren grieff  and chad 
mchenry  chris clifton  john aberdeen  john burger  david day  lynette hirschman  and rod holland. 
