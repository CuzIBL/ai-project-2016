 
classification methods from statistical pattern recognition  neural nets  and machine learning were applied to four real-world data sets. each of these data sets has been previously analyzed and reported in the statistical  medical  or machine learning literature. the data sets are characterized by statisucal uncertainty; there is no completely accurate solution to these problems. training and testing or resampling techniques are used to estimate the true error rates of the classification methods. detailed attention is given to the analysis of performance of the neural nets using back propagation. for these problems  which have relatively few hypotheses and features  the machine learning procedures for rule induction or tree induction clearly performed best.1 
1 introduction 
many decision-making problems fall into the general category of classification  clancey  1  weiss and kulikowski  1  james  1 . diagnostic decision making is a typical example. empirical learning techniques for classification span roughly two categories: statistical pattern recognition  duda and hart  1  fukunaga  1  including neural nets  mcclelland and rumelhart  1   and machine learning techniques for induction of decision trees or production rules. while a method from either category is usually applicable to the same problem  the two categories of procedures can differ radically in their underlying models and the final format of their solution. both approaches to  supervised  learning can be used to classify a sample pattern  example  into a specific class. however  a rule-based or decision tree approach offers a modularized  clearly explained format for a decision  and is compatible with a human's reasoning procedures and expert system knowledge bases. 
　statistical pattern recognition is a relatively mature field. pattern recognition methods have been studied for many years  and the theory is highly developed  duda and hart  1 fukunaga  1 . in recent years  there has been a surge in interest in newer models of classification  specifically methods from machine learning and neural nets. 
　methods of induction of decision trees from empirical data have been studied by researchers in both artificial intelligence and statistics. quinlan's 1  quinlan  1  and c1  quinlan  1a  procedures for induction of decision trees are well known in the machine learning community. the classification and regression trees cart   breiman  friedman  olshen  and stone  
 1  procedure is a major nonparametric classification technique that was developed by statisticians during the same period as id1. production rules are related to decision trees; each path in a decision tree can be considered a 
this research was supported in part by onr contract n1-
k-1 and nih grant p1-rr1. 
distinct production rule. unlike decision trees  a disjunctive set of production rules need not be mutually exclusive. among the principal techniques of induction of production rules from empirical data are michalski s aq1 system  michalski  mozetic  hong  and lavrac  1  and recent work by quinlan in deriving production rules from a collection of decision trees  quinlan  1b . 
　neural net research activity has increased dramatically following many reports of successful classification using hidden units and the back propagation learning technique. this is an area where researchers are still exploring learning methods  and the theory is evolving. 
　researchers from all these fields have all explored similar problems using different classification models. occasionally  some classical discriminant methods arecited in comparison with results for a newer technique such as a comparison of neural nets with nearest neighbor techniques. in this paper  we report on results of an extensive comparison of classification methods on the same data sets. because of the recent heightened interest in neural nets  and in particular the back propagation method  we present a more detailed analysis of the performance of this method. we selected problems that are typical of many applications that deal with uncertainty  for example medical applications. in such problems  such as determining who will survive cancer  there is no completely accurate answer. in addition  we may have a relatively small data set. an analysis of each of the data sets that we examined has been previously published in the literature. 
1 methods 
we are given a data set consisting of patterns of features and correct classifications. this data set is assumed to be a 
random sample from some larger population  and the task is 
to classify new patterns correctly. tne performance of each method is measured by its error rate  if unlimited cases for training and testing are available  the error rate can readily be obtained as the error rate on the test cases. because we have far fewer cases  we must use resampling techniques for estimating error rates. these are described in the next section.1 
1. estimating error rates 
it is well known that the apparent error rate of a classifier on all the training cases1 can lead to highly misleading and 

usually over-optimistic estimates of performance  duda and hart  1 . this is due to overspecialization of the 
classifier to the data.1 
　techniques for estimating error rates have been widely studied in the statistics  efron  1  and pattern recognition  duda and hart  1 fukunaga  1  literature. the simplest technique for  honestly' estimating error rates  the holdout or h method  is a single train and test experiment. the sample cases are broken into two groups of cases: a training group and a test group. the classifier is independently derived from the training cases  and the error estimate is the performance of the classifier on the test cases. a single random partition of train and test cases can be somewhat misleading. the estimated size of the test sample needed for a 1%  confidence interval is described in  highleyman  1 . with 1 independent test cases  one can be virtually certain that the error rate on the test cases is very close to the true error rate. 
　instead of relying on a single train and test experiment  multiple random test and train experiments can be performed. for each random train and test partition  a new classifier is derived. the estimated error rate is the average of the error rates for classifiers derived for the independently and randomly generated partitions. random resampling can produce better error estimates than a single train and test partition. 
	a 	special 	case 	of 	resampling 	is 	known 	as 
leaving-one-out  fukunaga  1  efron  1 . leavingone-out is an elegant and straightforward technique for estimating classifier error rates. because it is computationally expensive  it is often reserved for relatively small samples. for a given method and sample size n  a classifier is generated using n-1 cases and tested on the remaining case. this is repeated n times  each time designing a classifier by leaving-one-out. each case is used as a test case and  each time nearly all the cases are used to design a classifier. the error rate is the number of errors on the single test cases divided by n. 
　evidence for the superiority of the leaving-one-out approach is well-documented  lachenbruch and mickey  1  efron  1 . while leaving-one-out is a preferred technique  with large samples it may be computationally expensive. however as the sample size grows  traditional train and test methods improve tneir accuracy in estimating error  kanal and chandrasekaran  1 . 
　the leaving-one-out error technique is a special case of the general class of cross validation error estimation methods  stone  1 . in k-fold cross validation  the cases are randomly divided into k mutually exclusive test partitions of approximately equal size. the cases not found in each test partition are independently used for training  and the resulting classifier is tested on the corresponding test partition. the average error rates over all k partitions is the cross-validated error rate. the cart procedure was extensively tested with varying numbers of partitions and 1-fold cross validation seemed to be adequate and accurate  particularly for large samples where leaving-one-out is computationally expensive ibreiman  friedman  olshen  and stone  1 for small samples  bootstrapping  a method for resampling with replacement  has shown much promise as a low variance estimator for classifiers  efron  1  jain  dubes  and chen  1  crawford  1 . this is an area of active research in applied statistics. 
figure 1 compares the techniques of error estimation for a 
　1in the extreme  a classifier can be constructed that simply consists of all patterns in the given sample. assuming identical patterns do not belong to different classes  this yields perfect classification on the sample cases. 
　1empirical results also support the stratification of cases in the train and test sets to approximate the percentage  prevalence  of each class in the overall sample. 
1 	machine learning 
sample of n cases. the estimated error rate is the average of the error rates over the number of iterations. while these error estimation techniques were known and published in the 1s and early 1s  the increase in computational speeds of computers  makes them much more viable today for larger samples and more complex classification techniques  steen  1 . 

figure 1: comparison of techniques for estimating error rates 
　besides improved error estimates  there are a number of significant advantages to resampling. the goal of separating a sample of cases into a training set and testing set is to help design a classifier with a minimum error rate. with a single train and test partition  too few cases in the training group can lead to the design of a poor classifier  while too few test cases can lead to erroneous error estimates. leaving-oneout  and to a lesser extent random resampling  allow for accurate estimates of error rates while training on most cases. for purposes of comparison of classifiers and methods  resampling provides an added advantage. using the same data  researchers can readily duplicate analysis conditions and compare published error estimates with new results. using only a single random train and test partition introduces the possibility of variability of partitions to explain the divergence from a published result. 
1. classification methods 
in this section  the specific classification methods used in the comparison will be described. we do not review the methods or their mathematics  but rather state the conditions under which thev were applied. references to all methods are readily available. our goal is to apply each of these methods to the same data sets and report the results. 
1.1. statistical pattern recognition 
several classical pattern recognition methods were used. figure 1 lists these methods. these methods are wellknown and will not be discussed in detail. the reader is referred to  duda and hart  1  for further details. instead  we give the specific variation of the method that we used. 

simplifies the normality assumption to equal covariance matrices. this is probably the most commonly used form of discriminant analysis; we used the canned sas and imsl programs. a recent report has demonstrated improved results in game playing evaluation functions using the quadratic classifier  lee  1 . 
we used the nearest neighbor method  k=l  with the 
euclidean distance metric. this is one of the simplest methods conceptually  and is commonly cited as a basis of comparison with other methods. it is often used in casebased reasoning  waltz  1 . 
　bayes rule is the optimal presentation of minimum error classification. all classification methods can be viewed as approximations to bayes optimal classifiers. because the bayes optimal classifier requires complete probability data for all dependencies in its invocation  tor real problems this would be impossible. as with other methods  simplifying assumptions are made. the usual simplification is to assume conditional independence of observations. while one can point to dozens of classifiers that have been built 
 particularly in medical applications  szolovits and pauker  1   using bayes rule with independence  such approaches have also been recently reported in the ai literature  although in the context of unsupervised learning   cheeseman  1 . although independence is commonly assumed  there are mathematical expansions to incorporate higher order correlations among the observations. in our experiments  we tried both bayes with independence and bayes with the second order bahadur expansion.1.1. neural nets 
a fully connected neural net with a single hidden layer was considered. the back propagation procedure  mcclelland and rumelhart  1  was employed and the general outline of the data analysis described in  gorman  1  was followed. the specific implementation used was  mcclelland and rumelhart  1 .1 in most experiments a learning rate of 1 and a momentum of 1 was used.1 patterns were presented randomly to the learning system.1. 
the analysis model of  gorman  1  corresponds to a 
1-fold cross validation. unlike the other methods examined in this study  back propagation usually commences with the network weights in a random state. thus  even with sequential presentation of cases  the weights for one learned network are unlikely to match the same network that starts in a different random state. there is also the possibility of the procedure reaching a local maximum. in this analysis model  for each train and test experiment  the weights are learned 1 times  and test results averaged over all 1 experiments. therefore  1 times the usual number of training trials must be considered. for a 1-fold cross-validation  1 learning experiments are made. 
　for each data set  these experiments were repeated for networks having 1 1 1  or 1 hidden units  in a single layer . this is equivalent to using resampling to estimate the appropriate numoer of hidden units. because the data sets may not be separable with these numbers of hidden units  we took the following measures to determine a sufficient amount of computation time. before doing the train and test experiments  the nets were trained several times on all samples for all size hidden units. we determined a number of epochs  i.e. complete presentations of the data set  that was sufficient to result in each increment of additional hidden units fitting the cases better than the lesser number of hidden units. in addition  for one problem where the data set was extremely large  we sampled the results every 1 epochs  and computed whether the average total squared error continued to be reduced. this indicated whether progress was being made. 
　one output unit was used for each class. the hypothesis with the highest weight was selected as the conclusion of the classifier  and the error rate was computed. 
this is the general outline of the procedures followed. in 
section 1  we describe the variations on this theme that were necessary for the specific data set analyses. 
　for computational reasons  in some instances it was necessary to reduce the number of repeated trials to be averaged. for back propagation  we described a computational procedure that performed 1 train and test experiments for each one that would be necessary for other methods. however  the data sets described in section 1 are not readily separable. thus  the computation demands are quite large. we estimate that 1 months of sun 1 cpu time were expended to compute the neural nets results in section 1. 
1.1. machine learning methods 
in this category  we place methods that produce logistic solutions. as indicated earlier these methods have been explored by both the machine learning and statistics community. these are methods that produce solutions posed as production rules or decision trees. conjunction or disjunction may be used as well as logical comparison operators on continuous variables such as greater than or less than. 
predictive value maximization  weiss  galen  and 
tadepalli  1  was tried on all data sets. this is a heuristic search procedure that attempts to find the best single rule in disjunctive normal form. it can be viewed as a 
heuristic approximation to exhaustive search. it is applicable to problems where a relatively short rule provides a good solution. for such problems  it should have an advantage in that many combinations are considered  in contrast to current decision tree procedures that split nodes without considering combinations. for more complex problems  a decision tree procedure is preferable. the appropriate rule length or tree size is determined by resampling. 
　in addition  for two of the smaller data sets  an exhaustive search was performed for the optimal rule of length 1 in disjunctive normal form. for the other 1 data sets  the published decision tree results are available for methods 
using variations of id1 and its successor c1. 
1 results 
in this section  we review the results of the various classification methods on four data sets. all of the data sets have been published  and in most instances we attempted to perform the analyses in a manner consistent with previously known results. 
1. iris data 
the iris data was used by fisher in his derivation of the linear discriminant function  fisher  1   and it still is the standard discriminant analysis example used in most current statistical routines such as sas or imsl. linear or quadratic discriminants under assumptions of normality perform extremely well on this data set. three classes of iris are discriminated using 1 continuous features. the data set consists of 1 cases  1 for each class. figure 1 summarizes the results. the first error rate is the apparent error rate on all cases; the second error rate is the leaving-
	weiss and kapouleas 	1 


1 	machine learning 

1. cancer data 
a data set for evaluating the prognosis of breast cancer recurrence was analyzed by michalski's aq1 rule induction program and reported in  michalski  mozetic  hong  and lavrac  1 . they reported a 1% accuracy rate tor expert physicians  and a 1% rate for aq1  and a 
1% rate tor the pruned tree procedure of assistant  kononenko  bratko  and roskar  1   a descendant of id1 the authors derived the error rates by randomly resampling 1 times using a 1% train and a 1% test partition. 
　tne samples consist of 1 samples  1 tests  and 1 classes. we created 1 randomly sampled data sets with 1% train and a 1% test partitions; each method was tried on each of the four data sets and the results averaged. thus  the experimental results are consistent with the original study. figure 1 summarizes the results. the first error rate is the apparent error rate on the training cases; the second error rate is the error rate on the test cases. 
figure 1: comparative performance on cancer data 
　the rule-based solution has 1 rule with a total of 1 
　variables.1 for the neural nets  the apparent error rate is the average of ten training trials. each testing result is the corresponding average testing result of tne same 1 complete trials.1 the nets were trained for 1 epochs. the best neural net in terms of cross-validated error occurs at 1 hidden units  and is the one listed in figure 1. the relationship between the number of hidden units and the error rates is listed in figure 1. 
1. thyroid data 
quinlan reported on results of his analysis of hypothyroid data in  quinlan  1b   and in greater detail in  quinlan  1a . the problem is to determine whether a patient referred to the clinic is hypothyroid  the most common thyroid problem. in contrast to the previous applications  
relatively large numbers of samples are available. 
the samples consist of 1 cases from the year 1. 
these are the same cases used in the original report and were used for training. the 1 cases from 1 were used as test cases. there are 1  principal  tests  and 1 classes. over 1% of the values are missing because some lab tests were deemed unnecessary. for purposes of comparison of figure 1: neural net error rates for cancer data the methods  these values were filled in with the mean value for the corresponding class. 
　figure 1 summarizes the results.1 the first error rate is the error rate on the 1 training cases; the second error rate is the error rate on the 1 test cases. from a medical perspective  it is known that  based on lab tests  excellent classification can be achieved for diagnosing thyroid dysfunction. for these data  the correct answer stored with each sample is derived from a large rule-based system in use in australia. while most error rates in figure 1 are low  it is important to note that 1% of the total sample represents over 1 people. over 1% of the samples are not hypothyroid. therefore  any acceptable classifier must do significantly better than 1%. 

figure 1: comparative performance on thyroid data 
　the rule-based solution has 1 rules with a total of 1 variables. for the neural nets  the apparent error rate is the best of 1 trials. the nets were trained for 1 epochs. the best neural net in terms of testing error occurs at 1 hidden units. the relationship between tne number of hidden units and the error rates is listed in figure 1. 
	weiss and kapouleas 	1 

figure 1: neural net error rates for thyroid data 
　the cpu times for training a neural net with back propagation on this size data set were great: for 1 hidden units 1 epochs required 1 hours of sun 1 cpu time  while 1 units required 1 hours. in figure 1  the apparent error rates for the larger numbers of hidden units support the hypothesis that additional training was necessary. we initiated a new set of experiments with fewer numbers of hidden units.1 we let these trials run for an unlimited period of time as long as slight progress was being made  as indicated by sampling every 1 epochs. therefore  for this experiment not every size neural net was run an equal number of epochs. figure 1 summarizes the results of this effort the best result encountered during the sampling of results occurred for 1 hidden units  and this result is listed in figure 1. 

figure i i : extended neural network training on thyroid data 
1 discussion 
the applications presented here represent a reasonable cross section of prototypical problems widely encountered in the many research communities. each problem has few classes and is characterized by uncertainty of classification. in some applications such as the cancer data  the features were relatively weak and good predictive capabilities are unlikely. in others  such as the thyroid data  the features are quite strong  and almost error-free prediction is possible. 
for the smaller data sets  resampling was used. with over 
1 cases  resampling techniques such as cross-validation should give excellent estimates for the true error rate. in 
   the momentum was changed to .1  and the learning rate to .1. to help prevent local maximums. 
1 	machine learning 
fact  the data from the iris study has been reviewed over many years  and comparisons have been made on the basis of the leaving-onc-out error. it is interesting to note  for those who wish to avoid concepts such multivariate distributions and covariance matrices   that a trivial set of 1 rules with a total of 1 variables can produce equal results. 
　for many application fields  this in fact is a major advantage of the logistic approaches  i.e. the rule based or decision tree based approaches. the solution is compatible with elementary human reasoning and explanations. it is also compatible with rule-based systems. thus  if everything were equal  many would choose the logistic solution. 
　in our experiments  everything was not equal. in every case a logistic solution was found that exceeded the performance of solutions posed using different underlying models. pvm has an advantage when a short rule works  but for more complex problems the decision tree would be indicated. we note that the largest problem studied  the thyroid application  is somewhat biased towards logistic solutions. the endpoints were derived from a rule-based system that apparently uses the same lab test thresholds to specify high or low readings for all hypotheses. 
　these results cannot necessarily be extrapolated to more complex problems. however  our experience is not unique. 
numerous experiments by the developers of cart  breiman  friedman  olshen  and stone  1  demonstrated that in most instances  they found a tree superior to alternative statistical classification techniques. 
　in our experiments  the statistical classifiers performed consistently with expectations. the linear classifiers  with the assumption of a normal distribution  gave good performance in all cases except the thyroid experiment. these classifiers are widely used  because they are simple and the training error rate usually holds up well on lest cases. the natural extension  the quadratic classifier  fits better to normally distributed data  but degrades rapidly with nonnormal data. it did poorly in most of our experiments. similarly bayes with independence does moderately well  but the 1nd order fits were not good on the test data. nearest neighbor does well with good features  but tends to degrade with many poor features. there are many alternative statistical classifiers that might be tried  such nonparametric piecewise linear classifiers  foroutan and sklansky  1 . in addition  one could try to reduce the number of features for training  i.e. feature selection   since many of these methods can actually improve 
performance on test cases by feature reduction.1 
　the neural nets did perform well  and they were the only statistical classifiers to do well on the thyroid problem. however  overall they were not the best classifiers; they consumed enormous amounts of cpu time; and they were sometimes equaled by simple classifiers. research on improving performance for neural nets training and representation is quite active  so it may be possible that performance can be improved. 
　the relationship between the number of hidden units and the two error rates followed the classical pattern for classifiers. as the number of hidden units increased  the apparent error decreased.1 however  at some point  as the classifier overfits the data  the true error rate curve flattens and even begins to increase. much the same behavior can be observed for decision trees as the number of nodes increases  or production rules  as the rule length increases. 

　the question remains open as to how well any classifier can do on more complex problems with many more features and many more classes  possibly non-mutually exclusive classes. there are also questions of how many cases are actually needed to learn significant concepts. our study does not answer many of these questions  but helps show in a limited fashion where we are currently with many commonly used classification techniques. 
appendix: induced rules 
  iris. petal length   1 -  iris setosa; petal length   1 or petal width   1-  iris virginica 
  appendicitis. mnea 1 or mbap 1 
  cancer. involved nodes 1 & degree=1 
  thyroid. tsh 1 & fti  1 -  primary hypothyroid; tsh 1 & tt1 & on thyroxin=false & fti 1 & surgery=false 
　　　-  compensated hypothyroid 