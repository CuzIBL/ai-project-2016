
linear discriminant analysis  lda  is a popular data-analytic tool for studying the class relationship between data points. a major disadvantage of lda is that it fails to discover the local geometrical structure of the data manifold. in this paper  we introduce a novel linear algorithm for discriminant analysis  called locality sensitive discriminant analysis  lsda . when there is no sufficient training samples  local structure is generally more importantthan globalstructure for discriminantanalysis. by discovering the local manifold structure  lsda finds a projection which maximizes the margin between data points from different classes at each local area. specifically  the data points are mapped into a subspace in which the nearby points with the same label are close to each other while the nearbypoints with differentlabels are far apart. experiments carried out on several standard face databases show a clear improvement over the results of lda-based recognition.
1 introduction
practical algorithms in supervised machine learning degrade in performance  prediction accuracy  when faced with many features that are not necessary for predicting the desired output. an important question in the fields of machine learning  knowledge discovery  computer vision and pattern recognition is how to extract a small number of good features. a common way to attempt to resolve this problem is to use dimensionality reduction techniques. two of the most popular techniques for this purpose are principal component analysis  pca  and linear discriminant analysis  lda   duda et al.  1 .
　pca is an unsupervised method. it aims to project the data along the direction of maximal variance. lda is supervised. it searches for the project axes on which the data points of different classes are far from each other while requiring data points of the same class to be close to each other. both of them are spectral methods  i.e.  methods based on eigenvalue decomposition of either the covariance matrix for pca or the scatter matrices  within-class scatter matrix and betweenclass scatter matrix  for lda. intrinsically  these methods try to estimate the global statistics  i.e. mean and covariance. they may fail when there is no sufficient number of samples. moreover  both pca and lda effectively see only the euclidean structure. they fail to discover the underlying structure  if the data lives on or close to a submanifold of the ambient space.
　recently there has been a lot of interest in geometrically motivated approaches to data analysis in high dimensional spaces. examples include isoamp  tenenbaum et al.  1   laplacian eigenmap  belkin and niyogi  1   locally linear embedding  roweis and saul  1 . these methods have been shown to be effective in discovering the geometrical structure of the underlying manifold. however  they are unsupervised in nature and fail to discover the discriminant structure in the data. in the meantime  manifold based semi-supervised learning has attracted considerable attention  zhou et al.  1    belkin et al.  1 . these methods make use of both labeled and unlabeled samples. the labeled samples are used to discover the discriminant structure  while the unlabeled samples are used to discover the geometrical structure. when there is a large amount of unlabeled samples available  these methods may outperform traditional supervised learning algorithms such as support vector machines and regression  belkin et al.  1 . however  in some applications such as face recognition  the unlabeled samples may not be available  thus these semi-supervised learning methods can not be applied.
in this paper  we introduce a novel supervised dimensionality reduction algorithm  called locality sensitive discriminant analysis  that exploits the geometry of the data manifold. we first construct a nearest neighbor graph to model the local geometrical structure of the underlying manifold. this graph is then split into within-class graph and between-class graph by using the class labels. in this way  the geometrical and discriminant structure of the data manifold can be accurately characterized by these two graphs. using the notion of graph laplacian  chung  1   we can find a linear transformation matrix which maps the data points to a subspace. this linear transformation optimally preserves the local neighborhood information  as well as discriminant information. specifically  at each local neighborhood  the margin between data points from different classes is maximized.
　the paper is structured as follows: in section 1  we provide a brief review of linear discriminant analysis. the locality sensitive discriminant analysis  lsda  algorithm is introduced in section 1. in section 1  we describe how to perform lsda in reproducing kernel hilbert space  rkhs  which gives rise to kernel lsda. the experimental results are presented in section 1. finally  we provide some concluding remarks in section 1.
1 related works
the generic problem of linear dimensionality reduction is the following. given a set x1 x1 ，，，  xm in rn  find a transformation matrix a =  a1 ，，，  ad  （ rn〜d that maps these m points to a set of points y  such that yi  represents  xi  where yi = atxi.
　linear discriminant analysis  lda  seeks directions that are efficient for discrimination. suppose these data points belong to c classes and each point is associated with a label l xi  （ {1 ，，，  c}. the objective function of lda is as follows:
atsba
	aopt = argmax atswa	 1 
a
		 1 
		 1 
where μ is the total sample mean vector  mi is the number of samples in the i-th class  μi is the average vector of the i-th class  and xij is the j-th sample in the i-th class. we call sw the within-class scatter matrix and sb the between-class scatter matrix. the basis functions of lda are the eigenvectors of the following generalized eigen-problem associated with the largest eigenvalues:
	sba = λswa	 1 
clearly  lda aims to preserve the global class relationship between data points  while it fails to discover the intrinsic local geometrical structure of the data manifold. in many real world applications such as face recognition  there may not be sufficient training samples. in this case  it may not be able to accurately estimate the global structure and the local structure becomes more important.
　besides lda  there is recently a lot of interests in graph based linear dimensionality reduction. the typical algorithms includes locality preserving projections  lpp   he and niyogi  1    local discriminant embedding  lde   chen et al.  1    marginal fisher analysis  mfa   yan et al.  1    etc. lpp uses one graph to model the geometrical structure in the data. lde and mfa are essentially the same. both of them uses two graphs to model the discriminant structure in the data. however  these two algorithms implicitly consider that the within-class and between-class relations are equally important. this reduces the flexibility of the algorithms.
1 locality sensitive discriminant analysis
in this section  we introduce our locality sensitive discriminant analysis algorithm which respects both discriminant and geometrical structure in the data. we begin with a description of the locality sensitive discriminant objective function.
1 the locality sensitive discriminant objective function for dimensionality reduction
as we described previously  naturally occurring data may be generated by structured systems with possibly much fewer degrees of freedom than the ambient dimension would suggest. thus we consider the case when the data lives on or close to a submanifold of the ambient space. one hopes then to estimate geometrical and discriminant properties of the submanifold from random points lying on this unknown submanifold. in this paper  we consider the particular question of maximizing local margin between different classes.
   given m data points {x1 x1 ，，，  xm}   rn sampled from the underlying submanifold m  one can build a nearest neighbor graph g to model the local geometrical structure of m. for each data point xi  we find its k nearest neighbors and put an edge between xi and its neighbors. let be the set of its k nearest neighbors. thus  the weight matrix of g can be defined as follows: ij = 1  otherwise.if xi （ n xj  or xj （ n xi   1 
the nearest neighbor graph g with weight matrix w characterizes the local geometry of the data manifold. it has been frequently used in manifold based learning techniques  such as  belkin and niyogi  1    tenenbaum et al.  1    roweis and saul  1    he and niyogi  1 . however  this graph fails to discover the discriminant structure in the data.
　in order to discover both geometrical and discriminant structure of the data manifold  we construct two graphs  i.e. within-class graph gw and between-class graph gb. let l xi  be the class label of xi. for each data point xi  the set n xi  can be naturally split into two subsets  nb xi  and nw xj . nw xi  contains the neighbors sharing the same label with xi  while nb xi  contains the neighbors having different labels. specifically 


	 a 	 b 	 c 	 d 
figure 1:  a  the center point has five neighbors. the points with the same color and shape belong to the same class.  b  the within-class graph connects nearby points with the same label.  c  the between-class graph connects nearby points with different labels.  d  after locality sensitive discriminant analysis  the margin between different classes is maximized.clearly  nb xi  ” nw xi  =   and nb xi  “ nw xi  = n xi . let ww and wb be the weight matrices of gw and
gb  respectively. we define:
	 	if xi （ nb xj  or xj （ nb xi 
 1  1 	otherwise.
if xi （ nw xj  or xj （ nw xi 
 1  1 	otherwise.
it is clear to see w = wb + ww and the nearest neighbor graph g can be thought of as a combination of within-class graph gw and between-class graph gb.
　now consider the problem of mapping the within-class graph and between-class graph to a line so that connected points of gw stay as close together as possible while connected points of gb stay as distant as possible. let y =  y1 y1 ，，，  ym t be such a map. a reasonable criterion for choosing a  good  map is to optimize the following two objective functions:
 1 
 1 
under appropriate constraints. the objective function  1  on within-class graph incurs a heavy penalty if neighboring points xi and xj are mapped far apart while they are actually in the same class. likewise  the objective function  1  on between-class graph incurs a heavy penalty if neighboring points xi and xj are mapped close together while they actually belong to different classes. therefore  minimizing  1  is an attempt to ensure that if xi and xj are close and sharing the same label then yi and yj are close as well. also  maximizing  1  is an attempt to ensure that if xi and xj are close but have different labels then yi and yj are far apart. the learning procedure is illustrated in figure 1.
1 optimal linear embedding
in this subsection  we describe our locality sensitive discriminant analysis algorithm which solves the objective functions  1  and  1 . suppose a is a projection vector  that is  yt = atx  where x =  x1 ，，，  xm  is a n〜m matrix.
by simple algebra formulation  the objective function  1  can be reduced to
a
= atxdwxta   atxwwxta
where dw is a diagonal matrix; its entries are column  or row  since ww is symmetric  sum of.
similarly  the objective function  1  can be reduced to

= atx db   wb xta
= atxlbxta
where db is a diagonal matrix; its entries are column  or row 
since wb is symmetric  sum of
db   wb is the laplacian matrix of gb.
　note that  the matrix dw provides a natural measure on the data points. if dw ii is large  then it implies that the class containing xi has a high density around xi. therefore  the bigger the value of dw ii is  the more  important  is xi. therefore  we impose a constraint as follows:
ytdwy = 1   atxdwxta = 1
thus  the objective function  1  becomes the following:
	min 1   atxwwxta	 1 
a
or equivalently 
	max atxwwxta	 1 
a
and the objective function  1  can be rewritten as follows:
	max atxlbxta	 1 
a
finally  the optimization problem reduces to finding:
	arg max	aa	 1 
a
atxdwxta=1
where α is a suitable constant and 1 ＋ α ＋ 1. the projection vector a that minimizes  1  is given by the maximum eigenvalue solution to the generalized eigenvalue problem:
	a	 1 
let the column vector a1 a1 ，，，  ad be the solutions of equation  1   ordered according to their eigenvalues  λ1   ，，，   λd. thus  the embedding is as follows:
xi ★ yi = atxi
　　　　　　　　a =  a1 a1 ，，，  ad  where yi is a d-dimensional vector  and a is a n 〜 d matrix.
　note that  if the number of samples  m  is less than the number of features  n   then rank x  ＋ m. conse-
quently  
. the fact that xdwxt and x αlb +  1 α ww xt are n〜n matrices implies that both of them are singular. in this case  one may first apply principal component analysis to remove the components corresponding to zero eigenvalues.
1 kernel lsda
lsda is a linear algorithm. it may fail to discover the intrinsic geometry when the data manifold is highly nonlinear. in this section  we discussion how to perform lsda in reproducing kernel hilbert space  rkhs   which gives rise to kernel lsda.
　suppose x = {x1 x1 ，，，  xm} （ x is the training sample set. we consider the problemin a feature space f induced by some nonlinear mapping
φ : x ★ f
for a proper chosen φ  an inner product can be defined on f which makes for a so-called reproducing kernel hilbert space  rkhs . more specifically 

holds where k . .  is a positive semi-definite kernel function. several popular kernel functions are: gaussian ker-
; polynomial kernel
; sigmoid kernel k x y 	=
　given a set of vectors {vi （ f|i = 1 ，，，  d} which are orthonormal   δi j   the projection of φ xi  （ f to these v1 ，，，  vd leads to a mapping from x to euclidean space rd through

we look for such {vi （ f|i = 1 ，，，  d} that helps {yi|i = 1 ，，，  m} preserve local geometrical and discriminant structure of the data manifold. a typical scenario is
x = rn f = rθ with d    n   θ.
let Φ denote the data matrix in rkhs:
Φ =  φ x1  φ x1  ，，，  φ xm  
now  the eigenvector problem in rkhs can be written as follows:
	v	 1 
　because the eigenvector of  1  are linear combinations of φ x1  φ x1  ，，，  φ xm   there exist coefficients αi i = 1 ，，，  m such that v
where α =  α1 α1 ，，，  αm t （ rm.
following some algebraic formulations  we get:
v

where k is the kernel matrix  kij = k xi xj . let the column vectors α1 α1 ，，，  αm be the solutions of equation  1 . for a test point x  we compute projections onto the eigenvectors vk according to

where αki is the ith element of the vector αk. for the original training points  the map can be obtained by y = kα  where the ith element of y is the one-dimensional representation of xi.
1 experimental results
in this section  we investigate the use of lsda on face recognition. we compare our proposed algorithm with eigenface  pca   turk and pentland  1    fisherface  lda   belhumeur et al.  1   and marginal fisher analysis  mfa   yan et al.  1  . we begin with a brief discussion about data preparation.
1 data preparation

figure 1: sample face images from the yale database. for each subject  there are 1 face images under different lighting conditions with facial expression.
table 1: recognition accuracy of different algorithms on the yale database
method1 train1 train1 train1 trainbaseline1% 1 1% 1 1% 1 1% 1 eigenfaces1% 1 1% 1 1% 1 1% 1 fisherfaces1% 1 1% 1 1% 1 1% 1 mfa1% 1 1% 1 1% 1 1% 1 lsda1% 1 1% 1 1% 1 1% 1 two face databases were tested. the first one is the yale database1  and the second one is the orl database1. in all the experiments  preprocessing to locate the faces was applied. original images were normalized  in scale and orientation  such that the two eyes were aligned at the same position. then  the facial areas were cropped into the final image for matching. the size of each cropped image in all the experiments is 1 〜 1 pixels  with 1 gray levels per pixel. thus  each image can be represented by a 1-dimensional vector in image space. no further preprocessing is done. different pattern classifiers have been applied for face recognition  including nearest neighbor  turk and pentland  1   bayesian  moghaddam  1   and support vector machines  phillips  1   etc. in this paper  we apply nearest neighbor classifier for its simplicity. in our experiments  the number of nearest neighbors  k  is taken to be 1. the parameter α is estimated by leave one out cross validation.
　in short  the recognition process has three steps. first  we calculate the face subspace from the training set of face images; then the new face image to be identified is projected into d-dimensional subspace; finally  the new face image is identified by nearest neighbor classifier.
1 face recognition on yale database
the yale face database is constructed at the yale center for computational vision and control. it contains 1 grayscale images of 1 individuals. the images demonstrate variations in lighting condition  left-light  center-light  right-light   facial expression  normal  happy  sad  sleepy  surprised  and wink   and with/without glasses. figure 1 shows some sample images of one individual.
　for each individual  l = 1 1  images were randomly selected as training samples  and the rest were used for testing. the training set was used to learn a face subspace using the lsda  eigenface  and fisherface methods. recognition was then performed in the subspaces. we repeated this process 1 times and calculate the average recognition rate. in general  the recognition rates varies with the dimension of the face subspace. the best performance obtained by these algorithms as well as the corresponding dimensionality of the optimal subspace are shown in table 1. for the baseline method  we simply performed face recognition in the original 1-dimensional image space. note that  the upper bound of the dimensionality of fisherface is c   1 where c is the number of individuals  duda et al.  1 .
　as can be seen  our algorithm outperformed all other three methods. the eigenface method performs the worst in all cases. it does not obtain any improvement over the baseline method. it would be interesting to note that  when there are only two training samples for each individual  the best performance of fisherface is no longer obtained in a c   1 = 1  dimensionalsubspace  but a 1-dimensionalsubspace. lsda reaches the best performance almost always at c   1 dimensions. this property shows that lsda does not suffer from the problem of dimensionality estimation which is a crucial problem for most of the subspace learning based face recognition methods.
1 face recognition on orl database
the orl  olivetti research laboratory  face database is used in this test. it consists of a total of 1 face images  of a total of 1 people  1 samples per person . the images were captured at different times and have different variations including expressions  open or closed eyes  smiling or nonsmiling  and facial details  glasses or no glasses . the images were taken with a tolerancefor some tilting and rotationof the face up to 1 degrees. 1 sample images of one individual are displayed in figure 1. for each individual  l = 1 1  images are randomly selected for training and the rest are used for testing.
　the experimental design is the same as before. for each givenl  we average the results over1 randomsplits. the best result obtained in the optimal subspace and the corresponding dimensionality for each method are shown in table 1.
　as can be seen  our lsda algorithm performed the best for all the cases. the fisherface method performed comparatively to lsda as the size of the training set increases. moreover  the optimal dimensionality obtained by lsda and fisherface is much lower than that obtained by eigenface.
1 discussion
several experiments on two standard face databases have been systematically performed. these experiments have revealed a number of interesting points:
1. all the three algorithms  lsda  mfa  and fisherface  performedbetter in the optimal face subspace than in the original image space. this indicates that dimensionality reduction can discover the intrinsic structure of the face manifold and hence improve the recognition rate.
1. in all the experiments  our lsda algorithm consistentlyoutperformed the eigenface  fisherface and mfa methods. especially when the size of the training set is small  lsda significantly outperformed fisherface. this is probably due to the fact that fisherface fails to accurately estimate the within-class scatter matrix from only a small number of training samples.

figure 1: sample face images from the orl database. for each subject  there are 1 face images with differentfacial expression and details.
table 1: recognition accuracy of different algorithms on the orl database
method1 train1 train1 train1 trainbaseline1% 1 1% 1 1% 1 1% 1 eigenfaces1% 1 1% 1 1% 1 1% 1 fisherfaces1% 1 1% 1 1% 1 1% 1 mfa1% 1 1% 1 1% 1 1% 1 lsda1% 1 1% 1 1% 1 1% 1 1. eigenface fails to gain improvement over the baseline.this is probably because that eigneface does not encode the discriminating information.
1. in all the experiments  the optimal dimensionality obtained by lsda is always c-1  where c is the number of classes. in practice  when the computational complexity is a major concern  one can simply project the face images into a c-1 dimensional subspace.
1 conclusion
we have introduced a novel linear dimensionality reduction algorithm called locality sensitive discriminant analysis  lsda . for the class of spectrally based dimensionality reduction techniques  it optimizes a fundamentally different criterion compared to classical dimensionality reduction approaches based on fisher's criterion lda  or principal component analysis. the most prominent property of lsda is the complete preservation of both discriminant and local geometrical structure in the data. for lda  on the other hand  it can only preserve the global discriminant structure  while the local geometrical structure is ignored. we have applied our algorithm to face recognition. experiments on yale and orl databases have been conducted to demonstrate the effectiveness of our algorithm.
