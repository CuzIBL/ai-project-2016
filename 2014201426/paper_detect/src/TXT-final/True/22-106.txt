 
in many applications in mobile robotics  it is important for a robot to explore its environment in order to construct a representation of space useful for guiding movement. we refer to such a representation as a map  and the process of constructing a map from a set of measurements as map learning. in this paper  we develop a framework for describing map-learning problems in which the measurements taken by the robot are subject to known errors. we investigate two approaches to learning maps under such conditions: one based on valiant's probably approximately correct learning model  and a second based on rivest sz sloan's reliable and probably nearly almost always useful 
learning model. both methods deal with the problem of accumulated error in combining local measurements to make global inferences. in the first approach  the effects of accumulated error are eliminated by the use of reliable and probably useful methods for discerning the local properties of space. in the second  the effects of accumulated error are reduced to acceptable levels by repeated exploration of the area to be learned. finally  we suggest some insights into why certain existing techniques for map learning perform as well as they do. 
1 	introduction 
many of the problems faced by robots navigating in the environment can be facilitated by using expectations in the form of explicit models of objects and the spaces that they occupy. we use the term map to refer to any model of large-scale space used for purposes of navigation. map 
    this work was supported in part by the national science foundation under grant iri-1 and by the advanced research projects agency of the department of defense and was monitored by the air force office of scientific research under contract no. f1-c-1. 
1this work was supported in part by a national science 
foundation presidential young investigator award ccr1 with matching funds from ibm  and by national science foundation research grant ccr-1. 
learning involves exploring the environment  making observations  and then using the observations to construct a map. the construction of useful maps is complicated by the fact that observations involving the position  orientation  and identification of spatially remote objects are invariably error prone. in this paper  we explore a number of problems involved in constructing useful maps from measurements taken with sensors subject to known errors. 
   in previous work  dean  1   we have looked at various optimization problems related to constructing maps  e.g.  construct the most accurate map consistent with a set of measurements . even in cases involving only a single dimension  such optimization problems can turn out to be np-hard  yernini  1 . in this paper  rather than look at problems that involve doing the best with what you have  we consider problems that involve going out and getting what you need to generate useful representations. in particular  we consider a form of reliable and probably almost always useful learning  rivest and sloan  1  in which the robot gathers information to ensure that it nearly always  with probability 1  can provide a guaranteed perfect path from one location to another. a prerequisite to this sort of learning is that the robot  in moving around in its environment  can discern the local properties of space with absolute certainty with high probability having expended an amount of effort polynomial in 1 and n  where n is some measure of the size of the environment. 
   by eliminating local uncertainty  small errors incurred in making local measurements are not allowed to propagate rendering global queries unacceptably inaccurate. in general  local uncertainty accumulates as the product of the distance in generating global estimates. one way to avoid this sort of accumulation is to establish strategies such that the robot can discern properties of its environment with certainty. most existing map learning schemes exploit this sort of certainty in one way or another  see section 1 . the rehearsal strategies of kuipers  are one example of how a robot might plan to eliminate uncertainty. once we have a method for eliminating uncertainty  the problem then reduces to one of planning out and executing the necessary experiments to extract certain information about the environment. 
¡¡in situations in which it is not possible to eliminate local uncertainty completely  it is still possible to reduce 
	basye  dean and vitter 	1 


the effects of accumulated errors to acceptable levels by performing repeated experiments. to support this claim  we describe a map-learning technique based on valiant's probably approximately correct learning model  valiant  1  that  given small 1   1  constructs a map to answer global queries such that the answer provided in response to any given query is correct with probability 1 - 1. the techniques presented apply to a wide range of map-learning problems of which the specific problems addressed in this paper are meant to be merely illustrative. 
1 	spatial representation 
we model the world  for the purposes of studying map learning  as a graph with labels on the edges at each vertex. in practice  a graph will be induced from a set of measurements by identifying a set of distinctive locations in the world  and by noting their connectivity. for example  we might model a city by considering intersections of streets to be distinguished locations  and this will induce a grid-like graph. kuipers  develops a mapping based on locations distinguished by sensed features like those found in buildings  see figure 1 . figure 1 shows a portion of a building and the graph that might be induced from it. levitt  l1  develops a mapping based on locations in the world distinguished by the visibility of landmarks at a distance. 
   in general  different mappings result in graphs with different characteristics  but there are some properties common to most mappings. for example  if the mapping is built for the purpose of navigating on a surface  the graph induced will almost certainly be planar and cyclic. other properties may include regularity or bounded degree. in what follows  we will always assume that the graphs induced are connected and undirected; any other properties will be explicitly noted. 
   following  aleliunas et a/.  1   a graph model consists of a graph  g -  v  e   a set l of labels  and a labeling  ¦µ : {v x e} -  l  where we may assume that l has a null element ¡À which is the label of any pair 
 v e v  e e e  where e is not an edge from v. we will frequently use the word direction to refer to an edge and its associated label from a given vertex. with this notation  we can describe a path in the graph as a sequence of labels indicating the edges to be taken at each vertex. we can describe a procedure to follow as a function from v -  l indicating the preferred direction at each location. 
   if the graph is a regular tessellation  we may assume that the labeling of the edges at each vertex is consistent  i.e.  there is a global scheme for labeling the edges and the labels conform to this scheme at every vertex. for example  in a grid tessellation  it is natural to label the edges at each vertex as north  south  east  and west. in general  we do not require a labeling scheme that is globally consistent. you can think of the labels on edges emanating from a given vertex as local directions. such local directions might correspond to the robot having a compass that is locally consistent but globally inaccurate  or local directions might correspond to locally distinctive features visible from intersections in learning the map of a city. 
   in the following  we identify three sources of uncertainty in map learning. first  there may be uncertainty in the movement of the robot. in particular  the robot may occasionally move in an unintended direction. we refer to this as directional uncertainty  and we model this type of uncertainty by introducing a probabilistic movement function from {v x l} -  v. the intuition behind this function is that for any location  one may specify a desired edge to traverse  and the function gives the location reached when the move is executed. for example  if g is a grid with the labeling given above  and we associate the vertices of g with points  i  j  in the plane  we might define a movement function as follows: 

where the  . . .  indicate the distribution governing movement in the other three directions. the probabilities associated with each direction sum to  . if all directions are equally likely regardless of the intended direction  then the movement function is said to be random. throughout this paper  we will assume that movement in the intended direction takes place with probability better than chance. 
   a second source of uncertainty involves sensors  and in particular recognizing locations that have been seen before. the robot's sensors have some error  and this 

can cause error in the recognition of places previously visited; the robot might either fail to recognize some previously visited location  or it might err by mistaking some new location for one seen in the past. we refer to this type of uncertainty as recognition uncertainty  and model it by partitioning the set of vertices into equivalence classes. we assume that the robot is unable to distinguish between elements of a given class using only its sensors. 
   a third source of error involves another manifestation of sensor error. in representing the world using a graph  some mapping must be established from a set of distinguished locations in the world to v. error in the sensors could cause the robot to fail to notice a distinguished location some of the time. for example  a robot taxi might use intersections as distinguished locations  leading to a grid-like graph. but if sensor error causes the robot not to notice that he is passing through an intersection  his map will become flawed. in exploring an office environment  the point in a hallway in front of a door may correspond to a vertex in the induced graph. if the door is closed  there is some chance that the robot will not recognize the vertex in traversing the hall. we model this type of uncertainty by introducing a probabilistic movement function that can skip over vertices. we refer to this type of movement function as discontinuous and to the type of uncertainty modeled as continuity uncertainty. 
   apparently  the three types of uncertainty described above are orthogonal in the sense that none implies or precludes the others. the issues involved in modeling and reasoning about continuity uncertainty are complex and will not be treated further in this paper. in the following  we are concerned with directional and recognition uncertainty. 
1 	map learning 
for our purposes  a map is a data structure that facilitates queries concerning connectivity  both local and global. answers to queries involving global connectivity will generally rely on information concerning local connectivity  and hence we regard the fundamental unit of information to be a connection between two nearby locations  i.e.  an edge between two vertices in the induced undirected graph . we say that a graph has been learned completely if for every location we know all of its neighbors and the directions in which they lie  i.e.  we know every triple of the form  u  /  v  where u and v are vertices and / is the label at u of an edge in g from u to v . we assume that the information used to construct the map will come from exploring the environment  and we identify two different procedures involved in learning maps: exploration and assimilation. exploration involves moving about in the world gathering information  and assimilation involves using that information to construct a useful representation of space. exploration and assimilation are generally handled in parallel  with assimilation performed incrementally as new information becomes available during exploration. in this section  we are concerned with the conditions under which a graph can be completely learned  and how much time will be required for the exploration and assimilation. 
1 	tessellation graphs 
it's not hard to see that any connected  undirected graph can be completely learned easily if there is no uncertainty;  kuipers and byun  1  describes a way of doing this by building up an agenda consisting of unexplored paths leading out of locations and then moving about so as to eventually explore all such paths. nothing about the graph need be known before the exploration begins. introducing the kinds of uncertainty described in section 1 complicates things considerably. if  however  the graph has additional structure  then that structure can often be exploited to eliminate uncertainty. in the following  we sketch a proof that it is possible to efficiently learn maps that correspond to regular tessellations with boundaries. it turns out that the exploration component of learning regular tessellations is quite simple; random walks suffice for polynomial-time performance. in the longer version of this paper  we describe an efficient incremental assimilation procedure that is called whenever the robot encounters a location during exploration  and then prove the following1. 
lemma 1 the assimilation algorithm  provided will learn a finite tessellation completely if the exploration tour traverses every edge in the graph. the overall cost of assimilation is o m  where m is the length of the tour. 
we now have to ensure that during exploration the robot traverses each edge in the graph at least once with high probability. the following two lemmas establish that  for any connected  regular  undirected graph g and any b   1  a random walk of length polynomial in 1/b and the size of g is sufficient for traversing every edge in g with probability j - 1. 
lemma 1 for any d   1  there exists a polynomial p d 1j  of order o dlogj  such that with probability 1-s p visits to a vertex of order d result in traversing all edges out of the vertex at least once. 
lemma 1 for any connected  regular  undirected graph 
g -  v  e  with order d  any s   1  and any m   i  there exists a polynomial p |e|  ra  1/b  such that with probability 1 - s  a random tour on g of length p visits every vertex in v at least m times. 
in most cases  we can do better than random exploration. 
if the robot moves in the direction it is pointing with probability better than chance  then the robot can traverse every edge in the graph with high probability in time linear in the size of the graph. using the above three lemmas it is easy to prove the following. 
theorem 1 any finite regular tessellation g -  v  e  can be reliably  probably almost always usefully learned. 
¡¡the lemmas and form of the proof described above provide a framework for proving that other kinds of graphs can be reliably probably almost always usefully learned in a polynomial number of steps. in general  all 
1
¡¡¡¡to meet the submission length requirements  all proofs have been omitted. the longer version of the paper  including all proofs basye et al.  1   is available upon request. 
	basye  dean and vitter 	1 
we require is that a polynomial number of visits to every vertex provides enough information to learn the graph. perhaps  the most important lesson to extract from this exercise is that the effects of multiplicative error in learning maps of large-scale space can be eliminated if there is a reliable method for eliminating local uncertainty that works with high probability. the above approach to map learning was inspired by rivest's model of learning  rivest and sloan  1   in which complex problems are broken down into simple subproblems that can be learned independently. in order to learn a useful representation of the global structure of its environment  it is sufficient that a robot have reliable and usually effective methods for sensing the local structure of its environment and a method for composing the local structure to generate an accurate global structure. the sensing methods need not always provide useful answers; they need only guarantee that the answer returned is not wrong. the problem then becomes largely one of determining a sequence of sensing and movement tasks that will provide useful answers with high probability. there are situations  however  in which reliable sensing methods are not available  and it is still possible to learn useful maps of large-scale space. 
1 	general graphs 
the next problem we look at involves both recognition and directional uncertainty with general undirected graphs. we show that a form of valiant's probably approximately correct learning is possible when applied to learning maps. in this section  we consider the case in which movement in the intended direction takes place with probability better than chance  and that  upon entering a vertex  the robot knows with certainty the local name of the edge upon which it entered. we call the latter requirement reverse movement certainty. results for related models are summarized in the next section. 
   at any point in time  the robot is facing in a direction defined by the label of a particular edge/vertex pair-the vertex being the location of the robot and the edge being one of the edges emanating from that vertex. we assume that the robot can turn to face in the direction of any of the edges emanating from the robot's location. we also assume that upon entering a vertex the robot can determine with certainty the direction in which it entered. directional uncertainty arises when the robot attempts to move in the direction it is pointing. let 1   1 be the probability that the robot moves in the direction it is currently pointing. more than 1% of the time  the robot ends up at the other end of the edge defining its current direction  but some percentage of the time it ends up at the other end of some other edge emanating from its starting vertex. while the robot won't know that it has ended up at some unintended location  it will know the direction to follow in trying to return to its previous location. 
¡¡to model recognition uncertainty  we assume that the vertices v are partitioned into two sets  the distinguishable vertices d and the indistinguishable vertices i. we are able to distinguish only vertices in d. we refer to the vertices in d as landmarks and to the graph as a landmark graph. we define the landmark distribution parameter'  r  to be the maximum distance from any vertex in i to its nearest landmark  if r = 1  then i is empty and all vertices are landmarks . we say that a procedure learns the local connectivity within radius r of some v e d if it can provide the shortest path between v and any other vertex in d within a radius r of v. we say that a procedure learns the global connectivity of a 
¡¡graph g within a constant factor if  for any two vertices u and v in d  it can provide a path between u and v whose length is within a constant factor of the length of the shortest path between u and v in g. 
¡¡we begin by showing that the multiplicative error incurred in trying to answer global path queries can be kept low if the local error can be kept low  that the transition from a local uncertainty measure to a global uncertainty measure does not increase the complexity by more than a polynomial factor  and that it is possible to build a procedure that directs exploration and map building so as to answer global path queries that are accurate and within a small constant factor of optimal with high probability. 
l e m m a 1 let g be a landmark graph with distribution parameter r  and let c be some integer   1. given a procedure that  for any s1   1  learns the local connectivity within cr of any landmark in g in time polynomial in 1- with probability 1 - s1  there is a procedure that learns the global connectivity of g with probability 1 - sg for any sg   1 in time polynomial in 1/s- and the size of the graph. any global path returned as a result will be at most c/c-1 times the length of the optimal path. 
the procedure presented in the proof of lemma 1 searches outward from a vertex v e d to a distance cr  and then uses the edges found while entering vertices on the outward path to attempt to return to v. the directions used on the way out form an expectation for the labels observed on the way back. when these expectations are not met  the traversal is said to have failed  and the procedure tries again. the procedure keeps track of the edge/vertex labels associated with vertices visited during exploration in order to ensure that it explores all paths of length cr or less emanating from each vertex in d with high probability. 
   there is a possibility that some combination of movement errors could result in false positive or false negative tests. but we show by exploiting reverse certainty that we can statistically distinguish between the true and false test results. by attempting enough traversals  the procedure can ensure with high probability that the most frequently occurring sets of directions corresponding to perceived traversals actually correspond to paths in g. what is required  then  is for the learning procedure to do enough exploration to identify all paths of length cr or less in g with high probability. 
l e m m a 1 there exists a procedure that  for any s1   1  learns the local connectivity within cr of a vertex in any landmark graph with probability l - 1  in time polynomial in 1/s1  1-1r and the size of g  and exponential in r. 


	basye  dean and vitter 	1 

retrace its steps if it remembers the directions it took at each point during exploration. the authors show that the problem is unsolvable in general  but that by providing the robot with a number of distinct markers  k;   1  the robot can learn the graph in time polynomial in the graphs size. in order to place a marker on a particular vertex  the robot must visit that vertex; in order to recover the marker at later time  the robot must return to the vertex. a vertex with a marker on it acts as a temporary landmark. no assumption is made regarding the planarity of the graph. the problem with a single marker that can be placed once but not recovered is also unsolvable  but  if you allow a compass in addition  the problem can be solved in polynomial time. 
   levitt et a/ describe an approach to spatial reasoning that avoids multiplicative error by introducing local coordinate systems based on landmarks. landmarks correspond to environmental features that can be acquired and  more importantly  reacquired in exploring the environment. given that landmarks can be uniquely identified  one can induce a graph whose vertices correspond to regions of space defined by the landmarks visible in that region. the resulting problem involves neither recognition nor movement uncertainty. our results in section 1 bear directly on any extension of levitt's work that involves either recognition or movement uncertainty. 
1 	conclusion 
this paper examines the role of uncertainty in map learning. we assume an environmental model that provides for a finite set of distinctive locations that can be reliably detected and repeatedly found. under this assumption  the problem of map learning reduces to one of extracting the structure of a graph through a process of exploration in which only small parts of the structure can be sensed at a time and sensing is subject to error. we are particularly interested in showing that cumulative errors in reasoning about the global properties of the environment based on local measurements can be reduced to acceptable levels using a polynomial  in the size of the graph  amount of exploration. the results in this paper shed light on several existing approaches to map learning by showing how they might be extended to handle various types of uncertainty. our basic framework is general enough to be applied to a wide variety of map learning problems. we have identified one particular source of uncertainty  namely continuity uncertainty  see section 1   that we believe of particular interest in learning maps of buildings and other environments possessing an easily discernable structure. 
