 
this paper describes multidimensional neural preference classes and preference moore machines as a principle for integrating different neural and/or symbolic knowledge sources. we relate neural preferences to multidimensional fuzzy set representations. furthermore  we introduce neural preference moore machines and relate traditional symbolic transducers with simple recurrent networks by using neural preference moore machines. finally  we demonstrate how the concepts of preference classes and preference moore machines can be used to integrate knowledge from different neural and/or symbolic machines. we argue that our new concepts for preference moore machines contribute a new potential approach towards general principles of neural symbolic integration. 
1 introduction 
in previous years there has been a fair amount of interest in hybrid and connectionist systems  that is  in systems which integrate symbolic  neural and/or statistical knowledge for solving difficult real-world tasks 
 reilly and sharkey  1; miikkulainen  1; yager  
1; medsker  1; dorffner  1; cleeremans and destrebecqz  1 . much work on hybrid systems has been guided by the particular tasks at hand  dyer  1; honavar and uhr  1; sun and bookman  1  and only little work has concentrated towards more general rigorous models of neural interpretation  for an early exception see  smolensky  1; sharkey and jackson  1  . this lack of fundamental principles of hybrid neural/symbolic integration was also identified at a recent international workshop on hybrid intelligent systems  wermter and sun  1  and this paper addresses this current issue. 
in this paper we want to start with focusing on the integration of simple symbolic machines  fuzzy representations and recurrent neural networks. first  we introduce the general concept of multidimensional neural preferences. then  we relate multidimensional neural pref-
1 	machine learning 
erences to multidimensional fuzzy set representations and show that the corner preference order on preference classes is a partial order. this allows us to rank different neural preferences and provides a basic link between neural preferences and symbolic fuzzy representations at the preference class level. 
then  we introduce neural preference moore machines and relate traditional symbolic transducers with simple recurrent networks by using neural preference moore machines. preference moore machines provide a link between simple recurrent networks and symbolic transducers at the preference moore machine level finally  we demonstrate how the concepts of preference classes and preference moore machines can be used to integrate knowledge from different neural and/or symbolic machines. we introduce operations like intersection and union on preference classes and show that these operations on preference classes are commutative  associative  and monotonic. these operations provide a link between several neural or symbolic modules at the system architecture level. 
1 neural preferences  their order  and their preference value 
typically  artificial neural networks receive analogous input from a number of network units  input layer  and they produce output for a number of network units  output layer . while the actual processing within different networks may be very different  their external interface may be modeled by a general multidimensional preference for input and/or output. 
definition 1  preference  a preference is an analog representation which is represented by an m-dimensional vector  
definition 1  preference mapping  
a preference mapping is a mapping between preferences:  positive integers. 
such a preference mapping could be a transformation of the input or a prediction of the next input based on the current input. if we want to rank preferences according to their strength  we need to specify an order for m-dimensional preferences in  within this tridimensional space  we will consider a preference as being 

large if the values of the individual vector elements are close to 1 or 1. in contrast  we will consider a preference as being small if the values of the individual vector elements are close to 1. this is our goal criterion for determining a partial order on preferences. in general  we define a preference a as being larger than another preference 1  if a has a smaller distance to a reference. 
it is possible to determine multiple references  for instance  the corner references from {1  l}n. these corner references are particularly interesting since they allow a direct symbolic interpretation of preferences for input and output. 
definition 1  corner reference order  if r is a corner reference r = then the 
reference order is based on the distance of two preferences from this corner reference. we call this special 
form of the reference order the corner reference order. 
that is  referring to a corner reference r  a preference a 
is greater than or equal to a preference 1  if the distance of a to r is smaller than or equal to the distance of b to r. the corner reference can be interpreted as a strict  sharp preference. below  we will specify that r a  is the next corner reference with minimal distance to a currently considered preference a. we define in detail: 
definition 1  next corner reference  the next corner reference which is closest to is determined for as: 
we consider an example:  1 1  and  1 1  are comparable and  1 1    1 1   because  1 1  is closer to the next corner reference  1  than  1 1  to  1 . furthermore  it holds that:  1 1    1 1   because the distance of  1 1  to  1  is smaller than the distance of  1 1  to a corner reference. the closer a preference is to a corner reference  the greater the preference. this can be defined more formally by assigning a preference value from the interval  1  to each preference a related to its next corner reference r in the m-dimensional space: 
definition 1  preference value of a preference  let r a  be the next corner reference for a preference a in m-dimensional space. let distance a r a   be the euclidean distance between a andr a . then we define the preference value of a preference a with respect to r a  as: 
     is the maximum distance in m-dimensional space to the next corner reference  that is the distance from the center to the corner references. therefore  the values are between 1 and 1. if a is close to its next corner reference t h e n i s close to 1. 
if a is close to the center reference then is close to 1. 
figure 1 shows the preference values for the twodimensional space. for each two-dimensional preference the corresponding preference value z is shown. in 
general  the value has been given as the preference value of a preference a referring to a reference r a . for instance  a preference value for a categorization would specify how strong a certain category assignment would be. 

figure 1: preference values z of two-dimensional preferences  x y  
1 neural preferences as multidimensional fuzzy set representations 
an m-dimensional preference can be seen as an tridimensional vector of a neural network as well as an m-dimensional fuzzy set. for a neural interpretation  the preference value is a measure of how far away a neural preference is from a discrete symbolic corner vector  which represents the corner reference. for a fuzzy interpretation  the preference value is a measure of how far away a fuzzy set is from the corresponding symbolic sharp set which represents the corner reference. 
	wermter 	1 

for each preference in m-dimensional space  we can specify a preference value in  1 . because of the definition of the corner reference order and the definition of the preference value  only preferences with the same corner reference can be compared. this property is useful  since the preferences  1 1  and  1 1  for the different references  1  and  1  would provide the same preference value pref  1  1 1  and pref 1  1 1 ; however  it cannot be decided whether  1 1  or  1 1  are greater  since these preferences belong to different corner references. it is only possible to compare preferences which have the same corner reference. those preferences which have the same distance to the same corner reference are judged as equal  for instance  1 1  and  1 1   because pref  1  1 1  = pref  1  1 1 . it is not possible to determine which of these preferences is greater and closer to the corner reference  1 . our previous definition of the corner reference order is not yet a partial order. however  a partial order is a minimum requirement for the definition of all fuzzy sets with multi-dimensional goal domains  klir and folger  1 . the corner reference order is already transitive and reflexive  but it is not antisymmetric. for antisym-
metry it must hold: if 	and 	then 
however   1 1   1 1  and  1 1   1 1   but both preferences are different. therefore  we cluster those preferences which belong to the same next corner reference into one class. we want to define the corner reference order based on these classes. 
definition 1  class of preferences  let 	a 	= 
be a preference and  
       is next corner reference. then the class of preferences of a is called c a  and contains all those preferences for next corner reference r a   which have the same distance from r a  as a. 
definition 1  order on preference classes  let 
be two preferences and 
their common next corner reference 	= 
then the corner reference order on classes of preferences is defined as follows:  
here   is the usual order for real numbers and is the distance 
of the preference a from reference r. we say that preference doss c a  is greater than or equal to the preference class c b . 
definition 1  preference value of a class  the preference value of a preference class c a  is the preference value of an arbitrary preference which belongs to this class. 
theorem i the corner reference order for preference classes is a partial ordering. 
sketch of proof: 
let a =  be two preferences with their corresponding preference classes c a  and c b . let r =  be their com-
mon next corner reference. then it is straightforward to show reflexivity  antisymmetry and transitivity for the 
1 	machine learning 
preference classes. 
the corner reference order for classes of preferences is a partial order which meets the particular requirements for a neural interpretation of preferences  multidimensional and uncertain close to 1  but also the general requirements for a fuzzy interpretation of preferences  at least partial order in the goal domain  and also the general requirements of neural and symbolic integration  symbolic corner reference as a reference for classes of neural preferences . the preference value of a class of output preferences of a neural network can be understood as the membership degree of these output preferences for an m-dimensional fuzzy set which represents a reference  for instance a corner reference  in m-dimensional space. figure 1 shows examples of four preference classes which have the same distance to their corresponding corner reference. 

figure 1: classes of preferences in three-dimensional space 
another reason for the use of classes of preferences is based on symbolic processing. if a preference value for  feature 1  feature1  has to be specified  a single value  e.g. 1  can be given. this preference value corresponds to all those preferences which have the same corresponding distance from the specified corner reference. therefore a class of preferences also supports the integration of symbolic and neural representations. a class of preferences represents a high-dimensional hypersphere of an unlimited number of preferences with the same distance from the specified corner reference. 
1 interpretation of dynamic preference mappings 
so far we have concentrated on static preferences and preference classes. our next step is to focus on dynamic preference mappings which can be associated with certain sequential machines. as one possibility for relat-

ing principles of symbolic computational representations ences symbolically and to integrate symbolic preferences and neural representations by means of preferences  we with neural preferences. consider a so-called neural preference moore machine. each preference of a neural trajectory is a representative we have chosen this type of machine since they are of its preference class and it is possible to assign a sym-
simple and widely applicable. 	bolic description as a corner reference together with a 
preference value. in this way  neural preferences can be* 
definition 1  preference moore machine  	interpreted symbolically. on the other hand  symbolic 
a preference moore machine pm is a synchronous se- knowledge can be integrated with neural knowledge by quential machine  which is characterized by a 1-tuple associating a preference value with a symbolic corner refpm = with i  o and s non-empty sets erence. this preference value of the symbolic reference of inputs  outputs and states. is determines which neural preference class is associated the sequential preference mapping and contains the state with the symbolic reference. 

transition function fs and the output function f1. here 
i  o and s are and dimensional preferences with values from and respectively. 
a general version of a preference moore machine is shown to the left of figure 1. the preference moore machine realizes a sequential preference mapping  which uses the current state preference s and the input preference i to assign an output preference o and a new state preference. 
figure 1: neural preference moore machine and its relationship to a simple recurrent neural network 
simple recurrent networks  also called srn   elman  
1  have the potential to learn a sequential preference 
mapping automatically based on in-
put and output examples  see figure 1   while traditional moore machines or fuzzy-sequential-funetions  santos  1  use manual encodings. 
such a simple recurrent neural network constitutes a neural preference moore machine which generates a sequence of output preferences for a sequence of input preferences. here  internal state preferences are used as local memory. a feedforward network represents a neural preference moore machine with a degenerated sequential memory  since there is no possibility to have an influence from previous patterns. 
on the one hand  we can associate a neural preference moore machine in a preference space with its symbolic interpretation. on the other hand  we can represent a 
symbolic transducer in a neural representation. using the symbolic m-dimensional preferences and the corner reference order  it is possible to interpret neural prefer-
1 combination of symbolic/neural preferences 
an integration of symbolic moore machines and neural preference moore machines has a number of advantages. known knowledge can be represented as manually coded symbolic moore machines. unknown knowledge can be learned in neural preference moore machines. symbolic regular relations can be understood as a top-down specification for symbolic moore machines. alternatively  a training set can be viewed as a bottom-up specification for neural preference moore machines. 
1 	a connection via preference classes 
for one training set there can be several different neural preference moore machines which realize the training set and which differ in their connections and weights. symbolic moore machines represent knowledge at a higher discrete abstraction level compared to neural preference moore machines. therefore we want to examine a possible integration of symbolic and neural moore machines. we will consider a single neural or symbolic moore machine as a unit  whose input and output should be integrated. we suggest that a preference class could be a suitable connection between different symbolic and/or neural moore machines  see also section 1 . below we will focus on operations on preference classes. 
1 	operations on preference classes 
notation for preference classes 
let  be a mapping which associates in-
put preferences with output preferences. for preference classes from  we can define the operations for intersection and union. 
let a = 	be two prefer-
ences from with their corresponding preference classes and let be 
preference value of a preference a for a reference r a   similarly this holds for  if it is clear that the reference of a preference is the next corner reference we say p a  rather than and c a  rather than  
for simplicity  we will consider a preference class in a slightly modified compact notation as a pair of reference and preference value: preference class =  reference preference value . for instance    1  1  
	wermter 	1 

is a preference class in the two-dimensional space which contains all preferences which have the preference value 1 for the reference  1 . 

in general  this definition can be described as follows: if the  symbolically interpretable  reference of two preference classes is equal  then the reference will be kept and the union provides the preference class with the larger preference value. if the reference of two preference classes is different then the union is extended to the references. thus  the union provides the preference class with the larger preference value. 

that is  if the  symbolically interpretable  reference of two preference classes is equal  then the reference will be kept and the intersection provides the preference class with the smaller preference value. if the reference of two preference classes is different  then the intersection is extended to the references. thus  the intersection provides the intersected preference class with the smaller preference value. 
1 	relationship of m-dimensional 
preference classes to fuzzy sets 
we will now examine whether our operations for preference classes fulfill axioms which are a basic precondition for a relationship of preference classes to fuzzy sets. the axioms are: 1  generalization of sharp sets  1  commutativity  1  monotonicity  and 1  associativity. the following theorems can be proven: 
theorem 1 let  be the partial ordering for the mdimensional preference space  1  l m. then the union pu on preference classes fulfills the axioms generalization of sharp sets  commutativity  monotonicity  and associativity. 
theorem 1 let  be the partial ordering for the mdimensional preference space  1  l m. then the intersection pi on preference classes fulfills the axioms generalization of sharp sets  commutativity  monotonicity  and associativity. 
1 	machine learning 
now we will illustrate the use of the union and intersection for preference classes. we consider the 1dimensional space. in the following illustration  we refer to a small preference value with ''s  and to a large with  l . then   1   s  is the preference class which contains those preferences which have a preference value s with respect to the reference  1 . 
if the preference classes have the same corner reference  they are directly comparable with their preference values. the preference class pu  r a  p a     r fe  p 1    is the preference class with the largest preference  that is pu   1  s     1   l   =   1   l . on the other hand  
pi  r a  p a     r b  p b    is the preference class with the smallest preference  that is pi   1  s     1   l   =   1   s . 
if the preferences classes have a different corner reference  the preference classes cannot be judged only by their preference value. in this case  the preference classes pu  r a  p a    r b  p b    and pj  r a  p a    r 1  p 1    are a generalization of the standard union and intersection. therefore  it holds that for instance pu   1  1    1  1   =   1   1  but p/   1  1    1  1   =   1  s . this is based on the following motivation: for instance  if there is a preference for  no noun  no verb  and at the same time a preference for  no noun  verb   then pu provides the optimistic integration  namely  no noun  verb  and pi provides the pessimistic integration  namely  no noun no verb . the preference value of the intersection of preference classes is the minimum of the preference values of the arguments  and the preference value of the union of preference classes is the maximum of the preference values of the arguments. 
we have illustrated that the union and intersection on preference classes and fuzzy sets fulfill equivalent axioms. furthermore  fuzzy sets and preference classes represent uncertainty by a fuzzy value and a preference value  respectively. therefore there is a tight relationship between fuzzy sets and preference classes if we interpret them as points in m-dimensional space. 
1 discussion and conclusion 
we have introduced multidimensional neural preference classes and preference moore machines as one general principle for integrating different neural and/or symbolic knowledge sources. for ranking different preferences  we introduced a new reference order and showed that the corner preference order on preference classes is a partial order. this allowed us to rank different neural preferences and provides preference classes as a basic link between neural preferences and symbolic fuzzy representations at the preference level. we introduced neural preference moore machines which provide a link between simple recurrent networks and symbolic transducers at the preference moore machine level finally  we demonstrated how the concepts of preference classes and preference moore machines can be used to integrate knowledge from different neural and/or symbolic machines. 

we used operations like intersection and union on preference classes and proved that these operations on preference classes are commutative  associative and monotonic. these operations provide a link between several neural or symbolic modules at the system architecture level. 
recently  the question as to whether simple recurrent networks can emulate each symbolic moore machine and each finite automaton has been examined  kremer  1 . on the other hand it has been shown  goudreau and giles  1; goudreau et al.  1  that a recurrent network with only one input layer  one context layer and one output layer  so-called single-layer-firstorder-network  is not sufficient for realizing arbitrary finite automata. therefore  the link with our preferences between simple recurrent networks and symbolic moore machines is particularly important. 
here we focused on moore machines because they are relatively simple and efficient. so far  it could be shown that simple recurrent networks can emulate certain restricted properties of a pushdown automaton  in particular the recursive representation of structures up to a limited depth  elman  1; wiles and elman  1 . in the future  more complex machines  like different pushdown automata with explicit unlimited memory  may be further candidates for additional principles of neural symbolic integration. neural preference moore machines like simple recurrent networks can support such properties but can also benefit from the integration with known symbolic heuristics. 
