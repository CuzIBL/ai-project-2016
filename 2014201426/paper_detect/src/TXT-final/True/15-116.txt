 
	this 	paper 	proposes 	an 	alternative 	to 
quinlan's algorithm for forming classification trees from large sets of examples. my algorithm is guaranteed to terminate. quinlan's algorithm is usually faster. 
i. the nature of the problem. 
　　　we have a population of objects which we want to classify into two** groups. we have a set of attributes  each with a snail f i n i t e number of distinct values  and a set of examples whose attributes have been measured and which have already been classified. our goal is to find a rule  based on these examples  which we can use to 
classify other members of the population. 
　　　in general this w i l l lead us to statistical methods such as cluster analysis  everitt  1  kendall  1  sturt  1a  sturt  1b . the larger our collection of examples  the more likely it is that some of them are misclassified.  sturt  1a  provides an excellent illustration of how improving the f i t of a rule to the training set  beyond a certain point  can make it perform worse on the rest of the population. 
　　　even so  there are interesting tasks where the domain is formal  and we can be sure that we 
have a l l the information we need and that our classifications are correct. there are  however  interesting problems where the domain is formal rather than real  and we can be sure that a l l the relevant information is available and our classifications are correct. chess positions and algebraic equations are two such domains. 
　*this work was supported by a commonwealth scholarship. computing was done on the serc dec-1 under grant cr/c/1 
　**id1 and ray algorithm are both presented in terms of two categories. the information 
heuristic generalises to any fixed number of categories  and so do the algorithms. 

1 r. o'keefe 

　*this is where the generalisation to more than the only difference between the concept two categories is made. formation algorithm and radix sort is that the 

sons of a node nay test different attributes  while in a left-to-right radix sort they must test the sane attribute. we adapt the data structure of radix sort  a pair of randomly accessed files  to the abstract algorithn of section 1. this is the only new idea in this paper. as it keeps no examples in memory  if it fails to find a rule  that is a guarantee that the information heuristic will never generate a rule that fits in memory. as it adds no extra choices to the abstract algorithm  it must terminate. 
       we hold the examples in two files  which are always permutations of each other. an example set 
is represented by a triple 
 whichfile  first  next  
where the whichfile field says which 	of 	the 	two files 	the set may be found in  first is the index of the first example in this set  and next is 	the index of the first example following this set. 	an empty 	set is represented by a triple with first ＊ next. 
       selecting an attrb for a node requires one pass over e-set node   where we calculate the counts from which the p are derived. we 
jkc 
initialise the e-sets for the sons as follows: 
set  filel.first next    e-set node . set file1 - o t h e r   f i l e   f i l e l   . set f i r s t - first. for k - 1 .|attrb node | do set e-set son node k   -  file1   first first . 
     set first 	first+nk. od. 
where mk  t.he number of examples with attribute value k  was determined in the first pass. we 
then make one more pass over the data  doing 
for place = first..next-1 do read an example from filel place . set k = the attribute value. 
set  file1 first last  - e-set son node k  . 
write the example to file1 last . 
     set e-set son node k   -  file1  first last+l . od   
as we permute the examples into the same range of records  the e-set labels of all existing nodes remain valid. 
       main memory is needed to hold the counts for attribute selection  less than 1 cells for quinlan's examples  and to hold the rule. if the rule w i l l not f i t in main memory  this algorithm w i l l stop. we are assured that the algorithm could not find any simpler rule. 
r. o'keefe 1 
       this algorithn nay take as many passes over the training set as there are levels in the decision tree  so the total cost of finding a rule is proportional to the product 
 depth of decision tree  x 
 total number of attribute values  x 
 number of examples in training set  
a pass of my algorithm reads a file twice and writes another once. a pass of id1 reads the data f i l e once. when id1 discovers a simple rule  it usually takes fewer passes than the modified radix sort. but the point of the modified radix sort is to cope with any sort of data. i doubt that it 
can be done nore cheaply. 
