 
this paper addresses a problem of induction  generalization learning  which is more difficult than any comparable work in ai. the subject of the present research is a hard problem of new terms  a task of realistic constructive induction. 
     while the approach is quite general  the system is analyzed and tested in an environment of heuristic search 
where noise management and incremental learning are necessary. here constructive induction becomes feature formation from data represented in elementary form. a high-level attribute or feature such as  piece advantage  in checkers is much more abstract than an elementary descriptor or primitive such as contents of a checkerboard square. features have often been used in evaluation functions; primitives are usually too detailed for this. 
     to create abstract features from primitives  i.e. to restructure data descriptions   a new form of clustering is used which involves layering of knowledge and invariance of utility relationships related to data primitives and task goals. the scheme  which is both model- and data-driven  requires little background  domain-specific knowledge  but rather constructs it. the method achieves considerable generality with superior noise management and low computational complexity. although the domains addressed are difficult  initial experimental results are encouraging.1 
	i 	introduction 
a fundamental problem in ai is the automation of inductive inference  1 1 .1 induction can be described as generalization from particular cases  1 1   as learning categorizations from examples or observation  1 1   as intelligent compression of massive data  1 1   or as discovering regular  coherent characterizations of events or objects  1 1 .1 whatever the interpretation  induction begins either with objects or with partially formed groupings  and creates one or more classes. these meaningful sets are also called categories or concepts. a concise representation of 
　　1. this work was supported in part by an operating grant from the natural sciences and engineering research council of canada. 
　　1. the problem of induction is also a basic study outside of ai. it has occupied scholars in philosophy   psychology   pattern recognition  1 1 1   and other fields . 
　　1. formally: given a set s of individual objects  induction is the inference of a larger class or hypothesis t  such that s c t. since t is a generalization it may not be true; associated with t is its credibility  the estimated veracity of the induction. 
knowledge is imperative for reasons of space and time efficiency: in practical application  detailed information is simply too expensive either to store or to acquire  while induction of classes allows prediction of details  1 1 . 
     automated induction is crucial. for example  in an expert system  computer generalization means mechanized knowledge acquisition  which may reduce costs. present expert systems are prone to unexpected error  whereas induction would increase reliability and obviate maintenance. even in its primitive state  automated induction has outperformed knowledge engineering approaches . 
a. views of inductive difficulty 
     unfortunately  induction is inherently difficult  1 1 . noisy and sparsely distributed data offer little help in distinguishing hypothetical classes or concepts  and there are many hypotheses which are plausible  useful  sensible  or credible. 
     inductive multiplicity can be understood in various ways. first  a credible hypothesis may be sought in a space of category descriptions. in this view of induction  search is conducted through a set of expressions which are systematically related to one another to facilitate their explicit formation  such expressions are otherwise only implicit  because of their immense number   1 1 . in this search formalization  exploration of plausible hypotheses must cope with exponential growth of their explicit representations. 
     the most abstract view of the inductive problem involves classification  1 . the number of ways of combining objects into classes is extreme. if a small 1 grid of bits encodes letters of the alphabet  the number of 
different classes is 1 l1   and very few of these are credible. a view of induction which is intuitively close to the classification perspective is the feature space representation  illustrated in fig. 1. 
     a third way of analyzing the difficulty of induction considers the quantitative problem as a qualitative one  1 1 . class formation often requires restructuring of data: objects must be reorganized by transforming description variables. this means a change of knowledge representation  constructive induction  the problem of new terms   which is particularly difficult to automate  1 . fig.l compares constructive induction with the simpler selective induction which needs no reorganization.1 
　　1. by selective induction we mean a case in which one or a 
　　few neighborhoods in feature space can easily be partitioned. michalski's definition of this term  is more precise  but it excludes some cases we would want to call simple  such as classes discriminated by linear decision functions. 
     to structure data  an expressive language such as predicate logic is required  but there are tradeoffs between expressiveness and efficiency  at least with current models .1 greater expressive power causes a worse combinatorial explosion in search. 
b. research directions 
     one approach to the combinatorial problem of induction is to limit hypotheses by imposing constraints on their expression  1 1 . these constraints may take various forms. a straightforward tactic is simply to limit the description language without confining its power too much. for example  many methods permit conjunctions but not disjunctions  1 . another way of restricting candidate descriptions is to use some criterion to narrow search. examples of criteria include  simplicity   quality of  fit  to the data  1   and  invariance  under transformation of task elements . a relaxed form of invariance has long been used in statistics and pattern recognition  viz.  similarity  in cluster analysis . the author's scheme involves a special kind of similarity constraint based on the domain environment. 
the precise form of this similarity has to do with success or 
 utility  in the performance of some task  1 1 .1 overall  constraining search for abstract knowledge has seen only limited success in terms of efficiency  effectiveness  and extensibility  1 1 . 
     generalization algorithms have been designed either for selective induction  1 1 1  or for quite simple constructive induction  1 . the true nature of this ai work has sometimes been obscure because little attention has been given to the usual methods of science: delineation of abstract phenomena  detection of relevant variables  measurement of precise relationships  and development of guiding principles. unified views of inductive systems are scarce  though there are  1 1  . furthermore  ai research often ignores earlier germane results  including  1 . consequently no standard exists for answering important questions such as: how difficult is the inductive task being studied  how much knowledge is acquired autonomously  versus the amount given by the user  similar questions have recently been considered elsewhere  1 . 
c. quantitative analysis 
     while the current state of induction in ai is understandable  the field being new and difficult   the time may have come for a more rigorous approach.  see  1 1  for similar sentiments.  in keeping with this goal  the author has begun to pursue a means for comparing inductive tasks and systems. based on  1   this involves a quantification of inductive difficulty  both for task domains and for learning systems. this attempt began in  1 1 . 
     1. see  for a discussion of representation languages. important issues include equivalences of superficially different representations  and the distinction between languages appropriate for expression of concepts and languages useful for concept formation. the current paper and  examine structures for mediation between data and concepts. 
     1. task-related utility is used as the criterion for clustering in the original probabilistic learning system plsl this system has produced unique results such as convergence to optimal heuristics  1 1 . pls1 can handle noisy environments and incremental learning. the system is efficient. 
	l rendell 	1 
     the idea is simple: since induction creates a class t from a set of objects s  the difficulty of the generalisation task depends on the nature of the information compression from cases s to concept t. 1 first  the more cases t must cover  the harder it is to describe t accurately while differentiating s from negative instances of the concept  see the approximation of conceptual knowledge as amount of information compression in  1  . secondly  if the attributes describing s do not support straightforward selective induction  as in fig. la   but must instead be redefined by constructive induction  fig. lb   then inductive difficulty depends on the kind of reconstruction required. when examined in the light of these measures  many systems  such as  1 1   perform only a moderate amount of induction. in these systems  the total number of possible generalizations is confined to be relatively small from the outset. 
d. 	substantial induction 
     in contrast to these simplified approaches  the current research attempts a very difficult task. not only is the required amount of information compression very large  but  more important  a large degree of constructive induction is needed. in the proposed system  little domain-specific guidance is provided by the user or program. this scheme is conceptually compact and appears tractable. 
     the method is related to a successful system for selective induction plsl 1 one aspect common to pls1 and the new system plso is the observation of a goal-oriented measure  the utility. the utility divergence  a function of  features  of objects  and also based on data patterns  is the criterion for induction. when pls1 uses utility for categorization  conceptual clustering   various efficiencies and other advantages ensue. the scheme has been incremental and insensitive to noise since its inception  1 1 . 
     retaining these valuable and unusual characteristics  plso implements a new form of constructive induction unlike other systems in important respects: although the inductive difficulty is great  little background knowledge is given  the language of concept expression is quite general  the algorithms have low computational complexity  and the approach appears extensible. these claims will be elaborated. 

figure 1. degrees of inductive difficulty. a simple case of clustering successful objects  left  requires only that a few boundaries be inserted. this is selective induction. complex cases may involve varying degrees of heterogeneity  which may be more concisely described using concepts. an example is the class defined by variables f1 and f1 both being even  right . formation of concepts is constructive induction. 
     1. this is a simplification. more than one class t may be involved  or the set of objects s may already be partially formed into categories. for present purposes these details may be ignored without affecting the essence of inductive difficulty. 

1 l. rendell 
ii an example in heuristic search 
this section illustrates some of the above ideas and relates them to a hard problem of constructive induction. we introduce task utility for inductive guidance  and layered abstraction of task knowledge. in later sections  these terms and those in the diagrams will be more carefully defined. 
figure 1. the three level information structure of flso. shown is the construction of a feature f which counts the number of diagonal juxtapositions of two friendly pieces in checkers. this begins with a detailed description of board configurations in terms of 1 primitives e  giving the contents of individual squares.  see fig. 1 for detail.  when subspaces such as e1 and e1 are examined  and utilities are clustered after superposition of subspaces  important structural patterns emerge having common utility descriptors 

 ud's . a pattern class results  level 1 . eventually a class may be generalized using a group of transformations  level 1 . 
figure 1. clustering small samples of primitive subobjects. shown is a projection of the 1 dimensional space of subobjects  checkerboard squares . here the tenth and fourteenth squares are sampled. the possible values for each square are blank or 1  origin   our player's man or 1  p.man   opponent's king or -1  o.king   etc.  for a total of 1 x 1 = 1 coordinates in this two-dimensional subspace. the fractions beside each point  extreme left  indicate the proportion of winning states. the clusters  extreme right  compress this utility information  omitting points with no observations . this rightmost view becomes the least abstract level of fig. 1. 

proper utility class  for an average of about 1 = 1 in each class. in terms of primitives  a logic description of such a class would be highly irregular  involving a combination of an immense number of terms. 
     utility classes are much easier to describe in terms of predefined features. as fig. 1 illustrates  the number of classes at the feature level is very roughly 1 - this is the size of a typical feature space  1  1 . utility bears a smooth relationship to features  so the induction required is merely selective  fig. la   and only a few simple descriptions are needed  fig. 1 .1 in contrast  raw data descriptions in the form of primitive vectors are more detailed  having 1 values as opposed to 1 . moreover  utility-primitive relationships are discontinuous  so the much more difficult constructive induction is needed  see figs. 1 and 1 . 
c. automated feature construction 
     this paper begins to explore a method for discovery of features from primitives  a problem hardly addressed previously  but see  1 1  . conceptually and experimentally  the approach appears tractable and error resilient. a layered   divide and conquer  approach restricts complexity  not just  generating and testing  hypotheses  but rather constructing simple ones from previously validated components. 
l. rendell 1 
     how can utility-primitive relationships be captured and generalized  induction is infeasible without some guidance from regularities  assumed or else discovered . one technique  curve fitting  is often inadequate even in selective induction when features are the starting point  1 1 . a more flexible approach is to record information in feature space cells  as in fig. 1  1 . an important tool for inducing this knowledge is the method of cluttering. 
ffl clustering  a tool for induction 
clustering classifies data so that events or objects are similar within any class  but dissimilar across classes . this technique has been used in successful learning systems by michalski  1 1   and earlier by the author  1 1   who originated a form of conceptual clustering. this kind of clustering takes into account not only feature values  but also forms of concepts  e.g. feature space rectangles  and aspects of the environment  e.g. observed utility . michalski has emphasized concept constraints  and the author has stressed relationships between task domain and inductive algorithms. these are dual aspects: model- and data-drivenness. 
     in this section we describe general characteristics of clustering  and then examine plsl's algorithm for selective induction as a preliminary for plso's more powerful con-


figure 1. levels of abstraction in search. elementary data in the form of primitive descriptions represent fully detailed knowledge  but are massive and infeasible to gather. at the other extreme  maximal compression expresses utility classes concisely. intermediate representations facilitate expression: e.g. features discriminate utility quite smoothly. in contrast  primitive measurements determine utility very irregularly. 
　　1. because most of the  knowledge  resides so regularly in the features  an evaluation function can often be a linear combination of them. see . 
structive induction. 
a. mutual data support 
     clustering has several desirable properties. within ascribed boundaries of a cluster  missing data presumably share characteristics of their neighbors  so the process can be predictive  see figs  la and 1 . once a class is formed  its determining data may be dismissed  so storage and computation can be economical. if statistics is employed  susceptibility to error may be low and credibility may improve as a result of clustering the data. further  the structure used for clustering may concisely describe a concept that emerges automatically as information is compressed  1 . 
     as an example of these characteristics  consider the leftmost rectangle of fig. 1. because of the imposed rectangular shape  the description of this  concept  is simple and easily 
stored:  moreover the associated utility u = 1 may have come from many data: out of perhaps n = 1 objects observed in this rectangle  g = 1 of them may have been  successful . if a few observations were in error  the value of  would still be close to the  true  utility  here a probability . as 

figure 1. a region set is a partition of feature space with associated information. shown are classes  rectangles  r and their values  utilities  u for some task. a region can simply be the pair  r u . in pls1 a feature space region set is used as an evaluation function. in plso a primitive space region set is used to create features. 

1 l. rendell 
long as the utility does not vary too much within the rectangle's boundaries  a case of selective induction   u may relate to any enclosed point  observed or unobserved. as an added benefit  u may be more accurate because it has been measured over many  similar  objects. we call this important coincidence of information compression  concept formation  and accuracy improvement mutual data support . 
b. goal-directed cluttering using utility 
     mutual data support arises in the statistical technique of clustering. in  1   1   the author introduced a special kind of clustering which employs not just attributes or features of an object  but also the quality or utility of that object in the task environment.1 this clustering criterion is central in the family of probabilistic learning systems  pls   1 1 .1 in pls1  the utility u is a probability: the number g of  good  objects contributing to task success  divided by the total number n of observed objects: the utility 
e.g.  the object might be a state in a problem or game  and success might mean appearing in a solution or win. 
c. regions as clusters of similar utility 
   pls1 clusters utility  thereby associating state descriptions of common quality. the cluster or region r is a triple feature space volume r having utility u with error factor  this codes the interval  because utility bears a smooth relationship to typical features  clustering may profitably be constrained as a partition of feature space  the region set  see fig. 1 . the region set is suitable for selective induction  fig. la ; it is a compressed representation of the utility surface in this augmented feature space. 
     characteristics of regions  e.g. rectangle size and shape  are consistent with data encountered: a region set remains small enough to economise resources but large enough to express important knowledge. to accomplish this  pls1 employs a utility revision rule for correcting values of u  a specialization rule for refining cells r  and generalization and reorganization operators for otherwise modifying regions. 
     various unusual or unique advantages result. computation is inexpensive  and more autonomous than samuel's signature table method . the algorithms automatically produce an effect similar to a criterion using  similarity  and  fit   see  1 1  and compare  1  . results include efficient discovery of optimal heuristics. 
d. region refinement and credibility 
　　here we consider in detail the pls1 refinement operator for splitting regions: the algorithm cluster 1 . a region r is dichotomised when utility data within it are found to diverge. the criterion for splitting involves a dissimilarity  distance  measure d. if and are the two utilities for a tentative dichotomy  and and their errors  then 	the dissimilarity d is 
　　1. see anderberg  l pp. 1ff | for discussion of  external criteria  in clustering. 
　　1. the original pls1 has been extended to become a more stable  powerful and efficient learning system but here we refer to both as plsl. 

pairs. for example  the utility of simultaneously positive values of is about equal to the utility of the same condition on  see fig. 1 . a prerequisite for 
mechanizing this inference is some language to express utility in the two dimensional subspace determined by separate from any other coordinate ek. 
a. knowledge level 1: 	subobject relationships 
     to permit this structuring  knowledge level 1 uses projections of the n-dimensional primitive space  here n=1 . a subspace specifier  ss  is a string of length 
our illustrative example f can be expressed using a uniform combination of ss's  ail with  for example  one 
member of the adjacency concept is e1 e1  fig. 1 . 	paired 
with each  is its utility descriptor  ud   a region set expressing utility relationships in this primitive subspace. an ss together with its ud is a primitive pattern. fig. 1 and level 1 of fig. 1 show two primitive patterns  one for the subspace specifier e1  and the other for the ss e1. 
     a primitive pattern is a compressed representation of the function u e   where u is the utility  and e is the full primitive vector. the compression is of two types: projection of primitive space into subspaces  and approximation of u as a step function. the subspaces are indicated as subspace specifiers  ss's   and the step function is expressed as a utility descriptor  a ud is a primitive region set . 
     the purpose of knowledge level 1 is to compress utility relationships concise forms distinguishing striking aspects of objects. consider fig.1 again. the  are 
both meaningful as subobject pairs. these two ss's represent similar structures  one just a translation of the other  so their ud's are somewhat alike. primitive patterns may stand out like this  based on small  biased domain samples. we shall return to small sample effects in section v. 
b. level 1: 	p a t t e r n classes of similar structures 
     knowledge level 1 facilitates search for similar subobject structures: those exhibiting similar utility behavior are merged  eventually to produce a sensible feature. regularities are recorded in a pattern class  a union of primitive patterns  i.e. a set of ss's with a common ud  fig. 1 . in our example  ss's would arise consisting of two adjacent coordinates  such as  these would be 
placed in a distinctive pattern class because their individual ud's are similar and can be combined  details are given later . this category would indicate indistinguishable utility behavior of each component ss and also nonseparability of e1 e1 into single coordinates  etc.; i.e. the primitives are meaningful as pair*. 
     the overall purpose of knowledge level 1 is to unify similar primitive structures  e.g. patterns of checkerboard squares  as functionally equivalent subobjects  i.e. to cluster ss's whose associated utility descriptors agree when superimposed. correspondence of ud's strengthens knowledge about utility-primitive relationships  since more information is present in the union  mutual data support . these regularized pattern classes become prospective feature elements. little information is wasted since only the strong patterns survive or even appear. 
	l. rendell 	1 
c. knowledge level 1: p a t t e r n groups 
     knowledge level s is the most advanced structure for feature formation. here we discuss it only briefly. 
     a level 1 pattern class is augmented by a set of transformation operators which  when applied to subspace specifiers in the class  reproduce extant members and fill in  missing  ss's. operators are selected which give the  best  closure in this induction of the pattern group. the feature f requires translation and rotation  of 1＜ . several pairs like e1  etc.  might be needed for confidence in the general transformation  which induces a group of 1 primitive patterns. 
     an alternative formalization for knowledge level 1 is grammatical inference . the above example would become a single production rule. 
d. s u m m a r y of layered knowledge 
     at the lowest knowledge level  data are clustered to distinguish prominent utility surfaces in primitive subspaces. discriminating combinations of primitives  ss's  are identified  and their utility relationships are condensed as utility descriptors  ud's . 
     at the second level  these results are consolidated into sets of corresponding primitive subspaces having mutually similar utility surfaces  ud's . as a consequence of matching and coalescing utility relationships  important structure in domain subobjects is identified and extracted. this structure creation emerges from clustering utility surfaces. 
     at the third knowledge level  individual primitive patterns of a given class are used to induce a complete group: a general rule is discovered for transforming one member of the class into another  and missing elements are inferred. as explained below  utility invariances help to generate these compound structures incrementally and efficiently. 
	v 	r e a l i s t i c c o n s t r u c t i v e i n d u c t i o n 
this section outlines feature creation from the knowledge structures just described and considers the reduced computational complexity resulting from their restriction. 


1 	l. rendell 
b. restricted use of primitives 
     even using exhaustive search  knowledge acquisition at level 1 is relatively inexpensive for current domains  although not generally  . here we will focus on levels 1 and 1  which  combinatorially  are extremely complex. the number of possible subspace specifiers  ss's  is 1 for checkers  and finding appropriate ss's is only one step in the induction. however the problem may be simplified. 
     in plso  the explicit creation of an ss may begin with straightforward primitive clustering which determines various ground values and structures  dg  udg  etc. - see section iii . since not all variables differentiate utility in a practical  small  data set  this ground processing reduces the 
effective dimensionality  by about two thirds in trial experiments . the remaining primitives are active for current data. the fact that only some primitives are active accounts for the phenomenon of useful information extraction in ground clustering; even though abstract features are confounded at the primitive level  structures do arise for small data sets. moreover  strong patterns appear first. 
　　utility-primitive relationships are reinforced through comparison with other formative subobject structures  i.e. with similar components of the board . to discover meaningful patterns among these subobjects  i.e. among subspaces of primitive space   certain arrangements a of variables from the active set s are considered. let the size of s be n; each a is a relation  over sk  where k n. for example  suppose is the active set. then n = 1. 
for the pair 
 e1 e1  determines the ss e1  etc.  and a defines a superposition of primitive subspaces  i.e. a class of ss's  here the class is  a precursor of f . 
c. mutual data support for structuring 
     given a superposition a of active variables  cluster  section iii  is now run with overlaid primitives treated as one. e.g.  for and would be identified. because of this equivalence  the sample sizes n  section iii  are effectively increased  by a factor roughly equal to the number of superpositions - it is patterns within states - subobjects that are counted . since larger n implies lower errors  the discrimination assurance d is higher in cases where primitive patterns coincide; i.e. when utilities match up in the overlaid dimensions  and merged ud's support each other  see figs. 1& 1 . if  instead  utility behavior differs  the mutual support is weaker  and if misalignment is extreme  d is even lower than the ground value dg. 
     the simple example of fig. 1 shows three cases of superposition  in three tables. the leftmost is ground clustering 
 no overlaying  with subspace specifier e1e1. in the first row   indicates that the tenth square is blank  and that the fourteenth and sixteenth squares both 
　　1. a feature is tentative until enough support is found for its determinative pattern class. this support increases gradually as experience is gained  additional data result in more regions per ud and in more patterns per class . once defined  features are independently assessed and selected by pls1  1 . 
contain a friendly man. this structure has a sample size n of only 1. in the third row of this leftmost table   1 1  has  note that in row one   while in row three  
 in no other row is either variable zero. 
   the center table shows identification of with in the first row of this table  n is the sum of the n values from the first and third rows of the leftmost table. this gives a sample size of 1 for  equal to zero and 
the other table entries are computed similarly. 
　　recall from section iii that utility  and that d is the sum of utility dissimilarities when rectangles are split. comparing the central and rightmost tables  we see that the discrimination assurance d should be substantial in the central table but likely zero in the rightmost one. thus the rightmost overlay is rejected. since the center case has utility values similar to the ground case  but the quantities are larger in the center overlay  d is higher there. hence the superposition of with is supported  these two variables are  similar  . 
     this is small sample similarity  which has been verified in experiments. when various superpositions are examined  certain of them are found to stand out  i.e. to have high d's; these become components of pattern classes. in summary  overlaid clustering extracts utility commonalities in components of the object  board description   and thereby discovers and strengthens patterns of meaningful structure. 
d. constrained  yet general constructive induction 
　　without some guidance  examining superpositions is extremely complex: the number of trials is  where n is the number of active primitives. the value of n may be reduced by screening inactive variables with ground clustering  subsection b   but more important  trials need not be exhaustive. they can be uniformly guided by general heuristics. intrinsic bonds in primitive variables manifest in poorer d's when elements of the combination are unduly superimposed. this discovery of small sample dissimilarity is illustrated above and in fig. 1  where and are distinguished. such a discovery disqualifies the superposition  as part of any other trial  which means that only 
pairs need be overlaid in preliminary testing. the complexity of this is 1 n1 . 

figure 1. three tables showing simplified data for ground clustering  left  and for superimposed clustering  center and right . the discrimination assurance d  section iii  depends on  i  dissimilar utilities g/n where n is the sample size and g is the number of successes  and  ii  the value of n  which affects error. since n increases with superposition of primitive variables  e.g. center and right   utility relationships may be strengthened. the central case is supported but the rightmost case is not. hence e1 and e1 are similar but e1 and e1 are not. superposition promotes discovery of meaningful structure in objects. 

     after pairs of small sample similar primitives are formed 
clustering is subsequently reapplied to classify pairs of similar primitives into larger sets  at this point  overlays are still one-dimensional . in this way  similar patterns emerge and cluster into mutually dissimilar sets. 
     finally  general ss's are constructed by appropriate selection. for example  if there are two dissimilar sets s1 = 
one ss class 
thus preserving primitive 
dissimilarity in distinct dimensions while identifying similar primitives and thereby creating structural pattern classes. 
     these algorithms have been programmed and tested with the fifteen puzzle.1 the 1 primitives used were city block distances of individual tiles from their home positions.  this gives a   1 % information compression advantage compared with the 1% of high level features - see .  the first feature induced was the total distance score fd. experiments show that discrimination assurance d is a good measure for trial superpositions. when data were gathered using breadth-first search   was created and no other pattern classes formed. in contrast  after fd emerged to guide search  
d values were less uniform in overlays  and other pattern classes began to arise. 
e. 	plso outlook and summary 
     one question about plso methodology is its generality. we can gain some insight by considering  for example  
samuel's features for checkers. several of these  such as piece advantage  guard  etc.  would present no problem. others  though  such as mobility  would require more sophisticated transformations  or additional knowledge levels  for efficient induction. not only games and puzzles  but also various real-world applications seem either to conform to the general plso approach as it stands  or else to possible extensions of the system. 
     to summarize: feature formation is straightforward once appropriate knowledge structures have been created. in limiting the huge quantity of possible structures   plso imposes few arbitrary constraints  compare    but rather the data help to simplify search structural patterns  primitive subspaces - ss's  and goal-directed information  utility relationships - ud's  are meaningfully combined using a credibility measure  discrimination assurance d  to assess candidate hypotheses  overlays . 
     this process creates a change of knowledge representation - to meaningful object components expressed as classes and groups of structures together with information about their utility. intrinsic bonds among primitive components become explicit. plso is designed for any cases in which components of objects may be identified  whenever transla-
     1. other ideas have not yet been programmed. one involves comparison with predicted d's assuming perfect superposition  i.e. full aligned utility surfaces . still another heuristic involves guidance from pattern classes already forming: since plso is incremental  current data may be compressed preferentially to match extant primitive patterns and classes - by searching for supporting superpositions  e.g. those having the same ss length as established classes. 
	l. rendell 	1 
tions  rotations  etc.  leave utility invariant . this applies to a large class of problems since goal-direction and invariance  similarity of object components  are prevalent. 
	v i 	c o n c l u s i o n s 
while still formative  plso is a general  substantial  and promising learning system for realistic constructive induction. its superior capability can be quantified  section ii . when assessed according to criteria for constructive induction   plso appears solid. the three level knowledge representation is adequate for domains considered and extensible to others. the rules of generalization are powerful  suited to problems not previously explorable. heuristics are general across domains. this stochastic scheme is insensitive to error. finally  plso is efficient; it effectively reduces computational complexity  e.g. from double exponential to polynomial in an important part of the problem - see section v . 
to improve efficiency  plso feature creation uses a 
 divide and conquer  scheme having three stages  each of which builds from elements verified at the level below. structures are generated  assessed  and improved so that the generate-and-test cycle of inductive inference is meaningfully constrained. each of the three stages is both model- and data-driven: general heuristics speed formation of hypothetical concepts  and task utility determines the more credible ones. 
     the basis for this constructive induction is clustered utility surfaces. invariance of utility relationships in primitive description spaces leads to a new form of clustering which creates new knowledge structures. progressive structuring relies on mutual data support during clustering of utility surfaces: information is simultaneously regularized and reinforced. mutual data support is important: it is the simultaneous occurrence of concept formation  noise management  accuracy improvement  and complexity reduction. 
     the computational complexity of the generalization problem may be reducible from intractably exponential to practically polynomial. 
r e f e r e n c e s 
 1  anderberg  m.r.  cluster analysis for applications  academic press  1. 
 1  banerji  r.b.  some linguistic and statistical problems in pattern recognition  pattern recognition s   1   1. 
 1  banerji  r.b.  pattern recognition: structural description languages  in belzer  j.  ed.   encyclopedia of computer science and technology 1  1   1. 
 1  bierre  p.  the professor's challenge  ai magazine 1  1  winter 1   1. 
 1  christensen  r.  foundations of inductive reasoning  entropy  ltd.  1. 
 1  dietterich  t g   london  b.  clarkson  k.  and dromey  g.  learning and inductive inference  stan-cs-1  stanford university  also ch.xiv of the handbook of artificial intelligence  cohen & feigenbaum  ed.   kaufmann  1. 
 1  dietterich  t.g. and michalski  r.s.  a comparative review of selected methods for learning from examples  in   1   1. 
 1  ernst  g.w. and goldstein  m.m.  mechanical discovery of classes of problem-solving strategies  j. acm s1   1  1. 
 1  fu  k.s.  syntactic pattern recognition and applications  prentice-hall  1. 

1 	l rendell 
 1  langley  p.  bradshaw  g.l.  and simon  h.a.  rediscovering chemistry with the bacon system  in  1   1   1. 
 1  lenat  d.b. and brown  j.s.  why am and eurisko appear to work  artificial intelligence s  1   1. 
 1  mccarthy  j.  ai needs more emphasb on basic research  ai magazine 1  1  winter 1   1. 
 1  med in  dx. and smith  e.e.  concepts and concept formation  annual review of psychology 1   1   1. 
 1  medin  dx.  wattenmaker  w.d.  and michalski  r.s.  the problem of constraints in inductive learning  as yet unpublished manuscript   1. 
 1  michakki  r.s.  a theory and methodology of inductive learning  artificial intelligence 1  1  1   1; reprinted in |1   1. 
 1  mkhalski  r.s.  carboneu  j.g.  and mitchell  t.m.  ed.   machine learning: an artificial intelligence approach  tioga  1. 
 1  michalski  r.s. and chilausky  r.l.  learning by being told and learning from examples: an experimental comparison of the two methods of knowledge acquisition in the context of developing an expert system for soybean disease diagnosis  int. j. policy analysis and information systems 1  1  1   1. 
 1  michalski  r.s. and stepp  r.e.  learning from observation: conceptual clustering  in |1   1. 
 1  quinlan  j.r.  learning efficient classification procedures and their application to chess end games  in |1|  1   1. 
 1  rendell  l.a.  a method for automatic generation of heuristics for state-space problems  dept of computer science cs-1  university of waterloo  1. 
 1  rendell  l.a.  a new basis for state-space learning systems and a successful implementation  artificial intelligence 1  1  1   1. 
 1  rendell  la.  toward a unified approach for conceptual knowledge acquisition  ai magazine 1  1  winter 1   1. 
 1  rendell  l.a.  utility patterns as criteria for efficient generalisation learning  proc. 1 conference on intelligent systems and machines   also univ. of illinois report no. uiucdcs-r1   1. 
 1  rendell  l.a.  genetic plans and the probabilistic learning system: synthesis and results  univ. of illinois report no. uiucdcs-r-1  submitted for publication   1. 
 1  rendell  l.a.  conceptual knowledge acquisition in search  university of guelph report cis-1  dept. of computing and information science  quelph  ontario  canada  1. 
 1  ritchie  g.d. and hanna  f.k.  am: a case study in ai methodology  artificial intelligence 1   1. 
 1  sahni  s.  concepts in discrete mathematics  camelot  1. 
 1  samuel  ax.  some studies in machine learning using the game of checkers ii-recent progress  ibm j. ret. and develop. ii  1  1. 
 1  ton  t.t. and gonsales  r.c.  pattern recognition principies  addison-wesley  1. 
 1  watanabe  s.  knowing and guessing: a formal and quantitative study wiley  1. 
 1  watanabe  s.  pattern recognition as information compression  in watanabe  s.  ed.   frontiers of pattern recognition  academic press  1  1. 
 1  winston  p.h.  artificial intelligence  addison wesley  1. 
acknowledgements 
i would like to thank steve chien  chris matheus  sheldon nichol  and raj seshu for discussions of ideas in this paper  and for useful criticisms of the text. i also appreciate the helpful comments from ijcai reviewers. 
appendix. glossary of terms 
     clustering. cluster analysis has long been used as a tool for induction in statistics and pattern recognition  1 . similar improvements to the basic techniques have been invented independently by the author and by michabki  see section iii . this paper presents still another extension of clustering  one suitable for constructive induction. 
     feature. a feature is an attribute or property of an object. features are usually quite abstract  e.g.  mobility  . the utility  see below  varies smoothly with a feature. compare  primitive*'. 
     induction. induction  hypothesis formation  or generalization learning is an important means for knowledge acquisi-
tion. information is actually created . induction forms data into classes or categories in order to predict future events efficiently and effectively. selective induction is relatively simple: it forms neighborhoods of feature space clusters. constructive induction is much more difficult  requiring 
creation of concepts. see sections i  iii and iv. 
     mediating structures. successful systems tend to incorporate knowledge structures which mediate objects and concepts during inductive processing. these structures include means to record growing assurance of tentative hypotheses. see . 
     mutual data support. this is a term coined by the author to express a subtle combination of phenomena in the inductive process. several causes and effects are interwoven: noise management  accuracy improvement  efficient processing  and concept formation  sections iii.a and v.c . 
     object. objects are any data to be induced into categories. components of objects  subobjects  may be interrelated  and regularities may be discovered through constructive induction. relationships usually depend on some task domain. see  utility*. 
     pls. the probabilistic learning system can learn what are sometimes called  single concepts    but pls is capable of much more difficult tasks  involving noise management  incremental learning  and normalization of biased data. pls1 uniquely discovered locally optimal heuristics in search 
 1 ; pls1 is an effective and efficient extension using the genetic paradigm ; and pls1 is the system for constructive induction examined in this paper. pls manipulates regions in augmented feature or primitive space  using various inductive operations . 
     primitive. a primitive is a detailed  low-level property of an object or subobject. the utility varies irregularly or discontinuously with a primitive. compare  feature . 
     region. the region is pls's basic structure for induction  clustering . it is a compressed representation of a utility surface in augmented feature space. see section iii.c. 
     utility. this is any measure of the usefulness of an object in the performance of some task. the utility u provides a link between the task domain and generalisation algorithms  u can be a probability: the number g of  good  objects contributing to task success  divided by the total number n of observed objects; i.e. u = g / n . utility is related to an evidential criterion for induction. see section hi. 
