
qualitative models are often more suitable than classical quantitative models in tasks such as model-based diagnosis  mbd   explaining system behavior  and designing novel devices from first principles. monotonicity is an important feature to leverage when constructing qualitative models. detecting monotonic pieces robustly and efficiently from sensor or simulation data remains an open problem. this paper presents scale-based monotonicity: the notion that monotonicity can be defined relative to a scale. real-valued functions defined on a finite set of reals e.g. sensor data or simulation results  can be partitioned into quasimonotonic segments  i.e. segments monotonic with respect to a scale  in linear time. a novel segmentation algorithm is introduced along with a scalebased definition of  flatness .
1 introduction
qualitative models are used in applications such as modelbased diagnosis  yan  1; struss  1   explaining system behavior   uc  1; forbus  1; kuipers  1   and designing novel devices from first principles  williams  1 . it is a challenge to build qualitative models for complex real world engineering systems. current research efforts are on automatic generation of qualitative models from numerical data obtained by numerical simulation or sensors. many proposed methods  such as in  struss  1  and  console et al.  1   work only when the functions are piecewise monotonic. hence  partitioning data arrays into quasi-monotonic segments is an important problem  yan et al.  1b; 1a . one could segment a data array in monotonic segments using a na ve algorithm: simply segment wherever there is an extremum. for example  it is easy to segment the array {1  1  1  1  1  1  1} in two monotonic segments. however  monotonic segments must be significant for the application at hand and algorithms must be robust because the data will unavoidably be subject to noise or insignificant features. hence  we might want to consider that the array {1  1  1  1  1  1  1} has only two  significant  monotonic segments since the drop from 1 to 1 is not large enough to indicate a downward trend. also algorithms must be fast  o n   and not require excessive amounts of memory  o n  .
모in this paper  we address the  scale-based monotonicity  problem: monotonicity can be defined relative to a scale. we define a metric for monotonic approximation. we solve the following problem: given a number of segments k  find k segments having the smallest monotonic approximation error. in short  the main novel results of this paper are:
1. a novel optimal segmentation algorithm;
1. a novel definition of scale-based flatness.
1 monotonicity in qualitative modelling
we just list a few examples to show that the monotonic features are important to leverage in qualitative modeling. in qsim  kuipers  1   the qualitative value qv f t   is the pair  qmag  qdir   where qmag is a landmark value li or an interval made up of landmark values  li li+1   and qdir takes a value from the sign domain  + 1 -  according the f 1 t  is increasing  steady  or decreasing respectively. in model-based diagnosis  mbd   qualitative models are used to describe system behavior. two kinds of qualitative models  based respectively on absolute or relative quantities  rely on the monotonicity of the function. the first one is finite relation qualitative model  frq   struss  1; yan  1   where the qualitative relation is represented by tuples of real valued intervals. for a monotonic segment  xa xb  of f  the tuple is determined by the bounding rectangle  =  xa xb  뫄  min f xa   f xb   max f xa   f xb     that is the smallest rectangle such that y = f x vxa 뫞 x 뫞 xb    x y  뫍 . the second one is qualitative deviation model  console et al.  1  where the qualitative relation is represented by the sign of deviation   y  from a reference point xref defined as + -  or 1  according to whether f is increasing  decreasing or flat. for example  if f is monotonic increasing  we have   y = sign f x   f xref   = sign x xref   =   x .
모in this paper  we focus on abstracting qualitative models from scattered data obtained from numerical simulation or sensors. this work is motivated by at least two kinds of applications. first  when applying model-based diagnosis to real world engineering systems  we need to build symbolic qualitative models by a numerical simulation of the engineering models. second  we need to explain system behaviors from sensor data. qualitative model abstraction in this paper is defined as transforming numerical values into qualitative values and functions into qualitative constraints   uc and bratko  1 . monotonicity analysis is defined as partitioning a finite series of real values x1 x1 ... xn over an interval d into  monotonic  segments.
모monotonicity analysis is a crucial step to abstract qualitative models from scattered data but it rises a problem. the difficulty is that when the scattered data contains noise  the monotonicity is not absolute in a neighborhood of any point. the monotonic segments need to be significant in the context of the problem. the small fluctuations caused by noise must be ignored. this requires that noise be removed by computationally efficient methods such that the number of remaining segments is dependent only on the characterization of the
noise.
linear splines is an obvious approach to solve this prob-
lem. we can use top-down  bottom-up and sliding window algorithms to approximate the data with a set of straight lines  keogh et al.  1 . then the same-sign slopes can be aggregated as monotonic segments. various algorithms are derived from classic algorithms  key et al.  1   hunter and mcintosh  1 . the downside of these methods is that there is no link between linear spline approximation error and the actual monotonicity of the data  consider y = ex . hence  using linear splines  it is difficult to specify either the desired number of monotonic segments or some  monotonicity error  threshold. in addition  linear fitting algorithms are relatively expensive.
모inductive learning is used in   uc and bratko  1  to automatically construct qualitative models from quantitative examples. the induced qualitative model is a binary tree  called a qualitative tree  which contains internal nodes  called splits  and qualitatively constrained functions at the leaves. a qualitative constrained function takes the form ms1 ... sm : rm 1뫸 r  si 뫍 {+  } and represents a function with m real-valued attributes strictly monotone increasing with respect to the i-th attribute if si = +  or strictly monotone decreasing if si =  . for example  f = m+   x y  means f is increasing when x is increasing  and decreasing when y is increasing. a split is a partition of a variable. the unsupervised learning algorithm eq-quin determines the landmarks for the splits. the training data set is all possible pairs of data points. eq-quin checks the best split against all possible hypotheses. its complexity is o n1m . quin is a more efficient algorithm that uses greedy search. its complexity is o n1 . we do not address multidimensional data in this paper.
1 monotonicity error
in this section  we define a measure of monotonicity. suppose we are given a set of n ordered samples noted f : d = {x1 ... xn}   r 뫸 r with real values f x1  ... f xn  and x1   x1   ...   xn. we define  f| a b  as the restriction of f over d 뫌 a b . we seek the best monotonic  increasing or decreasing  function f : r 뫸 r approximating f. let  뫺  resp.  뫻  be the set of all monotonic increasing  resp. decreasing  functions. the optimal monotonic approximation function error  omafe  of f is given by minf뫍 maxx뫍d |f  f| where   is either  뫺 or  뫻. sign + or - is associated to  뫺 or  뫻.
	the	segmentation	of	a	set	d	is	a	sequence	s =
x1 ... xm of closed intervals  called  segments   in d with
 mind maxd  = si xi such that maxxi = minxi+1 and xi 뫌 xj = 1/ for j 1= i i+1 i 1. alternatively  we can define a segmentation from the set of points xi 뫌xi+1 = {yi}. given f : {x1 ... xn} 뫸 r and a segmentation {xi}  the optimal piecewise monotonic approximation function error  opmafe  of the segmentation is given by maxi omafe f|xi  where the directions of the segments xi are alternating and such that the direction of the first segment is chosen so as to minimize the opmafe.
모solving for a best monotonic function can be done as follows. if we seek the best monotonic increasing function  we

first define f 뫺 x  = max{f y  : y 뫞 x}  the maximum of all previous values  and f  x  = min{f y  : y 뫟 x}  the mini-
모모모모모모모모뫺 mum of all values to come . if we seek the best monotonic

decreasing function  we define f 뫻 x  = max{f y  : y 뫟 x}  the maximum of all values to come  and f  x  = min{f y  :
뫻
y 뫞 x}  the minimum of all previous values . these functions which can be computed in linear time are all we need to solve for the best approximation function as shown by the next theorem which is a well-known result  brooks  1; ubhaya  1 .
theorem 1. given f : d = {x1 ... xn} 뫸 r  a best monotonic increasing approximation function to f is given by
f뫺+f뫺
f뫺 =	1	and a best monotonic decreasing approxima-
                       f뫻+f뫻 tion function is given by f뫻 =	1 . the corresponding er-
|f뫺 x  f뫺 x |
ror  omafe  is given by maxx뫍d 1  monotonic increasing  or maxx뫍d   monotonic decreasing .
모the implementation of the algorithm suggested by the theorem is straight-forward. given a segmentation  we can compute the opmafe in o n  time using at most two passes. the functions f뫺 and f뫻 are sometimes called the standard optimal monotone functions as the solution is not unique in general.
1 scale-based monotonicity
we present the notion of scale-based monotonicity. the intuition is that the fluctuations within a certain scale can be ignored.
모given an ordered set of real values d = {x1 x1 ... xn}  consider f : d = {x1 x1 ... xn} 1뫸 r. given some tolerance value 붻   1  we could say that the data points are not going down or are upward monotone  if consecutive measures do not go down by more than 붻  that is  are such that f xi  f xi+1    붻. however  this definition is not very useful because measures can repeatedly go down and thus the end value can be substantially lower than the start value. a more useful definition of upward monotonicity would be to require that we cannot find two values xi and xj  xi   xj  such that f xj  is lower than f xi  by 붻  i.e. f xi  f xj    붻 . this definition is more useful because in the worst case  the

figure 1: a 붻-pair.
last measure will be only 붻 smaller than the first measure. however  we are still not guaranteed that the data does in fact increase at any point. hence  we add the constraint that we can find at least two data points xk   xl such that f xl  is greater than f xk  by at least 붻  f xl  f xk  뫟 붻 .
모to summarize  given some value 붻   1  we say that a sequence of measures is upward 붻-monotone if no two successive measures decrease by as much as 붻  and at least one pair of successive measures increases by at least 붻. similarly  we say that a set of measures is downward 붻-monotone if no two successive measures increase by as much as 붻  and at least two measures decrease by at least 붻.
모this generalized definition of monotonicity was introduced in  brooks  1  using 붻-pairs  see fig. 1 :
definition 1.
  x y뫍d is a 붻-pair  or a pair of scale 붻  for f if |f y   f x | 뫟 붻 and for all z 뫍 d  x   z   y implies |f z   f x |   붻 and |f y  f z |   붻.
  a 붻-pair's direction is increasing or decreasing according to whether f y    f x  or f y    f x . notice that pairs of scale 붻 having opposite directions cannot overlap but they may share an endpoint. pairs of scale 붻 of the same direction may overlap  but may not be nested for a certain 붻.
we can define 붻-monotonicity as follows:
definition 1. let x be an interval  f is 붻-monotone on x if all 붻-pairs in x have the same direction; f is strictly 붻monotonic when there exists at least one such 붻-pair. in this case:
  f is 붻-increasing on x if x contains an increasing 붻pair.
  f is 붻-decreasing on x if x contains a decreasing 붻-pair.
we say that a pair is significant at scale 붻 if it is of scale 붻1 for 붻1 뫟 붻.
모in the next section  we discuss how to partition the data set into monotonic segments.
1 a scale-based algorithm for quasi-monotonic segmentation
suppose that d is a finite set of reals having at least two elements. f is a real-valued function on d  i.e.  f : d 뫸 r.
모we begin by defining a segmentation at scale 붻 or a 붻segmentation.
definition 1. let s = x1 ... xn be a segmentation of d  and let 붻   1  then s is a 붻-segmentation of f when all the following conditions hold.
  each xi is 붻-monotone.
  each xi for i 1= 1 n is strictly 붻-monotone.
  at least one xi is strictly 붻-monotone.
  adjacent strictly 붻-monotone segments have opposite directions.
  for each strictly 붻-monotone xi  and for all x 뫍 xi  f x  lies in the closed interval bounded by f minxi  and f maxxi .
  when x1 is not strictly 붻-monotone  then for all x 뫍 x1   maxx1  f x  lies in the open interval bounded by f minx1  and f maxx1 ; and when xn is not strictly 붻-monotone  then for all x 뫍 xn  minxn  f x  lies in the open interval bounded by f minxn 1  and f maxxn 1 .
each xi is called a 붻-segment of s. when strictly 붻-monotone  xi is a proper 붻-segment; when not strictly 붻-monotone  x1 or xn is an improper 붻-segment. for strictly 붻-monotone xi  minxi and maxxi are 붻-extrema.
모as the following theorems show  all 붻-segmentations are  equivalent  and the monotonic approximation error  opmafe  is known precisely.
theorem 1. let 붻   1. let s1 and s1 be 붻-segmentations of f; then |s1| = |s1|. furthermore  the first 붻-segments of s1 and s1 are both either proper or improper  and similarly for the last 붻-segments.
theorem 1. let 붻   1. let s be any 붻-segmentation of f; then the monotonic approximation error  opmafe  뫞 붻/1.
모now we want to compute a 붻-segmentation given f. there are two approaches depending on the application one has in mind. the first one is to choose 붻 and then solve for the segments  brooks  1   when the magnitude of noise is known. when one doesn't know how to choose 붻  the second approach is to set the maximal number of segments k  especially when one knows the shape of the function. we focus on this second approach. we begin by labelling the extrema with a corresponding scale   x .
definition 1. let x 뫍 d be a local extremum of f. x's deltascale   x  is the largest 붻   1 such that there exists a 붻segmentation having x as a 붻-extremum.
모it is immediate from the definition that if 붻     x   then x can't be a 붻-extremum  but also that if x is a 붻-extremum  then   x  뫟 붻. this is stated in the following proposition.
proposition 1. let 붻   1  and let s = x1 ... xn be a 붻segmentation of f. then   x  뫟 붻 for every 붻-extremum  x  of
s.
모we observe that there must be a smallest 붻 such that the cardinality of x붻 = {x|  x  뫟 붻} is at most k. however the set x붻 might contain repeated maxima or repeated minima and those might be removed before the set x붻 can define a
붻-segmentation. this is stated in the next theorem.
theorem 1. let 붻   1 and consider the set of extrema x붻 = {x|  x  뫟 붻} of f as an ordered set. each extremum in x붻 is either a maximum or a minimum  and a sequence of two minima or two maxima is possible. consider a subset x such that
  it has no repeated maxima or minima;
  there is no superset x in x붻 without repeated extrema;
then there exists a 붻-segmentation s of f such that x붻1 is exactly the set of 붻-extrema of s.
모in order to compute   x  for all extrema x  it is useful to introduce the following definitions.
definition 1. opposite-sense extrema x   z 뫍 x   d are an extremal pair for x if for all y 뫍 x  x   y   z implies f y  lies in the closed interval defined by f x  and f z . the extremal pair has an extent  f x   f z   or  f z   f x   and increasing or decreasing direction according to f x    f z  or f x    f z . an extremal pair is maximal for x when no other extremal pair in x has larger extent. similarly  an extremal pair is a maximal increasing  decreasing  when no increasing  decreasing  extremal pair has larger extent.
모intuitively  a maximal extremal pair forms the largest decreasing  increasing  segment inside a larger increasing  decreasing  segment x.
definition 1. we recursively define ordinary and special intervals: d is an ordinary interval. when i is an ordinary interval  then an interval j   i is special when j's endpoints constitute a maximal extremal pair in i; in this case j has direction inherited directly from its endpoints. recursively  if j is special and interval j1   j has endpoints constituting a maximal extremal pair in j of direction opposite to that of j  then j1 is special. let ji i = 1...n be all special intervals in an ordinary or special interval x. choose any nonempty subset of the collection {ji}  and let s be the segmentation of x defined by the endpoints of the special intervals in the subset. then any segment i 뫍 s that does not contain any of the ji is an ordinary interval.
모one can see that the special intervals are nested. the endpoints x z of the special intervals have   x  =   z  = |f x  f z |. we can call them  twins  due to the same   x  value. the ordinary intervals are not nested. the endpoints of the ordinary intervals have different   x . we call each of them  singleton .
모if there are several extrema having an equal value  the way to choose the endpoints to constitute a maximal extremal pair is not unique. therefore  there are different sets of special intervals. they are equivalent. for simplicity  we do not consider the case of equal valued extrema in algorithm 1 and algorithm 1. instead  we explain how to deal with it verbally after introducing the algorithms.
모algorithm 1 computes   x  for every extremum x 뫍d. the input to algorithm 1 is a list of extrema - i.e. the extrema data points in the data array.

algorithm 1 this algorithm labels the extrema in linear time  as in definition 1 .

input: data - a list of the successive extremal values of f  alternating between maximum and minimum. each element is an  extremum record   having three fields: value  sense  and index. index identifies the data point  value gives f's value at that data point  and sense indicates whether the extremum is a maximum or minimum.
output: a list of  scale records . each scale record has two fields: scale and extrema list. extrema list comprises either one or two extremum records. the semantics of a scale record is that the indicated extrema have the indicated scale as delta-scale. notes about the algorithm: 1  lists are accessed by element numbers; e.g. data 1  is the second element of data. 1  lists are manipulated with functions push and pop. push  thing  list   adds thing to list  resulting in thing being the first element of list. pop list  removes the first element of list  and returns this first element as the value of the function call. 1  makelist item1..   pushes the items into a list generated  starting from the first item. 1  makescalerecord creates a scale record.
let extrema = empty list
let scales = empty list
push  pop data  extrema  push  pop data  extrema  for next extremum in data do
while length extrema    1 and {{sense.next extremum =  maximum  and value.next extremum   value.extrema 1 } or {sense.next extremum =  minimum  and value.next extremum   value.extrema 1 }} do push makescalerecord  | value.extrema 1  value.extrema 1  |  makelist  extrema 1   extrema 1     
scales  pop extrema 
모pop extrema  if length  extrema   = 1 then
push  pop  extrema list.scales 1     extrema  
모push  next extremum  extrema   while length extrema    1 do push  makescalerecord  | value.extrema 1  value.extrema 1  |  makelist  extrema 1      scales  pop extrema 
push   extrema 1   extrema list.scales 1    return scales

모the principle of algorithm 1 is as follows: the while loop inside the for loop detects the special interval and labels the both endpoints with the difference of their values. but if the extrema list is empty after this labelling  which means that the top item in the list needs to be checked against the coming data  this top item is popped back to extrema in the if following the above while. then the next extremum is pushed into the extrema list for the next for loops. the while outside the for loop determines the 붻 for the ordinary intervals. the last push states that the last extremum in the extrema list has the same scale as the one just popped into scale. actually the endpoints of the biggest ordinary interval are always the last two pushed into scale record list and their scales are identical.
모if one desires no more than k segments  it is a simple matter of picking 붻 as small as possible so that you have no more than k +1 significant extrema    x  뫟 붻  together with the first and last extrema  indexes 1 and n   1 . algorithm 1 shows how once the labelling is complete  one can compute the segmentation in time o nklogk  which we consider to be linear time when k is small compared to n. it also uses a fixed amount of memory  o k  . the principle of algorithm 1 is to select the k +1 data points to be the endpoints of k segments. since the endpoints of d are by default  the remaining endpoints come from the largest 붻-extrema. the for is to select the largest 붻-extrema. as in algorithm 1  we know that the labels are either  twins  or  singleton . thus in the for loop  a maximum of k +1 data points are chosen. then  after the first if outside for  the smallest 붻-extrema are removed to reduce the total endpoints to at most k +1. the rest of the code checks whether the first and the last endpoints of d are included. if not  the smallest 붻-extrema will be removed in favor of the first and the last endpoints. the number of segments can be less than k since several extrema can take the same 붻 value and can be removed together.

algorithm 1 given a labelling algorithm  see algorithm 1   this algorithm returns an optimal segmentation using at most k segments. it is assumed that there are at least k+1 extrema to begin with.

input: an array d containing the values indexed from 1 to n 1 input: k a bound on the number of segments desired output: segmentation points as per theorem 1 and proposition 1
l 뫹 empty array  capacity k+1  for e is index of an extremum in d having scale 붻  e are visited in increasing order do
insert  e 붻  in l so that l is sorted by scale in decreasing order
 sort on 붻  using binary search
if length l  = k+1 then pop last l 
if length l    k+1 then
remove all elements of l having the scale of last l .
if indexes {1 n 1} 1  l then
if  index 1 뫍 l or index n  1 뫍 l  and length l  = k+1
then
모remove all elements of l having the scale of last l . if  index 1뫍l and index n 1뫍l  and length l 뫟k then remove all elements of l having the scale of last l .
return: the indexes in l adding 1 and/or n 1 when not already present

모algorithms 1 and 1 assume that no two extrema can have the same value. in case of equal valued extrema  we can modify these algorithms without increasing their complexity. in algorithm 1  we can generalize scalerecord so that entries in the extrema list field can also be lists of extrema of the same value. then in the while loop  we can put the extrema of the same value into the corresponding list. for algorithm 1  we just need to remember that whenever removing a middle point between two equal valued extrema  the two equal valued extrema have to be treated as one extrema to avoid having two adjacent minima or maxima.

figure 1: input flow of tank a  left  and level of tank b  right  with added white noise.
1 scale-based flatness
for applications  it is important to be able to find  flat  segments robustly in a data set. we propose the following definition:
definition 1. given 붻   1  consider a 붻-monotonic segment in a 붻-segmentation  an interval i in this segment is 붻-flat if the standard optimal monotonic approximating function  as defined in theorem 1  is constant over i.
모because we can compute the standard optimal monotonic approximating function in o n  time  we can find flat segments in o n  time.
모the next proposition addresses the flat segments in different 붻-segmentations. the first point is derived from the fact that when the 붻 increases  the new segments are always the result of mergers of the previous segments. hence  the segments at a small 붻 are always subsets of the segments at a larger 붻. the second point says that the old flat segments will still be flat in the merged segment.
proposition 1. let s be a 붻-segmentation and let s1 be a 붻1segmentation for 붻1   붻  then 1  any 붻-segment x 뫍 s is a subset of some 붻1-segment x1 뫍 s1  1  for any 붻-flat interval i
in x  i is also 붻1-flat in x1.
1 experimental results
1 cascade tank sample data
as a source of synthetic data  we consider a system which consists of two cascade tanks a and b. each tank has an input pipe  incoming water  and an output pipe  outgoing water . tank a's output pipe is the input pipe of tank b. in the equations below  let a and b be the level of water for the two tanks respectively. the change of water level is proportional to the difference of the input flow and the output flow. assume in is the input flow of tank a. f a  is the out flow of tank a. g b  is the output flow of tank b  kuipers  1 . thus  we have
	a1	=	in  f a 
	b1	=	f a  g b 
모in principle  f a  and g b  are increasing functions. we assume f a  = k1a and g b  = k1b. if we variate the input flow of tank a  in  we can control the level of tank b. in this way  we generated some sample data and added noise to it. see fig. 1 for the the input flow  in  of tank a at the left and the level of tank b at the right.

figure 1: opmafe versus number of segments k for tank b with  above  and without white noise  below .
1 optimal algorithm vs. top-down algorithm
we choose the top-down linear spline approximation algorithm to compare the performance with the optimal algorithm developed in this paper. the top-down algorithm is a refined process choosing best split points that minimizes the linear regression error. the top-down algorithm we implemented has complexity of o nk1  which is the best complexity on record. the optimal algorithm in this paper is o nklogk . the optimal algorithm is faster for k sufficiently large. we also compare the monotonicity approximation error using opmafe. figure 1 compares the opmafe for segmentations computed using either algorithm. the optimal algorithm we presented in this paper performs significantly better than the top-down algorithm. note that opmafe is an absolute error measurement  which grows with the amplitude of the data .
1 conclusion
monotonicity is the most important feature to leverage when building the qualitative model from the scattered numerical data. this paper gives a solid mathematical foundation to the problem of defining monotonicity based on scale theory  where the values of the data points  not the distance between the data points  determine the monotonicity. it proves to be suitable in qualitative modeling. we present efficient algorithms to segment the data set into monotonic segments in linear time. we will continue to work on the multidimensional case.
