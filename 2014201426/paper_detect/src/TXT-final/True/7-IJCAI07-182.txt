
kernel based nonlinear feature extraction  kfe  or dimensionality reduction is a widely used preprocessing step in pattern classification and data mining tasks. given a positive definite kernel function  it is well known that the input data are implicitly mapped to a feature space with usually very high dimensionality. the goal of kfe is to find a low dimensional subspace of this feature space  which retains most of the information needed for classification or data analysis. in this paper  we propose a subspace kernel based on which the feature extraction problem is transformed to a kernel parameter learning problem. the key observation is that when projecting data into a low dimensional subspace of the feature space  the parameters that are used for describing this subspace can be regarded as the parameters of the kernel function between the projected data. therefore current kernel parameter learning methods can be adapted to optimize this parameterized kernel function. experimental results are provided to validate the effectiveness of the proposed approach.
1 introduction
feature extraction or dimensionality reduction is a widely used pre-processing step for classification and data mining tasks  since extracting proper features can reduce the effect of noise and remove redundant information in the data that is irrelevant to the classification or data analysis tasks.
¡¡suppose that we are given a set of n data points   where xi ¡Ê x   rd is the input data  x is the input space. traditional feature extraction approaches  such as the principle component analysis  pca  and linear discriminant analysis  lda  are linear methods and they project the input data xi into a low dimensional subspace of the input space x.
¡¡recently  constructing nonlinear algorithms based on the kernel methods  scholkopf and smola  1¡§   have proved successful. for a given positive definite kernel function
k : x ¡Á x ¡ú r  the input data xi  1 ¡Ü i ¡Ü n are implicitly mapped to a feature space f with usually very high dimensionality. let ¦Õ ¡¤  denote the map from x to f  then
a kernel based algorithm essentially applies linear methods in f for the mapped data . for example  in the kernel principal component analysis  kpca  algorithm  scholkopf and smola  1¡§    pca is used to extract a representative subspace of f. compared with the traditional linear approaches  kernel methods are more powerful since they can explore nonlinear structures of the data  and more flexible as we can recover the linear algorithms by simply using the linear kernel in the kernel based methods.
¡¡usually the dimensionality of f is very high or even infinite  which is helpful for separating different classes of data. however  such a high dimensional space f may contain some redundancy that is irrelevant or even noisy for the given classification or data mining tasks. hence  as is the case for feature extraction in the input space  it may be also helpful for classification or data mining tasks to find a lower dimensional subspace s of f.
many kernel based feature extraction  kfe  approaches
have been proposed to find a lower dimensional subspace s of the feature space f. for example  kpca  scholkopf and¡§ smola  1  is widely used for this task. as mentioned above  it essentially performs linear pca in the feature space f. the goal is to find directions along which the data variance is the largest.
¡¡in this paper  we discuss feature extraction methods with the focus on improving the classification accuracy. in the c-class classification problem  each data point xi is associated with a label yi ¡Ê rc  where y  and yik = 1  1 ¡Ü k ¡Ü c  if xi belongs to class k  and 1 otherwise.1 it can be seen that kpca may be not effective for classification problems since it is an unsupervised feature extraction method  which ignores the labels of the given data.
¡¡hence several supervised kfe algorithms have been proposed  which make use of both the input data and the corresponding labels. like kpca  they also perform linear feature extraction or linear dimensionality reduction in the feature space f.
¡¡the kernel fisher discriminant analysis  kfda   mika et al.  1  aims to find a data projection by minimizing the within-class variance and maximizing the between-class variance simultaneously  thus achieving good discrimination

also possible.1other strategies for constructing the label yi  1 ¡Ü i ¡Ü n  arebetween different classes. an efficient variant of kfda based on qr decomposition  called akda/qr  is proposed in  xiong et al.  1 . a distinct property of akda/qr is that it scales as o ndc . and in akda/qr  the number of features extracted is fixed to the number of classes.
¡¡the partial least squares  pls  algorithm  wold  1  has been widely applied in the domain of chemometrics. unlike the pca algorithm  which extracts features only based on the variance of the input data  the pls algorithm uses the covariance between the inputs and the labels to guide the extraction of features. the kernel pls  kpls  algorithm is proposed in  rosipal and trejo  1 .
¡¡the orthogonal centroid  oc   park and park  1  algorithm is a linear dimensionality reduction method that preserves the cluster structure in the data. in this algorithm  the given data are firstly clustered  and then projected into a space spanned by the centroids of these clusters. an orthogonal basis of this subspace is computed by applying qr decomposition to the matrix whose columns consist of the cluster centroids. in  kim et al.  1   this method is applied for dimensionality reduction in text classification tasks and exhibits good results. its kernel based nonlinear extension  i.e. the kernel orthogonal centroid  koc  algorithm is also presented in  park and park  1 . to incorporate the label information  the koc  and oc  algorithm treats input data in the same class as one single cluster  therefore the number of extracted features equals the number of classes. however this method can be easily extended by allowing more clusters in each class.
¡¡in this paper  we propose a subspace kernel  based on which the nonlinear feature extraction problem can be transformed into a kernel parameter learning problem.
¡¡the rest of this paper is organized as follows. in section 1  we propose the basic idea of our approach and formulate the subspace kernel. some connections to the related methods are described in section 1. in section 1  we present one possible way to optimize the proposed subspace kernel. experimental results are provided in section 1 and we conclude the paper in the last section.
1 nonlinear feature extraction via kernel parameter learning
1 basic idea
as mentioned before  a given positive definite kernel k implicitly introduces a mapping of the given data ¦Õ xi   1 ¡Ü i ¡Ü n  to a usually high dimensional feature space f. when projecting ¦Õ xi   1 ¡Ü i ¡Ü n  into a subspace s of f  the kernel function has to be modified correspondingly since the feature space has changed from f to s. for convenience  we call this modified kernel function the subspace kernel. as will be shown later  the parameters that are used for describing s are also the parameters of the corresponding subspace kernel. therefore current kernel parameter learning methods can be adapted to optimize this kernel function. this way we can find a discriminating subspace s where different classes of data are well separated. in the following  we will explain the above idea in detail by formulating the aforementioned subspace kernel.
1 the subspace kernel
suppose s is an nf dimensional subspace of f and o =
 o1 ... onf  is a matrix whose columns constitute an orthogonal basis of s. let t denote the subspace spanned by the mapped data ¦Õ xi   1 ¡Ü i ¡Ü n  in f  then each oi can be uniquely decomposed into two parts  one is contained in t and the other one is in the orthogonal complement of t   o
where  = 1 for 1 ¡Ü i ¡Ü n. therefore for any ¦Õ xi   its projection into s can be computed as
	o	 1 
where o
¡¡equation  1  indicates that to compute the projection of ¦Õ xi  in s  it is enough to only consider the case where s is a subspace of t   which implies that any vector in s can be expressed as a linear combination of ¦Õ xi   1 ¡Ü i ¡Ü n. therefore  for any nf vectors z1 ... znf ¡Ê s  let z denote  z1 ... znf   and x denote  ¦Õ x1  ... ¦Õ xn    then z can be written as
	z = xw	 1 
where w =  wik  ¡Ê rn¡Ánf is a matrix of combination coefficients.
¡¡moreover  if z1 ... znf are linearly independent  then the nf dimensional subspace s can be spanned by these nf vectors. thus the elements of w introduce a subspace s of f  for which we have the following lemma.
lemma 1. when projecting the data ¦Õ xi  into s  the kernel matrix of the projected data in s can be computed as 1
	k	 1 
	kw 	 1 
where k =  kij  ¡Ê rn¡Án is the kernel matrix of the input data  i.e. kij = k xi xj .
proof. for any ¦Õ xi   1 ¡Ü i ¡Ü n  in order to calculate its projection into the subspace s  spanned by the columns of z = xw  we need an orthogonal basis u of s. we build u as follows:
	u = zt	 1 
in the above equation  t is computed as follows: assume kz = zz then
	t = v¦« 1	 1 
where ¦« ¡Ê rnf¡Ánf is a diagonal matrix of eigenvalues off f matrix kz  and v ¡Ê rn ¡Án is a matrix whose columns are eigenvectors of kz. equation  1  leads to
	k	 1 

     1more precisely  the result of equation  1  is the coordinate of the projection of ¦Õ xi  in s. as is widely done in the literature of feature extraction and dimensionality reduction  this coordinate will be used as the extracted features for classification.
     1here the  kernel matrix of the projected data  refers to the matrix whose elements equal the inner product of the projected data in s.
and
t i	 1 
where i is the unit matrix. the following equation follows from  1  and  1  
u i
so the columns of u form an orthogonal basis of the subspace
ssubspace.thus  fors can be computed as¦Õ xi  ¡Ê f  1 ¡Ü i ¡Ü n  their projections into the
	xx	 1 
where xw is the matrix whose columns are the projections of ¦Õ xi  in s  1 ¡Ü i ¡Ü n. xw  we can now com-
¡¡having obtained the projected data pute the inner product between points in the subspace as the following:
	kx	 1 
ztt z x
 1 
xw 
	kw 	 1 
where we used equation  1  in the second line  equation  1  in the third line and equation  1  in the fifth line. the equations  1  and  1  are identical to  1  and  1  respectively  therefore the lemma is proven. 
¡¡the proof also tells that for a given w  the projection of the data into the subspace s introduced by w can be computed as equation  1 .
¡¡let kw ¡¤ ¡¤  denote corresponding subspace kernel function. then according to  1  and  1   for any x   the subspace kernel kw ¡¤ ¡¤  between them can be computed as


where

is the empirical kernel map  scholkopf and smola  1¡§	 .
¡¡equation  1  illustrates that the elements of w  by which ters ofthe subspacekw   s . so in order to find a discriminating subspaceis described  also serve as the kernel paramewhere different classes of data are well separated  we can¡¤ ¡¤ sturn to optimize the corresponding subspace kernel kw.
1 connections to related work
1 feature selection via kernel parameter learning
in  weston et al.  1; chapelle et al.  1   kernel parameter learning approaches are adopted for feature selection problem. the kernel of the following form is considered
	k¦È u v  = k ¦È.   u ¦È.   v 	 1 
where .  denotes the component-wise product between two vectors. namely  for  and u =
. by optimizing
the kernel parameter ¦È with margin maximization or radiusmargin bound  chapelle et al.  1  minimization  and with a 1-norm or 1-norm penalizer on ¦È  feature selection can be done by by choosing the features corresponding to the large elements of the optimized ¦È.
¡¡feature selection locates a discriminating subspace of the input space x. similarly as the above approaches  we also use kernel parameter learning algorithms to find a discriminating subspace. however  in this paper  we address the problem of feature extraction but not feature selection  and the subspace we want to find is contained in the feature space f but not the input space x.
1 sparse kernel learning algorithms
the subspace kernel function given by  1  is in a general form. as described before  each column in the matrix z =  z1 ... znf   c.f  1   is a vector in the feature space f. now we show that this kernel relates to the work of  wu et al.  1  in the special case where each column of z has a preimage  scholkopf and smola  1¡§   in the input space x. that is  for each zi ¡Ê f  there exists a vector z i ¡Ê x  such that zi = ¦Õ z i . so now the subspace s can be spanned by ¦Õ z 1  ... ¦Õ z nf .
¡¡for convenience  let z  =  ¦Õ z 1  ... ¦Õ z nf    note that z  = z . then in this case  according to  1   the subspace kernel function now becomes:
 1 
where  and k.
¡¡in  wu et al.  1   an algorithm for building sparse large margin classifiers  slmc  is proposed  which builds a sparse support vector machine  svm   vapnik  1  with nf expansion vectors  where nf is an given integer. in  wu et al.  1   it is pointed out that building an slmc is equivalent to building a standard svm with the kernel function computed as  1 . and the slmc algorithm essentially finds an nf dimensional subspace of f  which is spanned by ¦Õ z 1  ... ¦Õ z nf   and where the different classes of data are linearly well separated.
¡¡in  wu et al.  1   the kernel function  1  is obtained with the lagrange method  which is different from the one adopted in the above. and the kernel function  1  is a special case of the subspace kernel  1 . therefore it can be seen that based on the general subspace kernel  1   useful special cases can be derived for some applications.
1 optimizing kw
we optimize kw based on the kernel-target alignment  kta   cristianini et al.  1   which is a quantity to measure the degree of fitness of a kernel for a given learning task. in particular  we compute w by solving the following kta maximization problem:
		 1 
w
wheredenotes the frobenius product between two matrices that are of the same size  i.e. for any two equally sized matrices m and n  . in  1  
ky ¡Ê rn¡Án is the gram matrix between the labels  defined by
	ky	 1 
where y =  y1 ... yn  ¡Ê rc¡Án  and yi is the label of xi  1 ¡Ü i ¡Ü n.
¡¡the elements in ky reflect the similarities between labels  as kyij equals 1 if xi and xj belong to the same class  and 1 otherwise. therefore 'aligning' kw with ky will make the similarities between the data points in the same class higher than the similarities between the points in different classes. thus by maximizing a w   we can find a subspace of f  where points in the same class are closer to each other than those in different classes. hence a good classification performance can be expected for the data projected into this subspace.
¡¡note that the subspace kernel allows us to apply many kernel parameter learning algorithms to the feature extraction problem. therefore apart from kta  we can also choose other approaches to compute w  such as the one based on the radius-margin bound  chapelle et al.  1 . for simplicity  we use kta in this paper.
¡¡gradient based algorithms can be used to maximize a w . in our implementation  we use the conjugate gradient algorithm to solve problem  1 . to compute a w   we utilize the fact that k  see  1   and y
 see  1  . thus  we can decompose kw and ky as follows
nf
	k	 1 

	k	 1 
j=1
where x i ¡Ê rn  1 ¡Ü i ¡Ü nf  denotes the i-th column of xw and y j ¡Ê rn  1 ¡Ü j ¡Ü c  denotes the j-th column of y. based on the above two equations  we have
 1 
 1 
¡¡equation  1  and  1  can be computed with time complexity o ncnf  and o nn1f  respectively. when both nf and c are small  they are more efficient than computing the frobenius product directly  which requires time complexity of o n1 .
¡¡similarly  to compute  a w   we can use the following equations:
w
 1 
 1 
where wuv  1 ¡Ü u ¡Ü n 1 ¡Ü v ¡Ü nf  is the element of
w. inspired by  1  and  1   we investigate how to compute
  where ¦Á ¡Ê rn is an arbitrary vector. actually  by performing linear algebra straightforwardly  we have
		 1 
where ¦Âv is the v-th element of a vector ¦Â  computed as
	kw 	 1 
and in  1   tu is the u-th element of a vector t  defined as: t = k¦Á   kw¦Â  1 
note that for any given ¦Á  the vectors ¦Â and t need to be computed only once  according to  1  and  1  respectively  then  can be calculated as  1  for 1 ¡Ü u ¡Ü n and 1 ¡Ü v ¡Ü nf. now we can apply  1  to  1  and  1   and  a w  can be calculated.
1 experimental results
1 experimental settings
we empirically investigate the performance of the following kfe algorithms on classification tasks: kpls  koc  akda/qr and the proposed subspace kernel based feature extraction  skfe  method. following the same scheme in  xiong et al.  1   the features extracted by each kfe algorithm are input to a 1-nearest neighbor  1-nn  classifier  and the classification performance on the test data is used to evaluate the extracted features. as a reference  we also report the classification results of the 1-nn algorithm using the input data directly without kfe.
¡¡as mentioned before  in a c-class classification problem  the number of features nf extracted by both akda/qr and koc is fixed at c. to compare with these two algorithms  the value of nf for skfe is also set to c in the experiments  although the number of features extracted by skfe can be varied. for kpls  three different values of nf are tried: c/1  c/1 and c. the best results are reported for kpls.1
¡¡for our proposed skfe algorithm  the function a w  in  1  is not convex  so the optimization result depends on the initial choice of w. to get a good initial guess  we can use the subspaces found by other kfe algorithms for initialization. in the experiments  for efficiency we use the koc algorithm to compute the initial w.

1when c = 1  only two values of nf are tried for kpls: 1 and
1.
1 experiments on microarray gene expression data
in this subsection  we take seven microarray gene datasets to test various kfe methods: brain tumor1  brain tumor1  leukemia1  leukemia1  prostate tumor  dlbcl and 1 tumors.1 descriptions of these datasets are presented in table 1. as shown in table 1  a typical characteristic of these datasets is that the number of data n is much smaller than the data dimensionality d.
table 1: datasets adopted in the experiments. the first seven are microarray gene datasets  while the last seven are text datasets. for each of them  the number of data n  the dimensionality d and the number of classes c are provided.

b.tumor1gene11b.tumor1gene11leukemia1gene11leukemia1gene11p.tumorgene11dlbclgene11 tumorsgene11tr1text11tr1text11tr1text11tr1text11la1text11la1text11hitechtext11a gaussian kernel is used in the experiments:
		 1 
¡¡five fold cross validation is conducted for parameter selection  and the best cross validation error rate is used to measure the performance of different algorithms. the experiment is repeated 1 times independently. and the results in table 1 show the mean cross validation error and the standard deviation over these 1 runs.
¡¡from table 1  we can observe that skfe and kpls compare favorably to the other kfe algorithms. in particular  skfe improves the results of koc algorithm in all cases  although koc is used to initialize skfe. it can also be seen that skfe and kpls are competitive with each other. they are are not significantly different  judged by t-test  on leukemia1  leukemia1  dlbcl and 1 tumors  and kpls is better than skfe on brain tumor1  while skfe outperforms kpls on brain tumor1 and prostate tumor.
1 experiments on text classification
in this subsection  we investigate different kfe methods on the text classification task. it has been observed that there usually exist cluster structures in the text data. the oc algorithm  or equivalently the koc algorithm with the linear kernel   which can keep these structures  is used for dimensionality reduction in text classification tasks in  kim et al.  1  and exhibits good results.
¡¡seven text datasets from the trec collections are adopted: tr1  tr1  tr1  tr1  la1  la1 and hitech. more information about these seven datasets are available at table 1.
¡¡similar to the microarray gene data  the data used in text classification tasks are also of very high dimensionality. another characteristic of these seven datasets is that they are highly unbalanced  which means that the number of data contained in different classes are quite different. for example  in the tr1 dataset  there are 1 data points contained in the seventh class  while just 1 data points in the ninth class  only 1% of the former.
¡¡on each dataset  we randomly select half of the data from each class to form the training set and use the remaining data for test. as is done in the oc algorithm  the linear kernel is used in this set of experiments. similarly as before  for each dataset  the experiment is repeated independently 1 times. the average test error and the standard deviation over these 1 runs are reported in table 1.
¡¡table 1 illustrates that skfe outperforms other kfe methods on most datasets. also it can be seen from both table 1 and 1 that in most cases  all the kfe algorithms obtain better performances than the 1-nn algorithm with the raw data  whilst reducing the data dimensionality dramatically from d to nf  where nf    d.  c.f. section 1 for the choice of nf.  although skfe compares favorably to the other kfe methods in terms of the classification accuracy  its computational cost is higher than the others. for the problems reported in table 1  on a 1 ghz pentium-1 pc  kpls requires from 1 to 1 seconds  akda/qr takes between 1 and 1 seconds  koc requires between 1 and 1 seconds  while skfe takes between 1 to 1 seconds. the optimization step of skfe is implemented in c++  and the others are implemented in matlab.
1 conclusion
we have presented a subspace kernel based on which nonlinear feature extraction can be conducted by kernel parameter learning. connections to related work have been explained. in particular  the comparison with the spare large margin classifier  slmc   wu et al.  1  illustrates that useful special cases can be derived from the proposed subspace kernel for some applications. we have also described a method to optimize the subspace kernel by kernel-target alignment  kta   cristianini et al.  1  maximization. but other kernel parameter learning approaches can also be applied. finally  experimental results have been provided to validate the effectiveness of our approach.
