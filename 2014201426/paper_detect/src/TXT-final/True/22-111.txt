 
　explanation-based learning  ebl  depends on the ability of a system to explain to itself  based on the domain theory  that a given training example is a member of the target concept. however  in many complex domains it is often intractable to do this. in this paper i introduce a learning technique called lazy explanation-based learning as a solution to the problem of intractable explanation process in ebl. this technique is based on the idea that when the domain theory is intractable  it is possible to learn by generalizing incomplete explanations and incrementally refining the over-general knowledge thus learned when met with unexpected plan failures. i describe a program that incrementally learns planning knowledge in game domains through lazy explanation-based learning. i present both empirical and theoretical evidence for the viability of lazy explanation-based learning. 
1 introduction 
explanation-based learning  ebl  systems learn by proving to themselves that a given training example is a member of a target concept. if they are successful in doing this  they generalize the instance to the class for which the same explanation holds  mitchell et al.  1  dejong and mooney  1 . 
　however  in many combinatorially explosive domains like chess and circuit design  it may not be possible to prove that an example is a member of a target concept even if the system has a complete and correct domain theory. this problem is called the intractable theory problem fmitchell et al.  1  tadepalli  1a . two-person games are an interesting domain where the explanation process is intractable because of the inherent uncertainty in the actions of the opponent. in particular  the complete explanation that a person has a forced win in two-person games involves exploring every possible action of the two players in the worst case. in fact  to learn only correct rules  even this much is not enough. since many opponent's moves which are not applicable in the example may be applicable for problems in the generalized position  it is necessary to specialize the concept so that a given strategy works for all possible moves the opponent could ever make  tadepalli  1b . assuming that the domain theory of the system simply consists of goals and primitive move definitions for each player  this involves exponential search even if the system is given a sequence of optimal moves played by the two players. in this paper  i propose a solution to the intractable theory problem and illustrate it using a working implementation. 
　the main idea in this paper is that when the domain theory is intractable  it is possible to learn by generalizing incomplete explanations  and to incrementally refine the over-general knowledge thus learned. in fact  most of the explanations that we give in everyday life are incomplete in the sense that they usually make a number of default assumptions  mccarthy  1 . e.g.  consider the kidnapping story in  dejong and mooney  1 . the explanation of kidnapping is simply that john  the kidnapper  figured that one could make money by kidnapping a rich person's daughter and demanding ransom. this explanation is so incomplete that it does not even consider issues like the personal safety of the kidnapper. however  considering all possible hypothetical scenarios is computationally prohibitive. our approach to this problem is to use the previously learned knowledge to reasonably constrain the hypothetical scenarios to a tractable minimum. incomplete explanations  when generalized  lead to over-general rules. if the personal safety of the kidnapper is ignored in explaining the above example  the learned rule does not check for possible escape of the victim by threatening the kidnapper with a gun. when and if that happens  the system encounters an unexpected plan failure  which gives it an opportunity to refine its plans  provided  of course  it survives the surprise! 
　the next section describes our knowledge representation  and the latter introduces our learning technique called lazy explanation-based learning through a program that learns planning knowledge in two person games. the program is illustrated using examples from king and pawn endgames in chess. a complexity analysis of our learning algorithm and some empirical results on our program arc presented next. section 1 describes some previous work related to lazy 
explanation-based learning  followed by a discussion of various tradeoffs involved in lazy ebl systems. the paper concludes with a summary and directions to future work. 
1 knowledge representation 
　*this work is supported by the national science foundation under contract number iri-1  and defense advanced research projects agency under contract number n1-k-1. the opinions expressed in this paper are those of the author and do not reflect any policies either expressed or implied of any granting agency. 
1 	machine learning a domain is described to the system in the form of a domain theory consisting of a set of goals for each player  a set of legal moves defined as strips-operators  a set of hornclauses which define the preconditions of operators in terms of more primitive predicates  and a set of operational 
predicates of which the preconditions of the learned rules are to be composed. 
　knowledge is represented by a set of inter-related goals and plans for each player. a goal consists of a definition expressed in a quantifier-free logic  a sign   and a number called promise that indicates the worth of the goal. associated with each goal  there may be a set of optimistic 
plans  or o-plans that can be used to achieve the goal. the body of an o-plan is a sequence of generalized moves  each move being preceded by the weakest conditions that must be true of a state so that the rest of the move sequence is applicable in that state  cf. table 1 . each move in the o-plan is associated with a player who makes that move. the order of moves in the o-plan is fixed  except that when two successive moves are to be made by the same player  it is assumed that there is an irrelevant move by the opponent between those two moves.1 all the moves in the o-plan are locally relevant in that each move either directly achieves a part of the goal or enables another move that follows it by satisfying some of its preconditions. 
　o-plans are related to other o-plans through sub-plan and counter-plan relations. an o-plan p is a sub-plan of another o-plan q  if  under some circumstances  p enables some conditions necessary for q. similarly  an o-plan p is a 
　counter-plan of qt if  under some circumstances  achieving p disables some conditions necessary for q. 
　to use an o-plan in planning  the free variables in the precondition of some suffix of the o-plan body must be instantiated. because of the inherent uncertainty introduced by the moves of the opponent in two-person games  in many positions a single o-plan is not adequate to achieve one's goals. so the planner combines o-plans into more complicated c-plans using plan combinators such as seq  and mesh. trivially  every instantiated o-plan is a c-plan. 
seq produces new c-plans by sequentially composing its component c-plans. mesh produces new c-plans by inter'eaving its component c-plans in all possible ways.  see .tadepalli  1  for details.  similar plan combinators are used in  bratko  1   and  campbell  1 . the complexity of a c-plan is the total number of o-plans in the c-plan. 
1 lazy explanation-based learning 
in this section  i introduce a learning technique called lazy explanation-based learning  which is based on the idea that it is much easier to produce an incomplete explanation in many domains than it is to produce a complete explanation. incomplete explanations  when generalized  give rise to over-general o-plans. in many cases  one might never need to revise the o-plans acquired in this fashion. however  in some cases  an o-plan might lead to an unexpected plan failure at which point the explanation is elaborated just enough to explain the failure  and a new counter-plan to the failed o-plan is learned. 
our method is embodied in a program called lebl  lazy 
explanation-based learner   which is implemented in two person game domains. our system has two main components: a learner and a planner  cf. figure 1 . in addition to the domain theory  the input to the learner consists of example games  i.e.  a board position and a sequence of moves played from that position. typically some goals of either player are achieved during the play. the output of the learner is a set of new o-plans and possibly new goals  and modifications to the old o-plans. the planner accepts a board position as input and outputs a  partial  solution tree for that position. the solution tree may be criticized by the teacher by playing with the system which helps the learner refine its o-plans. the learner calls the planner to explain the consequences of any alternative moves  i.e.  moves other than those input by the user  of the two players generated using the previously learned o-plans. 

1 learning o-plans from incomplete explanations 
a complete explanation that a board position is a forced win for a certain player  is a proof  or solution tree  that the min-max of that position evaluates to win. an incomplete explanation that a board position is a forced win for a player is a proof  partial solution tree  that the min-max of that position evaluates to win when the moves of the two players are restricted to some subset of all possible moves. an incomplete explanation is typically produced by considering only a few of all the possible moves at each intermediate state in the expansion of the state space. a crucial problem here is to decide which moves to consider at each node. our system assumes that apart from the moves actually input to the system  the only moves relevant are those that occur in the c-plans generated by the planner using previously learned o-plans; i.e.  the planner assumes that it knows all the o-plans necessary to explain all the relevant alternative moves by both the players. we call this assumption the omniscience assumption. this allows the system to limit its search only to what it reasonably expects with its current knowledge of o-plans  while considering more alternatives for both the players as it learns more o-plans. 
　the system is shown the chess position in figure 1 and the subsequent line of play by the two players resulting in white's queening a pawn. our top-level learning algorithm is described in table 1. lebl first checks that the move sequence is valid by proving that the preconditions of each move are satisfied when the move is made. the generalized versions of the proofs are also computed simultaneously in a manner described in  kedar-cabelli and mccarty  1 . going backward from the final state  it then checks whether any goals in the system are satisfied for either player during the play. in our example  white's goal of queening his pawn was satisfied in the final state. if there is already an o-plan to queen a white pawn in the system and if this move can be 
	tadepalli 	1 


interpreted as part of that  it makes that o-plan active. since currently there is no such o-plan in the system  it creates a new o-plan with the postconditions initialized to the goal. 
　each move m is tested to see if it enables the preconditions of any active o-plan  old or new. this is done by back-propagating the current preconditions of the active oplan across the m and testing whether there is any change. if m enables the precondition of a new o-plan  as in the case of white's move in this example  it is added to the o-plan along with the generalized back-propagated precondition. if the move is an expected move of an old o-plan p  nothing is done. if the move is not expected in p  but still enables it  sub-plan links are created from each o-plan p' which contain that move to p. if no o-plan contains m  a new goal is created with the preconditions of p  and a new o-plan p' is started for this goal. also  p' is made a sub-plan of p. 
　if a move has no effect on the preconditions of a new or old o-plan  that move is irrelevant to that o-plan  and is dropped from that o-plan. since none of black's moves in our example enables the preconditions of white's o-plan  except by yielding the turn to white   all moves of black are considered irrelevant. all moves of white are relevant since they either directly achieve the goal  white's c1 -  c1  or enable white's other relevant moves. 
　for each intermediate state in the move sequence  the learner calls the planner to search for any alternative plans that could have been used by either player leading to his goals. the planner works by alternately generating new cplans for each player and testing them against all the old c-plans  of complexity   a user-given parameter k  of the opponent. the generation consists of combining its current library of o-plans  the omniscience assumption  using the plan combinators seq and mesh. the testing consists of expanding the game tree to include the moves consistent with the two plans being tested and reevaluating its minmax. the planner switches sides from a player if it is successful in finding a c-plan that changes the min-max value of the position in that player's favor. it terminates when it fails to generate any new c-plans of a given maximum complexity  1 in our experiments  that can improve the min-max value of the position.  see  tadepalli  1  for more details on the planner.  after the planning is complete  the c-plans that occur in the solution tree of the two players are added to the corresponding active old plans. 

　in this example  since the system does not have any oplans in its ubrary at first  the planner returns with no additional moves explored. from this example  lebl extracts a simple plan of pushing a pawn through from the 1'th rank of any column to the 1'th rank  and queen. after the backpropagation  the generalized preconditions for each move are simplified. the simplification consists mainly of removing the redundant preconditions  partial evaluation  and sorting the preconditions according to a predefined predicate order for efficient matching. table 1 shows the o-plan pl  learned from the first example. 

the body of the o-plan pl  consists of three rules  each corresponding to the position of the pawn in each of the intermediate states in the example including the first state. the left hand side of each rule describes the weakest conditions under which the rest of the o-plan can be executed 

provided no other o-plan of the opponent interferes with it. e.g.  the first rule recommends pushing a white pawn in the fifth rank if the squares in the sixth  seventh and eighth ranks in the same file arc free. similarly  the second rule says that a white pawn in the sixth rank must be pushed if the corresponding seventh and eighth rank squares are free  and so on. 
1 refining over-general knowledge 
from our point of view  the most important thing to notice in the first example is that it is not fully explained how white could have won for all possible lines of play by black. as a result  the plan pl1 is over-general  and sometimes leads to unexpected failures as in the following example  figure 1 . failures present the system with opportunities to refine its over-general plans. the refinement occurs by learning a new o-plan for the opponent and storing it as a counter-plan to the original o-plan. 
1. back-propagates the negative goal across the input move sequence and learns a new o-plan for the opponent. in our example  the body of this new o-plan pl1  see table 1  consists of two moves: the first move is by white and it consists of pushing a pawn when there is a black pawn two ranks ahead in its left adjacent file. the second move consists of the black pawn taking the white pawn  in its right diagonal position . 
1. indexes the negative plan under the negative goal  and stores it as a counter-plan to the failed o-plan.  i.e.  the new o-plan of black  pl1  is stored as one of the counter-plans to pl .  
   interestingly  it proposes the plan  c1- c1 c1 - c1 cl - c1  instead! the problem is that the notion of symmetry is not captured in the way the rules are represented in the system  and thus the two pawn captures by the black pawn have syntactically different explanations. after a subsequent  similar learning experience  the system learns to prepare for this kind of pawn capture as well. 
	tadepalli 	1 

parts: one part is checking that the move sequence is correct and extracting any goals and/or plans present in the sequence  and the second part is exploring alternative moves by calling the planner. the complexity of the first part of the algorithm is given by 

　the planning cost at each intermediate state is  see  tadepalli  1  for details : 

　multiplying the above by s  the length of the input move sequence  and adding it to the cost of back-propagation  and simplifying  we have 

　in order to get some idea of the savings obtained in our system compared to an ebl system that learns from complete explanations  we compare its worst-case complexity to that of an algorithm. it should be remembered  however  that even an algorithm cannot be directly used to learn correct rules in games since the moves which arc not applicable in the example might be applicable in the generalization of the example  tadepalli  1b . however  the complexity of producing correct rules is at least as high as that of an  algorithm assuming that the program has no access to any other control knowledge. if the average branching factor of the game tree is b 1we have 

　since we assumed that the match costs in both the algorithms are of the same order of complexity  the major savings in lebl is going to come from the exponent 1k  as opposed to s in  in order to be able to find the solution that lebl's planner finds  the  program must at least search for a depth that lebl searches  which is 1lk  hence we can say that all other things being equal  lebl performs better than if the average length of the o-plans / is high. further  it helps if the total number of o-plans p  the average o-plan branching factor n  and the maximum allowed c-plan complexity k are low. the above comparison reveals that lazy explanation-based learning performs better than an 
ebl program based on  search if the natural distribution of problems in a domain is such that a high proportion of the problems are solvable by c-plans of low complexity built from a small library of long o-plans with a small branching factor. in the next section  this claim is supported empirically by comparing the number of nodes searched by our program to that of a program based on search. 
1 empirical results 
in order to compare the performance of lebl to that of a program based on  search  i implemented a separate program called abe  alpha-beta explainer   which accepts a board position  and a move sequence as input  builds a 
complete solution tree for that position using the algorithm  and outputs the number of nodes searched. abe's evaluation function is the sum of the promises of all the goals in its domain theory which are satisfied in a given position. it is given one of the longest paths in the solution tree of the input position as a training move sequence and uses it to order the moves in its search  so that the move in 
1 in our empirical experiments  b=1  and w=1 

the training sequence is tried first when applicable . abe limits its search to a fixed depth given by the length of the input move-sequence and is allowed to return any solution tree whose min-max value is at least as good as the value of the final state in the training sequence. the only advantage of lebl over abe is its knowledge of o-plans. 
　initially  lebl is given three goals for each player: queening a pawn which is worth 1   or -1 if it is a black's pawn   taking the opponent's king of worth 1 
 or -1  and taking the opponent's pawn of worth 1  or -1 . we then trained lebl on a set of 1 examples from the king and pawn endings. we input the examples one after another and recorded the number of states visited by the two programs. lebl learned a total of 1 o-plans and 1 new goals in this training session. after training  lebl is once again given the same set of training examples to see how much more search is involved in explaining the same examples when more o-plans are present. this time  lebl learned 1 new o-plans and no new goals. the new o-plans learned by lebl this time were for the goals learned in the first training session. we ran lebl a third time on the same set of training examples and lebl learned no new o-plans or goals. 
　the number of nodes searched by abe and in the three training sessions of lebl on the 1 examples are tabulated in table 1. ignoring the cost of matching the o-plans with problems  the number of nodes searched is a reasonable estimate of the search effort involved in learning. it is clear from the table above that lebl consistently searched one to two orders of magnitude less number of nodes than abe did on most of the problems tested. even when the search effort of lebl increased with the number of learned o-plans  it remained significantly less than the number of nodes searched by abe. surprisingly  in a few cases  e.g.  problem #1   the search decreased with learning more oplans. the reason for this is that when there is no o-plan present in the system to achieve a goal  it tries to combine a number of o-plans to achieve it. this causes additional search which is avoided by having an o-plan which directly achieves the goal. 
　while training  lebl is tested on a set of 1 new problems it has not been trained on  and each time  it is found to search at least an order of magnitude fewer nodes 

than the a-p search program. the number of errors as measured by an incorrect first move on the 1 test problems decrease gradually from 1 to 1  as lebl learns from the 1 training examples. these results support our claim that lebl performs reasonably well even by learning from incomplete explanations. it searches much fewer nodes than a program based on a-p search  and its errors on novel problems decrease gradually with learning. 
1 related work 
prior to this work   pitrat  1  and  minton  1j used methods similar to ebl to learn in game domains. pitrat' s program used procedurally encoded domain-specific heuristics to  simplify  the chess position before generalizing. minton's program avoided some complexity by learning only from those move sequences in which all the opponent's moves are  forced . minton found that the preconditions of the learned rules are too complex to be efficiently evaluated during the problem-solving  and suggested that one must try to learn rules that recommended plausible good moves rather than provably optimal moves. the work presented in this paper can be seen as an approach in that direction. 
　there are other ebl systems that make various simplifications and approximations to make explanations more tractable  ellman  1 chien  1  bennett  1|. ellman's program automatically makes assumptions like 
 ignore the history of the game  that greatly simplify the explanation. chien's program uses defeasible assumptions about the persistence of certain facts during action sequences to simplify the explanations and refines the learned rules when it is faced with plan failures. this approach appears similar to lazy ebl in many respects  one difference being that chien's program  unlike ours  is mainly intended for single agent planning domains. bennett's system deals with the intractability in mathematical reasoning by making simplifying approximations to mathematical formulae. while our work is consistent with all these approaches in general  it exploits a specific kind of simplification  that of giving an incomplete explanation  the extent of completeness being determined by the current knowledge of the system instead of by explicitly represented assumptions or simplification rules. 
　in  doyle  1   doyle presents a program that is able to repair inconsistent domain theories by reasoning and learning at multiple levels of abstraction. however  at each level of abstraction  the explanation is complete. unlike his program  lebl is not given any abstract version of the domain theory. the component of our program that learns from negative examples is similar in spirit to the programs described in  mostow and bhatnagar  1    gupta  1  and  rajamoney et al.  1 . when learning from failures  it is necessary to constrain learning in order to avoid learning too many uninteresting failure plans. lebl addresses this issue by learning from failures only when a previously learned o-plan fails to achieve its goal. 
1 tradeoffs in learning 
any learning system like ours must make several fundamental tradeoffs. one of them is the tradeoff between the learning time and the problem-solving  planning  time. the early ebl systems treated learning as forming schemata that can be directly instantiated during the problem-solving  dejong and mooney  1 . however  in complex domains like chess  it is simply not possible to learn a schema for every possible tactic  since that would require exorbitantly many schemata and many examples to learn them  instead  the problem-solver must be smart enough to flexibly apply its knowledge to the problem at hand. in our system  one way this tradeoff appears is as the question of when are o-plans composed. our system makes the choice of learning only o-plans and composing them as needed during planning  thus requiring less training at the cost of more planning. 
　another tradeoff our system exploits is between the learning effort for a single example and the number of examples needed to achieve a given competence. the conventional ebl systems completely explain each example  and hence spend more effort on each example. lazy ebl systems distribute the explanation effort over several examples  thus requiring many examples to converge to the complete explanation produced by an ebl system. however  lazy ebl performs better than an ebl system if the problem distribution is such that a rule learned from incomplete explanation is often adequate to make the correct predictions. 
　a third tradeoff  also discussed in  ellman  1   is between the tractability of planning and the accuracy of the results. as our system learns more o-plans  the search space of the planner increases  and planning becomes less tractable. however  as the search becomes more exhaustive  the number of errors in planning decrease which means that the accuracy of planning increases. thus  the system can be described as traversing the accuracy vs. tractability tradeoff curve in the increasing direction of accuracy. 
1 conclusions and future work 
in this paper  we showed that one way to solve the intractable theory problem is to generalize from incomplete explanations and to refine the overgeneral plans when faced with plan failures. we illustrated this technique with an implemented program in two-person games and presented some empirical and analytical results to bolster our claims. while the program is  in theory  general enough to accommodate any two-person games whose moves are describable as strips operators  it has only been tested in a simple version of king and pawn endgames of chess. we believe that this method can be generalized to single agent domains by mapping the game trees to and/or goal trees. 
　there are several open problems in our approach to intractable theory problem. one of the main problems is the expensiveness of match - an instance of the utility problem discussed in  minton  1 . we observed that much time is spent in lebl in matching problems with o-plans  and this could become a major source of inefficiency after a number of o-plans are learned. unfortunately  the match problem is np-hard  which means that there are no general ways of making it faster in the worst case. in  tambe  1   tarn be and rosenbloom address this problem by restricting the expressive power of learned rules. another promising approach  pursued in  flann  1   is to learn more abstract  and hence  easy to match  high-level control knowledge to guide search in the planning space. 
　the problem of generalizing number and the structure of the explanation appear rather acutely in our domain. e.g.  from the first example  a smarter program should be able to learn to queen a pawn from any rank. this is called the generalization-to~n problem. it appears that all the solutions proposed to solve this problem  see  prieditis  1   
  shavlik and dejong  1   and  cohen  1   can be implemented in the lazy ebl in a straight-forward way. 
there are also several theoretical questions related to the 
	tadepalli 	1 

lazy ebl. e.g.  how does the number of examples needed by a lazy ebl system relate to the number needed by a normal ebl program to achieve the same level of competence  is there a sense in which a lazy explanation-based learner may be said to converge  and if so  how many examples does it need to converge  we raised similar questions about ebl in  mahadevan et al.  1  and provided one possible way to answer them. we are looking for more comprehensive theoretical models that can answer some of these questions for lazy explanation-based learning. 
acknowledgments 
　i thank my advisor tom mitchell for encouraging me to work on this problem and for supporting and guiding me throughout this work. many helpful suggestions are made by neeraj bhatnagar  oren etzioni  haym hirsh  sridhar mahadevan  tom mitchell  jack moslow  lou steinberg  
milind tambe  ming tan  chris tong  and the anonymous reviewers of this paper. special thanks are due to jeff schlimmer whose detailed comments greatly improved the readability of the paper. 
