
determining the number of solutions of a csp has several applications in ai  in statistical physics  and in guiding backtrack search heuristics. it is a #pcomplete problem for which some exact and approximate algorithms have been designed. successful csp models oftenuse high-arity globalconstraints to capture the structure of a problem. this paper exploits such structure and derives polytime evaluations of the number of solutions of individual constraints. these may be combined to approximate the total number of solutions or used to guide search heuristics. we give algorithms for several of the main families of constraints and discuss the possible uses of such solution counts.
1 introduction
many important combinatorial problems in artificial intelligence  ai   operations research and other disciplines can be cast as constraint satisfaction problems  csp . one is usually interested in finding a solution to a csp if it exists  the combinatorial existence problem  and much scientific literature has been devoted to this subject in the past few decades. another important question  though not as thoroughly studied  is how many solutions there are  the combinatorial enumeration problem . our ability to answer this question has several applications in ai  e.g.  orponen  1   roth  1   darwiche  1    in statistical physics  e.g.  burton and steif  1    lebowitz and gallavotti  1    or more recently in guiding backtrack search heuristics to find solutions to csps  horsch and havens  1  kask et al.  1  refalo  1 . in this latter context  an estimation of the number of solutions is sought repeatedly as search progresses. the best strategy to keep the overall runtime low may be to trade accuracy for speed while computing these approximations.
¡¡the model of a csp is centered on the constraints  which give it its structure. this paper proposes to exploit this inherent structure to derive polytime approximations to the solution count of a csp by combining solution counts of component constraints. at the level of the individual constraint  consistency algorithms act on the domains of the variables in its scope  i.e. on which the constraint is defined  to filter out values which do not belong to any solution of this constraint  thus avoiding some useless search. for many families of constraints  all such values can be removed in polynomial time even though globally the problem is np-hard. the only visible effect of the consistency algorithms is on the domains  projecting the set of solutions on each of the variables. but a constraint's consistency algorithm often maintains information which may be exploited to evaluate the number of valid tuples. we intend to show that a little additional work is often sufficient to provide close or even exact solution counts for a constraint  given the existing consistency algorithm.
¡¡the global counting problem for csps  denoted #csp  is #p-complete even if we restrict ourselves to binary constraints and is thus very likely intractable. recent theoretical work  bulatov and dalmau  1  characterizes some tractable classes of csps for this problem but these are quite restrictive  especially for practical problems. the counting problem for boolean csps  #sat  is the best studied subclass  see e.g.  birnbaum and lozinskii  1  .
¡¡for binary csps  exponential time exact algorithms have been described  e.g.  angelsmark and jonsson  1  . for backtrack search   kask et al.  1  adapts iterative joingraph propagation to approximate the number of solutions extending a partial solution and uses its results in a valueorderingheuristic to solve csps  choosingthe value whose assignment to the current variable gives the largest approximate solution count. an implementation optimized for binary constraints performs well compared to other popular strategies.  refalo  1  proposes a generic variable-ordering heuristic based on the impact the assignment of a variable has on the reduction of the remaining search space  computed as the cartesian product of the domains of the variables. it reports promising results on multi-knapsack  magic square  and latin square completion benchmarks for  not necessarily binary  csps. the value- and variable-ordering heuristics described above rely on an approximation of the solution count. both can benefit from an improvement in the quality and/or efficient computation of this approximation.
¡¡in the rest of the paper  section 1 revisits several families of constraints to evaluate the effort necessary to providesolution
counts  section 1 explores possible uses for them  section 1 gives some examples  and section 1 discusses future work.
1 looking at constraints
this section proceeds through a short list of usual constraints  for their origin  see e.g. re¡ägin  1   and examines how solution counts may be derived. we assume throughout that the constraints are domainconsistent  unless stated otherwise. given a variable x  we denote its domain of possible values as d x   the smallest and largest value in d x  as xmin and xmax respectively  and the number of values in d x  as dx. we use #¦Ã to mean the number of solutions for constraint ¦Ã.
1 binary constraints
many arc consistency algorithms have been developed for binary constraints of the form ¦Ã x y . among them  ac-1  mohr and henderson  1  maintains a support counter for each value in d x  and d y . the total solution count is then simply the sum of those counters over the domain of one of the variables:
	#¦Ã x y 	= x sc x v 	 1 
v¡Êd x 
where sc x v  stands for the support counter of value v in the domain of variable x. however  other consistency algorithms only compute support witnesses as needed and hence would not have support counters available.
1 arithmetic binary constraints
typically  support counters are not maintained by the consistency algorithm for arithmetic binary constraints but one rather relies on the semantics of the constraint. take for example x   y: knowing xmin and ymax is sufficient to remove all the inconsistent values. to compute the number of valid couples we can simply enumerate them  which done naively would require   dx ¡¤ dy  time. provided the domains are maintainedin sorted order  a reasonableassumption  that enumeration can be done in ¦¨ dx + dy  time by using a running pointer in each domain as described in algorithm 1.
function ltcard x y 
let tx and ty be tables containing the values of the domains of x and y  respectively  in increasing order; := 1;
           y while p ¡Ü dx and tx p    ty q  do
p := p + 1;
c := c + p   1; q := q + 1;
return c;algorithm 1: computing the number of solutions to x   y.
	# x   y 	=	ltcard x y 	 1 
for each value v in d y   it finds out of the number of supports sc y v  from d x  and adds them up in c. we can make the algorithm incremental by storing sc y v  for each v in d y  and q w   the smallest q such that w   ty q   for each w in d x : if v is removed from d y   the total number of solutions decreases by sc y v ; if w is removed from d x   the total number of solutions decreases by dy  q w +1 and sc y v  is decremented by one for q w  ¡Ü v ¡Ü dy.
¡¡sometimes the domains of variables involved in such constraints are maintained as intervals. this simplifies the computation  for which we derive a closed form:
	ymax xmin	ymax xmax 1
# x   y =	x i	  x i
i=ymin xmin	i=1=ymax ¡¤  xmax + 1    xmin ¡¤ dy	 1 ¡¤  x1max + xmax + ymin1	  ymin   1 
¡¡clearly  what preceded equally applies to constraints x ¡Ü y  x   y  and x ¡Ý y  with small adjustments. counting solutions for equality and disequality constraints is comparatively straightforward  remember that domain consistency has been achieved :
	# x = y 	=	dx	 1 
	# x 1= y 	=	dx ¡¤ dy   |d x  ¡É d y |	 1 
with an appropriate representation of the intersection of d x  and d y   an incremental version of the count for x 1= y requires constant time.
¡¡in some applications such as temporal reasoning  constants are present in the binary constraints  e.g. x   y + t for some constant t . the previous algorithms and formulas are easily adapted to handle such constraints.
1 linear constraints
 many constraint models feature linear constraints of the form where   stands for one of the usual relational operators and the ai's and b are integers. bound consistency is typically enforced  which is a fast but rather weak form of reasoning for these constraints. working from domains filtered in this way is likely to yield very poor approximations of the number of solutions.
example 1 constraint 1+1 x1x1 = 1 with bound consistent domains d xi  = {1 1}  1 ¡Ü i ¡Ü 1 admits only seven solutions even though the size of the cartesian product is 1. since the value of any one variable is totally determined by the combination of values taken by the others  the upper bound can be lowered to 1 but this is still high.
¡¡these constraints are actually well studied under the name of linear diophantine equations and inequations. in particular   ajili and contejean  1  offers an algorithm adapted to constraint programming that produces a finite description of the set of solutions of a system of linear diophantine equa-
tions and inequations over n
¡¡when the constraints are of the form with the ai's and xi's belonging to n  we get knapsack constraints for which  trick  1  adapts a pseudo-polynomial dynamic programming algorithm. it builds an acyclic graph ghaii hxii b b¡ä of size o nb  in time o nbd   where d = max{dxi}  whose paths from the initial node to a goal node correspond to solutions. it is pointed out that enumerating solutions amounts to enumerating paths through a depth-first search  in time linear in the size of the graph.
n
# b¡ä ¡Ü xaixi ¡Üb =|{paths traversing ghaii hxii b b¡ä}|  1  i=1
interestingly  the approach could be generalized so as to lift the nonnegativity restriction on the coefficients and variables at the expense of a larger graph of size where r represents the magnitude of the range of pi=1 aixi over the finite domains.
1 element constraints
the ability to index an array with a finite-domain variable  commonlyknown as an element constraint  is often present in practical models. given variables x and i and array of values a  element i a x  constrains x to be equal to the ith element of a  x = a i  . note that this is not an arbitrary relation between two variables but rather a functional relationship. as a relation  it could potentially number dx ¡¤ di solutions  corresponding to the cartesian product of the variables. as a functional relationship  there are exactly as many tuples as there are consistent values for i:
	#element i a x 	=	di	 1 
¡¡this exact count improves on the product of the domain sizes by a factor dx.
1 regular language membership constraints
given a sequence of variables x = hx1 x1 ... xni and a deterministic finite automaton a  regular x a  constrains any sequence of values taken by the variables of x to belong to the regular language recognized by a  pesant  1 . the consistency algorithm for this constraint builds a directed acyclic graph gx a whose structure is very similar to the one used by  trick  1  for knapsack constraints  as reported in section 1. here as well  each path corresponds to a solution of the constraint and counting them represents no complexity overhead with respect to the consistency algorithm.
#regular x a 	=	|{paths traversing gx a}|	 1 
1 among constraints
under constraint among c x v    variable c corresponds to the number of variables from set x taking their value from set v  beldiceanu and contejean  1 . let r = {x ¡Ê x : d x    v }  the subset of variables required to take a value from v   and p = {x ¡Ê x   r : d x  ¡É v 1=  }  the subset of variables possibly taking a value from v . those two sets are usually maintained by the consistency algorithm since the relationship |r| ¡Ü c ¡Ü |r|+|p| is useful to filter the domain of c.
¡¡define ¦Ís as the number of variables from set s taking their value from v . to derive the number of solutions  we first make the following observations:
1. ¦Ír = |r|  a constant  so that in any solution to the constraint the value taken by x ¡Ê r can be replaced by any other in its domain and it remains a solution.
1. ¦Íx  r¡Èp  = 1  a constant  so that in any solution to the constraint the value taken by x ¡Ê x    r ¡È p  can be replaced by any other in its domain and it remains a solution.
1. any assignment of the variables of p such that ¦Íp = c   |r| of them take a value in v can be extended into qx¡Êx p dx complete solutions.
1. there are such assignments for every value of ¦Íp from cmin   |r| to cmax   |r| and for every subset s of p identifying the ¦Íp variables in question. how many assignments  given s  each variable y ¡Ê s has |d y ¡Év | possible values and each variable z ¡Ê p  s has |d z   v | possible values  totaling #a s  = qy¡Ês |d y  ¡É v | ¡¤ qz¡Êp s |d z    v | assignments.
putting it all together gives:
cmax
	#among c x v   = y dx ¡¤x x #a s 	 1 
x¡Êx p k=cmin|s|s= k p|r|
¡¡in the worst case  the number of s sets to consider is exponential in the number of variables. it may be preferable sometimes to compute faster approximations. we can replace every #a s  by some constant lower or upper bound. let tin be a table in which the sizes of d x  ¡É v for every x ¡Ê p appear in increasing order. similarly  let tout be a table in which the sizes of d x    v for every x ¡Ê p appear in increasing order. we define a lower bound
=1	=1
and an upper bound
.
then
cmax
	#among c x v  ¡Ý y dx ¡¤x  k |p|r| |  ¡¤   a k  	 1 
x¡Êx p k=cmin
cmax
#among c x v  ¡Ü y dx ¡¤x  k |p|r| |  ¡¤ u a k    1 
x¡Êx p k=cmin
note that if the variables in p have identical domains then the lower and upper bounds coincide and we obtain the exact count.
example 1 consider among c {x1 x1 x1 x1 x1} {1}  with d c  = {1}  d x1  = {1 1}  d x1  = {1}  d x1  = {1}  d x1  = {1}  and d x1  = {1 1}. we obtain r = {x1 x1}  p = {x1 x1}  and #among c {x1 x1 x1 x1 x1} {1}  = dx1 ¡¤ dx1 ¡¤ dx1 ¡¤

1 ¡¤ 1 ¡¤ 1 ¡¤  1 ¡¤ 1 + 1 ¡¤ 1 + 1 ¡¤ 1  = 1  by  1 . using  1   1  we get 1 ¡Ü #among c {x1 x1 x1 x1 x1} {1}  ¡Ü 1. in comparison  the size of the cartesian product is 1.
1 mutual exclusion and global cardinality constraints
besides among  other constraints are concerned with the number of repetitions of values. constraint alldiff x  forces the variables of set x to take different values. constraint gcc y v x   for a sequence of variables y = hy1 y1 ... ymi and a sequence of values v = hv1 v1 ... vmi  makes each yi equal to the number of times a variable of x takes value vi. it is a generalization of the former.
¡¡already for the alldiff constraint  the counting problem includes as a special case the number of perfect matchings in a bipartite graph  itself equivalent to the np-hard problem of computing the permanent of the adjacency matrix representation of the graph. so currently we do not know of an efficient way to compute #alldiff x  or #gcc y v x  exactly. a polytime randomized approximation algorithm for the permanent was recently proposed in  jerrum et al.  1  but its time complexity  in   n1   remains prohibitive. therefore even getting a reasonable approximate figure is challenging. we nevertheless propose some bounds.
upper bound
if d x    d y  for some variables x y ¡Ê x then regardless of the value v taken by x we know that at most dy   1 possibilities remain for y since v ¡Ê d y . we generalize this simple observation by considering d = {d x  : x ¡Ê x}  the distinct domains of x. for each d ¡Ê d  define ed = {x ¡Ê x : d x  = d}  the set of variables with domain d  and sd = {x ¡Ê x : d x    d}  the set of variables whose domain is properly contained in d. then
|ed|
	#alldiff x ¡Ü y y |d|   |sd|   i + 1 	 1 
d¡Êd i=1
computing it requires taking the product of n terms  each of which can be computed easily in o nm  time and probably much faster with an appropriate data structure maintaining the ed and sd sets. this upper bound is interesting because for the special case where all domains are equal  say
{v1 v1 ... vm}  it simplifies to the exact solution count 

lower bound
as hinted before  the consistency algorithm for alldiff computes a maximum matching in a bipartite graph. it is known that every maximum matching of a graph can be obtained from a given maximum matching by successive transformations through even-length alternating paths starting at an unmatched vertex or through alternating cycles.1 each transformation reverses the status of the edges on the path or cycle in question: if it belonged to the matching it no longer does  and vice versa. the result is necessarily a matching of the same size. finding every maximum matching in this way  or any other  would be too costly  as we established before  but finding those that are just one transformation away is fast and provides a lower bound on the number of maximum matchings  or equivalently on the solution count of alldiff.
¡¡by not applying transformations in succession  we also avoid having to check whether two matchings obtained by distinct sequences of transformations are in fact identical  which could happen. however  successive transformations from distinct connected components is safe: each resulting matching is distinct. we may therefore take the product of the number of maximum  sub matchings in each connected component of the bipartite graph. identifying the connected components c takes time linear in the size of the graph. enumerating all the appropriate cycles and paths of each connected component can be done in o nm1  time. let their number be ¦Íg for each connected component g ¡Ê c. then
	#alldiff x ¡Ý y ¦Íg + 1 	 1 
g¡Êc
¡¡for the gcc constraint  we may be able to adapt the ideas which gave rise to the previous lower bound. the consistency algorithm for this constraint is usually based on network flows. given a network and a flow through it  a residual graph can be defined and the circuits in this graph lead to equivalent flows. another idea is to decompose the constraint into among constraints on singleton sets of values and use  1 - 1 .
1 using solution counts of constraints
this section examines the possible uses of solution counts of constraints.
1 approximate the solution count of a csp
they can be used to approximate the number of solutions to a csp. given a model for a csp  consider its variables as a set s and its constraints as a collection c of subsets of s defined by the scope of each constraint. add to c a singleton for every variable in s. a set partition of s is a subset of c whose elements are pairwise disjoint and whose union equals s  see section 1 for some examples . to each element of the partition corresponds a constraint  or a variable in case of a singleton  for which an upper bound on the number of solutions can be computed  or taken as the cardinality of the domain in case of a singleton . the product of these upper boundsgives an upperboundon the total numberof solutions. in general there are several such partitions possible and we can find the smallest product over them. if it is used in the course of a computation as variables become fixed and can be excluded from the partition  new possible partitions emerge and may improve the solution count.
1 guide search heuristics
as we saw in the introduction  these counts are also useful to develop robust search heuristics. they may allow a finer evaluation of impact in the generic search heuristic of  refalo  1  since their approximation of the size of the search space is no worse than the cartesian product of the domains. value-ordering heuristics such as  kask et al.  1  are also good candidates. other search heuristics following the firstfail principle  detect failure as early as possible  and centered on constraints can be guided by a count of the number of solutions left for each constraint. we might focus the search on the constraint currently having the smallest number of solutions  recognizing that failure necessarily occurs through a constraint admitting no more solutions  see section 1 .
1 evaluate projection tightness
we can also compute the ratio of the  approximate  solution count of a constraint to the size of the cartesian product of the appropriate domains  in a way measuring the tightness of the projection of the constraint onto the individual variables. a low ratio stresses the poor quality of the information propagated to the other constraints  i.e. the filtered domains  and identifies constraints whose consistency algorithm may be improved. tightness can also serve as another search heuristic  focusing on the constraint currently exhibiting the worst tightness  i.e. the smallest ratio . an ideal ratio of one corresponds to a constraint perfectly captured by the current domains of its variables.
1 some examples
the purpose of this section is to illustrate solution counts and their possible uses through a few simple models  easier to analyse.
1 map coloring
consider the well-known map coloring problem for six european countries: belgium  denmark  france  germany  luxembourg  and the netherlands. any two countries sharing a border cannot be of the same color. to each country we associate a variable  named after its first letter  that represents its color. constraints
f 1= b f 1=   f 1= g   1= g   1= b b 1= n g =1 n g =1 d g 1= b
model the problem. if we allow five colors  there are 1 legal colorings. the cardinality of the cartesian product of the domains of the variables initially overestimates that number to be 1  1 . a set partition such as {f 1=   b 1= n g 1= d} provides a slightly better estimate  using  1 :
 dx ¡¤ dy   |d x  ¡É d y | 1 =  1 ¡¤ 1   1 = 1.
noticing that four of these countries are all pairwise adjacent  an alternate model is alldiff {b f g  }  b 1= n g 1= n g 1= d
yielding a still better upper bound on the solution count  1! ¡¤ 1 = 1  using  1  with set partition {alldiff {b f g  }  n d}. we see here that the solution counts of the constraints in a csp model  particularly those of high arity  can quickly provide good approximations of the number of solutions of csps. the projection tightness of b 1= n  g 1= n  and g 1= d is 1 = 1; that of alldiff {b f g  } is 1!/1 = 1. this could be taken as an indication that search should focus on the latter.
¡¡let us now analyse the effect of coloring one of the countries red. regardless of the country we choose  the number of legal colorings will decrease five fold to 1 since colors are interchangeable. the true impact of fixing a variable  measured as 1 minus the ratio of the remaining search space after and before   refalo  1    is thus 1. table 1 reports after each country is colored red the cardinality of the cartesian product of the domains  column 1   the upper bound on the solution count from each of the two set partitions  columns domains	1st partition	1nd partition
var	solns	impact	solns	impact	solns	impact
 1.1.1.1f1.1.1.1b1.1.1.1g1.1.1.1n1.1.1.1d1.1.1.1avg-.1-.1-.1table 1: comparing different approximations of the number of solutions of a small map coloring problem.
1 and 1   and their corresponding impacts  columns 1  1  and
1 .
¡¡again we note a significant improvement in the approximation of the number of solutions over the cartesian product  particularly from the second set partition based on the model using an alldiff constraint. the latter also provides a closer approximation of the true impact: the average computed impact is less than 1% away whereas it is more than 1% away for the average computed impact using cartesian products.
1 rostering
consider next a simple rostering problem: a number of daily jobs must be carried out by employees while respecting some labor regulations. let e be the set of employees  d the days of the planning horizon  and j the set of possible jobs  including a day off. define variables jde ¡Ê j  d ¡Ê d e ¡Ê e  to represent the job carried out by employee e on day d. to ensure that every job is performed on any given day  we can state the following constraints:
gcc h1 ... 1i j   {day off}  jde e¡Êe 	d ¡Ê d.  1 
some jobs are considered more demanding than others and we wish to balance the workload among employees. we introduce variables wde representing the workload of employee e on day d and link them to the main variables in the following way:
	element jde   j j¡Êj wde 	d ¡Ê d  e ¡Ê e  1 
where  j corresponds to the load of job j. we then add constraints enforcing a maximum workload k:
	pd¡Êd wde ¡Ü k	e ¡Ê e.	 1 
finally work patterns may be imposed on individual rosters: regular  jde d¡Êd a 	e ¡Ê e	 1  with the appropriate automaton a.
¡¡to approximate the number of solutions  there are a few possibilities for a set partition of the variables  jde d¡Êd e¡Êe and  wde d¡Êd e¡Êe: constraints  1  1  are one  constraints  1  1  are another  and so are constraints  1 . in deciding which one to use  its size  the number of parts   the quality of the bounds  and the projection tightness of each constraint may help. partition  1  has size |d| ¡¤ |e| compared to |d| + |e| and 1 ¡¤ |e| for the other two: a smaller size means larger parts and probably a better overall approximation since each solution count has a more global view. we presented exact counts for every constraint used here except  1   making partition  1  1  less attractive. a set partition that includes constraints with low tightness  which is difficult to assess here without precise numbers  is more likely to do significantly better than the size of the cartesian product of the individual domains  which can be considered a baseline.
1 discussion
this paper argued that looking at the number of solutions of individual constraints in csps is interesting and useful. it can approximate the number of solutions as a whole or help guide search heuristics. efficient ways of counting solutions were given for several families of constraints. the concepts of set partition over the variables and of projection tightness were defined in order to combine solution counts and measure the efficacy of consistency algorithms  respectively.
¡¡but this is a first step and many questions remain. some of the main families of constraints have been investigated but others were left out: for example  those useful in scheduling or packing problems such as cumulative and diffn  or those for routing problems such as cycle. how close can we get to the solution count for them and at what computational cost  for the simplest constraints that we investigated  we mentioned how the algorithms computing solution counts could be made incremental. this is an important issue to achieve efficiency when such counts are computed repeatedly as in backtrack search. what can be done for the other constraints  the whole question of the usefulness of such an approach hasn't been settled either. the answer is likely to come  at least in part  from empirical evidence. computational experiments will need to be run on larger and more realistic models.
acknowledgements
philippe refalo's work on impact-based search sparked the idea for this paper. i thank jean-charles re¡ägin for discussions and the anonymous referees for their constructive comments. this work was partially supported by the canadian natural sciences and engineering research council under grant ogp1.
