 
   an interpreter for logic programs is defined which executes some goals in parallel. or parallelism exploits the parallelism defined from nondeterministic choices  and is essentially a replacement for backtracking. and parallelism comes from solving goals in the body of a single clause in parallel  and is the only way to exploit parallelism in deterministic functions written as logic programs. a unique feature of our model is that it allows both forms of parallelism for the same computation. 
1  introduction 
   in this paper we present a method for exploiting and parallelism within the context of the and/or process model   1   . this paper focuses on the division of a logic program into pieces that can be executed in parallel. we are concerned with the management of abstract processes -- their creation. termination  and communication. we postpone dealing with the significant problems of effective distribution of processes to processing elements  pes  and the physical structure of the interconnection of pes and the resulting communication times. at this point  we assume only that each pe w i l l have a large amount of local memory  sufficient to store some number of processes and its own  unchanging  copy of the entire program. a more complete description of the and/or process model and and parallelism can be found in conery's dissertation   1   . 
1
   this research was supported by the naval ocean systems center under contract n1-c-1. 
1. sources of parallelism 
   a logic program is a set of clauses of the form p1  - p1   . . . a p n   where each pi is a l i t e r a l . the literals p i # i   1. form the body of the clause and the literal pq is the head of the clause. a computation in a resolution based logic programming system is a sequence g1 . .   gm. of goal statements. each of which is a set of l i t e r a l s . the derivation of statement gi+1 from gi requires two choices: f i r s t   select a l i t e r a l pg in gi. then find a clause which has a head that can be unified with pg using unifier 1. statement g i + 1 is the resolvent of gi and the selected clause  and contains a l l literals  except p   from gi. plus the literals from the body of the selected clause   1   . 
　　normally  the choice of pg is fixed. for the second choice  if there is more than one clause with a head that unifies with p . then the current statement is a choice point  and a tree of goal statements is defined. the typical sequential control in logic programming systems  e.g.   is a depth f i r s t search for an occurrence of the null clause in this tree. 
   one possible parallel control method is a parallel search of the tree: at each choice point. distribute the goal statements at descendant nodes to interpreters running on independent computers. the expected speedup in execution w i l l be obtained if one of these interpreters derives the null clause more quickly than an interpreter that performs a simple depth f i r s t search. the amount of time required 
 ignoring inter-computer communication times  w i l l be directly proportional to the length of the shortest path in the search tree from the root to a null clause. the amount of time required by a depth f i r s t interpreter is proportional to the sum of path lengths in a l l branches to the left of the f i r s t branch that contains the null clause. 

1 j. conery and d. kibler 
　　the search tree for a deterministic program has exactly one null clause* which is typically at the end of the leftmost branch. parallel search cannot speed up the execution of one of these programs. an interpreter based on and parallelism is able to effectively shorten the length of this chain of derivations* and as a result obtains the same sort of parallelism for deterministic logic programs that is seen in other functional programming languages   1   . the outline of one such interpreter is presented here* in the context of the and/or process model. 
1. the amd/or process model 
　　our parallel model is based on the notion of independent processes which communicate via messages. an or process is basically an independent interpreter  created to solve a goal statement that contains exactly one l i t e r a l . when solutions are obtained  they are communicated to the process that created the or process in a success message. for example* if the l i t e r a l to be solved is * sum 1.x '  the response w i l l be 'success bum 1 1  '. if no solutions are possible  or after a l l solutions have been sent* the or process sends a f a i l message. 
   an and process is created to solve a conjunction of goals p1 a       a pn  where n   1. when a l l of the goals in the conjunction have been solved  the and process sends its creator a 
   success message; if one of the goals cannot be solved then a f a i l message is sent. 
　　an and process solves the individual goals in its conjunction by creating or processes for each one. an or process solves its l i t e r a l l by  1  creating and processes to solve the bodies of nonunit clauses with heads that are unifiable with l  or  1  sending 'sucess c1   when l unifies with a unit clause c. 
　　the relationship between processes defines an and/or tree. the user's original goal statement is an and process at the root of the tree. success and f a i l messages pass only upward in the tree* from a process to its parent. messages that can be sent from a process to one of its offspring are redo  reset  and cancel. a redo message instructs the descendant to re-solve its problem. if there is another solution  a success message  presumably with a different set of bindings for variables  w i l l be sent. if there is not another solution* a f a i l message is returned. a reset message sent by an and process to an or descendant tells the or process to start over; the or process w i l l retransmit a l l previous answers in response to redo messages. a cancel message indicates that the descendant is no longer needed  and that it should terminate. 
   by or parallelism we mean the parallel execution of one or more children of an or process  i.e. or parallelism is the simultaneous execution of several clause bodies where the heads match a single goal. a straightforward 
 sequential  implementation of an and process just solves the goals left to right: when the or process for l i t e r a l pi sends a success message  then an or process w i l l be created to solve l i t e r a l pi+1 if pi cannot be solved   i . e . a f a i l message is received  then the process for l i t e r a l p i   1 is sent a redo message. and parallelism is the parallel execution of one or more children of an and process  i.e. a parallel and process creates more than one or process simultaneously. the remainder of this paper describes our current implementation of and parallelism. 
   note that our definition of or parallelism  which originally appeared in   1     is not the same as that used by some other authors  for example   1     . what they refer to as or parallelism is the parallel search described in an earlier section and called goal list parallelism 
in   1   . 
1. amd parallelism 
   a guiding principle in our work is that the programmer should not be concerned with the details of parallel interpretation. this implies that the programmer should not be forced to annotate his program with control information* as is done in ic-prolog   1   . moreover he is guaranteed that the answers produced by parallel interpretation include those achieved by depth f i r s t interpretation. 
   there are three independent components in our implementation: an ordering algorithm determines 
which literals must be solved before others  and which can be solved in parallel; the forward execution component that handles success messages  and decides if other literals can be solved as a result; and the backward execution component that handles f a i l and redo messages* and decides which literal s  from the body must be re-solved. 

1 . 1 . ordering algorithm 
   the only constraint on whether or not two literals can be solved in parallel is the sharing of uninstantiated variables. if a set of literals have an uninstantiated variable v in common* one of the literals is designated as the generator of values for v* and is solved f i r s t . its solution is expected to instantiate v. when the generator has been solved* the other literals* the consumers. may be scheduled for solution. if a generator instantiates a variable to a non-ground term  one that contains new variables * then the ordering algorithm must be applied again in order to find generators for the new variables. 
   the execution order of the literals* as determined by the ordering algorithm* can be expressed as a dataflow graph   1   . there is a node in the graph for every l i t e r a l * including the head. an arc* labeled with a variable name* is drawn from every generator to literals that consume the variable. 
   the ordering algorithm uses three rules when deciding which l i t e r a l should be designated as the generator of a variable. mode declarations may indicate that a certain l i t e r a l must be or cannot be a generator. for example* using the syntax of the dec-1 prolog compiler   1   . the predicate 
 	!
mode sum +*+ -   	means 	that 	the 	f i r s t 	two 
arguments to a call to 'sum' must be terms or instantiated variables* and that the third argument must be an uninstantiated variable. thus* when the ordering algorithm encounters a l i t e r a l 'sum 1 x y '  for example* it infers that this l i t e r a l cannot be the generator of x and must be the generator of y. the interpreter has mode declarations built in for some of the known evaluable predicates  primitive procedures  and the user can add mode declarations to his program if he wishes. 
   the connection rule states that if there is a l i t e r a l p x y  in the body* and a generator has already been found for the variable x* then p x y  is a good candidate to be the generator of y. this is just one of many possible heuristics* or general policies* that can be used to select generators. other heuristics are described in  and . 
   the last rule is that the leftmost l i t e r a l that contains an uninstantiated variable should be the generator of that variable; this rule is used if the other two f a i l to identify a generator. 
	j. conery and d. kibler 	1 
the dataflow graphs shown in figures 1 and 1. note that except for evaluable predicates or user predicates with mode declarations* any literal can be a generator. the ordering algorithm is used primarily to ensure that mode constraints are not violated* and secondarily to produce an efficient ordering. 
1. forward execution 
   after the ordering algorithm creates an ordering of literals* an and process enters the forward execution phase. forward execution is a graph reduction algorithm: when a l i t e r a l is resolved away from the body of a clause   i . e . when the and process receives a success message from the or process created to solve the l i t e r a l   * the corresponding node and a l l of the arcs leaving it are removed from the dataflow graph. 
   recall that the only arcs in the graph are those that connect generators to consumers. an or process w i l l be created for a l i t e r a l only when there are no arcs leading in to the corresponding node in the graph* i.e. when a l l preceding generators have been solved. finally* when a generator creates a non-ground term  containing a new variable v' * the ordering algorithm must be invoked again* so that a generator is determined for v. note that this w i l l add arcs to the graph* and may delay solution of some literals that contained the original variable v. 
1. backward execution 
   when an and process receives a f a i l message from an or process* it must re-solve some previously solved l i t e r a l . the last parent  as defined by a linear ordering of literals* obtained by a depth f i r s t traversal of the dataflow graph  for the failed process is sent a redo message. if an or process with no parents fails* the and process itself f a i l s . 
   to be more specific* consider the clause body gl x  a g1 y.z    p x.y  a q y  where gl x  is the generator of x and g1 y z  is the generator of y. when p x.y  fails* the or process for g1 y z  is sent a redo message. 
an 	implementation of 	this 	algorithm produced 	a more complicated 	case arises when the or    in a simple case  that process w i l l return another success message  with a different value for y  and the and process can resume forward execution. note that the process for q y  must be canceled when p x y  fails* and replaced when the new value of y it generated. 
1 j. conery and d. kibler 
process for g1 y z  fails  signaling that there are no more ways to instantiate y. at f i r s t it would appear that the and process should f a i l   since there are no predecessors for g1 y z  in the dataflow graph. however  since p x y  may succeed with a different value for x. the appropriate action is to send a redo to the process for gl x  and to reuse a l l values of y by sending a reset message to g1 y z . in other words  p x y  has to be tested with a l l possible combinations of x and 
y. 
　　the method for deciding which generat1r s  must be redone. and in what order  is quite complicated. the and process maintains for each l i t e r a l a l i s t   called a redo l i s t   containing a l l of the l i t e r a l ' s ancestors in the dataflow graph. a failure context is used to determine which of the ancestors in a redo l i s t have in fact been sent redo messages. the and process must keep multiple failure contexts  for those situations when a number of literals f a i l * and redo messages have to be sent to ancestors of each failed l i t e r a l   1   . 
1. example computations 
   we illustrate and parallelism by describing the execution trace printed by our interpreter as it solved two sample problems. 
   fast factorial is an example of a deterministic function  where the subgoals w i l l not f a i l * and the backward execution component is not required. the map coloring problem  used by pereira and 
porto to illustrate intelligent backtracking   is a problem that does require backward execution. 
1 . 1 . fast factorial 
　　the fast factorial program  figure 1  typifies the divide and conquer approach to solving large problems. the procedure is called with two integers  named h and l  as inputs* and instantiates f to the result. note that the head of the clause is the generator of h and l and the consumer of f. the divide and conquer version of this function is to compute the midrange between h and l  find the products f1 and f1 of a l l integers in these smaller ranges  and then multiply the results. as the graph shows  the computation of f1 and f1 can proceed independently* as soon as the midpoints are computed. 
   a depth f i r s t interpreter must solve the subproblems sequentially. this is an example of a deterministic function for which the only answer is found at the end of the leftmost branch in the 

figure 1: dataflow graph for fast factorial 
search tree  and thus an example of a program for 
which a parallel search w i l l not gain any speedup over a depth f i r s t search. in our interpreter  however  the recursive calls are solved by independent interpreters  or processes   and the time complexity on a multiprocessor  under the right circumstances  could be reduced to 1 log 1 t from 1 t   where t is the number of steps taken by a sequential interpreter. 
1. map coloring 
   a map coloring problem  its representation as a logic program* and the dataflow graph produced for it are shown in figure 1. the forward execution phase of the computation was: 'next a*b v is the only node without incoming arcs  so it was the only one for which an or process was created in the f i r s t step. as soon as  next a b ' was solved  processes to solve   next a c '  
v	v	f
next a'd  * and  next b*e  were created. 	when 
 
next a c ' 	succeeded 	 instantiating 	c 	to 	a 
color   a process for 'next b c ' was created to verify the values of b and c. similarly  after ''next a*d ' and 'next b e ' were solved' processes for the remaining three literals were created. 
the generators for b and c instantiated those 


     figure 1: dataflow graph f o r map coloring v a r i a b l e s to the same color  so the process f o r 
 
next b c ' 	f a i l e d   	thus 	t r i g g e r i n g 	backward 
execution. the redo l i s t  sorted l i s t of ancestors  for t h i s l i t e r a l i s 
  n e x t   a   c     n e x t   a   b     . so the process for 'next a c ' was sent a redo message* in order to get a d i f f e r e n t value f o r c. since the l i t e r a l s  next c d  f and 'next c e  v also consume c1 the corresponding processes were canceled. when the process f o r v next a c  v sent the next success message* processes for the three l i t e r a l s that consume c were started  using t h i s new v a l u e     and a l l four l i t e r a l s at the bottom of the graph were solved s u c c e s s f u l l y . 
　　in t h i s example the and process behaved very much l i k e the i n t e l l i g e n t backtracking i n t e r p r e t e r of pereira and porto* in general  however  our backtracking algorithm w i l l not be as smart as the t h e i r s   since ours must be executed by a number of processes as opposed to a single process w i t h global i n f o r m a t i o n   and our f a i l messages contain no information about xhg an or process f a i l e d . 
j. conery and d. kibler 1 
1. 	conclusion 
　　we have o u t l i n e d an algorithm f o r the p a r a l l e l execution of the body of a clause. the algorithm r e l i e s on the use of generators to order the s o l u t i o n of l i t e r a l s in a clause body so that a d a t a f l o w - l i k e computation is performed. the algorithm allows generators to be nondeterministic  meaning they can produce a 
　　number of d i f f e r e n t answers. a major source of complication is in the coordination of these nondetermini1tic generators   1   . 
