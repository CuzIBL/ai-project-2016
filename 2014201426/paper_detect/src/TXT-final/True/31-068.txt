 
novelty detection techniques are conceptlearning methods that proceed by recognizing positive instances of a concept rather than differentiating between its positive and negative instances. novelty detection approaches consequently require very few  if any  negative training instances. this paper presents a particular novelty detection approach to classification that uses a redundancy compression and nonredundancy differentiation technique based on the  gluck & myers  1  model of the hippocampus  a part of the brain critically involved in learning and memory. in particular  this approach consists of training an autoencoder to reconstruct positive input instances at the output layer and then using this autoencoder to recognize novel instances. classification is possible  after training  because positive instances are expected to be reconstructed accurately while negative instances are not. the purpose of this paper is to compare hippo  the system that implements this technique  to c1 and feedforward neural network classification on several applications. 
1 	introduction 
many practical applications of supervised learning are concept learning problems  that is  problems that involve 
discriminating instances according to whether or not they belong to a given class. this class can be thought of as the concept to be learned. usually  concept learning involves learning correct classification of a training set containing both positive and negative instances of a concept  followed by a testing phase in which novel examples are classified. good performance often depends on constructing training sets which contain a broad range of positive and negative examples. many classification methods have been designed under these conditions  including c1  quinlan  1  and backpropagation ap{ lied to a feedforward neural network  ff classification  rumelhart  hinton  & williams  1 . 
center for molecular and behavioral neuroscience 
1 	connectionist models 
catherine myers and mark gluck 
aidekman research center* rutgers university 
newark  new jersey 1 
{myers gluck} pavlov. rutgers.edu 
　as an alternative to learning a concept using a broad range of positive and negative training examples  some concept-learning techniques that require mainly-or only-positive training examples have recently been introduced  petsche & gluck  1 . such techniques are grouped as novelty detection methods  the term stems for the fact that negative inputs are recognized as being novel compared to positive inputs which are more familiar as they belong to the class that was used for training . novelty detection techniques proceed by examining instances of a concept  trying to find their commonalities and generalizing from them. these techniques differ from more conventional classification approaches  c1  ff classification  in that they attempt to recognize instances of a concept rather than to differentiate between instances of both classes. 
　the advantage of novelty detection approaches is that they can be used on problems that cannot easily be addressed by more conventional approaches. such problems are those for which negative examples are very expensive or difficult to obtain. in machine fault diagnosis  for example  positive examples are plentiful and typically involve recording from the machine during normal operation. negative examples  however  involve causing the machine to break down in each manner in which future failure is possible so that a recording can be made of each failure type. monitoring tasks which consist of examining a system's readily available signals and issuing an alarm when a potential problem is detected fall in this category of problems. the need to monitor a system's operation arises frequently and the development of reliable novelty detection techniques would provide important benefits for critical military and commercial systems  e.g.  helicopter gearboxes  shipboard fire pumps  motors  and generators . novelty detection methods  therefore  also need to demonstrate a certain level of reliability. the purpose of this paper is to introduce a particular novelty detection technique-redundancy compression and non-redundancy differentiation- and demonstrate experimentally that  in addition to requiring fewer negative examples  this technique is able to classify novel examples more accurately than the conventional classification approaches. 
　the particular novelty detection technique introduced in this paper uses an autoencoder  hinton  1 . an autoencoder is a neural network which learns to map from its inputs  through a narrow hidden layer  to output nodes which attempt to reconstruct the input. because the network has a narrow hidden layer  it is forced to compress redundancies in the input while retaining and differentiating non-redundant information. to implement our technique  the network is trained to reconstruct as well as possible a training set consisting of positive examples only. after having been trained on positive instances of the concept  the autoencoder should be able to adequately reconstruct subsequent positive instances  but should perform poorly on the task of reconstructing subsequent negative instances of the concept which present different structural regularities. identifying positive and negative instances of a concept is therefore equivalent to assessing how well such instances are reconstructed by the autoencoder. these same processes have previously been proposed to model computations occurring in the hippocampus  gluck & myers  1   a part of the brain involved in learning and memory. 
　this paper presents hippo  a concept-learning system based on this idea and assesses its performance by applying it to three real-world problems: ch1 helicopters gearbox fault detection  recognition of promoter regions of dna  and classification of sonar targets; hippo's error rates on these applications are then compared to those produced by c1 and ff classification. the next section describes hippo  including its redundancy compression and non-redundancy differentiation component and its threshold-determination component. the following section discusses the testing of hippo and its results. 
1 hippo 
implementing the idea of redundancy compression and 
non-redundancy differentiation results in a device able to issue two sorts of signals: one for recognized data  and an other for novel data. designing such a device  however  is not sufficient for building a powerful  standalone concept-learning system: one also needs to be able to discriminate between  recognized  and  novel  signals. such discrimination is called threshold determination. this section first describes hippo's redundancy compression and non-redundancy differentiation component  and then discusses its threshold determination component. the last part is an overview of the overall functioning of the system. 
1 	redundancy compression and 
non-redundancy differentiation 
redundancy compression and non-redundancy differentiation is a process believed to occur in the hippocampus  gluck & myers  1 . gluck and myers have used connectionist models to study the function of the hippocampal region in the brain and its implications for learning and memory behaviors. they suggested that the hippocampus forms new stimulus representations during learning  and that these representations specifically compress redundant information while preserving or differentiating non-redundant information. the system they built based on these considerations was able to 

figure 1: an autoencoder with i = 1 = 1 and h = 1. 
accurately predict a range of classical conditioning behaviors observed in normal and hippocampal-damaged animals  gluck & myers  1 . because these compression and differentiation constraints appear to be useful during learning in the brain  gluck and myers suggest that they may be useful for machine learning tasks as well. 
　the redundancy compression and non-redundancy differentiation technique uses an autoencoder  of the type that was proposed by  hinton  1 . an autoencoder is an artificial neural network composed of a given number  1  of input nodes; the same number of output nodes  1; and a given number  h  of hidden nodes with h   i = o. this type of network learns to reproduce its inputs at the output layer  using a multilayer learning algorithm such as backpropagation  rumelhart  hinton  & williams  1   the learning paradigm used in this work. figure 1 presents an autoencoder with i = 1 = 1 and h = 1. in the past  autoencoders have been used for estimating learning algorithms reliability  pomerleau  
1  and for solving the catastrophic inference problem  kortge  1 . we now discuss how autoencoders can also be used for novelty detection. 
　in order to be used for novelty detection  the autoencoder is trained on positive instances of the concept  using backpropagation. once trained  the autoencoder can be fed new instances that it tries to reconstitute at its output layer. the quality of reconstruction is evaluated by computing the sum of the absolute error at each corresponding input and output node  i.e.  error = j1i-i  inp i  - 1ut i   where inp i  and out i  are the corresponding input and output nodes at position t and i is the size of the input. this error is recorded at various epochs. if after the autoencoder has been sufficiently trained  this error is small  then the instance should be labeled  positive   otherwise  it should be labeled  negative . the part of the system responsible for evaluating the size of the error and labeling the new instances is a semi-automated module called the threshold-determination component and will be described in section 1. 
　the phenomena that take place while using an autoencoder can be understood as follows  the narrow internal layer of the autoencoder forces it to generate an internal representation that compresses redundancies in the input pattern while retaining and differentiating non-

redundant information. when the autoencoder is specifically used for classification  it is trained on positive instances only. during training  consequently  the autoencoder learns to reconstruct positive data  but does not learn to reconstruct negative data. since negative data present structural regularities that are different from those of the positive data  i.e.  the inputs that are redundant in the positive data may not be redundant in the negative data and vice-versa   reconstruction of negative data  must be done differently. at testing time  therefore  reconstruction of positive data will succeed  whereas reconstruction of negative data will fail  and this success or failure will be the criterion used in classifying new instances. 
1 	threshold determination 
threshold determination consists of determining a boundary that discriminates between the reconstruction errors of positive and negative data. the threshold determination component is a semi-automated component composed of two algorithms: one for the noiseless case  which requires only positive or only negative data and one for the noisy case  which requires both positive and negative data. for every application  one of the two algorithms is manually selected according to the availability of data and the the expected quality of separation between positive and negative reconstruction errors. 
noiseless case 
in the noiseless case  the separation between positive and negative data is clear and stable in that the reconstruction error of all the positive instances is much lower than that of all the negative instances after sufficient training took place. in such a case  only positive or only negative instances are necessary. 
　in the case where only negative training instances are provided  the procedure we built simply computes the lower-bound of the reconstruction error of all the negative training instances at every epoch considered and then relaxes this bound by reducing it by a certain percentage. new instances are subsequently classified by checking whether the reconstruction error of the new instance is higher than that of the relaxed boundary in at least a certain acceptable proportion of the epochs considered. in such a case  the new instance is negative; otherwise  it is positive. the case where only positive training instances are provided was treated in a similar fashion. figure 1 a  illustrates the noiseless case and shows the boundary that was derived when using only negative data. in the particular case study that uses this algorithm-ch1 helicopter gearboxes-  the relaxation ratio was set to 1%; the epochs considered are all the recorded epochs that occur after epoch 1; and the acceptable proportion of epochs considered was set to one half. 
noisy case 
in the noisy case  the separation between positive and negative data is not clear in that although the majority of positive instances have low reconstruction errors and the majority of negative instances have high reconstruction errors  some positive examples have a high recon-
1 	connectionist models 
struction error and some negative examples have a low reconstruction error. in such a case  the threshold determination component needs to process both positive and negative instances in order to establish a boundary  and will need to decide what data to ignore as exceptional or possibly noisy. the procedure that we built for this case tries to find the epoch that shows the best separation between the reconstruction errors of positive and negative instances among all the epochs considered and at the same time  considers how stable this separation is. 
　in order to find the best separation  the procedure begins by constructing the boundaries of the absolute and intermediate positive and negative regions of the epoch versus reconstruction error space  at every epoch considered. instances that belong to a given class with great certainty have reconstruction errors that fail in the absolute region of this class while instances that belong to this class with less certainty have reconstruction errors that fall in its intermediate region. the absolute negative region is located above the intermediate negative region  while the absolute positive region is located below the positive intermediate region. absolute and intermediate regions differ from actual regions which span the entire negative and positive instance sets respectively. absolute and intermediate regions are illustrated in figure 1 b . to construct the negative intermediate region in particular  our procedure begins by stating the lower and higher boundaries of the actual negative region  and then proceeds by repeatedly shrinking this region by manipulating its boundaries  until it believes that it found the most accurate intermediate negative region. the region located above the upper boundary of the final intermediate negative region defines the absolute negative region. in the particular case studies that use this algorithm -promoter and sonar targets-  the density level of the final intermediate negative region is such that the lower half of the final intermediate negative region contains less than two fifth of the negative data while half of the negative data contained in this lower half is contained in its lowest fifth. the intermediate positive region is constructed in a similar fashion and the boundary for classifying positive and negative examples is established as the midpoint between the lower boundary of the negative intermediate region and the upper boundary of the positive intermediate region. 
　in order to find the epoch with the most stable separation  the program calculates how stable the separation is at each recorded epoch. the stability of a given epoch is defined as the slope of the line that goes through the separation of this epoch and the next epoch recorded. the separation that is selected for classification is the best separation whose stability is higher than half the greatest stability encountered. 
1 	overall functioning of the system 
the training of hippo is carried out in two phases. in the first phase  the redundancy compression and nonredundancy differentiation component is trained with positive instances of the concept. this phase results in the computation of a specialized autoencoder which can figure 1: the two cases of threshold determination. 
differentiate between a positive and a negative instance by showing a small reconstruction error in the positive case and a large one otherwise. the second phase of training consists of training the threshold determination component. in this phase  the specialized autoencoder is used with positive and/or negative instances. for each instance  the reconstruction error is recorded and fed into the threshold determination component which analyzes the reconstruction error of all the instances and issues a discriminator. the discriminator can be interpreted as a boundary between positive and negative instances. 
　once the two components have been trained  hippo can be used as follows: first  an unlabeled instance can be input to the specialized autoencoder which will issue a reconstruction error. the reconstruction error can then be input to the discriminator which will issue a classification. figure 1 illustrates the functioning of the overall concept learner. 
1 experiments 
hippo was tested in three domains: ch1 helicopter gearbox fault detection  molecular biology promoter recognition  and sonar target classification. its results are compared to those of two standard approaches to classification: c1  a decision tree learning system  quinlan  1  and ff classification  another connectionist learning method  rumelhart  hinton  & williams  1 . we begin by introducing the three domains considered and the methodology used to evaluate the three approaches. we then discuss the results of the experiments. 

1 	the case studies 
the ch1 helicopter gearbox data was obtained from nrad kolesar & nrad  1 . the ch1 helicopter problem is a monitoring problem that consists of discriminating between faulty and non-faulty ch1 helicopter gearboxes  according to the whining sound they emit during their operation. the sudden  unexpected failure of ch1 helicopter gearboxes is currently very costly both in terms of lives and equipment. the development of a monitoring system that can identify imminent failures before takeoff or when in flight is of paramount importance. the data for this problem was obtained by pre-processing the vibration time signal of the gearboxes of various faulty and non-faulty helicopters. the complete data set is composed of 1 non-faulty instances and 1 faulty ones which come in the form of 1 long vectors of real numbers. in this particular problem  we chose the non-faulty examples to represent the positive class. 
the promoter problem takes as input segments of dna  some subset of which represent promoters. a promoter is a sequence that signals to the chemical processes acting on the dna where a gene begins. the goal of the problem is to train a classifier to be able to recognize promoters  which are taken to be the positive class. the training set is composed of 1 examples  1 promoters and 1 negatives   each of which is composed of a set of 1 nucleotides  where each nucleotide can take one of four values {a  c  g  or t}. the promoter data was obtained from the u.c. irvine repository of machine learning and was modified in response to norton's critique of the biological flaws underlying the original formulation of the data  norton  1 . in addition  as is usual for this problem when run on a connectionist system  each example was converted into a 1-bit long vector where each nucleotide was represented with 1 bits. 
the sonar target recognition problem takes as input the signals returned by a sonar system in the cases where mines and rocks were used as targets. the sonar data was obtained from the u.c. irvine repository of machine learning though only a subset of 1 instances 
 1 positive and 1 negative  from this data was used in 
this particular case study.1. the transmitted sonar signal is a frequency-modulated chirp  rising in frequency. the data set contains signals obtained from a variety of different aspect angles. each instance of this data is represented as a 1-bit long vector. in this particular case study  we chose the signals returned by the mine targets to constitute the positive class. 
1 	general methodology 
connectionist models such as hippo and ff classification are more difficult to train than symbolic models like c1. not only do connectionist models require to be tuned before they can actually be trained but also  their training requires two phases: concept-learning and threshold-determination. no tuning is necessary and a single phase is sufficient for symbolic models. 
the learning rate  momentum  and bias for hippo and 
ff classification were arbitrarily set to 1  1  and 1 respectively  and held constant for all three case studies. the number of hidden units and recorded epochs were determined experimentally for each case study on random subsets of the entire data sets. for the helicopter gearbox application  hlppo used 1 and ff classification used 1 hidden units. both systems were run for 1 epochs which were recorded every 1 epochs for the first 1 epochs and every epoch subsequently. for the promoter problem  both systems used 1 hidden units while for the sonar target recognition problem  they used 1 hidden units. in both applications  the systems were run for 1 epochs which were recorded every 1 epochs. 
　the threshold-determination method of section 1 was used with both hlppo and ff classification. it was tuned on random subsets of the entire data sets.1 in every case study  the thresholds were established on the same data sets for both systems. for the helicopter gearbox problem  the noiseless method of section 1 was selected and applied to 1 negative instances. for both the promoter and the sonar target recognition problems  the noisy method of section 1 was selected and applied to 1 positive and 1 negative instances. 
　in the three domains considered  the three systems were evaluated using 1-fold crossvalidation  weiss & kulikowski  1 . at every fold of every experiment  the training set used by c1 was divided into a training set for concept-learning and one for threshold-determination for hippo and ff classification. since hippo learns a concept from positive data only  the negative data was eliminated from its concept-learning training set while it was kept for ff classification. in every experiment  the testing sets of the three systems always corresponded. note that at every fold of every experiment  hlppo uses significantly fewer negative data for overall training than the other two systems: for the ch1 helicopter gearbox 
   'this explains why the results reported in section 1 are different from those reported in previous experiments on this data  such as  gorman &. sejnowski  1  
　　1  for use with ff classification  the input of the thresholddetermination component was taken to be the value of the output node and its output had to be reversed since positive instances are supposed to return a larger signal than negative ones. 
1 	connectionist models 
problem  hippo uses 1 negative data while ff clasification and c1 use between 1 and 1 such data. for the other two case studies  hlppo uses 1 negative instances while ff classification and c1 use between 1 and 1 such instances. 
1 	results 
the error rates of hlppo  c1  and ff classification in the three case studies considered are listed in table 1. numbers after each  ＼  are standard deviations for each of the five-fold averages. 
　table 1 shows that in both the ch1 helicopter and the sonar target recognition case studies  hlppo performed much better than either ff classification or c1. in the promoter case study  hippo and ff classification performed equally well and better than c1. these comparisons are all statistically significant with p   .1  except for the comparison with c1 in the sonar target recognition study. 
　altogether  this shows that in addition to requiring a much smaller number of negative training data than the other two systems  hlppo is capable of classifying novel instances more accurately than both c1 and ff classification in all cases except for the promoter data  where hippo's performance is matched by ff classification's. however  we believe that the limited performance of hippo with respect to ff classification has to do with the weakness of the representation used in this version of the promoter problem  hirsh & noordewier  1; norton  1 . 
1 	conclusion 
this paper has presented a new approach to classification that uses the idea of novelty detection and in particular  that of redundancy compression and non-redundancy differentiation. the system we introduced-hippo- attempts to learn how to recognize concepts  rather than to differentiate between positive and negative instances of a concept. the method works in two phases. in a first phase  a concept is learned from positive instances only and in a second phase  the system learns how to identify positive and negative instances of that concept. hlppo was tested on three realworld applications and compared with two conventional classification systems: c1 and feedforward classification. in all applications  hlppo performed better than c1 and in two of them  it performed better than feedforward classification  in the third application hippo and feedforward classification performed equally well . furthermore  in all applications  hlppo used a significantly smaller number of negative training data than the other two systems. 

　the work presented in this paper opens up a large number of possible theoretical and practical issues to consider in the future. it would be useful  in particular  to establish the strength and limitations of our approach more precisely  by experimenting in other domains  both artificial and real  and comparing hippo's results with methods other than c1 and feedforward classification. we could also attempt to improve the two components of hippo  using a more refined version of the autoencoder and fully automating the threshold determination component. such studies would contribute to the exploration of this promising new approach to conceptlearning which is more accurate than conventional methods and requires fewer negative data for training. 
acknowledgements 
this research was supported by the office of naval research through the young investigator program  mg  and grants n1-k-1  mg   n1-j-1  rg  and n1-j-1  rg . parts of this work was conducted at the centre for neural networks at king's college in london and at the laboratoire laforia at l'universite pierre et marie curie in paris. we thank rick kaye and brian davison for helpful comments on earlier drafts of this paper  and bob kolesar for his contributions to the helicopter analysis and feature extraction. 
