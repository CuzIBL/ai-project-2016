 
　　　the  teachable letter recognizer   tlr  is a program for letter learning which uses a method not vet described in the char-
acter recognition literature. tlr has two main characteristics:  1  it uses a discrimination tree as a knowledge representation. the discrimination tree is a quadtree with some additions.  1  the two types of global features that are used to characterise a letter are the density of pixels and the overall  color   white  black or grey. tlr is invariant to shifts and shows several interesting effects which are related to human behavior  for example occasionally it becomes confused when learning new letters. tlr's importance as a letter recognition program lies in its ability to recognize some distortions of letters which it has never seen before  and for which it also does not have a transformation algorithm. 
	1 	introduction 
	this paper describes the 	teachable letter recognizer  
 tlr . tlr belongs to the class of programs which are intended to know nothing about the domain of usage and acquire their knowledge by interaction with a  teacher . tlr owes some of its goals  as well as its name  to the 'teachable language comprehended  quillian  1 . instead of programming' knowledge into tlr 1 want to build a general knowledge representation and then teach letters by showing them to tlr. the other important a! source for tlr was the idea of using a discrimination net as a knowledge representation. this has been influenced by epam  leigenbaum  1 . 
　　　tlr is conceptually different from classical approaches to letter recognition. nevertheless tlr shares with some graphics programs the use of a quadtree as a hierarchical data-structure. eor a quadtree oriented approach to shape recognition see  chien and aggarwal  1 . the reader is referred to  samet  1  for a survey about quadtrees. 
　　　to show the contrast between tlr and classical methods of character recognition 1 want to compare it with two of the major techniques that have been discussed in the literature use of low level features and template matching. eor a review of some prin ciples of character recognition see  harmon  1 . a psychological treatment of letter recognition can be found for instance in  mcclelland and rumelhart  1 . 
　　　one of the classical approaches in character recognition is to employ some method of low level feature extraction. examples of low level features include  line segments    intersections    corners   etc. without questioning the idea of using features per se it seems disturbing that a vision system should have to do a vast amount of computation before it is able to say anything significant about the object that it is  watching . even though there is a lot of evidence that early processing in the human brain is not accessible to introspection  it still seems obvious that we can say something about the appearance of an object before we have perceived all its little details. therefore we have to look for 
different types of features that are more  global  or  structural . 
　　　the fact that 1 want to look for  structural  as opposed to local features does not exclude a bottom up approach. recognition of the global features should therefore not require the computation of small features  this means that  structural features  should be easily accessible  on the surface . in other words  1 am looking for a characterization of an object which can be obtained by ignoring details. 
the other approach that 1 want to mention as different from 
tlr has been referred to by harmon as a  parallel processing  technique  namelv template matching. template matching does not create the same problem as local feature extraction because it is definitely a global process. however  template matching seems to make immediate use of all the information available in the picture instead of accessing the easily available structural features first and looking at details later. 
　　　tlr gains a lot of cjantv bv using a very simple control mechanism namelv depth first tree search. 
	ii 	tlr 
a. 	input data format 
　　　conceptually the reader can think of the input data as consisting of arrays of binarv values with each arrav representing one letter. due to the fact that the program is implemented in lisp a letter is represented as a list structure of b's and w's. figure 1 shows a reduced  1  binarv matrix and the corresponding list representation of the letter  1 . 
b. 
　　　informally a quadtree is a tree representing a picture where every node is either a leaf or has exactly four sons. a leaf contains either a  w   white  or a  b   black  and represents a square 
	1 1 1 1 	 w w w w w w w w  
	1 1 1 1 	u w w w w w w w  
	1 1 1 1 	w w w b w w w w  
	1 1 1 1 	w w w w w w w w  
	1 1 1 1 	 w w w b w w w w   
	1 1 1 1 	 w w w b w w w w  
	1 1 1 1 	 w w w b w w w w  
	1 1 1 1 	 w w w b w w w w    
figure 1. matrix and list representation of   i   
1 	j. geller 
of the picture in the given color. a non leaf is said to represent a grey area and is obtained by combining four sons in a way that a 
square of double the length of the  son squares  is built  the root represents the whole picture. if the root is a leaf  then it describes 
an entirely black or entirely white square. the precise method of representing a picture with a quadtree is explained in  samet  1 . 
　　　the quadtree used in this paper adds three aspects to a normal quadtree   l  the representations of all known letters are integrated into a single tree structure.  1  all nodes contain information about the objects they belong to.  1  every node contains information about the relative densities of black pixels of the area it is representing  relative to the father area . 
　　　item 1 from above is the source for the name given to this type of quadtree  namely  density-quadtree . items 1 - 1 seem to justify the treatment of a density quadtree as a form of knowledge representation as opposed to simply a data structure. i will refer to the additional information in the quadtree as  object knowledge'. figure 1 shows a bne definition of a densityquadtree. 

figure 1. bnt definition: density-quadtree 
　　　 1 ... 1 in figure 1 stands for all rational numbers between 1 and 1 with a precision of two digits after the period. this is an arbitrary limit which is easily changeable. 
　　　in order to explain the meaning of the different parts of a density-quadtree a small example follows. assume the following  son : 
  w 1 .1 a e w 1 .1 a e o 1 gq    1 .1 gq ml 
 m  ....  1   j l k 1  mn    .1 
　　　the line containing  w  indicates that the given node is a leaf node for the letters  a e o g q . this means that all these letters have a purely white sub-area corresponding to this node. the line consisting only of  nil  indicates that this node is not a 
leaf for any letter with a corresponding purely black area. finally the line containing an  m  indicates that the letters  j 1 k m n  are grey in the given area. therefore they have to be described by a recursive sub-tree which is indicated by  .... . 
　　　the number 1 is the approximate density of black pixels at this node relative to the number of black pixels of the area corresponding to the immediate parent node for the letters  g  and  q . 
c 	learning 
in the learning phase letters given in a format similar to 
figure 1 are subjected to a density-quadtree analysis. the tree structures of all letters are integrated into one single densityquadtree. the process of doing this and the very structure of the tree itself are designed to permit a high degree of structure sharing between objects that are looking similar. 
　　　on the other hand two letters create a more complicated tree if they are very different  because they can share less information. the important characteristic of a letter is its structural distribu tion of black  white and grey values  and letters share structural distributions if their two-dimensional appearances are similar. 
　　　the second measure used to ensure that the tree is not growing unnecessarily is to keep the number of  value s in the  value-hint-l'  small by using a learning scheme with a  competing letter . any time tlr is asked to learn a new letter it first tries to recognize this new letter. if the recognition gives a different letter name than the one told by the teacher  then there is a potential danger of repeating this mistake at the next recognition process. therefore the letter obtained bv the recognition process is called the competing letter' and given as an additional parameter to the tree building function. 
　　　if the current new letter has a density value that is slightly different from all the values at the currently analyzed node  then the tree building function will not add a new density value  unless this would put a letter and a competing letter together into the same  hint hst . if the new letter was already in the  hint-list   then nothing will be added at all.  this could hap pen if two slightly different handwritten versions of the same letter are taught . 
　　　finally it is necessary to mention that there is a preprocessor for  learn*' that removes the  white strips  around the letter before quadtree analysis starts. this makes the recognition process invariant towards shifts but unfortunately forces the pro gram frequently to deal with areas of odd numbered size which cannot be broken down evenly. in this case the analysis is contin ued with four areas of different size. 
d. 	recognition 
　　　in the recognition phase an unknown letter is analyzed in the same way as in the learn phase. however  while analyzing the area  the integrated quadtree is also traced through its 
corresponding nodes. if the unknown letter happens to have a black area at the given level  then analysis stops for this branch of the quadtree and the  hint-list  of the  b-seg  is added to a 
result list.  the analogue thing happens for a purely white area . 
　　　if the unknown letter is grey  and the given node contains an  m-seg   then analysis is continued. the four sons of the subtree are recursively analyzed relative to the four sub-areas of the area.  hint-list s for the computed densities are added to the result list. this use of the quadtree bears a strong resemblance to the use of a discrimination tree. after the whole tree has been searched in the described way  the result list might look like   a b   a   a c   c   a   a   for a quadtree which contained pictures of  a    b  and  c . 
　　　the most important characteristic of the result list is that it contains sublists of varying length. sublists that were contributed by the top nodes in the density quadtree tend to be long  because on the top levels most letters look the same. this sounds like a strong statement  but on the top level all letters are grey and therefore similar. 
　　　tlr decides what letter it saw by counting the number of occurrences of each letter in the result list. however before this is done the result list is pruned by removing all sub lists that are longer than a so called  effort  value. if the effort value is set to a low value  1  then the evaluation of the result list can be done comparably fast. it is not possible  however  to rely strictly on the short sub-lists derived from the leaves because the analysis of a new distortion might bottom out at an earlier level. therefore in some cases short sub-lists alone might not be sufficient and 
a higher effort  alue would be necessary. 
　　　after learning more and more letters the length of   hmt list s will get longer even at the leaves which will also require the user to raise the effort of recognition. 
iii rlsllts ok working with tlr 
　　　in this section two series of tests of tlr will be described. the goal of both of them was to investigate tlr's most important feature: its ability to recognize some distortions that it has not been confronted with before. 
　　　the first experiment used onlv 1 letters  namelv  a    b .  g  and  o ;  a  and  o  were chosen because they look similar in many handwritings  fvery letter was shown in one original form and ten different distortions. that means tlr was shown eleven sets of the letters  a    b    g  and  o  in precisely this order. the first set of lour letters could of course not be recog ni/ed  because tlr is started without any prior knowledge. the results of this test run were quite encouraging. 
　　　thirt  one distortions out of fortv were immediately recognized. nevertheless  all of them were presented a second time to tlr in order to improve its knowledge. lor three of the letters tlr suggested two or more possible solutions with almost identical hint counts. in these cases correct solutions could be obtained by raising the  effort level . in six cases tlr confirmed wrong results. after teaching these letters again tlr finally recognized them. 
　　　test data consisted of hand printed lower case letters of one subject that were  digitized  with an editor. a bias to the handwriting of the subject cannot be excluded  however an effort was made to make distorted letters really different. 
　　　lnforcing knowledge of one letter can weaken the memory of another letter  because the relative number of hints for the second letter diminishes in the tree. to test for this possibility all forty letters were presented again for recognition after the experiment was finished. four letters were recognized incorrectly  and three out of these four required unexpectedly high effort values  approximately 1  to be recognized correctly. this stands in contrast to the case where tlr returns two letters with similar hint counts  because in that case the necessity of a higher effort level is immediately obvious. 
　　　one positive result of this experiment was that the number of errors that tlr made became smaller with greater knowledge  cf. table 1. ; however this gain was not accompanied by a degradation of the recognition times. it will require tests with larger data sets to find out whether a saturation effect of learning can be achieved. 
　　　learning times for a letter were usually less than one minute real time on a vax-1. recognition times varied between several seconds and almost one minute  however were mostly in the order of fifteen seconds. 
cycles letters errors 1 
1 
1 
1 
1 
1 
1 
1 
1 1 
1 
1 1 1 
1 
1 1 
1 
1 
1 
1 
1 
1 
1 
1 table 1. number of accumulated errors 
	j. geller 	1 
　　　the second experiment involved a test with the complete alphabet. two sets of the entire alphabet were used  one learning set and one set of distortions. this time letters were not presented again after successful recognition. seventeen out of twenty six letters were recognized immediately  one after adjusting the effort level  and eight letters required additional teaching. 
　　　it should be noted that some of the distortions that were recognized were of different size or consisted of strokes of double width. these two types of variations could be recognized without the use of a scaling algorithm or a thinning algorithm. however tlr was not designed to deal with rotated letters. 
	iv 	criticism of tlr 
　　　when tlr finds its own competing letter to differentiate it from a new letter that has to be learned  then some of its choices would surprise a human observer. so for example   i   was taken as an obvious competing letter for   j     but  g  was chosen as  competing letter  for  k   although people do not consider these letters to be similar. a detailed analysis of the density quadtree should show similar density values for these two letters. one reason why we see  g  and  k  as different and tlr does not is that we have a literal base line in mind  when we look at the letters. we also might think of words where  g  occurs and  g  
clearly extends below the word but  k  does not. tlr never saw a  base line   and it does not know words. 
	v 	conclusions 
this paper introduced a letter recognition program called the 
 teachable letter recognizer which uses a quadtree with added object knowledge as a discrimination tree and knowledge representation. global structural features  namely the b/w/grev 
structure and relative density values are used for the recognition process. an intelligent learning process that avoids unnecessary information and is based on  competing letters  is used. an effort value permits tlr to prune a list of hint-lists. some distortions which are not known to the program beforehand and for which no hans formation algotithm is implemented are nevertheless recognized. 
acknowledgements 
i am indebted to jon hull  david shapiro  stuart shapiro  soon chun and three anonymous referees for valuable contributions. 
