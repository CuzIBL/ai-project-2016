 
   recent work on learning apprentice systems suggests new approaches for using interactive programming environments to promote software reuse. methodologies for software specification and validation yield natural domains of application for explanation-based learning techniques. this paper develops a relation between data abstractions in software and explanationbased generalization problems and shows how explanation-based learning can be used to generalize program abstractions to promote their reuse. this method is applied in the design of a system called lasr  learning apprentice for software reuse  which will acquire programming knowledge by capturing and generalizing interconnections between abstract data type theories. the technical role of theories in defining learned concepts in this application suggests their more general use in representing problems in explanation-based learning. 
1. introduction 
　　there has been considerable attention given recently to making use of formal program specifications to promote software reuse and to developing programming methodologies suited to that purpose  1 . among the requirements for software to be reusable are that it possess higher levels of robustness and generality than are usual in ordinary programming practice. the purpose of this paper is to show how explanation-based learning can be used with formal specifications to capture and generalize program abstractions developed in practice to increase their potential for reuse. this particular approach to applying machine learning in software is motivated especially by the explicit domain knowledge embodied in data type specifications and the mechanisms for reasoning about such knowledge used in validating software. these deductive methods from software engineering fit well with the requirements for explanation-based learning problems in which a single training example is explained  validated  in terms of a domain theory. the training examples in our setting correspond to  semantically correct  interconnections between software components. in generalizing from these interconnections  new data abstractions are formed which yield minimal requirements for reuse. such synthesized abstractions can also be useful in their own right in suggesting alternatives in program derivations. 
　　the software principles involved in this application are more abstract than those most commonly used in programming practice. it is generally hoped that time will close 
1 	knowledge acquisition 
this gap. nevertheless  the level of abstraction seems most appropriate both for this application of machine learning and also for promoting advances in software reuse. the discussion and examples given below attempt to show that the implications of those principles are in fact quite concrete and relevant the notion of a theory is of central importance in much modern software research. mechanisms for composing theories support construction of complex abstractions. it turns out to be natural in this application of explanation-based learning to define the learned concepts in terms of theories. this theory interpretation of learned concepts seems applicable in representing other explanation-based learning problems. 
　　it is desirable to incorporate these methods for generalizing software with an interactive programming environment such as goguen describes in . while supporting practical system development  such a system would automatically add new generalized components to its software base for reuse. such a system would be a learning apprentice in the sense of . this paper also describes the design of a system called lasr  learning apprentice for software reuse  which is under development at hewlett-packard. its purpose is to determine the potential for applying the ideas presented here to practical software engineering. 
1. learning from explanations 
　　explanation-based learning is a comparatively recent paradigm in machine learning concerned with generating concepts using an explanation of a single training example in terms of a domain theory. the papers  and  contain extensive discussion and references to work in explanation-based learning. the methods of explanationbased generalization developed in  in particular derive a concept definition by analyzing a proof which accounts for the training example as a logical consequence of domain axioms. it is the use of deductive reasoning in a formal domain which gives a basis for applying explanation-based learning to software. 
　　the explanation-based approach to machine learning is often contrasted with so-called similarity-based methods which derive conceptual classifications by noticing patterns in multiple observations. similarity-based learning is more computationally intensive  typically involving searching in a large space of possible concepts  e.g.  combinations of constraints on feature values . it is data-

intensive  and its results are always subject to revision since further observations may invalidate empirical generalizations. similarity-based learning is essential  however  especially in the absence of sufficient domain knowledge. in   similarity-based learning is applied to program synthesis by considering learned concept predicates as declarative computer programs. 
　　in using an explanation to generalize a training example  one obtains a characterization of a family of examples of a more general goal concept  including the given example. the learned concept definition is thus a specialization of that goal concept. the process of specialization  goal regression  transforms the goal concept over inference steps in the explanation until it is  operational . operationality of a concept definition depends on the particular generalization problem. in the application of these methods to software  we define operationality in terms of levels of abstract machine. following   figure 1 shows the requirements for an explanation-based generalization problem. for such a problem  the generalization process consists of first constructing an explanation of the training example in terms of the domain theory  and then using that explanation to accomplish the generalization. 
given: 
* goal concept 
a concept definition describing the concept to be learned. 
* training example 
an example of the goal concept. 
* domain theory 
a set of rules and facts to be used in explaining how the training example is an example of the goal concept. * operationality criterion 
a predicate over concept definitions  specifying the form in which the learned definition must be expressed. 
determine: 
* a generalization of the training example that is a sufficient concept definition for the goal concept and that satisfies the operationality criterion. 
figure 1. explanation-based generalization problem 
　　we describe below examples of explanation-based generalization problems in designing software and mechanisms for solving them automatically. for further discussion and examples  see . 
1. domain theories in software 
　　in order to apply explanation-based learning to software  it is necessary to associate goal concepts and domain theories with computer programs. in doing so  it is reasonable to expect relationships between goal concepts and program abstractions  and between explanation and program verification. one rather obvious approach to applying explanation-based learning to procedural abstractions in software is to use hoare-style verifications to derive the characteristic behaviors of procedures in imperative languages  notably in terms of weakest preconditions. under various names  there is already a considerable literature on this subject in both the software and ai communities  1 1 . while that approach does merit elaboration from the point of view of machine learning  the goal of this paper is to discover other applications of explanation-based learning in software which seem more enlightening and more promising for future developments both in software engineering1 and machine learning. 
　　in modern programming methodology  formal theories of data abstractions in programs play an important role in designing and validating software and in promoting its reuse  1 . such theories are represented explicitly in languages such as obj  and the larch shared language . as will be shown below  these theories give rise naturally to applications of explanation-based learning by providing domain theories for explanation-based generalization problems. the theories discussed here  so-called many-sorted equational theories  correspond to collections of abstract data types. such a theory consists of finite sets of sorts  the  data types    operations on them  and equations relating those operations. those equations  or equational axioms  provide a logical theory completely characterizing their associated  data types . figure 1 gives a simple presentation of a theory of stacks. the presentation of a theory can be thought of as having two parts. the first declares the operations and their types; such declarations are conventional in the specification parts of modules in languages like ada and modula-1. the second part provides the semantic constraints which the operations satisfy. operations are side-effect free; in figure 1 it is necessary to provide separate top and pop operations. 
theory stack 
sorts stack element 
ops empty: -  stack push: element stack -  stack pop: stack -  stack top: stack -  element empty : stack -  boolean 
vars e: element s: stack 
eqns pop empty  = empty pop push e s   = s top push e s   = e empty  empty  = true empty  push e s   = false 
figure 1. data theory of stacks 
1. we have learned quite recently  subsequent to submitting this paper  of current work by c.a.r. hoare  he jifeng  and j.w. sanders  1  which  although not employing explanation-based methods  contains ideas quite similar to some of those presented here. 
	hill 	1 

   to make data theories useful in constructing programs  there are mechanisms for composing and refining them. a particularly important construct in this regard is the theory morphism. a theory morphism from a theory tl to a theory t1 maps sorts and operations of tl  respectively  to sorts and operations of t1. moreover  it preserves the equations of tl in the sense that each axiom of tl  when rewritten in terms of the sorts and operations of t1  can be deduced in t1. we will call tl the source theory and t1 the target theory. 
   figure 1 gives an example of a morphism from the theory of stacks to the theory of arrays with a distinguished index. note that this morphism implements stack operations in a  programming  language based on arrays and natural numbers. from that point of view  the morphism plays the role of a program  the theories are specifications  and the proof that the axioms are preserved is a validation of the program. we will see how this leads naturally to a generalization of the program using the validation as explanation. 
   the example in figure 1 of the integer-array theory can be interpreted as an instance of a parameterized array theory with an unconstrained  element  sort analogous to the one in figure 1. it turns out  that theory morphisms in general provide the bindings for instantiating generic parameters. what goguen calls views in  are essentially theory morphisms. 
1. using validations to generalize software 
   in adapting explanation-based generalization to data abstractions  the data theories provide domain theories for explaining explicit morphisms  which take the role of training examples. concept definitions in this setting are presentations of target data theories of morphisms. intuitively  such concept definitions specify a language for interpreting  or implementing1  the morphism's source theory. the object of generalization is to use a proof that a particular interpretation is valid to reduce the language  indeed  its semantics as well as its syntax  to one which provides a minimal valid interpretation. in other words  the generalization provides requirements for an interpreting language. 
1. a little care must be taken in interpreting this definition. the target theory is obtained by extending the i-array theory to make the ops rules shown in figure 1 into equational axioms. this requires adding new operations to the theory corresponding to push  pop  etc. we call the operation       and the operations imported from theories nat and array 
primitive. primitive operations are used to characterize operationality. it is conventional in presenting morphisms to suppress obvious correspondences  e.g.  element =  element  associating stack elements with array elements. 
1. an implementation or data refinement is a morphism which has reasonable behavior on models for its source and target theories. while any morphism can be generalized  many readers will find the notion of implementation more intuitive. 
   what most distinguishes this form of explanationbased learning is that the learned concepts are presented explicitly in terms of theories; there is no difference in kind between the domain theory used for the explanation and the new target theory obtained from generalization. nevertheless  the differences between this approach and  for example  that of  are only in interpretation; the learned concepts in the examples in that paper are described in terms of single predicates  but could also be presented in terms of theories in an appropriate logic. making theories explicit is advantageous for representing and reasoning about complex abstractions . also  in contrast to attempting to reuse individual procedures or predicates  theories provide a granularity which better promotes software reuse much as classes do in object-oriented programming. 
1 validations are explanations 
   a morphism m between data theories tl and t1 can be used for constructing explanation-based generalization problems. the requirements shown in figure 1 are met by defining the following correspondences: 
training example: 
the mapping on sorts and operations defined by m. 
domain theory: 
the data theory t1. 
goal concept: 
a morphism m' from tl to a subtheory t1' of t1 
   the operationality criterion is defined by the requirement that the axioms defining the target theory be expressed in terms of primitive operations of t1. 
   an explanation is  of course  a proof that m is a morphism. in other words  it is a collection of proofs for the axioms in the source theory when interpreted in the target theory. 
1 goal regression in equational theories 
   validations of morphisms between the equational data theories described above are collections of proofs that equations corresponding to axioms of one theory follow from the axioms of the other. a simple example of such a proof from the stack-with-offset morphism of figure 1 is the following: 
assign a j e  i  = if i=j then e else a i  assign a n+l e  i  = a i  if not i=n+l  assign a n+l e  i  = a i  if i =n  assign a n+l e  n  =  a n  pop  assign a n+l e  n+l   =  a n  pop push e  a n    =  a n  
   proof steps in equational logic use so-called rules of equational reasoning  reflexive  symmetric  and transitive laws  together with substitution and instantiation  and may 

1 	knowledge acquisition 

also use axioms and inference rules from first-order predicate logic  and reasoning about representing and 

distinguishing terms. the text  treats equational reasoning for data theories. 
   viewing each axiom in the source theory of a morphism as a goal  the equational inference steps transform it into formulas  ultimately in the target theory. this transformation process constitutes goal regression of the axiom  and corresponds to the regressing of goal concepts through explanation steps as described in   with the rules of equational reasoning explicitly added. the algorithm of  for using regression to extract the generalization from an explanation of a goal applies directly here  except formulas for  conjunctive  branches in the proof are collected as distinct axioms for the derived theory. since the validation assures that the regressed goals follow from the original implementing theory  the generalized theory is the subtheory consisting of the regressed goals and the sorts and operations used to express them. 
   theories describing learned concepts here are in a sense more  rigid  than simple predicates would be for the same purpose. there is a certain give-and-take in regressing a collection of axioms. while the collection of regressed source axioms  or their conjunction  may be distinct from the original target axioms  the theory they determine may be equivalent. in that case  the effort of regressing the axioms is wasted since the original target axioms could be used just as well. the significance of this distinction between theory and predicate becomes more apparent in observing that structural properties of a theory morphism may detect such equivalence. although it isn't immediately obvious  this is the case  for example  if the operations in the source theory are explicit generators. for example  a stack s can always be written in the form pop s'  from the axiom pop push e s  =s. it is not true that regressing that axiom over the given proof above would weaken the target axiom 
    a n  =  b m  if m=n and a i =bfi  for i =n for the morphism in figure 1 to 
    assign a n+l e  n  =  a n  since pop  a n   =  a n-l  can be applied inductively to recover the original axiom. an example of non-trivial goal regression is given below. 
1. examples 
1 stacks as arrays with distinguished index 
   figure 1 describes a theory morphism refining the abstract stack data type to that of arrays with a distinguished index  called here the theory of i-arrays. the i-array theory in figure 1 imports the sorts and operations of the theories nat  of natural numbers  and array. in particular  i-array uses the operations  assign  from array and +  from nat. 
   what is interesting about this example is that if the iarray theory is replaced by any other target in defining this morphism  then the generalization recovers precisely the i-array theory. in effect  this morphism is the result of theory i-array / nat array sorts i-array 
ops       : array nat -  i-array 
vars a b: array m n: nat 
eqns  a n  =  b m  if m=n and a i =b i  for i =m 
figure 1. data theory of i-arrays 
morphism stack-with-offset stack =  i-array sorts  stack =  i-array  
vars e: element a: array 
n: nat 
ops  empty =  new-array 1   
 push e  a n   =   assign a n+l e  n+l   
 pop  a n   =   a n-l   
 top  a n   =  a n   
 empty   a n   =  if n=1 then true else false  
figure 1. stack implementation morphism 
generalization. in practice  an implementation of stacks is with respect to some richer theory. although the theories discussed here are not adequate to specify conventional programming languages such as lisp or c  it is nevertheless helpful to think of the operations of a theory as the primitives for a programming language it defines. the generalization process here is analogous to going from a program in lisp  say  to a  more abstract  program which could be transformed into another language such as c. this process  however  should not be confused with that of translating between two specific languages. it is better to think in terms of extracting an application from a larger system in which it has been implemented to retarget it to other  possibly smaller systems. figure 1 prescribes an implementation of stacks in any theory  language  containing pairs of arrays and natural numbers. the recovery of the i-array theory from a validation of figure 1 which was discussed above shows that this morphism can't be generalized further. 
1 optimizing functions on arrays 
   in this example  integer arrays are implemented as arrays with an extra slot to hold the value of some given function of the array. the particular example1 with f a  = 
a m  can be generalized to other integer-valued array 
m=1 
1. the upper summation index is suppressed to make the formulas more readable. the sums are over all  finitely many  non-zero array values starting at the lower summation index. the size operation in this theory provides the upper bound in such sums and is used in reasoning about 
them. f a = ♀ a m  should be read f a = ♀ a m  m=1 	m=1 
	him 	1 

functions. the motivation for this data refinement operation is that  in practice  often when an array instance is created  certain functions may be evaluated repeatedly for it  in which case storing the value as part of the array entity makes sense in the implementation. needless to say  this is a very concrete case of a general programming strategy. 
theory integer-array / nat integer sorts integer-array 
ops new-array: -  integer-array assign: integer-array nat integer -  integer-array j j : integer-array nat -  integer size: integer-array -  nat sum: integer-array -  integer 
vars a: integer-array j m n: nat 
i: integer 
eqns 
 1  assign a n i  m  = if n=m then i else a m  
 1  new-array n  = 1 
 1  size new-array  = 1 
 1  size assign a n i   = max size a  n  
 1  sum new-array  = 1  1  sum assign a n i   = ♀ a j  - a n  + i 
j=1 
figure 1. data theory of integer-arrays 
	morphism array-with-sum 	integer-array =  integer-array 
vars a: array n: nat i: integer 
	ops 	 a nl= a n+l   
 size a  =  max size' a -l 1   
 sum a  =  a'  
 assign a n i  =  assign' assign' a n+l i  1 ♀ a j+l '- a n+1 ' + i   j=1 
figure 1. caching sums morphism 
　　the implementation is given by the morphism shown in figure 1 which maps the theory integer-array  figure 1  to itself. note that for convenience in this example  integer arrays are initialized to zero. we haven't bothered bounding the arrays; the size operation just keeps track of the highest index for which a  possibly  non-zero value has been set: in figure 1  to distinguish operations in the source and target theories  those in the latter are primed  e.g. assign' is the assign operation in the target theory. 
　　a validation of the morphism in figure 1 consists of proofs for each of the 1 equational axioms for the integerarray theory. given such a validation  the morphism is generalized by constructing a new target theory from those axioms regressed over the proofs. it is not difficult to construct such a validation for which regressions of axioms  1  through  1  essentially reproduce those axioms as a group  except axiom  1  is weakened to 
	assign' a n i  n  ' = if n 1 then 	 1 a  
if m=n then i else a n ' 
the following formulas show the main steps in a proof of axiom 1  using  la  : assign' a 1 t ' = t assign' a 1  ♀ a m '  ' = ♀ a m ' 
	m=l 	m=l 
assign ' assign ' a n+l i  1  ♀ a m+l ' - a n+l ' + i ' 
= 1 afm+1 '-a n+l ' + i 
m=1 
	assign a n i ' 	= ♀ a m+l ' - a n+l ' + i 
sum assign a n i   = ♀ a m  - a n  + i. 
for this derivation  the regressed goal is then assign' assign' a n+l i  1  ♀ a m+l ' - a n+l ' + i ' 
= ♀ a m+l ' -a n+l ' + i . 
using  la   this reduces to 
	assign' a 1  ♀ a m '  ' = ♀ a m ' 	 lb . 
	m=l 	m=l 
　　the generalized theory is gotten by replacing the axiom  1  with  la  together with the weaker regressed goal  lb . in that theory  the 1 array slot is required to behave in the usual way only if it contains the sum of the other array values. this generalization allows many alternative specializations  for example  
	assign a 1 i  n  = if n=1 then ♀ a m  	 lc  
m=l 
else a n  
which leads to an array-like data type in which  however  assignments to slot 1 are ignored and slot 1 always evaluates to the sum of the other slots  or assign a 1 i  n  = if n=1 then max  i  ♀ a m    id  
else a n  
in which assignments are effective only for values greater than the sum of the other slots. these special types are reminiscent of structures which occur especially in hardware optimizations in which the behavior of special registers or memory addresses may differ in similar ways from ordinary array slots. in this case  explanation-based generalization yields an abstraction which can suggest more efficient alternative specializations. 
1. learning apprentice systems 
　　learning apprentice systems were introduced in . a learning apprentice is a knowledge-based interactive system which assists a user working in a problem-solving domain. it acquires new knowledge in the course of normal use by capturing and generalizing the problem-solving steps carried out by the user. the system recycles this 

1 	knowledge acquisition 

knowledge by using it to make suggestions in subsequent problem-solving situations. 
   the objective of learning apprentice systems is to reduce the knowledge acquisition bottleneck by minimizing the separate investment of effort by experts and knowledge engineers in creating and maintaining the knowledge-base. instead  it provides a mechanism for adding to the knowledge base automatically through expert use. in the domain of software development  programming languages and software libraries typically constitute invested expert knowledge. as discussed below  a learning apprentice for software development is a programming environment which makes components of user programs  as they are created  effectively reusable. 
   a particular learning apprentice system called leap is described in . leap applies explanation-based learning to vlsi design. it suggests structural decompositions of vlsi circuits described by functional specifications. the leap user may accept its advice or else construct an alternative refinement. in the latter case  leap will attempt to validate the user's solution and use the validation  explanation  to generalize it 
   leap's knowledge is in the form of rules: if a circuit performs a certain function  e.g.  conversion of a serial signal to parallel   then it may be implemented using a particular network  e.g.  a shift register . leap's generalization component takes a particular implementation and abstracts both sides of the rule. 
   current work on learning apprentice systems centers on applying explanation-based learning. that approach is especially appropriate for applications to programming since domain theories are inherent in software  as we've discussed in this paper. nevertheless  there is every reason to expect future learning apprentice systems to benefit from and in fact stimulate research in other machine learning paradigms. 
   the knowledge acquisition bottleneck addressed by learning apprentice systems has a direct counterpart in software engineering. programming expertise is invested  for example  in developing component libraries  functions  procedures  modules  classes  etc.  depending on the programming paradigm . the selection and creation of library components to promote reuse require considerable effort which is distinct from the primary problem-solving activity of programming. a learning apprentice system for programming should promote reuse of  live  code. in other words  it should take over responsibility for generalizing and validating program components and cataloging them. 
   the use of software validation is especially interesting here. it is generally recognized that validation is important for the reliability of reusable code. in this application  the use of validations to make particular software more generic adds a new dimension and helps offset the additional cost of validation. 
1. lasr: a learning apprentice for software reuse 
   lasr is an experimental learning apprentice system which applies explanation-based learning to abstract data types using the principles developed in this paper. it brings together essential ideas from  and . its primary purpose is for use in testing the reuse potential of the interconnections derived from explanation-based generalization described in this paper and for experimenting with programming methodologies which promote such reuse. lasr is being developed currently at hewlett packard. the following diagram gives an overview of the components of lasr which deal explicitly with reuse. 

   the lasr interface allows the user to specify a module to be refined. the system first attempts to match the specification against possible refinements in its software base. the software base consists of views  essentially theory morphisms  which are represented as pairs of data theories together with a mapping from sorts and operations of one to the other  and a proof that the mapping is valid  i.e.  that it preserves the properties of the operations. views are retrieved from the software base using consistent matching of terms in the view's source specification with the specification provided by the user. the view's source specification may contain more operations and axioms than are used in the matching. for example  if the software base contains an implementation of deques  two-ended stacks   that implementation could be suggested for implementing stacks. this corresponds at least in part to generalizing the left-hand side of leap's rules and to the notion of implementation inheritance in . 
   if no suitable match for the specification is found  or if the proposed implementations are rejected  then the user refines the specification directly. the system attempts to validate the refinement step using an equational theorem prover. if a validation is obtained  then it is used to generalize the view provided by the refinement. it is the generalization  of course  which is added to the software base. 
	hill 	1 

1. future research 	acknowledgements 

   along with using lasr to evaluate the benefits of learning apprentice systems for practical software engineering  there are a number of ways in which the methods described here may be fruitfully extended. for example  equational theories have important technical limitations to their use in specifying software systems. there are other logical systems with different properties such as hom-clause logic  temporal logic and process logic which  like equational logic  may be used for representing and reasoning about programs. the notion of an institution  gives a common framework for treating such systems. it is desirable to carry over the application of explanationbased methods and goal regression for equational theories to theories in other institutions1 and to seek a more intrinsic characterization1 of the generalized objects obtained from their application. also the notion of morphism discussed here inhibits transformations on theories which are well motivated in practice . it is important to determine how the methods presented here can be applied to that programming knowledge as well. in a different direction  it is quite natural to ask how reuse based on the idea here of weakening the specification of the base theory in an implementation is related to reuse using other forms of generalization  e.g. through parametrization or inheritance. 
1. conclusions 
   the application of explanation-based learning presented in this paper provides a new approach to generalizing software developed in practice. it depends on and complements other research into the problem of software reuse based on formal methods for program specification. the abstract data types synthesized using explanation-based generalization provide minimal requirements for the valid reuse of interconnections between software components. at the same time  they can be used directly in modifying program derivations. 
   the creation of reusable software components constitutes a knowledge acquisition bottleneck for which a learning apprentice system can prove valuable. the results of this paper provide the basis for developing such a system as we are now doing  lasr . 
   the adaptation of explanation-based generalization is framed in terms of theories. theories provide a granularity more suitable for application to software reuse than single goal predicates do and are more suitable for representing and reasoning about complex abstractions. they may also prove valuable in other applications of explanationbased learning. 
1. the work reported in  1  suggests in particular applications involving non-determinism  and has been applied to deriving a minimal lowest layer of a multi-layered communications network architecture . 
1. intuitively  explanation-based generalization here can be thought of as the adjoint to the functor which maps validated morphisms to their validation. 
1 	knowledge acquisition 
   i would like to thank tom mitchell for introducing me to explanation-based learning and learning apprentice systems  and especially for a discussion which led to this work. discussions with c.a.r. hoare and he jifeng were very helpful for understanding their notion of weakest prespecification in data refinement; they also suggested the interpretation of generalizations in terms of adjoint functors. i am very grateful to alan snyder  martin griss  and ira goldstein of hp labs for enabling me to pursue this research. 
