
using the principle of homeostasis  we derive a learning rule for a specific recurrent neural network structure  the so-called self-adjusting ring module  sarm . several of these ring modules can be plugged together to drive segmented artificial organisms  for example centipede-like robots. controlling robots of variable morphologies by sarms has major advantages over using central pattern generators  cpgs . sarms are able to immediately reconfigure themselves after reassembly of the robot's morphology. in addition  there is no need to decide on a singular place for the robot's control processor  since sarms represent inherently distributed control structures.
1 introduction
in order for an organism to move  it is necessary that its limbs and its body itself move in a coordinated manner. a special case of coordination is found in segmented organisms like centipedes  where similar parts of the body move in the same way  but phase-shifted. if we want to implement an artificial organism that moves  i.e. an autonomous robot  we can use a central pattern generator  cpg  to drive the motor joints and thus select one of the well documented architectures from the literature  golubitsky et al.  1 . but when a modular structure that can be assembled during runtime is asked for  it is unclear where a cpg should be located  so a distributed approach will better fit the requirements.
¡¡in the following sections we introduce a concept of distributed control for the generation of locomotion  based on a single neural building block  the self-adjusting ring module  sarm .
1 two neurons form an so 1 -oscillator
using a discrete time  fully connected recurrent neural network with two neurons  no bias terms  and tanh as transfer function  the only parameters to specify are the weights which form a 1¡Á1-matrix w1. this matrix is based on a rotation matrix  thompson and steward  1   i.e an element of the unitary group so 1  and has the form
 
where
and
 
here s stands for the neurons' recurrent self-connections  and r for the ring-connections between the neurons. if we choose and thus compensate for the damping property of the transfer function tanh  the two neurons approximately oscillate with frequency f1 = ¦Õfs  relative to the update or sampling frequency fs of the network  pasemann et al.  1 . since the two neurons produce almost sine waves that differ in phase by ¦Ð/1  it is possible to generate a sine wave and other waveforms of the same frequency  but with arbitrary amplitude and phase  just using a weighted sum of the two neurons' output signals  hornik et al.  1; cybenko  1 .
1	a simple ring module
we can extend the so 1 -oscillator to the more general form of a ring oscillator by introducing more neurons  pasemann  1 . using m neurons  m ¡Ý 1   the weights can be written as the following m¡Ám-matrix
	s	r
vm =  wi j  =... ...       	s r    
where again s stands for the self-connections  for the ringconnections  and all other entries are zero. for the ring to oscillate we choose the condition
 
which for n ¡Ý 1 and m = 1n is fulfilled by the following alternative arrangement of weights:
.
clearly we have w1 = v1 = u1  but for n   1 always
  therefore u1n can
be implemented as n identical copies of the following ring module:

figure 1: structure of a ring module
the modules have a switch at each end which connects the two outputs as long as no other module is plugged in  thus single modules oscillate stand-alone. when modules are plugged together  their switches open up  so that the former two rings unite to one. for even n one needs to introduce a negation at some position in the ring  e.g. by means of a simple switch.
¡¡interesting applications of those modules emerge  if they have motor joints included  which are driven by a linear combination of the modules' internal neurons. flipping of one ring module within a chain of modules changes the phase relationships of all modules.
1	varying frequency and amplitude
the mapping s = cos 1¦Ð¦Õ  r = sin 1¦Ð¦Õ  no longer holds for the ring oscillators of type u1n  because the additional neurons introduce a delay which in turn considerably lowers the frequency. at the same time the amplitude rises  since there are more self-connections present which boost up the energy of the whole system.
¡¡there does not exist a simple formula for calculating s and r as a function of ¦Õ  but if the desired frequency is relatively low compared to the network's update frequency  i.e. for small ¦Õ  we find that:
s = cos 1¦Ð¦Õ  ¡Ö 1  r = sin 1¦Ð¦Õ  ¡Ö 1¦Ð¦Õ.
in other words  the weights of the self-connections always stay close to 1  mainly controlling the level of energy flow in the system and therefore the amplitude  whereas the weights of the ring-connections are approximately proportional to the frequency. the proportion ratio depends on the number of neurons. this is exactly what we will need to implement a ring module that is able to self-adjust its weights in order to maintain a certain frequency and amplitude.
1 introducing the principle of homeostasis
a central tenet of modern physiology is homeostasis  der and pantzer  1 . it describes the principle  that every living organism tries to settle down to a healthy internal state after it has been disturbed. this desired state is also called an equilibrium. we define the ring module to have reached its equilibrium  if it oscillates with target frequency f1 and target amplitude a1. the phase shift will not be of concern here. in order to apply homeostatic mechanisms to the ring module  we first have to introduce an appropriate measure for the equilibrium  urzelai and floreano  1 .
1	discrete sampled sine waves
if we sample one full period of a sine wave at n discrete time steps we can study the following two sums:
.
the first sum sa a n  calculates the average amplitude of the rectified signal and can easily be approximated by:

thus  a sine wave of peak amplitude a results in an average absolute neural activity of 1a/¦Ð  independent of the frequency. for n   1 the second sum sf a n  can be approximated by

the approximation error

is zero if n is a multiple of four. in general  the relative error is below 1% for n   1  and below 1% for n   1. altogether  assuming a constant amplitude  the average variation of neural activity is reciprocally proportional to the oscillation period.
1	the learning rules
having defined an equilibrium condition  we now need some example sets for weights of stable oscillating ring networks. as a starting point  we assume a network update frequency of fs = 1hz  which corresponds to the timing of standard servo motors. choosing an oscillation frequency of f1 = 1hz as moderate speed for a walking machine  we have ¦Õ = f1/fs = 1.
¡¡the larger the amplitude  the larger the distortions of the sine wave  due to the non-linearity of tanh  so using a1 = 1 as target amplitude is a good choice. varying the number of ring modules n ¡Ê {1 ... 1} with u1n as corresponding weight matrix  we get the following results by numerical simulation:
1ns1nr1ns1n + r1n1.1.1.11111.1.1.11111.1.1.1111table 1: weight settings
¡¡if we start with three coupled ring modules and then add one module  the amplitude grows by ¦¤a = 1 from 1 to 1. in order to compensate for this effect  we have to adjust the self-coupling weights by s1   s1 =  1. if we leave out one module  the weights have to be adjusted accordingly by s1   s1 = +1  so the average absolute adjustment is ¦¤s = 1. putting this together  we end up with a simple learning rule for the self-coupling weights  which stabilizes the amplitude:
  
where s t  is the weight and ¦Ò t  the neuron's output at time t  a1 and f1 are target amplitude and frequency  and ¦¤s  ¦¤a  and fs are defined as above. ¦Ås is the normalized learning rate  i.e. ¦Ås = 1 means  that after changing the number of connected ring modules  the target amplitude a1 will be reached again after the next full oscillation period.
¡¡two things should be noted: firstly  the appearance of f1 in the learning rule is just part of the learning rate's normalization and not essential for the proper amplitude regulation. secondly  since all parameters are known in advance  the formula can easily be implemented as s¦Ä t  = ¦Ç  ¦Ì   |¦Ò t |  
with pre-calculated constants ¦Ç and ¦Ì  according to a1  ¦¤a  f1  fs  ¦Ås  and ¦¤s. thus  only a multiplication  an addition  and a sign flip is needed. because of the computationally low cost  this learning rule can run even on simplest 1bitmicroprocessors  hikawa  1 .
¡¡in an analogous manner we get a learning rule for the weights of the ring-connections  which stabilizes the oscillation frequency:
 
where r t  is the weight at time t  ¦Ò t  and ¦Ò  t  are the outputs of the neurons after and before the r-weighted connection  respective  ¦År is the normalized learning rate  and the rest as defined above. again  this learning rule can also be written in a simple form as r¦Ä t  = ¦Ç  ¦Ì   |¦Ò t    ¦Ò  t |  
with pre-calculated constants ¦Ç and ¦Ì. note  that despite the term  ¦Ò  t   both formulas are identical. clearly  ¦Ò  t  must be zero in the first learning rule  since there is only one neuron involved with a self-connection.
¡¡since r¦Ä t  controls the oscillation frequency  but depends on a1 and f1 at the same time  this learning rule will only work properly combined with the learning rule for the selfconnections. care has to be taken with the choice of ¦Ås and ¦År  because too high a value will deform the waveform's shape and cause uncontrolled interactions between the two learning rules. good parameter choices are given at the end.
1	diffusion processes
if we assemble some ring modules with well-tuned parameters and then switch them on  the learning rules reliably adjust oscillation frequency and amplitude within a few oscillation periods. this will work equally well for a single module or up to six and more modules.
¡¡now  if we separate some modules and reassemble them all during runtime  something special may happen. since the separated modules go on oscillating and adjusting their weights independently from each other  we have an unpredictable distribution of weight values and neural activities at the time we reconnect the modules. the modules then end up with an unequal weight distribution. frequency and amplitude will be well preserved by the learning rules  but the relative phase shift between the modules on one side  and between the two neurons within one module on the other side will no longer be the same. in order to compensate for this  we introduce two independent diffusion processes  one for each weight pairwithin a module  as follows:
 
and similarly for the ring-connections:
.
the diffusion processes have to be applied immediately after the learning rule's weight update. generally it is sufficient to use small values for ¦Äs and ¦Är  i.e. .
¡¡when implemented on a simple 1bit-microprocessor  as mentioned above  only additions are needed  since a division by two can be implemented as a right shift. the same is true for smart choices of ¦Äs and ¦Är.
1 results and discussion
to illustrate the effect of the homeostatic learning rules and diffusion processes  we simulate hot plugging and unplugging of one to five continuously operating sarms  first with fixed weights  then with homeostatic control of the weights. for the first simulation we set s = 1  r = 1  and start with three concatenated sarms. the result can be seen in the following figure:

figure 1: neural output when using fixed weights
as expected  the three ring modules oscillate with amplitude a1 = 1 and frequency ratio f1/fs = 1  which corresponds to an output frequency of 1hz  assuming 1 time steps per second. after 1 seconds  at time step 1  two additional sarms are plugged in. in consequence  the amplitude slightly increases and the frequency considerably lowers down from 1hz to 1hz. after further 1 seconds  at time step 1  four of the five sarms are removed and almost immediately the oscillation completely breaks down. 1 seconds later again two sarms are added  at time step 1   but the system is not able to recover from zero to oscillation. theoretically oscillation should restart  but due to the finite accuracy  even  of double precision numbers this is practically never the case.
¡¡we now repeat the whole sequence of plugging and unplugging  but with learning rules and diffusion processes switched on. in order to have comparable conditions  we use appropriate parameters for target amplitude  target frequency  and network update frequency. as pointed out earlier  it is crucial to quickly reach a stable amplitude  since the frequency controlling adjustment of the ring-connections
parametersymbolvaluetarget amplitudea1.1target frequencyf1hznetwork update frequencyfs1hzlearning rate self-connections¦Ås1learning rate ring-connections¦År1diffusion self-connections¦Äs1diffusion ring-connections¦Är1table 1: parameter settings
depends on the amplitude  whereas the amplitude controlling adjustment of the self-connections is frequency independent. table 1 shows which parameter settings have been proven to be robust. using these settings we get the following system behavior:

figure 1: neural output of self-adjusting ring modules
clearly the sarms now find back to their equilibrium  after being disturbed by hot reassembly. as can be seen  first the amplitude is back to 1 after a few periods  whereas the frequency settles down afterwards  see figure 1  time steps 1 to 1  1 to 1  and 1 to 1 . three findings are to be mentioned in comparison to the simulation with fixed weights:
  depending on the number of active sarms  all weights target their optimal value as listed in table 1. this implicates  that each sarm in principle is able to derive the exact number of attached sarms - without dedicated communication and regardless of their distance. the starting weights s = 1 and r = 1 are reached again  once the starting number of sarms is restored. loosely spoken  each part of the body statically feels the rest of the body and is dynamically aware of morphological changes to the whole body.   the oscillation never breaks down  see time steps 1 to 1 . more loosely spoken and in analogy to physiology it can be stated  that homeostasis prevents the  artificial  organism from death.
  close inspection of the slow varying self-connections reveals  that the target frequency shines through slightly  can be best seen shortly after time step 1 . this indicates an optimal setting of the self-connection's learning rate. a higher rate would lead to destabilization  whereas lower rates would prolongate reaction time.
¡¡despite these more analytical and philosophical reflections  the self-adjusting ring modules can practically be built in hardware and used to make artificial creatures move. an actuated modular body construction system as described in  raffle  1  forms an optimal application. it then is a good idea to have the modules equipped with a switch which introduces a negation before one ring-connection  as necessary for an even number of interconnected ring modules. now  this switch can also be used to turn off the oscillation of the whole network and settle down into a configuration of constant neural output signals. different steady output configurations are possible  just by switching on the negation at different modules.
