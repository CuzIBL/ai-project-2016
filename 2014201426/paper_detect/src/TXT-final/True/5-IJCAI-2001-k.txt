
when several agents learn concurrently  the payoff received by an agent is dependent on the behavior of the other agents. as the other agents learn  the reward of one agent becomes non-stationary. this makes learningin multiagentsystems moredifficult than single-agent learning. a few methods  however  are known to guarantee convergence to equilibrium in the limit in such systems. in this paper we experimentally study one such technique  the minimax-q  in a competitive domain and prove its equivalence with another well-known method for competitive domains. we study the rate of convergence of minimax-q and investigate possible ways for increasing the same. we also present a variant of the algorithm  minimax-sarsa  and prove its convergence to minimax-q values under appropriate conditions. finally we show that this new algorithm performs better than simple minimax-q in a general-sum domain as well.
1	introduction
the reinforcement learning  rl  paradigm provides techniques using which an individual agent can optimize environmental payoff. however  the presence of a non-stationary environment  central to multi-agent learning  violates the assumptions underlying convergence proofs of single agent rl techniques. as a result  standard reinforcement learning techniques like q-learning are not guaranteed to converge in a multi-agent environment. the focus of convergence for multiple  concurrent learners is on an equilibrium strategy-profile rather than optimal strategies for the individual agents.
　the stochastic-game  or markov games  framework  a generalization of markov decision processes for multiple controllers  has been used to model learning by agents in purely competitive domains  littman  1 . in that work  the author has presented a minimax-q learning rule and evaluated its performance experimentally. the q-learning rule  being a model-free and online learning technique  is particularly suited for multi-stage games  and as such  an attractive candidate for multiagent-learning in uncertain environments. hu and wellman  1 extended littman's frameworkto enable the agents to converge to a mixed-strategy nash equilibrium. there have also been other research on using the stochastic-gameframeworkin multiagent learning  such as an empirical study of multiagent q-learning in semi-competitive domains  sandholm and crites  1  among others.
　the first two techniques mentioned above are known to converge in the limit. in this paper we show that these two techniques are essentially identical in purely competitive domains. to increase the convergence rate of the minimaxq rule  we extend it to incorporate sarsa  rummery  1; sutton and burto  1  and q     peng and williams  1  techniques  and study the convergence rates of these different methods in a competitive soccer domain  littman  1 .
1	multiagent q-learning
definition 1 a markov decision process  mdp  is a quadruple   where is the set of states  is the set of actions  is the transition function 
　　　 	being a probability distribution  and r is the reward function 	.
a multiagent reinforcement-learningtask can be looked upon as an extended mdp  with specifying the joint-state of the agents  being the joint-actions of the agents  where is the set of actions available to the agent  as the joint-transition function  and the reward function is
redefined as	. the functions	and
are usually unknown. the goal of the agent is to find a strategy that maximizes its expected sum of discounted rewards 	where is the initial joint-state  is the reward of the agent at time   is the discount factor  and is the strategy-profile of 's opponents.
definition 1 a bimatrix game is given by a pair of matrices 
	   each of size	for a two-agent game 
where the payoff of the	agent for the joint action
is given by the entry
.
each stage of an extended-mdp for two agents  it can be extended to agents using -dimensional tables instead of matrices  can be looked upon as a bimatrix game. a zerosum game is a special bimatrix game where
.
definition 1 a mixed-strategy nash equilibrium for a bimatrix game is a pair of probability vectors
such that
where	is the set of probability-distributions over the agent's action space.
no player in this game has any incentive for unilateral deviation from the nash equilibrium strategy  given the other's strategy. there always exists at least one such equilibrium profile for an arbitrary finite bimatrix game  nash  1 .
　an individual learner may  but need not  use a model of the environment to learn the transition and reward functions. qlearning is one example of model-free learning. in greedy policy q-learning  an agent starts with arbitrary initial qvalues  or action values  for each state-action pair   and repeatedly chooses actions  noting its rewards and transitions and updating as
where
 1 
and	is the learning rate.	watkins and dayan
 1  have proved that the above iteration converges to optimal action values under infinite sampling of each state-action pair and a particular schedule of the learning rate.
　in the case of multiagent learning  the above iteration would not work  since the maximization over one's action is insufficient in the presence of multiple actors. however  if the reward-function of the opponent is negatively correlated  then actions can be selected by solving the bimatrix-game greedilyfor the opponent andpessimistically for oneself  to guarantee a minimum expected payoff. this produces littman's minimax-q algorithm for simultaneousmove zero-sum games  for which the value-functionfor agent 1 is
 1 
where and are the action-sets of the learning-agent agent 1  and its opponentrespectively  and is a vector of action values of the learner correspondingto its opponent's action . the current policy can be solved by linearprogramming for the constrained  minimax optimization on
       . the minimax-q learning rule has been proved to converge to optimal action values  szepesva＞ri and littman  1 .
　for general-sum games  however  the agent needs to know   in absence of which it has to model its opponents. in such games  each agent can observe the other agent's actions and rewards and maintains separate q-tables for each of them in addition to its own  hu and wellman  1 . the value-function for agent 1 in this case is
 1 
where are the nash-strategies of the agents for the bimatrix game	  which can be solved by quadratic-programming technique  mangasarian and stone  1 . in zero-sum games  the value-function in  1  simplifies to
 1 
this algorithm converges to a nash equilibrium  for a restrictive class of nash-equilibria  hu and wellman  1   that in addition to the constraints imposed by its definition  satisfies the following
　though this may not be true in general  it holds for zerosum domains  where any deviation by the opponent from its equilibrium-strategy decreases its expected payoff  thus increasing the modeler's expected payoff. hence  the convergence of this algorithm is guaranteed in zero-sum domains. furthermore  the algorithms developed by littman  1  and hu and wellman  1   we call the latter nash-q  differ structurally only in the use of rules  1  and  1  respectively  in updating . but contrary to the statement made by hu and wellman  1   these expressions are functionally equivalent in zero-sum games. game theory  thie  1  states that in a zero-sum game
 1 
an informal argument may go as follows. let
	  where	represents the	th column of
	and	.	then
	for any	. when
is treated as a random variable from the distribution   we have
on the other hand  since any pure strategy is also a mixed strategy  we have
consequently we have the equality  1 . therefore we observe that both minimax-q and nash-q compute identical policies and value-functions in zero-sum domains.
1	expediting minimax-q learning
since minimax-q and nash-q algorithms are equivalent in the purely competitive domains that we consider in this paper  we focus on the minimax-q algorithm since it does not require maintenance of opponents' q functions and hence is resource-efficient. we now turn to explore possible ways to speed-up the chosen algorithm.
1	fast minimax-q   
since q-learning updates only the last state with the reinforcements  it is substantially slow in updating the action values. a well-known technique to speed-up singleagent q-learning is to integrate it with td    rewardestimation scheme  producing the q   -learning rule  peng and williams  1 . for experimentation  we have used a faster version of the peng-williams' algorithm  where the updates are 'lazily' postponed until necessary  wiering and schmidhuber  1 . q    can be applied to each of the two learning schemes in the previous section  by defining in the q    algorithm by the equation in  1  or  1  as the case may be. the guarantee of convergence  however  may no longer hold.
1	minimax-sarsa learning
the previous techniques were all off-policy learning rules  where the expected value of the next state is used to update the value of the current state. the sarsa 1  technique is  on the other hand  an on-policy learning rule that depends heavily on the actual learning policy followed  rummery  1; sutton and burto  1 . in general  off-policy algorithms can separate control from exploration while on-policy reinforcementlearning algorithmscannot. despite this  on-policy algorithms with function approximationin single agent learning appear to be superior to off-policy algorithms in control as well as prediction problems  boyan and moore  1; sutton  1; tsitsiklis and roy  1 . on-policy algorithms can learn to behaveconsistently with exploration  sutton and burto  1  moreover  on-policy algorithms are more natural to combine with eligibility traces than off-policy algorithms are. this raises the following question that this research effort endeavors to answer: can on-policy rl algorithms perform equally well in multiagent domains  in that case we can possibly achieve faster convergence of a hybrid of the sarsa technique and minimax-q rule.
　in a simple q-learning scenario  the sarsa technique modifies the update rule  1  as . thus a sarsa rule learns the values of its own actions  and can converge to optimal values only if the learning policy chooses optimal actions in the limit. in a multiagent minimax-q setting  the rule  1  would be replaced  for agent 1  by
while the policy to choose actions would still be computed by the original minimax-q rule. to achieve convergence of this rule to minimax-q values  we follow an -minimax strategy that satisfies the need of infinite exploration while being minimax in the limit  i.e.  decays to 1 in the limit. we call such exploration'minimax in the limit with infinite exploration'or mlie. our convergence result rests on the following lemma established by singh et al.  1 .
lemma 1 consider a stochastic process            where satisfy the equations
where let be a sequence of increasing -fields such that and are measurable and and	are	measurable 	. assume that the following hold:
1. is finite.
1. w.p.1.
1. 1	  where	and converges to 1 w.p.1.
1.   for some constant	.
then 	converges to 1 with probability 1  w.p.1 . the update rule for sarsa for agent 1 say  is
 1 
we also note that the fixed point of the minimax-q
rule  szepesva＞ri and littman  1   for agent 1  is
 1 
now we state and prove the theorem for convergence of minimax-sarsa learning using lemma 1.
theorem 1 the learning rule specified in  1  converges to the values in equation  1  with probability 1 provided is chosen using an mlie scheme at each step   the immediate rewards are bounded and have finite variance  the q-values are stored in lookup tables  the learning rate    satisfies condition 1 in lemma 1  and the opponent plays greedily in the limit.
proof:  outline  writing	in lemma 1 as  	  and
as	  and defining unless	  we have
 1 
which gives rise to
　it can be shown that the measurability and variance conditions are satisfied and that
for some  since minimax-q operator is a contraction   according to the outline provided by singh et al.  1 . the remaining task is to show that vanishes in the limit  under mlie exploration. we consider the following cases:
case 1:	.

since
	 	we	have
	and	the	corresponding	ex-
pected value vanishes in the limit if the opponent plays greedily in the limit.
case 1:	.

again 
where
	.	hence
	.	the
associated expected value vanishes again in the limit due to the assumption of an mlie policy on part of agent 1  and independent of the opponent's behavior.
let be the maximum of the two upper limits on established above. we see that
and the r.h.s van-
ishes for each state-action tuple. hence  vanishes for each state-action tuple  which implies that vanishes in the limit under mlie exploration and optimal play by the opponent in the limit. setting in lemma 1 to   we conclude that minimax-sarsa rule converges to the minimax-q values under mlie exploration with probability 1  if the opponent plays greedily in the limit  and under appropriate structure
of	.	 q.e.d. 
　note that case 1 needs the boundedness of that follows easily under additional assumptions. it might be argued that the condition of greedy play by the opponent in the limit is restrictive. however  this is typical of a convergence proof of on-policy algorithms that requires more details of the actions taken by the agents. gains from on-policyalgorithmsin terms of learning efficiency and cost offset the condition of greedy play in the limit.
1	experiments in a competitive domain
to evaluate the proposed schemes  we used the purely competitive soccer domain  littman  1 . it is a grid containing two agents  and   as shown in figure 1  that always occupy distinct squares. the goal of agent is on the left  and that of on right. the figure 1 shows the initial positions of the agents  with the ball being given to an agent at random at the start of each game  agent in figure . each agent can choose from a fixed set of five actions at each state: going up  left  down or right  or staying where it is.

figure 1: the experimental soccer domain.
　when both the agents have selected their actions  these actions are executed in a random order. if an agent bumps onto another  the stationary agent receives the ball  and the movement fails. an agent receives reinforcements of +1 for a goal  or a sameside by the opponent  and -1 for a self-goal  or a goal by the opponent  to maintain the zero-sum character of the game  and in all other cases the reinforcement is zero. whenever a non-zero reward is received  the game resets to the initial configuration. we shall call an agent following littman's minimax-q algorithm an m-agent.
　in the training phase of the experiments  we performed symmetric training between two ordinary m-agents  two magents both using the rule  and two m-agents both using the sarsa rule. the respective policies learnt  are denoted as   which are recorded at the end of each iterations. each training lasted 1 iterations in all. we used identical exploration-probabilities as that by littman  1  and the decay-factor for the learningrate was set to 1.

figure 1: games are played by	 left  and
 right  against for various values of  horizontal axis . the percentages of wins  vertical axis  by the former in each case are plotted  averaged over 1 runs .

figure 1: games are played by against for various values of . the percentages of wins  averaged over 1 runs  by the former in each case are plotted.
　in the test phase  we allowed an policy to play against an policy  for . each test iteration results in a draw with a probability of 1  to break possible deadlocks. the resultant percentages of win by the policies over its opponent are reported in figure 1  left . the approximate trend suggests that ordinary minimax-q initially dominates but minimax-sarsa gradually catches up and outperforms the former. in figure 1  right   the corresponding results from playing against are shown. in this case the minimax-q    rule outperforms the ordinary minimax-q algorithm from the very beginning. however  the policies gradually lose their edge as the ordinary minimax-q rule learns better progressively. the figure 1 corroboratesthese observations as performswell against
　　　  but this performance decays with increasing . was set to 1 in both experiments.
　we note that percentage of wins in such games may not be a good comparative estimator of the policies. a better estimator would be the average rms deviations of the q-values from their convergencevalues. however  the latter can be calculated in this domain only with extensive off-line computation. we also stress that the results reported are far from convergence  at which all the algorithms should perform equally well. the reason why smm beats mm can be understood in the context of updates. while smm uses the actual action value from the next state to update the current state  mm still uses the minimax value from the next state  which postpones relying on the individual table-entries. as a result  we expect mm to catch up  in fig. 1 left  when learning continues.
1	experiments in a general-sum domain
we note that minimax-q rule is applicable in general-sumdomains as well  where the rationale of the assumption of minimizing policy of the opponent is to guarantee a minimum security level to the learner  instead of maximizing the reward of the opponent itself as in the zero-sum interpretation. the sarsa and versions will still work in such domains. for the purpose of experimentation  we introduce a general-sum domain that we call  tightly coupled navigation.  this is a gridworld as shown in figure 1. the values in the lower left corner of each cell in figure 1 is the reward to agent 1 for reaching the state corresponding to that cell. similarly the values in the upper right corner are those for agent 1. the rewards in this domain are state-based  i.e. the reward corresponding to a cell is received if the agents reach or remain in that cell. here the agents are tightly coupled as they must always occupy the same cell. each agent has three available actions in each state  viz. up  down  right. however  since they are coupled  they can move only when they choose the same action; otherwise they remain in the same state. the starting and the absorbing states have been shown in the figure 1. when the agents reach the goal state  each receives the reward 1 and without making any update in this iteration  the game restarts with the agents reshifted to the start-state and updates begin once again.
　a more realistic scenario for this domain is car-driving. consider two agents in the same car and each having a steering wheel in its hands. the car moves in a given direction if both move the wheels in that direction; otherwise the car does not move. there may be different paths that the agents wish to follow to reach their common goal. however  since they are tightly coupled  they must strike a compromise and find an intermediate path that both can be maximally satisfied with  given the coupling.
　we have symmetrically trained two minimax-q and two minimax-sarsa agents in this domain. the exploration probabilities forthe agents in each iterationwere the same as in the
1-11111111-111-1goal / absorbing
start
statestate
figure 1: the tightly coupled navigation domain.
soccer domain  viz. . we varied the probability of rewardgeneration in each iteration using three values  viz. 1  1 and 1  where 1 stands for the case where rewards are generated only when the agents reach the goal state. we wanted to study the effect of infrequent rewards  which is a realistic scenario in most practical domains  on the convergence of our algorithms. we expected the convergence rates to fall with more and more infrequent rewards. in order to study the convergence the exact minimax-qtables were computedoff-line and an average rms deviation of the learned q-tables every 1 training-iterations were plotted. the trainings lasted a total of 1 iterations.

1
1 1 1 1 1
number of iterations  1s 
figure 1: mean rms deviation plots of minimax-sarsa  solid  and ordinary minimax-q for probability of rewardgeneration = 1.
	from figures 1  1 and	1  we can see that the minimax-
sarsa rule always performs better than the ordinary minimaxq rule. the errors in all the cases decrease monotonically which suggests that both the rules will eventually converge. as expected  the error-levels fall with increasing probability of reward-generation. a scrutiny of the minimax-q tables show that the minimax path learned by each agent should be different from the nash-equilibrium path that is corroborated by the learned q-tables.

1
1 1 1 1 1
number of iterations  1s 
figure 1: mean rms deviation plots of minimax-sarsa  solid  and ordinary minimax-q for probability of rewardgeneration = 1.

1
1 1 1 1 1
number of iterations  1s 
figure 1: mean rms deviation plots of minimax-sarsa  solid  and ordinary minimax-q for probability of rewardgeneration = 1.
1	conclusion and future work
we conclude that both the sarsa and versions of minimax-qlearningachievespeed-upon littman's minimaxq rule  and more so for the rule. though this latter rule works well  we are not aware of the theoretical convergence properties of this method. exploring these properties is one open area. we also note that a combination of minimax-sarsa and to form what could be called minimax-sarsa     would probably be more expedient than either of the two  by naturally combining their disjoint areas of expedience  seen in the plots in figure 1. results from associated experiments are awaited. we could also substitute nash-learning for minimax-learning and achieve nashq    and nash-sarsa  specialized fast learning proceduresfor general-sum domains. a theoretical proof of convergence of such a nash-sarsa would be along the same lines as presented in this paper for minimax-sarsa. we plan to conduct experiments with all these hybrid algorithms.
references
 boyan and moore  1  j.a. boyan and a.w. moore.
generalization in reinforcement learning: safely approximating the value function. in advances in neural information processing systems 1  pages 1  1.
 hu and wellman  1  j. hu and m. p. wellman. multiagent reinforcement learning: theoretical framework and an algorithm. in proc. of the 1th int. conf. on machine learning  ml'1   pages 1  san francisco  ca  1. morgan kaufmann.
 littman  1  m. l. littman. markov games as a framework for multi-agent reinforcement learning. in proc. of the 1th int. conf. on machine learning  pages 1  san mateo  ca  1. morgan kaufmann.
 mangasarian and stone  1  o. l. mangasarian and h. stone. two-person nonzero-sum games and quadratic programming. journal of mathematical analysis and applications  1 - 1  1.
 nash  1  john f. nash. non-cooperative games. annals of mathematics  1 - 1  1.
 peng and williams  1  j. peng and r. williams. incremental multi-step q-learning. machine learning  1 - 1  1.
 rummery  1  g. a. rummery. problem solving with reinforcement learning. phd thesis  cambridge university engineering department  1.
 sandholm and crites  1  t. sandholm and r. crites. on multiagent q-learning in a semi-competitive domain. in g. wei and s. sen  editors  adaptation and learning in multi-agent systems  pages 1. springer-verlag  1.
 sutton and burto  1  r. sutton and a. g. burto. reinforcement learning: an introduction. mit press  1.
 sutton  1  r. sutton. generalization in reinforcement learning: successful examples using sparse coarse coding. in advances in neural information processing systems 1  1.
 szepesva＞ri and littman  1  csaba szepesva＞ri and m.l. littman. a unified analysis of value-function-based reinforcement-learning algorithms. in neural computation  1. submitted.
 thie  1  p. r. thie. an introduction to linear programming and game theory. john wiley and son  1nd. edition  1.
 tsitsiklis and roy  1  j.n. tsitsiklis and b. van roy. an analysis of temporal-difference learning with function approximation. ieee transactions on automatic control  1.
 wiering and schmidhuber  1  m. wiering and j. schmidhuber. fast online q   . machine learning  1   pages 1  1.

machine learning and
data mining
reinforcement learning/robotics

multi-agent systems by incremental gradient reinforcement learning
	alain dutech	olivier buffet	franc ois charpillet
buffet dutech charp  loria.fr loria  bp 1 vandoeuvre-le`s-nancy
france

abstract
a new reinforcement learning  rl  methodology is proposed to design multi-agent systems. in the realistic setting of situated agents with local perception  the task of automatically building a coordinated system is of crucial importance. we use simple reactive agents which learn their own behavior in a decentralized way. to cope with the difficulties inherent to rl used in that framework  we have developed an incremental learning algorithm where agents face more and more complex tasks. we illustrate this general framework on a computer experiment where agents have to coordinate to reach a global goal.
1	introduction
multi-agent systems  mas  - systems where autonomous entities called agents interact with each other - are a growing interest in the artificial intelligence community  both in terms of research and applications  see  ferber  1  . whereas mas are usually build  by hand  using simulations to tune up the system to reach a desired global behavior  this paper deals with a key problem : the design of mas in an automated fashion through learning.
　learning in mas can indeed take many forms  stone and veloso  1 . we opted for reinforcement learning  rl  methods  sutton and barto  1  as they do not require a teacher knowing before hand a solution of the problem. an evaluation of the current agents' actions  using a scalar value for example  is enough to learn.
　furthermore  we consider multi-agent systems composed of simple reactive situated agents with local perceptions. thus  the system is easier to build and can nevertheless solve complex problems as  in this decentralized framework  each agent faces simpler tasks.
　as rl suffers from combinatorial explosion  decentralization of the learning process should bring the same benefits  i.e. each agent learns its own behavior by itself which simplifies the task to be learned. besides  it is consistent with the localized aspect of realistic situated agents. but  in this context  two major difficulties burden rl :
hidden global state. usual situated agents can only rely on an imperfect  local and partial perception of their world. then  the global state of the system stays unknown  which prevents classical rl algorithms from finding an optimal policy  as shown by  singh et al.  1 .
credit assignment problem. when a positive reward is given by the environment  it is not always evident to credit positively the  good  actions that led to this reward. with many agents  this problem is even more crucial as we must also decide which agents to reward.
　our answer to these problems is a decentralized incremental learning algorithm based on a classical rl technique to find stochastic policies. actually  agents with stochastic behaviors are more adapted to the problem of partial perception. by incremental  we mean that agents are progressively pitted against harder and harder tasks so as to progressively learn a complex behavior. another aspect of the incremental learning is to begin learning with very few agents  so as to minimize coordination and cross-work actions  and then export these basic behaviors to tasks with more and more agents. eventually  behaviors can be further refined by learning in these more demanding environments. thus  the originality of our approach is twofold : learning is decentralized and incremental.
　in this paper  we present our method for learning in a multiagent system and an experiment on which we tested our ideas. section 1 gives the details of our framework  then  sections 1  1 and 1 describe the experiments conducted to test the viability of our approach. a discussion about our work follows in section 1  highlighted by some comparisons from other similar works. future directions and conclusive remarks end the paper in section 1.
1	our framework
in this part  we first present the kind of agents we consider. then we show the problems one faces when using reinforcement learning in a multi-agent setting. lastly  we explain how incremental learning brings a partial solution to these problems.
1	the agents
we are interested in designing multi-agent systems  mas  by having each individual agent learn its behavior. as written earlier  we have decided to work with very simple reactive agents for complexity reasons. besides  it allows us to concentrate on the learning aspect of the design. among many possible choices  our agents can be characterized as :
situated with local perception : local perceptions are more thoroughly discussed in section 1.
possibly heterogeneous : through the learning process where agents learn individually  each agent can acquire a different behavior from the others.
cooperative : all agents share the same goal and they will have to coordinate to reach it.
we say nothing about other characteristics  for example communications  as they are not important for our framework and could be easily incorporated into the agents.
1	limitations of classical rl
reinforcement learning  rl  methods are very appealing ways of learning optimal memoryless behaviors for agents as they only require a scalar feedback from the system to the agents for them to learn. besides  these techniques can be used when there is uncertainty in the world's evolution.
　but the convergence of rl algorithms  like q-learning or td     has only been proven for markov decision processes  mdp . a mdp is defined as a tuple  being a finite set of states and a finite set of actions. when the system is in given state   an action being chosen  the probability for the system to end in state is given by
　　　　. after each transition  the environment generates a reward given by . the problem is then to find the optimal mapping between states and actions so as to maximize the reward received over time  usually expressed as a utility function	. such a mapping is called a policy.
　as pointed out by  boutilier  1  the evolution of the kind of mas we are interested in is a mdp. as such  it could be solved using classical reinforcement learning algorithms where the state of the system is the composition of the states of all agents and an action is a joint action composed of all individual actions of the agents. thus  the number of states and actions in a centralized view of the problem should quickly prove to be too big for rl to be applied  as illustrated in section 1. besides  solving our problem this way would mean to solve it in a centralized way whereas we aim at a noncentralized solution as each agent should learn by itself.
　unfortunately  as shown by  bernstein et al.  1   solving the problem in a non-centralized way when the agents only have a partial perception of the system's state is nexpcomplete  i.e. there is provably no polynomial algorithm to solve the problem. we face two major difficulties :
1. non-stationary transitions. reactive agents with a local view of the environment can not use joint actions to solve the problem. in fact  other agents are unpredictable elements of the environment. as a consequence  the transitions from one state of the system to another  as seen by an agent  are non-stationary : for example the probability for an agent to move ahead depends greatly on the actions of other adjacent agents.
1. partial observability. as the agent's perception is local they can not know the global state of the problem. as such  the problem at hand belongs to the class of partially observed markov decision models  see  littman et al.  1  .
　classical stationary partially observed markov decision processes are nearly impossible to solve when there are more than a hundred states  dutech  1 . the combination of the two problems  i.e non-stationarity and partial observation  makes the problem non-solvable without using approximations.
1	incremental gradient reinforcement learning
we propose to find approximate solutions by using incremental gradient rl. the main idea of this methodology is to progressively scale up with the complexity of the problem. agents run their own local version of rl  here a gradient descent described in  baxter and bartlett  1 .
stochastic behaviors
in the context of partially observedmdp  stochastic behaviors perform better than deterministic policies  see  singh et al.  1  . as previous experiments  see  buffet et al.  1   with q-learning were not conclusive to this regard  we chose to work with a gradient descent reinforcement learning algorithm especially suited for stochastic policies. this algorithm  designed by baxter  baxter and bartlett  1  is detailed in section 1.
incremental learning
to speed up learning and reduce the problems of complexity and credit assignment  we propose a methodology for incremental learning. the most obvious way is to use :
growing number of agents : learning starts with a small number of agents  each learning its own strategy. there must be enough agents to solve the problem. then  more agents are added  with initial policies taken from the original agents and refined through learning if needed.
incremental learning is also possible along the complexity dimension of the problem. agents are pitted against harder and harder tasks. first tasks are  near   in term of number of actions  positive reinforcement positions  then further and further away. so  we use :
progressive tasks : learning begins with a very simple version of the task to be executed  or with the agent being heavily guided to solve the task. then  as learning progresses  the task is made harder usually by giving more freedom of action to the agents
1	on local perception
we use situated agents with local perceptions to reduce the complexity of the problem as  very often  centralized problems are often too huge to be solved  see section 1 . this means that an agent does not know the global state of the world  which makes rl difficult to use.
　however  local perceptions are a prerequisite for incremental learning  as behaviors of the agents can be based on the  nearby  world. thus  they scale immediately with the number of agents  the dimension of the world or the complexity of the task.
1	experimenting with incremental learning
an application of this general notion of incremental learning is given in the experiments described here. after a short description of the problem  we give the details of the incremental tasks we used : harder tasks first and then more agents.
1	problem description
the task chosen involves agents  either yellow or blue  in a grid world whose goal is to push yellow cubes against blue ones1. when two agents coordinate their movements to attain this goal -pushing together a pair of cubes- both cubes temporarily disappear. simultaneously  agents responsible for this fusion receive a positive reward. the goal is to merge as many cubes as possible in a given time.
　considering our agents' abilities  they simply have four possible actions corresponding to moving north  east  south and west  they always try to move . agents can push other agents and other cubes so the consequences of their actions are stochastic  depending on which constraints will be considered first.
agents' perceptions  as shown on figure 1  are :

dir oa  : direction of nearest agent from opposite color  n-e-s-w  
dir cy  : direction of nearest yellow cube  n-ne-e-se-s-sw-w-nw   dir cb  : direction of nearest blue cube  n-ne-e-se-s-sw-w-nw   near cy  : is there a yellow cube in one of the eight nearest positions  true|false   near cb  : is there a blue cube in one of the eight nearest positions  true|false .
combining these  there exists a maximum of observations  some combinations of perceptions are not possible . we reduce this number to by using symmetries of the problem. this number is very small compared to the 1.1 states of the totally observed centralized problem  and also independent of the world's size.
　the reward function we have chosen eases the creditassignment problem. only the two agents identified as having originated the fusion of two cubes get a reward      whereas in other cases the reward is zero for all agents. a  global  reward would not be very coherent with agents having only local informations.
　we conclude by some remarks about the simulation itself. to focus on the learning problem  we have implemented simple mechanisms to avoid some non-significant problems. for example  cubes cannot go on border cells of the grid. similarly  when cubes reappear on the grid  they can only be put on inner cells.

cy : yellow cube - cb : blue cube - oa : other agent
figure 1: perceptions' examples  two agents in a simple world 
1	learning algorithm used
as said earlier  each agent uses its own gradient descent algorithm to learn its policy . we precisely use an on-line version of the algorithm.
　as proposed by baxter  baxter and bartlett  1   a policy depends on a set of parameters . this leads to an expected utility . the gradient descent permits 1 to find a locally optimal policy by finding that maximizes . the set of parameters we chose is
     with a real valued parameter. the policy is defined by the probabilities of making action in state as :

1	the 1-agent and 1-cube case
1	reduced problem
the first step in our incremental learning scheme is to use small sized world with a minimal number of agents : one agent and one cube of each color. then  beginning with positive reinforcement situations  the agents will face harder and harder problems.
1	progressive task
for our problem  there is only one situation which leads to a positive reward. this situation is thus the starting point of incremental learning. agents face a sequence of tasks to learn before ending in a standard environment where they keep learning.
　to be more precise  we define a try as a sequence of steps1 beginning in a given situation. this try must be repeated sufficiently   times  to be useful. this succession of tries will be called an experiment for our agents. the trainer has to define a sequence of progressive experiments to help learning. following  asada et al.  1   we designed a training sequence where starting situations are more and

table 1: the sequence of experiments we used for incremental learning.

figure 1: 1 agents  1 cubes : incremental vs. raw learning
more  distant  from the goal  as asada showed the efficiency of such a method.
　table 1 shows a sequence of experiments we used to help our agents in their training. the first starting configuration  on a world  needs only one move to reach the goal  each agent pushing toward the cubes. however  they have up to 1 steps to reach it  so they can explore different moves   and they can try this 1 times.
　figure 1 shows the evolutions in the pair of agents' efficiency whether using or not progressive learning. the curve showing the efficiency of learning after a period of assisted training only begins after the 1 steps of this training. once this delay elapsed  notably better performances are reached with the assistance provided by our method.
cubesagents111111111.1.1.1.1.1111111.1.1.1.1.111111table 1: average efficiencies number of merges for 1 steps
1	more agents and more cubes
1	the next step in incremental learning
until now  only two agents were used. more could be necessary to reach our goal in a world with more cubes.
　in this section  we first have a look at the efficiency of a policy learned with and used with more agents and more cubes  and then we let agents' policies evolve in this more populated worlds.
　note : here  we use agents that have either already learned a policy through the 1c case with help or have never learned anything at all.
1	influence of the number of agents and cubes
in this part of our work  the first interest was to check the efficiency of different numbers of agents having to deal with different numbers of cubes  taking the same number of agents  or cubes  of each color . in this experiment  we use agents whose fixed behaviors have been learned in the case  which is not supposed to be very efficient with more cubes. nevertheless it gives them enough good reactions to have some good results.
　several tests were carried out with 1  1  1  1 or 1 cubes and 1  1  1  1 or 1 agents  always in a world of size  . we used series of 1 consecutive steps  these series beginning in random configurations. but as blocking situations could sometime occur  1 such series were made in each case.
　table 1 gives the average efficiency in each of the twentyfive cases  the efficiency is the number of merges made in 1 steps . it quickly shows that there seems to be an optimal number of agents for a given number of cubes as denoted by bold numbers in table 1.
　a growing number of agents improves the results  until they cramp each other and bring too many coordination problems. with a growing number of cubes  the results are improved only up to an optimal number beyond which agents hesitate on which cubes to work with and fall easily into oscillatory sequences of moves.
　after another set of tries  in the same conditions but with policy-less agents  it appears that agents with a random behavior have always bad results  whatever their number.
1	incremental learning along number of agents
with agents whose fixed behaviors have been learned in the 1 agents and 1 cubes case  1c-policies   we have seen that a growing number of cubes induces problems that can be solved

1
1 1 1 1 1 1 number of steps  *1 
figure 1: evolution while learning from scratch
 the and curves are practically confounded with abscissa axis. 

1
1 1 1 1 1 1 number of steps  *1 
figure 1: evolution while learning from 1c behaviors
with a sufficient number of agents. the incremental learning solution is to improve the policies by keeping on learning with more agents.
　figures 1 and 1 show the efficiencies evolving with time in different learning cases : different number of objects  and agents learning from scratch or not. as shown on figure 1  agents learning from scratch have a slow evolution if there are only a few objects  agents and cubes  in the grid-world. but their learning speed rises with the rate
. when a group of agents seems to
have reached a maximum efficiency  their results are better than the results obtained in section 1 with the same situation but fixed policies. this confirms the interest of adaptation through learning.
　one can notice that the optimal efficiency reached by agents using incremental learning is by far better than the performances reached using learning from scratch. as forecast  agents using 1c behaviors come with some useful knowledge that allows them to find a better local optimum.
1	discussion and similar works
1	on explicit coordination
our experiments show that our work can benefit from addressing explicitly coordination problems. in particular  it is often the case that two agents  when placed in a world with more than two cubes  are not able to coordinate so as to push the  same pair of cubes . moreover  when too many agents are present  they easily work at cross-purpose.
　in  boutilier  1   boutilier has studied this problem from a theoretical point of view  using multi-agent markov decision processes. in his framework  each agent can compute a globally optimal policy where only coordination problems are to be solved  i.e. when the optimal joint action is not unique. such coordination can be reached using social laws  communication or a learning mechanism. however  this centralized planning framework can not be used for decentralized rl. other works  like  hu and wellman  1   more oriented towards reinforcement learning  settle on game-theory and nash equilibria to solve coordination problems. once again  these theoretical works are difficult to implement  especially with more than two agents.
　another field of research deals with explicitly taking into account other agents when learning  or planning  in a mas  see  carmel and markovitch  1  or  vidal and durfee  1  . usually  an agent builds a model of the other agents  so as to estimate their next actions. but  in the general case  it seems that not modeling the others is better than having a bad model and modelization leads to more complex agents.
　communication could also be used to solve the problem of explicit coordination. but attention must be paid to the fact that communication in itself has a cost  xuan et al.  1 . is it worthwhile to pay this cost whereas  simply putting more agents seems enough to increase the performances  even if the coordination problem of two particular agents is not really and explicitly solved   this is one of the questions we would like to study in the near future. on the same subject  we would like to investigate the use of short term memory to provide our agent with attention mechanisms and help them to coordinate.
1	reward
mataric＞  in her work with multi-robots systems  see  mataric＞  1    took the option of adapting the reward function and the agents' actions and perceptions to use a simple reinforcement algorithm. the reward is not the kind of boolean process we have  no reward / big reward at goal  but rather a smoother reward adapted to give very frequent hints to each individual agent. by using behaviors  that can be seen as macro-actions  the size of state and action space is greatly reduced and learning is easier. on the other hand  this kind of specialization is very task-dependent whereas our agents could be made to learn another task without changing very much the agents or the learning process.
　the coin framework  wolpert et al.  1   collective intelligence  is similar to our framework : designing mas through learning. the main ideas behind their work is to adapt reward functions and clusters of agents to build a subworld-factored system. dealing with our problem   it would mean creating only one sub-group sharing the same reward  and other experiments we conducted show that learning becomes difficult and slow. whereas this framework is useful for finding  independent  sub-groups of agents  our ideas seem better adapted for in-group learning.
1	automatic incremental learning
as explained in 1  we need to decompose the problem at hand in more and more complex tasks. the starting points of this decomposition are positive reward situations. the decomposition process could be automatically performed online by the learning agents. in addition to classical methods  the agents could randomly explore their environment  mark  interesting situations  and then work around them. knowledge thus learned could then be transfered to other agents.
1	conclusion
in this paper we have addressed the problem of automatically designing large multi-agent systems. our framework is based on each individual agent using a reinforcement learning algorithm to adapt its behavior to the desired global task. this learning problem is generally unsolvable because of its decentralized aspect and more classical limitations like partial observability of state  credit assignment and combinatorial explosion. to that effect  we have emphasized the use of an incremental learning scheme where more and more agents are faced with harder and harder problems. this method is very generic as it can be adapted to any task without adapting the agents to the particular problem at hand.
　we have tested our approach on a simulated environment where agents have to coordinate in order to reach a shared goal : the fusion of different colored cubes. the experiments give credit to our framework as they show the efficiency of incremental learning. incremental learning leads to better learning rates than raw unassisted learning. furthermore  it is more efficient to learn a more complex task after an initial stage of incremental learning than learning directly this more complex task from scratch. as a whole  our framework led us to conclude that  the more the better .
　still  there is room for improvement. we have discussed several ways to overcome these limitations  like using communication for addressing explicit coordination  short-term memory and intentions to deal with oscillatory behaviors and policy-search reinforcement learning as a possibly more adequate learning algorithm. we have also considered modeling the behavior of the other agents as a mean to predict their actions and thus improve the behavior of agents.
1	acknowledgments
we are particularly thankful to bruno scherrer  iadine chade`s and vincent chevrier for numerous discussions and their invaluable help in writing this paper.
references
 asada et al.  1  m. asada  s. noda  s. tawaratsumida  and k. hosoda. purposive behavior acquisition for a real robot by vision-based reinforcement learning. machine learning  1-1  1.
 baxter and bartlett  1  j. baxter and p. bartlett. direct gradient-based reinforcement learning: i. gradient estimation algorithms. technical report  the australian national university  canberra  australia  1.
 bernstein et al.  1  d.s. bernstein  s. zilberstein  and n. immerman. the complexity of decentralized control of markov decision processes. in proc. of the 1th conf. on uncertainty in artificial intelligence  1.
 boutilier  1  c. boutilier. planning  learning and coordination in multiagent decision processes. in proc. tark'1  de zeeuwse stromen  the netherlands  1.
 buffet et al.  1  o. buffet  a. dutech  and f. charpillet. incremental reinforcement learning for designing multiagent systems. in proc. of agents'1  1.
 carmel and markovitch  1  d. carmel and s. markovitch. opponent modeling for learning in multi-agent systems. in proceedings of ijcai'1 workshop. springer  1.
 dutech  1  a. dutech. solving pomdp using selected past-events. in proc. of the 1th european conf. on artificial intelligence  ecai1  1.
 ferber  1  j. ferber. multi-agent systems. an introduction to distributed artificial intelligence. john wiley & sons inc.  new york  1.
 hu and wellman  1  j. hu and m. wellman. multiagent reinforcement learning: theoretical framework and an algorithm. in proc. of icml-1  1.
 littman et al.  1  m. littman  a. cassandra  and l. kaelbling. learning policies for partially observable environments: scaling up. in proc. of icml-1  1.
 mataric＞  1  m. mataric＞. reinforcement learning in the multi-robot domain. autonomous robots  1 :1  1.
 singh et al.  1  s. singh  t. jaakkola  and m. jordan. learning without state estimation in partially observable markovian decision processes. in proc. of icml-1  1.
 stone and veloso  1  p. stone and m. veloso. multiagent systems: a survey from a machine learning perspective. autonomous robotics  1   1.
 sutton and barto  1  r. sutton and g. barto. reinforcement learning. bradford book  mit press  1.
 vidal and durfee  1  j. vidal and e. durfee. agent learning about agents: a framework and analysis. in s. sen  editor  collected papers from the aaai-1 workshop on multiagent learning. aaai press  1.
 wolpert et al.  1  d. wolpert  k. wheeler  and k. tumer. general principles of learning-based multiagent systems. in proc. of agents'1  seattle  pages 1  1.
 xuan et al.  1  p. xuan  v. lesser  and s. zilberstein. communication in multi-agent markov decision processes. in proc. of icmas workshop on game theoretic and decision theoretic agents  1.
robot weightlifting by direct policy search
michael t. rosenstein and andrew g. barto
department of computer science
university of massachusetts amherst  ma 1 mtr  barto  cs.umass.eduabstract
this paper describes a method for structuring a robot motor learning task. by designing a suitably parameterized policy  we show that a simple search algorithm  along with biologically motivated constraints  offers an effective means for motor skill acquisition. the framework makes use of the robot counterparts to several elements found in human motor learning: imitation  equilibrium-point control  motor programs  and synergies. we demonstrate that through learning  coordinated behavior emerges from initial  crude knowledge about a difficult robot weightlifting task.
1	introduction
humans exhibit remarkable proficiency at complex motor tasks such as handwriting  juggling  and machine assembly. one way to characterize our success with such tasks is that we have a mastery of redundant degrees of freedom  cf. bernstein . the ease with which humans achieve motor coordination contrasts sharply with that of robots  where  simple  motor tasks such as walking and throwing pose challenges to roboticists and artificial intelligence researchers. the typical solution of path planning and trajectory tracking works well for highly structured problems but not for situations with complex  nonlinear dynamics  with inadequate models  and with no detailed knowledge of the best solution. in this paper we examine a form of trial-and-error learning as a means for motor skill acquisition. our results involve a simulated  three-link robotic arm that learns to raise heavy weights through efficient  timely use of its actuators.
　algorithms for trial-and-error learning typically fall near one of two extremes: those that take advantage of structure in the problem  and those that take advantage of structure in the solution. we have in mind the reinforcement learning problem where an agent interacts repeatedly with its environment and attempts to learn an optimal policy  i.e.  a mapping from states to actions that maximizes some performance criterion. algorithms that focus on the solution often make direct adjustments of the current policy  whereas methods that focus

　　copyright c 1  international joint conferenceon artificial intelligence  inc.  www.ijcai.org . all rights reserved.
on the problem tend to build an intermediate representation  such as a value function  that forms the basis for policy improvement. in any case  the distinguishing features of trialand-error learning are the exploratory activity and the simple evaluative feedback that reinforces successful behavior.
　reinforcement learning algorithms such as td     sutton  1  and q-learning  watkins and dayan  1  are particularly well-suitedto take advantage of structure in the problem. such algorithms use a value function to capture regularities in the observed rewards and state transitions that implicitly define the task. however  reinforcement learning algorithms typicallyignore structure in the solutionby treating the policy as a featureless mapping from states to actions. extensions to the basic reinforcement learning framework  such as function approximation  e.g.   tsitsiklis and van roy  1   variableresolution methods  e.g.   moore and atkeson  1   factored mdps  e.g.   boutilier et al.  1   and semi-markov decison problems  e.g.   sutton et al.  1   all constrain the form of the policy and  therefore  add strucutre to the solution. nevertheless  with each of these extensions the focus remains on the value function rather than on the policy.
　at the other extreme  methods for direct policy search  e.g.   anderson  1; baird and moore  1; baxter and bartlett  1; moriarty et al.  1   do away with intermediate representations of the policy and take advantage of constraints placed on the solution. for instance  evolutionary algorithms  moriarty et al.  1  reinforce regularities in the parameterized policy  the  genetic material   by restricting  reproduction  to the fittest members of the population. and direct methods for function optimization  swann  1  make the most of continuity by biasing their search toward promising regions of parameter space. however  such methods ignore structure in sequential decision problems by making policy adjustments on a trial-by-trialbasis after long-term rewards are known  rather than on a per-action basis.
　both direct policy search and value-based methods often start with little or no prior structure and rely on extensive explorationto gather the information needed to build an optimal policy. both approaches can benefit from domain knowledge. in this paper we utilize an algorithm for direct policy search because of its simplicity  not because we believe such methods are generally superior to value-based algorithms. our
goal is to show that biologicallymotivated structure can make trial-and-error learning an effective approach for a particular

figure 1: simulated three-link robotic arm in several configurations with no payload:  a  start   b  via point  and  c  goal. at the via point the joint angles are	1 	1  and	1 degrees  proximal to distal .
class of control problems. although we are certainly not the first to look to biology for inspiration with regard to machine learning and control  this work is novel in its combination of motor strategies for developing a coordinated robot.
1	robot weightlifting
by exploitingthe dynamics of the task-gravity  inertia  elasticity  and so on-olympic weightlifters solve a difficult coordination problem and raise more than twice their bodyweight over their heads. our simulated robotic weightlifter  while loosely based on olympic weightlifters  was inspired by the numerous acrobot-like examples studied by the machine learning and control engineering communities. figure 1 shows the robot in several configurations. the task is to lift a payload from the straight-down  stable equilibrium to the straight-up  unstable equilibrium. limited torque at each joint restricts the class of feasible solutions to those that execute a coordinated swing  i.e.  those that exploit momentum and intersegmental dynamics. kinematic redundancy  joint constraints  and obstacles  the striped object as well as the robot and its support structure  also complicate the task.
　the robotic arm was simulated as a three-link frictionless pendulum with each link having length 1 m and and mass 1 kg. at each joint  torque was limited to   1  nm  and at the middle and distal joints the range of motion was confined to   1  degrees. the equations of motion were generated iteratively using the newton-euler method  walker and orin  1  and solved numerically by euler integration with a step size of 1 s. the robot was motionless at the start  and the goal was defined as a six-dimensional hypercube with velocity limits of 1 deg/s   1 rad/sec  and with position limits of 1 degrees centered on the goal configuration. at the end of each trial  i.e.  when the robot's state entered the goal region  performance was quantified as the total integrated torque magnitude  and on those trials where the lifter exceeded its range of motion  contacted an obstacle  or failed to reach the goal within five seconds  the trial was terminated with the maximum cost of 1 nm s. in summary  the task was designed as a minimum-effort optimal control problem with both torque and kinematic constraints.
　optimal control theory provides a number of techniques for solving dynamic optimization problems such as our weightlifting task. for instance  wang et al.  used a gradient-based method starting from an initial feasible path to more than triple the recommended payload capacity of a puma industrial robot. however  such methods usually require precise models  or system identification  and protracted offline computation before the robot attempts its first lift. in this paper we make no assumptions about the availability of a detailed model. instead  we borrow several human motor learning strategies as a way to add the needed structure to an otherwise difficult machine learning problem.
1	structured policy parameterization
most research on human motor coordination has focused on control instead of learning  and so relatively little is known about the mechanisms of human motor learning. this leaves us  the robot designers  in a position to select from many available artificial intelligence and adaptive control techniques. below  we describe a simple form of trial-and-error learning. however  our focus is not the choice of a specific algorithm but rather the motor control strategies that place constraints on that algorithm. in particular  we design a biologically motivated  parameterized policy that is easily seeded with prior knowledge and easily adapted through learning.
1	biological motivation
our proposed solution begins with a sequence of via points  i.e.  desired configurations for the robotic arm.  a similar approach was used by morimoto and doya  for a robot stand-up task.  we regard each via point as the specification for a movement primitive that converges to the corresponding manipulator configuration. such primitives are suggestive of an equilibrium-point style of motor control  e.g.   feldman  1   whereby the endpoint of a movement is programmed and the spring-like properties of the muscles ensure convergence to that endpoint. the primary benefits of equilibriumpoint control are that complex limb dynamics can be ignored and that higher motor centers need not be involved in the detailed activation patterns of numerous actuators. the drawback is that to explain complicated movements  equilibriumpoint approaches give up their simplicity by requiring a virtual trajectory  or sequence of equilibrium points that induce the desired movement but are not targets for convergence.
　for the weightlifting task  one possible implementation of equilibrium-point control involves the use of a single movement primitive that brings the manipulator to the goal configuration. indeed  with no payload and with no obstacle a simple linear feedback controller accomplishes the task  although this solution fails with payloads as small as 1 kg. instead  we add a second movement primitive for the via point shown in figure 1b. this via point represents the knowledge that certain configurations avoid the leftmost obstacle and also reduce the effective moment arm for the proximal joint. we assume the via point is obtained through imitation of a successful lift  cf. schaal   or through instruction from a coach-a human programmer in our case. the via point is intended to convey crude path information  with no detailed knowledge of any successful trajectory.
　convergence first to the via point and then to the goal  extends the payload capacity to about 1 kg  beyond which adaptation is necessary. thus  for the robotic weightlifter the advantage of equilibrium-point control is also its drawback: the robot's intrinsic dynamics are ignored for their difficulties as well as their benefits. rather than turn to virtual trajectories as a way to exploit dynamics  we use a small number of movement primitives that act as the starting point for learning. the result is a hierarchical motor program that runs three feedback controllers: one for the via point and one for the goal  both adjustable  followed by another  fixed controller to regulate locally about the goal. this converts the closed-loop equilibrium-point solution to a  ballpark  open-loop solution  greene  1; schmidt  1  that handles minor errors at a lower  closedloop level of control.
1	implementation
to build the initial motor program  we first construct two proportional-derivative  pd  controllers  pd1 and pd1:
       pdi i kp *i kv  1  where and are the joint positions and velocities  respectively  and 1 is a vector of joint torques subject to saturation at the 1 nm torque limit. in eq.  1  i is a 1 gain matrix  *i is the target equilibrium point  and kp 1 nm rad-1 and kv 1 nm s rad-1 are the nominal proportional and derivative gains  respectively. the target equilibrium points  1 and 1  are initialized to the via point and goal configuration  respectively. both 1 and 1 are initialized to the identity matrix  and so each pd controller initially acts as three uncoupled  scalar-output servomechanisms  one for each joint .
　next  the robot executes an  imitation  trial by running to convergence pd1 followed by pd1.  a threshold test on the position error establishes convergence.  at convergence we record t*1 and t*1-the time elapsed for the corresponding controller since the start of the trial. together t*1 and t*1 mark the switching times for the open-loop level of control. in particular  the program runs pd1 from the start of each new trial  time t 1  until t t*1  then a switch is made to pd1 which runs until t t*1  followed by the third controller  pd1  that runs until the trial terminates. pd1 is a fixed copy of pd1 that increases the flexibility of the learning system while providing convergence guarantees for movements close to the goal. however  pd1 plays no role during the imitation trial.
　the open-loop level of the motor program has two free parameters  the switching times  that we adjust by trial-anderror learning  described shortly . together  pd1 and pd1 have 1 free parameters  six from *i and 1 from i  that we adapt with the same learning algorithm. as a form of shaping  we increase the payload at a nominal rate of 1 kg every 1 trials of learning. and with each new payload  learning proceeds in two phases: a shorter phase  1 trials  that adjusts the open-loop timing of the motor program  followed by a longer phase  1 trials  that adjusts the lower-level pd controllers.
1	direct policy search
table 1 summarizes the simple random search  srs  algorithm developed for the robot weightlifting task. the algorithm performs random search in a k-dimensional parameter space  centered at a base point  . perturbations    are normally distributed with zero mean and standard deviation equal to the search size  . each test point     is evaluated and the best observed point is kept as the algorithm's return value. for the weightliftingtask  the evaluation function gives the total effort during a simulated trial with the new parameter settings. updates to the base point are made by taking a step toward the most recent test point with probability or toward the best observed point with probability 1 . thus  provides an easy adjustment for the exploration-exploitation tradeoff  with the extremes of a pure random walk   1  and of movement along a rough estimate of the gradient   1 . even with set to zero  considerable exploration is possible for large values of   which decays by a factor after each iteration  to the minimum min.

input
initial point k step size  1  search strategy  1 
search size	1 search decay factor	 1 
   minimum search size	min	1 initialize
best
   ybest	best repeat
1.
1. y 1.	y	ybest
1.	best 1. ybest y
1.
	best	1
	1.	min
until convergence or number of iterations too large return best

table 1: the simple random search  srs  algorithm. for the motor program  1 at the open-loop level of control and 1 at the closed-loop level.

figure 1: configuration-space trajectories for  a  the simple equilibrium-point solution with no payload  no learning  and no obstacle   b  the
 imitation  trial with no payload   c  a representative solution with a 1 kg payload  and  d  an unexpectedsolution with an 1 kg payload. s  v  and g denote the start  via-point  and goal configurations  respectively. the srs algorithm step size and search strategy parameters were set to 1 and 1  respectively. for the motor program  the initial search size was 1 with a minimum of min 1 and a decay factor of 1. for the pd controllers  the corresponding parameter values were 1  min 1  and 1.　the srs algorithm has several properties that make it a nice choice for trial-and-error learning. first  srs is easy to implement for a wide variety of optimization problems. like other direct search methods  swann  1   the srs algorithm needs no derivative information-an important feature when the cost function is non-differentiable or when the gradient is difficult to estimate  as with deterministic policies subject to noise . the algorithm also has some neurobiological support  albeit speculative  anderson  1 . and compared to  pattern search  algorithms  beveridge and schechter  1   such as the simplex method  srs makes rapid progress in high-dimensional spaces where some algorithms first require numerous exploratory moves to establish a search direction or to build a simplex  for instance.
1	learning to exploit dynamics
with no payload  the robotic weightlifter can use a number of qualitatively different solutions to reach the goal configuration. for example  the simple equilibrium-point solution with no obstacle  figure 1a  follows a direct path from the start to the goal  whereas the  imitation  trial  figure 1b  takes a longer path through configuration space  first converging to the via point. as the payload increases  the space of feasible solutionsshrinks  and so the via point represents an attempt to start the learning system at a favorable place. the  standard  solution after learning  figure 1c  passes by the via point while coordinating the joints to exploit the robot's intrinsic dynamics.  the standard solution is robust to sensor noise as well as variabilityin the via point.  figure 1 also shows an unexpected  reversal  solution  where the robot moves through an entirely different region of configuration space  details below .
　as indicated in section 1  each pd controller initially behaves as three independent  linear servomechanisms- one for each joint-but the learning algorithm transforms them into the robot analogue of a synergy  bernstein  1; greene  1  or  smart  nonlinear controller that accounts for intersegmental effects.  although eq.  1  describes a linear controller  saturation of the output represents a set of nonlinear constraints  possibly inactive  that the learning system can exploit.  to examine the coupling of the individual controllers  we quantify joint coordination as
c pd1 pd1	d	1	d	1	 1 
where d is the following measure of diagonal dominance of a pd controller gain matrix:
j wjj
d 	 1 
j k wjk
　figure 1 shows the change in c throughout learning- starting with no coupling  both 1 and 1 diagonal  for the initial trial and increasing to about 1 where half the  mass  in the gain matrices falls along the main diagonals. with no payload  the increase in coupling mirrors the drop in effort  and with all but the lightest payloads  we observe a statistically significant increase in coupling over the no-payload condition. thus  active coordination by the control system appears to play an important role for the weightlifting task  although the functional significance of these results remains a question for future work.

trial
figure 1: effects of payload and learning on controller joint coordination  averaged over 1 runs . solid markers denote statistically significant differences  p 1  at the corresponding trial number.

figure 1: the reversal solution with a 1 kg payload. the manipulator was nearest the two via points  v1 and v1  at t=1 s and t=1 s.　early in learning  initial motion at the middle and distal joints induces counterclockwise movement at the proximal joint-despite the action of the pd controller which attempts to drive the upper arm clockwise  away from the support structure and directly toward the via point. in other words  passive mechanincal coupling conflicts with the initially uncoupled  active control. through subsequent learning  however  the best solutions exploit intersegmental dynamics and initial swing  i.e.  momentum  by reversing the sign of the  shoulder  torque for the first 1 to 1 ms.
　on rare occasions-less than one percent of all runs- the learning system discovered the  reversal  solution shown first in figure 1d and again  with more detail  in figure 1. throughout much of the movement  the positions of the middle and distal joints have opposite sign with respect to the standard solution. in effect  the reversal solution abandons the prior knowledge supplied by the via point. interestingly  we observed the reversal solution only for those experiments with increased variability  involving sensor noise and a large initial search size  .
　once we know of an alternative solution  we can use the same approach to policy parameterization to offer new  coaching  to our robotic weightlifter. for instance  we can encourage initial counterclockwise swing by inserting an extra via point  and an extra pd controller  prior to the one shown in figure 1b. separately  we can also encourage the reversal solution by designing two new via points as depicted in figure 1  frames three and five . these alternatives resulted in statistically significant improvements  p 1  as summarized in figure 1. thus  by injecting simple  approximate knowledge  we turned the reversal solution from a rare occurrence into a new standard by which to gauge future performance.
1	conclusions
we attribute much of the success of the srs algorithm to the prior structure supplied by both the parameterized policy and the via points. we suspect that more sophisticated forms of direct function optimization  e.g.   swann  1   will yield improvements in terms of learning time  but that the qualitative results will remain the same. the more interesting possibility for future work is the use of gradient-based methods for continuing control tasks-whether a direct policy search algorithm  e.g.   baxter and bartlett  1   or a value-based approach  e.g.   sutton et al.  1 . such methods are particularly important for extending the basic approach outlined

figure 1: effects of  coaching on maximum payloadlifted successfully after learning  averaged over 1 runs .
in this paper to non-episodic tasks with either occasional or continuing rewards.
　for robotic manipulators  the classic solution to a movement problem involves  not reinforcement learning  but rather the construction of a kinematic trajectory that avoids obstacles and singularities. the underlying assumption is that the robot's intrinsic dynamics are something to compensate for  instead of something to exploit. even for optimized trajectories  imprecise dynamic models often lead to overly conservative acceleration constraints and  therefore  to sub-optimal movements  craig  1 . rather than use adaptive control techniques to deal with inaccurate models  in this paper we gave up the stability guarantees associated with control engineering methods and developed a learning framework motivated by human motor control. as part of future work  we plan to test this framework on a real  non-planar robot with seven degrees of freedom. like other reinforcement learning methods  our approach is geared toward optimization; thus  the resulting policies are inherently ones that exploit dynamics.
　as mentioned in section 1  one criticism of equilibriumpoint styles of motor control is the need for virtual trajectories to explain complicated multi-joint movements. van ingen schenau et al.  also argue that equilibrium-point models are kinematic in nature and  therefore  ill-suited for tasks that exploit dynamics or require control of contact forces. although we agree with these criticisms  equilibriumpoint control nevertheless plays a key role in the present work. in particular  the movement primitives  i.e.  the via points and the goal configuration  supply the hierarchical motor program with an initial kinematic solution that the learning algorithm then transforms into one that exploits  rather than cancels the dynamics.
acknowledgments
we thank theodore perkins and richard van emmerik for comments and helpful discussions. this research was supported by the national science foundation under grant no. iri-1.
references
 anderson  1  russell w. anderson. biased randomwalk learning: a neurobiological correlate to trial-anderror. in omid omidvar and judith dayhoff  editors  neural networks and pattern recognition  pages 1. academic press  san diego  ca  1.
 anderson  1  c. w. anderson. approximating a policy can be easier than approximating a value function. technical report cs-1  colorado state university  1.
 baird and moore  1  l. baird and a. moore. gradient descent for general reinforcement learning. in michael s. kearns  sara a. solla  and david a. cohn  editors  advances in neural information processing systems 1  pages 1. the mit press  cambridge  ma  1.
 baxter and bartlett  1  jonathan baxter and peter l. bartlett. reinforcement learning in pomdps via direct gradient ascent. in proceedings of the seventeenth international conference on machine learning   1.
 bernstein  1  n. a. bernstein. the co-ordination and regulation of movements. pergamon press  oxford  1.
 beveridge and schechter  1  gordon s. g. beveridge and robert s. schechter. optimization: theory and practice. mcgraw-hill book co.  new york  1.
 boutilier et al.  1  craig boutilier  richard dearden  and moises goldszmidt. stochastic dynamic programming with factored representations. artificial intelligence  1-1 :1  1.
 craig  1  john j. craig. adaptive control of mechanical manipulators. addison-wesley publishing company  reading  ma  1.
 feldman  1  a. g. feldman. once more on the equilibrium-point hypothesis   model  for motor control. journal of motor behavior  1-1  1.
 greene  1  p. h. greene. problems of organization of motor systems. in r. rosen and f. m. snell  editors  progress in theoretical biology  volume 1  pages 1- 1. academic press  new york  1.
 greene  1  p. h. greene. why is it easy to control your arms  journal of motor behavior  1 :1  1.
 moore and atkeson  1  a. w. moore and c. g. atkeson. the parti-game algorithm for variable resolutionreinforcement learning in multidimensional state-spaces. machine learning  1 :1  1.
 moriarty et al.  1  david e. moriarty  alan c. schultz  and john j. grefenstette. evolutionary algorithms for reinforcement learning. journal of artificial intelligence research  1-1  1.
 morimoto and doya  1  j. morimoto and k. doya. reinforcement learning of dynamic sequence: learning to stand up. in proceedings of the ieee/rsj international conference on intelligent robots and systems  volume 1  pages 1  1.
 schaal  1  stefan schaal. is imitation learning the route to humanoid robots  trends in cognitive science  1- 1  1.
 schmidt  1  richard a. schmidt. a schema theory of discrete motor skill learning. psychological review  1 :1  1.
 sutton et al.  1  richard s. sutton  doina precup  and satinder singh. between mdps and semi-mdps: a framework for temporal abstraction in reinforcement learning. artificial intelligence  1-1  1.
 sutton et al.  1  richard s. sutton  david mcallester  satinder singh  and yishay mansour. policy gradient methods for reinforcement learning with function approximation. in sara a. solla  todd k. leen  and klaus-robert muller  editors  advances in neural information processing systems 1  pages 1. the mit press  cambridge  ma  1.
 sutton  1  richard s. sutton. learning to predict by the method of temporal differences. machine learning  1- 1  1.
 swann  1  w. h. swann. direct search methods. in w. murray  editor  numerical methods for unconstrained optimization  pages 1. academic press  new york  1.
 tsitsiklis and van roy  1  j. n. tsitsiklis and b. van roy. an analysis of temporal-difference learning. ieee transactions on automatic control  1-1  1.
 van ingen schenau et al.  1  g. j. van ingen schenau  a. j. van soest  f. j. m. gabreels  and m. w. i. m.
horstink. the control of multi-joint movements relies on detailed internal representations. human movement science  1-1 :1  1.
 walker and orin  1  michael w. walker and david e. orin. efficient dynamic computer simulation of robotic mechanisms. journal of dynamic systems  measurement and control  1-1  1.
 wang et al.  1  c.-y. e. wang  w. k. timoszyk  and j. e. bobrow. weightlifting motion planning for a puma 1 robot. in proceedings of the ieee 1 international conference on robotics and automation  volume 1  pages 1  1.
 watkins and dayan  1  c. j. c. h. watkins and p. dayan. q-learning. machine learning  1/1 :1  1.

machine learning and
data mining
inductive logic programming

oi-implication: soundness and refutation completeness
floriana esposito  nicola fanizzi  stefano ferilli  giovanni semeraro
dipartimento di informatica  universita di bari  via orabona  1 i-1 bari  italy` {esposito fanizzi ferilli semeraro} di.uniba.it

abstract
weakening implication by assuming the object identity bias allows for both a model-theoretical and a proof-theoretical definition of a novel and more manageable ordering relationship over clausal spaces. in this paper  we give two important results  namely the soundness and the refutation completeness  through a subsumption theorem  of the underlying derivation procedure  that make this relationship particularly appealing for inducing a generalization model for clausal search spaces.
1	introduction
logical implication  as a relationship on clausal spaces  is particularly hard to handle due to many negative results descending from its non-decidability  schmidt-schauss  1  and intrinsic complexity  gottlob and fermuller  1：  . this is the reason why subsumption  being more tractable and efficiently implementable than stronger relationships  i.e.  relationships that entail it   is the generalization model commonly exploited in inductive logic programming  ilp .
　attempts have been made to weaken subsumption and implication in order to obtain more manageable ordering relationships between clauses. consider  for instance  forms of non-decreasing subsumption such as the weak subsumption  badea and stanciu  1   or weaker but decidable forms of implication such as t-implication  idestam-almquist  1 . indeed  the algebraic structure of the search space  and hence the refinement operators that can be defined  are affected by the choice of the underlying ordering relationship.
　weakening implication by assuming the object identity bias allowed for the definition of oi-implication  a novel relationship between clauses which makes the search space more manageable  esposito et al.  1 . specifically  it has been possible to prove the existence of ideal refinement operators for the search spaces ordered by the resulting form of logical implication  while this is not possible in standard clausal spaces  nienhuys-cheng and de wolf  1 . this is particularly important for the efficiency of the refinement in search spaces with dense solutions  badea and stanciu  1 . indeed  it has been shown as the computational complexity of the resulting operators is much more appealing than the one of the refinement operators defined for other search spaces.
　furthermore  the object identity framework for machine learning has been also exploited in the definition of operators performing inductive learning as well as abstraction and abduction  so to have a unique knowledge representation and reasoning framework for multistrategy learning.
　in this paper  a proof-theoretic definition is adapted from  esposito et al.  1  and a new model-theoretic definition of the implication under object identity is introduced. these definitions are shown to be equivalent. indeed  two main results are demonstrated  namely the soundness of the derivation procedure compliant with the assumption of the mentioned bias and a subsumption theorem which allows for the demonstration that this procedure is also refutation-complete.
　these results open new directions for the investigation of other interesting properties of this form of implication  such as the proof of a compactness theorem  the decidability and all the other algebraic properties of the search space  ordered by the relationship induced by this form of implication.
　this paper is organized as follows. in section 1  a semantics is defined  complying with the object identity assumption; hence  the definition of oi-implication is presented together with the proof of the soundness of the related proof procedure. refutation completeness through the subsumption theorem  are proven in section 1. section 1 draws the conclusions of this research  outlining the ongoing developments.
1	oi-implication
in our framework  we adopt a representation language l expressing theories as logic programs made up of clauses. the basic notions about clausal representation can be found in  lloyd  1; nienhuys-cheng and de wolf  1 .
　the framework we propose relies essentially on the following assumption  which is commonly employed. for instance  defining the concept of bicycle  one would mention the two wheels among its components. then  in order to rule out an instance of a different concept such as a monocycle  the identification of the two wheels should be avoided  a bicycle is two-wheeled by definition .
　a thorough treatment of these issues for the datalog clausal language is offered in  semeraro et al.  1 . we want to extend this to a more expressive clausal language  without the restriction to function-free terms. in our setting we want to avoid the identification of terms.
assumption 1  object identity  in a clause  terms denoted with different symbols must be distinct  i.e. they represent different entities of the domain.
　this notion can be viewed as an extension of reiter's unique names assumption  reiter  1 . object identity is the basis for the definition of both an equational theory for the language clauses and a quasi-order upon them.
　this assumption brings to a modified equational theory consisting in the addition of one axiom to an equality theory  lloyd  1   namely:
    t1 = t1 
for all pairs  t1 t1  of distinct terms occurring in a clause.
　nienhuys-cheng and de wolf have embedded an equality theory in the semantics  for demonstrating the completeness of the sldnf resolution  when dealing with the completion of normal programs  yielding the so called e-herbrand interpretations  nienhuys-cheng and de wolf  1 . badea and stanciu  in the definition of the weak subsumption   deal with substitutions which do not identify literals. in this paper  we adopt a different approach  founded on a semantic basis. a similar approach to capturing the object identity assumption can be considered that of the unique substitution semantics  khardon  1 .
1	semantics and model-theoretic definition
some definitions follow  specifying how the object identity assumption changes the semantics for the clausal representation. we start off by defining the relation between the terms of the language and the domain we deal with.
　a pre-interpretation j under object identity of a first-order language is exactly a standard pre-interpretation:
  a non-empty set d called the domain of the preinterpretation;
  each constant in l is assigned to an element of d;
  each n-ary function symbol f is assigned a mapping jf from dn to d.
　then  we need to map variables  and terms  to the objects of the domain:
definition 1 let j be a pre-interpretation of l with domain d. a variable assignment under object identity v w.r.t. l is a one-to-one mapping from the variables of l to the domain d of j. we use v  x/d  to denote that the variable x is mapped to d （ d in the variable assignment v   and the other variables are mapped according to v . the term assignment under object identity z in a formula φ w.r.t. j and v of terms in l is the following mapping from the terms in l to d:
1. j maps each constant to an element of d;
1. v maps each variable to a distinct element of d; 1. if d1 ... dn are the elements of d to which the terms t1 ... tn are mapped  then the term f t1 ... tn  is mapped to jf d1 ... dn   where jf is the function from dn to d assigned to the function symbol f by j  s.t.  t1 t1 （ terms φ  : z t1/d1  z t1/d1  implies d1= d1.
　by combining pre-interpretations and variable  term  assignments  we can specify the notion of interpretation under object identity:
definition 1 given j a pre-interpretation of l with domain d  an interpretation i under object identity  oiinterpretation  based on j assigns a mapping ip from dn to {t f} to each predicate symbol p. given a variable assignment under object identity v   be z the corresponding term assignment w.r.t. j and v . then a formula φ in l has a truth value under i defined as follows:
1. for each atom a = p t1 ... tn   if  i = 1 ... n : di （ d is assigned to ti by z  then the truth value of a under i and v is ip d1 ... dn ;
1. the truth values of the formula φ obtained by using connectives is determined using the standard truth tables.
1. if φ =  xψ then φ has truth value t under i and v if there exists d （ d for which ψ has truth value t under i and v  x/d   where v  x/d  is a legal variable assignment under object identity. otherwise  φ has truth value f under i and v .
1. if φ =  xψ then φ has truth value t under i and v if for all elements d （ d ψ has truth value t under i and v  x/d   where v  x/d  is a legal variable assignment under object identity. otherwise  φ has truth value f under i and v .
　the standard notions of tautology  contradiction  satisfiability and consistency can be straightforwardly extended to the new semantics.
　in ilp we focus our interest on clauses. since they are closed formulas  the truth values do not depend on variable assignments. we can represent an interpretation as a set of ground literals  i.e. those that are true according with the interpretation.
definition 1 given a closed formula φ  an oiinterpretation i is called a model under object identity  oi-model  for φ iff i satisfies φ. given a set of closed formulas Σ  i is an oi-model for Σ iff it is an oi-model for all formulas φ （ Σ.
　oi-interpretations are always standard interpretations. conversely  we show with an example a case when an oimodel for a formula is not a model.
example 1 let c = p x  ‥ p a . an oi-model for c  assumed the domain is d = {a b}  is represented by the herbrand oi-interpretation i = {p b }.
i is not a model for c in the standard meaning. indeed  since p a  is stated as false  the truth of the clause depends on p x   where variable x is universally quantified. under object identity  it is sufficient that p b  be true for  xp x  to hold. the same does not apply in the standard semantics where both p a  and p b  should be true.
　we are now ready to define the form of implication that is compliant with the object identity assumption:
definition 1 given a set of formulas Σ and a formula φ  Σ oi-implies φ or φ is a logical consequence under object identity of Σ  denoted with Σ|=oiφ  iff all oi-models i of Σ are oi-also models of φ.
thus  useful results hold also in this semantics:
theorem 1  deduction theorem  let Σ be a set of formulas and φ and ψ be formulas. then Σ “ {φ}|=oiψ iff Σ|=oi φ ★ ψ .
proof. Σ “ {φ}|=oiψ iff all oi-models of Σ “ {φ} are oi-models of ψ iff all oi-models of Σ are oi-models of  φ or of ψ iff all oi-models of Σ are oi-models of  φ ‥ ψ iff Σ|=oi φ ★ ψ .
　as a consequence  it is possible to prove also this result: theorem 1 let Σ be a set of formulas and φ be a formula. then Σ|=oiφ iff Σ “ { φ} is unsatisfiable.
proof. Σ|=oiφ iff  by theorem 1  Σ “ { φ}|=oi1 iff all oi-models of Σ “ { φ} are oi-models of 1 iff Σ “ { φ} is unsatisfiable under object identity.
　as concerns clauses  in the standard semantics it is possible concentrate the interest on herbrand models only  lloyd  1   this can be extended to our setting.
proposition 1 let Σ be a set of clauses in a first order language l. then Σ has an oi-model iff Σ has an herbrand
oi-model.
proof.
    suppose Σ has an oi-model i. then we define the her-
brand oi-interpretation i with:
1. the same pre-interpretation of i and having as domain the herbrand universe ul; 1. let p an n-ary predicate symbol occurring in Σ.
then we define the function ip from ul to {f t} as follows: ip t1 ... tn  = t if p t1 ... tn  is true under i and ip t1 ... tn  = f otherwise.
    obvious.
1	proof-theoretic definition and soundness
given a notion of subsumption under object identity  and using a subsumption theorem  we define implication under object identity proof-theoretically. the goal here is to define this relationship in a constructive way.
　first  we have to define a form of resolution coping with the object identity assumption. to this purpose  we discuss some further properties required to substitutions in order to fulfill this assumption. in fact  a substitution can be regarded as a function mapping variables to terms. hence  we require these functions to satisfy certain additional properties in order to preserve object identity.
definition 1 given a set of terms t  a substitution σ is an oi-substitution w.r.t. t iff  t1 t1 （ t : t1= t1 implies t1σ 1= t1σ.
　in the following  we will omit the set of terms t when it is obvious. based on oi-substitutions  it is possible to define other related notions such as the composition of oisubstitutions  ground and renaming oi-substitutions  as well as instance clauses  ground clauses and variants.
　in order to cope with the object identity assumption  a new relationship has been derived  semeraro et al.  1  from the classic θ-subsumption. its definition can be given by exploiting the notion of oi-substitution  esposito et al.  1 .
definition 1 let c and d be two clauses. then  c θoi-subsumes d iff there exists σ  an oi-substitution w.r.t. terms c   such that cσ   d.
　the notion of θoi-subsumption induces a quasi-order upon the space of clauses  semeraro et al.  1 . besides  a notion of equivalence for θoi-subsumption can be defined  that partitions the space of clauses in equivalence classes. since θoi-subsumption maps each literal of the subsuming clause onto a single literal in the subsumed one  equivalent clauses under θoi-subsumption must have the same number of literals. thus  a search space ordered by θoi-subsumption is made up of non-redundant clauses. this yields smaller equivalence classes than those in a space ordered by θ-subsumption.
　now  as we have done for the substitutions  we can specify a notion of a unifier fulfilling assumption 1:
definition 1 given a finite set of simple expressions s  we say that θ is an oi-unifier iff  e  ei （ s : eiθ = e and θ is an oi-substitution w.r.t. terms ei . an oi-unifier θ for s is called a most general oi-unifier  mguoi  for s iff  for each
oi-unifier σ of s  there exists an oi-substitution τ such that σ = θτ.
　in this setting  clauses cannot have proper subsets as factors. hence  this notion can be used only for standardizing apart the variables of the parent clauses before a step of resolution  esposito et al.  1 .
definition 1 given the clauses c and d  suppose they are standardized apart. a clause r is an oi-resolvent of c and d iff  there exist m   c and n   d such that {m n} is unifiable through the mguoi θ and
r =   c   m  “  d   n  θ
we write r = roi {c d} .
　this definition of oi-resolution follows robinson's original definition  robinson  1 . however  a different definition has been given  based on the one given in  chang and lee  1   taking into account one complementary pair of literals for each resolution step  esposito et al.  1 .
definition 1 let t be a set of clauses. the oi-resolution   denoted by r oi Σ   is defined inductively:
 n = 1  roi Σ  = Σ
 n   1  rnoi Σ  = rnoi 1 Σ  “
　　　　　　　　{r = roi {c d} | c d （ rnoi 1 Σ } r oi Σ  = r1oi Σ  “ r1oi Σ  “ ，，， “ rnoi Σ  “ ，，， if there exists an oi-derivation of c from   this will be denoted with Σ`oic which is equivalent to c （ roi Σ .
　it is interesting to consider also the case of linear resolution  chang and lee  1   which was originally exploited for the proof-theoretic definition of oi-implication  esposito et al.  1 . if c can be derived by means of zero or more linear oi-resolution steps from the set of clauses Σ  we denote this with  where loi denotes the linear
oi-resolution operator and l oi stands for its closure.
　in order to prove the soundness of the proof-procedure defined above  some preliminary results are needed.
lemma 1 let c1 and c1 be clauses. if there exists r that is the oi-resolvent of c1 and c1 then it holds {c1 c1}|=oir.
proof. assume that the two clauses are standardized apart. suppose r is an oi-resolvent of c1 and c1  resolved upon the subsets of literals m and n  and let θ be the
mguoi m n  employed. suppose the oi-interpretation i with domain d is an oi-model for {c1 c1}.
let {x1 ... xn} = vars c1  “ vars c1  and v be a variable assignment under object identity. then   i = 1 ... n  di （ d if mθ is false under i and
v  x1/d1 ，，， xn/dn   at least one of the other literals in c1 is true under i and v  x1/d1 ，，， xn/dn . similarly
for n. note that r is made up of all literals in c1θ and c1θ except mθ or nθ.
since	either	mθ	or	nθ	is	false	under	i	and
v  x1/d1 ，，， xn/dn   at least one of the literals in r is true under i and v  x1/d1 ，，， xn/dn   hence i is an oi-model of r. therefore  {c1 c1}|=oir.
　this result can be inductively extended to oi-derivations  proving the soundness of oi-resolution: theorem 1  soundness  let Σ be a set of clauses  and c be a clause. if Σ`oic then Σ|=oic.
proof. suppose Σ`oic. then there exists an oi-derivation r1 ... rk = c from Σ. by induction on k  it holds:
 k = 1  r1 = c （ Σ  thus obviously Σ|=oic.
 k   1  suppose the thesis holds for m ＋ k.
let r1 ... rk+1 = c be an oi-derivation of c from Σ. if rk+1 （ Σ then the theorem is obvious. otherwise  rk+1 is an oi-resolvent of some ri and rj  i j ＋ k . by the inductive hypothesis  we have Σ|= ri . from lemma 1  it follows that i j =oi k+1 = .
　now we can give a proof-theoretic definition of the notion of oi-implication. in the next section  we will justify this formally  showing the equivalence of the two versions. definition 1 let c and d be two clauses. then c implies d under object identity  c oi-implies d   denoted c  oi d iff either d is a tautological clause or there exists a clause  such that e θoi-subsumes d. equivalence
under oi-implication is denoted by  oi.
　from the definition above  it is easy to see that oiimplication is a strictly a stronger ordering relationship than θoi-subsumption. moreover the set of most specific clauses 〕 will be made up by tautologies.
we will prove in the next section that definitions 1 and
1 are equivalent  by means of a subsumption theorem.
1	refutation completeness
the soundness theorem for oi-implication bridges the gap
from the proof-theoretic to the model-theoretic definition of this notion. in this section we give a proof of a completeness result to make the same step in the other direction. following the proof-schema given in  nienhuys-cheng and de wolf  1  for the standard case  first we prove this result for ground clauses  then we lift it to the general case.
1	preliminaries
let us start from the simplest case when both Σ and c are ground.
lemma 1 let Σ be a logic program of ground clauses and c be a ground clause. if Σ|=oic then there exists d such that
Σ`oid and d   c.
the proof is almost the same as in the case of standard implication  see  nienhuys-cheng and de wolf  1   since no variable assignment is made  for the clauses are ground. yet in our case we considered finite sets of clauses. this can be extended by means of a compactness theorem.
　in order to prove the subsumption theorem when only clause c is required to be ground  we need two theorems that are valid for the notion of unsatisfiability for standard interpretations but can be proven also when the object identity is assumed.
theorem 1 a set of clauses Σ is unsatisfiable under object identity iff there exists a finite unsatisfiable set Γ of ground instances of clauses from Σ.
see  nienhuys-cheng and de wolf  1  for a proof for standard interpretations. in this setting  it comes by using proposition 1.
　from this result and the deduction theorem another result follows:
theorem 1 let Σ be a non-empty set of clauses and c be a ground clause. then Σ|=oic iff there exists a finite set Γ of ground instances of clauses from Σ such that Γ|=oic.
again  see  nienhuys-cheng and de wolf  1  for a proof in the standard case. in this setting  theorems 1 and 1 are to be exploited.
　now we have to show how to lift an oi-resolution step from the case of ground parent clauses to that of unrestricted clauses.
lemma 1 let c1 and c1 be two clauses and c1 and c1  respectively  two instances of them. if r1 is an oi-resolvent of c1 and c1  then there exists a clause r  that is an oiresolvent of c1 and c1  such that r1 is an instance of r.
proof. suppose that c1  c1  c1 and c1 are standardized apart. be σ1 the oi-substitutions such that and c1 = c1σ1.
let be the subsets of literals resolved upon for obtaining r1  then there exist n1   c1 and n1   c1 such that and. suppose that μ
be an mguoi n1 n1 .
now letfor some d1   n1 and d1   n1. hence  r1 =  d1 “d1  μ =  d1σ1“d1σ1 μ. since the clauses are standardized apart  we can write: r1 =  d1 “ d1 σ1σ1μ.
consider n1 and n1. they can be unified through the oi-
unifier σ1σ1μ. then  let θ be an mguoi n1 n1   there exists an oi-substitution δ such that σ1σ1μ = θδ.
thus  c1 and c1 can be resolved under object identity upon n1 and n1 by using θ: r =   c1 n1 “ c1 n1  θ =  d1“ d1 θ. then rδ =  d1 “ d1 θδ =  d1 “ d1 σ1σ1μ = r1. hence r1 = rδ.
　we can generalize the lifting technique to the case of oiderivations  i.e. to multiple oi-resolution steps.
lemma 1 let Σ be a finite set of clauses and Σ1 a set of instances of clauses from Σ. suppose that r1  ... rk1 be an oi-derivation of clause from Σ1. then there exists an oiderivation r1 ... rk of clause rk from Σ such that ri1 is an instance of ri  for each i = 1 ... k.
proof. by induction on the length of the derivation k.
 k = 1  in this case. but Σ1 is made up of instances of the clauses in Σ. hence   r1 （ Σ such that r1 is an instance of r1.
 k   1  by hypothesis  let us suppose the thesis proven up to a derivation of length m. we have to prove it for k = m + 1.
let be an oi-derivation of  from is the resolvent of two clauses in
	Σ	1	m .
by hypothesis  there exists an oi-derivation r1 ... rm of rm from Σ such that  i = 1 ... m : ri1 is an instance of ri. hence  by lemma 1  there exists rm+1 such that is one of its instances.
　we have now all the instruments for proving the following result:
lemma 1 let Σ be a finite set of clauses and c be a ground clause. if Σ|=oic then there exists d such that Σ`oid and d   c.
proof. assume that c is not a tautology. we look for a clause d fulfilling the thesis. from Σ|=oic and theorem 1  then there exists a finite set Σg such that each clause in Σg is a ground instance of a clause in Σ and Σg|=oic. then  from lemma 1 there exists a clause d1 such that
Σg`oid1 and d1   c. let  be an oiderivation of d1 from Σg.
by means of lemma 1  the oi-derivation can be lifted to an oi-derivation r1 ... rk of rk from Σ  where d1 = rk1 is an instance of rk. let d = rk. then Σ`oid and d θoisubsumes c  since d1 = dδ and d1   c .
1	subsumption theorem and consequences
preliminarily  we have to recall the notion of a particular substitution that will be used in the next proofs: definition 1 let Σ be a set of clauses and c be a clause. let {x1 ... xn} = vars c  and suppose we have a set of constants {a1 ... an} ” consts Σ “ c  =  . then σ = {x1/a1 ... xn/an} is a skolem substitution for c w.r.t. Σ.
this notion is easily extensible to this setting  it is easy to see that skolem substitutions are also oi-substitutions.
　now  given the results of the previous sections  we can prove the main result and some consequences.
theorem 1  subsumption theorem  let Σ be a finite set of clauses and c be a clause. then Σ|=oic iff there exists a clause d such that Σ`oid and d θoi-subsumes c.
proof.
    assume that c is not a tautology. let θ be a skolem oisubstitution for c w.r.t. Σ. then cθ is a ground clause which is not a tautology and Σ|=oicθ. by lemma 1  there is a clause d such that Σ`oid and d θoi-subsumes cθ. since d is derived from Σ  d cannot contain constants yielded by θ. therefore θ maps variables of c to constants that are not in d. let δ be an oi-substitution such that d θoi-subsumes cθ and σ be the oi-substitution obtained by replacing each binding xi/ti in δ with xi/ai. then δ = σθ. since θ only replaces the variables xi by ai  1 ＋ i ＋ n   it follows that dσ   c. then d θoi-subsumes c. hence  the thesis holds.
    for the soundness of the oi-derivation  and θoisubsumption .
　as a consequence of this theorem  similarly to the standard case  it is nearly straightforward to demonstrate some important results originally due to gottlob .
definition 1 a clause c is ambivalent when there are two literals l1 l1 （ c with the same predicate symbol but opposite sign. besides  if they unify then the clause c is called recursive.
　of course only unification through oi-substitutions is taken into account here. given these notions  it is possible to show when oi-implication and θoi-subsumption coincide: proposition 1 let c and d be clauses. if c is not recursive and d is not tautological  then c|=oid iff c θoisubsumes d.
proof. assume that c|=oid. by theorem 1 it holds that
-subsumes d. since c is not recursive . hence  c θoi-subsumes d. the     part is obvious.
　now  given a clause c  we denote with c+ and c   respectively  the set of its positive and negative literals.
proposition 1 let c and d be clauses. if c|=oid then c+ θoi-subsumes d+ and c  θoi-subsumes d .
proof. observe that c+ θoi-subsumes c then c+|=oic. by the hypothesis  it holds: c|=oid  thus c+|=oid. note that by construction c+ and d+ contain only positive literals  then c+ cannot be oi-resolved with itself  and d+ is not a tautology. by proposition 1  it must hold that c+ θoisubsumes d.
again  observing that c+ is made up of positive literals only  one can conclude that c+ θoi-subsumes d+. analogously c  θoi-subsumes d .
this property can be used for the proof of the decidability of the implication under object identity. yet  this subject goes beyond the scope of this paper.
　finally  we state a sufficient condition for the equivalence to hold between oi-implication and θoi-subsumption:
proposition 1 let c and d be clauses. if d is not ambivalent  then c|=oid iff c θoi-subsumes d.
proof. assume c|=oid and d non ambivalent. thus d cannot be a tautology.
by proposition 1 it holds c+θ   d+ and c δ   d  for some oi-substitutions θ and δ.
now  if c were recursive then there would be a literal l = lσ1 = lσ1 such that lσ1 （ c+ and  lσ1 （ c   with σ1 and σ1 oi-unifiers. but then lσ1θ （ d+ and  lσ1δ （ d   which does not hold since d is not ambivalent. thus c is not recursive and d is not tautological. by proposition 1  it follows that c θoi-subsumes d.
　the subsumption theorem showed that oi-resolution  together with θoi-subsumption  can derive the same conclusions drawn model-theoretically by implication under object identity. in standard implication  resolution is not deduction complete  some θ-subsumption steps could be needed   yet it might be complete with respect to an unsatisfiable set of clauses. we prove this result in the object identity setting.
theorem 1  refutation completeness  let Σ be a finite set of clauses. then Σ is unsatisfiable under object identity iff Σ`oi1.
proof.
    be Σ a finite set of clauses such that Σ|=oi1. for the subsumption theorem 1  there exists d such that Σ`oid and d θoi-subsumes 1. then d must be the empty clause itself. hence Σ`oi1
    for the soundness of the oi-derivation  theorem 1 .
　all the theorems and propositions of the previous section can be extended to the case of an infinite set of formulas Σ if a compactness theorem like that for standard models  boolos and jeffrey  1  can be demonstrated. yet  here we are interested in these issues concerning logic programs  hence finite sets of clauses only.
1	conclusions and further work
a framework has been presented for the novel notion of implication under object identity. by assuming this bias over a clausal representation  we were able to define oi-implication both model-theoretically and proof-theoretically. the two definitions can be unified by exploiting the soundness of the oi-derivations and a subsumption theorem which is the basis for proving the refutation completeness of our procedure. this relationship  that is stronger than θoi-subsumption  induces a quasi-order over the clausal search space conferring different algebraic properties than standard implication.
　we expect encouraging results on the complexity of learning/revising logic theories in such a search space. hence  from a theoretical viewpoint  we are now concerned with the demonstration of the compactness and decidability of this new relationship. another research stream will concern the algebraic properties of the resulting search space. in particular  algorithms for calculating minimal generalizations and specializations are to be developed.
　in a more long term perspective  we aim at building an integrated framework for multistrategy learning that will benefit by these theoretical findings.
references
 badea and stanciu  1  l badea and m. stanciu. refinement operators can be  weakly  perfect. in s. dzeroski・ and p. flach  editors  proceedings of the 1th international workshop on inductive logic programming - ilp1  volume 1 of lnai  pages 1. springer  1.
 boolos and jeffrey  1  g.s. boolos and r.c. jeffrey. computability and logic. cambridge university press  cambridge  u.k.  1rd edition  1.
 chang and lee  1  c.l. chang and r.c.t. lee. symbolic logic and mechanical theorem proving. academic press  san diego  ca  1.
 esposito et al.  1  f. esposito  n. fanizzi  s. ferilli  and g. semeraro. ideal refinement under object identity. in p. langley  editor  proceedings of the 1th international conference on machine learning - icml1  pages 1  palo alto  ca  1. morgan kaufmann.
 gottlob and fermuller  1：   g. gottlob and c.g. fermuller.： removing redundancy from a clause.
artificial intelligence  1-1  1.
 gottlob  1  g. gottlob. subsumption and implication. information processing letters  1 :1  1.
 idestam-almquist  1  p. idestam-almquist. generalization of clauses under implication. journal of artificial intelligence research  1-1  1.
 khardon  1  r. khardon. learning function-free horn expressions. machine learning  1 :1  december 1.
 lloyd  1  j.w. lloyd. foundations of logic programming. springer  1nd edition  1.
 nienhuys-cheng and de wolf  1  s.-h. nienhuyscheng and r. de wolf. the subsumption theorem in inductive logic programming: facts and fallacies. in l. de raedt  editor  advances in inductive logic programming  pages 1. ios press  1.
 nienhuys-cheng and de wolf  1  s.-h. nienhuyscheng and r. de wolf. foundations of inductive logic programming  volume 1 of lnai. springer  1.
 reiter  1  r. reiter. equality and domain closure in first order databases. journal of acm  1-1  1.
 robinson  1  j.a. robinson. a machine-oriented logic based on the resolution principle. journal of the acm  1 :1  january 1.
 schmidt-schauss  1  m. schmidt-schauss. implication of clauses is undecidable. theoretical computer science  1-1  1.
 semeraro et al.  1  g. semeraro  f. esposito  d. malerba  n. fanizzi  and s. ferilli. a logic framework for the incremental inductive synthesis of datalog theories. in n.e. fuchs  editor  proceedings of lopstr1  volume 1 of lncs  pages 1. springer  1.
the levelwise version space algorithm and its application to molecular fragment finding
luc de raedt and stefan kramer albert-ludwigs-university freiburg
institute for computer science
georges-ko：hler-allee geb. 1
　d-1 freiburg in breisgau  germany deraedt  skramer  informatik.uni-freiburg.deabstract
a tight integration of mitchell's version space algorithm with agrawal et al.'s apriori algorithm is presented. the algorithm can be used to generate patterns that satisfy a variety of constraints on data. constraints that can be imposed on patterns include the generality relation among patterns and imposing a minimum or a maximum frequency on data sets of interest.
the theoretical framework is applied to an important application in chemo-informatics  i.e. that of finding fragments of interest within a given set of compounds. fragments are linearly connected substructures of compounds. an implementation as well as preliminary experiments within the application are presented.
1	introduction
mannila and toivonen  mannila and toivonen  1  formulate the general pattern discovery task as follows. given a database   a language for expressing patterns  and a constraint   find the theory of with respect to and   i.e.
. viewed in this
way contains all sentences within the pattern language considered that make the constraint true. this formulation of pattern discovery is generic in that it makes abstraction of several specific tasks including the discovery of association rules  frequent patterns  inclusion dependencies  functional dependencies  frequent episodes  ... also  efficient algorithms for solving these tasks are known  cf.  mannila and toivonen  1  .
　so far the the type of constraint that has been considered is rather simple and typically relies on the frequency of patterns. in the past decade the data mining community spent a lot of effort to efficiently compute patterns having a minimum frequency  such as e.g. apriori  agrawal et al.  1 . in this paper  we will extend this popular data mining model by allowing the user to specify a variety of different constraints on the patterns of interest. the constraints that will be considered involve generality constraints on patterns  e.g. to specify that the patterns of interest are  resp. are not  more general than a specific pattern  as well as frequency constraints. frequency constraints that are considered either impose a maximum or a minimum frequency on a data set of interest. these constraints can then be combined e.g. in order to discover all patterns that are more general than pattern   have a minimum frequency on the dataset   and a maximum frequency on data set . the result is a flexible and declarative query language to specify the patterns of interest. from this point of view  the work also fits in the inductive database framework considered by researchers such as  imielinski and mannila  1; han et al.  1; de raedt  1 .
　the key problem in discovering theories that involve a conjunction of primitive constraints is to efficiently combine the solvers for the primitive constraints . it is at this point where mitchell's version space approach  mitchell  1  is extremely useful. indeed  each of the primitive constraints results in a version space. consider e.g. the minimum frequency constraint. as shown by  mannila and toivonen  1   the minimum frequency constraint results in a space of solutions with the most general pattern as the only element of the -set and the boundary as the -set. because this property holds for all primitive constraints  the space of solutions of the conjunction of constraints can also be specified as a version space. moreover  it can in principle be computed using hirsh's version space intersection method  hirsh  1 . however  rather than applying hirsh's framework  we will employ a tighter integration of version spaces with apriori.
　to demonstrate the relevance of level-wise version spaces  we present an implementation as well as experiments in the domain of molecular fragment finding. molecular fragments are sequences of linearly connected atoms. they are useful and important for the induction of so-called structureactivity relationships  sars   which are statistical models that relate chemical structure to biological activity. the use of automatically derived fragments in sars originates from the case/multicase systems developed by  rosenkranz et al.  1 . with more than 1 published references  the case/multicase systems are the most extensively used sar and predictive toxicology systems. previous approaches in these areas are based on the  decomposition  of individual compounds: these methods generate all fragments occurring in a given single compound. in this regard  our contribution is a language that enables the formulation of complex queries regarding fragments - users can specify precisely which fragments they are interested in. we also implemented an efficient solver to answer queries in this language. thus  from the algorithmic point of view  it is no longer necessary to process the results of queries post-hoc.
　molecular fragment finding has also been studied within the context of inductive logic programming and knowledge discovery in databases. for instance  warmr  dehaspe and toivonen  1  or the approach by inokuchi et al.  inokuchi et al.  1  have been used in this context. warmr is a system discovering frequently succeeding datalog queries  and thus is not restricted to fragments. the approach by inokuchi et al. deals with arbitrary frequent subgraphs  and thus is not restricted to linear fragments. both approaches differ in that their pattern domain is more expressive  but finding frequent patterns is likely to be more expensive and complex than for linear fragments. another approach to fragment finding is bottom-up propositionalization  kramer and frank  1 . all of these approaches handle only minimum frequency thresholds.
　in contrast to all these approaches  the presented work allows the specification of all sorts of constraints  for instance regarding generality or frequency. also  for the first time  one can pose constraints on the maximum frequency of fragments  and not only on the minimum frequency.
　finally  we would like to stress that the integration with version spaces results in a very compact representation of the resulting solutions. previous methods would typically output all patterns within the version space. this is interesting for understandability reasons and also for further learning with these fragments as features.
　the paper is organised as follows : in section 1  we introduce the molecular fragment finding task and the primitives for querying such fragments  in section 1  we present the level-wise version space algorithm  in section 1  we discuss some experiments in molecular fragment finding  and in section 1  we conclude and touch upon related work.
1	framework
1	molecular fragment finding
the task to which we will apply our integrated version space - apriori framework is that of finding all molecular fragments that satisfy a conjunction of constraints .
　a molecular fragment is defined as a sequence of linearly connected atoms. for instance  ' ' is a fragment meaning:  an oxygen atom with a single bond to a sulfur atom with a single bond to a carbon atom . in such expressions ' '  ' '  ' '  etc. denote elements  and ' ' denotes a single bond  '=' a double bond  '#' a triple bond  and ' ' an aromatic bond. as common in the literature  we only consider  heavy   i.e.  non-hydrogen  atoms in this paper.
　we assume that the system is given a database of example compounds and that each of the example compounds in the database is described using a 1-d representation. the information given there consists of the elements of the atoms of a molecule and the bond orders  single  double  triple  aromatic . an example compound in such a representation is shown in fig. 1.

figure 1: example compound in a 1-d representation. '
               ' is an example fragment occurring in the molecule.
　a molecular fragment covers an example compound if and only if considered as a graph is a subgraph of example
 . for instance  fragment '	' covers the example compound in fig. 1.
　there are a number of interesting properties of the language of molecular fragments :
fragments in are partially ordered by the is more general than relation; when fragment is more general than fragment we will write ;
within this partial order  two syntactically different fragments are equivalent only when they are a reversal of one another; e.g. ' ' and ' ' denote the
same substructure;
　　　if and only if is a subsequence of or is a subsequence of the reversal of ; e.g. ' ' ' '. there is a unique maximally general fragment  the empty fragment   which we denote by there is no maximally specific fragment; however  for convenience we add an artificial one to   which we denote by .
　note that the representation of molecular fragments is relatively restricted compared to some other representations employed in data mining  such as first-order queries  dehaspe and toivonen  1  or subgraphs  inokuchi et al.  1 . although fragments are a relatively restricted representation of chemical structure  it is easy for trained chemists to recognize the functional group s  that a given fragment occurs in. thus  the interpretation of a fragment reveals more than meets the eye.
1	constraints on fragments
the task addressed in this paper is that of finding the set of all fragments which satisfy a conjunction of primitive constraints . the primitive constraints imposed on the unknown target fragments are :
         and : where is the unknown target fragment and is a specific pattern; this type of primitive constraint denotes that should  not  be more specific  general  than the specified fragment ; e.g. the constraint ' ' specifies that should be more specific than ' '  i.e. that should contain
'	' as a subsequence; denotes the frequency of a fragment	on a set of molecules	; the frequency of a fragment	w.r.t. a dataset	is defined as the number of molecules in that	covers;
　　　　　　　 	where	is a positive integer and	and	are sets of molecules; this constraint denotes that the frequency of	on the dataset
should be larger than  resp. smaller than  or equal to ; e.g. the constraint denotes that the target fragments should have a minimum frequency of 1 on the set of molecules .
　these primitive constraints can now conjunctively be combined in order to declaratively specify the target fragments of interest. note that the conjunction may specify constraints w.r.t. any number of datasets  e.g. imposing a minimum frequency on a set of active molecules  and a maximum one on a set of inactive ones. e.g. the following constraint:
'	'	'	'
queries for all fragments that
include the sequence '	'  are not a subsequence of
' '  have a frequency on that is larger than 1 and and a frequency on that is smaller than 1.
1	solving constraints
in this section  we will discuss how to find the set of all solutions in to a conjunctive constraint
.
1	the search space
due to the fact that the primitive constraints are independent of one another  it follows that
　so  we can find the overall solutions by taking the intersection of the primitive ones.
　secondly  each of the primitive constraints is monotonic or anti-monotonic w.r.t. generality  cf.  mannila and toivonen  1  . a constraint is anti-monotonic  resp. monotonic  w.r.t. generality whenever
 resp.	 . the basic anti-monotonic constraints in our framework are:
  the basic monotonic ones are
                 . furthermore the negation of a monotic constraint is anti-monotonic and vice versa.
　monotonic and anti-monotonic constraints are important because their solution space is bounded by a border. this fact is well-known in both the data mining literature  cf.  mannila and toivonen  1    where the borders are often denoted by   as well as the machine learning literature  cf.  mitchell  1    where the symbols and are typically used.
　to define borders  we need the notions of minimal and maximal elements of a set w.r.t. generality. let be a set of fragments  then define
we can now define the borders	and	1 of a primitive
constraint	as
anti-monotonic constraints will have and for proper constraints ; proper monotonic constraints have and . furthermore  as in mitchell's version space framework we have that
this last property implies that  resp.   are proper borders for anti-monotone  resp. monotone  constraints.
　so  we have that the set of solutions to each primitive constraint is a simple version space completely characterized by and . therefore  the set of solutions to a conjunctive constraint will also be completely characterized by the corresponding
             and two issues:. at this point there are1. computing the borders tive constraintand	for each primi-1. computing theand	giventhe individual bordersand　the first issue could be addressed using  variants of  the common level-wise algorithm for data mining  cf.  mannila and toivonen  1  . the second one could - in principle - be solved using hirsh's version space merging algorithm. by now  it is probably clear that we need to integrate the level-wise algorithm with that of version spaces. however  rather than taking a loose coupling of the two approaches  as sketched above  we will provide a tighter integration of the two approaches. as we will show in the experimental section  this will lead to computational advantages.
　the integrated algorithm computes the overall border sets incrementally. it initializes the borders and with the minimal and maximal elements  and then repeatedly updates for each primitive constraint. to update the borders with regard to a primitive constraint involving generality  it employs mellish 's description identification algorithm. mellish's algorithm extends mitchell's version space algorithm in that
it not only allows to process constraints fo the type and 1 but also handles the dual constraints and . secondly  to update the version space w.r.t. frequency constraints  the integrated algorithm uses a variant of the level-wise algorithm that starts from the given borders rather than from or .
1	mellish's description identification algorithm
in order to formulate mellish's description identification algorithm  we need to introduce some operations on fragments and	.
the smallest  merged  fragments the largest common subfragments
the smallest fragments more specific than but not more general than
the largest fragments more general than	but not more specific than
notice that these operators may generate more than one fragment; e.g.	. these operations can now be used to instantiate mellish's algorithm:
	:=	;	;
for all primitive constraint	do case	of	:
:=
:= max
	case	of	:
:=
:= min
	case	of	:
:=
:= max
	case	of	:
:=
:= min
1	variants of the level-wise algorithm
the algorithms outlined below employ refinement operators.
a refinement operator   i.e. extending a fragment by one atom.
a generalization operator
　　　　  i.e. removing one atom from one side of a fragment.
　to deal with the frequency constraints   we may employ the following generalization of the level-wise algorithm. this is the downwards version.
let	be a constraint of type
;
while	do
   and p satisfies constraint the set of infrequent fragments considered endwhile
　to explain the algorithm  let us first consider the case where and . in this case  the above algorithm will behave roughly as the level-wise algorithm. the will then contain only fragments of size and the algorithm will keep track of the set of frequent fragments as well as the infrequent ones. the algorithm will then repeatedly compute a set of candidate refinements   delete those fragments that cannot be frequent by looking at the frequency of its generalizations  and evaluate the resulting possibly frequent fragments on the database. this process continues until becomes empty.
　the basic modifications to the level-wise algorithm that we made are concerned with the fact that we need not consider any fragment that is not in the already computed version space  i.e. any element not between an element of the and the set . secondly  we have to compute the updated set  which should contain all frequent fragments whose re-
finements are all infrequent.
　finding the updated and sets can also be realized in the dual manner. in this case  one will initialize with the elements of and proceed otherwise completely dual. the resulting upwards algorithm is shown below:
let	be a constraint of type
;
while	do
and p satisfies constraint
the set of infrequent fragments considered
endwhile
　whether the top down or bottom up version works more efficiently is likely to depend on the application and query under consideration. at this point it remains an open question as to when which strategy works more efficiently.
　finally  it is also possible to modify the above algorithms  exploiting the dualities  in order to handle monotonic frequency constraint of the form . in this case  one can use the following algorithm  or its dual :
let	be a constraint of type
;
while	do
and p satisfies constraint
the set of frequent fragments considered
endwhile
1	optimisations
various optimisations to the algorithms are possible.
　first  though we have adopted the standard level-wise algorithm to search for the borders when handling frequency constraints  it would also be possible to adopt some more recent and more efficient algorithms  such as those presented by  bayardo  1; gunopulos et al.  1 . these directly focus on the most specific  the longest  patterns  i.e. the set.
　secondly  apriori-style algorithms can be made more efficient  if elements of one level in level-wise search are combined to give the candidates for the subsequent one. this can also be done for fragments. for instance  if ' ' and ' ' are known to be frequent at level 1  ' ' is a candidate for a frequent fragment at level 1. however  several variants  with respect to order  have to be considered; e.g.
' ' and ' ' can be combined into ' ' as well as into ' '.
　thirdly  we keep track of the fragments in canonical form. as indicated earlier  each fragment is equivalent to its reversal. in the implementation  we use the canonical form of a fragment which is defined as the maximum  w.r.t. a lexicographic ordering  of the fragment and its reversal. the implementation of the operators takes care of this.
　fourthly  one problem with the implementation of the framework for fragments stems from the fact that the bottom is not a  valid  fragment that can be manipulated. in particular  is not defined. thus  we cannot search upwards from the set if . instead  we have to search downwards from . if however  is not equal to   then we can process maximum frequency constraints  upwards  starting with . for the same reason   is undefined   we cannot start a query with a constraint   where is a concrete fragment.
1	optimisation primitives
two primitives that seem especially useful are minimize and maximize. indeed  one could imagine being interested in those fragments that satisfy a number of constraints and in addition have maximum frequency on a certain dataset or minimally general. it is easy to extend the framework with primitives and that finds those fragments in that satisfy a conjunctive constraint and that are minimal or maximal with regard to the specified criterion. in this paper we consider only criteria that are monotonic or anti-monotonic  such as frequency and generality .
　in order to find the elements with regard to these optimisation primitives  one first computes the and sets with regard to and then selects those elements within or  depending on the   that are minimal or maximal with regard to the criterion.
1	experiments
in order to validate our approach  we applied it to the predictive toxicology evaluation challenge dataset of srinivasan et al.  srinivasan et al.  1 . this data set consists of over 1 compounds  and takes more than 1 mbyte of memory in its prolog encoding  and has been used as a standard benchmark in predictive toxicology and artificial intelligence. in this application  the goal is to discover molecular fragments that are  relatively  frequent in carcinogenic compounds and infrequent in non-carcinogenic compounds. such activating  toxic fragments are called structural alerts in the toxicological literature  ashby and patton  1 . one interesting question in this context is whether it is possible to rediscover known alerts. in the following  we summarize our experience with the new approach with example queries and systematic experiments.
1	some interesting queries
one open research question in toxicological research is the role of chlorinated compounds in carcinogenicity. in the new framework  an example query concerning chlorinated fragments that are frequent in active compounds and infrequent in inactive ones looks as follows:
　a related query concerns the existence of activating  nonhalogenated fragments:
1	quantitative results
to gather more quantitative evidence  we performed systematic experiments in the above domain. from the application side  it is quite clear that structural alerts are relatively rare for carcinogenicity  so that it can safely be assumed that alerts have a frequency of less than 1 in the positive  active compounds. also  we are not interested in fragments with any frequencies in the positive resp. negative examples. rather  we are seeking fragments that are  statistically significant  overrepresented in the active compounds and under-represented in the inactives. setting the minimum frequency in the actives to 1  1  1 and 1  respectively  we apply the -test to a contingency table with the class as one variable and the occurrence of the fragment as the other one to determine the maximum allowable frequency in the inactive compounds. in this way  we obtain maximum frequency thresholds of 1  1  1 and 1  respectively. for instance  we require the minimum to be 1 and the maximum to be 1 for the first experiment.
　methodwise  we performed a comparison of three approaches to the search for these fragments. each of these approaches consists of two stages: the first stage handling the minimum frequency query  using the first algorithm in section 1   and the second stage handling the maximum frequency query. the three approaches differ in the second stage  dealing with the maximum frequency query.
　the first approach is based on version spaces  searching upwards from  using the dual version of the third algorithm in section 1 . in table 1  the results for this method can be found in the column for . in contrast  the second one searches downwards  starting from until all elements of set are determined  using the third algorithm in section 1 -
minmax1111111111111111table 1: runtimes in seconds on a pentium ii.
column in table 1 . the third method performs simple post-processing  column in the table : it filters those fragments that are too frequent in the given dataset. table 1 summarizes the runtimes of these three methods for the given minimum/maximum frequency parameter settings in seconds cpu time.
　the outcome of these experiments is not clear a priori  because additional bookkeeping is done by our version space approach. still  the experiments show that in 1 out of 1 cases  the version space approach pays: it outperforms the rather ad-hoc post-processing method in terms of computation time. another result from the experiments is that - for the given queries - it does not make a big difference whether we search upwards or downwards for maximum frequency queries.
　perhaps the most important outcome of the experiments is the answers to the queries  which are shown below. they indicate that - for the given queries - version spaces constitute indeed a suitable and compact representation for the solution sets to the queries. also  as outlined above  the computational time needed for answering the queries is reasonable.
1: g={c-c-c-c-o-c-c c}
s={c-c-c-c-o-c-c c c c c c}
1: g={c c-c c  br  c-o-c c c c c c-n c-o-c c-n}
s={c c c c c c-c c c c c c  br-c  c-o-c c c c c c-n  c-o-c c-n}
1: g=s={c-c c c c-n}
1: g=s={n-c c c c c c-o  c-c c c c-n n-c c-o}
　further results and experiments in the context of feature construction and propositionalization can be found in  kramer and de raedt  1 .
1	conclusions and related work
the presented work contributes to 1  the theory of data mining and machine learning because of its integration of version spaces with the level-wise algorithm  1  the framework of inductive databases  because the constraints can and should be interpreted as queries in a molecular fragment finding language  and  as discussed above  1  to molecular fragment finding. we briefly review the key contributions in these domains and relate to relevant work where possible.
　with regard to 1  and 1  our work builds on that by  de raedt  1; 1   who presents an integration of the version space and level-wise algorithms. however  we expand our earlier theoretical work in various respects. indeed  in contrast to our earlier work  we report on an implementation  experiments that show the validity of the framework and an application in molecular fragment finding. thus our work provides - for the first time - evidence that the framework is not only of theoretical interest but also effective with regard to applications.
　with regard to 1   the presented algorithm provides a generalized theoretical framework for data mining. the resulting framework extends the borders in the levelwise techniques sketched by  mannila and toivonen  1   who link the level-wise algorithm to the set of mitchell's version space approach but do not further exploit the version space model. the experimental evidence indicates that this approach could form a viable extension to the classical level-wise algorithm.
　using two borders  i.e. the version space representation  to characterize the space of solutions to inductive queries is also done by  dong and li  1 . they use the version space representation to search for emerging patterns. emerging patterns are defined as itemsets whose supports increase significantly from one dataset to another. such patterns are also closely related to the significant fragments we discover using the test. however  the primitive constraints we support seem to be different from those by  dong and li  1 . to compute the borders  dong and li do not employ the levelwise algorithm. instead  they rely on more efficient algorithms such as bayardo's  max-miner. in principle it must be possible to adapt these more recent algorithms  bayardo  1; gunopulos et al.  1  to our framework too.
　for what concerns 1   our work can also be regarded as a domain specific inductive database  imielinski and mannila  1; meo et al.  1 . as sketched by  han et al.  1   inductive databases allow the user to specify constraints on the patterns of interest. recently  many of these constraints have been considered in the data mining literature  cf. e.g.  ng et al.  1; han et al.  1; 1 . in this context however  the use of frequency constraints on different data sets seems new.
　finally  let us also note that the presented work on discovering molecular structures and regularities  also using version spaces  is related to the well-known meta-dendral system by  buchanan and mitchell  1 .
references
 agrawal et al.  1  r. agrawal  t. imielinski  a. swami. mining association rules between sets of items in large databases. in proceedings of acm sigmod conference on management of data  1.
 ashby and patton  1  j. ashby and d. paton. the influence of chemical structure on the extent and sites of carcinogenesis for 1 rodent carcinogens and 1 different human carcinogen exposures. mutation research  1  1.
 bayardo  1  r. bayardo. efficiently mining long patterns from databases. in proceedings of acm sigmod conference on management of data  1.
 buchanan and mitchell  1  b.	buchanan	and	t.
mitchell. model-directed learning of production rules. in waterman  d. a. and hayes-roth  f.  eds.  patterndirected inference systems  academic press  new york  1.
 dehaspe and toivonen  1  l. dehaspe  h. toivonen. discovery of frequent datalog patterns  in data mining and knowledge discovery journal  vol. 1  1.
 de raedt  1  l. de raedt. an inductive logic programming language for database mining. in proceedings of the 1th international conference on artificial intelligence and symbolic computation  lecture notes in artificial intelligence  vol. 1  springer verlag  1.
 de raedt  1  l. de raedt. a logical database mining query language. in proceedings of the 1th inductive logic programming conference  lecture notes in artificial intelligence  vol. 1  springer verlag  1.
 dong and li  1  g. dong and j. li. efficient mining of emerging patterns : discovering trends and differences. in proceedings of kdd  acm  1.
 gunopulos et al.  1  d. gunopulos  h. mannila  s. saluja: discovering all most specific sentences by randomized algorithms. in foto n. afrati  phokion kolaitis  eds. : database theory - icdt '1  1th international conference  lecture notes in computer science 1  springer 1.
 han et al.  1  j. han  l. v. s. lakshmanan  and r. t. ng  constraint-based  multidimensional data mining  computer  vol. 1 : 1  1.
 han et al.  1  j. han  j. pei  and y. yin. mining frequent patterns without candidate generation. in proceedings of acm sigmod conference on management of data   1.
 hirsh  1  h. hirsh. generalizing version spaces. machine learning  vol. 1 : 1  1 .
 imielinski and mannila  1  t. imielinski and h. mannila. a database perspective on knowledge discovery. communications of the acm  1 :1  1.
 inokuchi et al.  1  a. inokuchi  t. washio  h. motoda. an apriori-based algorithm for mining frequent substructures from graph data. in d. zighed  j. komorowski  and j. zyktow  eds.  proceedings of pkdd 1  lecture notes in artificial intelligence  vol. 1  springerverlag  1.
 kramer and frank  1  s. kramer and e. frank. bottomup propositionalization. proceedings of the work-inprogress track at the 1th international conference on inductive logic programming  1  1.
 kramer and de raedt  1  s. kramer and l. de raedt. feature construction with version spaces for biochemical applications. in proceedings of the 1th international conference on machine learning  morgan kaufmann  1.
 mannila and toivonen  1  h. mannila and h. toivonen  levelwise search and borders of theories in knowledge discovery  data mining and knowledge discovery  vol. 1  1.
 meo et al.  1  r. meo  g. psaila and s. ceri  an extension to sql for mining association rules. data mining and knowledge discovery  vol. 1  1.
 mellish  1  c. mellish. the description identification algorithm. artificial intelligence  1.
 mitchell  1  t. mitchell. generalization as search  artificial intelligence  1.
 ng et al.  1  r. t. ng  l. v.s. lkshmanan  j. han  and a. pang. exploratory mining and pruning optimizations of constrained associations rules. in proceedings of acm sigmod conference on management of data   1.
 rosenkranz et al.  1  h.s. rosenkranz  a.r. cunningham  y.p. zhang  h.g. clayhamp  o.t. macina  n.b. sussmann  s.g. grant and g. klopman. development  characterization and application of predictivetoxicology models. sar and qsar in environmental research  1-1  1.
 srinivasan et al.  1  a. srinivasan  r.d. king and d.w. bristol. an assessment of submissions made to the predictive toxicology evaluation challenge. proc. of ijcai1  1  1.

machine learning and
data mining
probabilistic learning

active learning for structure in bayesian networks
simon tong
computer science department
　　stanford university simon.tong cs.stanford.edudaphne koller
computer science department
stanford university koller cs.stanford.edu
abstract
the task of causal structure discovery from empirical data is a fundamental problem in many areas. experimental data is crucial for accomplishing this task. however  experiments are typically expensive  and must be selected with great care. this paper uses active learning to determine the experiments that are most informative towards uncovering the underlying structure. we formalize the causal learning task as that of learning the structure of a causal bayesian network. we consider an active learner that is allowed to conduct experiments  where it intervenes in the domain by setting the values of certain variables. we provide a theoretical framework for the active learning problem  and an algorithm that actively chooses the experiments to perform based on the model learned so far. experimental results show that active learning can substantially reduce the number of observations required to determine the structure of a domain.
1	introduction
determining the causal structure of a domain is frequently a key issue in manysituations. bayesiannetworks  bns   pearl  1  are a compact graphical representation of joint probability distributions. they can also be viewed as providing a causal model of a domain  pearl  1 . if we assume that the graphical structure of the bn represents the causal structure of the domain  we can formalize the problem of discovering the causal structure of the domain as the task of learning the bn structure from data.
　over the last few years  there has been substantial work on discovering bn structure from purely observational data. however  there are inherent limitations on our ability to discover the structure based on randomly sampled data. experimental data  where we intervene in the model  is vital for a full determinationof the causal structure. however  obtaining experimental data is often time consuming and costly. thus the experiments must be chosen with care.
　in this paper  we provide an active learning algorithm that selects experiments that are most informative towards revealing the causal structure. with active learning the choice of the next data case is based upon the results seen so far. the possibility of active learning can arise naturally in a variety of domains and in several variants. in interventional active learning  the learner can ask for experiments involving interventions to be performed. this type of active learning is the norm in scientific studies: we can ask for a rat to be fed one sort of food or another. an intervention in the model causes certain probabilistic dependenciesin the model to be replaced by our intervention  pearl  1  - the rat no longer eats what it would normally eat  but what we choose it to. by observing the results of this experiment  we can determine the direction of causal influence in cases where purely observational data is inadequate.
　in such active learning settings  where we have the ability to actively select experiments  we need a mechanism that tells us which experiment to perform next. we present a formal framework for active learning in bayesian networks  based on the principles of bayesian learning. we maintain a distribution over bayesian network structures  which is updated based on our data. we define a notion of quality of our distribution  and provide an algorithm that selects queries in a greedy way  designed to improve model quality as much as possible. we provide experimental results on a variety of domains  showing that our active learning algorithm can provide substantially more accurate estimates of the bn structure using the same amount of data. interestingly  our active learning algorithm provides significant improvements even in cases where it cannot intervene in the model  but only select instances of certain types. thus  it is applicable even to the problem of learning structure in a non-causal setting.
1	learning bayesian networks
let be a set of random variables  with each variable taking values in some finite domain dom . a bayesian network  bn  over is a pair that represents a distribution over the joint space of . is a directed acyclic graph  whose nodes correspond to the random variables in and whose structure encodes conditional independence properties about the joint distribution. we use to denote the set of parents of . is a set of parameters which quantify the network by specifying the conditional probability distributions  cpds  .
　the bayesian network represents a joint distribution over the set of variables via the chain rule for bayesian networks:	. viewed as a probabilistic model  it can answer any query of the form where and are sets of variables and an assignment of values to . however  a bn can also be viewed as a causal model  pearl  1 . under this perspective  the bn can also be used to answer interventional queries  which specify probabilities after we intervene in the model  forcibly setting one or more variables to take on particular values. in pearl's framework  an intervention in a causal model that sets a single node replaces the standard causal mechanism of with one where is forced to take the value . in graphical terms  this intervention corresponds to mutilating the model by cutting the incoming edges to . intuitively  in the new model  does not depend on its parents; whereas in the original model  the fact that would give us information  via evidential reasoning  about 's parents  in the experiment  the fact that tells us nothingabout the values of 's parents. for example  in a fault diagnosis model for a car  if we observe that the car battery is not charged  we might conclude evidentially that the alternator belt is possibly defective  but if we deliberately drain the battery  then the fact that it is empty obviously gives us no information about the alternator belt. thus  if we set   the resulting model is a distribution where we mutilate to eliminate the incoming edges to nodes in   and set the cpds of these nodes so that with probability 1.
　our goal is to learn a bn structure from data. we assume that there are no hidden variables and in addition we make two further standard assumptions:
causal markov assumption: the data is generated from an underlying bayesian network	over	.
faithfulness assumption: the distribution over induced by satisfies no independences beyond those implied by the structure of .
our goal is to reconstruct from the data. clearly  given enough data  we can reconstruct . however  in general  does not uniquely determine . for example  if our network has the form   then is equally consistent with . given only samples from   the best we can hope for is to identify the markov equivalence class  pearl  1  of : a set of network structures that induce precisely the same independence assumptions. in a markov equivalence class  the skeleton of the network - the set connected pairs - is fixed; for some of the pairs  the direction of the edge is fixed  while the other edges can be directed either way  spirtes et al.  1 .
　if we are given experimental as well as observational data  our ability to identify the structure is much larger  cooper and yoo  1 . intuitively assume we are trying to determine the direction of an edge between and . if we are provided experimental data that intervenes at   and we see that the distribution over does not change  while intervening at does change the distribution over   we can conclude  based on the assumptions above  that the edge is .
1	bayesian learning with experimental data
as discussed in the introduction  our goal is to use active learning to learn the bn structure - learning from data where we are allowed to control certain variables by intervening at their values. we formalize this idea by assuming that some subset of the variables are query variables. the learner can select a particular instantiation for . the request is called a query. the result of such a query is called the response and it is a randomlysampled instance of all the non-query variables  conditioned on . in other words  is the result of an experiment where we intervened in the model by setting to take the values ; our assumptions then imply that is sampled from the mutilated model described above.
　we use a bayesian framework to learn the bn structure. more precisely  we maintain a distribution over possible structures and their associated parameters. we begin with a prior over structures and parameters  and use bayesian conditioning to update it as new data is obtained. following  heckerman et al.  1   we make several standard assumptions about the prior:
structure modularity: the prior can be written in the form: pa .
parameter independence:
parameter modularity: for two graphs	and	  if then:	.
in this paper  we also assume that the cpd parameters are multinomials and that the associated parameter distributions are the conjugate dirichlet distributions. however  our analysis holds for any distribution satisfying the parameter modularity assumption.
　given a complete  randomly sampled instance over   using bayes rule we have that the posterior distribution over
g is proportional to: . is the marginal likelihood of the data and can be expressed as an integral over all possible parameter values in :
　now  instead of having a complete random sample  suppose that we have an interventional query   and resulting response . we need to define how to update the distribution given this query and response. we break this into two problems by using the identity:
. thus
we need to determine how to update the parameter density of a structure and also how to update the distribution over structures themselves.
　for the first term in this expression  consider a particular network structure and a prior distribution over the parameters of . it is clear that we cannot use the resulting complete instance to update the parameters of the nodes themselves since we have forced to take on specific values. all other variables can  however  be updated with the complete instance.  note that there is no issue of selection bias when updating the parameters of the ancestors of since all of incoming edges to query nodes are cut.  thus  we define a variable to be updateable in the context of an interventional query if is not in .
　our update rule for the parameter density is now very simple. given a prior density and a response from a query   we do standard bayesian updating of the parameters  as in the case of randomly sampled instances  but we update only the dirichlet distributions of updateable nodes. we use to denote the distribution obtained from this algorithm; this can be read as  the density of after performingquery and obtaining the complete response  . note that this is quite different from the density which denotes standard bayesian conditioning. we also note that performing such an interventional update to the parameters still preserves parameter modularity.
　now consider the distribution over structures. we use to denote the posterior distribution over structures after performing the query and obtaining the response. the following theorem tells us how we can easily update the posterior over given an interventional query:
theorem 1 given a query	and complete response
   if satisfies parameter independence and parameter modularity  then:
		score
with score
　proof outline: note that score where and are the values of and in the data instance
. we have that score . finally  notice that	.
1	active learning
our goal in this paper is not merely to update the distribution based on interventional data. we want to actively select instances that will allow us to learn the structure better.
　a  myopic  active learner is a function that selects a query based upon its current distribution over and
   . it takes the resulting response   and uses it to update its distribution over and . it then repeats the process. we described the update process in the previous section. our task now is to construct an algorithm for deciding on our next query given our current distribution .
1	loss function
as in the work of tong and koller   a key step in our approach is the definition of a measure for the quality of our distribution over graphs and parameters. we can then use this measure to evaluate the extent to which various instances would improve the quality of our distribution  thereby providing us with an approach for selecting the next query to perform.
　more formally  given a distribution over graphs and parameters we have a loss function loss that measures the quality of our distribution over the graphs and parameters. given a query we define the expected posterior loss of the query as:
exploss
	loss	 1 
this definition immediately leads to the following simple algorithm: for each candidate query   we evaluate the expected posterior loss  and then select the query for which it is lowest. note  however  that the expected loss appears to be computationally expensive to evaluate. we need to maintain a distribution over the set of structures  and the number of structures in is super-exponential in the number of nodes. furthermore  given a query  to compute the expected posterior loss we have to perform a computation over the set of structures for each of the exponential number of possible responses to the query.
　to make this high-level framework concrete  we must pick a loss function. recall that our goal is to learn the correct structure; hence  we are interested in the presence and direction of the edges in the graph. for two nodes and   there are three possible edge relationships between them: ei-
ther	  or	or	. our distribution
over graphs and parameters induces a distribution over these three possible edge relationships. we can measure the extent to which we are sure about this relationship using the entropy of this induced distribution:
 1 
the larger this entropy  the less sure we are about the relationship between and . this expression forms the basis for our edge entropy loss function:
	loss	 1 
in certain domains we may be especially interest in determining the relationship between particular pairs of nodes. we can reflect this desire in our loss function by introducing scaling factors in front of different terms.
　now that we have defined the loss function for a distribution   our task is to find an efficient algorithm for computing the expected posterior loss of a given query relative to . we note that is our current distribution  conditioned on all the data obtained so far. initially  it is the prior; as we get more data  we use bayesian conditioning  as described above  to update   and then apply the same algorithm to the posterior.
　our approach to obtaining a tractable algorithm is based on the ideas of friedman and koller  - we first consider the simpler problem of restricting attention to network structures consistent with some total ordering  . then  we introduce a distribution over the orderings.
1	analysis for a fixed ordering
let be a total ordering of . we restrict attention to network structures that are consistent with   i.e.  if there is an edge   then . following  friedman et al.  1   we also assume that each node has a set of at most possible candidate parents that is fixed before each query round. in certain domains  we can use prior knowledge to construct ; in others we can use a technique mentioned in  friedman and koller  1  where we can use the data itself to point out nodes that are more likely to be directly related to . we define the set of candidate parents for a node that are consistent with our ordering as:
	  where	if
for all	. we note that the number of structures in induced by and is still exponential in the number of variables in .
　the key impact of the restriction to a fixed ordering is that the choice of parents for one node is independent of the choice of parents for another node  buntine  1; friedman and koller  1 . two important consequences are the following theorems 1   which give us closed form  efficiently computable expressions for key quantities:
theorem 1 given a query   we can write the probability of a response to our query as:
	pa	score
	pa	score
where	pa	.
theorem 1 given a query and completion we can write the probability of an edge as:
	pa	score

　　　　　　　　　pa	score where we define score	if	.
　notice that since we are performing bayesian averaging over multiple graphs the probability of an edge will generally only be high if is a direct cause of rather than if merely has some indirect causal influence on . now  consider the expected posterior loss  eq.  1   given
:
exploss
 1 
we can compute	by using theorem 1. also  notice from theorem 1 that the expression depends only on the values that	and	give to	 	 	and	. using this fact and then applying theorem 1  we can rewrite the expected posterior loss as:
exploss
	pa	score
 1 

where 
	pa	score
 this expression still involves summations over the exponential number of possible completions of a query. however  notice that for each and in eq.  1   the summation over completions resembles the expression for computing a marginal probability in bayesian network inference where we are marginalizing out . in fact and each can be regarded as factors and we can use standard graphical model inference procedures  lauritzen and spiegelhalter  1  to evaluate this expression effectively. the restriction to a candidate set of parents for each node ensures that each factor
is over at most variables  and each factor over at most variables. after applying bayesian network inference we end up with a factor over the variables where for each possible query we have the value of the expression
.
　we need to perform such an inference for each pair. however  since we restricted to at most candidate parents  the number of possible edges is at most . thus  the computational cost of computing the expected posterior loss for all possible queries is the cost of applications of bayesian network inference.
1	analysis for unrestricted orderings
in the previous section  we obtained a closed form expression for computing the expected posterior loss of a query for a given ordering. we now generalize this derivation by removing the restriction of a fixed ordering. the expression for the expected posterior loss can be rewritten as:
exploss
loss
loss
the expectation over orderings can be approximated by sampling possible orderings from our current distribution over graphs and parameters. as shown by friedman and koller   sampling from orderings can be done very effectively using markov chain monte carlo  mcmc  techniques.
　the expression inside the expectation over orderings is very similar to the expected posterior loss of the query with a fixed ordering  eq.  1  . the only difference is that we now must compute the entropy terms without restricting ourselves to a single ordering. this entropy term is based on probability expressions for relationships between nodes:
 1 
each of the terms inside the expectation can be computed using theorem 1. naively  we can compute the expec-
tation for each query	and completion	by sam-pling orderings fromand then comput-ing. clearly  this approachis impractical. however  we can use a simple approximation that substantially reduces the computational cost. our general mcmc algorithm generates a set of orderings sampled from . in many cases  a single data instance will only have a small effect on the distribution over orderings; hence  we can often use our samples from to be a reasonably good approximation to samples from the distribution
               . thus  we use our current set of sampled orderingsto approximateeq.  1 . note that this small approximation error will not accumulate since we are not using the approximation to update any of the parameters of our model  just merely to predict the value of possible queries in this current round.
　we note that  as in the fixed ordering case  the entropy term depends only on the values given to the variables	and	. thus  we can use the same bayesian network inference method to compute the expression	.
1	algorithm summary and properties
to summarize the algorithm  we first sample a set of orderings from the current distribution over graphs and parameters.
we then use this set of orderingsto computethe entropy terms
                     . next  for each ordering we compute	using a standard bayesian network inference algorithm to obtain a factor over all possible queries. we then average all of these query factors obtained from each ordering. the final result is a query factor that  for each possible query  gives the expected posterior loss of asking that query. we then choose to ask the query that gives the lowest expected posterior loss.
　we now consider the computational complexity of the algorithm. for each ordering we need to compute
	.	this
involves at most bayesian network inferences. each inference returns a factor over all possible queries and so the inference will take time exponential in the number of query variables. the time complexity of our algorithm to generate the next query is: # of sampled orderings
cost of bn inference .
　in addition  we need to generate the sampled orderings themselves. friedman and koller  provide techniques that greatly reduce the cost of this process. they also show that the markov chain mixes fairly rapidly  thereby reducing the numberof steps in the chain requiredto generate a random sample. in our setting  we can reduce the number of steps required even further. initially  we start with a uniform prior over orderings  from which it is easy to generate random orderings. each of these is now the starting point for a markov chain. as we do a single query and get a response  the new posterior distribution over orderings is likely to be very similar to the previous one. hence  our old set of orderings is likely to be be fairly close to the new stationary distribution. thus  a very small number of mcmc steps from each of the current orderings will give us a new set of orderings which is very close to being sampled from the new posterior.

figure 1:  a  original cancer network.  b  cancer network after 1 observations.  c  cancer network after 1 observations and 1 uniform experiments.  d  cancer network after 1 observations and 1 active experiments. the darker the edges the higher the probability of edges existing. edges with less than 1% probabilty are omitted to reduce clutter.
1	experimental results
we evaluated the ability of our algorithm to reconstruct the network structure by using data generated from a known network. we experimentedwith three commonlyused networks: cancer  with five nodes; asia  with eight nodes; and car troubleshooter  with twelve nodes. for each test network  we maintained 1 orderings  as described above. we restricted the set of candidate parents to have size . it typically took a few minutes for the active method to generate the next query.
　we compared our active learning method with both random sampling and uniform querying  where we choose a setting for the query nodes from a uniform distribution. each method produces estimates for the probabilities of edges between each pair of variables in our domain. we compared each method's estimate with the true network by using the edge error of the estimate:
error
 1 
where	if	holds in	and is zero otherwise.
　we first considered whether the active method provides any benefit over random sampling other than the obvious additional power of having access to queries that intervene in the model. thus  for the first set of experiments  we elimi-

figure 1:  a  cancer with one query node.  b  car with four query nodes.  c  car with three query nodes and weighted edge importance.  d  asia with any pairs or single or no nodes as queries.  e  cancer with any pairs or single or no nodes as queries.  g  cancer edge entropy.  f  car with any pairs or single or no nodes as queries.  h  car edge entropy. legends reflect order in which curves appear. the axes arezoomed for resolution.
nated this advantage by restricting the active learning algorithm to query only roots of . when the query is a root  a causal query is equivalent to simply selecting a data instance that matches the query  e.g.   give me a 1-year-old male  ; hence  there is no need for a causal intervention to create the response. situations where we can only query root nodes arises in many domains; in medical domains  for example  we often have the ability to select subjects of a certain age  gender  or ethnicity  variables which are often assumed to be root nodes. all algorithms were informed that these nodes were roots by setting their candidate parent sets to be empty. in this batch of experiments  the candidate parents for the other nodes were selected at random  except that the node's true parents in the generating network were always in its candidate parent set.
　figures 1 a  and 1 b  show the learningcurves for the cancer and car networks. we experimented with using uniform dirichlet  bde  priors and also more informed priors  simulated by sampling 1 data instances from the true network1 . the type of prior made little qualitative difference in the comparative performance between the learning methods  the graphs shown are with uniform priors . in both graphs  we see that the active method performs significantly better than random sampling and uniform querying.
　in some domains  determining the existence and direction of causal influence between two particular nodes may be of special importance. we experimented with this in the car network. we modified the l1 edge error function eq.  1   and the edge entropy eq.  1  used by the active method  to make determining the relationship between two particular nodes  the fuelsubsystem and enginestart nodes  1 times more important than a regular pair of nodes. we used three other nodes in the network as query nodes. the results are shown in fig. 1 c . again  the active learning method performs substantially better.
　note that  without true causal interventions  all methods have the same limited power to identify the model: asymptotically  they will identify the skeleton and the edges whose direction is forced in the markov equivalence class  rather than identifying all edge directions in the true causal network . however  even in this setting  the active learning algorithm allows us to derive this information significantly faster.
　finally  we considered the ability of the active learning algorithm to exploit its ability to perform interventional queries. by using a simple extension to our analysis we permitted our active algorithm to choose to set any pair of nodes or any single node or no nodes at all. we compared this approach to random sampling and also uniformly choosing one of our possible queries  setting a single node  pair of nodes  or no nodes . experiments were performedon the asia  cancer  and car networks with an informed prior of 1 random observations. in this batch of experiments  we also experimented with different methods for choosing the candidate parents for a node . as an alternative to using random nodes together with the true parents  we chose the variables which had the highest individual mutual information with . in practice this information could be obtained from observational data. empirically  both methods of choosing the candidate parents gave very similar results  despite the fact that for one node in the car network  a true parent of a node happened not to be chosen as a candidate parent with the mutual information method. we present the results using the mutual information criterion for choosing parents.
　figures 1 d   1 e  and 1 g  show that in all networks our active method significantly outperforms the other methods. we also see  in figures 1 f  and 1 h   that the prediction error graphs are very similar to the graphs of the edge entropy  eq.  1   based on our distribution overstructures. this shows that the edge entropy is  indeed  a reasonable surrogate for predictive accuracy.
　figures 1 b   1 c  and 1 d  show typical estimated causal edge probabilities in these experiments for random sampling  uniform querying and active querying respectively for the cancer network  fig. 1 a  . figure 1 b  demonstrates that one requires more that just random observational data to learn the directions of many of the edges  and fig. 1 d  shows that our active learning method creates better estimates of the causal interactions between variables than uniform querying. in fact  in some of the trials our active method recovered the edges and direction perfectly  when discarding low probablity edges  and was the only method that was able to do so given the limitation of just 1 queries. also  our active method tends to be much better at not placing edges between variables that are only indirectly causally related; for instance in the network distribution learned by the active method  summarized in fig. 1 d    the probability of an edge from cancer to coma is only 1% and from cancer to papilledema only 1%.
1	discussion and conclusions
this paper introduces the problem of active learning of bayesian network structure using interventional queries. we have presented a formal framework for this task  and a resulting algorithm for adaptively selecting which queries to perform. we have shown that our active method provides substantially better predictions regarding structure than both random sampling  and a process by which interventional queries are selected at random. somewhat surprisingly  our algorithm achieves significant improvements over these other approaches even when it is restricted to querying roots in the network  and therefore cannot exploit the advantage of intervening in the model.
　there is a significant body of work on the design of experiments in the field of optimal experimental design  atkinson and bailey  1 ; there  the focus is not on learning the causal structure of a domain  and the experiment design is typically fixed in advanced  rather than selected actively. active learning in bayesian networks has been considered by tong and koller  for parameter estimation where the structure is assumed to be known. there have been several studies on learning causal models from purely observational data  spirtes et al.  1; heckerman et al.  1 . cooper and yoo  consider learning the structure of causal networks from a mixture of experimental and observational data  but in a non-active learning setting. furthermore  although they derive a scoring metric for full networks  they only apply their technique to learn the relationship between single pairs of variables which they further assume are not confounded. on the other hand  our framework permits combining observational and experimental data for learning the structure over all variables in our domain  allowing us to distinguish the structure at a much finer level  taking into consideration both indirect causation and confounding influences.
　there are many interesting directions in which we can extend our work  e.g.  a treatment of continuous variables  temporal processes and different costs for different queries. most interesting is the problem of dealing with hidden variables and missing data. we might use active learning to decide which extra variable to observe or which extra piece of missing data we should try to obtain in order to best learn the model. another exciting direction is the potential of using active learning in order to try to uncover the existence of a hidden variable in our domain.
acknowledgements the experiments were performed using the phrog system  developed primarily by lise getoor  uri lerner  and ben taskar. the work was supported by darpa's information assurance program under subcontract to sri international  and by onr muri n1-1.
references
 atkinson and bailey  1  a. c. atkinson and r. a. bailey. one hundred years of the design of experiments on and off the pages of  biometrika . biometrika  1. in press.
 buntine  1  w. buntine. theory refinement on bayesian networks. in proc. uai  1.
 cooper and yoo  1  g. f. cooper and c. yoo. causal discovery from a mixture of experimental and observational data. in proc. uai  1.
 friedman and koller  1  n. friedman and d. koller. being bayesian about network structure. in proc. uai  1.
 friedman et al.  1  n. friedman  i. nachman  and d. pe'er. learning bayesian network structure from massive datasets: the  sparse candidate  algorithm. in proc. uai  1.
 heckerman et al.  1  d. heckerman  d. geiger  and d. m. chickering. learning bayesian networks: the combination of knowledge and statistical data. machine learning  1-1  1.
 heckerman et al.  1  d. heckerman  c. meek  and g. cooper. a bayesian approach to causal discovery. technical report msr-tr-1  microsoft research  1.
 lauritzen and spiegelhalter  1  s. l. lauritzen and d. j. spiegelhalter. local computations with probabilities on graphical structures and their application to expert systems. j. royal statistical society  b 1   1.
 pearl  1  j. pearl. probabilistic reasoning in intelligent systems. morgan kaufmann  1.
 pearl  1  j. pearl. causality: models  reasoning  and inference. cambridge university press  1.
 spirtes et al.  1  p. spirtes  c. glymour  and r. scheines. causation  prediction and search. mit press  1.
 tong and koller  1  s. tong and d. koller. active learning for parameter estimation in bayesian networks. in proc. nips 1  1. to appear.
probabilistic classification and clustering in relational data
ben taskar
computer science dept.
stanford university
　stanford  ca 1 btaskar cs.stanford.edueran segal
computer science dept.
stanford university
 stanford  ca 1 erans cs.stanford.edudaphne koller
computer science dept.
stanford university
stanford  ca 1 koller cs.stanford.eduabstract
supervised and unsupervised learning methods have traditionally focused on data consisting of independent instances of a single type. however  many real-world domains are best described by relational models in which instances of multiple types are related to each other in complex ways. for example  in a scientific paper domain  papers are related to each other via citation  and are also related to their authors. in this case  the label of one entity  e.g.  the topic of the paper  is often correlated with the labels of related entities. we propose a general class of models for classification and clustering in relational domains that capture probabilistic dependencies between related instances. we show how to learn such models efficiently from data. we present empirical results on two real world data sets. our experiments in a transductive classification setting indicate that accuracy can be significantly improved by modeling relational dependencies. our algorithm automatically induces a very natural behavior  where our knowledge about one instance helps us classify related ones  which in turn help us classify others. in an unsupervised setting  our models produced coherent clusters with a very natural interpretation  even for instance types that do not have any attributes.
1	introduction
most supervised and unsupervised learning methods assume that data instances are independent and identically distributed  iid . numerous classification and clustering approaches have been designed to work on such  flat  data  where each data instance is a fixed-length vector of attribute values  see  duda et al.  1  for a survey . however  many real-world data sets are much richer in structure  involving instances of multiple types that are related to each other. hypertext is one example  where web pages are connected by links. another example is a domain of scientific papers  where papers are related to each other via citation  and are also related to their authors. the iid assumption is clearly violated for two papers written by the same author or two papers linked by citation  which are likely to have the same topic.
　recently  there has been a growing interest in learning techniques for more richly structured datasets. relational links between instances provide a unique source of information that has been proved useful for both classification and clustering in the hypertext domain  slattery and craven  1; kleinberg  1 . intuitively  relational learning methods attempt to use our knowledge about one object to reach conclusions about other  related objects. for example  we would like to propagate information about the topic of a paper to papers that it cites. these  in turn  would propagate informationto papers that they cite. we would also like to use information about 's topic to help us reach conclusion about the research area of 's author  and about the topics of other papers written by that author.
　several authors have proposed relational classification methods along the lines of this  influence propagation  idea. neville and jensen  present an iterative classification algorithm which essentially implements this process exactly  by iteratively assigning labels to test instances the classifier is confident about  and using these labels to classify related instances. slattery and mitchell  propose an iterative algorithm called foil-hubs for the problem of classifying web pages  e.g.  as belonging to a university student or not. however  none of these approaches proposes a single coherent model of the correlations between different related instances. hence they are forced to provide a purely procedural approach  where the results of different classification steps or algorithms are combined without a unifying principle.
　in clustering  the emphasis so far has been on dyadic data  such as word-document co-occurrence  hofmann and puzicha  1   document citations  cohn and chang  1   web links  cohn and hofmann  1; kleinberg  1   and gene expression data. kleinberg's  hubs and authorities  algorithm exploits the link structure to define a mutually reinforcing relationship between hub and authority pages  where a good hub page points to many good authorities and a good authority page is pointed to by many good hubs.
　these techniques can be viewed as relational clustering methods for one or two  types  of instances  e.g.  web pages  documents and words   with a single relation between them  e.g.  hyperlinks  word occurrence . however  we would like to model richer structures present in many real world domains with multiple types of instances and complex relationships between them. for example  in a movie database the instance types might be movies  actors  directors  and producers. instances of the same type may also be directly related. in a scientific paper database  a paper is described by its set of words and its relations to the papers it cites  as well as to the authors who wrote it . we would like to identify  for each instance type  sub-populations  or segments  of instances that are similar in both their attributes and their relations to other instances.
　in this paper  we propose a general class of generative probabilistic models for classification and clustering in relational data. the key to our approach is the use of a single probabilistic model for the entire database that captures interactions between instances in the domain. our work builds on the framework of probabilistic relational models  prms  of koller and pfeffer  that extend bayesian networks to a relational setting. prms provide a language that allows us to capture probabilistic dependencies between related instances in a coherent way. in particular  we use it to allow dependencies between the class variables of related instances  providing a principled mechanism for propagating information between them.
　like all generative probabilistic models  our models accommodate the entire spectrum between purely supervised classification and purely unsupervised clustering. thus  we can learn from data where some instances have a class label and other do not. we can also deal with cases where one  or more  of the instance types does not have an observed class attribute by introducing a new latent class variable to represent the  unobserved  cluster. note that  in relational models  it is often impossible to segment the data into a training and test set that are independent of each other since the training and test instances may be interconnected. using naive random sampling to select training instances is very likely to sever links between instances in the training and test set data. we circumvent this difficulty by using a transductive learning setting  where we use the test data  albeit without the labels  in the training phase. hence  even if all the instance types have observed class attributes  the training phase involves learning with latent variables.
　we provide an approximate em algorithm for learning such prms with latent variables from a relational database. this task is quite complex: our models induce a complex web of dependencies between the latent variables of all of the entities in the data  renderingstandard approachesintractable. we provide an efficient approximate algorithm that scales linearly with the number of instances  and thus can be applied to large data sets.
　we present experimental results for our approach on two domains: a dataset of scientific papers and authors and a database of movies  actors and directors. our classification experiments show that the relational information provides a substantial boost in accuracy. applied to a clustering task  we show that our methods are able to exploit the relational structure and find coherent clusters even for instance types that do not have any attributes.
1	generative models for relational data
probabilistic classification and clustering are often viewed from a generative perspective as a density estimation task. data instances are assumed to be independent and identically distributed  iid  samples from a mixture model distribution. each instance belongs to exactly one of classes or clusters. in clustering  a latent class random variable is associated with the instance to indicate its cluster. other attributes of an instance are then assumed to be samples from a distribution associated with its class. a simple yet powerful model often used for this distribution is the naive bayes model. in the naive bayes model  the attributes of each instance are assumed to be conditionally independent given the class variable. although this independence assumption is often unrealistic  this model has nevertheless proven to be robust and effective for classification and clustering across a wide range of applications  duda et al.  1; cheeseman and stutz  1 . both classification and clustering involve estimation of the parameters of the naive bayes model; however  clustering is significantly more difficult due to the presence of latent variables.
　the iid assumption made by these standard classification and clustering models is inappropriate in rich relational domains  where different instances are related to each other  and are therefore likely to be correlated. in this section  we describe a probabilistic model for classification and clustering in relational domains  where entities are related to each other. our construction utilizes the framework of probabilistic relational models  prms   koller and pfeffer  1; friedman et al.  1 .
1	probabilistic relational models
a prm is a template for a probability distribution over a relational database of a given schema. it specifies probabilistic models for different classes of entities  including probabilistic dependencies between related objects. given a set of instances and relations between them  a prm defines a joint probability distribution over the attributes of the instances.
　relational schema. a relational schema describes attributes and relations of a set of instance types
　　　　　　. each type is associated with a set of attributes . each type is also associated with a set of typed binary relations . we associate each relation with the type of its first argument  allowing us to use the relation as a set-valued function  whose value is the set of instances related to an instance . for example  for an actor   role is the set of movies in which the actor has appeared.
　in certain cases  relations might have attributes of their own. for example  the  role  relation might be associated with the attribute credit-order  which indicates the ranking of the actor in the credits. we can introduce an explicit type corresponding to the relation. in this case  a relation object is itself related to both of its arguments. for example  if one of the role objects is  meryl streep in sophie's choice   this role object wouldbe related to the actor object meryl streep  and the movie object  sophie's choice . by definition  these relations are many-to-one. it will be useful to distinguish between entity types  such as or    and relation types  such as  .
　an instantiation specifies the set of objects in each type  the relations that hold between them  and the values of the attributes for all of the objects. a skeleton specifies only the objects and the relations. we will use to denote the set of objects of type .

	 a 	 b 	 c 
figure 1:  a  model for imdb domain;  b  model for cora domain;  c  fragment of unrolled network for cora model.　probabilistic model. a probabilistic relational model specifies a probability distribution over a set of instantiations of the relational schema. more precisely  a prm is a template  which is instantiated for different skeletons . the result of this instantiation is a probabilistic model over a set of random variables corresponding to all of the attributes of all of the objects in the skeleton. we can view a prm as a compact way of representing a bayesian network for any skeleton over this schema.
　a prm consists of a qualitative dependency structure    and the parameters associated with it  . the dependency structure is defined by associating with each attribute a set of parents pa . each parent of has the form of either or for .  prms also allow dependencies along chains of relations  but we have chosen to omit those for simplicity of presentation. 
　for a given skeleton   the prm structure induces an unrolled bayesian network over the random variables . for every object   depends probabilistically on parents of the form or . note that if is not singlevalued  then is actually a set of random variables  one for each . we address this problem by interpreting the dependence of on as dependence on an aggregatefunction e.g.  modeor mean of the multiset of values of these variables  see below .
　the quantitative part of the prm specifies the parameterization of the model. given a set of parents for an attribute  we can define a local probability model by associating with it a conditional probability distribution  cpd . for each attribute we have a cpd that specifies pa . when one of the parents is of the form and is many-valued  the cpd represents the dependence on the value of the aggregate. the cpd for is used for in the unrolled network  for every in . thus  the cpd for is repeated many times in the network.
　aggregates. there are many possible choices of aggregation operator to allow dependencies on a set of variables. an obvious choice for categorical variables is the mode aggregate  which computes the most common value of its parents. more precisely  consider some variable whose parents we wish to aggregate into a single variable . let the domain of each be   and note that has the same domain. the effect of the mode aggregator is as follows: we define a distribution
for each ; given a multiset of values for   we use the distribution for the value which is the most common in this multiset.
　the mode aggregator is not very sensitive to the distribution of values of its parents; for example  it cannot differentiate between a highly skewed and a fairly uniform set of values that have the same most frequent value. an aggregate that better reflects the value distribution is a stochastic mode aggregator. in this case  we still define a set of distributions	  but the effect of the aggregator is that is a weighted average of these distributions  where the weight of	is the frequency of this value within	. we accomplish this behavior by using an aggregate variable	stochastic-mode	  defined as follows. the aggregate variable also takes on val-
ues in	. let	be the number of variables
that take on the value	. then we define
       . it is easy to verify that this aggregator has exactly the desired effect.
　we note that this aggregate can also be viewed as a randomized selector node that chooses one of its parents uniformly at random and takes on its value. one appealing consequence is that  like min or max  the stochastic model can be decomposed to allow its representation as a cpd to scale linearly with the number of parents. we simply decompose the aggregate in a cascading binary tree. the first layer computes aggregates of disjoint pairs  with each aggregate randomly selecting the value of one of its parents; the following layer repeats the procedurefor disjoint pairs of results from the first layer  and so on.  this construction can also be extended to cases where the number of variables is not a power of ; we omit details for lack of space. 
1	classification and clustering models
we use the prm framework as the basis of our models for relational classification and clustering. as in the  flat  probabilistic generative approaches  our approach is based on the use of a special variable to represent the class or the cluster. this variable is the standard  class  variable in the classification task. as usual  we deal with the clustering task by introducing a new latent class variable. thus  for each entity class we have a designated attribute in .
　as in flat classification and clustering  we define the attributes of to depend on the class variable. for simplicity  we choose the naive bayes dependency model for the other attributes: for each attribute   the only parent of
is . note that we have only defined class attributes for entity types. we connect the attributes of relation types to the class attributes of the two associated entity types. thus  for example  an attribute such as credit-order in the relation class will depend on the class attributes of and
     . note that  as the dependence in this case is singlevalued by definition  no aggregates are necessary. most interestingly  we also allow direct dependence between class attributes of related entities. thus  for example  we could allow a dependence of on   or vice versa. in this case  as the relation is many-to-many  we use aggregates  as described above.
　fig. 1 a  shows a simple model for a movie dataset  extracted from the internet movie database  imdb   www.imdb.com . we see that  role  is both a class on its own  as well as defining the relation between movies and actors. we have chosen  in this case  not to have the attribute
　　credit-order depend on the class of movies  but only of actors. fig. 1 b  shows a model for a domain of scientific papers and authors  derived from the cora dataset  mccallum et al.  1   cora.whizbang.com . in this case  we see that the  cites  relation connects two objects of the same type. we have chosen to make the class attribute of the cited paper depend on the class attribute of the citing paper. note that this dependency appears cyclic at the type level. however  recall that this model is only a template  which is instantiated for particular skeletons to produce an unrolled network; fig. 1 c  shows a fragment of such a network. if we do not have  citation cycles  in the domain  then this unrolled network is acyclic  and the prm induces a coherent probability model over the random variables of the skeleton.  see  friedman et al.  1  for more details. 
　we can also use latent variable models to represent dyadic clustering. consider  for example  a domain where we have people and movies  and a relation between them that cor-
responds to a person rating a movie. in this case  we will have a class   corresponding to the relation  with the attribute rating representing the actual rating given. this attribute will depend on the cluster attributes of both and   leadingnaturallyto a two-sidedclustering model. however  our approach is flexible enough to accommodate a much richer model  e.g.  where we also have other attributes of person  and perhaps an entire relational model for movies  such as shown in fig. 1 a . our approach will take all of this information into consideration when constructing the clusters.
1	learning the models
we now show how we learn our models from data. our training set consists of a partial instantiation of the schema  one where everything except the values of some or all the class attributes is given. we can view this data as a single large  mega-instance  of the model  with a large number of missing values. note that we cannot view the data as a set of independent instances corresponding to the objects in the model. in our setting  we typically assume that the structure of our latent variable model is given  as described in section 1. thus  our task is parameter estimation.
1	parameter estimation
in this case  we assume that we are given the probabilistic dependency structure   and need only estimate the parameters
　  i.e.  the cpds of the attributes. a standard approach is to use maximum likelihood  ml  estimation  i.e.  to find that maximize	.
　if we had a complete instantiation   the likelihood function has a unique global maximum. the maximum likelihood parameters can be found very easily simply by counting occurrences in the data. recall that all of the objects in the same class share the same cpd. thus  to estimate the parameter for
	pa	  we simply consider all objects	of class
　  and count the number of times that each combination that and its parents jointly take. these counts are known as sufficient statistics. see  friedman et al.  1  for details. the case of incomplete data is substantially more complex. in this case  the likelihood function has multiple local maxima  and no general method exists for finding the global maximum. the expectation maximization  em  algorithm  dempster et al.  1   provides an approach for finding a local maximum of the likelihood function. starting from an initial guess for the parameters  em iterates the following two steps. the e-step computes the distribution over the unobservedvariables given the observed data and the current estimate of the parameters. letting be the set of unobserved cluster variables  we compute   from which it can compute the expected sufficient statistics:
n	pa
to compute the posterior distribution over the hidden variables  we must run inference over the model. the m-step reestimates the parameters by maximizing the likelihood with respect to the distribution computed in the e-step.
n

n
1	belief propagation for e step
to perform the e step  we need to compute the posterior distribution over the unobserved variables given our data. this inference is over the unrolled network defined in section 1. we cannot decompose this task into separate inference tasks over the objects in the model  as they are all correlated.  in some cases  the unrolled network may have several connected components that can be treated separately; however  it will generally contain one or more large connected components. 
　in general  the unrolled network can be fairly complex  involving many objects that are related in various ways.  in our experiments  the networks involve tens of thousands of nodes.  exact inference over these networks is clearly impractical  so we must resort to approximate inference. there is a wide variety of approximation schemes for bayesian networks. for various reasons  some of which are described below   we chose to use belief propagation. belief propagation  bp  is a local message passing algorithm introduced by pearl  pearl  1 . it is guaranteed to converge to the correct marginal probabilities for each node only for singly connected bayesian networks. however  empirical results  murphy and weiss  1  show that it often converges in general networks  and when it does  the marginals are a good approximation to the correct posteriors.  when bp does not converge  the marginals for some nodes can be very inaccurate. this happens very rarely in our experiments and does not affect convergence of em. 
　we provide a brief outline of one variant of bp  referring to  murphy and weiss  1  for more details. consider a bayesian network over some set of nodes  which in our case would be the variables  . we first convert the graph into a family graph  with a node for each variable in the bn  containing and its parents. two nodes are connected if they have some variable in common. the cpd of is associated with . let represent the factor defined by the
cpd; i.e.  if	contains the variables	  then
is a function from the domains of these variables to . we also define to be a factor over that encompasses our evidence about : if is not observed. if we observe   we have that and elsewhere. our posterior distribution is then   where is a normalizing constant.
　the belief propagation algorithm is now very simple. at each iteration  all the family nodes simultaneously send message to all others  as follows:
where is a  different  normalizing constant and is the set of families that are neighbors of in the family graph. at any point in the algorithm  our marginal distribution about any family is . this process is repeated until the beliefs converge.
　after convergence  the give us the marginal distribution over each of the families in the unrolled network. these marginals are precisely what we need for the computation of the expected sufficient statistics.
　we note that occasionally bp does not converge; to alleviate this problem  we start the em algorithm from several different starting points  initial guesses . as our results in section 1 show  this approach works well in practice.
1	influence propagation over relations
among the strong motivations for using a relational model is its ability to model dependencies between related instances. as described in the introduction  we would like to propagate information about one object to help us reach conclusions about other  related objects. recently  several papers have proposed a process along the lines of this  influence propagation  idea. neville and jensen  propose an iterative classification algorithm which builds a classifier based on a fully observed relational training set; the classifier uses both base attributes and more relational attributes  e.g.  the number of related entities of a given type . it then uses this classifier on a test set where the base attributes are observed  but the class variables are not. those instances that are classified with high confidence are temporarily labeled with the predictedclass; the classification algorithmis then rerun  with the additional information. the process repeats several times. the classification accuracy is shown to improve substantially as the process iterates.
　slattery and mitchell  propose an application of this idea to the problem of classifying web pages  e.g.  as belonging to a university student or not. they first train a classifier on a set of labeled documents  and use it to classify documents in the test set. to classify more documents in the test set  they suggest combining the classification of the test set pages and the relational structure of the test set. as a motivating example  they describe a scenario where there exists a page that points to several other pages  some of which were classified as studenthome pages. theirapproachtries to identify this page as a student directory page  and conclude that other pages to which it points are also more likely to be student pages. they show that classification accuracy improves by exploiting the relational structure.
　neither of these approaches proposes a single coherent model of the dependencies between related objects and thus combine different classification steps or algorithms without a unifyingprinciple. our approachachieves the influence propagation effect through the probabilistic influences induced by the unrolled bayesian network over the instances in our domain. for example  in the cora domain  our network models correlations between the topics of papers that cite each other. thus  our beliefs about the topic of one paper will influence our beliefs about the topic of its related papers. in general  probabilistic influence  flows  through active paths in the unrolled network  allowing beliefs about one cluster to influence others to which it is related  directly or indirectly . moreover  the use of belief propagation implements this effect directly. by propagating a local message from one family to another in the family graph network  the algorithm propagates our beliefs about one variable to other variables to which it is directly connected. we demonstrate this property in the next section.
　this spreadinginfluenceis particularlyuseful in our framework due to the application of the em algorithm. the em algorithm constructs a sequence of models  using the probabilities derived from the belief propagation algorithm to train a new model. hence  we not only use our probabilistic inference process to spread the information in the relational structure  we then use the results to construct a better classifier  which in turn allows us to obtain even better results  etc. from a different perspective  we are using the structure in the test set not only to provide better classifications  but also to learn a better classifier. as we show below  this process results in substantial improvements in accuracy over the iterations of em. we note that this bootstrappingability arises very naturally in the probabilistic framework  where it is also associated with compelling convergence guarantees.
1	experiments
we evaluated our method on the cora and imdb data sets.
　cora. the structure of the cora dataset  and the model we used  are shown in fig. 1 b c . for our experiments  we selected a subset of 1 papers from the machine learning category  along with 1 of their authors. these papers are classified into seven topics: probablistic methods  neural networks  reinforcement learning  rule learning  casebased  and theory.
	fraction labeled	loopyiteration	1	1 em iteration1	1	1
	 a 	 b 	 c 
figure 1:  a  comparison of classification accuracies;  b  influence propagation in bp;  c  accuracy improvement in em.　we evaluated the ability of our algorithm to use the relational structure to aid in classification. we took our entire data set  and hid the classifications for all but a fraction of the papers. we then constructed our model based on all of this data  including the documents whose topics were unobserved. the resulting model was used to classify the topic for the test documents. in effect  we are performing a type of transduction  where the test set is also used to train the model  albeit without the class labels .
　to investigate how our method benefits from exploiting the relational structure  we considered four different models which vary in the amount of relational information they use. the baseline model does not use relational information at all. it is a standard multinomial naive bayes model  nb  over the set of words  bag of words model  in the abstract. the full model  ac  was shown in fig. 1 b ; it makes use of both the authors and citations. the other two models are fragments of ac: model a incorporates only the author information  eliminating the citation relation from the model   and model c only citations. all four models were trained using em; model nb was trained using exact em and the others using our algorithm of section 1. we initialized the cpds for the word attributes using the cpds in a naive bayes model that was trained only on the observed portion of the data set. all models were initialized with the same cpds.
　we varied the percentage of labeled papers  ranging from 1% to 1%. for each different percentage  we tested the classification accuracy over five random training/test splits. the results are shown in fig. 1 a . each point is the average of the accuracy on the five runs  and the error bars correspond to the standard error. as can be seen  incorporating more relational dependencies significantly improves classification accuracy. both a and c outperform the baseline model  and the combined model ac achieves by far the highest accuracy.
　as discussed in section 1  the local message passing of loopy belief propagation  bp  resembles the process of  spreading the influenceof beliefs fora particularinstance to its related instances. for example  suppose paper cites several labeled papers. upon initialization  we have some initial belief about the topic of from its words alone. however  after the first iteration  this belief will be updated to reflect the labels of the papers it cites  and is likely to become more peaked around a single value  increasing the confidence in 's topic. in the following iteration  unlabeled papers that cite  as well as unlabeled papers that cites  will be updated to reflect the increased confidence about the topic of   and so on. to measure this effect  we examine the belief state of the topic variable of the unlabeled papers after every iteration of loopy belief propagation. for every iteration  we report the fraction of variables whose topic can be determined with high confidence  i.e.  whose belief for a single topic is above a threshold of . fig. 1 b  shows several series of these measurements on a dataset with 1% labeled papers. the series show bp iterations performed within the first  third and seventh iteration of em. each series shows a gradual increase of the fraction of papers whose topics we are confident in. the accuracy on those high-confidence papers is fairly constant over the iterations - around 1  1  and 1 for the first  third and seventh iteration of em  respectively.
　loopy belief propagation is an approximation to the inference required for the e step of em. although loopy bp is not guaranteed to converge  in our experiments  it generally converges to a solution which is good enough to allow em to make progress. indeed  fig. 1 c  shows that the classification accuracy improves for every em iteration. this figure also demonstrates the performanceimprovementobtained from bootstrapping the results of iterative classification  as discussed in section 1.
　imdb. the attributes and relations in the imdb database  and the latent variablemodel we used  are shown in are shown in fig. 1 a ; the genre attribute actually refers to a set of 1 binary attributes  action  comedy  ... . note that actors and directors have almost no descriptive attributes and hence cannot be clustered meaningfully without considering their relations. we selected a subset of this database that contains 1 movies  1 actors  and 1 directors. in fig. 1  we show two example clusters for each class  listing several highest confidence members of the clusters.
　in general  clusters for movies consist of movies of predominantly of a particular genre  time period and popularity. for example  the first movie cluster shown can be labeled as classic musicals and children's films. the second cluster corresponds roughly to action/adventure/sci-fi movies. in our model  the clusters for actors and directors are relational in nature  since they are induced by the movie attributes. for example  the first cluster of actors consists primarily of action

swiss family robinson	cinderella
sound of music  the	love bug  the
wizard of oz  the	pollyanna
parent trap  the	mary poppinsrobin hood: prince of thieves	batman
hunt for red october  the	batman forever
mission: impossible	goldeneye
terminator 1: judgment day	starship troopers
hitchcock  alfred
kubrick  stanley
coppola  francis ford
lean  david
forman  milos
gilliam  terry
wyler  williamspielberg  steven
cameron  james
mctiernan  john
burton  tim
scott  tony
schumacher  joel
stallone  sylvester	russell  kurt
schwarzenegger  arnold	costner  kevin
van damme  jean-claude	willis  bruce
ford  harrison	seagal  stevenjones  tommy lee	de niro  robert
hopkins  anthony	keitel  harvey
freeman  morgan	oldman  garyfigure 1: clusters of movies  actors  and directors
movie actors and the second of actors who primarily play in dramas. similarly for directors  the first cluster corresponds to directors of dramas and while the second to directors of popular action and adventure films.
1	discussion and conclusions
many real-world domains have a rich relational structure  with complex webs of interacting entities: the web; papers and authors; biomedical datasets; and more. traditional machine learning algorithms ignore this rich relational structure  flattening it into a set of iid attribute vectors. recently  however  there has been growing interest in learning methods that exploit the relational structure of the domain.
　in this paper  we provide a general method for classification and clustering in richly structureddata with instances and relations. our approach has coherent probabilistic semantics  allowing us to build on powerful tools for probabilistic reasoning and learning. our algorithm uses an effective combination of these techniques to provide linear scaling in the number of instances; it can thus be applied to very large domains.
　we have shown in a transduction task that the relational information allows us to achieve substantially better accuracies than a standard  flat  classification scheme. we have also shown anecdotally that our algorithm constructs interesting clusters based on relational information. finally  our approach induces a compelling behavior unique to relational settings: because instances are not independent  information about some instances can be used to reach conclusions about others. our approach is the first to provide a formal framework for this behavior.
　there are many interesting extensions to this work. most obvious is the problem of model selection. in this paper  we have used predetermined models  both the edges in the dependency model and the number of clusters of each latent class attribute. our framework allows us to incorporate into our models a great deal of prior knowledge about the semantics of the underlying domains. however  when domain expertise is lacking  automatic model construction becomes crucial. we can extend our approach using techniques for model selection in bayesian networks  friedman  1; cheeseman and stutz  1   allowing our learning algorithm to select the model structure that best suits the data.
acknowledgments. this work was supported by onr contract n1-c-1 under darpa's hpkb program. eran segal was also supported by a stanford graduate fellowship  sgf .
references
 cheeseman and stutz  1  p. cheeseman and j. stutz. bayesian classification  autoclass : theory and results. in fayyad u.  piatesky-shapiro g.  smyth p.  and uthuruasmy r.  editors  advances in knowledge discovery and data mining  pages 1- 1. aaai press  menlo park  ca  1.
 cohn and chang  1  d. cohn and h. chang. probabilistically identifying authoritative documents. in proc. sigir  1.
 cohn and hofmann  1  d. cohn and t. hofmann. the missing link: a probabilistic model of document content and hypertext connectivity. in proc. nips  1. to appear.
 dempster et al.  1  a. p. dempster  n. m. laird  and d. b. rubin. maximum likelihood from incomplete data via the em algorithm. journal of the royal statistical society  b 1-1  1.
 duda et al.  1  r. o. duda  p. e. hart  and d. g. stork. pattern classification. john wiley & sons  new york  1.
 friedman et al.  1  n. friedman  l. getoor  d. koller  and a. pfeffer. learning probabilistic relational models. in proc. ijcai  1.
 friedman  1  n. friedman. the bayesian structural em algorithm. in proc. uai  1.
 hofmann and puzicha  1  t. hofmann and j. puzicha. latent class models for collaborative filtering. in proc. ijcai  1.
 kleinberg  1  j. kleinberg. authoritative sources in a hyperlinked environment. in proc. 1th acm-siam symposium on discrete algorithms  1.
 koller and pfeffer  1  d. koller and a. pfeffer. probabilistic frame-based systems. in proc. aaai  1.
 mccallum et al.  1  a. mccallum  k. nigam  j. rennie  and k. seymore. automating the construction of internet portals with machine learning. information retrieval journal  1  1.
 murphy and weiss  1  k. murphy and y. weiss. loopy belief propagation for approximate inference: an empirical study. in uai  1.
 neville and jensen  1  j. neville and d. jensen. iterative classification in relational data. in proc. aaai-1 workshop on learning statistical models from relational data  pages 1. aaai press  1.
 pearl  1  j. pearl. probabilistic reasoning in intelligent systems. morgan kaufmann  1.
 slattery and craven  1  s. slattery and m. craven. combining statistical and relational methods in hypertext domains. in proc. ilp  1.
 slattery and mitchell  1  s. slattery and t. mitchell. discovering test set regularities in relational domains. in proc. icml  1.

machine learning and
data mining
machine learning and data mining

adaptive web navigation for wireless devices
corin r. anderson  pedro domingos  daniel s. weld university of washington  seattle  wa  usa
{corin  pedrod  weld} cs.washington.edu

abstract
visitors who browse the web from wireless pdas  cell phones  and pagers are frequently stymied by web interfaces optimized for desktop pcs. simply replacing graphics with text and reformatting tables does not solve the problem  because deep link structures can still require minutes to traverse.
in this paper we develop an algorithm  minpath  that automatically improves wireless web navigation by suggesting useful shortcut links in real time. minpath finds shortcuts by using a learned model of web visitor behavior to estimate the savings of shortcut links  and suggests only the few best links. we explore a variety of predictive models  including na： ve bayes mixture models and mixtures of markov models  and report empirical evidence that
minpath finds useful shortcuts that save substantial navigational effort.
1	introduction
perkowitz and etzioni  challenged the ai community to build adaptive web sites: sites that automatically improve their organization and presentation by learning from visitor access patterns. in the spirit of this challenge  many research projects have been proposed and implemented  perkowitz and etzioni  1; fink et al.  1; yan et al.  1; juhne： et al.  1; joachims et al.  1; pazzani and billsus  1; sarukkai  1 . many of these projects  like much of the web today  assume the visitor is browsing with a large color display and fast network connection. in addition  these works typically assume that a visitor's interest in a site lies in viewing many pages of content  as opposed to a specific destination. consequently  the adaptations they emphasize include graphical highlighting of existing hypertext links and automatic generation of indices that link to many pages.
　however  a growing community of web visitors does not fit this mold: wireless web visitors  who browse content from cell phones  pagers  and wireless pdas. these mobile devices do not have the same display or bandwidth capabilities as their desktop counterparts  but sites  adaptive or otherwise  nonetheless deliver the same content to both desktop pcs and mobile devices. moreover  our experience with mobile visitors indicates that they seldom browse through many pages  but rather are interested only in specific destinations  and would like to reach those pages by following as few links as possible. while this observation is also largely true of desktop visitors  reducing the number of links followed is particularly poignant for mobile visitors for two reasons. first  simply finding a specific link on a page requires the visitor to spend considerable time scrolling through the page on a tiny screen. second  because of the low bandwidth and high latency of wireless networks  following a link to even a simple page of text can take as long as tens of seconds. consequently  navigating through even four or five different pages can easily take several minutes  frustrating visitors to the point of abandoning their effort.
　we believe adaptive web sites hold a great promise for improving the web experience for wireless devices  and  conversely  that wireless web browsing is a  killer app  of adaptive web sites. in ongoing research  anderson et al.  1   we have developed a general framework for adapting web sites for wireless visitors that takes into account the value visitors receive for viewing a page and the effort required to reach that page  i.e.  the amount of scrolling and number of links followed . in this paper  we explore a particular adaptation for use in our framework: automatically adding shortcut links to each page a visitor requests. we assume that wireless visitor behavior is dominated by information gathering tasks  which can be accomplished by viewing specific destination pages on the site. shortcut links connect two pages not otherwise linked together and can help visitors reach these destinations as quickly as possible. for example  if a ★ b denotes a link  and the only path from a to d is a ★ b ★ c ★ d  then a shortcut a ★ d saves the visitor over 1% of the navigation effort  and perhaps more if one counts the scrolling avoided on the interim pages. of course  there is a tradeoff between the number of shortcut links and their usefulness. in an absurd example  if one added shortcuts between every pair of pages on the site  a visitor could reach any destination in one link  but finding the right link would be practically impossible. thus  we concentrate on generating only a small number of high quality shortcuts  for example  only as many as will fit on the wireless device without scrolling. this paper makes the following contributions:
  we present an algorithm  minpath  for automatically finding high-quality shortcuts for mobile visitors in real time. offline  minpath learns a model of web usage based on server access logs  and uses this model at runtime to predict the visitor's ultimate destination. minpath is based on the notion of the expected savings of a shortcut  which incorporates the probability that the link is useful and the amount of visitor effort saved.
  we evaluate a variety of visitor models  including na： ve bayes mixture models and mixtures of markov models  and discuss their applicability in minpath.   we provide experimental evidence that suggests a mixture of markov models is the best model for minpath  and that minpath substantially reduces the number of links mobile visitors need to follow  and thus their navigational effort.
in the next section  we define our problem and present the
minpath algorithm for finding shortcuts. section 1 explores variations on the models for predicting web usage and section 1 evaluates minpath using these models. section 1 discusses related research  and we conclude in section 1.
1	finding shortcuts with minpath
we begin by defining terminology  to facilitate the discussion.
1	definitions
a trail  wexelblat and maes  1  is a sequence of page requests made by a single visitor that is coherent in time and space. coherence in time requires that each subsequent request in the trail occurs within some fixed time window of the previous request. coherence in space requires that each subsequent request be the destination of some link on the previous page. more precisely  if we denote the time of the request for page pi as time pi   then a trail t = hp1 p1 ...pni is a sequence of page requests for which:
    i 1 ＋ i   n pi ★ pi+1 exists; and
    i 1 ＋ i   n 
time pi  ＋ time pi+1  ＋ time pi + timeout
　the length of a trail is n  the number of links followed. from the perspective of the adaptive web site which is watching a visitor's behavior midway through the trail  only a prefix  hp1 ... pii is known. the trail suffix  hpi+1 ... pni  needs to be hypothesized by the adaptive web site.
1	finding shortcuts
the objective of our work is to provide shortcut links to visitors to help shorten long trails. our system adds shortcut links to every page the visitor requests. ideally  the shortcuts suggested will help the visitor reach the destination of the trail with as few links as possible. we state the shortcut link selection problem precisely as:
  given: a visitor v   a trail prefix hp1 ... pii  and a maximum number of shortcuts m;
  output: a list of shortcuts pi ★ q1 ... pi ★ qm that minimizes the number of links the visitor must follow between pi and the visitor's destination.
　the last page in the trail prefix  pi  is the page the visitor has requested most recently  and the page on which the shortcuts are placed. we calculate the savings that a single shortcut pi ★ q offers as the number of links the visitor can avoid by following that shortcut. if we know the entire trail t = hp1 ... pi ... pni  then the number of links saved is: 1 if q = pj for some i   j ＋ n
	1	otherwise
that is  if the shortcut leads to a page further along the trail  then the savings is the number of links skipped  we subtract one because the visitor must still follow a link - the shortcut link . if the shortcut leads elsewhere  then it offers no savings.
1	the minpath algorithm
if one had knowledge of the complete trail hp1 ... pi ...pni  selecting the best shortcut at any page pi would be easy: simply  pi ★ pn. of course  at runtime  a visitor has viewed only a trail prefix  and the adaptive web site must infer the remaining pages. our approach relies on a model of the visitor's behavior to compute a probability for every possible trail suffix hqi+1 ... qni on the site. intuitively  these suffixes are all the possible trails originating from pi. given a suffix and its probability  we assign an expected savings to the shortcut pi ★ qj to each qj in the suffix as the product of the probability of the suffix and the number of links saved by the shortcut. note that a particular shortcut pi ★ qj may appear in many trail suffixes  i.e.  many trail suffixes may pass through the same page qj   and so the expected savings of a shortcut is the sum of the savings of the shortcut for all suffixes.
　a brief example will elucidate these ideas. suppose that a visitor has requested the trail prefix ha b ci and we wish to find shortcuts to add to page c. suppose that our model of the visitor indicates there are exactly two sequences of pages the visitor may complete the trail with: hd e f g hi  with a probability of 1  and hi j h ki with a probability of 1. the expected savings from the shortcut c ★ e would be 1 〜 1 = 1  because the trail with page e occurs with probability 1 and the shortcut saves only one link. the expected savings for shortcut c ★ h includes a contribution from both suffixes: 1 〜 1.1 〜 1 = 1+1 = 1.
　the minpath algorithm is shown in table 1. the expectedsavings function constructs the trail suffixes by traversing the directed graph induced by the web site's link structure  often called the  web graph  . starting at the page last requested by the visitor  pi  expectedsavings computes the probability of following each link and recursively traverses the graph until the probability of viewing a page falls below a threshold  or a depth bound is exceeded. the savings at each page  currentsavings  is the product of the probability  ps  of reaching that page along suffix ts and the number of links saved  l   1. the minpath function collates the results and returns the best m shortcuts. the next section describes how we obtain the model required by minpath.
1	predictive models
the key element to minpath's success is the predictive model of web usage; in this section  we describe the models we have evaluated. the probabilistic model minpath uses must predict the next web page request pi given a trail prefix hp1 ... pi 1i and the visitor's identity v  the identity can lead to information about past behavior at the site  demographics  etc. : p pi = q|hp1 ... pi 1i v  . of course  a model may condition this probability on only part or even none of the available data; we explore these and other variations in this section. to simplify our discussion  we define a  sink  page that visitors  implicitly  request when they inputs:
t	observed trail prefix hp1 ... pii pi	most recent page requested v	visitor identity m	number of shortcuts to return
minpath t pi v m 
s ○ expectedsavings pi t v hi 1 1 {} 
sort s by expected page savings
return the best m shortcuts in s
inputs:
p	current page in recursive traversal t	trail prefix  observed page requests 
v	visitor identity
ts	trail suffix  hypothesized pages in traversal  ps	probability of suffix ts l	length of suffix ts
s	set of shortcut destinations and their savings
expectedsavings p t v ts ps l s 
if  l − depth bound  or  ps ＋ probability threshold 
return s
if  l ＋ 1 
currentsavings ○ 1
else
　currentsavings ○ ps 〜  l   1  if p 1（ s
add p to s with savings p  = currentsavings
else
savings p  ○ savings p  + currentsavings
trail ○ concatenate t and ts
for each link p ★ q
pq ○ probability of following p ★ q given trail and v
tq ○ concatenate ts and {q}
　s ○ expectedsavings q t v tq pq l +1 s  return s
table 1: minpath algorithm.
end their browsing trails. thus  the probability p pi = psink|hp1 ... pi 1i v   is the probability that the visitor will request no further pages in this trail. finally  note that the models are learned offline  prior to their use by minpath. only the evaluation of the model must run in real time.
1	unconditional model
the simplest model of web usage predicts the next page request pi without conditioning on any information. we learn this model by measuring the proportion of requests for each page q on the site during the training period1:
number of times q requested
p pi = q  = 
total number of page requests
　we assume the visitor can view a page only if it is linked from the current page. thus minpath forces the probabilities of pages not linked from the current page to be zero and renormalizes the probabilities of the available links. if the current page is pi 1  then minpath calculates:
	|	1	q1	if pi 1 ★ q exists
                        	1	otherwise where the q1 are all the pages to which pi 1 links.
　because we have a limited volume of training data  approximately 1 page requests for approximately 1 unique urls in a site with 1 web pages   we cannot build a model that predicts each and every page - many pages are requested too infrequently to reliably estimate their probability. instead  we group pages together to increase their aggregate usage counts  and replace page requests by their corresponding group label  much in the spirit of  zukerman et al.  1  . specifically  we use the hierarchy that the url directory structure imposes as a hierarchical clustering of the pages  and select only the most specific nodes  the ones closest to the leaves  that account for some minimum amount of traffic  or usage  on the site. the pages below each node share a common url prefix  or stem  which we use as the label of the node. by varying the minimum usage threshold  we select more or fewer nodes; in section 1  we report how minpath's performance is correspondingly affected.
1	na： ve bayes mixture model
the unconditional model assumes all trails on the site are similar - that a single model is sufficient to accurately capture their behavior. common intuition suggests this assumption is false - different visitors produce different trails  and even the same visitor may follow different trails during separate visits. as an alternative  we hypothesize that each trail belongs to one  or a distribution  of k different clusters  each described by a separate model. we can thus compute the probability of requesting page q by conditioning on the cluster identity ck:
p pi = q|hp1 ... pi 1i  =
k
	xp pi = q|ck p ck|hp1 ... pi 1i 	 1 
k=1
the result is a mixture model that combines the probability estimates p pi = q|ck  of the k different models according to a distribution over the models. by bayes' theorem  p ck|hp1 ... pi 1i  『 p ck p hp1 ... pi 1i|ck . to calculate p hp1 ... pi 1i|ck   we make the na： ve bayes assumption that page requests in the trail are independent given the cluster  thus: p hp1 ... pi 1i|ck  =
qj=1...i 1 j|ck . the resulting model is a na： ve bayes p p mixture model  similar to those used in autoclass  cheeseman et al.  1   for which we learn the model parameters p pi = q|ck  and the cluster assignment probabilities p ck  using the em algorithm  dempster et al.  1 .
	the	mixture	model	uses	the	probabilities
p ck|hp1 ... pi 1i  as a  soft  assignment of the trail to the cluster - each cluster ck contributes fractionally in the sum in equation 1. alternatively  we may use a  hard  assignment of the trail to the most probable cluster  c . we explore both of these possibilities in section 1. the value of k may be fixed in advance  or found using holdout data. for each value of k  we compute the likelihood of the holdout data given the previously learned model  and choose the k that maximizes the holdout likelihood.
　an additional piece of information useful when selecting the cluster assignment is the visitor's identity  which we can incorporate by conditioning equation 1 on v . if we assume that page requests are independent of the visitor given the cluster  then the only change to the right side of equation 1 is that p ck  becomes p ck|v  . unlike an individual trail  a visitor's behavior may not be well represented by any single model in the mixture. thus  we represent a visitor as a mixture of models  and estimate the p ck|v   as the proportion of the visitor's history that is predicted by ck. specifically  let h = {t1 ... th} be the set of h trails the visitor has produced on the site previous to the current visit  and p ti|cj  the probability that cluster cj produced trail ti; then.
1	markov models
both the unconditional and na： ve bayes mixture models ignore a key piece of information from the web accesses: the sequential nature of the page trails. a first-order markov model  on the other hand  incorporates this information by conditioning the probability of the next page on the previous page: p pi = q|pi 1 . the markov model is trained by counting the transitions from pages pi 1 to pi in the training data  and by counting how often each page appears as the initial request in a trail. as we did earlier  we replaced the urls of page requests with url stems to increase the volume of relevant training data. the need for this transformation is even greater for the markov model because it has quadratically more probability values to estimate than the unconditional model  and the events  the links pi 1 ★ pi  are more rare.
in addition to a single markov model  we also evaluate
minpath using a mixture of markov models  cadez et al.  1 . we use the same em-based method to build these mixtures as we did to learn the na： ve bayes mixture model.
1	positional and markov/positional models
in addition to conditioning the probability on the last requested page  we also consider conditioning on the ordinal position of the request in the visitor's trail: p pi = q|i  or p pi = q|i pi 1 . effectively  this model is equivalent to training a separate model  either unconditional or markov  for each position in the trail  although  for practical purposes  we treat all positions after some limit l as the same position . visual inspection of the training trails led us to hypothesize that these models may better predict behavior  although conditioning on the additional information increases the amount of training data necessary to properly fit the model.
1	results
we evaluate minpath's performance on usage at our home institution's web site. we used web access data during september 1 to produce a training set of 1 trails  approximately 1 days of web usage  and a test set of 1 trails  approximately 1 days of usage ; the time period from which the test trails were drawn occurred strictly after the training period. during the training and testing periods  1 unique pages were requested from the total population of 1 unique urls at the site. we selected only those trails with link length at least two  because shorter trails cannot be improved. we set minpath's link depth bound to 1 and probability threshold to 1; in all our experiments the probability threshold proved to be the tighter bound.
　we measure minpath's performance by the number of links a visitor must follow to reach the end of the trail. we estimate visitor behavior when provided shortcuts by making two assumptions. first  we assume that  when presented with one or more shortcuts that lead to destinations along the visitor's trail  the visitor will select the shortcut that leads farthest along the trail  i.e.  the visitor greedily selects the apparently best shortcut . second  when no shortcuts lead to pages in the visitor's trail  the visitor will follow the next link in the trail  i.e.  the visitor will not erroneously follow a shortcut . note  finally  that minpath places shortcuts on each page the visitor requests  and so the visitor may follow multiple shortcut links along a single trail.
　without shortcuts  the average length of trails in the test set is 1 links. given an oracle that could predict the exact destination of the visitor's current trail  minpath could reduce the trail to exactly one link. the difference between 1 links and one link is the range of savings minpath can offer web visitors.
　we first explored the relationship between the minimum url usage threshold and the performance of minpath. we compared thresholds of 1%  which produces 1 url stems   1%  1 stems   1%  1 stems   and 1%  all the unique urls in the training data . we found that minpath's performance improves as we increase the number of url stems  until the threshold falls below 1%. after that point  the average number of links per trail increases; we hypothesize that  because of data sparseness  we cannot learn the model as well. thus  for all the experiments in this section  we use the best threshold we found  1%. in ongoing work  we are evaluating the use of a lower threshold when minpath is given substantially more training data.
　we next compared minpath's performance when using a variety of models  see figure 1 . the first column shows the number of links followed in the unmodified site. in the second and third sets of columns  minpath uses  respectively  an unconditional and markov model and produces 1  1  or 1 shortcuts. in the last two sets  minpath uses mixture models of either 1 or 1 clusters  and selects the distribution of the models in the mixtures based on only the current trail prefix  ignoring past visitor behavior . this graph demonstrates first that minpath does reduce the number of links visitors must follow - when using a mixture of markov models and suggesting just three shortcuts  minpath can save an average of 1 links  or 1% of the possible savings. second  we see that the markov model  by conditioning on the sequence information  outperforms the unconditional model substantially - three shortcuts suggested with the markov model are better than five shortcuts found with the unconditional model. third  these results indicate the mixture models provide a slight advantage over the corresponding single model  for example  1 for the na： ve bayes mixture model versus 1 for the unconditional model . we computed the average of the difference in trail length between the single model and the mixture model for each test trail  and found the gains are significant at the 1% level. finally  we found that the dif-
1	1
unmodified unconditional markov na ve bayes mixture of model model mixture model markov models
figure 1: minpath's performance. each column shows the average number of links followed in a trail. the mixture model columns are annotated with the number of clusters. all error-bars denote 1% confidence intervals.

figure 1: varying model assignment strategy. each of the four series represents a different model assignment strategy.
ferences between 1 and 1 clusters in the mixture are not statistically significant.
　in figure 1  we compare methods for selecting the mixture distribution for a trail prefix  using mixtures of 1 models. each group of columns shows a different model and assignment type  hard or soft  combination. in each group  we condition the assignment on no information  i.e.  we use a uniform distribution for the soft assignment and random selection for the hard assignment   the visitor's past trails  the visitor's current trail  and both the past and current trails. our first conclusion is that soft assignment is a better choice for both mixture models  significant at the 1% level . second  both past trails and the current trail prefix help minpath select an appropriate assignment to the cluster models. however  the combination of both features is not significantly better than using just the current trail prefix with the na： ve bayes mixture model  and does slightly worse than just the current trail with the mixture of markov models. this result is somewhat surprising; we had expected  especially when the prefix is short  that the past trails would provide valuable information. apparently  however  even the first one or two page requests in a trail are sufficient to assign it to the appropriate clusters. in future work we will investigate if this result remains true for larger sites.
　our last variation of models conditions the probability on the ordinal position of the page request in the trail. we compared the unconditional and markov models against positional and markov/positional models  choosing several values of the limit l of number of positions. in all cases  minpath did not perform significantly differently when using the positional information than when ignoring the position.
　we finally note that minpath's running time is quite small. the models minpath uses are learned offline  but the process usually requires only several minutes. given a model and the trail prefix  minpath finds a set of shortcuts in 1 seconds on an average desktop pc  fast enough to suggest shortcuts in real time for wireless visitors.
1	related work
perkowitz  addresses the shortcut link problem  but uses a simpler shortcut prediction method: for each page p viewed on the site  record how often every other page q is viewed after p in some trail. when page p is requested in the future  the shortcuts are the top m most-requested pages q. effectively  this approach estimates the probability that a visitor at p will eventually view q by counting how often this event has occured in the training data. minpath also estimates this probability  but does so by composing the page transition probabilities along a trail through the site. the advantage of our approach is that it reduces data sparseness  although at the expense of making a first-order assumption that may not hold in practice. however  experience in other applications  e.g.  speech  nlp  computational biology  suggests the advantage outweighs the disadvantage. minpath offers two additional improvements relative to perkowitz's approach. first  minpath can build more accurate models of visitor behavior by clustering visitors and building mixture models; in contrast  perkowitz's approach builds a single shortcut table for all the visitors at the site. second  minpath admits a more versatile selection of shortcuts. for example  we are currently extending minpath to calculate the expected savings of each shortcut given the existence of the other shortcuts added to the requested page. perkowitz's approach cannot take advantage of this conditional information  because it derives its recommendations directly from the original usage data.
　our minpath algorithm shares many traits with a number of web page recommendation systems developed in recent years. letizia  lieberman  1  is a client-side agent that browses the web in tandem with the visitor. based on the visitor's actions  e.g.  which links the visitor followed  whether the visitor records a page in a bookmarks file  etc.   letizia estimates the visitor's interest in as-yet-unseen pages. unlike minpath  which resides on a web server  letizia is constrained to the resources on the web visitor's browsing device  and is thus not well suited to a wireless environment. in addition  letizia cannot leverage the past experiences of other visitors to the same site - letizia knows about the actions of only its visitor.
　webwatcher  joachims et al.  1   ariadne  juhne： et al.  1   and adaptive web site agents  pazzani and billsus  1  are examples of web tour guides  agents that help visitors browse a site by suggesting which link each visitor should view next. with the assistance of a tour guide  visitors can follow trails frequently viewed by others and avoid becoming lost. however  tour guides assume that every page along the trail is important  and typically are limited to only suggesting which link on a page to follow next  as opposed to creating shortcuts between pages .
　surflen  fu et al.  1  and pagegather  perkowitz and etzioni  1  suggest pages to visit based on page requests co-occurrent in past sessions1. these algorithms suggest the top m pages that are most likely to co-occur with the visitor's current session  either by presenting a list of links  surflen  or by constructing a new index page containing the links  pagegather . however  both of these systems assume the visitor can easily navigate a lengthy list of shortcuts  and thus provide perhaps dozens of suggested links. minpath improves on these algorithms by factoring in the relative benefit of each shortcut  and suggesting only the few best links specific to each page request.
　the predictive web usage models we present are related to previous works on sequence prediction and web usage mining. these works are too numerous to review here  but we mention two closely related ones. most similar to our own work  webcanvas  cadez et al.  1  is a system for visualizing clusters of web visitors using a mixture of markov models. we apply similar models to web behavior  although our goal is to build predictive structures  while webcanvas emphasizes visualizing the clusters themselves. sarukkai  uses a markov model of web usage to suggest the most probable links a visitor may follow  and notes the need to reduce the size of the model by clustering the urls. our work explores this model as well as many others  and uses the expected savings of a link  not just the link probability  to sort the resulting suggestions.
1	conclusions
wireless web devices will soon outnumber desktop browsers  and sites must be prepared to deliver content suited to their unique needs. because of the high cost of navigation on a mobile device  shortcut links are a fruitful adaptation to augment existing content. in this paper we have made the following contributions:
  we developed the minpath algorithm  which finds shortcut links targeted for each web visitor's information gathering behavior;
  we explored several predictive models of web usage and evaluated how they perform with minpath;
  we provided empirical evidence that minpath can find useful shortcut links. using a mixture of markov models  minpath can save wireless visitors more than 1% of the possible link savings.
　minpath offers many fruitful lines of continued research  and we are currently exploring several directions. one direction is studying how minpath will scale to larger sites  with more pages  more links between pages  and more traffic. another is automatically selecting concise and descriptive anchor texts for shortcut links. in a third direction we are integrating minpath with our general framework for adapting web sites  which includes a more elaborate visitor model and incorporates the cost of adding a shortcut link and the probability of erroneously following a shortcut. finally  we are currently conducting a user study to evaluate minpath on a fielded web site.
references
 anderson et al.  1  c. r. anderson  p. domingos  and d. s. weld. personalizing web sites for mobile users. in proc. 1th intl. www conf.  1.
 cadez et al.  1  i. v. cadez  d. heckerman  c. meek  p. smyth  and s. white. visualization of navigation patterns on a web site using model based clustering. in proc. 1th intl. conf. on knowledge discovery and data mining  1.
 cheeseman et al.  1  p. cheeseman  j. kelly  m. self  j. stutz  w. taylor  and d. freeman. autoclass: a bayesian classification system. in proc. 1th intl. conf. on machine learning  1.
 dempster et al.  1  a. dempster  n. laird  and d. rubin. maximum likelihood from incomplete data via the em algorithm. j. royal stat. soc.  ser. b  1 :1  1.
 fink et al.  1  j. fink  a. kobsa  and a. nill. user-oriented adaptivity and adaptability in the avanti project. in designing for the web: empirical studies  microsoft usability group  1.
 fu et al.  1  x. fu  j. budzik  and k. j. hammond. mining navigation history for recommendation. in proc. 1 conf. on intelligent user interfaces  1.
 joachims et al.  1  t. joachims  d. freitag  and t. mitchell. webwatcher: a tour guide for the world wide web. in proc. 1th intl. joint conf. on art. int.  1.
 juhne： et al.  1  j. juhne  a. t. jensen  and k. gr nb k. ari-： adne: a java-based guided tour system for the world wide web. in proc. 1th intl. www conf.  1.
 lieberman  1  h. lieberman. letizia: an agent that assists web browsing. in proc. 1th intl. joint conf. on art. int.  1.
 pazzani and billsus  1  m. j. pazzani and d. billsus. adaptive web site agents. in proc. 1rd intl. conf. on autonomous agents  1.
 perkowitz and etzioni  1  m. perkowitz and o. etzioni. adaptive web sites: an ai challenge. in proc. 1th intl. joint conf. on art. int.  1.
 perkowitz and etzioni  1  m. perkowitz and o. etzioni. towards adaptive web sites: conceptual framework and case study. art. int. j.  1-1   1.
 perkowitz  1  m. perkowitz. adaptive web sites: cluster mining and conceptual clustering for index page synthesis. phd thesis  dept. of comp. sci. and eng.  univ. of washington  1.
 sarukkai  1  r. r. sarukkai. link prediction and path analysis using markov chains. in proc. 1th intl. www conf.  1.
 wexelblat and maes  1  a. wexelblat and p. maes. footprints: history-rich tools for information foraging. in proc. acm chi 1 conf. on human factors in comp. sys.  1.
 yan et al.  1  t. yan  m. jacobsen  h. garcia-molina  and u. dayal. from user access patters to dynamic hypertext linking. in proc. 1th intl. www conf.  1.
 zukerman et al.  1  i. zukerman  d. albrecht  a. nicholson  and k. doktor. trading off granularity against complexity in predictive models for complex domains. in proc. 1th intl. pacific rim conf. on art. int.  1.

　　using text classifiers for numerical classification sofus a. macskassy  haym hirsh  arunava banerjee  aynur a. dayanik
{sofmac arunava aynur hirsh} cs.rutgers.edu
department of computer science
rutgers university
1 frelinghuysen rd
piscataway  nj 1

abstract
considerasupervisedlearningprobleminwhichexamplescontain bothnumerical-andtext-valuedfeatures.tousetraditionalfeaturevector-basedlearningmethods onecouldtreatthepresenceorabsenceofawordasabooleanfeatureandusethesebinary-valued featurestogetherwiththenumericalfeatures.however theuseofa text-classificationsystemonthisisabitmoreproblematic-inthe moststraight-forwardapproacheachnumberwouldbeconsidereda distincttokenandtreatedasaword.thispaperpresentsanalternativeapproachfortheuseoftextclassificationmethodsforsupervisedlearningproblemswithnumerical-valuedfeaturesinwhichthe numericalfeaturesareconvertedintobag-of-wordsfeatures thereby makingthemdirectlyusablebytextclassificationmethods. we showthatevenonpurelynumerical-valueddatatheresultsoftextclassificationonthederivedtext-likerepresentationoutperformsthe morenaivenumbers-as-tokensrepresentationand moreimportantly  iscompetitivewithmaturenumericalclassificationmethodssuchas c1andripper.
1	introduction
the machine learning community has spent many years developing robust classifier-learning methods  with c1 quinlan  1  and ripper  cohen  1  two popular examples of such methods. although for many years the focus has been on numericaland discrete-valuedclassification tasks  over the last decade there has also been considerable attention to textclassification problems sebastiani  1; yang  1 . typically such methods are applied by treating the presence or absence ofeach wordasa separate booleanfeature. thisiscommonlyperformedeither directly  by generatinga largenumber of such features  one for each word  or indirectly  by the use of set-valued features  cohen  1   in which each text-valued field of the examplesis viewed as a single feature whose value for an example is the set of words that are present in that field for this example.
　the information retrieval community has similarly spent many years developing robust retrieval methods applicable to many retrieval tasks concerning text-containing documents  with vector-space methods  salton  1  being the best known examples of techniques in this area. although for many years the focus has been primarily on retrieval tasks  here  too  the last decade has seen a significant increase in interest in the use of such methods for text-classification tasks. the most common techniques use the retrieval engine as the basis for a distance metric between examples  for either direct use with nearest-neighbor methods  yang and chute  1   or  based on the closely related rocchio  relevance feedback technique  for use after creating a summary  document  for each class and retrieving the nearest one  schapire et al.  1 .
　theironyis thatalthoughwe nowhavemuchexperienceon placing text-clasification problems in the realm of numericalclassification methods  little attention has been brought to the question of whether numerical-classification problems can be effectively brought into the realm of text-classification methods. since the text-retrieval methods on which they are based have manydecade'smaturity  if doneeffectivelythey have the potential of broadening further our base of methods for numerical classification.
　more importantly for us  however  is the fact that many real-world problems involve a combination of both text- and numerical-valued features. for example  we came to ask these questions by confronting the problem of email classification  where we wanted to explore instance-representations that considered not only the text of each message  but also the length of the message or the time of day at which it is received  macskassy et al.  1 . although the machinelearning-derived methods that we now know how to apply to pure text-classification problems could be directly applied to these  mixed-mode  problems  the application of information-retrieval-based classification methods was more problematic. the most straight-forward approach is to treat each number that a feature may take on as a distinct  word   and proceed with the use of a text-classification method using the combinationof true words and tokens-for-numberswords. the problemis that this makesthe numbers1 and 1 as dissimilar as the numbers1 and 1 - all three values are unrelated tokens to the classification method. what we would like is an approach to applying text-classification methods to problems with numerical-valued features so that the distance between such numerical values is able to be discerned by the classification method.
　this paper presents one way to do just this  converting numerical features into features to which information-retrievalbased text-classification methods can apply. our approach presumes the use of text-classification methods that treat a piece of text as a  bag of words   representing a text object by an unorderedset of tokens presentin the text  most commonly words  butoccasionallytokensof a morecomplexderivation .
　the core of our approach is  roughly  to convert each number into a bag of tokens such that two numbers that are close together have more tokens in common in their respective bags than would two numbers that are farther apart. the high-level idea is to create a set of landmark values for each numerical feature  and assign two tokens to each such landmark. every value of a feature for an example will be compared to each landmark for that feature. if the example's value is less than or equal to the landmark  the first of that landmark's two tokens is placed in that example's  bag . if the value is more than the landmark the second token is instead used in the bag. the result is that every feature gets converted into a bag of tokens  with each bag containing the same number of entries  each differing only to reflect on which side of each landmark a value lies.1
　the key question then becomes how to pick landmark values. although our experiments show that even fairly naive approaches for selecting landmarks can perform quite well  we instead appeal to the body of work on feature discretization that has already been well-studied within machine learning  catlett  1; kerber  1; fayyad and irani  1; dougherty et al.  1; kohavi and sahami  1; frank and witten  1 . learning methods such as c1 would normallyhaveto considera largenumberofpossibletests oneach numericalfeature  in the worst case one between each consecutive pair of values that a feature takes on. these discretization methods instead use a variety of heuristic means to identify a more modest - and hence more tractable - subset of tests to consider during the learning process. our approach is thusto applysuch methodsto identifya set oflandmarkvalues for each numerical feature and create two possible tokens for each landmark value  exactly one of which is assigned to each example for each landmark. the result is a  bag of words  representation for each example that can then be used by textclassification methods.
　in theremainderof thispaperwe first describeourapproach for converting numerical features into bag-of-words features in more detail  including the landmark-selection methods that we used. we then describe our experimental evaluation of ourapproach: thelearningmethods evaluationmethodsused  and our results - which show that the text-classification methods using our bag-of-wordsrepresentation performcompetitively with the well-used methods c1 and ripper when applied to the original numerical data. we conclude the paper with some analysis of these results and some final remarks.
1	converting numbers to bags of tokens
the approach taken in this paper is to convert every number into a set of tokens such that if two values are close  these sets will be similar  and if the values are further apart the sets will be less similar. this is done for each feature by finding a set of  landmark values  or  split-points  within the feature's range of legitimate values by analyzing the values that the feature is observed to take on among the training examples. given an example  its numerical value for a given feature is compared to each split-point for that feature generated by our approach  and for each such comparison a token will be added  representing either that the value is less than or equal to the particular split-point or greater than that split-point. this will result in exactly one token being added per split-point.
　for example  consider a news-story classification task that includes a numerical feature representing the story's length. we can artificially invent a set of split-points to demonstrate our process  such as 1  1  and 1. for each splitpoint we define two tokens  one for either side of the splitpoint a value may lie. using the above split-points for the length of a news-story would result in the tokens  lengthunder1    lengthover1    lengthunder1    lengthover1    lengthunder1   and  lengthover1 . a
	split1	split1	split1	split1

figure 1: an example feature range and construction of bins. dots represent values and rectangles represent bins.
new message of length 1 would thereby have its length feature converted into the set of tokens  lengthover1    lengthover1   and  lengthunder1 . these would be added to the bag-of-words representation of the example - whether the other words in the bag were created from other numerical features  or the result of pre-existing text-valued features. more abstractly  consider the hypothetical numerical feature plotted along a number line in figure 1. if a training example is obtained whose value for this feature falls in bin1  the set {morethansplit1  lessthansplit1  lessthansplit1  lessthansplit1} would be the bag-of-wordsrepresentation created for this value. note that more than one value can be given the same representation  as long as they all fall between the same two consecutive split-points.
　the key question  of course  is how these split-points are selected. we use two methods in our main results. the first  called themdl method  uses an existing entropy-based method for discretization to find good split-points fayyad and irani  1 . this method is very similar to one that uses c1 on the training data  restricting it to ignore all but the single feature for which split-points are being selected  harvesting the decision points found within each of the internal nodes of the tree  kohavi and sahami  1 . the second method  which we call the density method  selects split-points that yield equal-density bins - where the number of values that fall between consecutive split-points stays roughly constant. the number of bins is selected using hill-climbing on error rate using a hold-out set. in the rest of this section we describe these two methods in more detail  concluding with a discussion of howwe handleexamplesthat are missing values for one or more of their features.
1	the mdl method
the mdl method  fayyad and irani  1; kohavi and sahami  1  makes use of information-theoretic techniques to analyse the values of a numeric feature and create splitpoints that have high information gain. it does so in a recursive fashion  finding one split-point in the overall set of values  then finding another split-point in each of the two created subsets  until a stopping criteria based on minimum description lengthpriciples rissanen  1 is met. each numerical feature is treated separately  yielding a different set of splitpoints for each feature. due to space limitations we refer the reader to the given citations for further details about this popular discretization algorithm.
1	the density method
the density method  described in algorithmic form in figure 1  begins by obtaining and sorting all values observed in the data for a feature f  yielding an ordered list sf. thus  for example  the result for some feature f might be sf = h1 1 1 1 1i. given some desired number of splits k  the density method splits the feature set sf into k+1 sets of equal size  except when rounding effects require being off by one  such that split-point sj is the -th point in
sf. using this split-point  two tokens are generated; one for when a numerical value is less than or equal to the split-point  and another for when the numerical value is greater than the

inputs: sets of values for each numeric feature.
 /* assume errors run from 1   1 */
maxsplitswhile numsplits   maxsplits○ argmaxf（numerical features  do  |sf|  for each numerical feature f
nf ○ min numsplits |sf|  /* create nf split-points for feature f. */
use as split-points for f the j-th element of sf
for with i running from 1 to nf
end
divide data into 1% for train and 1% for test. c ○ run learner on train with current split-points.
currerror ○ evaluate c on test.
if currerror = 1  do
maxsplits
else if lasterror   currerror  do
 numsplits ○ numsplits/1 maxsplits ○ 1 else
numsplits ○ 1 〜 numsplits lasterror ○ currerror
end
end
outputs: lasterror

　　figure 1: density algorithm for finding split-points. split-point. k - the final number of split-points - is found using a global hill-climbing search with the number of splitpoints growing geometrically by a factor of two until the observed error rate on a 1% hold-outset is observed to increase or reach 1.
　one final detail of the density method is that the algorithm in figure 1 is actually run twice. the first time is based on a  volumetric  density calculation  where duplicate values are included in the analysis. after doing this the algorithm is run a second time after duplicate values have been removed  yielding a second set of split points that have an  approximately  equal number of distinct values between each split-point. thus  for example  for the feature f this second run would now use the list sf = h1 1 1h. whichever method yielded a lower error rate gives the final set of split points. if they have the same performance  whichever had fewer split-points is selected.
1	missing values
a common occurrence for many learning problems is when some examples are missing values for some of the features. this can be a complicating factor both during the learning phase  when assessing the importance of features in forming some learning result  and in classification  when making a decision when values of some of the attributes are unavailable. common approaches range from simply deleting data or features to remove such occurrences  to imputing some value for the feature - such as through learning or through something as simple as using the median  mean  or mode value in the training data  to more complex methods such as are used in learning algorithms such as c1. our approach for creating bag-of-wordfeaturesoutofnumericalfeaturescontributesanother interesting way to handle missing values. the idea is  quite simply  to add no tokens for a feature of an example when the value for this feature is missing. by not committing to any of the tokens that this feature mightotherwise have added it neither contributes nor detracts from the classification process  allowing it to rely on the remaining features in assigning a label to the example. 1 learning algorithms
in this section we briefly describe the learning algorithms that we use in our experiments. recall that our goal is to demonstrate that  using our approach  text-classification methods can perform as credibly on numerical classification problems as more traditional methods that were explicitly crafted for such problems. to show this we use a sampling of four different approaches for text classification. our first is based on the rocchio-based vector-space method for text retrieval mentioned earlier  which we label tfidf  joachims  1; schapireet al.  1;sebastiani  1 . we also consider two probabilistic classification methods that have become popular for text classification  naive bayes domingos and pazzani  1; joachims  1; mitchell  1  and maximum entropy  nigam et al.  1 . finally  we also use the ripper rule learning system  cohen  1; 1   using its capability of handling set-valued features so as to handle text classification problems in a fairly direct fashion.  the first three methods used the implementations available as part of the rainbow text-classification system mccallum  1 .  the baselines to which we compare the text-classification methods are two popular  off the shelf  learning methods  c1  release 1   quinlan  1  and ripper. note that ripper has been mentioned twice  once as a text-classification method using ourtransformedfeaturesencodedforripperasset-valuedfeatures  and the other using the originalnumericalfeaturesin the same fashion as they are used with c1. thus ripper is in the unique position of being both a text-classification method when used with one representation  and as a numerical classification method when used with the other. missing values were handled for the text-classification methods as discussed earlier  by simply not generating any tokens for a feature of an example when it had no given value  and for c1 and ripper  when used with numericalfeatures  by their built-in techniques for handling missing values.
　the tfidf classifier is based on the relevance feedback algorithm by rocchio  using the vector space retrieval model. thisalgorithmrepresentsdocumentsasvectorssothat documents with similar content have similar vectors. each component of such a vector corresponds to a term in the document  typically a word. the weight of each component is computed using the tfidf weighting scheme  which tries to reward words that occur many times but in few documents. in the learning phase  a prototype vector is formed for each class from the positive and negativeexamplesof that class. to classify a new document d  the cosines of the prototype vectors with the corresponding document vector are calculated for each class. d is assigned to the class with which its document vector has the highest cosine.
　naive bayes is a probabilistic approach to inductive learning. it estimates the a posteriori probability that an example belongs to a class given the observed feature values of the example  assuming independenceof the features. the class with the maximum a posteriori probability is assigned to the example. the naive bayes classifier used here is specifically designed for text classification problems.
　the maximum entropy classifier  labeled maxent in our results  estimates the conditionaldistributionof the class label given a document  which is a set of word-count features. the high-level idea of this technique is  roughly  that uniform distributionsshouldbepreferedin the absenceof externalknowledge. a set of constraints for the model are derived from the labeled training data  which are expected values of the features. these constraints characterize the class-specific expectations for the model distribution and may lead to minimal non-uniform distributions. the solution to the maximum entropy formulation is found by the improved iterative scaling algorithm  nigam et al.  1 .
　ripper is a learning method that forms sets of rules  where each rule tests a conjunction of conditions on feature values. rules are returned as an ordered list  and the first successful rule provides the prediction for the class label of a new example. importantly  ripper allows attributes that take on sets as values  in addition to numeric and nominal features  and a condition can test whether a particular item is part of the value that the attribute takes on for a given example. this was designed to make ripper particularly convenient to use on text data  where rather than listing each word as a separate feature  a single set-valued feature that contains all of an instance's words is used instead. rules are formed in a greedy fashion  with each rule being built by adding conditions one at a time  using an information-theoretic metric that rewards tests thatcause a ruleto excludeadditionalnegativedata while still hopefully covering many positive examples. new rules are formed until a sufficient amount of the data has been covered. a final pruning stage adjusts the rule set in light of the resulting performance of the full set of rules on the data.
　c1 is a widely used decision tree learning algorithm. it uses a fixed sets of attributes  and creates a decision tree to classify an instance into a fixed set of class-labels. at every step  if the remaining instances are all of the same class  it predicts that class  otherwise  it chooses the attribute with the highest information gain and creates a decision based on that attribute to split the training set into one subset per discrete value of the feature  or two subsets based on a thresholdcomparison for continuous features. it recursively does this until all nodes are final  or a certain user-specified threshold is met. once the decision tree is built  c1 prunes the tree to avoid overfitting  again based on a user-specified setting.
1	evaluation methodology
tocompareourtext-likeencodingofnumberswhenusedwith text-classificationsystemsto theuse ofc1andripperon the original numerical features we used 1 data sets taken from the uci repository blake andmerz  1 . table 1 shows the characteristics of these datasets. the first 1 represent problems where all the features are numeric. the final 1 represent problems in which the designated number of features are numeric and the rest are discrete or binary-valued.
　the accuracy of a learner was done through ten-fold stratified cross-validation kohavi  1 . each dataset was represented in one of four ways for our experiments:   the original feature encoding - using numbers - for use with c1 and ripper.
  the bag-of-words encoding generated by the density method  for use with the four text-classifications.
  the bag-of-words encoding generated by the mdl method  for use with the four text-classifications.
  the bag-of-words encoding generated using the tokens-for-numbers approach  for use with the five textclassifications. this was accomplished by converting every number into its english words - for example   1  becomes  five  and  1  becomes  twopointthree .
the first of these represents our baseline  using a machine learning method designed for numerical classification. the
#of #of #of #of basedatasetinstancesfeaturesnumericclassesaccuracy
features % bcancerw111diabetes111glass111hungarian111ionosphere111iris111liver111musk1 11new-thyroid111page-blocks111segmentation111sonar111vehicle111wine111arrhythmia1 11autos111cleveland111credit-app111cylinder-bands 111echocardiogram111horse	111post-operative	111sponge	111 themdlapproachwasunabletofindanysplit-pointsforthis data-set soitisomittedinanycomparisonsonthemdlmethod.

           table 1: properties of all datasets. next two are the new approaches presented in this paper. the final one is the representation that simply treats each number as a distinct  word  without regard to its value.
1	results
the first question we ask is the key one: to what extent does our approach yield a competitive learning method on numerical classification problems  figure 1 shows the results of comparing the four text-learning methods using our mdlalgorithm features to the two numerical classification methods. each point represents a single data set  where the x-axis is the accuracy of either c1 or ripper and the y-axis is the accuracy of one of the four text methods. points above the y=x line represent cases where the numerical-classification method was inferior to our use of a text-classification method  and points below the line are cases where the numerical method was superior. the qualitative flavor of this graph is that the mdl-algorithm features allows text-classification methodsto performcrediblyin many cases  exceedingnumerical methodsin some cases  althoughperformingless successfullyinmanycasesaswell. we plotin figure1a similar graph comparing the four text methods using the density-algorithm features to the two numerical methods.  the  outlier  cases at the bottom of the graphs are for the post-operative data set  which has only 1 examples and only 1 of its 1 features being numeric. 
　since the preceding graphs collapse eight different comparisons  four text methods versus two numerical methods 
tfidfnbmaxentrippermdl/c11/1/11/1/1mdl/ripper1/1/11/1/1density/c11/1/11/1/1density/ripper1/1/11/1/1table 1: comparing the number of wins/losses/ties for each featurization  mdl first two rows  density second two rows  when coupled with one of the four text-classification methods labeling the columns  versus a numerical method.
compare mdl vs. numeric methods

figure 1: comparing learning with mdl-generated features to numeric learning.
into a picture table 1 also shows for how many data sets each text method beat a numerical method. each entry in the table is the number of wins/losses/ties for the new featurization methodused with a textmethodcomparedto a numericalclassification method. the columns label the text method used. the first two rows are results when the mdl method is used  the next two are for the density method  in each case the first comparison is to numerical classification with c1 comes in the first of the two rows  followed by ripper. these results show that in a non-trivial number of cases the use of our approach for converting numerical features into text-based data beats out the popular learning methods  with absolutle improvements in accuracy ranging as high as 1%. while these results do not show that the approach is unconditionally superior to numerical classification  they do show that the approach does merit consideration for use as a numerical classification method.
　the next questionwe ask is whether the use of two different featurization methods is necessary: does either dominate the other  figure /reffig:mdlvsdens shows the results of such a comparison  where each point represents a single data set and a text learning method  with the x-axis representing the result of using the mdl method with that learning algorithm  and the y-axis representing the result of using the density method with that learning algorithm. here  too  the results show that neither method is clearly superior  with perhaps a bit better performance in general by the mdl method  but with some cases still going to the density method.
1	additional analysis
we began the paper stating that the obvious approach to converting numbers into text-based feature is convert each number into a unique token  to be added as-is to the set of words compare density vs. numeric methods maxent-c1 maxent-ripper naivebayes-c1 naivebayes-ripper ripper-c1 ripper-ripper tfidf-c1 tfidf-ripper
figure1: comparinglearningwith density-algorithmfeatures to numeric learning.
compare mdl and density approaches

figure 1: comparing learning with mdl-algorithm features to density-algorithm features.
for a given example. one additional question we can ask is whether the complexity of our methods are necessary: perhaps this simple tokenization approach performs as effectively  figure 1 shows an analysis of this question. each point is a data set  with the x-axis value representing the accuracy of the tokenization approach with a particular textclassification method  and the y-axis represents the accuracy with one of our two featurization methods using the same learning method. as is clearly evident from the figure  the tokenization approach is not as effective in general as our more sophisticated approach.
　we conclude this section by noting that kohavi and sahami  discuss a different discretization method that is very similar to the mdl method. this method simply runsc1 on the data  ignoring all features except the one for which splitpoints are being created. kohavi and sahami show that this method is slightly inferior to the mdl approach. however  just because it is inferior for discretization for decision-tree learning does not imply that it must be the case here  too. to test this we compared the four text classification methods using the mdl method to the c1 method. figure 1 shows the results of this experiment. each pointrepresentsa data set and a learning method. the x-axis represents the accuracy of the c1 approach  and the y-axis represents the accuracy of the mdl approach. as is clear  although there is some difference between the performance of the methods  they are somewhat similar in behavior.
1	final remarks
this paper has described an approach for converting numeric features into a representation enabling the application of text-classification methods to problems that have traditionally been solved solely using numerical-classification methcompare tokenization vs. bag of tokens.
maxent-density maxent-mdl
1 naivebayes-density naivebayes-mdl ripper-density ripper-mdl tfidf-density tfidf-mdl
figure1: comparingthe text-learningapproachesto the naive tokenization approach.
compare c1-split and mdl approaches.

figure 1: comparing the mdl and c1 approaches.
ods. in additionto openingup the use of text-methodsto problems that involve  mixed-mode  data - both numerical- and text-valued features - it yields a new approach to numericalclassification method in its own right. our experiments show thatin a non-trivialnumberofcases the resultingmethodsoutperform highly optimized numerical-classification methods. also importantly  our experiments show that our approach yields a vast improvement over the naive method of converting a numeric into its equivalent textual token.
　there are many directions in which we are now taking this work. our original motivation for performing this work was to broaden the class of learning methods that can be applied to mixed-mode data. now that we have done so  we can return to some of the work that motivated this  performing additional evaluations of this work on mixed-mode data. doing so  however requiresa set ofbenchmarkproblems something thatdoesnotpresentlyexist. we are thereforeinthe processof creating such data sets so we can perform this evaluation process. we also notedthatour approachyieldsan intriguingway to deal with data with missing values  and understanding its benefits and liabilities compared to other approaches remains a question that we hope to explore. finally  as is usually the case when comparing any two learning methods that are successful in competing cases  it is difficult to make any definitive statements about when they each may be successful. various conjectures include differences in the amount of missing values in the differentdata sets  the number of numeric versus non-numericfeatures  etc. for the given data sets we were unable to discern any such pattern in our results. this remainsan important question that we also plan to study.
acknowledgments
we would like to thank foster provost  lyle ungar  and members of the rutgers machine learning research group for helpful comments and discussions.
references
 blakeandmerz 1 c.l.blakeandc.j.merz.ucirepository ofmachinelearningdatabases 1.
 catlett 1 j.catlett. onchangingcontinuousattributesinto ordereddiscreteattributes.iny.kodratoff editor  proceedings of the european working session on learning pages1. berling:springer-verlag 1.
 cohen 1 w.w.cohen. fasteffectiveruleinduction. in proceedings of the twelfth international conference on machine learning laketahoe california 1.
 cohen 1 w.w.cohen. learningtreesandruleswithset- valuedfeatures.in aaai1.
 domingosandpazzani 1 p.domingosandm.pazzani. beyondindependence: conditionsfortheoptimalityofsimple bayesianclassifier.inproceedings of the 1th international conference on machine learning pages1 1.
 dougherty et al. 1 k.dougherty r.kohavi andm.sahami. supervisedandunsuperviseddiscretizationofcontinuousfeatures.in proceedings of the 1th international conference on machine learning pages1.morgankaufmann 1.
 fayyadandirani 1 u.m.fayyadandk.b.irani. multiintervaldiscretizationofcontinuous-valuedattributesforclassificationlearning.inproceedings of the 1th international joint conference on ai pages1.morgankaufmann 1.
 frankandwitten 1 e.frankandi.h.witten.makingbetter useofglobaldiscretization.in proceedings of the 1th international conference on machine learning slovenia 1.
 joachims 1 t.joachims.aprobabilisticanalysisoftherocchioalgorithmwithtfidffortextcategorization.inproceedings of the fourteenth international conference on machine learning  1.
 kerber 1 r.kerber.discretizationofnumericattributes.in proceedings of the 1th national conference on artificial intelligence pages1 menlopark ca 1.aaaipress/mit
press.
 kohaviandsahami 1 r.kohaviandm.sahami.error-based andentropy-baseddiscretizationofcontinuousfeatures.in proceedings of the second international conference on knowledge discovery and data mining pages1 menlopark ca  1.aaaipress/mitpress.
 kohavi 1 r.kohavi. astudyofcross-validationandbootstrapforaccuracyestimationandmodelselection. in proceedings of the 1th international joint conference on artificial intelligence pages1 sanfrancisco ca 1.morgan kaufmann.
 macskassy et al. 1 s.a.macskassy  a.a.dayanik and h.hirsh. emailvalet:learninguserpreferencesforwireless email. in proceedings of learning about users workshop  ijcai'1 stockholm sweden 1.
 mccallum 1 a.k.mccallum. bow:atoolkitforstatisticallanguagemodeling textretrieval classificationandclustering.http://www.cs.cmu.edu/゛mccallum/bow 1.
 mitchell 1 t.mitchell. machine learning. mcgrawhill  1.
 nigam et al. 1 k.nigam j.lafferty anda.mccallum.usingmaximumentropyfortextclassification. inproceedings of machine learning for informationfilteringworkshop  ijcai'1  stockholm sweden 1.
 quinlan 1 j.r.quinlan. c1: programs for machine learning.morgankaufmann sanmateo ca 1.
 rissanen 1 i.rissanen.minimumdescriptionlengthprinciple. encyclopedia of statistical sciences 1-1.
 rocchio 1 j.rocchio.relevancefeedbackininformationretrieval.insalton editor  the smart retrieval system: experiments inautomatic document processing chapter1 pages1- 1.prentice-hall 1.
 salton 1 g.salton.developmentsinautomatictextretrieval. science 1-1.
 schapire et al. 1 r.schapire y.singer anda.singal.boostingandrocchioappliedtotextfiltering.inproceedings of acm sigir pages1 1.
 sebastiani 1 f.sebastiani. machinelearninginautomated textcategorisation:asurvey.technicalreportiei-b1-1  istitutodielaborazionedell'informazione 1.
 yangandchute 1 y.yangandc.chute.anexample-based mappingmethodfortextclassificationandretrieval.acm transactions on information systems 1 :1 1.
 yang 1 y.yang.anevaluationofstatisticalapproachestotext categorization. information retrieval 1/1 :1 1.
faster association rules for multiple relations
siegfried nijssen and joost kok snijssen liacs.nl and joost liacs.nl
leiden institute of advanced computer science  leiden university
niels bohrweg 1  1ca leiden  the netherlandsabstract
several algorithms have already been implemented which combine association rules with first order logic formulas. although this resulted in several usable algorithms  little attention was payed until recently to the efficiency of these algorithms. in this paper we present some new ideas to turn one important intermediate step in the process of discovering such rules  i.e. the discovery of frequent item sets  more efficient. using an implementation that we coined farmer  we show that indeed a speed-up is obtained and that  using these ideas  the performance is much more comparable to original association rule algorithms.
1	introduction
the formalism of association rules was introduced by agrawal  for the purpose of basket analysis. an important step in the discovery of such rules is the construction of frequent item sets. these are  for instance  sets of items that are frequently bought together in one supermarket transaction. as this discovery step is time critical  it is obligatory that it is performed reasonably fast. much research has been done in order to develop efficient algorithms. a wellknown algorithm resulting from this research is apriori  of which many variants have been developed  such as aprioritid  agrawal et al.  1  and a breadth-first algorithm introduced by pijls and bioch .
　on the other hand  efforts have been done to extend the usability of association rules beyond the basic case of basket analysis. dehaspe and de raedt  use the notion of atom sets as a first order logic extension of item sets. the incorporation of techniques from inductive logic programming allows for more complex rules to be found which also take into account background knowledge. consequently  this also allows data mining of data which is spread over tables which can not reasonably be merged into one table. an algorithm was implemented based on this notion  which was called warmr. the usefulness of this algorithm was demonstrated in several real-world situations  see  for example   dehaspe et al.  1  . these experiments  however  also showed the major shortcoming of the algorithm: its efficiency proved to be very low  some experiments even taking several days.
　we propose to obtain a gain in efficiency by tackling two properties of the warmr algorithm:
while still using the first order logic notation  we remove the need for prolog; by using a more sophisticated datastructure borrowed from an implementation of apriori  our algorithm does not depend on a time consuming test for equivalence.
the algorithm that we introduce has some ressemblance with the algorithm that was developped in  blockeel et al.  1 . that algorithm however did not tackle one of the most time consuming steps of warmr: a test for equivalence under subsumption. our algorithm pays special attention to this step and offers an alternative solution. under some restrictions we will show that our algorithm is equivalent to warmr. experiments with our algorithm then show a considerable speed-up compared to warmr.
　the paper is organized as follows. in the second section we summarize the association rule algorithm on which our work is based. in the third section we discuss some important notions introduced in the warmr algorithm. the fourth section introduces our modifications  which are verified by giving results of experiments in the fifth section. the sixth section concludes.
1	breadth-first apriori
our algorithm is based on a variation of apriori that was introduced by pijls and bioch . the algorithm performs the same task as apriori. given a database which contains subsets of a set of items the algorithm discovers all frequent item sets  which are the subsets for which
exceeds a predefined threshold. an item set of size is called a item set. an important property of is:
 1 
as every subset of occurs in every transaction that contains . this property turns an efficient bottom-up levelwise search possible: it can apriori be determined that a -item set is not frequent if a -subset is infrequent.
　the breadth first-algorithm is such a bottom-up levelwise algorithm. it starts with candidates of size one  after which a process is repeated of counting candidate item sets and of using them to obtain candidate item sets. all these

	1	1	1
figure 1: a small example of a trie datastructure
steps are performed on a trie datastructure  of which figure 1 displays an example. every path from the root to a node corresponds to an item set; all leafs at the deepest level correspond to candidate item sets. paths which do not reach until the deepest level are maintained only when they correspond to frequent item sets and are displayed by dotted lines for the sake of clarity. the trie is used in the following fashion:
in the step of candidate counting  a tree traversal is performed for every transaction  as follows: if an item occurs in a transaction  all its children are checked recursively. if a leaf is reached  the support count of the corresponding item set is increased.
in the step of candidate generation  for every frequent item set new children are generated  consisting of all frequent right brothers. in the example is expanded by its frequent right brothers and . this copying mechanism in combination with the order of the items takes care of generating every item set at most once.
in both steps  this mechanism distinguishes itself from the original apriori algorithm. instead of building a new tree for each round  this procedure efficiently constructs a new set of candidates by merely copying nodes. furthermore  during the counting phase  it passes through the tree and checks for the existence of candidates in the current transaction. this in contrast to the original algorithm  where for a given subset in the transaction  a search in a hash node is performed to check whether there is a candidate to be counted. it will appear that these both characteristics make this variant of apriori suitable for our purposes.
1	warmr
as a first order extension of item sets  dehaspe and de raedt  use sets of atoms  which they also refer to as queries when nearly all variables are existentially quantified and the set is ordered. the free variables are bound by a special purpose key predicate. the relation of the key and the query is illustrated in the following horn clause:
 1 

	key	query
in this example the predicate can be thought of as a table which describes the products that clients are buying  while the predicate refers to a table containing properties of clients. in the sequel we will use abbreviations such as
for   for and for . this example shows how several tables can be combined more elegantly in a query than in an item set.
　the support of the query is formalized using the key and is defined to be the number of variable bindings for which the key predicate can be proved. in the given example the support of is the number of variable bindings of for which can be proved given the horn clause in formula  1  and a knowledge base defined in prolog.
　while for item sets the definition of the search space is straightforward  this is not the case for atom sets. apart from the choice of predicate  there are also many possibilities for the usage of variables in the query. to define the bias of the search space warmr uses a refinement operator based on mode declarations. every mode declaration prescribes the way in which a predicate can be added to a query. the following is an example of a mode:
 1 
it states that predicate may be added to a query when the first parameter is bound to an existing variable  the second parameter introduces a new variable and the last parameter is bound to the constant . the parameters are called mode constraints; here  we will call parameters and constant parameters input parameters and parameters output parameters. often an integer is associated with every mode to indicate how many times at most the mode may be applied in the same query.
　the usage of atoms instead of items turns it more difficult to create an efficient apriori-like algorithm: it is no longer reasonable to use the subset relation to express relations between atom sets. as replacement for the subset relation  and as approximation of logical implication  warmr uses subsumption. an atom set subsumes an atom set
	  denoted by	  if there is a substitution	such that
       . the -subsumption relation induces an equivalence relation   that is defined as follows: iff and
　　　. it can be shown that a property similar to formula  1  also holds for -subsumption on atom sets:
 1 
for a set of frequent queries of size  denoted by   and a set of infrequent queries of size  denoted by    warmr uses this algorithm to generate a new set of candidate queries
:
warmr-gen = ; for all do
	for all refinements	of	do
	add	to	unless:
 a  there is a	  or
 b  there is a	.
restriction  a  removes queries which are apriori determined to be infrequent. restriction  b  removes queries which have the same meaning as previously considered frequent queries or candidates. we will illustrate this on a small example. consider the following set of mode declarations:
this may lead to these two queries:
these clauses however have the same meaning and logically imply eachother.
　a major problems of warmr is that it heavily depends on a good implementation of subsumption. this is prohibitive as -subsumption is an np-complete problem  kietz and lu：bbe  1 .
1	farmer
we propose two modifications of warmr in order to make this algorithm more efficient. each of the following two subsections will discuss one of them.
1	knowledge base
when taking a closer look at the mode declarations  it can easily be seen that they can be mapped to procedures. as an example  consider the following facts:  
           . given mode   this mode could be associated with a procedure which returns for and returns for . furthermore  a mode could be associated with a procedure which for input returns . in farmer this idea is incorporated by binding all mode declarations to a procedure of one of these two types:
boolean procedures  which for an input vector return true or false; outputting procedures  which for an input vector return a set of output vectors. of course  this set may be empty and need not be computed entirely before all elements are used.
the first kind of procedures should be used in modes which do not have output parameters. the second kind is used in outputting modes.
　a data structure for a knowledge base of prolog facts is created and accessed by procedures  as follows: for every mode declaration a multidimensional matrix is allocated; every element in the matrix corresponds to a set of input values and contains a truth value or a list of output values. when a fact for a predicate is read  all corresponding mode matrices are updated accordingly. the advantage of this mechanism is that it takes a constant amount of time to determine the truth of an atom  especially in our current implementation which stores the matrices in core memory1. knowledge can however only be specified using facts or ad-hoc procedures.
1	search
the search which farmer performs differs in two aspects from the original warmr algorithm: it does not use -subsumption; it manipulates a trie datastructure to generate the queries which are defined by the bias.
the trie datastructure is a tree which contains all candidate queries as a path from the root to a leaf. an example of such a trie is given in figure 1. the tree is used for both counting and generating candidates.

figure 1: a trie in farmer
counting
we will describe the algorithm by giving pseudo-code. in this pseudo-code  we use the following notation: a capital refers to an atom in the tree; a capital refers to a set of variable assignments  which is a set containing a mapping from variables to values; is an interpretation function  which returns true if an atom can be proved using assignment . in this function  one of the aforementioned procedures is used; is an assignment function. the assignment set gives a value to some variables occuring in . for the remaining unbound variables  this function returns all sets of possible assignments. the aforementioned outputting procedure is used here.
for all values of the key variables  the tree is traversed recursively  as follows:
count a b 
if	then if	is a leaf then disable increase support
　return true else for all	do
for all	children do if	not disabled then
	count 	 	 
if d = true then disable return
return
initially all nodes are enabled. for a given node all values for the outputs are checked as long as there are children which have not been satisfied. a similar idea is also applied in  blockeel et al.  1 . we show the integration of the procedures and mode declarations here.
candidate generation algorithm
we will first introduce the mechanism of candidate generation by giving the algorithm. afterwards we will compare this generation mechanism with the -subsumption based method of warmr.
　the generation mechanism is based on the following idea: when an atom with an input variable is moved to the beginning of a query  that variable could become an output variable  hereby violating the mode declarations. however  for every atom in a query there is one first position at which it can occur without violation. all atoms that can be added to the end of a query can be subdivided in the following three classes:
1. atoms that could not have been added at an earlier position  as they use at least one new variable of the last atom in the query; we call these dependent atoms;
1. atoms that are a copy of the last atom in the query  except for the names of the output variables;
1. other atoms that could have been added at an earlier position.
examples of these classes are given by the superscripts in figure 1.
　during the construction of the tree this subdivision is used. given a trie and an ordered set of mode declarations  the trie is expanded as follows:
expand a  if	is internal then for all	children do
	expand  	 
else if	is frequent then add as child from left to right:
1. all dependent atoms of
1. all frequent right brothers of
1. a copy of	with new output variables  if allowed
else
remove
the tree in figure 1 is obtained using this mechanism when it is assumed that all queries of the following  typed  bias are frequent:
 1 
the superscripts also in this case denote the mechanism that was used to create a node.
　the first mechanism serves the purpose of introducing atoms which could not be added previously. the atoms are introduced in the same order as the corresponding mode declarations and a deterministic mechanism is used to go through all the input variables.
　the dependent atoms are brothers of eachother; the second mechanism takes care that all subsets are generated afterwards - if not infrequent. by keeping the children in order  every subset is generated only once  or  equivalently  only one permutation out of a set of dependent atoms is considered. if necessary  the second mechanism gives new names to output variables to make sure they remain outputs.
　the third mechanism is intentionally separated from the other two. generation of repeating nodes is not desirable in many situations and should in any case be bound to a maximum. in our settings  the bias should explicitely state whether duplication of an atom is allowed.
　atoms of the third kind do not fit very well in the distinction that was introduced. a repeating node could in any case be exchanged with its parent. it would however not be efficient to introduce a set of identical nodes to overcome this problem. later on  we will also see some additional disadvantages of these atoms.
candidate generation discussion
due to the absence of -subsumption  it can easily be seen that farmer does not prune as many queries as warmr does. in this section we will show which restrictions should be applied to the bias in order to make sure that farmer will generate the same output.
in warmr -subsumption is used for two purposes: to prune infrequent queries before counting; to remove queries which  mean the same  as other queries.
only the -subsumption relation that is used for the latter purpose will be considered here  as only this relation influences the set of queries that is found. infrequent queries will not occur in the results even if they are not pruned.
　the -subsumption equivalence relation is only one method for determining that queries mean the same. a less strict relation is the equality relation under substitution  which we will denote with here and is defined for two  unordered  sets of atoms as follows: iff there exist substitutions and such that and . the correspondence between these relations can be expressed using plotkin's reduced clauses.
definition 1 a clause	is called reduced iff	and imply	.  plotkin  1 
theorem 1 let	and	be reduced sets of atoms. then iff	.
proof  	 : this is clear as	and	.
 	 : as	 	  and as	 	.
let	. because	and	  also holds  by definition   and then	because is reduced. thus	. in the same way 	.
as	and	 	. as	 
       and finally . by combining and   is shown. this proves that
.
from this theorem it also follows that if atom sets are reduced  they can never be subsumption equivalent when they differ in length.
　we will show that for a restricted bias  farmer will always generate reduced atom sets. then we will show that farmer does not generate two different atom sets that are substitution equivalent. from this we conclude that  given a restricted bias  farmer will not generate queries that subsume eachother.
definition 1 a redundancy restricted bias should obey the following rules:
1. no functions may be used;
1. repetition of an atom by an attom which differs only in the name of the output variables is not allowed;
1. no two modes for the same predicate may exist for which the constraint parameters differ  unless the corresponding parameters are both constant parameters.
the second rule prevents queries such as	  from being generated.	the third rule disallows the biases	 	and	 
	b a a b    1	t b   c 1

	b a j b   1	s a b    b    1
figure 1: a partial order for a query
   and consequently queries and	 	  .query	remains possible.
theorem 1 for a redundancy restricted bias farmer will always generate reduced atom sets.
proof assume that is a query in the trie  obtained by using a redundancy restricted bias. we will show that for any subset   can never be true. in order for this  there must at least be two atoms and in which are mapped to the same atom in : . for any pair we will try to construct such a substitution. by definition of the bias  both atoms must have inputs at the same positions  restriction 1   while the input variables must be different  restriction 1 in combination with the tree building procedure  where such atoms could only be generated as brothers . construct a substitution which unifies and . this substitution will always map variables to variables  as no functions are allowed and no modes with constants and variables at the same parameters. apply to the whole query. consider the set of atoms that introduced the variables used in and   then there are two possibilities:
1. this set contains one atom which has two outputting parameters. by these are bound to the same variable. such an atom can never be generated according to the mode mechanism used by farmer;
1. this set has at least two different atoms. of both atoms an output is bound to the same variable by . in whatever order these atoms are placed  one of them has now an input at a position where an output occured. this would require another mode  which is not allowed in this restricted bias.
thus there can not exist redundant queries.
theorem 1 given a redundancy restricted bias  farmer will never generate two queries that are substitution equivalent.
proof we first remark that for ordered atom sets  such as queries  a deterministic variable numbering can be used. furthermore we note that two queries must be of equal size and that the substition can only map from variables to variables. thus  to determine whether two queries substitution equal eachother  it suffices to find a permutation of atoms  followed by a variable renumbering  that makes two queries equal. we will show that farmer generates one permutation.
　the restricted bias is such that for every atom in an  unordered  atom set  there is only one possible mode declaration. the usage of input and output parameters determines a partial order on the atoms  which can be depicted in a graph such as in figure 1 for the atom set
	and	the
bias
	. use this strategy to order the nodes in a query	:
order a  add	to the end of
:= nodes with incoming arrow from	and no incoming arrow from outside
	order	according to mode declarations and
a deterministic input variable numbering strategy
for all in order do order   
the order obtained by this strategy corresponds to the order of farmer: the set corresponds to the set of dependent nodes; the tree building mechanism which places new nodes before copied nodes takes care of the recursion by acting as a sort of lifo queue.
corollary 1 given a redundancy restricted bias farmer will never generate queries that -subsume eachother.
1	experimental results
we have compared farmer and warmr on two datasets. we should remark that in our experiments we used an implementation of warmr that did not yet use the tree datastructure discussed in  blockeel et al.  1 ; a comparison of both algorithms is therefore not completely fair. experiments in  blockeel et al.  1  revealed speed-ups of 1 for warmr in some situations.
bongard
the bongard dataset  bongard  1  contains descriptions of artificial images. the task is to discover patterns in the images. every image consists of several figures that can be included into eachother. no redundancy restricted bias can be used.
　in figure 1 the results of the experiments are depicted. figure 1 a  shows the number of queries that each algorithm finds. the number of farmer is higher in all cases  which is also expected for this bias. in figure 1 b  the execution times of the algorithms are compared1. paying attention to the fact that the scale is logarithmic  the speed-ups are considerable for this dataset.
frequent itemsets
in this experiment we compare the performance of farmer to a special purpose algorithm. as test case a binary digit dataset is used which contains 1 binary coded numbers. the special purpose algorithm which is used as comparison is the breadth-first implementation of apriori by  pijls and bioch  1 . farmer uses many of the mechanisms introduced in that algorithm and should perform comparable to that algorithm.
　in figure 1 the results of the experiments are depicted. a characteristic of the dataset is given in figure 1 a . the number of frequent itemsets appears to increase rapidly when the

1 1.1.1.1.1.1 minimum support	minimum support
 a   b 
figure 1: a comparison of farmer and warmr on the bongard dataset.  a  the number of queries in the output.  b  execution times in seconds. note that the scales are different.

 a 

 b   c 
figure 1: comparison of results for the digit dataset.  a  the number of frequent itemsets for each minimum support.  b  the execution times of the algorithms.  c  the execution times consumed for each itemset. note that the scale is logarithmic on both axis.
minimum support is beneath . in figure 1 b  the execution times of the algorithms are given. the time graph of farmer is comparable to that of apriori  although still exponentially larger. warmr has a completely different behaviour than both other algorithms and had such high execution times that no experiments were carried out for low supports.
　in figure 1 c  both previous graphs are combined and the execution time for each itemset is shown. it makes clear how the algorithms react when the amount of solutions they have to find increases. while the execution times of warmr increase  the times of the other algorithms decrease. although the overhead for each itemset is larger in farmer  which could be explained by the additional mechanisms that are hooked in  the difference is acceptable. the decreasing trend can be explained by the increasing number of overlapping evaluations when the number of itemsets increases.
1	conclusions and further work
we introduced an efficient algorithm for discovering queries. it uses a tree datastructure both to count queries as to generate queries. we showed that for a restricted type of bias  this algorithm is equivalent to a previous algorithm  warmr  and performs much better.
　although we believe that our restricted bias already adds considerable expressive power to propositional association rules  we are looking at some possibilities to overcome these restrictions. it appears that in case the second restriction is lifted  the range of possible rules already increases considerably. we are investigating the possibility of using the order of the tree in combination with a more sophisticated default order of queries.
　furthermore  we plan to perform more experiments. we successfully performed some experiments on a database with one million records  but more experiments are necessary to find out the behaviour of farmer on datasets of this size.
references
 agrawal et al.  1  rakesh agrawal  heikki mannila  ramakrishnan srikant  hannu toivonen  and a. inkeri verkamo. fast discovery of association rules. in advances in knowledge discovery and data mining.  pages 1- 1. aaai/mit press  1.
 blockeel et al.  1  h. blockeel  l. dehaspe  b. demoen  g. janssens  j. ramon  and h. vandecasteele. executing query packs in ilp proceedings of ilp1 - 1th international conference on inductive logic programming  1.
 bongard  1  m. bongard. pattern recognition. hayden book company  spartan books   1.
 dehaspe and de raedt  1  l. dehaspe and l. de raedt. mining association rules in multiple relations. in s. dz・eroski and n. lavrac・  editors  proceedings of the 1th international workshop on inductive logic programming  volume 1  pages 1. springer-verlag  1.
 dehaspe et al.  1  l. dehaspe  h. toivonen  and r. d. king. finding frequent substructures in chemical compounds. in r. agrawal  p. stolorz  and g. piatetskyshapiro  editors  1th international conference on knowledge discovery and data mining  pages 1. aaai
press.  1.
 kietz and lu：bbe  1  j-u. kietz and m. lu：bbe. an efficient subsumption algorithm for inductive logic programming. in s. wrobel  editor  proceedings of the 1th international workshop on inductive logic programming  volume 1  pages 1. gesellschaft fu：r mathematik und datenverarbeitung mbh  1.
 nijssen  1  s. nijssen. data mining using logic  master's thesis  leiden university  1.
 pijls and bioch  1  w. pijls and j. c. bioch. mining frequent itemsets in memory-resident databases. in e. postma  editor  proceedings eleventh belgium/netherlands artificial intelligence conference  pages 1  1.
 plotkin  1  g. d. plotkin. a note on inductive generalization. in b. meltzer and d. michie  editors  machine intelligence 1  pages 1  edinburgh  1. edinburgh university press.
a simple feature selection method for text classification  
 
pascal soucy and guy w. mineau 
dept. of computer science 
universit└ laval  qu└bec  canada  g1k 1 
{pascal.soucy  guy.mineau} ift.ulaval.ca  
 
abstract 
 
in text classification most techniques use bag-of-words to represent documents. the main problem is to identify what words are best suited to classify the documents in such a way as to discriminate between them. feature selection techniques are then needed to identify these words. the feature selection method presented in this paper is rather simple and computationally efficient. it combines a well known feature selection criterion  the information gain  and a new algorithm that selects and adds a feature to a bag-of-words if it does not occur too often with the features already in a small set composed of the best features selected so far for their high information gain. in brief  it tries to avoid considering features whose discrimination capability is sufficiently covered by already selected features  reducing in size the set of the features used to characterize the document set. this paper presents this feature selection method and its results  and how we have predetermined some of its parameters through experimentation. 
1   introduction 
in automatic text classification most techniques use bag-ofwords to represent documents. the main problem is then to identify what words are best suited to classify the documents in predefined classes. feature selection techniques are then needed to identify these words. 
　many feature selection techniques use some metric to determine the relevancy of a term with regard to a classification criterion. one such technique uses the information gain metric assessed over the set of all words encountered in all texts  and then chooses the best k words according to that metric  lewis and ringuette  1 . because of computational efficiency issues  it may not be possible to use the learning algorithm itself to determine which k is best suited to learn the descriptions of the classes of documents so that the accuracy rate of the algorithm is maximized. in that case  there is little guidance to determine what value k should be. some naive techniques just predetermine a k using the learning algorithms over a small set of data sets  and then uses that value of k directly on other data sets even though these data sets may be quite different from those used to learn the value of k. to our opinion  this k becomes rather arbitrary unless data sets can be characterized in such a way to be easily associated with previously tested data sets.  
　the feature selection method presented in this paper is rather simple and computationally efficient. efficiency is a major issue in text classification of large corpora  like found on the www  since many thousand features are usually involved. our method is a filter approach  hall and smith  
1   as the learning algorithm is not involved in the selection process of features and is self-adaptable to the nature of the corpus. it combines a feature selection method known to be among the best for text classification  yang and pedersen  1   that is  thresholding on information gain  and a new algorithm that selects and add features to a bag-ofwords according to their average coocurrence in the training set with the features already in a small set composed of the best features selected so far for their high information gain  
called the pool.  this paper presents this feature selection method and how we have predetermined some of its parameters  called thresholds  through experimentation. it also shows how these thresholds can be used as such in other domains and still render a good performance level in terms of the relevancy of the feature set extracted for subsequent classification purposes.  
1   experimental testbeds 
1   learning algorithms 
in order to assess the potential of our feature selection method  we used it with two different learning algorithms that are widely used in text classification and retrieval: knearest neighbors  knn  algorithms  with weighted and non weighted features according to different weighting functions  and rainbow  a naive bayes classification algorithm for text classification  mccallum  1; f┨rnkranz et al.  1 . however  because of lack of space  this paper only presents the results obtained with the non weighted features knn algorithm and rainbow  since all weighted knn algorithms performed more poorly than their non- 
task name dataset classes size1 train/test  balance webkbcourse webkb1 course/noncourse 1 1 reuterscornweat reuters-1 corn/wheat 1 1 ling-spam ling-spam1 spam/non-spam 1 1 wwwprisoner www1 relevant/irrelevant 1 1 wwwbeethoven www1 relevant/irrelevant 1 1 usenetnews1 usenet1 　　soc.history/ soc.religion.christians 1 1 table 1 : the data sets used in our experiments. each data set is a binary classification task between two classes. size is the total number of documents in the data set.  train/test balance is the number of documents in the training and testing sets respectively. note that the training sets have been voluntarily kept small for we believe that for most applications  few examples are available or if larger sets are available  manual labeling of the documents becomes prohibitive. weighted counter-part. appendice a briefly describes the weighting functions tested to try and improve the nonweighted version of our knn algorithm  without success. 
1   data sets 
for our experiments  we used data sets previously studied in other work  and our own data sets. the data sets used in our experiments are described in table 1. these data sets may be categorized in two types: those including documents written with a concise vocabulary and those more freely written. 
the former set includes data sets such as reuters  brief news  and webkb  web pages mainly retrieved from computer science university departments  while the latter set includes noisy data sets such as ling-spam  consisting of emails belonging to a mailing-list  including junk emails   prisoner  the tv show  and beethoven  the composer  
 web pages retrieved with altavista using a keyword search   and news  unmoderated usenet newsgroups on history and christianism . the difficulty associated with a classification task depends highly on the type of vocabulary used  scott and matwin  1 . words were stemmed using the porter algorithm for our knn algorithms  and using the --use-stemming option with rainbow.  
1   feature selection 
information gain  ig  in text classification measures the reduction in entropy  heterogeneity of a set  gained using a term presence or absence to determine class membership. it is often used in text classification in the bag-of-words approach  joachims  1; nigam et al.  1; sahami et al.  1 . this measure is the same used by id1  a decision tree learning algorithm  to select the best attribute at each step of the tree construction  quinlan  1 . 
1   the k best features using the information gain 
under this approach  a text classification task is considered using the first k words that rank highest on that metric. that number k  however  depends highly on the kind of corpus and task involved. under a bayesian classification approach  many noticed that a naive bayesian classifier works best with many features  but better if not every available feature is used. from various experiments it was noticed that the k which seemed to be an approximation of the average number of features leading to reasonable results with that algorithm varies from 1 features  craven et al.  1  to 1 features  sahami et al.  1  depending on the nature of the corpus.  
　these findings show a great variation in the size of the required vocabulary. if we consider that bayesian classifiers suffer when irrelevant features are kept as selected features  it is difficult to justify why a predefined number of features would allow to optimize the accuracy rate of the classifier. 
actually  in such data sets as reuters  there are some words that are so characteristic of some class that fewer features need to be selected than with a data set involving newsgroups or email messages. it seems obvious that the number of required features highly depends on the nature of the corpus and of the learning algorithm. therefore  how could we characterize the nature of the corpus in terms of the available features so that guidelines on how to select a proper k are easily inferred  this paper proposes one such way to determine k according to the nature of the terms composing the vocabulary with regard to their discrimination power relative to one another. 
1   using information gain with a threshold 
a simple feature selection method could look at the ig of each feature  taken individually  with regard to the classification task. for instance  there are methods that select any feature with an ig greater than some threshold  yang and perdersen  1 . this is fully relevant to bayesian classifiers. it is easy to see that features with low ig are useful to characterize only a handful of documents. therefore  their probability distribution over the whole set of documents can not be estimated correctly. bayesian classifiers rely on these probability estimates. 
similarly  infrequently used dimensions  attributes  in a 
knn approach introduce noise more than anything else. as a matter of fact  knn seems to produce better results when fewer features are involved  using the ig to keep the best features  joachims  1 . knn bias depends mostly on the kind of similarity function it uses  but it always uses the vector model  a weakness of this model being the fact that features are considered to be independent of each other. the function that seemed most successful  see appendix a  a non weighted cosine similarity function   assigns to each feature the same weight. given that  we should only consider features that are promising in terms of class characterization  as the presence of many irrelevant features diminish the influence of very good  relevant  features. choosing a fixed number of features may lead to consider irrelevant ones  as with the reuters data sets  if k is too high  or remove some good features  as with junk email classification  if k is too low. 
　however  even if we obtained good results on average using the knn and bayes classifiers  by looking at each data set individually we noticed that it would be possible to restrict the set of selected features even more. for instance  with the reuters data set  we saw that in most binary classification tasks  for example  classifying documents according to their relevance to  corn  or  wheat    only a few words  often under 1  were needed to obtain the best accuracy score. we observed that high ig features could represent classification alternatives  in which case one is useless   or could be complementary. therefore  we could assess the cooccurrence of highly ranked features and eliminate those that represent alternatives of already selected high ig features.  
　we conducted extensive experiments where we had the number of features ranging from 1 to 1 and ht e ig threshold ranging from 1 to 1. from all these experiments  we found that the best threshold to classify the data sets that we had was 1 in average. therefore  we propose to first identify the terms whose ig is higher or equal to 1  and then  select only those features that are not covered by the strong features already in the set  in terms of their discrimination capability   as explained below. in what follows  we prove that this further refinement of the term set improves both the accuracy of the classifier and the interpretability of the characterization so produced since the number of selected features is sensibly reduced. 
1   our feature selection algorithm  
　the algorithm of figure 1 proposes a refinement of the set of selected features  already offering an ig higher than some threshold  by removing those features that cooccur with very high ig features. cooccurrence is a kind of measure of dependency between features  which is a weakness of most statistical models used today to characterize a set of features. because of computational issues  dependencies between terms are usually ignored. we propose to identify a small subset of terms that we know are relevant for the classification task at hand  according to their individual ig   and to comp ute dependencies between any new term and these preselected terms. the cooccurrence metric that we consider 
a simple feature selection algorithm: mcooccurrence
   let v = set containing every word found in the corpus                 sorted by information gain  in decreasing order  
   let pool = {}  the pool  
   let d = set of documents 
   let igain thres = 1    let cooc thres = 1 
   let nbkeep = 1  
 
   pool 	 nbkeep first features from v 
 	 
   for each word w in v - pool 
      if ig w d    igain thres then 
           v = v - {w} 
      elsif μcooc w pool d    cooc thres    
then v = v - {w}     
      endif 
   endfor 
 
μcooc  w  pool d  :  
    let sum = 1 
    for each pi in pool  
          sum 	 sum +  p w | pi  in d  
    return  sum / nbkeep  
  figure 1 : a simple feature selection algorithm 
in the algorithm of figure 1  written μcooc w pool d   is based on a partial cooccurence probability of w with each word in pool  considering the whole training set. it is computed as the mean value of p w|pi  for all pi in pool  where p w|pi  is the conditional probability to find w in any document if it already contains pi.  
　let us take an example and assume that the word  corn  has an ig value of 1  meaning that it almost perfectly divides the training set. then let us suppose that  p starch|corn  = 1 and ig starch    ig corn . since  starch  always occurs when  corn  does and since its ig is lower than that of  corn    starch  is unlikely to bring new information  in terms of discrimination power  to the feature set  and could therefore be removed from it.  
　needless to say that the reality is not that perfect. removing a feature only when it is totally covered by some other feature  in terms of its discrimination capability  would remove only a few features  if any  which goes against our aggressive selection objective. some approximation of coverage between terms is thus required to effectively reduce the vocabulary to those features that are meaningful in terms of their overall discrimination power. our approach is to consider a mean of partial cooccurrence between any new term and the terms in p  the set of the nbkeep highest ranked features according to their ig  called the pool. of course  nbkeep and cooc thres  see the algorithm of figure 1  have been empirically determined using our own data sets: prisoner  beethoven and news  and tests have been conducted over previously studied data sets: reuters  ling-spam and webkb  in order to prevent over fitting the parameters on these data sets. due to a lack of space  we won't show here the results of this parameterization  but we concluded from these tests that the values used in the algorithm of figure 1  hold for all our data sets  and seem to hold for other data sets that we are currently experiencing with.  
1   results 
table 1 shows the results obtained with the different feature selection methods that we tested over each corpus  under our two learning algorithms: a non weighted features knn algorithm  first three columns  and rainbow  a well-used bayesian classifier  last five columns . for each method  we show k  the number of features selected and used thereafter by the learning algorithm  and the accuracy score  %  obtained subsequently.  
　the first column shows the k features selected when we set a threshold of 1 on the information gain score  ig  of each feature. again  this threshold was assessed empirically over all possible values of ig ranging from 1 to 1. from the resulting set  our μcooccurrence refinement algorithm was applied to produce a smaller set of features  and the results are shown in the second column. one can see that the number of features is almost always reduced  and if so  significantly and without impact on the accuracy rate  meaning that we could eliminate useless features in terms of the prediction capability of the learning algorithms used. this helps produce a characterization of the learned concepts with a smaller vocabulary  and therefore facilitates the interpretation of the results and reduces the computation cost to classify new documents. 
　then we wondered if this minimal number of features could have been reached through a fine tuning of the ig score that we used. we found that an ig of 1 would produce the same number of selected features  taken over all corpora   but that the accuracy rate would then go down  as shown in the third column. obviously  the selected features in this case are not the same as those of the previous experiment. furthermore  we see that the small fluctuation in the ig rate  1  has great impact on the individual corpora in terms of the number of selected features. considering our detailed search for a good ig rate  it is not surprising to see such a fluctuation and its negative impact on the accuracy rate of the classifiers. this tends to show that 1 is a better threshold than 1  as validated in a previous experiment  and that our feature selection method is therefore useful to reduce that set by removing useless  or not so useful  features. it also suggests  at least on this type of corpora  that our method is sound and useful to determine a relevant k which  without such a method  would be impossible to determine.  
　for the bayesian classifier  we observe the same phenomenon. the forth column gives the number of selected features for ig = 1 and the accuracy rate obtained after training rainbow with these features. for comparison purposes  we also trained rainbow with 1 features  fifth column  as is suggested in  craven et al.  1 . by comparing the forth and fifth columns we can see that the 1 feature sets performs better than what we obtain with ig = 1 except in cases where there are a few words with extreme discrimination capabilities like with the reuters and beethoven corpora. despite this small loss in accuracy  the feature set with ig = 1 is nevertheless significantly smaller than 1  which would undoubtedly help the interpretation of the learned concepts. 
　in the next experiment  sixth column   we applied our μcooccurrence refinement algorithm to reduce even further the size of the feature set. again we obtain a reduced set without any impact on the accuracy rate  facilitating even more the subsequent interpretation of the learned concepts. 
　however  by setting ig = 1  we obtain the same number of features  taken over all feature sets  and a slightly smaller accuracy rate  see the seventh column .  
　finally  as expected  an unbounded number of features has a negative impact on the accuracy rate of a bayesian classifier  as shown in the last column. though not illustrated in table 1  this phenomenon also holds for knn classifiers when the number of attributes is extremely high and the number of relevant attributes is much lower than the number of attributes  which is the case in text classification. therefore  there is a need to determine a proper k for training the classifier with the relevant attributes. 
1   complexity issues 
let n be the number of documents  let v be the size of the vocabulary  and let p be the size of the pool. by choosing to compute cooccurrence only between the elements of the vocabulary and the pool  we avoid having to compute it for all o v1  possibilities  when pairs of features are considered   producing a o vpn  complexity factor rather than o v1n  for assessing the cooccurrence of terms over all n documents. since computing the ig for all features in v is 
data set ig = 1 
knn μcooc knn ig = 1 
knn  ig = 1 bayes k=1 bayes μcooc bayes ig = 1 bayes all features bayes k % k % k % k % % k % k % k % webkbcourse 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 reuterscornwheat 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ling-spam 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 wwwprisoner 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 wwwbeethoven 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 usenetnews1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 micro-average 1 1 1 1 1 1 1 1 number of features 1 1 1 1 1 1 1 1  
table 1 : number of features  k  and accuracy score  %  for each data set under different feature selection and learning methods. the total number of features is the summation of the k column; the micro-average calculates the average of correctly classified instances over all data sets and is therefore not an average of the individual accuracy scores. o vn   and since ordering them by their ig is o v log v   for large corpora  where log1 v    n   the dominating complexity factor of our algorithm is the assessment of the cooccurrence of terms. consequently  our method works under a complexity factor of o vpn . provided that we can determine p independently of v and n  as was done in our experiments  the resulting complexity figure for our method then becomes o vn   which is the same complexity figure for initially assessing the ig of each feature in the vocabulary. consequently  our μcooccurrence feature selection algorithm has the same asymptotic complexity as a feature selection method that solely computes the ig of each feature in its feature set  with a hidden constant that has now doubled however. nevertheless  the method is computationally efficient. 
1   conclusion and future work 
we have presented in this paper a very simple feature selection method for text classification that refines a set of features previously selected according to an ig criterion. this refinement is based on the cooccurrence of new features with a predetermined subset of highly ranked features  called the pool  hoping to avoid to include features whose discrimination capability with regard to the task is  partially  covered by the features of the pool. we seek to improve the classification speed of the learner without reducing accuracy and reduce the set of attributes that will be used later on to support the interpretation of the learned concepts. this paper shows that we reached this objective for the corpora that we studied. the method that we propose in this paper certainly proved to be promising for certain types of text classification tasks  as demonstrated by our experiments. at first glance  we are tempted to conclude that μcooccurrence has the capability to remove the features that brings no further information to those already selected. of course further experiments will be conducted to assess the validity of that claim under various conditions. 
　additional investigation will also be conducted in order to understand why μcooccurrence seems to have more impact on  non weighted feature  knn algorithms than on bayesian classifiers. though we feel that the probabilities that ponder the involvement of attributes provides some explanation  we still can not understand why weighted features knn approaches failed to surpass the non weighted knn algorithm that we used  despite extensive experimentation using a wide range of distance functions.  domingos and pazzani  1  demonstrated why naive bayes learners perform well in spite of the  so-called  word independence assumption. therefore  we will focus our investigation towards the study of word dependence and its impact on the 
knn model.  
　further effort is also needed to extend the μcooccurrence algorithm so that it can handle multiple concept learning. our experiments have been conducted only over binary classification tasks.  
　another field of investigation will be to use multistrategic approaches to feature selection. for instance  we could assess the information gain of sequences of features. we could base our choice of feature sequences on the first few levels of decision trees built from our initial feature set  and increment the vocabulary accordingly. 
　finally  though we set the ig and cooccurrence thresholds at 1 and 1 through extensive experimentation  more study on the impact of these parameters on the classification task with regard to different types of corpora is required. 
appendix a 
the learning bias of the knn model is mainly related to its distance  similarity  function and to its neighbor weighting function to compute that distance. distance weighted knn adjusts the importance of each neighbor by weighting their contribution to the classification of a new instance. in this section  we consider the weights associated with the features themselves rather than with individual instances.  
　this section presents the weight and distance functions that have been studied in our search to determine which knn algorithm would be used in the assessment of our feature selection method. surprisingly  the simplest of these functions worked better that the others.  
let us define an instance x is as  
 a1 x   a1 x   a1 x   ...  an x   
where aj x  is the weight of the jth attribute  feature  on x. 
using non-weighted features 
if we consider that the weight of each feature is the same  then we have:  
a j   x   = 1 if the word is in document x a j  x  = 1 otherwise 
using weighted features 
the goal reached using weighted features is to stretch the neighborhood space so that irrelevant features are given less importance. there are several ways to do this feature weighting. here are a few examples that were tested:  
  the weight of a feature for a document is its ig if the word is present in the document  1 otherwise 
  the weight of a feature is its frequency in the document 
  the weight of a feature is its tfidf   term frequency/inverse document frequency  
 we also tried many variants of these methods  for example  squaring the ig so that features with high ig would get yet higher weights. under these various weight assignment policies  we experimented with two distance functions : 
euclidian distance function d  x1  x1   = ‘n  a j  x1    a j  x1  1
j =1
 
cosine similarity function  salton1   
n
‘
aj x1    aj x1 
cossim  x1  x1  =	j=1
	n	n
‘aj x1  ‘aj x1
	j=1	j=1
references 
 craven et al.  1  m. craven  d. dipasquo  d. freitag  a. mccallum  t. mitchell  k. nigam  and s. slattery. 
learning to extract symbolic knowledge from the world wide web. technical report. departement of computer science. carnegie mellon univ.. 1. 
 
 domingos and pazzani  1  p. domingos and m. pazzani. beyond independence: conditions of the optimality of the simple bayesian classifier. machine learning.  1  pp 1. 1. 
 
 f┨rnkranz et al.  1  j. f┨rnkranz  t. mitchell and e. 
riloff. a case study in using linguistic phrases for text 
categorization on the www. working notes of the 1 
aaai/icml workshop on learning for text categorization. 
 
 hall and smith  1  m. a. hall and l. a. smith. feature selection for machine learning: comparing a correlationbased filter approach to the wrapper. proceedings of the florida artificial intelligence symposium  flairs-1 . 
1. 
 
 joachims  1  t. joachims. text categorization with 
support vector machines: learning with many relevant features. proceedings of the european conference on machine learning. springer. 1. 
 
 lewis and ringuette  1  d. d. lewis and m. ringuette. a comparison of two learning algorithms for text categorization. third annual symposium on document analysis and information retrieval. pp 1. las vegas. 1.  
 
 mccallum  1  a. k. mccallum. bow: a toolkit for statistical language modeling  text retrieval  classification and clustering. http://www.cs.cmu.edu/~mccallum/bow. 
1.  
 
 nigam et al.  1  k. nigam  j. lafferty and a. mccallum. using maximum entropy for text classification. ijcai-1 workshop on machine learning for information filtering. 1 
 
 quinlan  1  j. r. quinlan. induction of decision trees. machine learning  1  pp 1. 1. 
 
 sahami et al.  1  m. sahami  s. dumais  d. heckerman  and e. horvitz. a bayesian approach to filtering junk e-mail.  aaai/icml workshop on learning for text categorization. wisconsin. 1. 
 
 salton  1  g. salton. automatic text processing: the transformation  analysis  and retrieval of information by computer. reading  ma: addison-wesley. 1. 
 
 scott and matwin  1  s. scott and s. matwin. feature engineering for text classification. proceedings of icml-1  
1th international conference on machine learning. pp. 1. morgan kaufmann publishers. 1. 
 
 yang and perdersen  1  y. yang and j. p. pedersen. a comparative study on feature selection in text categorization. proceedings of the fourteenth international conference on machine learning  icml'1 . 1. 
link analysis  eigenvectors and stability
	andrew y. ng	alice x. zheng	michael i. jordan
computer science division	computer science division	cs div. & dept. of statistics
	u.c. berkeley	u.c. berkeley	u.c. berkeley
	berkeley  ca 1	berkeley  ca 1	berkeley  ca 1abstract
the hits and the pagerank algorithms are eigenvector methods for identifying  authoritative  or  influential  articles  given hyperlink or citation information. that such algorithms should give consistent answers is surely a desideratum  and in this paper  we address the question of when they can be expected to give stable rankings under small perturbations to the hyperlink patterns. using tools from matrix perturbation theory and markov chain theory  we provide conditions under which these methods are stable  and give specific examples of instability when these conditions are violated. we also briefly describe a modification to hits that improves its stability.
1	introduction
recent years have seen growing interest in algorithms for identifying  authoritative  or  influential  articles from webpage hyperlink structures or from other citation data. in particular  the hits algorithm of kleinberg  and google's pagerank algorithm  brin and page  1  have attracted the attention of many researchers  see also  osareh  1  for earlier developments in the bibliometrics literature . both of these algorithms use eigenvector calculations to assign  authority  weights to articles  and while originally designed in the context of link analysis on the web  both algorithms can be readily applied to citation patterns in academic papers and other citation graphs.
　there are several aspects to the evaluation of a link analysis algorithm such as hits or pagerank. one aspect relates to the specific notion of  authoritativeness  embodied by an algorithm. thus specific users may have an understanding of what constitutes an authoritative web page or document in a given domain  and the output of hits or pagerank can be evaluated by such users. while useful  such analyses often have a rather subjective flavor. a more objective criterion- the focus of the current paper-concerns the stability of a link analysis algorithm. does an algorithm return similar results upon a small perturbation of the link structure or the document collection  we view stability as a desirable feature of a link analysis algorithm  above and beyond the particular notion of authoritativeness that the algorithm embodies. if an article is truly authoritative or influential  then surely the addition of a few links or a few citations should not make us change our minds about these sites or articles having been very influential. moreover  even in the context of a fixed link structure  a dynamic  unreliableinfrastructuresuch as the web may give us different views of the structure on different occasions. ideally  a link analysis algorithm should be insensitive to such perturbations.
　in this paper  we use techniques from matrix perturbation theory and coupled markov chain theory to characterize the stability of the ranks assigned by hits and pagerank. some ways of improving the stability of hits are also briefly metioned; these algorithmic changes are studied in more detail in  ng et al.  1 .
1	an example
let us begin with an empirical example. the cora database  mccallum et al.  1  is a collection containing the citation information from several thousand papers in ai.
　we ran the hits and pagerank algorithms on the subset of the cora database consisting of all its machine learning papers. to evaluate the stability of the two algorithms  we also constructed a set of five perturbed databases in which 1% of the papers from the base set were randomly deleted.   since cora obtained its database via a web crawl  what if  by chance or mishap  it had instead retrieved only 1% of these papers    if a paper is truly authoritative  we might hope that it would be possible to identify it as such with only a subset of the base set.
　the results from hits are shown in the following table. in this table  the first column reports the rank from hits on the full set of machine learning papers  whereas the five rightmost columns report the ranks in runs on the perturbed databases. we see substantial variation across the different runs:
1  genetic algorithms in search  optimization...   goldberg 1 1 1  adaptation in natural and artificial systems   holland 1 1 1  genetic programming: on the programming of...   koza 1 1 1  analysis of the behavior of a class of genetic...   de jong 1 1 1  uniform crossover in genetic algorithms   syswerda 1 1 1  artificial intelligence through simulated...   fogel 1 1 1  a survey of evolution strategies   back+al 1 1 1
1	 optimization of control parameters for genetic...   grefenstette 1 1 1	 the genitor algorithm and selection pressure   whitley	1 1	1
1  genetic algorithms + data structures = ...   michalewicz 1 1 1  genetic programming ii: automatic discovey...   koza 1 - - - 1
1  learning internal representations by error...   rumelhart+al	-	1	1  learning to predict by the method of temporal...   sutton - 1 1  some studies in machine learning using checkers   samuel - - 1 1  neuronlike elements that can solve difficult...   barto+sutton - - 1 - -
1  practical issues in td learning   tesauro	-	-	1	-
1  pattern classification and scene analysis   duda+hart - 1 1  classification and regression trees   breiman+al - 1 1  uci repository of machine learning databases   murphy+aha - 1 - 1  irrelevant features and the subset selection...   john+al - 1 - - 1  the cn1 induction algorithm   clark+niblett - 1 - - 1  probabilistic reasoning in intelligent systems   pearl - 1 - - -
although it might be thought that this variability is intrinsic to the problem  this is not the case  as shown by the results from the pagerank algorithm  which were much more stable:
1	 genetic algorithms in search  optimization and...   goldberg	1	1	1	 learning internal representations by error...   rumelhart+al	1	1	1
1  adaptation in natural and artificial systems   holland 1 1 1  classification and regression trees   breiman+al 1 1 1  probabilistic reasoning in intelligent systems   pearl 1 1 1  genetic programming: on the programming of ...   koza 1 1 1  learning to predict by the methods of temporal ...   sutton 1 1 1  pattern classification and scene analysis   duda+hart 1 1 1  maximum likelihood from incomplete data via...   dempster+al 1 1 1
1  uci repository of machine learning databases   murphy+aha	1 1	1
1  parallel distributed processing   rumelhart+mcclelland - - - 1  introduction to the theory of neural computation   hertz+al - 1 - - -
　these results are discussed in more detail in section 1. it should be stated at the outset  however  that our conclusion is not that hits is unstable while pagerank is not. the issue is more subtle than that  involving considerations such as the relationships between multiple eigenvectors and invariant subspaces. we do wish to suggest  however  that stability is indeed an issue that needs attention. we now turn to a brief description of hits and pagerank  followed by our analysis.
1	overview of hits and pagerank
given a collection of web pages or academic papers linking to/citing each other  the hits and pagerank algorithms each  implicitly  construct a matrix capturing the citation patterns  and determines authorities by computing the principal eigenvector of the matrix.1
1	hits algorithm
the hits algorithm  kleinberg  1  posits that an article has high  authority  weight if it is linked to by many pages with high  hub  weight  and that a page has high hub weight if it links to many authoritative pages. more precisely  given a set of web pages  say  retrieved in response to a search query   the hits algorithm first forms the -by- adjacency matrix   whose -element is 1 if page links to page   and 1 otherwise.1 it then iterates the following equations:  where     means page links to page   to obtain the fixed-points and  with the vectors renormalized to unit length . the above equations can also be written:
when the iterations are initialized with the vector of ones
           this is the power method of obtaining the principal eigenvector of a matrix  golub and van loan  1   and so  under mild conditions 	and	are the principal eigenvectors of	and	respectively. the  authoritativeness  of page	is then taken to be	  and likewise for hubs and	.
1	pagerank algorithm
given a set of web pages and the adjacency matrix  defined previously   pagerank  brin and page  1  first constructs a probability transition matrix by renormalizing each row of to sum to . one then imagines a random web surfer who at each time step is at some web page  and decides which page to visit on the next step as follows: with probability   she randomly picks one of the hyperlinks on the current page  and jumps to the page it links to; with probability   she  resets  by jumping to a web page picked uniformly and at random from the collection.1 here  is a parameter  typically set to 1-1. this process defines a markov chain on the web pages  with transition matrix   where is the transition matrix of uniform transition probabilities
  for all  . the vector of pagerank scores is then defined to be the stationary distribution of this markov chain. equivalently  is the principal eigenvector of the transition matrix  see  e.g. golub and van
loan  1   since by definition the stationary distribution
satisfies
 1 
the asymptotic chance of visiting page   that is    is then taken to be the  quality  or authoritativeness of page .
1	analysis of algorithms
we begin with a simple example showing how a small addition to a collection of web pages can result in a large change to the eigenvectors returned. suppose we have a collection of web pages that contains 1 web pages linking to http://www.algore.com  and another 1 web pages
	1	1
	 a 	 b 
figure 1: jittered scatterplot of hyperlink graph.
linking to http://www.georgewbush.com. the adjacency matrix has all zeros except for the two columns corresponding to these two web pages  therefore the principal eigenvector will have non-zero values only for algore.com and georgewbush.com. figure 1 a  presents a jittered scatterplot of links to these two web pages  along with the first two eigenvectors.  only the non-zero portions of the eigenvectors are shown.  now  suppose five new web pages trickle into our collection  which happen to link to both algore.com and georgewbush.com. figure 1 b  shows the new plot  and we see that the eigenvectors have changeddramatically with the principaleigenvector now near the line. thus  a relatively small perturbation to our collection has caused a large change to the eigenvectors.1if this phenomenonis pervasive  then it needs to be addressed by any algorithm that uses eigenvectors to determine authority. in the next two sections  we give characterizations of whether and when algorithms can be expected to suffer from these problems.
1	analysis of hits
hits uses the principal eigenvector of to determine authorities. in this section  we show that the stability of this eigenvector under small perturbations is determined by the eigengap of   which is defined to be the difference between the largest and the second largest eigenvalues.
　here is an example that may shed light on the importance of the eigengap. figure 1 plots the contours associated with two matrices and before  with solid lines  and after  with dashed lines  the same additive perturbation have been made to them.1 the eigenvalues of the matrices are indicated by the directions of the principal axes of the ellipses. the matrix shown in figure 1a has eigengap   and a small
perturbation to  and hence the ellipse  results in eigenvectors away from the original eigenvectors; the matrix shown in figure 1b has eigengap   and the perturbed eigenvectors are nearly the same as the original eigenvectors. so  we see how  in this example  the size of the eigengap directly affects the stability of the eigenvectors.  readers fa-
	 a 	 b 
figure 1: contours of two matrices with different eigengaps.
miliar with plots of multivariate gaussians can also think of these as the contours of a gaussian with small perturbations imposed on the  inverse  covariance matrix. 
　in the sequel  we use a tilde to denote perturbed quantities.  for instance  denotes a perturbed version of .  we now give our first  positive result  that so long as the eigengap is large  then hits is insensitive to small perturbations.1
theorem 1. let be given. let be the principal eigenvector and the eigengap of . assume the maximum out-degree of every web page is bounded by . for any   suppose we perturb the web/citation graph by adding or deleting at most links from one page  where
		  where	. then the
perturbed principal eigenvector	of the perturbed matrix satisfies:
 1 
　so  if the eigengap is big  hits will be insensitive to small perturbations. this result is proved by showing i  the direction of the principal eigenvector does not change too much  and ii  the magnitudes of the relevant eigenvalues do not change too much  so the second eigenvector does not  overtake  the first and become the new principal eigenvector.
proof. let denote the frobenius norm.1 we apply theorem v.1 from matrix perturbationtheory  stewart and sun  1 : suppose is a symmetric matrix with principal eigenvalue and eigenvector   and eigengap . let be a symmetric perturbation to . then the following inequalities hold for the old principal eigenpair and some new eigenpair   .
		 1 

 1 
 assuming that the denominator in  1  is positive . let the complementary eigenspace to be represented by
         i.e. is orthonormal  and its columns contain all the eigenvectors of except ; is diagonal and contains the corresponding eigenvalues  all of which are at least less

1
   our analyses also apply directly to hub-weight calculations  simply by reversing link directions and interchanging and . 1
	the	frobenius	norm	is	defined	by
.
than ; and holds for :. a bound similar to equation  1 
 1 
　let be the largest eigenvalue of . using corollary iv.1 from stewart and sun   one can show that equation  1  implies

 1 

if in turn   then equations  1  and  1  together will ensure that   i.e. is the principal eigenpair of .
　since we are adding or deleting links from only one page  let denote the perturbation to one row of   so that
. it is straightforward to show

　and	. we can thus bound the norm of the perturbation to :

 1 
using equations  1  and  1  to determine when we may guarantee equation  1  to hold  we arrive at the bound
	  where	. one can easily

verify that the same bound on also ensures  which also guarantees that the denominator in  1  is positive   hence as previously stated. 
　next we give the converseof this result  that if the eigengap is small  then eigenvectors can be sensitive to perturbations.
theorem 1. suppose	is a symmetric matrix with eigengap
 . then there exists a perturbation1 to that causes a large     change in the principal eigenvector.
proof. since	  it can be diagonalized: where	is orthogonal  and whose columns are the	's eigenvectors.	let	denote the	-th column of	.	we pick
　　　　　　　. since   the norm of the perturbation is only . moreover 
as   is the new principal eigenpair. but is orthogonal to   so . 
　to ground these results and illustrate why theorem 1 requires a bound on out-degrees  we give another example of where a small perturbation-addinga single link-can have a large effect. in this example we use the fact that if a graph has
multiple connectedcomponents then the principaleigenvalue will have non-zero entries in nodes only from the  largest 

figure 1: picture of a web community.
connectedcomponent moreformally the componentwith the largest eigenvalue .1
　consider the web/citation-graph shown in figure 1  which we imagine to be a small subset of a much larger graph. solid arrows denote the original set of hyperlinks; the dashed arrow represents the link we will add. the original principal eigenvalue for each of the two connected components shown is ; with the addition of a single link  it is easy to verify that this jumps to . suppose that the community shown is part of a larger web/citation graph with multiple subcommunities  and that originally the biggest subcommunity had eigenvalue . by adding one link  the
graph shown in figure 1 becomes the biggest subcommunity  and the principal eigenvectornow has positive values only for nodes shown in this figure  and zeros elsewhere.
1	analysis of pagerank
we now analyze the sensitivity of pagerank's authority scores to perturbations of the web/citation-graph.
theorem 1. let be given  and let be the principal right eigenvector of . let articles/pages be changed in any way  and be the corresponding  new  transition matrix. then the new pagerank scores satisfies:
		 1 
　thus  assuming is not too close to 1  this shows that so long as the perturbed/modified web pages did not have high overall pagerank scores  as measured with respect to the unperturbed pagerank scores    then the perturbed pagerank scores will not be far from the original.
proof. we construct a coupled markov chain over pairs of web pages/documents as follows.
is drawn according to the probability vector   that is  from the stationary distribution of the pagerank  random surfer  model. the state transitions work as follows: on step   we decide with probability to  reset  both chains  in which case we set and to the same page chosen uniformly at random from the collection. if no  reset  occurs  and if and is one of the unperturbedpages  then is chosen to be a random page linked to by the page
　　. in all other cases  is chosen to be a random page linked to by page   and independently of it  is chosen to be a random page linked to by page .
	thus  we now have two  coupled  markov chains	and
  the former using the transition probabilities
　　　  and latter   but so that their transitions are  correlated.  for instance  the  resets  to both chains always occur in lock-step. but since each chain is following its own state transition distribution  the asymptotic distributions of and must respectively be and . now  let . note   since always.
letting denote the set of perturbed pages  we have: where to derive the first inequality  we used the fact that by construction  the event     is possible only if is one of the perturbed pages. using the fact that and by iterating this bound on in terms of   we obtain an asymptotic upper-bound: . thus  if is drawn from the stationary distribution of the correlated chains-so the marginal distributions of and are respectively given by and -then
. but if two random
variables have only a small chance of taking different values  then their distributions must be similar. more precisely  by the coupling lemma  e.g.  see aldous  1  the variational distance between the distributions must also be bounded by the same quantity . this shows   which concludes the proof. 
1	lsi and hits
in this section we present an interesting connection between hits and latent semantic indexing  deerwester et al.  1   lsi  that provides additional insight into our stability results  see also cohn and chang  1 . in lsi a collection of documents is represented as a matrix   where is 1 if document contains the -th word of the vocabulary  and 1 otherwise. lsi computes the left and right singular vectors of  equivalently the eigenvectors of and  . for example  the principal left singular vector  which we denote   has dimension equal to the vocabulary size  and measures the  strength  of word 's membership along the -dimension. the informal hope is that synonyms will be grouped into the same singular vectors  so that when a document  represented by a column of   is projected onto the subspace spanned by the singular vectors  it will automatically be  expanded  to include synonyms of words in the document  leading to improved information retrieval.
　now consider constructing the following citation graph from a set of documents. let there be a node for each document and for each word. the node of a word links to the
	principal eigenvector	second eigenvector

italian	french 	english	italian	french 	english
	 c 	 d 
　　　　　figure 1: results on random corpora. document nodes it appears in. let be the adjacency matrix of this graph. if we apply hits to this graph  we find only the word-nodes have non-zero hub weights  since none of the document-nodeslink to anything and only the documentnodes have non-zero authority weights. moreover  the vector of hits hub weights of the word-nodes is exactly   the first left singular vector found by lsi.
　this connection allows us to transfer insight from experiments on lsi to our understanding of hits. in this vein  we conducted an experiment in which random corpora were generated by sampling from a set of english  french  and italian documents.1 given that these random corpora are combinations of three distinct languages  the solution to information retrieval problems such as clustering or synonymidentification are exceedingly simple. the issue that we are interested in  however  is stability. to study stability  we generated 1 such collections and examined the direction of the principal eigenvectors found by hits.
　the principal eigenvector lies in the high dimensional joint-vocabulary space of the three languages. to display our results  we therefore defined english  french  and italian  directions   and measured the degree to which the eigenvector lies in each these directions.1 fifteen independentrepetitions of this process were carried out  and the results plotted in figure 1a. as we see  despite the presence of clear clusters in the corpora  the eigenvectors are highly variable. moreover  this variability persists in the second and third eigenvectors  figures 1b c .

1
   the corpora were generated by taking paragraphs from novels in the three languages. typical  documents  had 1 words  and the vocabulary consisted of the most common 1 words per language. the collection was also manually  balanced  to equally represent each language. 1
	this was done by picking a vector	of unit-norm and whose
 -th element is proportional to the frequency of word in the english collection-thus  should be thought of as the  canonical  english direction-and taking the amount that lies in the english direction to be the absolute magnitude of the dot-product between and   and similarly for french and italian.
　note that the variability is not an inherent feature of the problem. in figure 1d  we display a run of a different algorithm  a variant of the hits algorithmthat we brieflydescribe in section 1  and is studied in more detail in  ng et al.  1  . here the results are significantly less variable.
1	further experiments
in this section we report further results of perturbation experiments on the cora database. we also describe an experiment using web pages.
　recall our methodology in the experiments with the cora database: we choose a subset of papers from the database and generate a set of perturbations to this subset by randomly deleting 1% of the papers. our first experiment used all of the ai papers in cora as the base set. our results largely replicated those of cohn and chang -hits returned several genetics algorithms  ga  papers as the top-rankedones. with the database perturbed as described  however  these results were very variable  and hits often returned seminal papers from broader ai areas as its top-ranked documents. repeating the experiment excluding all the ga papers  hits did slightly better; the results on five independent trials are shown below:
1  classification and regression trees   brieman+al 1 1 1  pattern classification and scene analysis   duda+hart 1 1 1  uci repository of machine learning databases   murphy+aha 1 1 1
1  learning internal representations by error...   rumelhart+al	1 1	1
1  irrelevant features and the subset selection problem   john+al 1 1 1  very simple classification rules perform well on...   holte 1 1 1  c1: programs for machine learning   quinlan 1 1 1
1  probabilistic reasoning in intelligent systems   pearl 1 1 1  the cn1 induction algorithm   clark+niblett 1 1 1
1  learning boolean concepts in the ...   almuallim+dietterich	1 1	1
1  the monk's problems: a performance comparison...   thrun - 1 - 1 1  inferring decision trees using the mdl principle   quinlan - 1 - 1
1  multi-interval discretization of continuous...   fayyad+irani	-	-	-	-	1
1  learning relations by pathfinding   richards+moon - 1 - - 1  a conservation law for generalization performance   schaffer - 1 - 1  the feature selection problem: traditional...  kira+randall - - - - 1
1  maximum likelihood from incomplete data via...  dempster+al1 - 1 - 1  learning to predict by the method of temporal...   sutton 1 - 1 - 1  introduction to the theory of neural computation   hertz+al - - 1 - 1  explanation-based generalization: a unifying view   mitchell - - 1 - 1 a robust layered control system for a mobile robot   brooks - - 1 - -
　we see that  apart from the top 1 ranked papers  the remaining results are still rather unstable. for example  pearl's bookwas originallyranked 1th; on the second trial  it dropped to rank 1. similarly  brooks' paper was rank 1  and jumped up to rank 1 on trial 1. however  this variability is not intrinsic to the problem  as shown by our pagerank results  all pagerank results in this section were generated with
 :
1  classification and regression trees   breiman+al 1 1 1  probabilistic reasoning in intelligent systems   pearl 1 1 1  learning internal representations by error...   rumelhart+al 1 1 1  pattern classification and scene analysis   duda+hart 1 1 1
1  a robust layered control system for a mobile robot   brooks 1 1 1  maximum likelihood from incomplete data via...' dempster+al 1 1 1  learning to predict by the method of temporal...   sutton 1 1 1
1  uci repository of machine learning databases   murphy+aha	1	1	1
1  numerical recipes in c   press+al	1	1	1
1  parallel distributed processing   rumelhart+al	1	1	1
1  an implementation of a theory of activity   agre+chapmanre	-	1 1  introduction to the theory of neural computation   hertz+al	-	1	-	-	1  a representation and library for objectives in...   valente+al	-	-	-	-	1
　the largest change in a document's rank was a drop from 1 to 1-these results are much more stable than for hits.
closer examination of the hits authority weights reviews that its jumps in rankings are indeed due to large changes in authority weights  whereas the pagerank scores tended to remain fairly stable.1
　we also carried out experiments on web pages. given a query  kleinberg  describes a method for obtaining a collection of web pages on which to run hits. we use exactly the method described there  and perturbed it in a natural way.1 for the sake of brevity  we only give the results of two experiments here. on the query  mp1 players   hits' results were as follows  long urls are truncated :
1 http://www.freecode.com/	1	1	1
1 http://www.htmlworks.com/	1	1	1
1 http://www.internettrafficreport.com/	1	1	1
1 http://slashdot.org/	1	1	1
1 http://windows.davecentral.com/	1	1	1	http://www.gifworks.com/	1	1	1	http://www.thinkgeek.com/	1	1	1
1 http://www.animfactory.com/	1	1	1
1 http://freshmeat.net/	1	1	1
1 http://subscribe.andover.net/membership.htm	1	1	1
1 http://ourstory.about.com/index.htm	1	-	-	-	1
1 http://home.about.com/index.htm	1	-	-	-	1
1 http://home.about.com/musicperform/index.htm	1	-	-	-	1
1 http://home.about.com/teens/index.htm	1	-	-	-	1
1 http://home.about.com/sports/index.htm	1	-	-	-	1
1 http://home.about.com/autos/index.htm	1	-	-	-	1
1 http://home.about.com/style/index.htm	1	-	-	-	1
1 http://home.about.com/careers/index.htm	1	-	-	-	1
1 http://home.about.com/citiestowns/index.htm 1 - - - 1 http://home.about.com/travel/index.htm 1 - - - 1
in contrast  pagerank returned:
1 http://www.team-mp1.com/	*	1	1
1 http://click.linksynergy.com/fs-bin/click	1	1	1
1 http://www.elizandra.com/	1	1	1
1 http://stores.yahoo.com/help.html	1	1	1	http://shopping.yahoo.com/	1	1	1
1	http://www.netins.net/showcase/phdss/	*	1	1 1	http://www.thecounter.com/	1	1	1
1 http://ourstory.about.com/index.htm 1 1 1 http://a-zlist.about.com/index.htm 1 1 1 http://www.netins.net/showcase/phdss/getm * 1 1 1 http://software.mp1.com/software/ 1 - - 1
1 http://www.winamp.com/	1	-	-	-	-
1 http://www.nullsoft.com/	1	-	-	-	-
1 http://www.consumerspot.com/redirect/1cac	1	-	-	1
while pagerank's rankings undergo small changes  hits' rankings display a mass  flipping  behavior. similar perturbation patterns to this  and the example below  for pagerank and hits are observed in fourteen out of nineteen queries. furthermore  hits' results displayed such mass  flips  in roughly 1% of the trials  which is in accordance with the 1% removal rate.
　here is another typical web result  this time on the query  italian recipes.  note that  *  means that the page was removed by that trial's perturbation  and therefore has no rank. hits' results were:
1	http://ourstory.about.com/index.htm*1	11	http://home.about.com/culture/index.htm*1	11	http://home.about.com/index.htm*1	11	http://home.about.com/food/index.htm*1	11	http://home.about.com/science/index.htm*1	11	http://home.about.com/shopping/index.htm*1	11	http://home.about.com/smallbusiness/index*1	11	http://home.about.com/sports/index.htm*1	11	http://home.about.com/arts/index.htm*1	11	http://home.about.com/style/index.htm*1	11	http://home.about.com/autos/index.htm----1	http://home.about.com/teens/index.htm----1	http://bestbrandrecipe.com/default.asp1----1	http://myrecipe.com/help/shopping.asp1----1	http://vegetarianrecipe.com/default.asp1----1	http://holidayrecipe.com/default.asp1----1	http://beefrecipe.com/default.asp1----1	http://beveragerecipe.com/default.asp1----1	http://appetizerrecipe.com/default.asp1----1	http://pierecipe.com/default.asp1----1	http://seafoodrecipe.com/default.asp1----1	http://barbequerecipe.com/default.asp
pagerank  on the other hand  returned:1 ----1	http://ourstory.about.com/index.htm*	111	http://a-zlist.about.com/index.htm*	111	http://www.apple.com/111	http://www.tznet.com/isenberg/11	1	http://frontier.userland.com/11	1	http://www.mikrostore.com/11	*1	http://www.amazinggiftsonline.com/11	*1	http://www.peck.it/peckshop/home.asp prov*	111	http://geocities.yahoo.com/addons/interac11	1	http://dvs.dolcevita.com/index.html1	*11	http://www.dossier.net/-	-	1	1	http://www.dolcevita.com/1	-	-	-	1	http://www.q-d.com/1	-	-	-	1	http://www.silesky.com/1 -	-	-	-1	discussion
it is well known in the numerical linear algebra community that a subspace spanned by several  e.g. the first   eigenvectors may be stable under perturbation  while individual eigenvectors may not  stewart and sun  1 . our results-both theoretical and empirical-reflect this general fact.
　if the outputof an algorithmis a subspace  then the stability considerations that we have discussed may not be a matter of primary concern. such is the case  for example  for the lsi algorithm  where the goal is generally to project a data set onto a lower-dimensional subspace.
　if we wish to interpret specific eigenvectors  however  then the stability issue becomes a matter of more serious concern. this is the situation for the basic hits algorithm  where primary eigenvectors have been interpreted in terms of a set of  hubs  and  authorities.  as we have seen  there are theoretical and empirical reasons for exercising considerable caution in making such interpretations.
　given that the principaleigenvectormay not have a reliable interpretation  one can consider variations of the hits approach that utilize multiple eigenvectors. indeed  kleinberg  suggested examining multiple eigenvectors as a way of obtaining authorities within multiple communities. again  however  it may be problematic to interpret individual eigenvectors  and in fact in our experiments we found significant variability in second and third eigenvectors. an alternative approach may be to automatically combine multiple eigenvectors in a way that explicitly identifies subspaces within the hits framework. this is explored in  ng et al.  1  the fact that the pagerank algorithm appears to be relatively immune to stability concerns is a matter of considerable interest. it is our belief that the  reset-to-uniformdistribution  aspect of pagerank is a critical feature in this regard. indeed  one can explore a variation of the hits algorithm which incorporates such a feature. suppose that we construct a markov chain on the web in which  with probability   we randomly follow a hyperlink from the current page in the forward direction  on odd time steps   and we randomly follow a hyperlink in the backwards direction  on even time steps . with probability   we reset to a uniformly chosen page. the asymptotic web-page visitation distribution on odd steps is defined to be the authority weights  and on even steps the hub weights. as in theorem 1  we can show this algorithm is insensitive to small perturbations  but unlike pagerank  we obtain hub as well as authority scores . the results of runningthis algorithm on the  three languages  problem are shown in figure 1d  where we see that it is indeed significantly more stable than the basic hits algorithm. this algorithm is also explored in more detail in  ng et al.  1 .
acknowledgments
we thank andrew mccallum for providing the cora citation data used in our experiments. we also thank andy zimdars for helpful comments. this work was supported by onr muri n1-1and nsf grant iis-1.
references
 aldous  1  david aldous. random walks on finite groups and rapidly mixing markov chains. in a. dold and b. eckmann  editors  seminaire de probabilit＞ es xvii＞ 1 lecture notes in mathematics  vol. 1  pages 1. springer-verlag  1.
 bharat and henzinger  1  k. bharat and m. r. henzinger. improved algorithms for topic distillation in a hyperlinked environment. in proc. 1st annual intl. acm sigir conference  pages 1. acm  1.
 brin and page  1  s. brin and l. page. the anatomy of a large-scale hypertextual  web  search engine. in the seventh international world wide web conference  1.
 chung  1  fan r. k. chung. spectral graph theory. american mathematical society  1.
 cohn and chang  1  d. cohn and h. chang. probabilistically identifying authoritative documents. in proc. 1th international conference on machine learning  1.
 deerwester et al.  1  s. deerwester  s. dumais  g. furnas  t. landauer  and r. harshman. indexing by latent semantic analysis. journal of the american society for information science  1 :1  1.
 golub and van loan  1  g. h. golub and c. f. van loan. matrix computations. johns hopkins univ. press  1.
 kleinberg  1  j. kleinberg. authoritative sources in a hyperlinked environment. proc. 1th acm-siam symposium on discrete algorithms  1.
 mccallum et al.  1  andrew mccallum  kamal nigam  jason rennie  and kristie seymore. automating the contruction of internet portals with machine learning. information retrieval journal  1-1  1.
 ng et al.  1  andrew y. ng  alice x. zheng  and michael i. jordan. stable algorithms for link analysis. in proc. 1th annual intl. acm sigir conference. acm  1.
 osareh  1  farideh osareh. bibliometrics  citation analysis and co-citation analysis: a review of literature i. libri  1-1  1.
 page et al.  1  lawrence page  sergey brin  rajeev motwani  and terry winograd. the pagerank citation ranking: bringing order to the web. unpublished manuscript  1.
 stewart and sun  1  g. w. stewart and ji-guang sun. matrix perturbation theory. academic press  1.

active learning for class probability estimation and ranking 
 
maytal saar-tsechansky and foster provost 
department of information systems 
leonard n. stern school of business  new york university {mtsechan|fprovost} stern.nyu.edu 

 
abstract 
for many supervised learning tasks it is very costly to produce training data with class labels. active learning acquires data incrementally  at each stage using the model learned so far to help identify especially useful additional data for labeling.  existing empirical active learning approaches have focused on learning classifiers.  however  many applications require estimations of the probability of class membership  or scores that can be used to rank new cases.  we present a new active learning method for class probability estimation  cpe  and ranking. bootstrap-lv selects new data for labeling based on the variance in probability estimates  as determined by learning multiple models from bootstrap samples of the existing labeled data.  we show empirically that the method reduces the number of data items that must be labeled  across a wide variety of data sets.  we also compare bootstrap-lv with uncertainty sampling  an existing active learning method designed to maximize classification accuracy.  the results show that 
bootstrap-lv dominates for cpe. surprisingly it also often is preferable for accelerating simple accuracy maximization. 
1   introduction 
supervised classifier learning requires data with class labels.  in many applications  procuring class labels can be costly. for example  to learn diagnostic models experts may need to analyze many historical cases. to learn document classifiers experts may need read many documents and assign them labels.  to learn customer response models  consumers may have to be given costly incentives to reveal their preferences.   
 active learning processes training data incrementally  using the model learned  so far  to select particularly helpful additional training examples for labeling.  when successful  active learning methods reduce the number of instances that must be labeled to achieve a particular level of accuracy. most existing methods and particularly empirical approaches for active learning address classification problems-they assume the task is to assign cases to one of a fixed number of classes.  
 many applications require more than simple classification. decision-making often requires estimates of the probability of class membership. class probability estimates  cpes  can be combined with decision-making costs/benefits to minimize expected cost  maximize expected benefit .  for example  in target marketing the estimated probability that a customer will respond to an offer is combined with the estimated profit  produced with a different model   zadrozny and elkan  1 . other applications require ranking of cases  to add flexibility to user processing.1 we agree with turney  turney  1  that machine learning systems should be able to take into account various cost/benefit information  including decision-making costs as well as labeling costs.  
 

 	1	1	1
 	training set size
figure 1: learning curves for the car data set 
 
 in this paper  we consider active learning to produce accurate cpes and class-based rankings.  figure 1 shows the desired behavior of an active learner.  the horizontal axis represents the number of training data  and the vertical axis represents the error rate of the probabilities produced by the model learned. each learning curve shows how error rate decreases as more training data are used.  the upper curve represents the decrease in error from randomly selecting training data; the lower curve represents active learning. the two curves form a  banana  shape: very early on  the curves are comparable because a model is not yet available for active learning.  the active learning curve soon accelerates  because of the careful choice of training data. 
given enough data  random selection catches up.   we introduce a new active learning technique  bootstrap-lv  which uses bootstrap samples of existing training data to examine the variance in the probability estimates for not-yet-labeled data. we show empirically across a wide range of data sets that bootstrap-lv decreases the number of labeled instances needed to achieve accurate probability estimates  or alternatively that it increases the accuracy of the probability estimates for a fixed number of training data. we also show that bootstrap-lv is surprisingly effective even for accuracy maximization. 
1    active learning: prior work 
the fundamental notion of active learning has a long history in machine learning. to our knowledge  the first to discuss it explicitly were  simon and lea  1  and  winston  1 .  simon and lea describe how machine learning is different from other types of problem solving  because learning involves the simultaneous search of two spaces: the hypothesis space and the instance space.  the results of searching the hypothesis space can affect how the instance space will be searched. winston discusses how the best examples to select next for learning are  near misses   instances that miss being class members for only a few reasons. subsequently  theoretical results showed that the number of training data can be reduced substantially if they can be selected carefully   angluin  1; valiant  1 .  the term active learning was coined later to describe induction where the algorithm controls the selection from a set of potential training examples  cohn et al.  1 .  
 	 

input: an initial labeled set l  an unlabeled set ul  an inducer i  a stopping criterion  and an integer m specifying the number of actively selected examples in each phase. while stopping criterion not met    
  	 /* perform next phase: */ 
 	apply inducer i to l  
   for each example { xi | xi （ul} compute esi   the effectiveness score  
 	select a subset s of size m from ul based on esi 
 	remove s from ul  label examples in s  and add s to l 
output: estimator e induced with i from the final labeled set l 
figure 1: generic active learning algorithm  
 
 a generic algorithm for active learning is shown in figure 1. a learner first is applied to an initial set l of labeled examples  usually selected at random or provided by an expert . subsequently  sets of m examples are selected in phases from a set of unlabeled examples ul  until some predefined condition is met  e.g.  the labeling budget is exhausted . in each phase  each candidate example xi （ul is given an effectiveness score esi based on its contribution to an objective function  reflecting the estimated magnitude of its contribution to subsequent learning  or simply whether it will or will not contribute . examples then are ranked by their effectiveness scores and the top m examples are selected for labeling. usually  multiple examples  rather than a single example  are selected at each phase due to computational constraints. once examples are selected  their labels are obtained  e.g.  by querying an expert  before being added to l  on which the learner is applied next.    cohn et al.  cohn et al.  1  determine esi based on identifying what they called the  region of uncertainty   defined such that concepts from the current version space are inconsistent with respect to examples in the region. the region of uncertainty is redetermined at each phase and subsequent examples are selected from this region. the main practical problem with this approach is that the estimation of the uncertainty region becomes increasingly difficult  as the concept becomes more complex. in addition  for complex concepts the region of uncertainty initially may span the entire domain before the concept is well understood  rendering the selection process ineffective. a closely related approach is query by committee  qbc  seung et al. 1 : classifiers are sampled from the version space  and the examples on which they disagree are considered for labeling. however  qbc is a theoretical approach that poses computational and practical constraints. particularly  it assumes the existence of hypotheses from the version space available for sampling  as well as noise-free data. several other approaches also address learning for classification. these methods target examples for which predictions of class membership are evenly split  for binary classes  among an ensemble of classifiers  or alternatively examples for which a single probabilistic classifier assigns cpe near 1  as indicating class uncertainty. specifically  lewis and gale  lewis and gale  1  proposed uncertainty sampling where a probabilistic classifier is employed  and examples whose probabilities of class membership are closest to 1 are considered for labeling. abe and mamitsuka  abe and mamitsuka  1  generate a set of classifiers and then select examples for which the classifiers are close to being evenly split. an approach due to iyengar et al.  iyengar et al.  1  directly estimates whether a classifier will assign an example to the wrong class. they employ a second classifier to assign classes to unlabeled examples  and examples are considered more informative for learning if estimated as being likely to be misclassified by the current ensemble of classifiers. these approaches are designed specifically for maximizing classification accuracy  not for optimizing cpes or rankings  which are our concern.   the method most closely related to our technique was presented by cohn et al.  cohn et al.  1  for statistical learning models. at each phase they compute the expectation of the variance of the model over the example space resulting from adding each candidate example to the training set. our approach is similar in that it estimates variance  but instead of modeling the variance of the model over the input space  we estimate the  local  variance for each xi （ul. the approach of cohen et al. requires knowledge 
of the underlying domain  as well as the computation in closed form of the learner's variance  a constraint that renders it impracticable for arbitrary models. our approach can be used for arbitrary models.  
1   the bootstrap-lv algorithm  
 bootstrap-lv actively samples examples from ul to learn class probability estimates  cpes . the description we provide here pertains to binary class problems where the set of class labels is c={1}. as the discussion above indicates  we wish to add to l examples that are likely to improve the available evidence pertaining to poorly understood subspaces of the example space.  
 ideally  the most direct indication of the quality of the current class probability estimate for example xi is the discrepancy between the estimated probability and its true probability. however  the true class probability for an instance is not known  nor is its actual class.  therefore we use the  local variance   lv  to estimate this quality. local variance refers to the variance in cpe for a particular example. if the estimated lv is high compared to that of other examples  we infer that this example is  difficult  for the learner to estimate given the available data  and is thus more desirable to be selected for learning. otherwise  if the lv is low  we interpret it as an indication that either the class probability is well learned or  on the contrary  that it will be extremely difficult to improve. we therefore decrease the likelihood of these examples being added to l.  
　given that a closed-form computation/estimation of this local variance may not  easily  be obtained  we estimate it empirically. we generate a set of k bootstrap subsamples  efron and tibshirani  1  b j   j =1 ...  k from l  and apply the inducer i to each subsample to generate k estimators ej   j =1 ...  k . for each example in ul we estimate the variance in cpes given by the estimators {ej }. each example in ul is assigned a weight  which determines its probability of being sampled  and which is proportional to the variance of the cpes. more specifically  the distribution from which examples are sampled is given by 
/ pi min
ds xi =   where p j  xi   denotes the 
               r estimated probability an estimator ej assigns to the event that example xi belongs to class 1  the choice of performing the calculation for class 1 is arbitrary  since the variance for both classes is identical and hence the same result for ds xi   is obtained for class 1 ; pi is the average i min is the average probability estimation as-
k
signed to the minority class by the various estimators  and 
r 	is 	a 	normalizing 	factor 
r=‘isize=1  ul  {‘kj=1  p j  xi    pi  1  }/ pi min   so that ds is a 
distribution. this is the bootstrap-lv algorithm  shown in figure 1. 
algorithm bootstrap-lv  

1 input: an initial labeled set l sampled at random  an unlabeled set ul  an inducer i   a stopping criterion  and a sample size m. 
 	 
1 for  s=1;until stopping criterion is met; s++   
1 generate k bootstrap subsamples b j   j =1 ... k from l 
1 apply inducer i on each subsample b j and induce estimator e j  
1 for all examples { xi | xi （ul} compute 
ds xi ={‘kj=1  	 pi 1  }/ pi min   pj xi 

r
1 sample from the probability distribution ds   a subset s of m examples from ul without replacement  
1 remove s from ul  label examples in s  and add them to l 
1 end for 
1 output: estimator e induced with i from l 

figure 1: the bootstrap-lv algorithm 
there is one additional technical point of note.  consider the case where the classes are not represented equally in the training data. when high variance exists in regions of the domain for which the minority class is assigned high probability  it is likely that the region is relatively better understood than regions with the same variance but for which the majority class is assigned high probability. in the latter case  the class probability estimation may be exhibiting high variance due simply to lack of representation of the minority class in the training data  and would benefit from oversampling from the respected region. that is why we divide the estimated variance by the average value of the minority-class probability estimates pi min . we determine the minority class once from the initial random sample.  
1   experimental evaluation  
　we are interested primarily in comprehensible models  so for these experiments we use decision trees to produce class probability estimates. however  bootstrap-lv applies to any technique for learning cpes. particularly  the underlying probability estimator we use is a probability estimation tree  pet -an unpruned c1 decision tree  quinlan  1  for which the laplace correction  cestnik  1  is applied at the leaves. the laplace correction has been shown to improve the cpes produced by pets  bauer and kohavi  1; provost et al.  1; provost & domingos  1 . 
　when evaluating cpe accuracy  if the true underlying class probability distribution were known  an evaluation of an estimator's accuracy could be based on a measure of the actual error in probability estimation. since the true probabilities of class membership are not known we compare the probabilities assigned by the model induced at each phase with those assigned by a  best  estimator  eb   as surrogates to the true probabilities. eb is induced from the entire set of examples  ul“ l   using bagged-pets  which have been shown to produce superior probability estimates compared to individual pets  bauer and kohavi  1; provost et al.  1; provost & domingos  1 . we compute the mean absolute error  mae  for an estimator e with respect to eb 's estimation  denoted by bmae. spe-
cifically  bmae=‘in=1 peb  xi  pe xi    where peb  xi   is the 
n
estimated probability given byeb ; pe  xi  is the probability estimated by e  and n is the number of examples examined.  to evaluate its performance  we applied bootstrap-lv to 1 data sets  1 from the uci machine learning repository  blake et al.  1  and 1 used previously to evaluate rulelearning algorithms  cohen and singer  1 . data sets with more than two classes were mapped into two-class problems. we compare the performance of bootstrap-lv against a method denoted by random  where estimators are induced with the same inducer and training set size  but for which examples are sampled at random. we show the comparison for different sizes of the labeled set l. in order not have very large sample sizes m for large data sets and very small ones for small data sets  we applied different numbers of phases for different data sets  varying between 1 and 1; at each phase the same number of examples was added to l. results are averaged over 1 random partitions of the data sets into an initial labeled set  an unlabeled set  and a test set against which the two estimators are evaluated.  for control the same partitions were used by both 
random and bootstrap-lv.  
　the banana curve in figure 1 above shows the relative performance for the car data set  where active learning refers to bootstrap-lv . as shown in figure 1  the error of the estimator induced with bootstrap-lv decreases faster initially  exhibiting lower error for fewer examples. this demonstrates that examples actively added to the labeled set are more informative  on average   allowing the inducer to construct a better estimator with fewer examples. for some data sets bootstrap-lv exhibits even more dramatic results; figure 1 shows results for the pendigits data set.  
 

	 	1 training set size1	1
 
figure 1: cpe learning curves for the pendigits data set 
 
bootstrap-lv achieves its almost minimal level of error at 1 examples. random requires more than 1 examples to obtain this error level. for 1 of the 1 data sets  our approach did not succeed in accelerating learning much or at all  as is shown for the weather data set in figure 1. note  however  that neither curve consistently resides above the other and the two methods' performance is comparable. 
 

	1	 1
training set size 
figure 1: cpe learning curves for the weather data set 
 
　table 1 presents a summary of our results for all the data sets. the primary motivation for applying active learning techniques is to allow learning with fewer examples. table 1 provides a set of measures pertaining to the number of examples  gained  using bootstrap-lv instead of random. the second column shows the percent of phases in which bootstrap-lv produced the same level of cpe accuracy with fewer examples than random  we will call this  phases-gained  . the third and fourth columns show the percentage and number of examples gained by applying bootstrap-lv. the gain is calculated as the difference between the number of examples used by random and that used by bootstrap-lv to obtain the same cpe accuracy. the percentage is calculated based on the number of examples used by random. because of the natural banana shape even for the ideal case  the performance of estimators induced from any two samples cannot be considerably different at the final phases  thus the averages as well as the phases-gained merely provide an indication of whether bootstrap-lv produces superior estimations. it is important also to observe the improvement at the  fat  part of the banana  where the benefit of active learning is concentrated . to allow a stable assessment we provide rather than the single best gain  the average of the largest 1% of the gains. columns 1 and 1 of table 1 show the average percent and average number  respectively  of examples gained for the top 1% gains.  it is important that these figures be viewed in tandem with column 1  phases-gained   to ensure that there is in fact a banana shape to the graph. 
　table 1 also includes summary results pertaining to the error rates achieved by both methods for the same number of examples. column 1 presents the average error reduction for the 1% of the sampling phases exhibiting the highest error reduction. for some data sets the generalization error for the initial training sets was small and was not considerably reduced even when the entire data was used for training  e.g.  for connect-1  only 1% error reduction was obtained  from 1 to 1 . we therefore also provide  in the last column  the top-1% error gain as a percentage of the reduction required to obtain the minimal error  the latter is referred to in the table as maximal gain . in the adult data set  for instance  bootstrap-lv exhibited only 1% error gain  for the top 1%   but this improvement constitutes 1% of the possible improvement were the entire data set used for training.  
 
 examples error  %  data set phases with positive gain  %   avg %  gained 
 
   avg # gained 
 
 top 1% 
　 % gained 
 top 1% 
# gained 
 
 avg top 
1% 
 %  
 avg top 
　1%   % from maximal gain  abalone 1 1 1  1 1 1  	1 adult  1 1 1 1 1 1 1 breast cancer-w 1 1 1 1 1 1 1 car  1 1 1 1 1 1 1 coding1  1 1 1 1 1 1 1 connect-1  1 1 1 1 1 1 1 contraceptive  1 1 1 1 1 1 1 german*  1 1 1 1 1 1 1 hypothyroid  1 1 1 1 1 1 1 kr-vs-kp  1 1 1 1 1 1 1 letter-a** 1 1 1 1 1 1 1 letter-vowel 1 1 1 1 1 1 1 move1  1 1 1 1 1 1 1 ocr1  1 1 1 1 1 1 1 optdigits  1 1 1 1 1 1 1 pendigits  1 1 1 1 1 1 1 sick-euthyroid  1 1 1 1 1 1 1 solar-flare  1 1 1 1 1 1 1 weather 1 -1 -1 1 1 1 1 yeast  1 1 1 1 1 1 1 * german credit database 
** letter-recognition  letter a 
table 1: improvement in examples needed and improvement in error using bootstrap-lv 
　since not all plots can be presented due to space constraints  we tried to express in the table various performance measures that would provide a comprehensive perspective. to assess bootstrap-lv's superiority we apply the combination of the following: phases-gained should be above 1%; both the average example and error gains should be positive  and the top-1% error reduction from the maximal gain should be 1% or higher. if phases-gained is between 1% and 1% we consider the methods to be comparable  and when it is below 1% we consider bootstrap-lv to be inferior.  as can be seen in table 1  in bold   in 1 out of the 1 data sets bootstrap-lv exhibited superior performance. particularly  in all but one phases-gained is 1% or above. in 1 of those  more than 1% of the examples were saved  for the top 1%   and in 1 data sets our method used less than 1% of the number of examples required for random to achieve the same level of accuracy.  for the sick-euthyroid data set  for instance  bootstrap-lv gradually improves until it requires fewer than 1% of the examples required by random to obtain the same level of accuracy. these results pertain to the top-1% improvement  so the maximal gain can be much higher.   for a single data set  weather  bootstrap-lv exhibited a negative average examples gain. however  phases-gained  showing that bootstrap-lv uses fewer examples in 1% of phases examined  and figure 1  both indicate that the two methods indeed exhibit comparable learning curves for this data set.  
　the measures pertaining to the number of examples gained and the error gain complement each other and may provide interesting insight. for instance  the number of examples gained can help evaluate the  difficulty  in error reduction in terms of the number of examples required by random to obtain such reduction. for example  although the average top-1% error gain for connect-1 was less than 1%  table 1 shows that it required random 1 additional examples on average to obtain the same improvement. a single data set  letter-vowel  exhibited a negative average error gain. however  phases-gained is exactly 1%  indicating that random indeed does not exhibit superior performance overall. both methods have similar learning curves.  
　we also assessed both methods with two alternatives to bmae: the mean squared error measure proposed by bauer and kohavi   as well as the area under the roc curve  bradley 1  which specifically evaluates ranking accuracy. the results for these measures agree with those obtained with bmae.  for example  bootstrap-lv generally leads to fatter roc curves with fewer examples. for those data sets in which bootstrap-lv exhibits insignificant or no improvement at all  training examples chosen at random seem to contribute to error reduction at an almost constant rate. their learning curves have an atypical shape  as shown for the weather data set in figure 1  where additional examples bring an almost constant reduction in error rather than the expected decreasing marginal error reduction. this may indicate that it is easy to obtain good examples for learning  and any additional example contributes to error reduction equally  regardless of what or how many examples have been already used for training.  thus intelligent selection of learning examples is less likely to improve learning significantly.  
1   additional experiments 
 tree-based models offer a comprehensible structure that is important in many decision-making contexts. however  they often do not provide the best probability estimates. in order to assess bootstrap-lv 's performance on a better cpe learner  we experimented with bagged-pets  which are not comprehensible models  but have been shown to produce markedly superior cpes  bauer and kohavi  1; provost et al.  1; provost & domingos  1 .  
　the results for the bagged-pets model agree with those obtained for individual pets. particularly  for 1 of the data sets bootstrap-lv exhibited phases-gained of more than 1%  in 1 of those phases-gained is more than 1% . the average top-1% example gain was 1% or higher in 1 of those data sets. only in two data sets is phases-gained less than 1%. figure 1 shows a comparison between bootstrap-lv and random for individual pets and for bagged-pets. as expected  the overall error exhibited by the bagged-pets is lower than for the pet  and for both models bootstrap-lv achieves its lowest error with considerably fewer examples than are required for random. 
 

figure 1: cpe learning curves for the hypothyroid data set 
 described above  uncertainty sampling  lewis and gale  1  was proposed for binary text classification. however  it too samples examples that are not well understood by the model. since it was shown to improve a model's classification accuracy  it may improve the model's cpe as well. it therefore is interesting to compare the improvements exhibited by bootstrap-lv against uncertainty sampling. we present a summary of the comparison results in table 1  where all the measures are the same as in table 1  except that the baseline comparison is 
uncertainty sampling rather than random. 
 
 examples error  %  data set phases with positive gain  %   avg %  gained 
  avg # gained 
 top 1% 
% gained 
 
 top 1% 
# gained 
 avg top 
1% 
 %  
 avg top 1% 
　 % from maximal gain  
 abalone 1  
1  
1 1  
1 1 1 adult  1 1 1 1 1 1 1 breast cancer-w 1 1 1 1 1 1 1 car  1 1 1 1 1 1 1 coding1  1 1 1 1 1 1 1 connect-1  1 1 1 1 1 1 1 contraceptive  1 1 1 1 1 1 1 german  1 1 1 1 1 1 1 hypothyroid  1 1 1 1 1 1 1 kr-vs-kp  1 1 1 1 1 1 1 letter-a 1 1 1 1 1 1 1 letter-vowel 1 1 1 1 1 1 1 move1  1 1 1 1 1 1 1 ocr1  1 1 1 1 1 1 1 optdigits  1 1 1 1 1 1 1 pendigits  1 1 1 1 1 1 1 sick-euthyroid  1 1 1 1 1 1 1 solar-flare  1 -1 -1 -1 -1 -1 -1 weather 1 1 1 1 1 1 1 yeast  1 1 1 1 1 1 1 table 1: summary results of bootstrap-lv versus  
uncertainty sampling  cpe  
 
bootstrap-lv exhibits markedly superior performance compared to uncertainty sampling. particularly  bootstrap-lv is superior in 1 of the data sets  and in 1 data sets the methods exhibit comparable performance  where phases-gained for bootstrap-lv between 1% and 1%. 
uncertainty sampling exhibits superior performance in one data set  solar-flare  for which it consistently produces better probability estimations.  
　in 1 out of the 1 data sets in which bootstrap-lv was superior  the average top error reduction was more than 1%. these results demonstrate that bootstrap-lv has a solid advantage when compared to uncertainty sampling for class probability estimation. moreover  for several data sets uncertainty sampling's performance was inferior to that of random. it is important to emphasize once again that indeed uncertainty sampling was not designed to improve class probability estimation  but rather to improve classification accuracy.  
　we also compared the performance of uncertainty sampling against bootstrap-lv for improving classification accuracy. since bootstrap-lv was found to improve cpes  a similar effect may be obtained for classification accuracy  but not necessarily: bootstrap-lv may select examples to improve class probability estimation even when the estimated decision boundary required for classification is already well understood  thereby  wasting  examples that do not improve classification accuracy.  
　our results for classification accuracy show that in 1 data sets bootstrap-lv exhibited superior performance for accuracy maximization. uncertainty sampling was superior in 1 data sets and the methods exhibited comparable performance for the remaining two. these results indicate that although bootstrap-lv is not uniformly superior to uncertainty sampling for classification tasks  it should be considered a viable alternative-it often yields much better performance.  interestingly  in most cases where bootstrap-lv does not dominate  it performs better in the initial phases  whereas uncertainty sampling surpasses bootstrap-lv in later phases. this phenomenon is demonstrated in figure 1 for the breast-cancer data set.  

figure 1: classification accuracy rate for breast-cancer  
 
　recall that uncertainty sampling uses the cpes to determine the potential contribution of an example for learning. therefore  its performance will be sensitive to cpe accuracy. poor cpes produced in the initial phases undermine the data selections by uncertainty sampling.  on the other hand  in later phases  more accurate probability estimations allow the selection process to focus in on the decision boundary. bootstrap-lv  on the contrary  focuses early on improving the cpes  and therefore performs well even very early on the learning curve; however  later on it indeed  wastes  examples to improve cpe.  
　in light of this behavior  a better strategy for actively improving classification accuracy may be a hybrid approach: 
bootstrap-lv is applied in initial phases and uncertainty sampling later.  when to switch   is an open question.  
1  conclusions and limitations 
　we introduced a new technique for active learning.  bootstrap-lv was designed to use fewer labeled training data to produce better class probability estimates from fewer labeled data. we showed empirically that it does this remarkably well. we also showed that bootstrap-lv is competitive with uncertainty sampling even for accuracy maximization. inspecting these last results also suggests a hybrid strategy that may be even more effective than either technique alone. 
　bootstrap-lv was designed to identify particularly informative examples to use for training in order to economize on labeling costs to obtain higher cpe accuracy. it does not address computational concerns  as do lewis and catlett  lewis and catlett  1 .  indeed bootstrap-lv is a computationally intensive approach  because of the need to induce at each phase multiple models from a set of bootstrap samples. yet  because of the typical shape of the learning curve  beyond a certain training set size the marginal error reduction is insignificant  whether active learning or random sampling is employed. thus  intelligent selection of examples for learning is only critical in the early part of the curve  where a relatively small number of examples are used for training. therefore  as long as the number of training examples remains relatively small- multiple model inductions from these samples do not constitute a considerable computational toll.  moreover  bootstrap-lv provides an appropriate solution whenever labeling costs are more important than computational costs  for example  when the primary concern is to obtain accurate cpe or ranking with minimal costly labeling. 
acknowledgments  
　we thank vijay iyengar for helpful comments and ibm for a faculty partnership award. 
references 
  abe and mamitsuka  1  abe  n. and mamitsuka  h. query learning strategies using boosting and bagging. in proceedings of the fifteenth international conference on machine learning  pp. 1. 
 angluin  1  angluin  d. queries and concept learning. machine learning  1-1  1. 
 bauer and kohavi  1  bauer  e.  kohavi  r. an empirical comparison of voting classification algorithms:  bagging  boosting  and variants. machine learning  1  1  1 . 
 blake et al.  1  blake  c.l. & merz  c.j. uci repository of machine learning databases. irvine  ca: university of california  department of information and computer science  1  http://www.ics.uci.edu/~mlearn/mlrepository.html . 
 bradley  1  bradley  a. p. the use of the area under the roc curve in the evaluation of machine learning algorithms. pattern recognition  1   1  1. 
  cestnik  1  cestnik  b. estimating probabilities: a crucial task in machine learning. in proceedings of the ninth european conference on artificial intelligence  1  sweden  1. 
 cohn et al.  1  cohn  d.  atlas  l. and ladner  r. improved generalization with active learning. machine learning  1  1. 
 cohn et al.  1  cohn  d.  ghahramani  z.  and jordan m. active learning with statistical models.  journal of artificial intelligence research  1-1  1.   
 cohen and singer  1  cohen  w. w. and singer  y. a simple  fast  and effective rule learner. in aaai-1  1  1. 
 efron and tibshirani  1  efron  b. and tibshirani  r. an introduction to the bootstrap  chapman and hall  1. 
 iyengar et al.  1  iyengar  v. s.  apte  c.  and zhang t. active 
learning using adaptive resampling. in sixth acm sigkdd intl. conf. on knowledge discovery and data mining. 1. 
 lewis and gale  1  lewis  d.  and gale  w. a.  a sequential algorithm for training text classifiers. in acm-sigir-1  1. 
 lewis and catlett  1   lewis  d. d.  and catlett  j. heterogeneous uncertainty sampling. in proceedings of the eleventh international conference on machine learning  1  1. 
  provost et al.  1  provost  f.; fawcett  t.; and kohavi  r. the case against accuracy estimation for comparing classifiers. in proc. of the intl. conf. on machine learning  1  1. 
 provost and domingos  1   provost  f. and domingos p. welltrained pets: improving probability estimation trees. ceder working paper #is-1  stern school of business  nyu.  
 quinlan  1  quinlan  j. r.. c1: programs for machine learning. morgan kaufman  san mateo  california  1. 
 seung et al.  1  h. s. seung  m. opper  and h. smopolinsky. query by committee. in proceedings of the fifth annual acm workshop on computational learning theory  1  1. 
 simon and lea  1  herbert a. simon and glenn lea  problem solving and rule induction: a unified view. in l.w. gregg  ed.   knowledge and cognitiom. chap. 1. potomac  md: erlbaum  1. 
 turney  1  turney  p.d. types of cost in inductive concept learning  workshop on cost-sensitive learning at icml1  stanford university  california  1. 
 valiant  1  valiant l. g. a theory of the learnable. communications of the acm  1-1  1. 
 winston  1 . winston  p. h. learning structural descriptions from examples. in  the psychology of computer vision   p. h. winston  ed.   mcgraw-hill  new york  1.  
 zadrozny and elkan  1  zadrozny b. and elkan c. learning and making decisions when costs and probabilities are both unknown. technical report no.cs1  dept. of computer science and engineering  uc san diego  january 1. 

machine learning and
data mining
machine learning

learning on the phase transition edge
alessandro serra  attilio giordana and lorenza saitta
dipartimento di scienze e tecnologie avanzate 
universita` del piemonte orientale 
corso borsalino 1  1  alessandria  italy

abstract
a previous research has shown that most learning strategies fail to learn relational concepts when descriptions involving more than three variables are required. the reason resides in the emergence of a phase transition in the covering test. after an in depth analysis of this aspect  this paper proposes an alternative learning strategy  combining a monte carlo stochastic search with local deterministic search. this approach offers two main benefits: on the one hand  substantial advantages over more traditional search algorithms  in terms of increased learning ability  and  on the other  the possibility of an a-priori estimation of the cost for solving a learning problem  under specific assumptions about the target concept.
1	introduction
in a recent paper giordana et al.  giordana et al.  1  have shown that relational learning becomes very hard when the target concept requires descriptions involving more than three variables. the reason is related to the presence of a phase transition in the covering test  prosser  1; giordana and saitta  1   i.e.  an abrupt change in the probability that an inductive hypothesis covers a given example  when the hypothesis and the example sizes reach some critical values. moreover  any top-down learner will search for discriminant hypotheses in the phase transition region  giordana et al.  1; giordana and saitta  1   and  finally  heuristics commonly used to guide top-down relational learning  quinlan  1; botta and giordana  1  become useful only in the same region. the consequence is that the top-down induction process is blind in its first steps  so that the path to the correct concept definition is very easily lost.
　this paper offers two major contributions. first  the results reported by  giordana et al.  1  are extended  providing a more thorough analysis of the behavior of a top-down learner. a formal estimate of the  difficulty  of a learning problem is proposed  in terms of the density of sub-formulas of the target concept in the hypothesis space. moreover  an estimate of the probability of detecting one of these is also given  as a function of the target concept location with respect to the phase transition.
　second  an induction algorithm  combining a monte carlo stochastic search  brassard and bratley  1  with local deterministic search  is proposed to  partially  avoid the pitfall that causes top-down search to fail. the new algorithm directly jumps into a region of the hypothesis space where the information gain heuristics has good chance of being successful  continuing its search exploiting a classical hill climbing strategy. the complexity of this algorithm is analyzed in a probabilistic framework  and it is proposed as a measure of the difficulty of the induction task. finally  the algorithm has been experimentally evaluated on the set of hard induction problems provided by  giordana et al.  1   and it shows a good agreement with the theoretical estimates.
1	covering test in relational learning
a key step in machine learning is the covering test  i.e.  checking whether an example belonging to a learning set e verifies a hypothesis  mitchell  1 . in relational learning is usually represented as a conjunction of ground literals   where the 's are constants belonging to a given set  muggleton and de raedt  1   and is a conjunction of non-ground literals containing the variables . the covering test checks whether a first order logic formula has at least one model in the universe
 . to this aim  a substitution of the variables with constants that satisfies is searched for. 1.
　this search process shows a phase transition for critical values of the number of constants 's in the universe  and the number of literals in the hypothesis  hogg et al. 
1; prosser  1; botta et al.  1 . the graph in the plane of the probability that a model of exists in is reported in figure 1 a   for a set of covering problems generated according to a random model described in  botta et al.  1 . in the following  we will refer to  phase transition  as to the locus of the points for which the probability that a model exists is . the phase transition is located in correspondence of the  waterfall  in figure 1 a . to the left of the phase transition  in the yesregion  i.e.  the high plateau in figure 1 a   almost any example verifies any hypothesis. to the right of the phase transition  in the low plateau  i.e.  the no-region  almost no example verifies any hypothesis. the steep step corresponds to the  mushy  region  where the probability of a model existing drops from 1 to 1.in this region  the complexity of the search reaches its maximum  as reported in figure 1 b .more details can be found in  giordana et al.  1; botta et al.  1 .

	 a 	 b 
figure 1: example of phase transition in the covering test problem. all hypotheses have four variables and contain only binary predicates. each predicate is associated to a table with 1 tuples in each example.  a  probability that a model exists for 1 variables.  b  corresponding complexity peak for the search of a model.
　if the number of variables in the formulas changes  the location of the phase transition moves  as described in figure
1.

figure 1: dependency of the phase transition location upon the number of variables in a hypothesis.
1	top-down induction
let us consider a learning set	and a conjunctive concept
 . every pair   with   represents a covering test in the plane. a learning problem corresponds then to a cloud of points  giordana et al.  1 . in the special case that the number of constants in all examples is the same  the entire learning problem corresponds to a single point. in a similar way  any inductive hypothesis   built up by a learner in the attempt of finding   or at least a good approximation of it  can be represented as a cloud of points  or a single point  in the plane. then  an induction process that proceeds by generating a sequence of hypotheses will describe a family of lines  or a single line  parallel to the axis.
　in order to investigate how the presence of a phase transition in the plane may affect the behavior of a relational learner  an extensive experimentation has been done using a large set of artificially generated learning problems 1. the results have been reported and discussed in  giordana et al.  1 . here experimental setting and the findings are recalled  in order to state the base line for the work described in this paper.
　the artificial learning problem set has been generated according to the following procedure. given and   let be a point in the plane . a set of 1 points has been sampled in the plane . then  for each point  a learning problem has been built up from the 1-tuple
	. a target concept	with	liter-
als and variables has been built up  and then a training set and a test set have been generated. let be a set of
constants; every example is a collection of relational tables of size obtained by sampling the binary table . in order to generate balanced training and test sets in each point of the plane  the random generation of the examples has been modified in order to obtain examples with models also in the no-region  and examples without models also in the yes region  see  giordana et al.  1  for more details .
the result has been the generation of training and test sets  and   each one with 1 positive and 1 negative ex-
amples in each	point.
　the experimentation has been done primarily using foil  quinlan  1 . a problem has been considered solved when the concept description found on was at least 1% accurate on . however  other learners  such as smart+  botta and giordana  1  and g-net  anglano et al.  1   have also been tried for comparison  reporting a strong agreement with foil both for the positive and for the negative cases. the results can be summarized as in the following. first  all tried learning strategies always end up generating concept descriptions lying on  or very close to  the edge of the phase transition. as discussed by  giordana and saitta  1   this phenomenon systematically occurs both in artificial and in real learning problems  as soon as the generated hypotheses involve more than three variables.
　second  a large  blind spot  located across the mushy region has been found: when the target concept lies there  no tried learner has been able to find any good generalization of the target concept  see figure 1 . quite surprisingly  learning problems become again solvable by top-down induction when they are located in the no-region  to the far right of the phase transition.
　finally  the computational complexity of matching inductive hypotheses lying in the mushy region may render the whole induction process infeasible. a possible way out has been proposed by  giordana and saitta  1   who suggested to use a stochastic algorithm for on-line estimation of the complexity  at the possible expenses of correctness.
1
figure 1: relational learning with foil: failure region  legend  .   and success region  legend  +    for	and
   . the contour plot corresponds to the value of the probability that randomly generated covering test is positive.
1	why top-down induction is misled
before possibly designing new heuristics  it is necessary to clearly understand why foil and the other learners fail in the blind spot. a preliminary analysis showed that heuristics relying on the number of models  hypotheses have on the examples  quinlan  1; botta and giordana  1; rissanen  1   are not reliable in the yes-region  because both wrong and correct generalizations of the target concept have there similar numbers of models both on the positive and on the negative examples. in support to this claim  figure 1 a  plots the average number of models a random hypothesis has as a function of   for . it is clear that an increase by 1 in the number of variables induces a large increase of and an even larger one on 's standard deviation . actually  when the standard deviation is larger than the number of positive examples in the learning set. then  even if we assume that generalizations of the target concept have at least one model more on the positive examples that on the negative ones  these generalizations cannot be distinguished from a purely random hypothesis. on the contrary  for hypotheses with 1 or 1 variables only  is much smaller  and we may expect that  provided that a correct generalization of with only two or three variables exists  it should be far easier to find it than any other one having four variables. this conjecture agrees with the fact that many solutions actually generated by foil have only three variables  even though the target concept is described by a formula with four variables.
　a more detailed analysis is reported in figure 1  where the line corresponds to the phase transition for . in figure 1 a  the  +  denote the locations of a set of target concepts
   whose description contains four variables. figure shows the location of the solutions found by foil. most of

figure 1: number	of models of a random hypothesis versus
	  for various values of	.  a 	for	 	and	.
 b  standard deviation of	.
these solutions are generalizations of with four variables  because no acceptable generalizations with three or less variables could be found. on the contrary  figure shows the locations of a set of target concepts whose description still contains four variables  but for which foil was able to find generalizations with no more than three variables  reported in figure  . problems in figure 1 b  are then easier to solve than problems in figure 1 a : this result corresponds to the fact that problems in figure 1 a  are closer to the phase transition than problems in figure 1 b . we observe that the probability of a problem being correctly approximated by a formula with only three variables increases with the number of literals in . in fact  when contains more literals  the number of its subformulas that are correct generalizations of increases as well; then  we may expect that at least some of them contains a number of variables smaller than 's. moreover  the difficulty of a learning problem increases when decreases  whereas the critical value increases. when the only solutions found have three variables. in conclusion  most learning problems in the no-region have been solved only because approximations with only three variables existed. when more than three variables are required  becomes larger  and available heuristics for top-down search are unable to grow valid inductive hypotheses starting from the yes-region. on the other hand  we could not find any bottom-up learning algorithm capable to cope with the complexity of working directly inside the no-region.
1	a stochastic approach
in the absence of reliable heuristics  stochastic search may be a valid alternative  especially if combined with deterministic search. an example of an effective combination of a monte carlo search with deterministic search in the n-queens problem can be found in  brassard and bratley  1 . the algorithm we propose is based on a two-step strategy. the first step creates a hypothesis with a complexity  number of literals  sufficient to reach the border between the yesregion and the mushy region; is the result of random sampling of the hypothesis space. the second step consists of a general to specific search  starting from according to a hill-climbing strategy guided by the information gain heuristics.
let
while halt condition does not hold do

	m	m
figure 1: solutions generated by foil for problems in the no region.  a  problems solved with four variables;  b  problems solved with hypothesis containing only three variables;  a'  and  b'  location in the plane of the solutions generated by foil corresponding to the problems in figura  a  and  b   respectively.
1. randomly generate a hypothesis close to the mushy region
1. make more specific by following a hillclimbing strategy guided by the information gain. let be the locally best hypothesis found in trial .
1. if	is better than	then replace	with
.
end
　if the target concept has a conjunctive description in the hypothesis space   the monte carlo algorithm is very likely  in the long run  to find   or at least a good generalization of it  correct on the learning set . now  we want to answer the following question: assuming that the complexity and the number of variables in are known  what is the complexity we would expect from algorithm   as  complexity  of the algorithm we mean the minimum number of trials necessary to reach a probability  1 -   of finding  or   as a specially lucky case . if we choose
             there is a strong experimental evidence that a hill-climbing search  guided by the information gain  almost surely will find . this happens because the search explores the region where the information gain becomes reliable.
　let be the probability that is a sub-formula of   then the probability of finding in no more than steps is given by the expression:
 1 
　given and   the probability can be estimated  for a generic number of literals  as the ratio
	  where	is
the number of sub-formulas of with literals and variables      and is the total number of formulas in the hypothesis space with literals and variables. an analytical expression for
and has been given in  serra  1   under the assumption that all predicates are binary. using such expres-

figure 1: contour plot of the average proportion of subformulas of a target concept as a function of and   for and .  the parameters on the curves are the exponent of 1 
sions  the contour plot of figure 1 have been obtained for the ratio
setting   and . it is immediate to verify that decreases exponentially with   while it remains constant with respect to   after an initial exponential decay for small values of . the value can be obtained from figure 1 by setting . as depends upon  see figure 1   depends upon   too. the plots of figure 1 report the dependency of upon for in the case of  dotted line   and in the case of
 continuous line . for the phase transition occurs for smaller values of than for . consequently  the values of for are order of magnitude larger than for
     . this explain why learning a concept with three variables only is much easier than learning a concept with four variables  as it has been stated in the previous section. we notice that the value is known when the learning examples are given  because is the average number of constants occurring in them.

figure 1: proportion of correct generalizations of evaluated on the edge of the phase transition  i.e.  when the probability of solution is   versus .
1	improving the stochastic search algorithm
expression  1  and the graphs in figure 1 allow one to estimate the number of trials required to reach a confidence  1-   that algorithm has found an approximation   provided that such an approximation exists. then  the search can be planned as follows:
1. assume an initial value for the number of variables in .
1. estimate the number of trials necessary to reach the confidence level . if is too high  stop  otherwise go to the next step.
1. run	for	trials.
1. if the result returned by is acceptable  stop  otherwise increase by 1 and go to step 1.
　we notice that the cost for generating and evaluating a hypothesis in the initial random sampling phase is much lower than the cost for the subsequent hill-climbing search. more specifically  the number of hypotheses to be generated and evaluated for the hill-climbing step can be estimated by:
 1 
being an integer that experimentally has been observed to range from to . in expression  1  the term is an estimate of the number of literals to be added to a hypothesis in order to cross the phase transition  whereas the term estimates the number of alternative specializations
to be considered at each hill-climbing step.
　then  the number of hypotheses to be evaluated in the hillclimbing phase is from one to two orders of magnitude larger than the number of stochastic trials.
　a second point worth noticing is that  assuming that the information gain heuristics is reliable for any hypothesis
   it is also likely that itself be scored higher than the average when it is a sub-formula of .
1
1
m
figure 1: results obtained by algorithm . the numbers denote the minimum value that was necessary to assume for in order to solve the learning problem. when the number is prefixed by  +   it means that has been assumed  otherwise . symbol  .  means that the problem has not been solved.
　on the basis of the previous considerations we introduce a new algorithm  which can be more effective in practice than algorithm . let be the number of trials estimated by  1  in order to reach a confidence on the output of algorithm . let moreover     be a user-defined parameter.
algorithm
1. create a set of hypotheses with complexity .
1. rank hypotheses in according to their information gain with respect to the formula .
1. starting from the top ranked hypothesis  apply the hill-climbing specialization step to the best ranked hypotheses.
1. return the best description	.
　algorithm tries to limit the number of hill-climbing steps to the hypotheses looking more promising  reducing thus the computational complexity. of course  parameter is an arbitrary choice  and the confidence level is guaranteed to be reached only when . in practice  we have observed that using values of relatively small   of   tends to produce the same results as for .
　algorithm has been tested on the set of learning problems  reported in figure 1  trying to solve the problems laying in the no-region following the strategy described above. we started with the minimal hypothesis . parameter has been set   which approximately corresponds to the edge of the phase transition for and . even if for smaller values of our strategy foresees larger values of   it has been found that was able to find a solution  with this setting  also for many problems where . afterwards more expensive hypotheses has been progressively considered firstly increasing   until the value foreseen by the phase transition has been reached  or an excessive cost was
foreseen  and then setting	. a value	of
has been used everywhere. the results are reported in figure 1. comparing to figure 1  we see that many problems lying in the blind spot have been solved. in practice almost all the problems above the phase transition and the line have been solved with a complexity larger as foil but still affordable. a cluster of 1 pentium iii  1 mhz  has been used for the experiments  where every single problem required a time ranging from few minutes to several hours  an elapsed time comparable to foil on a sequential machine . when the problem was not solved we progressively increased the complexity of the hypothesis increasing .
1	discussion
we have shown that combining stochastic search with local deterministic search it is possible to learn approximated concept descriptions where no known classical algorithm was successful. even if the algorithm is used under the stringent assumption that a conjunctive concept description exists  it is not difficult to extend it in order to cope with more general concept descriptions. for instance  disjunctive descriptions can be learned by integrating with a set covering algorithm as it is made in most relational learner  quinlan  1; botta and giordana  1 .
　however  this is not the fundamental result that emerges from the framework we propose. in our opinion  the most important outcome is the method for estimating the complexity of a learning problem: given a specific hypothesis about the structure of the concept  we have a method for predicting the expected cost for testing the hypothesis. moreover  a criterion for deciding on-line when stop testing the hypothesis is provided.
　a second important result is a negative one  and concerns the possibility of learning descriptions with many variables. even under the simplistic assumption that all predicates are relevant to the concept description  the task looks hard for many concepts requiring at least 1 variables. increasing the number of variables  the complexity rises up exponentially. considering the presence of irrelevant predicates  the analysis we performed still holds  but the the density of sub-formulas of the target concept close to the phase transition becomes even more tiny  and so the difficulty will increase further.
references
 anglano et al.  1  c. anglano  a. giordana  g. lobello  and l. saitta. an experimental evaluation of coevolutive concept learning. in proceedings of the 1th international conference on machine learning  pages 1  madison  wi  july 1.
 botta and giordana  1  m.	botta	and	a.	giordana.
smart+: a multi-strategy learning tool. in ijcai-1  proceedings of the thirteenth international joint conference on artificial intelligence  pages 1  chambe＞ry  france  1.
 botta et al.  1  m. botta  a. giordana  and l. saitta. an experimental study of phase transitions in matching. in proceedings of th 1th international joint conference on artificial intelligence  pages 1  stockholm  sweden  1.
 brassard and bratley  1  g. brassard and p. bratley. algorithmics: theory and practice. prentice hall  englewood cliffs  nj  1.
 giordana and saitta  1  a. giordana and l. saitta. phase transitions in relational learning. machine learning  x:to appear  1.
 giordana et al.  1  a. giordana  l. saitta  and m. sebag amd m. botta. an experimental study of phase transitions in matching. in proceedings of th 1th international conference on machine learning  stanford  ca  1.
 hogg et al.  1  t. hogg 	b.a. huberman 	and c.p.
williams  editors. artificial intelligence: special issue on frontiers in problem solving: phase transitions and complexity  volume 1-1 . elsevier  1.
 mitchell  1  t.m. mitchell. mchine learning. mcgrow hill  1.
 muggleton and de raedt  1  s. muggleton and l. de raedt. inductive logic programming: theory and methods. journal of logic programming  1:1  1.
 prosser  1  p. prosser. an empirical study of phase transitions in binary constraint satisfaction problems. artificial intelligence  1-1  1.
 quinlan  1  r. quinlan. learning logical definitions from relations. machine learning  1-1  1.
 rissanen  1  j. rissanen. modeling by shortest data description. automatica  1-1  1.
 serra  1  a. serra. apprendimento di concetti relazionali. technical report tr1  dista  universita' del pieminte orietale  feb 1.
 smith and dyer  1  b.m. smith and m.e. dyer. locating the phase transition in binary constraint satisfaction problems. artificial intelligence  1-1  1.
 williams and hogg  1  c.p. williams and t. hogg. exploiting the deep structure of constraint problems. artificial intelligence  1-1  1.
a simple additive re-weighting strategy for improving margins
fabio aiolli and alessandro sperduti
department of computer science  corso italia 1  pisa  italy e-mail: aiolli  perso  di.unipi.itabstract
we present a sample re-weighting scheme inspired by recent results in margin theory. the basic idea is to add to the training set replicas of samples which are not classified with a sufficient margin. we prove the convergence of the input distribution obtained in this way. as study case  we consider an instance of the scheme involving a 1-nn classifier implementing a vector quantization algorithm that accommodates tangent distance models. the tangent distance models created in this way have shown a significant improvement in generalization power with respect to the standard tangent models. moreover  the obtained models were able to outperform state of the art algorithms  such as svm.
1	introduction
in this paper we introduce a simple additive re-weighting method that is able to improve the margin distribution on the training set. recent results in computational learning theory  vapnik  1; schapire et al.  1; bartlett  1  have tightly linked the expected risk of a classifier  i.e. the probability of misclassification of a pattern drawn from an independent random distribution   with the distribution of the margins in the training set. in general  it results that we can expect best performances on generalization  minimal error on test data  when most of the patterns have high margins.
　the aforementioned results are at the basis of the theory of two of the most impressive algorithms: support vector machines and boosting. either svm's and boosting effectiveness is largely due to the fact that they  directly or not  effectively improve the margins on the training set. in particular  svm explicitly finds the hyper-plane with the largest minimum margin in a dimensional-augmented space where training points are mapped by a kernel function. in this case  margin theory permits to explain impressive performances even in very high dimensional spaces where data are supposed to be more separated. most of the recent efforts in svms are in the choice of the right kernels for particular applications. for example  in ocr problems  the polynomial kernel was proven to be very effective.
　on the other side  boosting algorithms  and in particular the most famous version adaboost  produce weighted ensemble of hypotheses  each one trained in such a way to minimize the empirical error in a given  difficult  distribution of the training set. again  it has been shown  schapire  1  that boosting essentially is a procedure for finding a linear combination of weak hypotheses which minimizes a particular loss function dependent on the margins on the training set  literally
. recently  research efforts related to
boosting algorithms faced the direct optimization of the margins on the training set. for example  this has been done by defining different margin-based cost functions and searching for combinations of weak hypotheses so to minimize these functions  mason et al.  1 .
　we will follow a related approach that aims to find a single  eventually non linear  optimal hypothesis where the optimality is defined in terms of a loss-function dependent on the distribution of the margins on the training set. in order to minimize this loss we propose a re-weighting algorithm that maintains a set of weights associated with the patterns in the training set. the weight associated to a pattern is iteratively updated when the margin of the current hypothesis does not reach a predefined threshold on it. in this way a new distribution on the training data will be induced. furthermore  a new hypothesis is then computed that improves the expectation of the margin on the new distribution. in the following we prove that the distribution converges to a uniform distribution on a subset of the training set.
　we apply the above scheme to an ocr pattern recognition problem  where the classification is based on a 1-nn tangent distance classifier  simard et al.  1   obtaining a significant improvement in generalization. basically  the algorithm builds a set of models for each class by an extended version of the learning vector quantization procedure  lvq  kohonen et al.  1   adapted to tangent distance. in the following we will refer to this new algorithm as tangent vector quantization  tvq .
　the paper is organized as follows. in section 1  we introduce the concept of margin regularization via the input distribution on the training set. specifically  we present the margin re-weighting strategy  which holds the property to guarantee the convergence of the input distribution. in section 1  we introduce a definition for the margins in a 1-nn scheme that considers the discriminative ratio observed for a particular pattern  and in section 1 we define the tvq algorithm. finally  in section 1 we present empirical results comparing tvq with other 1-nn based algorithms  including svm.
1	regularization of the margins
when learning takes place  the examples tend to influence in a different way the discriminant function of a classifier. a discriminant function can be viewed as a resource that has to be shared among different clients  the examples . often  when pure empirical risk minimization  erm  principle is applied  that resource is used in a wrong way since  with high probability  it is almost entirely used by a fraction of the training set. margin theory formally tells us that it is preferable to regularize the discriminant function in such a way to make the examples sharing more equally its support.
　inspired on the basic ideas of margin optimization  here  we propose a simple general procedure applicable  eventually  to any erm-based algorithm. it permits to regularize the parameters of a discriminant function so to obtain hypotheses with large margins for many examples in the training set.
　without generality loss we consider the margin for a training example as a real number  taking values in   representing a measure of the confidence shown by a classifier in the prediction of the correct label. in a binary classifier  e.g.
the perceptron  the margin is usually defined as where is the target and is the output computed by the clas-
sifier. anyway  it can be easily re-conduced to the range by a monotonic  linear or sigmoidal  transformation of the output. in any case  a positive value of the margin must correspond to a correct classification of the example.
　given the function that  provided an hypothesis   associates to each pattern its margin  we want to define a lossfunction that  when minimized  permits to obtain hypotheses with large margins  greater than a fixed threshold   for many examples in the training set. for this  we propose to minimize a function that  basically  is a re-formulation of svm's slack variables:
 1 
where	is a training set with	examples  and
　if   and 1 otherwise. the function is null for margins higher than the threshold and is linear with respect to the values of the margins when they are below this threshold.
　we suggest to minimize indirectly via a two-step iterative method that  simultaneously   1  searches for an a priori distribution for the examples that  given the current hypothesis   better approximates the function and  1  searches for a hypothesis  e.g. by a gradient based procedure  that  provided the distribution   improves the weighted function
 1 
　this new formulation is equivalent to that given in eq.  1  provided that converges to the uniform distribution on the -mistakes  patterns that have margin less than the threshold .
-margin re-weighting strategy
input:
t: number of iterations;
: hypotheses space;
: margin threshold;
: bounded function;
: training set;
initialize
 initial hypothesis ;for	 	;for
begin findsuch that
;
;; end return;	figure 1: the	-margin re-weighting strategy.
　the algorithm  shown in figure 1  consists of a series of trials. an optimization process  that explicitly maximizes the function according to the current distribution for the examples  works on an artificial training set   initialized to be equal to the original training set . for each   replicas of those patterns in that have margin below the fixed threshold are added to augmenting their density in and consequently their contribution in the optimization process. note that denotes the number of occurrences in the extended training set of the pattern .
　in the following  we will prove that the simple iterative procedure just described makes the distribution approaching a uniform distribution on the -mistakes  provided that is bounded.
1	convergence of the distribution
for each trial   given the margin of each example in the training set   we can partition the training sample as
where is the set of -mistakes and is the complementary set of -correct
patterns.
　let denote and let be the number of occurrences of pattern in at time   with density
　　　　　. moreover  let	be a suitable function of such that	.
let be the update rule for the number of occurences in   where is bounded and takes values in  note that may change at different iterations but it is independent from  . it's easy to verify that for each because of the monotonicity of   and that
with the number of iterations. in fact

at time	we have


　first of all we show that the distribution converges. this can be shown by demonstrating that the changes tend to zero
with the number of iterations  i.e.  we have


which can be easily bounded in module by a quantity that tends to 1:


we now show to which values they converge. let and be  respectively  the cumulative number and the mean ratio
of	-mistakes for	on the first	epochs 	  then


　given the convergence of the optimization process that maximizes in eq.  1   the two sets and are going to become stable and the distribution on will tend to a uniform distribution in  where   and will be null elsewhere  where  . this can be understood in the following way as well.
　given the definition of the changes made on the gamma values on each iteration of the algorithm  we calculate the function that we indeed minimize. since
	  after some algebra  we can rewrite	as:

thus  when    for which the minimum of function

is reached. note that  the minimum of	is consistent with the constraint	.
　in general  the energy function is modulated by a term decreasing with the number of iterations  dependent on the used but independent from gamma  that can be viewed as a sort of annealing introduced in the process. in the following  we study a specific instance of the margin re-weighting strategy.
1	margins in a 1-nn framework and tangent distance
given a training example   and a fixed number of models for each class  below  we give a definition of the margin for the example when classified by a distance based 1-nn classifier.
　given the example   let and be the squared distances between the nearest of the positive set of models and the nearest of the negative sets of models  respectively. we can define the margin of a pattern in the training set as:
		 1 
this formula takes values in the interval representing the confidence in the prediction of the 1-nn classifier. higher values of the 's can also be viewed as an indication of a higher discriminative power of the set of models with respect to the pattern. moreover  a pattern will result correctly classified in the 1-nn scheme if and only if its margin is greater than zero.
　in this paper  we are particularly interested in distances that are invariant to given transformations. specifically  we refer to the one-sided tangent distance  simard  1; hastie et al.  1; schwenk and milgram  1b; 1a  
which computes the distance between a pattern and a pattern as the minimum distance between and the linear subspace approximating the manifold induced by transforming the pattern according to a given set of transformations:
 1 
if the transformations are not known a priori  we can learn them by defining  for each class   a  one-sided  tangent distance model   compounded by a centroid  i.e.  a prototype vector for class   and a set of tangent vectors
 i.e.  an orthonormal base of the linear subspace
	   that can be written as	.
　this model can be determined by just using the positive examples of class   i.e.   as
 1 
which can easily be solved by resorting to principal compo-
nent analysis  pca  theory  also called karhunen-loeve＞ expansion. in fact  equation  1  can be minimized by choosing as the average over all available positive samples   and as the set of the most representative eigenvectors  principal components  of the covariance matrix
	.
　the corresponding problem for the two-sided tangent distance can be solved by an iterative algorithm  called  twosided  hss  based on singular value decomposition  proposed by hastie et al.  hastie et al.  1 . when the onesided version of tangent distance is used  hss and pca coincide. so  in the following  the one sided version of this algorithm will be simply referred as to hss.
　given a one-sided tangent distance model   it is quite easy to verify that the squared tangent distance between a pattern and the model can be written as:
 1 
where   and denotes the transpose of . consequently  in our definition of margin  we have
	  and	.
　given this definition of margin  we can implement the choice of the new hypothesis in the -margin re-weighting strategy by maximizing the margin using gradient ascent on the current input distribution.
1	improving margins as a driven gradient ascent
considering the tangent distance formulation as given in equation  1  we can verify that it is defined by scalar products. thus  we can derivate it with respect to the centroid and the tangent vectors of the nearest positive model obtaining:

considering that and we can compute the derivative of the margin with respect to changes in the nearest positive model:


a similar solution is obtained for the nearest negative model since it only differs in changing the sign and in exchanging indexes and . moreover  the derivatives are null for all the other models.
　thus  we can easily maximize the average margin in the training set if for each pattern presented to the classifier we move the nearest models in the direction suggested by the gradient. note that  like in the lvq algorithm  for each training example  only the nearest models are changed.
　when maximizing the expected margin on the current distribution   i.e.    for each model we have:

where is the usual learning rate parameter. in the algorithm  see figure 1   for brevity  we will group the above variations by referring to the whole model  i.e.  
tvq algorithm
input:
t: no. of iterations;
q: no. of models per class;
　　: margin threshold; initialize
	 	;
	  initialize	with random
　models; for
;
	  select	s.t.	and
are the nearest  positive and negative  models. compute as in eq.  1  and accumulate the changes on the nearest models


and orthonormalize its
tangents;
	  update the distribution	by the rule

normalize	's such that	;
;
endfigure 1: the tvq algorithm.
1	the tvq algorithm
the algorithm  see figure 1  starts with random models and a uniform distribution on the training set. for each pattern  the variation on the closest positive and the closest negative models are computed accordingly to the density of that pattern on the training set . when all the patterns in are processed  the models are updated performing a weighted gradient ascent on the values of the margin. moreover  for each pattern in the training set such that the value of the margin is smaller than a fixed value  the distribution is augmented. the effect is to force the gradient ascent to concentrate on hardest examples in the training set. as we saw in section 1 the increment to the distribution is simply the effect of adding a replica     of incorrectly classified patterns to the augmented training set.
　the initialization of the algorithm may be done in different ways. the default choice is to use random generated models however  when the training set size is not prohibitive  we can drastically speed up the algorithm by taking as initial models the ones generated by any algorithm  e.g.  hss . how-
methodparameterserr%hss 1-sided1 tangents1lvq 11 codebooks1td-neuron1 tangents1hss 1-sided1 tangents1euclidean 1-nnprototypes1svmlinear1svmpoly d=1.1svmpoly d=1.1svmpoly d=1.1table 1: test results for different 1-nn methods.
ever  in the case of multiple models per class the initialization through the hss method would generate identical models for each class and that would invalidate the procedure. a possible choice in this case  is to generate hss models by using different random conditional distributions for different models associated to the same class. another solution  which is useful when the size of the training set is relatively large  is to initialize the centroids as the average of the positive instances and then generating random tangents. experimental results have shown that the differences on the performance obtained by using different initialization criteria are negligible. as we could expect the speed of convergence with different initialization methods may be drastically different. this is due to the fact that when tvq is initialized with hss models it starts with a good approximation of the optimal hypothesis  see figure 1-    while random initializations implicitly introduce an initial poor estimate of the final distribution due to the mistakes that most of the examples do on the first few iterations.
1	results
we compared the tvq algorithm versus svms and other 1-nn based algorithms: 1-sided hss  1-sided hss  tdneuron  sona et al.  1   and lvq. the comparison was performed using exactly the same split of a dataset consisting of 1 digits randomly taken from the nist-1 dataset. the binary 1 digits were transformed into 1-grey level 1 images by a simple local counting procedure1. the only preprocessing performed was the elimination of empty borders. the training set consisted of 1 randomly chosen digits  while the remaining digits were used in the test set.
　the obtained results for the test data are summarized in table 1. for each algorithm  we reported the best result  without rejection  obtained for the dataset. specifically  for the svm training we used the svm package available on the internet1. different kernels were considered for the svms: linear and polynomial with degrees 1 and 1  we used the default for the other parameters . since svms are binary classifiers  we built 1 svms  one for each class against all the others  and we considered the overall prediction as the label with higher margin. the best performance has been obtained
q=1= 1=1=1= 1111111.1.1.1.1-table 1: test results for tvq.
with a polynomial kernel of degree 1. we ran the tvq algorithm with two different values for and four different architectures. moreover  we ran also an experiment just using a single centroid for class  i.e.    with . the smaller value for has been chosen just to account for the far smaller complexity of the model.
　in almost all the experiments the tvq algorithm obtained the best performance. results on the test data are reported in table 1. specifically  the best result for svm is worst than almost all the results obtained with tvq. particularly interesting is the result obtained by just using a single centroid for each class. this corresponds to perform an lvq with just 1 codebooks  one for each class.
　in addition  tvq returns far more compact models allowing a reduced response time in classification. in fact  the 1nn using polynomial svms with   needs 1 support vectors  while in the worst case the models returned by the tvq involve a total of 1 vectors  one centroid plus 1 tangents for each model .
　in figure 1  typical error curves for the training and test errors  1-    as well as the margin distributions on the training set  1-   and the induced margin distribution on the test set  1-   are reported. from these plots it is easy to see that the tvq doesn't show overfitting. this was also confirmed by the experiments involving the models with higher complexity and smaller values of . moreover  the impact of the -margin on the final margin distribution on the training set is clearly shown in 1-   where a steep increase of the distribution is observed in correspondence of at the expenses of higher values of margin. even if at a minor extent  a similar impact on the margin distribution is observed for the test data.
　in figure 1 we have reported the rejection curves for the different algorithms. as expected  the tvq algorithm was competitive with the best svm  resulting to be the best algorithm for almost the whole error range.
1	conclusions
we proposed a provably convergent re-weighting scheme for improving margins  which focuses on  difficult  examples. on the basis of this general approach  we defined a vector quantization algorithm based on tangent distance  which experimentally outperformed state of the art classifiers both in generalization and model compactness. these results confirm that the control of the shape of the margin distribution has a great effect on the generalization performance.
1	1	1 1	1	1	-1	1.1.1.1.1-1	1.1.1.1.1
figure 1: tvq with 1 tangents  and : comparison with different initialization methods; test and training error; cumulative margins on the training set at different iterations; cumulative margins on the test set at different　when comparing the proposed approach with svm  we may observe that  while our approach shares with svm the statistical learning theory concept of uniform convergence of the empirical risk to the ideal risk  it exploits the input distribution to directly work on non-linear models instead of resorting to predefined kernels. this way to proceed is iterations.

figure 1: detail of rejection curves for the different 1-nn methods. the rejection criterion is the difference between the distances of the input pattern with respect to the first and the second nearest models.
very similar to the approach adopted by boosting algorithms. however  in boosting algorithms  several hypotheses are generated and combined  while in our approach the focus is on a single hypothesis. this justifies the adoption of an additive re-weighting scheme  instead of a multiplicative scheme which is more appropriate for committee machines.
acknowledgments
fabio aiolli wishes to thank centro meta - consorzio pisa ricerche for supporting his phd fellowship.
references
 bartlett  1  p.l. bartlett. the sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. ieee trans. on infor. theory  1 :1  1.
 hastie et al.  1  t. hastie  p. y. simard  and e. sa：ckinger. learning prototype models for tangent distance. in g. teasauro  d. s. touretzky  and t. k. leen  editors  advances in neur. inform. proc. systems  volume 1  pages 1. mit press  1.
 kohonen et al.  1  t. kohonen  j. hynninen  j. kangas  j. laaksonen  and k. torkkola. lvq pak: the learning vector quantization program package. technical report a1  helsinki univ. of tech.  lab. of computer and inform. sci.  january 1.
 mason et al.  1  l. mason  p. bartlett  and j. baxter. improved generalization through explicit optimization of margins. technical report  dept. of sys. eng.  australian national university  1.
 schapire et al.  1  r.e schapire  y. freund  p. bartlett  and w.s. lee. boosting the margin: a new explanation for the effectiveness of voting methods. an. of stat.  1   1.
 schapire  1  r. schapire. theoretical views of boosting. in computational learning theory: proc. of the 1th european conference  eurocolt'1  1.
 schwenk and milgram  1a  h. schwenk and m. milgram. learning discriminant tangent models for handwritten character recognition. in intern. conf. on artif. neur. netw.  pages 1. springer-verlag  1.
 schwenk and milgram  1b  h. schwenk and m. milgram. transformation invariant autoassociation with application to handwritten character recognition. in g. teasauro  d. s. touretzky  and t. k. leen  editors  advances in neur. inform. proc. systems  volume 1  pages 1. mit press  1.
 simard et al.  1  p. y. simard  y. lecun  and j. denker. efficient pattern recognition using a new transformation distance. in s. j. hanson  j. d. cowan  and c. l. giles  editors  advances in neural information processing systems  volume 1  pages 1. morgan kaufmann  1.
 simard  1  p. y. simard. efficient computation of complex distance metrics using hierarchical filtering. in j. d. cowan  g. teasauro  and j. alspector  editors  advances in neur. inform. proc. systems  volume 1  pages 1. morgan kaufmann  1.
 sona et al.  1  d. sona  a. sperduti  and a. starita. discriminant pattern recognition using transformation invariant neurons. neur. comput.  1 :1  1.
 vapnik  1  v. vapnik. statistical learning theory. wiley  1.

machine learning and
data mining
knowledge acquisition

knowledge analysis on process models
jihie kim and yolanda gil information sciences institute
university of southern california
1 admiralty way
marina del rey  ca 1  u.s.a. jihie isi.edu  gil isi.edu

abstract
helping end users build and check process models is a challenge for many science and engineering fields. many ai researchers have investigated useful ways of verifying and validating knowledge bases for ontologies and rules  but it is not easy to directly apply them to checking process models. other techniques developed for checking and refining planning knowledge tend to focus on automated plan generation rather than helping users author process information. in this paper  we propose a complementary approach which helps users author and check process models. our system  called kanal  relates pieces of information in process models among themselves and to the existing kb  analyzing how different pieces of input are put together to achieve some effect. it builds interdependency models from this analysis and uses them to find errors and propose fixes. our initial evaluation shows that kanal was able to find most of the errors in the process models and suggest useful fixes including the fixes that directly point to the sources of the errors.
1	introduction
building process models is essential in many science and engineering fields. since some of these processes are quite complex  it is useful to provide tools that enable users to specify process models. figure 1 shows an example of a process model in cell biology to describe how a lambda virus invades a cell. to be able to specify such a model  the user has to specify each of the individual steps and connect them appropriately. there are different types of connections among the steps  including decomposition links between steps and substeps  ordering constraints  disjunctive alternatives  etc. even in process models of small size  the number of steps and connections between them is large enough that users would benefit from the assistance of intelligent acquisition tools that help them specify process models correctly. for example  the user may forget to specify the links between the steps  or may specify wrong links. we found such errors in a biological weapon production model built by a subject matter expert. even though it can be considered as a relatively simple

figure 1: example process model.
model  consisting of 1 steps  and many people had looked at it  there were at least two errors:  a  there were two steps that were both specified as the second substep of the same step;  b  there were two sequential substeps whose ordering information was missing. although these may be simple errors  in more complex models users may generate more serious problems and have difficulty noticing and fixing them.
　graphical tools to lay out a process model and draw connections among steps abound  but the tools are limited to simple checks on the process models because there is no semantics associated to the individual steps. in contrast  we assume a knowledge-rich environment that enables users to specify what the process and its steps mean  what they should accomplish and how  and what the features of the objects involved in those steps are. in order to check a process model such as the one in the figure thoroughly  the system would have some background knowledge about what a cell is  that it can be entered because it is a physical object with a barrier  etc. it would also be helpful for the tool to know what a change of location is  and that the enter step implies a change of location of its agent. domain ontologies and upper or middle level ontologies are commonly used to represent this kind of background knowledge. with this context  the tool can be much more helpful in checking that the process model makes sense within the background knowledge that it has.
　past research in validation and verification of knowledge bases addresses the detection of errors in rule bases  preece & shinghal  1; o'keefe & o'leary  1  or in ontologies  mcguinness et al.  1  and has not addressed process models specifically. research on interactive tools to acquire planning knowledge is also related  chien  1; myers  1; huffman & laird  1   but their focus is on acquiring knowledge about how to generate plans instead of acquiring the specific plans themselves. plan authoring tools are closer in spirit to the process authoring tools that we are aiming for  and as such kanal could be used to check errors in plans produced by plan editing tools. formal analyses of partial-order and hierarchical planning algorithms define some desirable properties of plans  such as justifiability and correctness  kambhampati  knoblock  & yang  1; yang  1; tate  1 . generative planners such as sipe 
noah  and nonlin  wilkins  1; sacerdoti  1; tate  1  use critics that detect problems in the plans that they generate while planning. much of the work on planning does not exploit background knowledge and ontologies  which we believe is crucial technology to advance the state of the art in process modeling.
　we have developed a tool that checks process models specified by a user  reports possible errors in the models  and generates specific suggestions to the user about how to fix those errors. our system is called kanal  knowledge analysis   and it helps users build or modify process models by detecting invalid statements and pointing out what additional knowledge needs to be acquired or what existing knowledge needs to be modified. our approach is inspired on previous work on expect using interdependency models  swartout & gil  1; kim & gil  1 . these models of the interdependencies between different pieces of knowledge can be derived by analyzing how knowledge is used during problem solving. by analyzing these interdependencies  a knowledge acquisition tool can detect inconsistencies and missing knowledge and alert the user of potential problems in the knowledge base. finally  based on the context provided by the interdependency models the tool can guide the user in fixing these problems by correcting inconsistencies and by adding further knowledge. kanal analyzes the interdependencies among the individual steps of a process model. for example  a simulation of the execution of the steps allows kanal to analyze interdependenciesbetween the conditions and effects of different steps  such as that the required conditions for each step are met when the step is supposed to take place  and that the expected effects of the overall process are in fact obtained.
　the paper begins by describing the representation of process models assumed in kanal. then we describe how interdependency models can be applied to check various features of process models. next  we present the current implementation of kanal and the algorithms that it uses to detect different kinds of errors. finally  we present the results from a preliminary evaluation that show that kanal can detect most of the errors that were randomly introduced in originally error-free process models  suggesting useful fixes that often point directly to the source of the errors.
1	representing process models
this section describes briefly our representation of process models  which is consistent with current efforts on standard languages and process ontologies  such as pddl  ghallab et al.  1  and nist's psl  tissot & gruninger  1 .
　a process model is composed of a number of  sub steps. each individual step has preconditions and effects  where the preconditions specify the conditions needed to be satisfied to activate the step and the effects describe changes that result from the execution of the step. for example  an  enter  step has a precondition that the objects to enter should be near the entrance of a container object. its effect can include a location change from outside of a space to inside of the space and also a status change to being contained within the container. these can be represented as a precondition list and add/delete lists  as in strips operators .
　the steps within a process model are connected to other steps through different kinds of links including:
decomposition links: users can specify superstep/substep relations. for example  an invade step can have arrive  enter and take control as its substeps  and each of these substeps can have their own substeps.
temporal links: users can specify ordering constraints among the steps. for example  in modeling virus invasion  the take control step should follow the enter step.
disjunctive links: there might be more than one way of performing a given task  and the alternatives can be represented by disjunctive links. for example  the dna of a lambda virus can either start its replication right after entering a cell or be integrated with the host chromosome before the replication.
causal links: if the editor allows users to specify enablement/disablement between steps  since kanal can compute the actual causal relationships among the steps from the simulation results  by examining the outcome of the steps and the preconditions checked by other steps   the user-specified causal links can be used for validating the model.
　each step can have several roles. for example  in an enter step an object can play the role of an agent and another object can play the role of the container being entered. a general description of an enter step can be instantiated for the virus invasion process by assigning the concept virus to the agent role of enter and the concept cell to the container role. these role assignments cause further interdependencies in the knowledge base  since the objects assigned to the roles have their own constraints and definitions that must be consistent with those of the process models and their steps.
1	acquiring process models: the end-to-end system
currently  kanal is being developed as a module within an ambitious end-to-end system that will support subject matter experts entering domain knowledge as part of the darpa rapid knowledge formation  rkf  program. in this project  users will build process models is by using  conceptcomposition   clark & porter  1 . users can build process models by retrieving components  actions and objects  and then connecting them using various kinds of links. the user interface

figure 1: the kanal interface for building process models.
and the component library have not been fully implemented and integrated with kanal  although a preliminary version of the three was done to illustrate the approach with a small scale scenario of a process model for virus invasion.
　figure 1 is an interface that we built to show how the component approach can be used to build process models by linking various objects in the knowledge base. the user has selected the invade component to start building the virus invasion model. the  agent  role can be assigned to the virus concept  the enter component can be linked as a subevent  and so on.
　although kanal is built to check process models constructed by concept composition  it can also be used for checking process models in other environments. for example  providing procedural knowledge required by intelligent tutoring systems has been an ongoing challenge  and they sometimes use simulators to build and refine their models  scholer et al.  1 . we are also investigating the use of kanal to help teachers formalize process models that will be used as lessons by a tutoring system.
1	using interdependency models
in past work  interdependencymodels have been successfully used in building and checking problem-solving knowledge in expect  kim & gil  1; kim & gil  1 . they have been used in analyzing how individual components of a knowledge base are related and interact when they are used during problem solving. an example of interdependency between two pieces of procedural knowledge is that one may be used by the other to achieve a subgoal. other kinds of interdependency models include interdependencies between factual knowledge and procedural knowledge. interdependency models can point out missing pieces in solving a problem and be used to predict what pieces are related and how. in this paper  we show a novel use of interdependencymodels to check process models.
　to guide users in developing process models  kanal builds interdependencies among objects in the knowledge base  and uses them to perform two kinds of checks: static checks and dynamic checks. static checks are performed by posing questions about various features of the process model  and dynamic checks are performed by simulating the execution of the process model. our initial work to date has focused on dynamic checks.
　in order to perform static checks  we plan to maintain a list of sample query templates  such as retrieving the values of different links  types of roles assigned to steps  etc. a list of instantiated queries can be generated for a particular model using these templates. users could select key queries from this list and also specify the answers expected from the queries. a trace of the answer to a query can be considered as a model of the interdependencies in that it reflects how different pieces of knowledge are put together to generate the answer.
　dynamic checks can be done on the simulated execution of the process model. the simulation results show how different steps are related to each other  including temporal ordering and causal relationships. the results also show how certain effects are produced by a set of sequences of steps. the resulting interdependency model enables checking if all the steps are properly linked  all the preconditions of each step are satisfied during the simulation  all the expected effects can be achieved  there are no unexpected effects  there are no impossible paths  etc. also the interdependencies can point to potential ways of solving errors and gaps in the model  such as changing ordering constraints to reinstate disabled effects  finding steps that can generate unachieved effects  adding missing links  etc.
　our current work focuses on dynamic checks  using the simulation as a tool to generate interdependencymodels. the next section describes how we check the process models using simulation results.
1	checking process models with kanal
our current implementation is built using the km knowledge representation and reasoning system  clark & porter  1   and invokes its simulator to generate alternative simulations of a process model. km's simulation of process models can be seen as a symbolic execution of a linearization of the process model using skolem instances. km provides a function that can execute a step in a given situation and create a new situation based on the add/delete lists of the step. kanal uses this function to execute the steps in the given model and check various kinds of problems. whenever a precondition test fails or any of the events are undoable  a step is undoable when not all of its previous steps were executed   kanal interrupts the simulation and reports the unreached steps  a step is unreached when the simulation stops before the step is simulated . it also reports other problems found until it finishes the simulation. from the simulation results  including the problems detected during the simulation  kanal can compute potential fixes based on interdependency models.
　the subsections below describe each type of check that kanal performs in detail.
1	checking unachieved preconditions
a precondition is not achieved either because there is no previous step that produces the needed effect or because some previous steps undo the precondition. for example  an integrate step of a virus dna into a host chromosomemay undo a precondition  that the viral dna is exposed  of a step to synthesize protein from dna. to be able to synthesize the viral protein needed for the replication  an additional dis-integrate step that can reinstate the exposure is required.
the general algorithm to check preconditionsis as follows:
1. detect problem
 a  run simulation with skolem instances
 b  collect failed step s 
 c  collect unachieved preconditions of failed step
 d  show them to user
1. help user fix problem
 a  suggest that there are missing steps:
- find components in the knowledge base that have the effects needed as preconditions by the failed step and suggest inserting one of these components somewhere within the current process model before the failed step
 b  suggest that there are missing ordering constraints: - find steps that were executed before the failed step that may have effects that undid the unachieved preconditions
- find steps that follow the failed step and have effects that assert the unachieved precondition and suggest inserting an ordering constraint between those steps and the failed step
 c  suggest modifying the step whose preconditions were not achieved
　for the above type of failure  kanal suggests  a  adding a dis-integrate step   b  changing or adding ordering constraints for the integrate step  and  c  deleting or modifying the synthesize step. suggestion  a  would be the one that user is looking for in order to fix the problem in this example.
1	checking expected effects
kanal informs the user of the effects of each step during simulation. this allows users to check that the process occurs as they anticipated. in addition  users can specify to kanal what they expect to be the case after the overall process happens. these expected effects are what the user indicates should result from the simulation and/or the postconditions of the composed process model. for example  the user may expect that after a virus invasion of a cell  the viral nucleic acid should be located inside the cell. these expected effects can be checked by looking at the results from the simulation. the simulation results are represented as an accumulation of added and deleted facts. since there may be multiple disjunctive branches in the model  the results from different paths are accumulated separately. kanal checks the results from each path to see if all of them satisfy the expectation. if there are unachieved effects  kanal can propose either to add new steps that would achieve them or to modify existing steps.
　currently kanal checks for expected effects  but does not check explicitly for unexpected effects. we are planning to highlight any unexpected effects to let the users examine whether they should in fact occur. the algorithm is as follows:
1. ask user to specify expected effects 1. detect problem
 a  run simulation with skolem instances
 b  collect unachieved effects from each path andrecord the steps in the failed paths
 c  show them to user
1. help user fix problem
 a  suggest that there are missing steps: - find components in the knowledge base that have the effects needed and suggest inserting one of these components somewhere within the current process model
 b  suggest modifying steps:
- find steps that may have effects that can potentially change the role values of the unachieved effects and suggest modifying those steps to achieve the effects needed
 c  suggest that there are missing ordering constraints: - find steps that may have effects that undid the expected effects and find actions that assert the expected effects and suggest inserting an ordering constraint in order to maintain the expected effect where needed
　following the example above  suppose that the user specifies that after the invasion occurs the viral nucleic acid should be located inside the cell. suppose also that the user forgot to add the enter step in the model of virus invasion. kanal suggests  a  adding new steps  such as move or enter  that would change the location of the virus   b  modifying arrive since it is an existing step that causes the virus to change location  or  c  changing/addingordering constraints among these steps. the user would choose option  a  to add an enter step  and the problem would be fixed.
1	checking unordered steps
sometimes the user may either forget to specify links between the steps  or may specify wrong links as in the biological weapon production example mentioned in the introduction. these problems may be detected by mapping the steps to the components in the knowledge base that have certain ordering constraints already specified for their substeps  or by running simulation and doing the checks as describe above for unachieved preconditions and effects. during the simulation  kanal walks through the steps and substeps using the user specified decomposition links and ordering constraints. the simulation is interrupted if some steps cannot be reached because of lack of orderingconstraints or the steps are undoable. kanal highlights these problems and proposes changing or adding ordering constraints among the steps.  we do not show the detailed algorithm here because of lack of space. 
1	checking inappropriate execution of steps
kanal can also find modeling errors by watching the execution of steps. for example  if some of the assertions to be deleted by a step are not true in the situation where the step is executed  these assertions are reported. also  if a step produces no effect  i.e.  it does not delete or add any assertions  then kanal reports such problem as well. this type of problem can occur when the step's roles or attributes are assigned to the wrong objects during the composition. also  if its previous steps have incorrect assignments already and produced unexpected effects  then the following steps cannot be executed appropriately.
　kanal proposes modifying the steps by changing their role assignments or modifying previous steps.
1	checking invalid expressions
during the simulation  kanal checks the truth/falsity of many assertions  especially for the precondition tests and the expected effect tests. whenever there are objects tested but undefined  kanal reports the problem of accessing undefined objects  or invalid expressions .
1	checking loops
loops are not necessarily a problem in process models. for example  the replication of dna can be repeated multiple times. however  they can be unintended repetitions especially when the user defines many ordering constraints across steps. kanal provides a warning for such cases to let the user check if the loops are in fact intended.
1	checking disjunctive branches
when there are disjunctive branches in a model  the user may not notice that some of the combinations of alternatives should in fact not be possible. kanal exposes different branches in the models by showing different alternative combinations of substeps. as in the case of loops  disjunctive branches are not necessarily a problem and kanal simply informs the user about them.
1	checking causal links
after the simulation  kanal computes the interdependencies between the steps based on how some steps generated effects that satisfied the preconditions of some other steps. for example  synthesizing viral protein needed for replication enables the replication step. these causal links may not have been explicitly indicated by the user. kanal informs users when it notices causal links in order to help them validate the model. it also informs users when they specified a temporal constraint and there does not seem to be a causal link between the steps to justify the ordering.
1	interaction among problems and fixes
this section shows some examples of the kinds of errors detected and the fixes proposed by kanal from a simplified lambda virus invasion model shown in figure 1.  in the model  first a lambda virus moves next to a cell  arrive   and then it enters into the cell  enter . the lambda dna forms a circle  circularize   and then either becomes integrated with

figure 1: a simplified lambda virus invasion model.
the host chromosome  integrate  or starts synthesizing viral protein needed for the formation of new viruses  synthesize . when it gets integrated  it lies dormant in the chromosome during cell divisions  divide . an environmental change can induce the genome to leave the host chromosome  disintegrate  and to start synthesizing viral protein and subsequent
dna replication  replicate   alberts et al.  1 .  inappropriate execution of a step
when the agent  virus  of the arrive step is not specified  kanal detects inappropriate execution of the step since its delete-list contains assertions that are invalid  the location of a thing instead of the location of the virus . in such cases  kanal proposes to either to modify the assignments of the arrive step or to add some steps before arrive so that they can assert the location of the thing. the first fix directly points to the deleted link  and we call it a direct fix. the latter one will eventually lead the user to the problem because it mentions the location but does not point to it directly. we call it an indirect fix.
unachieved preconditions
in the above case  missing agent of the arrive step   the precondition of the enter step that follows  location of the virus should be near the cell  may also fail because of the failure of the arrive step. kanal detects this unachieved precondition and proposes multiple ways of fixing the problem:  1  modify the arrive step so that it can change the location of the virus   1  add a new move or arrive step to achieve the precondition  or  1  modify the enter step to have different preconditions. note that in this case  the same fix  i.e.  modifying the arrive step  is suggested for solving two different errors  inappropriate execution of arrive and unachieved precondition of enter .
also  the same set of fixes may resolve two different errors. for example  if the assignment of the patient  virus  of enter is missing  its precondition  the patient should be near the destination  fails. kanal produces an error message and proposes similar fixes as in the above case.
failed expected effects
one of the expected effects was that the the virus should be inside the cell after an execution of the scenario. when the assignment of destination  the cell  of the enter step is missing  the effect  the virus enters into the cell  cannot be achieved. missing ordering constraints
when ordering between steps are missing  e.g.  the link between the arrive step and the enter step is not specified   kanal interrupts the simulation and reports an error. it proposes to add or modify temporal links between the steps so that the simulator can execute the unreached steps. when there is any ambiguity in the ordering  it generates a warning  asking for explicit ordering constraints among the steps.
　in addition to the above  kanal reports other results from the simulation: causal links
there were three causal links reported from the scenario:  1  arrive enabled enter by achieving the location the virus being near the cell   1  integrate enabled disintegrate by achieving the dna being integrated with the chromosome  and  1  synthesize enabled replicate by providing the viral protein needed for the replication. disjunctive links
the disjunctive branches stemming from circularize were shown to the user as two alternative paths.
simulated paths
there were two simulated paths because of the disjunctive branches:  arrive enter circularize integrate divide disintegrate synthesize replicate  and  arrive enter circularize synthesize
replicate .
　notice that kanal can detect the same error in multiple ways since one abnormality can lead to another. for example  missing an ordering constraint can make some steps unreached during the simulation  which can also lead to failed expected effects because of the unexecuted  unreached  steps. since whenever there are unreached steps there tends to be failed expected effects  users may want to focus on fixing problems about the unreached steps first. the same case holds for failed preconditions and unreached steps because failed preconditionsinterrupt simulation  leading to unreached steps. to help avoid confusion  kanal can selectively present fixes so that the user can concentrate on the actual source of the problem. for the case of failed preconditions and unreached steps  kanal presents the fixes for the failed preconditions first  but lets users check other fixes if they want.
1	preliminary evaluation
kanal is being integrated with concept composition  explanation tools and their interfaces in the end-to-end system mentioned above  and we are planning to perform an extensive user evaluation of this integrated system. the preliminary evaluation presented here focuses on how useful kanal is in itself as a module to detect and fix errors.
resultsvirus invasionlambda viruscheck draintotalw/o 1 linkinvasionmonitor# of test cases11# of errors11# of errors detected11# of errors with direct fixes11total # of fixes11- # of direct fixes11avg. # of fixes1111proposed avg resultsvirus invasionlambda viruscheck draintotalw/o 1 linksinvasionmonitor# of test cases11# of errors11# of errors detected11# of errors with direct fixes11# of fixes proposed11- # of direct fixes11avg. # of fixes11.1.1proposed avg resultsvirus invasionlambda viruscheck draintotalw/o 1 linksinvasionmonitor# of test cases11# of errors11# of errors detected11# of errors with direct fixes11# of fixes proposed11- # of direct fixes11avg. # of fixes111.1proposed avg table 1: kanal checks for process models.
　to evaluate kanal's help in detecting and fixing errors  we used three process models: a virus invasion process  a lambda virus invasion  and a check-condensatedrain-motor procedure from a high pressure air compressor  hpac  domain which has been also used for acquiring process models  lessons  for intelligent tutoring systems  scholer et al.  1 . the first and last ones were written by other researchers.
　we evaluated how kanal could help users with one important kind of error: if they forget to specify one  two  or three links or role assignments. for example  to specify the virus invasion model the user would need to make a total of 1 links and assignments if no errors are made. the test cases were generated by taking the original correct process models and randomly deleting a subset of the links or assignments that users would need to make.
　table 1 shows the results from our preliminary evaluation. the first rows in each table show the number of cases tested and the second rows show the total number of errors in the test cases. the third row shows how many of those errors were detected by kanal. kanal was able to detect most of the errors when there is only one error  1 of 1 . kanal misses one case in the lambda virus invasion model because its invade component has container as its patient which should in fact be a cell  a more special concept than container   but there was no explicit violation in any of the checks kanal performs. to be able to detect such problems we may need to examine slots tested in the model and check if they in fact belong to the concept. for example  cells contain cytoplasm but not any containers do in general.
　kanal missed some errors when more than one link were deleted  1 among 1 without 1 links  and 1 among 1 without 1 links . this is to be expected  since some errors interrupt the simulation  and other errors cannot be detected unless further steps are simulated  as described in the previous section.
　the number of errors which had direct fixes are shown in the fourth rows in the tables. there were a few cases where kanal detected errors but was not able to provide direct fixes. for example a step's role can refer to a deleted slot of another step  such as when the agent of the enter step refers to the agent of the invade step. if the enter step failed because of the missing agent of the invade step  then the failed step is different from the step with missing links  making it harder to find the sources of the problems. we are planning to examine how we can follow such links among the steps to trace back to the original sources. some other cases of lack of direct fixes happened when there are multiple errors at the same time because some errors are hidden as described above. kanal's selective presentation of fixes helps  and we expect that fixing one problem at a time may be easier for end users.
　the numbers of fixes shown in the fifth rows of the tables are based on the selected fixes described above. direct fixes should be more useful than indirect fixes in general. the number of direct fixes are shown in the sixth rows in the tables. for most of the errors detected  kanal was able to provide at least one direct fix.
in summary  our preliminary evaluations show:
kanal virtually always  1 of 1 test cases  detected an error and made suggestions to the user. the one case that kanal missed was a process model that was perfectly consistent although it was overgeneral  which is a problem that could only be noticed by a user.
detecting an error impaired detecting others since posterior steps will not be executed in the simulation. although kanal detected 1%  1%  and 1% of the errors in the case where one  two  and three links and assignments were missing  it always detected at least one error in each of the process models that had more than one error. once an error was fixed  the next error was always found by kanal  direct fixes .
for 1% of the errors detected  kanal's fixes pointed directly to the source of the errors.
1	conclusions
as we mentioned  we are planning to perform an extended evaluation with end users  biologists  when kanal is integrated with the end-to-end system described earlier. in doing so  we will also be able to test the usefulness of kanal with different types of errors than the ones we show here  including selecting wrong components in the knowledge base.
　there have been a lot of verification and validation techniques developed in software engineering  wallace et al.  1; basili  1 . although many of them are not directly applicable  there are many common issues in building process information  including efficiency  maintenance  cost  reuse  etc. we are planning to examine useful techniques developed for such issues.
　kanal is built for concept composition where components in the knowledge base are assumed not to have any errors. however  in other environments  such as in acquiring procedural knowledge for intelligent tutoring systems  we cannot expect that the models of actions will always be correct. often  incomplete operators are used to model procedures and they are refined based on instructor input or through autonomous learning by experimentation. we believe that kanal will be also useful in such environments.
acknowledgments
we would like to thank bruce porter and peter clark for their help in integrating kanal with the km simulator and for providing the virus invasion scenario  and andrew scholer for his help with the hpac process model and with the simulator of the tutoring system mentioned. we would also like to thank vinay chaudhri  mabry tyson  and jerome thomere for their comments and feedback on this work. kevin knight provided very helpful comments on earlier drafts. this research was funded by the darpa rapid knowledge formation  rkf  program with subcontract number 1-1 to sri international under contract number n1-c-1.
references
 alberts et al.  1  alberts  b.  bray  d.  johnson  a.  lewis  j.  raff  m.  roberts  k. & walter  p. essential cell biology: an introduction to the molecular biology of the cell. garland publishing inc  1.
 basili  1  basili  v. & selby r. comparing the effectiveness of software testing strategies. in ieee transactions on software engineering  1   1.
 chien  1  chien  s. static and completion analysis for knowledge acquisition  validation and maintenance of planning knowledge bases. in international journal of human-computer studies  1  pp. 1  1.
 clark & porter  1  clark  p. & porter  b. building concept representations from reusable components. in proceedings of aaai-1  pp. 1  1.
 clark & porter  1  clark  p. & porter  b. the knowledge machine. in http://www.cs.utexas.edu/users/mfkb/km.html  1.
 ghallab et al.  1  ghallab  m.  howe  a.  knoblock  c.  mcdermott  d.  ram  a.  veloso  m.  weld  d. & wilkins  d. pddl - the planning domain definition language. technical report  yale university. available at http://www.cs.yale.edu/pub/mcdermott/software/pddl.tar.gz.  huffman & laird  1  huffman  s. & laird  j. flexibly instructable agents. in journal of artificial intelligence research  1-1  1.
 kambhampati  knoblock  & yang  1  kambhampati  s.; knoblock  c.; and yang  q. 1. planing as refinement search: a unified framework for evaluating design tradeoffs in partial-order planning. artificial intelligence 1-1.
 kim & gil  1  kim  j. & gil  y. deriving expectations to guide knowledge base creation. in proceedings of aaai-1  pp. 1  1.
 kim & gil  1  kim  j. & gil  y. acquiring problemsolving knowledge from end users: putting interdependency models to the test. in proceedings of aaai-1  pp. 1  1.
 mcguinness et al.  1  mcguinness  d.  fikes  r.  rice  j. & wilder  s. an environment for merging and testing large ontologies. in proceedings of kr-1  1
 myers  1  myers  k. strategic advice for hierarchical planners. in proceedings of kr-1  1.
 o'keefe & o'leary  1  o'keefe  r. & o'leary  d. expert system verification and validation. in expert systems with applications: an international journal  1 : 1.
 preece & shinghal  1  preece  a. & shinghal  r. foundation and application of knowledge base verification. in international journal of intelligent systems  1 : 1  1
 sacerdoti  1  sacerdoti  e. 1. a structure for plans and behavior. new york: elsevier.
 scholer et al.  1  scholer  a.  rickel  j.  angros  r. & johnson  l. learning domain knowledge for teaching procedural tasks. in aaai-1 fall symposium on learning how to do things  1
 swartout & gil  1  swartout  w. & gil  y. expect: explicit representations for flexible acquisition. in proceedings of kaw-1  1.
 tate  1  tate  a. 1. generating project networks. in international joint conference on artificial intelligence.
 tate  1  tate  a. 1. representing plans as a set of constraints - the i-n-ova model. in drabble  b.  ed.  proc. third international conference on artificial intelligence planning systems. university of edinburgh: aaai press. available as pointer.
 tissot & gruninger  1  tissot  f.  & gruninger  m. nist process specification language. technical report  nist.
 wallace et al.  1  wallace  d.  ippolito  l. & cuthill b. reference information for the software verification and validation process. in nist special publication 1  1.
 wilkins  1  wilkins  d. e. 1. practical planning: extending the classical ai planning paradigm. morgan kaufmann.
 yang  1  yang  q. 1. formalizing planning knowledge for hierarchical planning. computational intelligence 1 :1.

integrating expectations from different sources to help end users acquire procedural knowledge
jim blythe
information sciences institute
university of southern california
marina del rey  ca 1  usa
blythe isi.eduabstract
role-limiting approaches using explicit theories of problem-solvinghave been successful for acquiring knowledge from domain experts1. however most systems using this approach do not support acquiring procedural knowledge  only instance and type information. approaches using interdependencies among different pieces of knowledge have been successful for acquiring procedural knowledge  but these approaches usually do not provide all the support that domain experts require. we show how the two approaches can be combined in such a way that each benefits from information provided by the other. we extend the role-limiting approach with a knowledge acquisition tool that dynamically generates questions for the user based on the problem solving method. this allows a more flexible interaction pattern. when users add knowledge  this tool generates expectations for the procedural knowledge that is to be added. when these procedures are refined  new expectations are created from interdependency models that in turn refine the information used by the system. the implemented ka tool provides broader support than previously implemented systems. preliminary evaluations in a travel planning domain show that users who are not programmers can  with little training  specify executable procedural knowledge to customize an intelligent system.
1	introduction
in order to be successful  deployed intelligent systems must be able to cope with changes in their task specification. they should allow users to make modifications to the system to control how tasks are performed and to specify new tasks within the general capabilities of the system. for example  consider a travel planning assistant that can locate flights and hotel reservations to help a user create an itinerary for some trip. many such systems allow users to search for hotels by cost  hotel chain and distance to some location. however  users often have individual requirements  such as  prefer a direct flight unless it is double the price of the cheapest connecting flight  or  if the flight arrives late in the evening  the hotel should be near the airport  otherwise it should be near the meeting.  these requirements often go beyond the initial abilities of the travel assistant tool  leaving the user to check them by hand and severely limiting the tool's usefulness.
　the ability to define requirements like these in a way that can be integrated with the tool is therefore essential for it to meet a wide range of users' needs. the requirements  which were suggested by an independent user  are typical in that they do not require adding new sources of information to the tool  which can already compute distances and knows flight arrival times. instead  they require processing the information to produce new criteria: in the first case for instance  finding the minimum value of the price for the set of all connecting flights and multiplying this value by 1. these are instances of procedural knowledge  rather than purely factual knowledge like the distances or costs. a tool that can incorporate new procedural knowledge like this from users can have a range of applicability that goes well beyond that originally envisaged by the developer.
　however  it is difficult for users who are not programmers to add procedural knowledge to systems. in the next section we discuss some of the challenges that users face in more detail. some ka approaches use expectations of the entered knowledge to aid users  kim & gil  1 . expectations are beliefs about the knowledge that the user is currently entering that can be used to constrain the possibilities for what can be entered next. for example  expectations can govern the return type of a function that the user is entering  or its general purpose within the system. they can be used both to interpret and check knowledge as it is entered  to provide feedback or to help a user enter correct knowledge.
　another common direction of work in knowledge acquisition  ka  aims to support users through explicit  domainindependent theories of the tool's problem-solving process  often called problem-solving methods  psms   breuker & de
velde  1; eriksson et al.  1 . these theories can encourage re-use across different applications and the structured development of intelligent systems as well as providing a guide for knowledge acquisition from experts. they can be used to structure the ka session for the user and provide context for the knowledge that is acquired. however  most ka approachesthat use problem-solvingmethods focus on assisting knowledge engineers rather than domain experts  fensel & benjamins  1; fensel & motta  1 .
　other ka tools such as salt  marcus & mcdermott  1  take a role-limiting approach  allowing domain experts to provide domain-specific knowledge that fills certain roles within a psm. however  most of these have been used to acquire instance-type information only. musen  musen  1  argues that although role-limiting provides strong guidance for ka  it lacks the flexibility needed for constructing knowledge-based systems  kbs . the problemsolving structure of an application cannot always be defined in domain-independent terms  and a single problem-solving strategy may be too general to address the particulars of an application. puerta et al. advocate using finer-grained psms from which a kbs can be constructed  puerta et al.  1 .
　gil and melz  gil & melz  1  address this problem by encoding the psm in a language that allows any part of the problem-solvingknowledge to be inspected and changed by a user. in their approach  a partially completed kbs can be analyzed to find missing problem-solving knowledge that forms the roles to be filled. this is done as part of the interdependency analysis performed by expect  swartout & gil  1; kim & gil  1   which looks at how both problem-solving knowledge and factual knowledge is used in the intelligent system. this work extended the role-limiting approach to acquire problem-solving knowledge and to determine the roles dynamically. however  gil and melz's tools were not adequate for end users. there are at least two reasons for this. first  there is no support for a structured interaction with the user as there is in tools like salt. the knowledge roles  once generated  form an unstructured list of items to be added  and it can be difficult for the user to see where each missing piece of knowledge should fit into the new kbs. second  the user must work directly with their proceduresyntax to add problem-solving knowledge  which is not appropriate for end users.
　one solution is to exploit knowledge from a variety of sources to guide the user through all the stages of adding procedural knowledge. we view all the ka tools mentioned above as providing different kinds of expectations on the knowledge to be entered  either from background theories in the form of the psms  or from interdependency analysis. this framework allows the tool to exploit the backgroundtheory from the psm to help a user begin the process of adding knowledge  and also to exploit interdependencies to help a user refine an initial definition of a procedure into an executable one. in our implemented system  which is built on expect  modules that use expectations from the two sources share information in the form of input-output characterizations of expected procedural knowledge. this sharing is mutually beneficial to both the background theory-based and interdependency-based approaches.
　in the next section i discuss some of the challenges that users who are not programmers face in defining procedural knowledge. next i describe the use of expectations in more detail and show how they are integrated in an implemented tool  called constable. i then report on initial user experiments with constable that demonstrate the value of the approach.
1 why do users find it difficult to enter procedural knowledge 
there are several challenges that users face in defining procedural knowledge. here i sketch how some of them can be addressed  and highlight the role played by expectations.
　users do not know where to start. adding a new capability to an intelligent system may require adding several related pieces of knowledge  in a form recognizable by the system. simply beginning this process can be difficult even for an experienced programmer who does not know the system well. expectations based on the problem-solving method can help identify the purpose and the initial structure of new procedural knowledge  eriksson et al.  1 .
　users do not know formal languages. a structured english editor allows users to modify english paraphrases of a procedure's formal representation  blythe & ramachandran  1 . users can select a fragment of the paraphrase and choose from a list of suggested replacements for the fragment  which are automatically generated based on expectations from method analysis. this approach hides the internal procedure syntax while avoiding the challenge of full natural language processing.
　users may not know whether the added knowledge is free of syntax errors. the new knowledge may have errors in formal syntax  e.g. an  if  statement with no condition or it may have type errors  e.g. trying to multiply the result of a subprocedure that returns a hotel . since users create procedures by choosing replacements from a list  the editor can effectively eliminate some syntax errors. others can be detected to generate a warning. if a procedure fragment is selected that contributes to a syntax error  some of the suggested replacements are formulated to fix the error  further helping the user.
　users may not know whether the added knowledge is correct. the procedure may be correct from a formal standpoint but not achieve the desired result. constable tests new knowledge against examples as soon as it is entered to help find these problems.
　it takes several steps to add new knowledge  so users can easily be lost. users often do not realize and/or forget the side effects of the changes that require following up. expectations from the psm can be used to guide users through the initial steps in adding new knowledge. this is done through a script  as the next section shows. the approach is related to knowledge acquisition scripts  tallis & gil  1   though not as general.
　this discussion of problems that users face indicates that providing help based on a combination of several different kinds of expectations may be key for non-programmers to add procedural knowledge. in the next two sections we show the help that can be given based on different kinds of expectations and describe how they are integrated in constable. we begin by describing expectations from psm task theories and then describe how the system makes use of expectations derived from analyzing procedure interdependencies. in the following section we describe how the expectations are integrated by expressing expectations from background theories in terms of input-output type expectations.
1	expectations from background theories
expectations derived from the background theory in a problem-solving method are used to help clarify the purpose of the new procedural knowledge to be added by identifying its place in the psm framework. once this identification is made  initial templates are created for the new knowledge  which the user can refine until they perform the desired task. in this way  expectations from background theories help a user begin the process of defining new procedural knowledge to perform some task. in our approach  the background theory has two main components:  1  an ontology of concepts related to the task  and  1  generic procedural knowledge for performing each subtask.
　this approach is general and can be applied to a wide range of generic tasks. in this paper we use an implemented problem-solving method for plan evaluation to illustrate the approach. plan evaluation problems belong to a domainindependent problem class in which an agent  typically a human expert  judges alternative plans according to a number of criteria. the aim is usually to see which of the alternative plans is most suited for some task. in terms of a standard problem solving method library such as commonkads  breuker & de velde  1   it is a special case of assessment.
　each criterion for judging a plan is represented explicitly in this framework. through experience with several intelligent systems for plan evaluation  valente et al.  1   we have identified several patterns in the ways that the criteria are evaluated  blythe & gil  1 . these patterns are regularities that can be re-used across planning domains and provide guidance for knowledge acquisition. they are represented through an ontology of plan judgment criteria  called critiques  that is partially shown in figure 1. for example  upper-bound represents the class of critiques that can be evaluated by checking that some property of the plan has a value that is below a maximum value. each class is identified with a pattern for evaluating a plan  implemented through generic procedural knowledge attached to the class. the psm also includes concepts related to plans and the use of resources.

figure 1: different types of criteria for judging plans are part of the background theory of plan evaluation.
the second component of the task theory consists of generic procedural knowledge attached to some of the subtasks within the domain. in the plan evaluation domain  these subtasks are the generic critique types. for example  the following method says that a step satisfies an upper bound critique if and only if the actual value of the associated property is less than or equal to its maximum value. the tasks of estimating the actual and maximum values for the property are two methods that can be defined by the user with our tool.
capability:  determine-whether  obj   thing is  inst-of thing   
 satisfies   bc is  inst-of upper-bound    
result-type:  inst-of boolean   method:
 check-that  obj  estimate  obj actual-value   of  bc   for  thing   
 is-less-than-or-equal-to  estimate  obj maximum-allowed-value 
 of  bc   for  thing    
1	using background theories in constable
the background theory can be used to create a working plan evaluation system for a particular domain by defining domain-specific critique classes within the ontology and adding the procedures needed complete each critique's definition. in the travel planning domain  for example  plans are itineraries for travel and the steps in plans represent reservations of flights  hotels and rental cars. one possible critique checks that no flight costs more than $1. this is implemented by defining the critique flight-cost as a subclass of both local-critique and upper-bound. the procedures for those classes are then used to evaluate the new critique  resulting in a check that the actual amount of flightcost for each step is less than or equal to the maximum amount. the user completes the definition by defining methods to compute the actual amount  by retrieving the cost of the flight  and the maximum allowed amount  $1 .
　figure 1 shows the main window through which a user defines the flight-cost critique in constable. the tool presents questions of two kinds: those aimed at classifying the critique in the ontology  e.g. questions 1  1 and 1  and those allowing the user to refine default procedural knowledge  which begin with the phrase  show me how to... . the questions are attached to the critique classes in the ontology and the tool asks them as it tries to classify the new critique. for instance  question 1   warn if flight-cost is too large   is used to classify the critique as an upper bound. once a positive classification is made  the tool gives the user an option to refine the procedural knowledge attached to the class. the generic procedural knowledge for this class include a default for  estimate the maximum allowed value of ..   so question 1 allows the user to refine this default for the flight cost.
　the use of background theories to classify new knowledge and define default procedural knowledge helps to solve the first problem that users face in creating procedural knowledge: how to get started. the tool begins by asking questions about the nature of the knowledge to be added  and the default procedures are guaranteed to be applicable within the system. the task theory is also used to break the new knowledge into manageable pieces  an important step that is hard for users who are not programmers. however  refining the methods to compute actual values and maximum allowed values for flight costs  for example  can still be a daunting task requiring the tool to offer more assistance. expectations based on interdependency analysis are one source of this assistance.
1	expectations from interdependencies
the expectations described in the last section come from background theories of tasks in problem-solving methods. here we consider expectations that come from comparing the new procedural knowledge with the procedure syntax and with existing procedural or factual knowledge  which we refer to as interdependency expectations. some examples of syntax-based expectations are that variables that are used in the procedure body are defined and that the rules of syntactic constructs such as  if ... then ... else ...  are observed. some examples of expectations generated by comparing the new knowledge with existing procedural and factual knowledge are that relations and objects used are defined in the knowledge base  that other procedural knowledge that is used is defined  and that the relations and procedural knowledge used will return information of an appropriate type for the way it is being used  e.g. the procedure does not try to multiply a hotel by a number . these expectations are derived from interdependency models  kim & gil  1 .
　figure 1 shows constable's editor for procedural knowledge being used to define how to compute the maximum allowed value of the cost of a flight reservation. the purpose and body of the procedure are automatically paraphrased in english  blythe & ramachandran  1 . when the user selects part of the procedure to change  in this case  pittsburgh   the editor suggests replacements in the lower panel  figure 1: constable's editor being used to define how to compute the maximum allowed value of the cost of a flight reservation.
　we distinguish two ways that a ka tool can use interdependency expectations to help a user define procedural knowledge  called hard and soft expectations. for a hard expectation  the tool does not allow procedural information that violates the expectation to be expressed. for example  the only way for a user to introduce an  if  construct into a procedure is to select one from the suggested replacements  so the user can never express a procedure that violates the basic syntax of the construct. all syntax expectations are hard expectations in this editor.
　for a soft expectation  the tool will allow a user to define proceduresthat violate the expectation  but will providewarnings  and possibly remedies  for the violation. for example the method in figure 1 currently violates a type expectation. to compute the maximum allowed cost for a flight  the user specifies  if pittsburgh then 1  otherwise 1   but in the formal syntax  the keyword  if  must be followed by code that returns a boolean value. the editor shows an error message in the lower panel and  when  pittsburgh  is selected to be replaced  suggests expressions that include  pittsburgh  but will produce a boolean value  including  check that the destination city is equal to pittsburgh  and  check that the origin city is equal to pittsburgh . these expressions are produced by an anytime search algorithm. the suggestions also include expressions that will not fix the violation  such as  compute the distance between pittsburgh and a location . choosing to make an expectation hard or soft can impact how easily a user can express a procedure through successive replacements and how easily a user can be confused by an intermediate form of the procedure. empirically  users find it useful to build intermediate expressions that violate interdependency expectations  such as in the above example or by using undefined procedures. this allows users to concentrate on other issues before fixing the violations. these are soft expectations in constable. however it is not so useful for users to violate syntactic expectations; these are hard in constable. different ka tools may choose different expectations to be hard or soft  and the optimal choice may also depend on the skill level of the user.
　almost all compilers find and report errors based on inputoutput expectations. constable goes beyond this in suggesting modifications to the existing code that would address the error. the suggestions are found by a breadth-first search through the space of possible sentences  taking the userselected fragment as a starting point. a node in the search space is a term in the formal syntax  and it is expanded by applying all known applicable relations or procedures. the search terminates when terms are reached that resolve the expectation violation  or match a user-typed search string. since the search space can be infinite  the suggestion module times out after a short time.
　users who are not programmerscan be daunted by the need for executable procedural knowledge to have precise syntax with subgoals returning correctly typed information. the use of the expectations described above  in both hard and soft form  can provide significant help.
1	integrating expectations from different sources
the previous two sections described how both expectations from background theories and method-based expectations provide valuable help for users to define procedural knowledge. in combination  the two kinds of expectations can provide more assistance than the sum of their parts. this is because useful information about the emerging procedural knowledge can be exchanged between the two sources.
　there are several examples of this information flow when constable is used to create the flight cost critique  summarized in the window in figure 1. as we described earlier  the questions in figure 1 are generated through expectations in the background task theory. some of the questions are to classify the new critique in the ontology and some are to refine the attached procedural knowledge.
　for each of the procedures to be defined in questions 1  1 and 1  type expectations are sent from the task theory to the method analysis module  which uses them to help guide the user. constable uses the task theory to assign types to each input variable and a desired output type  and calls the procedure editor. for example  the method  estimate the maximum allowed value of ...  has a desired type  inst-of number   so the method editor will warn the user and suggest remedies if the method defined does not produce a number.
　some of the type expectations for the default methods are refined in the method analyzer by considering their interdependencies. for example  the user defines a method  estimate the actual value of flight cost for a flight reservation  in figure 1. the input variable for this method has type  inst-of flight-reservation   but this cannot be inferred from the task theory alone: it comes from the result type of the method  find the things for which to evaluate a flight cost on a trip . this is defined by the user to return a set of flight reservations. when this definition is complete  expect's interdependency model is re-generated. it shows that the subsequent methods should take a flight reservation as input  and this information is passed back to the task theory from the method analyzer.
　information from the method analyzer is also used by the task theory in classifying the critique. for instance  once the user completes the definition of a procedure to  estimate the actual value of flight cost..   the method analyzer notes that it produces a number. this information is used in the task theory to classify the critique as a bounds-check according to the ontology of figure 1. question 1   warn if flight cost is too large    attempts to classify the critique as a lowerbound. if the method to compute the actual value produced a different type  for example an airline  this question will not be asked. instead  the task theory will classify the critique as an extensional critique and ask whether there are any preferred values  to test whether the critique is a positive-extensionalcritique.
　information is passed between the psm task theory and the method analyzer in both directions as the user defines a critique. the method analyzer makes use of type expectations from the task theory to help the user. it also passes refined type information to the task theory  based on the new procedural information and on interdependency analysis. the task analyzer uses this information as it classifies the new task. this interplay between the expectations from different sources significantly improves the guidance that can be given the user as the new knowledge is defined.
1	preliminary experiments
we performed a preliminary evaluation of the approach by evaluating the performance of six subjects  who were not programmers  at adding and modifying critiques in the travel planning domain. in training  users followed written instructions to modify three critiques and add one new critique using constable. in testing  users were asked to add and modify as many critiques as possible from a list of six. subjects took an average of one hour to complete the training phase and thirty minutes working on the test critiques.
　the simplest training task was to change a constant representing the maximum value of the distance between the hotel used and a meeting location. in the most complex task  the maximum distance for the hotel was defined as twice the distance of the closest available hotel. the procedure is:
 multiply  obj  find  obj  spec-of minimum  
 of  compute  obj  spec-of distance  
 between  find  obj  set-of  spec-of hotel   
 in  r-city  r-hotel  t     
              and  r-meeting  r-trip  t         by 1  
all subjects entered this definition correctly using the tool. even though they were following instructions  it is unlikely that this success rate would have been achieved without using constable. the test tasks had the same range of complexity as the training tasks.
　adding or modifying a critique may require a number of steps to be completed. to measure the partial performance in the test tasks we counted each correctly defined method as a step  and also counted classifying the new critique correctly in the ontology as a step. table 1 shows the average number of steps completed in each the first four test tasks. every subject was able to make modifications to constant maximum values. four of the six subjects were able to define a hotel cost critique according to the definition  if the city of the hotel is pittsburgh  then the maximum hotel cost is $1  otherwise it is $1 . defining this kind of critique is beyond the scope of tools that do not allow users to define procedural knowledge.
tasksteps completed / total stepsmodify constant maximum value1max value is conditional on city1 / 1complex new upper bound1 / 1complex new upper bound ii1 / 1table 1: average number of steps completed by subjects for each of four test tasks.
　in addition to the pre-defined critiques  subjects attempted to add their own critiques to the plan evaluation tool. these critiques were written down before subjects worked through the training or test cases  so that they would not affect their choices. no subject succeeded in completely adding these critiques. of the 1 critiques that subjects described  1 could in principle be added through constable. the others could be identified in the critique ontology but used relations and concepts  such as  bed-and-breakfast   that the tool did not include.
　one shortcoming of the current tool is that subjects were not able to easily see all the conceptsand relations available in the domain. the editor shows only those that are related to the current procedure definition. while this is helpful when users are making local changes to a procedure  they also need a way to see all available information. also  the short time-scale of the experiment limited the time that subjects could spend thinking about the critiques. we hope to build a tool that the subjects would use on a daily basis that includes constable  and investigate its impact.
　after completing the experiment  several subjects whose job includes travel planning volunteered more critiques that would be useful in their work. it is interesting to compare these with the critiques that were expressed before the experiment: they all entail more complex procedural reasoning than the earlier critiques  but can be expressed in the critique ontology. examples include  if the flight arrives late in the evening  the hotel should be near the airport  otherwise it should be near the meeting   and  prefer a hotel that is close to each of two meeting locations .
1	discussion
adding procedural knowledge to intelligent systems is a challenging task  but one that is necessary for the system to be useful to a wide range of users. we presented a novel approach in which expectations from background theories and from interdependencyanalysis are integratedto guide the user through the ka process. as well as providing assistance across a larger subset of the ka task than previous systems  the approach is able to combine information from the two sources to provide help that is not otherwise possible. in general  procedural knowledge should be added in concert with factual knowledge such as new concepts  relations and instances. constable includes tools for adding factual knowledge which were not described here for space reasons. information on more of the tools  but with less detail on the use of expectations  can be found in  blythe et al.  1 .
　we are currently working on a number of applications of constable. one is a travel planning tool that retrieves flight  hotel and other information from the web and uses constraints on partial travel plans to help users assemble an itinerary. integrating constable will help users specify individual preferences to assemble itineraries. we also intend to apply constable to acquire procedural knowledge for reasoning about the biochemistry of dna.
references
 blythe et al.  1  blythe  j.  kim  j.  ramachandran  s.  and gil  y. 1. an integrated environment for knowledge acquisition. best paper  proc. international conference on intelligent user interfaces.
 blythe & gil  1  blythe  j.  and gil  y. 1. a problem-solving method for plan evaluation and critiquing. in proc. twelfth knowledge acquisition for knowledge-based systems workshop.
 blythe & ramachandran  1  blythe  j.  and ramachandran  s. 1. knowledge acquisition using an englishbased method editor. in proc. twelfth knowledge acquisition for knowledge-based systems workshop.
 breuker & de velde  1  breuker  j.  and de velde  w. v. 1. commonkads library for expertise modelling: reusable problem solving components.
 eriksson et al.  1  eriksson  h.; shahar  y.; tu  s. w.; puerta  a. r.; and musen  m. a. 1. task modelingwith reusable problem-solving methods. artificial intelligence 1-1.
 fensel & benjamins  1  fensel  d.  and benjamins  v. r. 1. key issues for automated problem-solving methods reuse. in proc. the european conference on artificial intelligence.
 fensel & motta  1  fensel  d.  and motta  e. 1. structure development of problem solving methods. in proc. eleventh knowledge acquisition for knowledgebased systems workshop.
 gil & melz  1  gil  y.  and melz  e. 1. explicit representations of problem-solving strategies to support knowledge acquisition. in proc. thirteenth national conference on artificial intelligence.
 kim & gil  1  kim  j.  and gil  y. 1. deriving expectations to guide knowledge-base creation. in proc. sixteenth national conference on artificial intelligence  1- 1. aaai press.
 marcus & mcdermott  1  marcus  s.  and mcdermott  j. 1. salt: a knowledge acquisition language for propose-and-revise systems. artificial intelligence 1- 1.
 musen  1  musen  m. a. 1. overcoming the limitations of role-limiting methods. knowledge acquisition 1 :1.
 puerta et al.  1  puerta  a. r.; egar  j. w.; tu  s.; and musen  m. a. 1. a multiple-method knowledge acquisition shell for the automatic generation of knowledge acquisition tools. knowledge acquisition 1 :1.
 swartout & gil  1  swartout  w. and gil  y. 1. expect: explicit representations for flexible acquisition. in proc. ninth knowledge acquisition for knowledgebased systems workshop.
 tallis & gil  1  tallis  m.  and gil  y. 1. designing scripts to guide users in modifying knowledge-based systems. in proc. sixteenth national conference on artificial intelligence. aaai press.
 valente et al.  1  valente  a.; blythe  j.; gil  y.; and swartout  w. 1. on the role of humans in enterprise control systems: the experience of inspect. in proc. of the jfacc symposium on advances in enterprise control.

machine learning and
data mining
reinforcement learning

r-max - a general polynomial time algorithm for near-optimal reinforcement
learning
ronen i. brafman
computer science department
ben-gurion university
beer-sheva  1 israel brafman cs.bgu.ac.ilmoshe tennenholtz
computer science department
stanford university
stanford  ca 1 moshe robotics.stanford.edu
abstract
r-max is a simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time. in r-max  the agent always maintains a complete  but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model. the model is initialized in an optimistic fashion: all actions in all states return the maximal possible reward  hence the name . during execution  the model is updated based on the agent's observations. r-max improves upon several previous algorithms:  1  it is simpler and more general than kearns and singh's algorithm  covering zerosum stochastic games.  1  it has a built-in mechanism for resolving the exploration vs. exploitation dilemma.  1  it formally justifies the  optimism under uncertainty  bias used in many rl algorithms.  1  it is much simpler and more general than brafman and tennenholtz'slsg algorithmfor learning in single controller stochastic games.  1  it generalizes the algorithm by monderer and tennenholtz for learning in repeated games.  1  it is the only algorithm for near-optimal learning in repeated games known to be polynomial  providing a much simpler and more efficient alternative to previous algorithms by banos and by megiddo.
1	introduction
reinforcement learning has attracted the attention of researchers in ai and related fields for quite some time. many reinforcement learning algorithms exist and for some of them convergence rates are known. however  kearns and singh's algorithm  kearns and singh  1  was the first provably near-optimal polynomial time algorithm for learning in markovdecision processes  mdps . was extendedlater to handle single controller stochastic games  scsgs   brafman and tennenholtz  1  as well as structured mdps  kearns and koller  1 . in the agent learns by updating a model of its environment using statistics it collects. this learning

   permanent address: faculty of industrial engineering and management  technion  haifa 1  israel
process continues as long as it can be done relatively efficiently. once this is no longer the case  the agent uses its learned model to compute an optimal policy and follows it. the success of this approach rests on two important properties: the agent can determine online whether an efficient learning policy exists  and if such a policy does not exist  it is guaranteed that the optimal policy with respect to the learned model will be approximately optimal with respect to the real world.
　the difficulty in generalizing to adverserial contexts  i.e.  to different classes of games  stems from the adversary's ability to influence the probabilityof reachingdifferent states. in a game  the agent does not control its adversary's choices  nor can it predict them with any accuracy. therefore  it has difficulty predicting the outcome of its actions and whether or not they will lead to new information. consequently  it is unlikely that an agent can explicitly choose between an exploration and an exploitation policy. for this reason  the only extension of to adverserial contexts used the restricted scsg model in which the adversary influences the reward of a game only  and not its dynamics.
　to overcome this problem  we suggest a different approach in which the agent never attempts to learn explicitly. our agent always attempts to optimize its behavior  albeit with respect to a fictitious model in which optimal behavior often leads to learning. this model assumes that the reward the agent obtains in any situation it is not too familiar with  is the maximal possible reward - . the optimal policy with respect to the agent's fictitious model has a very interesting and useful property with respect to the real model: either it performs optimally  or it leads to efficient learning. the agent does not know whether it is optimizing or learning efficiently  but it always does one or the other. thus  the agent will always either exploit or explore efficiently  without knowing ahead of time which of the two will occur. since there is only a polynomial number of parameters to learn  as long as learning is done efficiently we can ensure that the agent spends a polynomial number of steps exploring  and the rest of the time will be spent exploiting. thus  the resulting algorithm may be said to use an implicit explore or exploit approach  as opposed to kearns and singh's explicit explore or exploit approach.
　this learning algorithm  which we call r-max  is very simple to understand and to implement. the algorithm converges in polynomial-time to a near-optimal solution. moreover  r-max is described in the context of zero-sum stochastic game  a model that is more general than markov decision processes. as a consequence  r-max is more general and more efficient than a number of previous results. it generalizes the results of kearns and singh 1 to adverserial contexts and to situations where the agent considers a stochastic model of the environment inappropriate  opting for a nondeterministic model instead. r-max can handle more classes of stochastic games than the lsg algorithm  brafman and tennenholtz  1 . in addition  it attains a higher expected average reward than lsg. r-max also improves upon previous algorithms for learning in repeated games  aumann and maschler  1   such as megiddo's  megiddo  1  and banos  banos  1 . it is the only polynomial time algorithm for this class of games that we know of  and it is much simpler  too. finally  r-max generalizes the results of monderer and tennenholtz  monderer and tennenholtz  1  to handle the general probabilistic maximin  safety level  decision criterion.
　the approach taken by r-max is not new. it has been referred to as the optimism in the face of uncertainty heuristic  and was considered an ad-hoc  though useful  approach  e.g.  see section 1.1 in  kaelbling et al.  1   where it appears under the heading  ad-hoc techniques  and section 1 in  sutton and barto  1  where this approach is called optimistic initial values and is referred to as a  simple trick that can be quite effective on stationary problems  . this optimistic bias has been used in a number of well-known reinforcement learning algorithms  e.g. kaelbling's interval exploration method  kaelbling  1   the exploration bonus in dyna  sutton  1   the curiosity-driven exploration of  schmidhuber  1   and the exploration mechanism in prioritized sweeping  moore and atkenson  1 . more recently  tadepalli and ok  tadepalli and ok  1  presented a reinforcement learning algorithm that works in the context of the undiscounted average-reward model used in this paper. one variant of their algorithm  called ah-learning  is very similar to r-max. however  as we noted above  none of this work provides theoretical justification for this very natural bias. thus  an additional contribution of this paper is a formal justification for the optimism under uncertainty bias.
　the paper is organized as follows: in section 1 we define the learning problem more precisely and the relevant parameters. in section 1 we describe the r-max algorithm. in section 1 we sketch the proof that it yields near-optimal reward in polynomial time. we conclude in section 1. because of length limitations  proofs are not included. a full version of this work appears at www.cs.bgu.ac.il/ brafman/sg1.ps.
1	preliminaries
we present r-max in the context of a model that is called a stochastic game. this model is more general than a markov decision process because it does not necessarily assume that the environment acts stochastically  although it can . in what follows we define the basic model  describe the set of assumptions under which our algorithm operates  and define the parameters influencing its running time.
1	stochastic games
a game is a model of multi-agent interaction. in a game  we have a set of players  each of whom chooses some action to perform from a given set of actions. as a result of the players' combined choices  some outcome is obtained which is described numerically in the form of a payoff vector  i.e.  a vector of values  one for each of the players. we concentrate on two-player  fixed-sum games  i.e.  games in which the sum of values in the payoff vector is constant . we refer to the player under our control as the agent  whereas the other player will be called the adversary.
　a common description of a game is as a matrix. this is called a game in strategic form. the rows of the matrix correspond to the agent's actions and the columns correspond to the adversary's actions. the entry in row and column in the game matrix contains the rewards obtained by the agent and the adversary if the agent plays his action and the adversary plays his action. we make the simplifying assumption that the size of the action set of both the agent and the adversary is identical. however  an extension to sets of different sizes is trivial.
　in a stochastic game  sg  the players play a  possibly infinite  sequence of standard games from some given set of games. after playing each game  the players receive the appropriate payoff  as dictated by that game's matrix  and move to a new game. the identity of this new game depends  stochastically  on the previous game and on the players' actions in that previous game. formally:
definition 1 a fixed-sum  two player  stochastic-game  sg  on states   and actions   consists of:
stage games: each state is associated with a two-player  fixed-sum game in strategic form  where the action set of each player is . we use to denote the reward matrix associated with stage-game .
probabilistic transition function: is the probability of a transition from state to state given that the first player  the agent  plays and the second player  the adversary  plays .
　an sg is similar to an mdp. in both models  actions lead to transitions between states of the world. the main difference is that in an mdp the transition depends on the action of a single agent whereas in an sg the transition depends on a joint-action of the agent and the adversary. in addition  in an sg  the reward obtained by the agent for performing an action depends on its action and the action of the adversary. to model this  we associate a game with every state. therefore  we shall use the terms state and game interchangeably.
　stochastic games are useful not only in multi-agent contexts. they can be used instead of mdps when we do not wish to model the environment  or certain aspects of it  stochastically. in that case  we can view the environment as an agent that can choose among different alternatives  without assuming that its choice is based on some probability distribution. this leads to behavior maximizing the worst-case scenario. in addition  the adversaries that the agent meets in each of the stage-games could be different entities.
　r-max is formulated as an algorithm for learning in stochastic games. however  it is immediately applicable to fixed-sum repeated games and to mdps because both of these models are degenerate forms of sgs. a repeated game is an sg with a single state and an mdp is an sg in which the adversary has a single action at each state.
　for ease of exposition we normalize both players' payoffs in each stage game to be non-negative reals between 1 and some constant . we also take the number of actions to be constant. the set of possible histories of length is
               and the set of possible histories    is the union of the sets of possible histories for all   where the set of possible histories of length 1 is .
　given an sg  a policy for the agent is a mapping from to the set of possible probability distributions over . hence  a policy determines the probability of choosing each particular action for each possible history.
　we define the value of a policy using the average expected reward criterion as follows: given an sg and a natural number   we denote the expected -step undiscounted average reward of a policy when the adversary follows a policy   and where both and are executed starting from a state   by  we omit subscripts denoting the sg when this causes no confusion . let denote the value that a pol-
icy	can guarantee in	steps starting from	. we define
. finally  we define
.
1	assumptions  complexity  and optimality
in what follows  we make two central assumptions. first  we assume that the agent always recognizes the identity of the state  or stage-game  it reached  but not its associated payoffs and transition probabilities  and that after playing a game  it knows what actions were taken by its adversary and what payoffs were obtained. second  we assume that the maximal possible reward is known ahead of time. this latter assumption can be removed.
　next  we wish to discuss the central parameter in the analysis of the complexity of r-max - the mixing time  first identified by kearns and singh 1. kearns and singh argue that it is unreasonable to refer to the efficiency of learning algorithms without referring to the efficiency of convergence to a desired value. they defined the -return mixing time of a policy to be the smallest value of after which guarantees an expectedpayoffof at least . in our case  we haveto take into account the existence of an adversary. therefore  we adjust this definition slightly as follows: a policy belongs to the set of policies whose -return mixing time is at most   if for any starting state and for any adversary behavior   we have that	.
　that is  no matter what the initial state is and what the adversary does  on the average  we have to apply policy no

we discuss this choice below.
   we would need to run the algorithm repeatedly for increasing values of . the resulting algorithm remains polynomial in the relevant parameters.
more than steps before our average accumulated reward is sufficiently close to the value of for the state . notice that this means that an agent with perfect information about the nature of the games and the transition function will require at least steps  on the average  to obtain an optimal value using an optimal policy whose -return mixing time is . clearly  one cannot expect an agent lacking this information to perform better.
　we denote by the optimal expected undiscounted average return from among the policies in . when looking for an optimal policy  with respect to policies that -mix in time   for a given    we will be interested in approaching this value in time polynomial in   in   in
    where is the failure probability   and in the size of the description of the game.
	the reader may have noticed that we defined	as
　　　　　　　. this choice may appear to make the learning task too easy. for instance  one may ask why shouldn't we try to attain the maximal value over all possible states  or at least the value of our initial state  we claim that the above is the only reasonable choice  and that it leads to results that are as strong as previous algorithms.
　to understand this point  consider the following situation: we start learning at some state whose value is very high. however  if we do not execute the action in   we reach some state that has a very low value. a learning algorithm without any prior knowledge cannot be expected to immediately guess that should be done in . in fact  without such prior knowledge  to conclude that is appropriate it must compare its outcome to that of the other actions in . thus  one can expect an agent to learn a near-optimal policy only if the agent can visit state sufficiently many times to learn about the consequences of different options in . in a finite sg  there will be some set of states that we can sample sufficiently many times  and it is w.r.t. these states that we can learn how to behave.
　in fact  it probably makes sense to restrict our attention to a subset of the states such that from each state in this set it is not too hard to get to any other state. in the context of mdps  kearns and singh refer to this as the ergodicity assumption. in the context of sgs  hoffman and karp 1 refer to this as the irreducibility assumption. an sg is said to be irreducible if the markov-chainobtainedby fixing any two  pure stationary strategies for each of the players is irreducible  i.e.  each state is reachable from each other state . in the special case of an mdp  irreducibility is precisely the ergodicity property used by kearns and singh in their analysis of .
　irreduciblesgs have a numberof nice properties  as shown by  hoffman and karp  1 . first  the maximal long-term average reward is independent of the starting state  implying that	for an optimal
policy . second  this optimal value can be obtained by a stationary policy  i.e.  one that depends on the current stagegame only . we believe that our results are primarily interesting in this class of games.
1	the r-max algorithm
recall that we consider a stochastic game consisting of a set of stage-games in each of which both the agent and the adversary have a set of possible actions. we associate a reward matrix with each game  and we use to denote a pair consisting of the reward obtained by the agent and the adversary after playing actions and in game   respectively. in addition  we have a probabilistic transition function    such that is the probability of making a transition from to given that the agent played and the adversary played . it is convenient to think of as a function associated with the entry in the stage-game
　. this way  all model parameters  both rewards and transitions  are associated with joint actions of a particular game. let . for ease of exposition  we assume throughout most of the analysis that the -return mixing time of the optimal policy    is known. later  we show how this assumption can be relaxed.
the r-max algorithm is defined as follows:
initialize: construct the following model	consisting of stage-games 	  and	actions 
　　　　　. here 	correspond to the real games 	correspond to the real actions  and is an additional fictitious game. initialize all game matrices to have	in all entries.	initialize for all	and for all actions	.
in addition  maintain the following information for each entry in each game :  1  a boolean value known/unknown  initialized to unknown;  1  the states reached by playing the joint action corresponding to this entry  and how many times ;  1  the reward obtained  by both players  when playing the joint action corresponding to this entry. items 1 and 1 are initially empty.
repeat:
compute and act: compute an optimal -step policy for the current state  and execute it for -steps or until a new entry becomes known.
observe and update: followingeach joint action do as follows: let be the action you performed in and let be the adversary's action.
if are performed for the first time in   update the reward associated with in   as observed.
update the set of states reached by playing in	.
if at this point your record of states reached from this entry contains

 elements  mark this entry as known  and update the transition probabilities for this entry according to the observed frequencies.

   the value 1 given to the adversary does not play an important role here.
　as can be seen  r-max is quite simple. it starts with an initial estimate for the model parameters that assumes all states and all joint actions yield maximal rewardand lead with probability 1 to the fictitious stage-game . based on the current model  the optimal policy is computed and followed. following each joint action the agent arrives at a new stagegame  and this transition is recorded in the appropriate place. once we have enough information about where some joint action leads to from some stage-game  we update the entries associated with this stage-game and this joint action in our model. after each model update  we recompute the policy and repeat the above steps.
1	optimality and convergence
in this section we provide the tools that ultimately lead to the proof of the following theorem:
theorem 1 let be an sg with states and actions. let   and be constants. denote the policies for whose -return mixing time is by   and denote the optimal expected return achievable by such policies by . then  with probability of no less than the r-max algorithm will attain an expected return of within a number of steps polynomial in   and .
　in the main lemma required for proving this theorem we show the following: if the agent follows a policy that is optimal with respect to the model it maintains for steps  it will either attain near-optimal average reward  as desired  or it will update its statistics for one of the unknown slots with sufficiently high probability. this is the implicit explore or exploit property of r-max: the agent does not know ahead of time whether it is exploring or exploiting - this depends in a large part on the adversary's behavior which it cannot control or predict. however  it knows that it does one or the other  no matter what the adversary does. using this result we can proceed as follows: as we show  the number of samples required to mark a slot as known is polynomial in the problem parameters  and so is the total number of entries. therefore  the number of -step iterations in which non-optimal reward is obtained is bounded by some polynomial function of the input parameters  say . this implies that by performing -step iterations times  we get that the loss obtained by non-optimal execution  where exploration is performed  is bounded by   for any .
　before stating our main lemma we extend kearns and singh's simulation lemma  kearns and singh  1  to the context of sgs with a slightly improved bound as well.
definition 1 let and be sgs over the same state and action spaces. we say that is an -approximation of if for every state we have:
1. if	and	are the probabili-
ties of transition from state to state given that the joint action carried out by the agent and the adversary is   in and respectively  then 
1. for every state   the same stage-gameis associated with in and in .
lemma 1 let and be sgs over states  where is an -approximation of   then for every state   agent policy   and adversary policy   we have that
　next  we define the notion of an induced sg. the definition is similar to the definition of an induced mdp given in  kearns and singh  1  except for the use of r-max. the induced sg is the model used by the agent to determine its policy.
definition 1 let be an sg. let be the set of entries marked unknown. that is  if then the entry correspondingto the joint action in the stagegame is marked as unknown. define to be the following sg: is identical to   except that contains an additional state . transitions and rewards associated with all entries in which are not in are identical to those in . for any entry in or in   the transitions are with probability 1 to   and the reward is for the agent and 1 for the adversary.
　given an sg with a set of unknown states  and some state   let denote the optimal -step policy for the induced sg starting at . when and are clear from the context  we refer to as the r-max policy  because this would be the policy executed by the r-max algorithm at this point .
we now state the implicit explore or exploit lemma:
lemma 1 let	be an sg  let	and	be as above. let
be an arbitrary policy for the adversary  let be some state  and let . then either  1 
　　　　　　  where is the expected -step average reward of in starting from ; or  1  an unknown entry will be played in the course of running on for steps starting at with a probability of at least .
in practice  we cannot determine   the adversary's policy  ahead of time. thus  we do not know whether r-max will attain near-optimal reward or whether it will reach an unknown entry with sufficient probability. the crucial point is that it will do one or the other  no matter what the adversary does.
　we can now outline the proof of theorem 1. first  we wish to show that the expected average reward is as stated. we must consider three models:   the real model  the actual model used  and   where is an -
approximation of such that the sg induced by and is . at each -step iteration of our algorithm we can apply the implicit explore or exploit lemma to and for the set applicable at that stage with . hence  at each step either the current r-max policy leads to an average reward that is close to optimal with respect to the adversary's behavior and the model or it leads to an efficient learning policy with respect to the same model. however  because is an -approximationof   then if the
r-max policy is -close to optimal w.r.t.   it must be close to optimal w.r.t. . we know that the number of -step

   any alternative definition of assigning to arbitrary entries in the same reward as that associated with their counterparts in is suitable as well.
phases in which we are exploring can be bounded polynomially. this follows from the fact that we have a polynomial number of parameters to learn  in and   and that the probability that we obtain a new  useful statistic is polynomial in and . thus  if we choose a large enough  but still polynomial  number of -step phases  we shall guarantee that our average reward is as close to optimal as we wish.
　the above analysis was done assuming we actually obtain the expected value of each random variable. this cannot be guaranteed with probability 1. yet  we can ensure that the probabilitythat the algorithmfails to attain the expectedvalue of certain parameters be small enough by sampling it a larger  though still polynomial  number of times. this is based on the well-known chernoff bound. using this technique one can show that when the variance of some random variable is bounded  we can ensure that we are sufficiently close to its average with probability by using a sufficiently large sample that is polynomial in .
　to remove the assumption that the -return mixing time is known  we proceed as in  kearns and singh  1 . from the proof of the algorithm we deduce some polynomial in the problem parameters such that if is the mixing-time  then after steps we are guaranteed  with probability   the desired return. we repeatedlyexecute the algorithmfor all values of   each time performing steps.
suppose that is the mixing time  then after steps  we will obtain the desired return. 
　notice that the r-max algorithmdoes not have a final halting time and will be applied continuously as long as the agent is functioning in its environment. the only caveat is that at some point our current mixing time candidate will be exponential in the actual mixing time   at which point each step of the algorithm will require an exponential calculation. however  this will occur only after an exponential number of steps. this is true for the algorithm too.
　another point worth noting is that the agent may never know the values of some of the slots in the game because of the adversary's choices. consequently  if is the optimal policy given full information about the game  the agent may actually converge to a policy that differs from   but which yields the best return given the adversary's actual behavior. this return will be no smaller than the return guaranteed by
 . the mixing time of will  in general  differ from the mixing time of . however  we are guaranteed that if is the -return mixing time of   and is its value  after time polynomial in   the agent's actual return will be at least  subject to the deviations afforded by the theorem .
1	conclusion
we described r-max  a simple reinforcement learning algorithm with guaranteed polynomial-time convergence to nearoptimal average reward in zero-sum stochastic games.
　r-max is an optimistic model-based algorithm that formally justifies the optimism in the face of uncertainty bias. its analysis is similar  in many respects  to kearns and singh's
   algorithm. however  unlike the   the agent does not need to explicitly contemplate whether to explore or to exploit. in fact  the agent may never learn an optimal policy for the game  or it may play an optimal policy without knowing that it is optimal. the  clever  aspect of the agent's policy is that it  offers  a catch to the adversary: if the adversary plays well  and leads the agent to low payoffs  then the agent will  with sufficient probability  learn something that will allow it to improve its policy. eventually  without too many  unpleasant  learning phases  the agent will have obtained enough information to generate an optimal policy.
　r-max can be applied to mdps  repeated games  and sgs provided that the latter satisfy the condition that the value of a state game can be computed efficiently. in particular  all single-controller stochastic game instances covered in  brafman and tennenholtz  1  fall into this category. however  r-max is much simpler conceptually and easier to implement than the lsg algorithm described there. moreover  it also attains higher payoff: in lsg the agent must pay an additional multiplicative factor that does not appear in r-max.
　two other sg learning algorithms appeared in the literature. littman  littman  1  describes a variant of qlearning  called minimax q-learning  designed for 1-person zero-sum stochastic games. that paper presents experimental results  asymptotic convergence results are presented in  littman and szepesvri  1 . hu and wellman  hu and wellman  1  consider a more general framework of multiagent general-sum games. this framework is more general than the framework treated in this paper which dealt with fixed-sum  two-player games. hu and wellman based their algorithm on q-learning as well. they prove that their algorithm converges to the optimal value  defined  in their case  via the notion of nash equilibrium . however  convergenceis in the limit  i.e.  provided that every state and every joint action has been visited infinitely often. note that an adversary can prevent a learning agent from learning certain aspects of the game indefinitely and that the polynomial time convergence of r-max to optimal payoff is guaranteed even if certain states and joint actions have never been encountered.
　the class of repeated games is another sub-class of stochastic games that satisfies the conditions of r-max. in repeated games    there are no transition probabilitiesto learn  and we need not use a fictitious stage-game. therefore  a much simpler version of r-max can be used. the resulting algorithm is much simpler and much more efficient than previous algorithms by megiddo  megiddo  1  and by banos  banos  1 . moreover  for these algorithms  only convergence in the limit is proven.
acknowledgments: we thank amos beimel for his help in improving the error bound in lemma 1 and the anonymous referees for their useful comments. the first author was partially supported by the paul ivanier center for robotics and production management.
references
 aumann and maschler  1  r. aumannand m. maschler. repeated games with incomplete information. mit press  1.

   an agent need not employ an optimal policy to obtain an optimal reward if the adversary plays sub-optimally.
 banos  1  a. banos. on pseudo games. the annals of mathematical statistics  1-1  1.
 brafman and tennenholtz  1  r. brafman and m. tennenholtz. a near-optimal polynomial time algorithm for learning in certain classes of stochastic games. artificial intelligence  1-1 :1  1.
 hoffman and karp  1  a.j. hoffmanand r.m. karp. on nonterminating stochastic games. management science  1 :1  1.
 hu and wellman  1  j. hu and m.p. wellman. multiagent reinforcement learning: theoretical framework and an algorithms. in proc. 1th international conference on machine learning  1.
 kaelbling et al.  1  l. p. kaelbling  m. l. littman  and a. w. moore. reinforcement learning: a survey. journal of ai research  1-1  1.
 kaelbling  1  l. p. kaelbling. learning in embedded systems. the mit press  1.
 kearns and koller  1  m. kearns and d. koller. efficient reinforcement learning in factored mdps. in proc. 1th international joint conference on artificial intelligence  ijcai   pages 1  1.
 kearns and singh  1  m. kearns and s. singh. nearoptimal reinforcement learning in polynomial time. in int. conf. on machine learning  1.
 littman and szepesvri  1  m. l. littman and csaba szepesvri. a generalized reinforcement-learning model: convergence and apllications. in proc. 1th intl. conf. on machine learning  pages 1  1.
 littman  1  m. l. littman. markov games as a framework for multi-agent reinforcementlearning. in proc. 1th intl. conf. on machine learning  pages 1  1.
 megiddo  1  n. megiddo. on repeated games with incomplete information played by non-bayesian players. international journal of game theory  1-1  1.
 monderer and tennenholtz  1  d. monderer and m. tennenholtz. dynamic non-bayesian decisionmaking. j. of ai research  1-1  1.
 moore and atkenson  1  a. w. moore and c. g. atkenson. prioratized sweeping: reinforcement learning with less data and less real time. machine learning  1  1.
 schmidhuber  1  j. h. schmidhuber. curious modelbuilding control systems. in proc. intl. joint conf. on neural networks  pages 1  1.
 sutton and barto  1  r. s. sutton and a. g. barto. reinforcement learning: an introduction. mit press  1.
 sutton  1  r. s. sutton. integrated architectures for learning  planning  and reacting based on approximating dynamic programming. in proc. of the 1th intl. conf. on machine learning. morgan kaufmann  1.
 tadepalli and ok  1  p. tadepalli and d. ok. modelbased average reward reinforcement learning. artificial intelligence  1-1  1.
from q    to average q-learning:
efficient implementation of an asymptotic approximation
	fred＞ erick＞	garcia and florent serre
inra - unite＞ de biome＞trie et intelligence artificielle.
bp 1  auzeville. 1 castanet tolosan cedex  france. fgarcia toulouse.inra.frabstract
q    is a reinforcement learning algorithm that combines q-learning and td   . online implementations of q    that use eligibility traces have been shown to speed basic q-learning. in this paper we present an asymptotic analysis of watkins' q    with accumulative eligibility traces. we first introduce an asymptotic approximationof q    that appears to be a gain matrix variant of basic qlearning. using the ode method  we then determine an optimal gain matrix for q-learning that maximizes its rate of convergence toward the optimal value function . the similarity between this optimal gain and the asymptotic gain of q    explains the relative efficiency of the latter for
 . furthermore  by minimizing the difference between these two gains  optimal values for the parameter and the decreasing learning rates can be determined. this optimal strongly depends on the exploration policy during learning. a robust approximation of these learning parameters leads to the definition of a new efficient algorithm called aq-learning  average q-learning   that shows a close resemblance to schwartz' r-learning. our results have been demonstrated through numerical simulations.
1	introduction
td    and q-learning are undoubtedly two of the most important methods that have been proposed in reinforcement learning. q-learning is a simple direct algorithm for learning optimal policies  and the td    rule is used in many existing reinforcement learning and stochastic dynamic programming algorithms for efficiently evaluating a given policy.
　the q-learning algorithm basically implements the td 1  update rule. in order to improve the convergence rate of qlearning  it has been proposed to integrate the td       with the q-learning update rule. that resulted in the family of q    algorithms  watkins  1; peng and williams  1 . it is commonly believed that q       improves the q-learning behaviour  but no definite theoretical analysis of q    has come in support of this conjecture: there is no proof of convergence for q     and as for td     the issue of the rational choice of remains  singh and dayan  1; kearns and singh  1 .
　the use of q    is hampered by another important difficulty. eligibility trace implementations of q    based on look-up tables require many updates of q-value and trace components after each transition. the time complexity of this approach may be very high for large state and action spaces. in spite of several recent attempts  cichosz  1; wiering and schmidhuber  1   the multi-component update rule of q    remains problematic.
　in this paper we propose a new algorithm that eliminate the above-mentioned shortcoming of q   . our approach relies on an asymptotic approximation of watkins' q     section 1 and 1   that appears to be a gain matrix variant of basic q-learning. then an optimal convergence rate analysis of q-learning based on the ode method leads us to determine an optimal gain matrix for q-learning  section 1 . the similar structure of these two gains explains the relative efficiency of q   . furthermore  by minimizing the difference between these two gains  we are able to determine nearoptimal and learning rates that allow en efficient implementation of a new reinforcement learning algorithm called aqlearning  average q-learning   which shows a close resemblance to schwartz' r-learning  schwartz  1   section 1 . we present in section 1 some experimental results that confirm the soundness of the asymptotic analysis and the nearoptimal behaviour of aq-learning.
1	the watkins' q    algorithm
like the td    and q-learning reinforcement learning algorithms  q    can be described within the framework of markov decision processes  mdp   puterman  1 . assume a stationary infinite-horizon mdp model that is defined by a state space of size and an action space of size
　  by a markovian dynamics on characterized by the transition probabilities of moving from to after applying the action   and by the instantaneous reward functions associated to each transition .
a policy is a function that assigns an action to any possible state . given an initial state   fol-
lowing a policy	defines a random trajectory
               with	 	  according to the probabilities	.
　the optimization problem associated to a discounted markov decision problem is to find a policy that maximizes for any initial state the expected sum of the discounted rewards along a trajectory
  where
is the discount factor.
　the value function of a policy can be directly learnt from observed trajectories by using the td    algorithm  without maintaining an estimation of the transition probabilities  sutton  1 . it is also possible to learn directly an estimation of the optimal value function of the problem with the q-learning algorithm  watkins  1 . qlearning regularly updates an estimation of the optimal q-value function denoted by :
which is characterized by the optimality equations:
an optimal policy can then be directly derived from	by
	 	.
　the principle of q    is to combine q-learning and the temporal difference error used in the td    method  in order to accelerate the convergence of q-learning. for the discounted value function  the potentially infinite length of the trajectories leads us to consider the q    methods based on eligibility traces. after each observed transition   these algorithms update the estimated value function by
　　　　　　　　　　　　　　　　　　　　　　　 1  where	is the current temporal difference 	is the stepsize and is the eligibility trace.
　usually  stepsizes decay regularly toward 1  or are very small constants. the dynamics of the eligibility trace is more complex. like in td     the value is increased for the pair   and decays exponentially according to the parameter for all pairs
       . following this general principle  several eligibility trace dynamics for q    have been proposed  watkins  1; peng and williams  1; singh and sutton  1 . the simplest one  due to watkins  watkins  1   is directly derived from the accumulative eligibility trace of td     and has the supplementary effect of resetting to 1 the whole trace vector when the current action is not a greedy action  that is
. the complete definition of
this trace is given by:
if if
if if
 1 
	  for	  with	 	  and
.
　q    with accumulative eligibility trace is a direct generalization of q-learning; q 1  exactly corresponds to qlearning  where is the only component updated after each transition. unlike the convergence of q-learning that has been proved  bertsekas and tsitsiklis  1; borkar and meyn  1   the convergence of q    and its overcomingof q-learning have onlybeen shown experimentally  peng and williams  1; wiering and schmidhuber  1 . in practice  the main drawback of q    is relative to the update costs of and that are proportional to the size of for . for large state-action space problems  the
computation time of q    	  can be very high.
1	asymptotic approximation of q   
in this section we intend to define an asymptotic approximation of watkins' q    when . the following assumptions are made. at each iteration the action is chosen with a semi-uniform exploration policy: a greedy action is selected with a probability   or any other action in
is chosen randomly. asymptotically  when has converged toward   this process defines a markov chain on
     . we assume that this markov chain is regular  recurrent  aperiodic  irreducible  and therefore has a unique invariant distribution .
1	asymptotic accumulating trace
let us consider a trajectory obtained by q     such that converges toward . generalizing our work developed for td     garcia and serre  1   the average asymptotic behaviour of the accumulative trace on this trajectory can be calculated:
proposition 1

　for the formula is similar to the one developed for td   . for   the trace is regularly set to 1 or 1  and its average value is equal to the invariant distribution . in the general case  we can see that the role of the trace in the update rule  1  is to increase credit of state-action pairs that occur more frequently  particularly when the factor is small.
　proposition 1 gives the average asymptotic value of when . from the update rule  1   we can derive an approximation of the asymptotic expectation of the trace at time conditioned on the current state-action pair
:
if if if if
	  with	.
1	asymptotic q   
we consider the asymptotic approximation aq    of q    obtained by substituting in the q    update rule  1  the trace factor	by its approximation	.
　let us denote by the diagonal matrix with diagonal entries   and by the eligibility matrix
where the state-action pairs of	are ordered as follows:	 
　　　　　 the column headings indicate the block dimensions .
	let	be the temporal difference vector :
　in vector notation  the asymptotic approximation aq    of q    is defined by
 1 
1	optimal gain matrix for q-learning
one can see that  1  is a gain matrix variant of the basic qlearning algorithm:

　using a gain matrix in order to guide and accelerate the convergence of a stochastic adaptive algorithm is a well-known approach in stochastic approximation theory  benveniste et al.  1; kushner and yin  1 . in this section  we calculate the optimal gain matrix for q-learning that maximizes its rate of convergence.
　the ordinary differential equation  ode  method provides some analytic tools for analysing the convergence of the general stochastic algorithm

where is the current estimation of the target   and is the observed input random vector at time . classic reinforcement learning algorithms like q-learning or td    have already been analysed with the ode method  bertsekas and tsitsiklis  1; borkar and meyn  1   in order to prove their convergence.
　another potential application of the ode method concerns the optimization of the rate of convergence of stochastic algorithms. one can show that under some general stability conditions  benveniste et al.  1  the new algorithm

where	is the gain matrix defined by
and	is the jacobian matrix of the function
still converges toward	  with a minimal asymptotic variance
.
　this approach  that has already been applied to q-learning in finite horizon  garcia and ndiaye  1  and td     garcia and serre  1   is developed here for q-learning. we have	   
and . the jacobian matrix and its inverse can be calculated:


where	is the	diagonal matrix with diagonal coef-

ficients   is the diagonal matrix with diagonal coefficients for   is the transition matrix of the markov chain on
defined by	  and	the	rectangular matrix

of coefficients	for	.
by developing this expression  we show
proposition 1
the optimal gain matrix for q-learning is
 1 
where

is the diagonal matrix with diagonal coefficients   and is the rectangular matrix of coefficients
where is the probability of moving from to by applying the first action and then by following the optimal policy on the next transitions.
　this optimal gain	leads to an  ideal  algorithm  called optimal-q-learning:
		 1 
　this algorithm theoretically maximizes the rate of convergence of q-learning  by minimizing its asymptotic variance
. one can see that the struc-
ture of implies that when a greedy action is chosen in   all the components have to be updated  and when an exploratory action has been selected  only is modified. note that is independent of the reward functions   and depends on the parameter that characterizes the exploration policy  through the invariant distribution.
1	average q-learning
optimal-q-learning cannot be directly implemented since it would require the costly online estimation of the coefficients. however  as shown next  it is possible to determine some and parameters that minimize the distance between the gain matrix of aq    and   and that allow an efficient implementation of the corresponding aq    algorithm.
1	optimization of aq   
to be consistent with the definition of that assumes a stepsize equal to   we consider the case    where

is the diagonal matrix with constant coefficients . we propose to optimize the choice of the and parameters by minimizing the quadratic norm
of the difference between the gain of aq    and the optimal gain of q-learning. we have

where the	matrix	and the	matrix

have coefficients equal to 1.
making equal the lower right blocks leads to the choice

         . a numerical analysis of the upper left blocks  similar to the one developed in  garcia and serre  1  for
td     results in the choice	  and thus
. then
　this equality exhibits some similarity with equation  1 . as we can note  the structures of the two gains are identical  which might explain the efficiency of q    for . minimizing

yields

like for   this value cannot be exactly implemented in practice in aq   . however   for uniform
mdps where all the transition probabilities are nearly equal to   then	 	 and so	 . it appears experimentally that for large random mdp  this approximation is still valid.
	the value	 leads to	.
this value greater than 1 could be surprising. in fact convergence of eligibility trace methods requires that
　　　 bertsekas and tsitsiklis  1   and so  could be implemented in q    assuming . in case    is no more an admissible value for   and the distance is
minimized for	 on the boundary.
　we are thus able to define a new optimized update rule from the asymptotic approximation  1  where  and
	 	:
		 1 
where	
1	efficient implementation of aq-learning
a direct implementation of the update rule  1  would have a complexity similar to q     proportional to . fortunately  an equivalent expression can be derived. it only requires a small number of updates per iteration  independent of the number of states and actions.
proposition 1
the update rule  1  is equivalent to

		if
	and	
proof:	from  1  we have	
		  where	denotes the vector
with coefficients equal to   and equals 1 or 1 according as or . we then define the new relative value function and the scaling factor by


the	error can be expressed as a function of	and	:
	note that the greedy action	is defined by
	.	the update
rule of can be implemented by replacing the stepsize  with its approximation   where
is the number of times the state has been visited at time   and equals or  whether or .
similarly  the condition for updating might be replaced by . this leads to the following algorithm aq-learning  or average q-learning:

aq-learning algorithm

;
	choose	in	;
	for	until	do
	choose	in	  -exploration policy ;
;
	simulate  	 	 ;
;
 ;
then
 ;
	for	do
		;
	return	;

　in that algorithm  is the length of the learning trajectory. aq-learning is particularly interesting  since it only requires 1 updates per iteration  namely   and . aq-learning should therefore improve the performance of watkins' q    for two distinct reasons. first  aq-learning is an optimized approximation of q    that minimizes the asymptotic variance around . second  like q-learning  aq-learning is an asynchronous algorithm that only updates one component of the value function  here
     per iteration. it is clear that for large state-action space problems  this feature is of crucial importance regarding the time complexity of the algorithm  as it is shown in simulations in the next section.
　amazingly it appears that aq-learning is very close to the r-learning algorithm proposed by schwartz  schwartz  1 . r-learning is supposed to converge to gain optimal policies  that maximize the average-cost

aq-learning and r-learning are identical when   with a correct choice of the r-learning learning rates. the fact that aq-learning is a near-optimal gain matrix variant of q-learning could explain why several experiments have demonstrated that r-learning could outperform q-learning  despite the fact that these two algorithms have not been designed for the same optimality criterion.
　we do not provide in this article a convergence proof for aq-learning. we think  however  that it might be possible to establish such a proof either by checking the stability condition of the gain matrix for   or by exploiting the ode approach of borkar  borkar  1  for two-time scale algorithms  it might be necessary in that case to modify the update rule .
1	simulations
we made numerical experiments in order to study the relative performance of q-learning  optimal-q-learning  watkins' q    and aq-learning. these reinforcement learning algorithms were applied to randomly generated markov decision problems  with all components   and with state rewards	.

figure 1: learning error 

figure 1: computation time for 
　we present here two kinds of learning results that fairly describe the general behaviour of these algorithms. for both of them  the value functions   and were updated on a unique long trajectory on with random initial state. all parameters were initially set to 1. the stepsizes were equal to . we chose a semi-uniform exploration strategy with . for each problem    and were exactly calculated fromthe data and .
　figure 1 shows the relative learning error  as a function of  average value and standard deviation on 1 random trajectories . the size of the mdp was	 
         with and . this graph illustrates the differentrates of convergence of q-learning  aq-learning 
q    for   and optimal-q-learning. these experimental results confirm that the faster rate of convergence is obtained with optimal-q-learning  and that aq-learning converges nearly as fast as this ideal algorithm. aq-learning even performs slightly better than q    with the same learning rates and the value. these three algorithms appear to be considerably faster than q-learning.
　figure 1 illustrates for the same algorithms the different computation times required to achieve a relative learning error equal to 1%  as a function of the size of the state space  for a constant number of actions   with and
       . we first observe on that graph that q-learning is very slow  despite the fact that it only updates one component per iteration. but the main observation is that aq-learning can be really more efficient than q     especially when the number of states is important; for 1 states  aq-learning is 1 times faster than q   .
1	conclusions
we have shown that an asymptotic approximation of q    could be efficiently implemented through two simple update rules on a relative value function and on a scaling factor . this new algorithm aq-learning can be described as an average variant of q-learning for the discounted criterion. its update complexity is independent of the number of states and actions  and experimental results show that its convergence toward the optimal value function is always faster than for q-learning or q   .
　our results have been obtained for the watkins' q    with accumulating eligibility trace. it would be interesting to apply the same approach to the case of replacing traces  singh and sutton  1   or to peng and williams' q     peng and williams  1  where the trace is never reset to 1  and for which an efficient implementation has been proposed by wiering and schmidhuber  wiering and schmidhuber  1 .
　another direction for future work concerns the development of new reinforcement learning algorithms that approximate optimal-q-learning. furthermore  we have seen that the optimal gain for q-learning depends on the exploration policy parameterized by . the question of finding the best value of remains open.
　finally  we need a better understanding of the relation between aq-learning the schwartz' r-learning algorithm  schwartz  1 .
references
 benveniste et al.  1  a. benveniste  m. metivier  and p. priouret. adaptive algorithms and stochastic approximation. springer-verlag  berlin new york  1.
 bertsekas and tsitsiklis  1  d. p. bertsekas and j. n. tsitsiklis. neuro-dynamic programming. athena scientific  belmont  ma   1.
 borkar and meyn  1  v. s. borkar and s. p. meyn. the o.d.e. method for convergence of stochastic approximation and reinforcement learning. siam journal of control and optimization  1-1  1.
 borkar  1  v. s. borkar. stochastic approximation with two-time scales. system and control letters  1-1  1.
 cichosz  1  p. cichosz. truncating temporal differences: on the efficient implementation of td    for reinforcement learning. journal of artificial intelligence research  jair   1-1  1.
 garcia and ndiaye  1  f. garcia and s. ndiaye. a learning rate analysis of reinforcement-learning algorithms in finite-horizon. in international conference on machine learning  volume 1  madison  usa  1.
 garcia and serre  1  f. garcia and f. serre. efficient asymptotic approximation in temporal difference learning. in proceedings of the 1th european conference on artificial intelligence  ecai 1   berlin  1.
 kearns and singh  1  m. kearns and s.p. singh.  biasvariance  error bounds for temporal difference updates. in proceedings of the colt 1 conference  1.
 kushner and yin  1  h. j. kushner and g. g yin. stochastic approximation algorithms and applications. springer  1.
 peng and williams  1  j. peng and r. j. williams. incremental multi-step q-learning. in international conference on machine learning  volume 1  pages 1  1.
 puterman  1  m. l. puterman. markov decision processes: discrete stochastic dynamic programming. wileyinterscience  new york  1.
 schwartz  1  a. schwartz. a reinforcement learning method for maximizing undiscounted rewards. in international conference on machine learning  volume 1  1.
 singh and dayan  1  s. p. singh and p. dayan. analytical mean squared error curves for temporal difference learning. machine learning  1.
 singh and sutton  1  s. p. singh and r. s. sutton. reinforcement learning with replacing eligibility traces. machine learning  1.
 sutton  1  r. s. sutton. learning to predict by the methods of temporal differences. machine learning  1-1  1.
 watkins  1  c. j. watkins. learning from delayed rewards. phd thesis  cambridge university  cambridge  england  1.
 wiering and schmidhuber  1  m. a. wiering and j. schmidhuber. fast online q   . machine learning  1.
exploiting multiple secondary reinforcers in policy gradient reinforcement learninggreg grudic
ircs and grasp lab
university of pennsylvania
philadelphia  pa  usa grudic linc.cis.upenn.edu lyle ungar
computer and information science
university of pennsylvania philadelphia  pa  usa ungar cis.upenn.edu

abstract
most formulations of reinforcement learning depend on a single reinforcement reward value to guide the search for the optimal policy solution. if observation of this reward is rare or expensive  converging to a solution can be impractically slow. one way to exploit additional domain knowledge is to use more readily available  but related quantities as secondary reinforcers to guide the search through the space of all policies. we propose a method to augment policy gradient reinforcement learning algorithms by using prior domain knowledge to estimate desired relative levels of a set of secondary reinforcement quantities. rl can then be applied to determine a policy which will establish these levels. the primary reinforcement reward is then sampled to calculate a gradient for each secondary reinforcer  in the direction of increased primary reward. these gradients are used to improve the estimate of relative secondary values  and the process iterates until reward is maximized. we prove that the algorithm converges to a local optimum in secondary reward space  and that the rate of convergenceof the performancegradient estimate in secondary reward space is independent of the size of the state space. experimental results demonstrate that the algorithm can converge many orders of magnitude faster than standard policy gradient formulations.
1	introduction
reinforcement learning  rl  is a trial and error procedure by which an agent learns to improve the reward it receives from the environment. at the core of all rl algorithms is a search procedure that directs the hypothesis and test process by which the agent discovers how to improve its behaviour. the efficiency of an rl algorithm is therefore governed by the size of the agent's search space  i.e. problem domain size  and the effectiveness of the search procedure used. when the problem domain is small enough  or enough prior knowledge is used to direct search  rl algorithms can yield effective solutions which are not easily achievable by other means  kaelbling et al.  1 .
　there is little doubt that mammals  as well as other biological systems  successfully use some form of reinforcement learning  sutton and barto  1 . furthermore  the environments where most mammals live are varied  complex  and unpredictable  implying a large problem domain that cannot be blindly searched to the animals advantage. thus  mammals must employ intelligent search strategies in order to learn how to improve the rewards they receive. one search strategy employed by mammals involves the use of secondary reinforcers  which are stimuli that can be associated with the primary reinforcement reward  and thus have similar reinforcing characteristics. secondary reinforcers are useful to an organism if it can more easily learn how to obtain the stimuli associated with secondary reinforcement than those associated with a primary reinforcement. in general  organisms seek the goals of successful breeding and long term survival. these however can rarely be sampled and therefore provide little guidance in the search for a survival 'policy'. more immediate stimuli such as fear  hunger  cold  pain or pleasure can be much more readily observed and have direct consequences for the animal's long term reward. for example  if a mouse learns that hanging around a kitchen  a secondary reinforcer  is sometimes associated with the primary reinforcer of assuaging hunger  and not starving   then  because the kitchen is always there while the food is not  the mouse can more easily learn to modify its behavior to attain the secondary reinforcer than the primary one.
　minsky  minsky  1  was one of the first to argue that the use of secondary reinforcers may be an important ingredient to creating an artificial intelligence that is able to learn through reinforcement learning. indeed  one of the motivating factors behind the widely successful temporal-difference reinforcement learning algorithm  td     is the notion of secondary reinforcers  sutton and barto  1 .
　in this work we propose to extend the use of secondary reinforcers to improve the rate of convergence of rl algorithms. to date  the use of secondary reinforcers in rl has largely been limited to simply noting which stimuli are correlated with the primary reward and having the agent learn to maximize these stimuli independently. however  this simple notion of blindly maximizing all secondary reinforcer stimuli is naive because there are potentiallycomplicated interactions among secondary reinforcers associated with a problem domain. in fact  the agent is likely to gain an advantage by controlling the relative amounts of stimuli from each reinforcer  rather than concentrating on achieving maximal values. even in the simple example of a mouse learning to spend time in the kitchen because food may fall to the floor  the mouse may not be best served by spending all of his time in the kitchen because a cat may also at times wander in  and greatly curtail the mouse's long term survival. thus the mouse might want to control the percentage of time it spends in kitchen in order to minimize the risk of running into the cat. hence  even in simple examples  controlling how much of each secondary reinforcer the agent should experience is important.
　consider the example of a student who must write a set of 1 hour exams  one per day   with 1 hours between when one exam finishes and the next exam is scheduled to begin. she knows that both studying and getting a good night's sleep are associated with getting good marks on an exam  so there are two secondary reinforcers which she can use. she can modify her behavior in such a way that she will study  the first secondary reinforcer  for a longer or shorter period  or converselyshe will learn how to get a longer or shorter night's sleep  the other secondaryreinforcer . however if she simply spends the next 1 hours sleeping  or if she forces herself to study for the next 1 hours  she will do poorly on the exam. thus  in order to maximize her exam marks  she will need to determine what policy actions  drink more coffee  spend more time at the library  drink more warm milk etc  she must perform in order to get the right ratio of the two secondary reinforcers  i.e. sleep and study .
1	srpg method overview
in this work we propose a new algorithm called secondary reinforcers policy gradient  srpg  for solving the problem of determining the relative amounts of secondary reinforcers an agent needs to experience in order to increase its primary reward. our framework assumes that the agent starts with prior domain knowledge defining a set of secondary reinforcers and in what relative amounts these should be experienced. we further assume that the secondary reinforcers are more readily observed than the primary reward  and thus the agent can more easily learn strategies which allow it to experience these stimuli in the appropriate ratios.
our algorithm consists of the following repeated steps:
step i: use prior domain knowledge to establish the initial estimate of desired relative amounts of secondary reinforcer stimuli. use rl to learn a policy which will yield these levels of stimuli. during this step  the agent need never experience the primary stimulus  only the secondary ones.
step ii: based on the policy established in step i a gradient on the primary reward is calculated with respect to each secondary reinforcer stimulus. these gradients are then used to update the desired relative amounts of the secondary reinforcers and step i of the algorithm is repeated.
if the important secondary reinforcers and the initial guess at their relative ratios is chosen wisely  then we show both theoretically and experimentally that the agent can improve the reward it receives significantly faster than if no such prior knowledge is used. because in rl problems it is often the case that primary reinforcers are infrequently experienced by the agent  the rate of convergencemeasureused in this work is the number of times the agent needs to experience a primary reward to achieve a specific amount of reward.
　in applying our proposed framework to the exam studying example given above  the student may start by knowing that sleep and study are both important stimuli  and that she will likely need eight hours of sleep and twelve hours of study. thus the student can spend many 1 hour periods learning which actions allow her to sleep for eight hours and study for twelve  without ever taking an exam. but by the time she does take an exam  she is quite good at sleeping for eight hours and studying for twelve. once she observes her first exam grades  she can learn to maximize her grade by systematically adjusting her sleep/wake ratio  i.e. her secondary reinforcers  which prior knowledge tells her are directly related to her mark  rather than just adjusting actions such as drinking milk or coffee based on a primary reward  i.e. her exam mark  which she only rarely observes.
　the proposed algorithm is based on methods derived from policy gradient reinforcement learning  pgrl   williams  1; 1; baird and moore  1; sutton et al.  1; konda and tsitsiklis  1; baxter and bartlett  1; grudic and ungar  1b; 1a   rather than value function reinforcement learning  vfrl  methods such as learning  sutton and barto  1 . the motivation for this is threefold. 1  vfrl methods work by learning a state-action value function which defines the value of executing each action in each state  thus allowing the agent to choose the most valuable action in each state. as a result  the search space grows exponentially with both states and actions and therefore in large problem domains learning a state-action value function can be infeasible  kaelbling et al.  1 . function approximation techniques have been proposed as a method for generalizing state-action value functionsin large state spaces  however  this is still very much an open research problem. conversely  pgrl methods work by starting with an initial parameterized policy which defines the probability of executing a given action in a given state. typically  far fewer parameters are used to define the policy than there are states  thus directly addressing the need for generalization in large state spaces. pgrl algorithms work by calculating a gradient along a direction of increased reward in parameter space  and then incrementally modifying the policy parameters. pgrl algorithms are theoretically guaranteedto convergeto only locally optimal policies  whereas vfrl algorithms can find globally optimal solutions. however  in practice it is usually not feasible to convergeto globally optimal solutions in large problem domains in any case. furthermore  the computational cost of pgrl algorithms grows linearly in the number of parameters used to define the policy  which is in sharp contrast to the exponential growth associated with vfrl algorithms. 1  because pgrl algorithms start with a parameterizedpolicy  it is relatively simple to choose a policy which incorporates prior knowledge via an appropriate choice of the parametric form of the policy. for example  if we know that in certain states only specific actions are possible  we can choose a policy parameterization which reflects this. the use of prior knowledge in vfrl algorithms is not as easily realized. finally  1  many real problem domains are only partially observable  i.e. the agent's current state cannot be directly observed  but rather must be inferred through observations taken over many time steps   and vfrl algorithms are known to be difficult to implement in such domains. conversely  pgrl algorithms have been shown to work effectively in partially observable problem domains  peshkin et al.  1 .
1	theoretical framework
1	mdp assumptions
we make the usual markov decision process  mdp  assumptions and for simplicity  and without loss of generality   we assume that the agent interacts with the environment in a series of episodes of duration . the agent's state at time is given by   . note that the state space can either be discrete or continuous. at each time step the agent chooses from one of discrete actions
       . the dynamics of the environment are characterized by transition probabilities
.
　because we are using a policy gradient algorithm  we further assume that the policy followed by the agent is characterized by a parameter vector	  and the probability that the agent executes action	in state	is given by	 
 sutton et al.  1 . in addition we assume that is differentiable with respect to .
1	primary reinforcer
we assume the primary reward the agent receives at time is given by . the agent's goal is to optimize either the average reward function given by:
		 1 
or the discounted reward function given by:
 1 
where is a discount factor and is some initial starting state. note that both reward functions are assumed to be a function of the agent's policy which is parameterized by
.
1	secondary reinforcers
at each time	the agent senses one of	stimuli denoted by
	  where	 	.
these stimuli constitute the agent's secondary reinforcers  and the average expected stimuli over an episode for each reinforcer  here termed secondary reinforcer frequencies  is given by:
		 1 
               . as with the primary reward functions  1  and  1   these secondaryreward stimulus functions are a function of the agent's policy which is defined by the parameters
.
1	srpg algorithm
let indicate the number of times each step in the algorithm has been executed  and be the current desired secondary reinforcer frequencies corresponding to . the secondary reinforcers policy gradient  srpg  algorithm consists of the following two steps:
step 1: learn a policy that achieves the currently desired frequency of secondary reinforcers : in this step the agent establishes these secondary reinforcer frequencies by learning a set of policy parameters that solve the system of nonlinear equations defined by:
 1 
step 1: calculate the gradient of increased primary reward in secondary reinforcers space and update desired secondary reinforcer frequencies: estimate the gradient of the primary reward function with respect to the desired secondary reinforcer frequencies :
		 1 
given this estimate  the next estimate for the secondary reinforcer frequencies    is given by:
		 1 
where is a small positive step size. increment and goto step 1.
　note that in step 1 of the algorithm the agent does not need to observe the primary reward . instead the agent must solve the nonlinear system of equations  1 . this can be done using a number of different algorithms  press et al.  1 ; here we briefly describe one such procedure. the algorithm used in this paper is to first estimate the gradient:

and then use the update equation:
		 1 
where is a small positive step size. assuming that the algorithm doesn't run into a local minimum  will converge to as becomes large. see section 1 for a discussion of what happens when the algorithm encounters a local minimum. note that the algorithm requires estimates of:

for . these estimates can be made using standard policy gradient algorithms  williams  1; 1; sutton et al.  1; konda and tsitsiklis  1; baxter and bartlett  1 . for example  if we use the formulation defined in  sutton et al.  1   then
			 1 
where is the stationary distribution of states under and is given by:

　in step 1 of the algorithm the agent must numerically calculate the gradient  1 . this is numerically estimated using the following:

 1 
where   such that every element of the vector is zero except for element . therefore  1  is estimated using small perturbations  i.e.   in each of the elements of   solving the system for as outlined above  and then observing the associated . thus in step 1 the agent must observe the primary reward function at least
times  once for and once for each of the secondary stimuli for	.
1	convergence results
we present two theorems. the first establishes the conditions under which the srpg algorithm converges.
　theorem 1: let     such that and . assume that   
		and that	exists such that
 1  is satisfied. then  the sequence of defined by the srpg algorithm converges such that for any mdp with bounded primary reward  .
　proof: the necessary condition on the bound  is established by the bound on  due to equation  1 .
the remainder of the proof then follows directly from proposition 1 on page 1 of  bertsekas and tsitsiklis  1 . 
　the second theorem proves that the rate of convergence of the gradient estimate  1   with respect to the number of primary reinforcers observed  is independent of the size of the agent's state space.
theorem 1:	and assume that there exists a finite
 i.e.
lipschitz smoothness condition . assume also that the variance in the primary reinforcer estimate is . then  for any arbitrary small positive   there exists a such

figure 1: convergence results  as a function the number of observations of the primary reinforcer  for mdp problems with 1  1  and 1 states are given.
that

                                              1  further  assume that each observation of is independent  then the variance in the gradient is given by:

 1 
where is the number of times and are observed.
	proof: equation  1  is satisfied by choosing	such that
. equation  1  is obtained by noting that if and if and are independent  then
	. further  if	and
	is observations of	  then if	is observed	times  then
	.	
1	experimental results
we implemented mdp examples with 1  1 and 1 discrete
states respectively. the mdps were fully connected and therefore in the 1 state example the agent had 1 actions to choose from in each state  in the 1 state example the agent had 1 actions in each state  and so on. the actions are chosen using a boltzmann distribution as follows:

where is the total number of states. the transition probabilities of the mdp are as follows: if action is chosen in state at time   then	. the agent interacts with the mdp in a series of episodes lasting 1 time steps each  always starting in state .
the following reward is given to the agent at the end of each episode:
where no of visits to state . thus  in all three examples  the agent achieves maximum reward if it spends 1% of the time in state and 1% of the time in state .
initially  the values for all the policy parameters are set to 1.
　two secondary reinforcers are used by the srpg algorithm: and . the agent starts with the desired secondary reinforcer frequencies and in step 1 of the srpg algorithm. the srpg algorithm learning rates are set to . figure 1 shows learning curves averaged over 1 runs with error bars indicating standard deviation. note that  as predicted by theorem1  the convergence of the algorithm as a function of the number of observed primary rewards is not affected by the number of states in the mdp.
　the rate of convergence of the srpg algorithm was compared to the the pg algorithm proposed in  sutton et al.  1 :

where is the stationary distribution of states under  estimated after each episodeby countingthe numberof times each state is visited  and is given by:

a typical learningcurve  as a functionof the numberof observations of the primaryreinforcer for this pg algorithmfor the 1 state mpd is given in figure 1  as with the srpg algorithm the learning step size was set to  . note that convergence took approximately 1 observations of the primary reward function . convergence of the pg algorithm for the 1 state example took about 1 observations of . for the 1 state example  convergence was not observed after 1 1 observations of   at which time the algorithm was stopped.
　figure 1 also shows the learning curves  averaged over 1 runs with error bars showing standard deviation  for the pg algorithm after the first 1 observations of the primary reward . note that although the srpg algorithm converges after about 1 runs for all three examples  the pg algorithm does not appreciably improve the reward received by agent over this number of observations of .
1	conclusion and discussion
in large problem domains  prior knowledge is essential for an agent to effectively learn via reinforcement reward. however  how domain specific knowledge should be added to rl algorithms to the agent's advantage is still an open research problem. secondary reinforcers are stimuli that are directly

figure 1: a typical learning curve for the pg algorithm  as a function the number of observations of the primary reinforcer  on an 1 state mdp problem.
related to a primary reinforcement  and thus their relative reward properties can be measured with respect to agent's global reward function. in this paper we show that secondary reinforcers are a fruitful source of domain specific knowledge which can be effectively used to guide search. in particular  if the primary reinforcement is observed only rarely  while secondary reinforcers are more readily observable  the rate of convergence of rl algorithms  with respect to the number of observations of a primary reward  can be significantly improved.
　however  not all secondary reinforcers should be blindly maximized. some are more important than others  and in order to make full use of secondary reinforcers  an agent should control the relative amounts of this environmental feedback. this paper presents a new policy gradient algorithm  called secondary reinforcer policy gradient  srpg   that iterates in the space of secondary reinforcer stimuli to a locally optimal policy. the algorithm consists of two steps. the first step starts with an assumption of the relative amounts of secondary reinforcer stimuli that the agent will need to establish to achieve good primary reward. in this first step the agent learns a policy which achieves these relative amounts of stimuli using a standard policy gradient rl algorithm. a key property of srpg is that a primary reward need not be observed during this step of the algorithm  and therefore the convergence of algorithm is not slowed because the primary reward is rarely observed. the second step of the algorithm involves the calculation of a gradient in the direction increased primary reward  in the space secondary reinforcer stimuli. if there are secondary reinforcers  then step two of the algorithm requires that the primary reinforcer be sampled times. however  we present theory showing that the rate of convergence of the gradient estimate is independent of the size of the state space  making the algorithm viable on high dimensional problems. this estimated gradient is used to update the relative desired amounts of secondary reinforcers  and step one of the algorithm is then executed to establish a policy to achieve these relative amounts of stimuli  thus improving the primary reward. step one and two are repeated until the policy converges to a local optimum.
　experimental results are presented which show that the srpg algorithm can improve the convergence of policy gradient  pg  algorithms  requiring many orders of magnitude fewer observations of the primary reward to converge. in addition  we present theory showing that the srpg algorithm converges to a locally optimal policy  with respect to the primary reward  in the space of secondary reinforcers. however  in order for this theorem to hold  step one of the algorithm must be able to establish a policy that achieves the specified relative amounts of secondary reinforcer stimuli. the algorithm used in this paper to solve this nonlinear system of equations is based on gradient descent  and is therefore prone to convergence to local minimum. however  in the experimental results presented here  the srpg algorithm converged to close to optimal primary reward  even when local minima were encountered in step one of the algorithm. the reason for this is that these local minima sufficiently improved the primary reward  thus allowing the overall algorithm to continue to converge. therefore  establishing a more relaxed set of conditions under which the srpg algorithm will converge is an open research topic.
　further open research questions include the use of secondaryreinforcerswithin the actor-critic reinforcementlearning framework  sutton and barto  1 . actor critic algorithms combine policy gradient and value function methods and thus can often achieve successful policies when either method alone would fail. such algorithms may further benefit from domain specific knowledge derived from secondary reinforcers. finally  an important open research topic is  given a large list of potential secondary reinforcers  formulate an algorithm that finds a small subset which can be used to improve rl algorithms.
acknowledgements
thanks to jane mulligan for useful discussions. this work was funded by the grasp lab  the ircs at the university of pennsylvania  and by the darpa ito mars grant no. dabt1-1.
references
 baird and moore  1  l. baird and a. w. moore. gradient descent for general reinforcement learning. in michael i. jordan  michael j. kearns  and sara a. solla  editors  advances in neural information processing systems  volume 1  cambridge  ma  1. mit press.
 baxter and bartlett  1  jonathan baxter and peter l. bartlett. reinforcement learning in pomdp's via direct gradient ascent. in proceedings of the seventeenth international conference on machine learning  icml'1   pages 1  stanford university  ca  june 1.
 bertsekas and tsitsiklis  1  d. p. bertsekas and j. n. tsitsiklis. neuro-dynamic programming. athena scientific  1.
 grudic and ungar  1a  g. z. grudic and l. h. ungar. localizing policy gradient estimates to action transitions.
in proceedings of the seventeenth international conference on machine learning  volume 1  pages 1. morgan kaufmann  june 1 - july 1.
 grudic and ungar  1b  g. z. grudic and l. h. ungar. localizing search in reinforcement learning. in proceedings of the seventeenth national conference on artificial intelligence  volume 1  pages 1. menlo park  ca: aaai press / cambridge  ma: mit press  july 1 - august 1.
 kaelbling et al.  1  l. p. kaelbling  m. l. littman  and a. w. moore. reinforcement learning: a survey. journal of artificial intelligence research  1-1  1.
 konda and tsitsiklis  1  v. r. konda and j. n. tsitsiklis. actor-critic algorithms. in s. a. solla  t. k. leen  and k.-r. mller  editors  advances in neural information processing systems  volume 1  cambridge  ma  1. mit
press.
 minsky  1  marvin minsky. steps toward artificial intelligence. in e. a. feigenbaum and j. feldman  editors  computers and thought  pages 1. mcgrawhill book company  new york  n.y.  1.
 peshkin et al.  1  leonid peshkin  kee-eung kim  nicolas meuleau  and leslie pack kaelbling. learning to cooperate via policy search. in proceedings of the sixteenth international conference on uncertainty in artificial intelligence  uai 1   stanford university  ca  june 1.
 press et al.  1  w. h. press  b. p. flannery  s. a. teukolsky  and w. t. vetterling. numerical recipes in c. cambridge university press  new york ny  1.
 sutton and barto  1  r. s. sutton and a. g. barto. time-derivative models of pavlovian reinforcement. in m. gabriel and j. moore  editors  learning and computational neuroscience: foundations of adaptive networks  pages 1. mit press  1.
 sutton and barto  1  r. s. sutton and a. g. barto. reinforcement learning: an introduction. mit press  cambridge  ma  1.
 sutton et al.  1  r. s. sutton  d. mcallester  s. singh  and y. mansour. policy gradient methods for reinforcement learning with function approximation. in s. a. solla  t. k. leen  and k.-r. mller  editors  advances in neural information processing systems  volume 1  cambridge  ma  1. mit press.
 williams  1  r. j. williams. a class of gradientestimating algorithms for reinforcement learning in neural networks. in proceedings of the ieee first international conference on neural networks  san diego  ca  1.
 williams  1  r. j. williams. simple statistical gradientfollowing algorithms for connectionist reinforcement learning. machine learning  1 :1  1.

machine learning and
data mining
machine learning and data mining

the foundations of cost-sensitive learning
charles elkan
department of computer science and engineering 1
university of california  san diego
la jolla  california 1 elkan cs.ucsd.edu

abstract
this paper revisits the problem of optimal learning and decision-making when different misclassification errors incur different penalties. we characterize precisely but intuitively when a cost matrix is reasonable  and we show how to avoid the mistake of defining a cost matrix that is economically incoherent. for the two-class case  we prove a theorem that shows how to change the proportion of negative examples in a training set in order to make optimal cost-sensitive classification decisions using a classifier learned by a standard non-costsensitive learning method. however  we then argue that changing the balance of negative and positive training examples has little effect on the classifiers produced by standard bayesian and decision tree learning methods. accordingly  the recommended way of applying one of these methods in a domain with differing misclassification costs is to learn a classifier from the training set as given  and then to compute optimal decisions explicitly using the probability estimates given by the classifier.
1	making decisions based on a cost matrix
given a specification of costs for correct and incorrect predictions  an example should be predicted to have the class that leads to the lowest expected cost  where the expectation is computed using the conditional probability of each class given the example. mathematically  let the entry in a cost matrix be the cost of predicting class when the true class is . if then the prediction is correct  while if the prediction is incorrect. the optimal prediction for an example is the class that minimizes
 1 
costs are not necessarily monetary. a cost can also be a waste of time  or the severity of an illness  for example.
　for each   is a sum over the alternative possibilities for the true class of . in this framework  the role of a learning algorithm is to produce a classifier that for any example can estimate the probability of each class being the true class of . for an example   making the prediction means acting as if is the true class of . the essence of cost-sensitive decision-making is that it can be optimal to act as if one class is true even when some other class is more probable. for example  it can be rational not to approve a large credit card transaction even if the transaction is most likely legitimate.
1	cost matrix properties
a cost matrix always has the following structure when there are only two classes:
actual negativeactual positivepredict negativepredict positiverecent papers have followed the convention that cost matrix rows correspond to alternative predicted classes  while columns correspond to actual classes  i.e. row/column = / = predicted/actual.
　in our notation  the cost of a false positive is while the cost of a false negative is . conceptually  the cost of labeling an example incorrectly should always be greater than the cost of labeling it correctly. mathematically  it should always be the case that and . we call these conditions the  reasonableness  conditions.
　suppose that the first reasonableness condition is violated  so but still . in this case the optimal policy is to label all examples positive. similarly  if but then it is optimal to label all examples negative. we leave the case where both reasonableness conditions are violated for the reader to analyze.
　margineantu  has pointed out that for some cost matrices  some class labels are never predicted by the optimal policy as given by equation  1 . we can state a simple  intuitive criterion for when this happens. say that row dominates row in a cost matrix if for all   .
in this case the cost of predicting is no greater than the cost of predicting   regardless of what the true class is. so it is optimal never to predict . as a special case  the optimal prediction is always if row is dominated by all other rows in a cost matrix. the two reasonableness conditions for a two-class cost matrix imply that neither row in the matrix dominates the other.
　given a cost matrix  the decisions that are optimal are unchanged if each entry in the matrix is multiplied by a positive constant. this scaling corresponds to changing the unit of account for costs. similarly  the decisions that are optimal are unchanged if a constant is added to each entry in the matrix. this shifting corresponds to changing the baseline away from which costs are measured. by scaling and shifting entries  any two-class cost matrix that satisfies the reasonableness conditions can be transformed into a simpler matrix that always leads to the same decisions:
11where	and
             . from a matrix perspective  a 1 cost matrix effectively has two degrees of freedom.
1	costs versus benefits
although most recent research in machine learning has used the terminology of costs  doing accounting in terms of benefits is generally preferable  because avoiding mistakes is easier  since there is a natural baseline from which to measure all benefits  whether positive or negative. this baseline is the state of the agent before it takes a decision regarding an example. after the agent has made the decision  if it is better off  its benefit is positive. otherwise  its benefit is negative.
　when thinking in terms of costs  it is easy to posit a cost matrix that is logically contradictory because not all entries in the matrix are measured from the same baseline. for example  consider the so-called german credit dataset that was published as part of the statlog project  michie et al.  1 . the cost matrix given with this dataset is as follows:
actual badactual goodpredict bad1predict good1here examples are people who apply for a loan from a bank.  actual good  means that a customer would repay a loan while  actual bad  means that the customer would default. the action associated with  predict bad  is to deny the loan. hence  the cashflow relative to any baseline associated with this prediction is the same regardless of whether  actual good  or  actual bad  is true. in every economically reasonable cost matrix for this domain  both entries in the  predict bad  row must be the same.
　costs or benefits can be measured against any baseline  but the baseline must be fixed. an opportunity cost is a foregone benefit  i.e. a missed opportunity rather than an actual penalty. it is easy to make the mistake of measuring different opportunity costs against different baselines. for example  the erroneous cost matrix above can be justified informally as follows:  the cost of approving a good customer is zero  and the cost of rejecting a bad customer is zero  because in both cases the correct decision has been made. if a good customer is rejected  the cost is an opportunity cost  the foregone profit of 1. if a bad customer is approved for a loan  the cost is the lost loan principal of 1. 
　to see concretely that the reasoning in quotes above is incorrect  suppose that the bank has one customer of each of the four types. clearly the cost matrix above is intended to imply that the net change in the assets of the bank is then 1. alternatively  suppose that we have four customers who receive loans and repay them. the net change in assets is then +1. regardless of the baseline  any method of accounting should give a difference of 1 between these scenarios. but with the erroneous cost matrix above  the first scenario gives a total cost of 1  while the second scenario gives a total cost of 1.
　in general the amount in some cells of a cost or benefit matrix may not be constant  and may be different for different examples. for example  consider the credit card transactions domain. here the benefit matrix might be
fraudulentlegitimaterefuse$1$1approve1where is the size of the transaction in dollars. approving a fraudulent transaction costs the amount of the transaction because the bank is liable for the expenses of fraud. refusing a legitimate transaction has a non-trivial cost because it annoys a customer. refusing a fraudulent transaction has a nontrivial benefit because it may prevent further fraud and lead to the arrest of a criminal. research on cost-sensitive learning and decision-making when costs may be example-dependent is only just beginning  zadrozny and elkan  1a .
1	making optimal decisions
in the two-class case  the optimal prediction is class 1 if and only if the expected cost of this prediction is less than or equal to the expected cost of predicting class 1  i.e. if and only if
which is equivalent to
given . if this inequality is in fact an equality  then predicting either class is optimal.
	the threshold for making optimal decisions is	such that
assuming the reasonableness conditions the optimal prediction is class 1 if and only if . rearranging the equation for leads to the solution
		 1 
assuming the denominator is nonzero  which is implied by the reasonableness conditions. this formula for shows that any 1 cost matrix has essentially only one degree of freedom from a decision-making perspective  although it has two degrees of freedom from a matrix perspective. the cause of the apparent contradiction is that the optimal decision-making policy is a nonlinear function of the cost matrix.
1	achieving cost-sensitivity by rebalancing
in this section we turn to the question of how to obtain a classifier that is useful for cost-sensitive decision-making.
　standard learning algorithms are designed to yield classifiers that maximize accuracy. in the two-class case  these classifiers implicitly make decisions based on the probability threshold 1. the conclusion of the previous section was that we need a classifier that given an example   says whether or not for some target threshold that in general is different from 1. how can a standard learning algorithm be made to produce a classifier that makes decisions based on a general  
　the most common method of achieving this objective is to rebalance the training set given to the learning algorithm  i.e. to change the the proportion of positive and negative training examples in the training set. although rebalancing is a common idea  the general formula for how to do it correctly has not been published. the following theorem provides this formula.
theorem 1: to make a target probability threshold correspond to a given probability threshold   the number of negative examples in the training set should be multiplied by

while the formula in theorem 1 is simple  the proof of its correctness is not. we defer the proof until the end of the next section.
　in the special case where the threshold used by the learning method is and   the theorem says that the number of negative training examples should be multiplied by this special case is used by breiman et al. .
　the directionality of theorem 1 is important to understand. suppose we have a learning algorithm that yields classifiers that make predictions based on a probability threshold . given a training set and a desired probability thresh-
old   the theorem says how to create a training set by changing the number of negative training examples such that applied to gives the desired classifier.
　theorem 1 does not say in what way the number of negative examples should be changed. if a learning algorithm can use weights on training examples  then the weight of each negative example can be set to the factor given by the theorem. otherwise  we must do oversampling or undersampling. oversampling means duplicating examples  and undersampling means deleting examples.
　sampling can be done either randomly or deterministically. while deterministic sampling can reduce variance  it risks introducing bias  if the non-random choice of examples to duplicate or eliminate is correlated with some property of the examples. undersampling that is deterministic in the sense that the fraction of examples with each value of a certain feature is held constant is often called stratified sampling.
　it is possible to change the number of positive examples instead of or as well as changing the number of negative examples. however in many domains one class is rare compared to the other  and it is important to keep all available examples of the rare class. in these cases  if we call the rare class the positive class  theorem 1 says directly how to change the number of common examples without discarding or duplicating any of the rare examples.
1	new probabilities given a new base rate
in this section we state and prove a theorem of independent interest that happens also to be the tool needed to prove theorem 1. the new theorem answers the question of how the predicted class membership probability of an example should change in response to a change in base rates. suppose that is correct for an example   if is drawn from a population with base rate positive examples. but suppose that in fact is drawn from a population with base rate . what is  
　we make the assumption that the shift in base rate is the only change in the population to which	belongs. formally  we assume that within the positive and negative	subpopulations 	example probabilities are unchanged: and	. given these assumptions  the following theorem shows how to compute	as a function of	    and	.
theorem 1: in the context just described 

proof: using bayes' rule 	is

because	and	are mutually exclusive 	is
let	  let	  and let	.
then

similarly 

now we can solve for	as a function of	and	. we have so	then the denominator for	is


finally we have

it is important to note that theorem 1 is a statement about true probabilities given different base rates. the proof does not rely on how probabilities may be estimated based on some learning process. in particular  the proof does not use any assumptions of independence or conditional independence  as made for example by a naive bayesian classifier.
　if a classifier yields estimated probabilities that we assume are correct given a base rate   then theorem 1 lets us

compute estimated probabilities that are correct given a different base rate . from this point of view  the theorem has a remarkable aspect. it lets us use a classifier learned from a training set drawn from one probability distribution on a test set drawn from a different probability distribution. the theorem thus relaxes one of the most fundamental assumptions of almost all research on machine learning  that training and test sets are drawn from the same population. the insight in the proof is the introduction of the variable that is the ratio of and . if we try to compute the actual values of these probabilities  we find that we have more variables to solve for than we have simultaneous equations. fortunately  all we need to know for any particular example is the ratio .
　the special case of theorem 1 where was recently worked out independently by weiss and provost . the case where is also interesting. suppose that we do not know the base rate of positive examples at the time we learn a classifier. then it is reasonable to use a training set with
       . theorem 1 says how to compute probabilities later that are correct given that the population of test examples has base rate . specifically 

this function of	and	is plotted in figure 1.
　using theorem 1 as a lemma  we can now prove theorem 1 with a slight change of notation.
theorem 1: to make a target probability threshold correspond to a given probability threshold   the number of negative training examples should be multiplied by

proof: we want to compute an adjusted base rate such that for a classifier trained using this base rate  an estimated probability corresponds to a probability for a classifier trained using the base rate .
　we need to compute the adjusted as a function of     and . from the proof of theorem 1 
               . collecting all the terms on the left  we have	  which gives that
the adjusted base rate should be

suppose that and so the number of negative training examples should be multiplied by to get the adjusted base rate . we have that is


therefore

note that the effective cardinality of the subset of negative training examples must be changed in a way that does not change the distribution of examples within this subset.
1	effects of changing base rates
changing the training set prevalence of positive and negative examples is a common method of making a learning algorithm cost-sensitive. a natural question is what effect such a change has on the behavior of standard learning algorithms. separately  many researchers have proposed duplicating or discarding examples when one class of examples is rare  on the assumption that standard learning methods perform better when the prevalence of different classes is approximately equal  kubat and matwin  1; japkowicz  1 . the purpose of this section is to investigate this assumption.
1	changing base rates and bayesian learning
given an example   a bayesian classifier applies bayes' rule to compute the probability of each class as
　　　　　　　　typically is computed by a function learned from a training set  is estimated as the training set frequency of class   and is computed indirectly by solving the equation .
　a bayesian learning method essentially learns a model of each class separately. if the frequencyof a class is changed in the training set  the only change is to the estimated base rate of each class. therefore there is little reason to expect the accuracy of decision-making with a bayesian classifier to be higher with any particular base rates.
　naive bayesian classifiers are the most important special case of bayesian classification. a naive bayesian classi-
fier is based on the assumption that within each class  the values of the attributes of examples are independent. it is well-known that these classifiers tend to give inaccurate probability estimates  domingos and pazzani  1 . given an example   suppose that a naive bayesian classifier computes as its estimate of . usually
is too extreme: for most   either is close to 1 and then or is close to 1 and then
.
however  the ranking of examples by naive bayesian clas-
sifiers tends to be correct: if	then
. this fact suggests that given a cost-
sensitive application where optimal decision-making uses the probability threshold   one should empirically determine a different threshold such that is equivalent to
　　　　　　　　. this procedure is likely to improve the accuracy of decision-making  while changing the proportion of negative examples using theorem 1 in order to use the threshold 1 is not.
1	decision tree growing
we turn our attention now to standard decision tree learning methods  which have two phases. in the first phase a tree is grown top-down  while in the second phase nodes are pruned from the tree. we discuss separately the effect on each phase of changing the proportion of negative and positive training examples.
　a splitting criterion is a metric applied to an attribute that measures how homogeneous the induced subsets are  if a training set is partitioned based on the values of this attribute. consider a discrete attribute that has values
through for some . in the two-class case  standard splitting criteria have the form
where and all probabilities are frequencies in the training set to be split based on . the function measures the impurity or heterogeneity of each subset of training examples. all such functions are qualitatively similar  with a unique maximum at   and equal minima at and .
　drummond and holte  have shown that for twovalued attributes the impurity function suggested by kearns and mansour  is invariant to changes in the proportion of different classes in the training data. we prove here a more general result that applies to all discretevalued attributes and that shows that related impurity functions  including the gini index  breiman et al.  1   are not invariant to base rate changes.
theorem 1: suppose	where and . for any collection of discrete-valued
attributes  the attribute that minimizes	usingis thesame regardless of changes in the base rate training set if   and not otherwise in general.
proof: for any attribute	  by definitionof thewhere	through	are the possible values of	.so bybayes' rule	is

grouping the	factors for each	gives that	is
now the base rate factors can be brought outside the sum  so is	times the sum
 1 
becauseis constant for all at-tributes  the attribute	for whichis minimum is deter-mined by the minimum of  1 . ifthen  1  dependsonly on	and  which do not dependon the base rates. otherwise   1  is different for different base rates because
unless the attribute	is independent of the class	  that is for	. 
　the sum  1  has its maximum value 1 if is independent of . as desired  the sum is smaller otherwise  if and are correlated and hence splitting on is reasonable.
　theorem 1 implies that changing the proportion of positive or negative examples in the training set has no effect on the structure of the tree if the decision tree growing method uses

the impurity criterion. if the algorithm uses a different criterion  such as the c1 entropy measure  the effect is usually small  because all impurity criteria are similar. the experimental results of drummond and holte 

and dietterich et al.  show that the criterion normally leads to somewhat smaller unpruned decision trees  sometimes leads to more accurate trees  and never leads to much less accurate trees. therefore we can recommend its use  and we can conclude that regardless of the impurity criterion  applying theorem 1 is not likely to have have much influence on the growing phase of decision tree learning.
1	decision tree pruning
standard methods for pruning decision trees are highly sensitive to the prevalence of different classes among training examples. if all classes except one are rare  then c1 often prunes the decision tree down to a single node that classifies all examples as members of the common class. such a classifier is useless for decision-making if failing to recognize an example in a rare class is an expensive error.
　several papers have examined recently the issue of how to obtain good probability estimates from decision trees  bradford et al.  1; provost and domingos  1; zadrozny and elkan  1b . it is clear that it is necessary to use a smoothing method to adjust the probability estimates at each leaf of a decision tree. it is not so clear what pruning methods are best.
　the experiments of bauer and kohavi  suggest that no pruning is best when using a decision tree with probability smoothing. the overall conclusion of bradford et al.  is that the best pruning is either no pruning or what they call  laplace pruning.  the idea of laplace pruning is:
1. do laplace smoothing: if training examples reach a node  of which are positive  let the estimate at this node of be .
1. compute the expected loss at each node using the smoothed probability estimates  the cost matrix  and the training set.
1. if the expected loss at a node is less than the sum of the expected losses at its children  prune the children.
we can show intuitively that laplace pruning is similar to no pruning. in the absence of probability smoothing  the expected loss at a node is always greater than or equal to the sum of the expected losses at its children. equality holds only if the optimal predicted class at each child is the same as the optimal predicted class at the parent. therefore  in the absence of smoothing  step  1  cannot change the meaning of a decision tree  i.e. the classes predicted by the tree  so laplace pruning is equivalent to no pruning.
　with probability smoothing  if the expected loss at a node is less than the sum of the expected losses at its children  the difference must be caused by smoothing  so without smoothing there would presumably be equality. so pruning the children is still only a simplification that leaves the meaning of the tree unchanged. note that the effect of laplace smoothing is small at internal tree nodes  because at these nodes typically and .
　in summary  growing a decision tree can be done in a cost-insensitive way. when using a decision tree to estimate probabilities  it is preferable to do no pruning. if costs are example-dependent  then decisions should be made using smoothed probability estimates and equation  1 . if costs are fixed  i.e. there is a single well-defined cost matrix  then each node in the unpruned decision tree can be labeled with the optimal predicted class for that leaf. if all the leaves under a certain node are labeled with the same class  then the subtree under that node can be eliminated. this simplification makes the tree smaller but does not change its predictions.
1	conclusions
this paper has reviewed the basic concepts behind optimal learning and decision-making when different misclassification errors cause different losses. for the two-class case  we have shown rigorously how to increase or decrease the proportion of negative examples in a training set in order to make optimal cost-sensitive classification decisions using a classifier learned by a standard non cost-sensitive learning method. however  we have investigated the behavior of bayesian and decision tree learning methods  and concluded that changing the balance of negative and positive training examples has little effect on learned classifiers. accordingly  the recommended way of using one of these methods in a domain with differing misclassification costs is to learn a classifier from the training set as given  and then to use equation  1  or equation  1  directly  after smoothing probability estimates and/or adjusting the threshold of equation  1  empirically if necessary.
references
 bauer and kohavi  1  eric bauer and ron kohavi. an empirical comparison of voting classification algorithms: bagging  boosting  and variants. machine learning  1-1  1.
 bradford et al.  1  j. bradford  c. kunz  r. kohavi  c. brunk  and c. brodley. pruning decision trees with misclassification costs. in proceedings of the european conference on machine learning  pages 1  1.
 breiman et al.  1  l. breiman  j. h. friedman  r. a. olshen  and c. j. stone. classification and regression trees. wadswoth  belmont  california  1.
 dietterich et al.  1  t. g. dietterich  m. kearns  and y. mansour. applying the weak learning framework to understand and improve c1. in proceedings of the thirteenth international conference on machine learning  pages 1. morgan kaufmann  1.
 domingos and pazzani  1  pedro domingos and michael pazzani. beyond independence: conditions for the optimality of the simple bayesian classifier. in proceedings of the thirteenth international conference on machine learning  pages 1. morgan kaufmann  1.
 drummond and holte  1  chris drummond and robert c.
holte. exploiting the cost  in sensitivity of decision tree splitting criteria. in proceedings of the seventeenth international conference on machine learning  pages 1  1.
 japkowicz  1  n. japkowicz. the class imbalance problem: significance and strategies. in proceedings of the international conference on artificial intelligence  las vegas  june 1.
 kearns and mansour  1  m. kearns and y. mansour. on the boosting ability of top-down decision tree learning algorithms. in proceedings of the annual acm symposium on the theory of computing  pages 1. acm press  1.
 kubat and matwin  1  m. kubat and s. matwin. addressing the curse of imbalanced training sets: one-sided sampling. in proceedings of the fourteenth international conference on machine learning  pages 1. morgan kaufmann  1.
 margineantu  1  dragos margineantu. on class probability estimates and cost-sensitive evaluation of classifiers. in workshop notes  workshop on cost-sensitive learning  international conference on machine learning  june 1.
 michie et al.  1  d. michie  d. j. spiegelhalter  and c. c. taylor. machine learning  neural and statistical classification. ellis horwood  1.
 provost and domingos  1  foster provost and pedro domingos. well-trained pets: improving probability estimation trees. technical report cder #1-is  stern school of business  new york university  1.
 weiss and provost  1  gary m. weiss and foster provost. the effect of class distribution on classifier learning. technical report ml-tr 1  department of computer science  rutgers university  1.
 zadrozny and elkan  1a  bianca zadrozny and charles elkan. learning and making decisions when costs and probabilities are both unknown. technical report cs1  department of computer science and engineering  university of california  san diego  january 1.
 zadrozny and elkan  1b  bianca zadrozny and charles elkan. obtaining calibrated probability estimates from decision trees and naive bayesian classifiers. in proceedings of the eighteenth international conference on machine learning  1. to appear.
mining soft-matching rules from textual data
un yong nahm and raymond j. mooney
department of computer sciences 
university of texas  austin  tx 1 pebronia mooney  cs.utexas.eduabstract
text mining concerns the discovery of knowledge from unstructured textual data. one important task is the discovery of rules that relate specific words and phrases. although existing methods for this task learn traditional logical rules  soft-matching methods that utilize word-frequency information generally work better for textual data. this paper presents a rule induction system  textrise  that allows for partial matching of text-valued features by combining rule-based and instance-based learning. we present initial experiments applying textrise to corpora of book descriptions and patent documents retrieved from the web and compare its results to those of traditional rule and instance based methods.
1	introduction
text mining  discovering knowledge from unstructured natural-language text  is an important data mining problem attracting increasing attention  hearst  1; feldman  1; mladenic＞  1 . existing methods for mining rules from text use a hard  logical criteria for matching rules  feldman and hirsh  1; ahonen-myka et al.  1 . however  for most text processing problems  a form of soft matching that utilizes word-frequency information typically gives superior results  salton  1; cohen  1; yang  1 . therefore  the induction of soft-matching rules from text is an important  under-studied problem.
　we present a method  textrise  for learning softmatching rules from text using a modification of the rise algorithm  domingos  1   a hybrid of rule-based and instance-based  nearest-neighbor  learning methods. such a hybrid is good match for text mining since rule-induction provides simple  interpretable rules  while nearest-neighbor provides soft matching based on a specified similarity metric. currently in textrise  we use the vector-spacemodel from information retrieval  ir  to provide an appropriate similarity metric  salton  1 .
　we present results on applying textrise to two text databases  one of book information extracted from an online bookstore  and another of patent applications available on the web. we evaluate the quality of the discovered rules on independent data by measuring the similarity of predicted text and actual text. by comparing results to the predictions made by nearest-neighbor and mined association rules  we demonstrate the advantage of mining soft-matching rules.
1	background
1	mining rules from text
several researchers have applied traditional rule induction methods to discover relationships from textual data. fact  feldman and hirsh  1  discovers rules from text using a well-known technique for association rule mining. for example  it discovered rules such as  iraq iran   and  kuwait and bahrain saudi arabia  from a corpus of reuters news articles. ahonen et al. 1  also applied existing data mining techniques to discover episode rules from text. for example:  if  chemicals  and  processing  occurs within 1 consequent words  the word  storage  co-occurs within 1 words.  is an episode rule discovered from a collection of finnish legal documents.
　in addition  decision tree methods such as c1 and c1  and rule learners such as foil  and ripper have been used to discover patterns from textual data  nahm and mooney  1b; ghani et al.  1 . all of these existing methods discover rules requiring an exact match.
1	mining information extracted from text
nahm and mooney 1a; 1b  introduced an alternative framework for text mining based on the integration of information extraction  ie  and traditional data mining. ie is a form of shallow text understanding that locates specific pieces of data in natural-language text. traditional data mining assumes that information is in the form of a relational database; unfortunately  in many applications  information is only available in the form of unstructured documents. ie addresses this problem by transforming a corpus of textual documents into a structured database. an ie module can extract data from raw text  and the resulting database can be processed by a traditional data mining component.
　in this work  extracted textual data was mined using traditional rule induction systems such as c1rules  quinlan  1  and ripper  cohen  1 . rules were induced for predicting the text in each slot using the extracted information in all other slots. however  the heterogeneity of textual databases causes a problem: the same or similar objects are often referred to using different  but similar  textual strings. this issue becomes clear when we consider the web  a vast and dynamic warehouse of text documents  as a potential target for text mining. since the web has no centralized moderator  it is highly heterogeneous  making it difficult to apply strict matching to text extracted from web documents  cohen  1 .
1	information retrieval vector-space model
the vector-space model is typically used in ir to determine the similarity of two documents. in this model  a text is represented as a vector of real numbers  where each component corresponds to a word that appears in the set of all documents and the value is its frequency in the document. this is also known as a bag of-words  bow  representation. the similarity of two documents and is the cosine of the angle between two vectors and representing and respectively  and calculated by the following formula:
		 1 
where	and	are the norms of each document vector.
　the tfidf  term frequency  inverse document frequency  weighting scheme  salton  1  is used to assign higher weights to distinguished terms in a document. tfidf makes two assumptions about the importance of a term. first  the more a term appears in the document  the more important it is  term frequency . second  the more it appears throughthe entire collection of documents  the less important it is since it does not characterize the particular document well  inverse document frequency . in the tfidf framework  the weight for term in a document   is defined as follows:
		 1 
where is the frequency of term in document   the total number of documents in collection  and the number of documents where term occurs at least once.
1	rise: learning soft-matching rules
the rise induction algorithm unifies rule-based and instance-based learning  domingos  1 . instead of requiring rules to match exactly  rise makes predictions by selecting the closest matching rule according to a standard distance metric used by nearest-neighbor methods  a modified euclidian distance . by generating generalized rules instead of remembering specific instances  and by using a similarity metric rather than exact matching to make predictions  it elegantly combines the properties of rule induction and instancebased learning.
　soft-matching rules are acquired using a specific-togeneral  bottom-up induction algorithm that starts with maximally specific rules for every example  and then repeatedly minimally generalizes each rule to cover the nearest example it does not already cover  unless this results in a decrease in book description
title : harry potter and the goblet of fire  book 1 
author : joanna k. rowling
comments: this book was the best book i have ever read. if you are in for excitement this book is the one you want to read. subject: fiction  mystery  magic  children  school  juvenile fiction  fantasy  wizards
representation
author =	 joanna    rowling 
title =	 harry    potter    goblet    fire    book 
comments =  book    book    read    excitement    read  subject =  fiction    mystery    magic    children    school    juvenile    fiction    fantasy    wizards 
figure 1: an example of representation for a book document
the accuracy of the overall rule base on the training data. this process repeats until any additional generalization decreases accuracy. when classifying examples  the nearest rule is used to predict the class. a leave-one-out method is used to determine the performance of the rule base on the training data  since an example is always correctly classified by its corresponding initial maximally-specific rule. in extensive experiments  rise was fairly consistently more accurate than alternative methods  including standard rule-based and instancebased algorithms. training is also reasonably efficient computationally  requiring time where is the number of examples  and the number of attributes.
1	the textrise algorithm
rise is not directly applicableto mining rules from extracted text because: 1  its similarity metric is not text-based and 1  it learns rules for classification rather than text prediction. textrise addresses both of these issues. we represent an ie-processed document as a list of bags of words  bows   one bag for each slot filler. we currently eliminate 1 commonly-occurringstop-words but do not perform stemming. figure 1 shows an example for a book description and its bow representation. standard set-operations are extendedto bags in the obviousway  peterson  1 . a learned rule is represented as an antecedent that is a conjunction of bows for some subset of slots and a conclusion that is a predicted bow for another slot  see figure 1 for examples .
　the standard tfidf-weighted cosine metric is used to compute the similarity of two bows. the similarity of two examples  i.e. extracted documents  or rules is the average similarity of the bows in their corresponding slots. bag intersection is used to compute the minimal generalization of two bows. the minimal generalization of two examples or rules is the minimal generalization of the bows in each of their corresponding slots. a rule is said to cover an example document if all of its antecedent bows are sub-bags of the example's corresponding bows. to extend the algorithm from classification to text prediction  we define a new measure for the accuracy of a rule set on an example set: is the average cosine similarity of
the predicted fillers for the examples in to the corresponding fillers predicted by a rule set . the algorithms for gen-
inputs:	is a rule

is an example.
	 	 	  and	are bags-of-words  possibly empty.
output:	is the generalized rule.

function most specific generalization  r  e 
for	:= 1 to	do
:=
:=
return	.
figure 1: generalization of a rule to cover an example
input:	is the training set.

output:	is the rule set.

function textrise  	 
	:=	.
compute	.
repeat
	for each rule	 
:=
	where	=	:	and
is not covered by
	:= most specific generalization 	 	 
	:=	with	replaced by
if
	then	:=
if	is identical to another rule in	  then delete	from	.
until no increase in	is obtained. return	.
figure 1: the textrise rule-learning algorithm
eralizing a rule to cover an example and for learning rules are described in figure 1 and figure 1 respectively. the algorithm is a straightforward modification of rise using the new similarity and predictive-accuracy metrics  and is used to induce soft-matching rules for predicting the filler of each slot given the values of all other slots. our implementation makes use of the the bow library  mccallum  1  for the bag-of-words text processing.
1	interestingness measures
the output of textrise is an unordered set of soft matching rules. ranking rules based on an interestingness metric can help a human user focus attention on the most promising relationships. several metrics for evaluating the  interestingness  or  goodness  of mined rules  such as confidence and support  have been proposed  bayardo jr. and agrawal  1 . however  the traditional definitions for these metrics assume exact matches for conditions. consequently  we modify these two common metrics for judging the goodness of soft-matching rules.
a rule consists of an antecedent and a consequent  and is
denoted as	where	is equal to
	. the similarity-support of an antecedent	  denoted as
　　　　　  is the number of examples in the data set that are soft-matched by . in other words  is the number of examples for which is the closest rule in the rule base. the similarity-support of rule	  denoted as
                 is defined as the sum of similarities between and the consequents of the examples soft-matched by in the data set. in these definitions  we replace the traditional hard-matching constraints for a rule with weaker constraints determined relative to all the other rules in the rule base. similarity-confidence of a rule   denoted by   is computed as below.

1	evaluation
1	data sets
two domains are employed in our evaluation of textrise: book data from amazon.com and patent data downloaded from getthepatent.com. we manually developed simple pattern-based ie systems or  wrappers  to automatically extract various labeled text-fields from the original html documents. the text extracted for each slot is then processed into a bag-of-words after removal of stop-words and remaining html commands.
　the book data set is composed of 1 subsets  science fiction  literary fiction  mystery  romance  science  and children's books. 1 titles were randomly selected for each genre to make the total size of the book data set to be 1. a wrapper extracts 1 slots: titles  authors  subject terms  synopses  published reviews  and customer comments. sample rules from this domain are given in figure 1. numbers associated to each word denotes the number of occurrences in the bag. the similarity-confidence and similarity-support values for each rule are also given.
　1 patent documents were collected from dynamically generated web pages returned by a keyword search for  artificial intelligence . four slots of titles  abstracts  claims  and descriptions are extracted for each patent. sample rules are given in figure 1.
1	results
unlike a standard rule learner that predicts the presence or absence of a specific slot value  textrise predicts a bag-ofwords for each slot. therefore  we evaluate the performance of textrise by measuring the average cosine similarity of the predicted slot values to the actual fillers for each slot. we compare the system to a standard nearest-neighbor method to show that textrise's compressed rule base is superior at predicting slot-values. in both methods  prediction is made by selecting the closest rule/example using only the text in the antecedent slots. we also tested nearest-neighbor without using information extraction to show the benefit of ie-based text mining. to clearly show ie's role  the only change made to nearest-neighbor was to treat the set of bow's for the antecedent slots as a single  larger bow.
　the experiments were performed on the 1 book descriptions using ten-fold cross validation. learning curves for predicting the title slot are shown in figure 1. the graph shows 1% confidence intervals for each point. all the results on average similarities  precisions  and f-measures were statistically evaluated by a one-tailed  paired t-test. for
rules from 1 book descriptions

title nancy 1   drew 1  synopses nancy 1 
subject children 1   fiction 1   mystery 1   detective 1   juvenile 1   espionage 1  author keene 1   carolyn 1 
synopses role 1   protein 1   absorption 1   metabolism 1   vitamins 1   minerals 1  reviews health 1  subject science 1   human 1   physiology 1  title nutrition 1 
author beatrice 1   gormley 1  synopses witness 1   ufo 1   landing 1  subject science 1   fiction 1  reviews aliens 1   ufo 1   book 1 
title charlotte 1   perkins 1   gilman 1  synopses work 1   utopias 1   herland 1   ourland 1  reviews gilman 1   author 1 
subject literature 1   criticism 1   classics 1   women 1   literary 1  comments utopia 1   feminist 1 
title dance 1  subject romance 1   fiction 1 
figure 1: sample rules from book descriptions
each training set size  two pairs of systems textrise versus nearest-neighbor and nearest-neighbor versus nearestneighbor without information extraction  were compared to determine if their differences were statistically significant
 	 .
　the results indicate that textrise does best  while nearest-neighbor without ie does worst. this shows tex-
trise successfully summarizes the input data in the form of prediction rules. the rule-compression rate of textrise is about 1%  which means the number of rules textrise produces is 1% of the number of examples originally stored in the initial rule base. we conducted the same experiments for other slots  and found similar results except for predicting the author slot. in predicting the author slot  neither information extraction nor textrise improves performance over simple nearest-neighbor.
　in addition to the textual similarity  we developed analogs for precision and recall. precision and recall were defined as follows  where is the correct bow and is the predicted one.
 1 
 1 
rules from 1 ai-related patent documents

abstract device 1   images 1  claims invention 1   system 1  description information 1   data 1   control 1   stored 1   point 1   user 1   application 1   flow 1   object 1   operation 1   software 1   storage 1  title video 1 
title automated 1 
claims device 1   based 1   determining 1   comprising 1   input 1   plurality 1   comprises 1   claim 1  abstract apparatus 1 
figure 1: sample rules from patent documents

1
1 1 1 1 1
number of training examples
figure 1: average similarities for book data  title 
f-measure is defined as the harmonic mean for precision and recall as follows:
		 1 
　learning curves for precision and f-measure are presented in figure 1 and figure 1. textrise provides higher precision  since the conclusions of many of its rules are smaller generalized bows  and overall f-measure is moderately increased.
　to compare textrise with traditional rule mining methods  we generated association rules using the apriori algorithm  agrawal and srikant  1  and a publicly available implementation  borgelt  1 . we treated each word in each slot as a separate item and generated associations between them. among all the generated rules  those with words for the slot to be predicted are selected. for each test example  a prediction is made by building a bow using the conclusions of all matching association rules. with the default

1
1 1 1 1 1
number of training examples
figure 1: precisions for book data  title 
parameter setting  minimum support of 1% and minimum confidence of 1%   the average similarity of predictions is almost 1%. we lowered the minimum support and the confidence until memory problems occurredon a sun ultra sparc 1  1mb . with the lowest minimum setting for support  1%  and confidence  1%   the average similarity remains very low: 1% for 1 training examples and 1% for 1 training examples. these results strongly suggest the usefulness of soft-matching rules in prediction tasks for textual data.
1	related research
several previous systems mine rules from text  feldman and hirsh  1; ahonen et al.  1 ; however  they discover hard-matching rules and do not use automated information extraction. ghani et al. 1  applied several rule induction methods to a database of corporationsautomatically extracted from the web. interesting rules such as  advertising agencies tend to be located in new york  were discovered; however  such learned rules must exactly match extracted text. whirl is a query processing system that combines traditional database and ir methods by introducing a  soft join  operation  cohen  1 . whirl and textrise share a focus on soft-matching rules for text processing; however  rules in whirl must be written by the user while textrise tries to discover such rules automatically.
1	future work
a potential extension of the system is to generalize to a knearest-neighbor method that uses the closest rules rather than just the single nearest rule. the predictions of these rules could be combined by taking the average of the bow vectors in their consequents. likewise during learning  rules could be generalized to the nearest uncovered examples using a similar averaging technique  possibly rounding values

1
1 1 1 1 1
number of training examples
figure 1: f-measures for book data  title 
to maintain integer counts and simplify the resulting rules. another potentially useful change to the generalization algorithm would be to use a semantic hierarchy such as wordnet  fellbaum  1 . for example  the terms  thermodynamics  and  optics  could be generalized to  physics.  finally  for short extracted strings  string edit distance  wagner and fisher  1  might be a more useful measure of textual similarity than the cosine measure.
　better metrics for evaluating the interestingness of textmined rules is clearly needed. one idea is to use a semantic network like wordnet to measure the semantic distance between the words in the antecedent and the consequent of a rule  preferring more  surprising  rules where this distance is larger. for example  this would allow ranking the rule  beer diapers  above  beer pretzels  since beer and pretzels
are both food products and therefore closer in wordnet.
　although our preliminary results are encouraging  we are planning to evaluate the approach on other corpora such as a larger database of patents  grant abstracts from the national
science foundation  or research papers gathered by cora  www.cora.whizbang.com  or researchindex  citeseer.nj.nec.com .
1	conclusions
the problem of discovering knowledge in textual data is an exiting new area in data mining. existing text-mining systems discover rules that require exactly matching substrings; however  due to variability and diversity in natural-language data  some form of soft matching based on textual similarity is needed. we have presented a system textrise that uses a hybrid of rule-based and instance-based learning methods to discover soft-matching rules from textual databases automatically constructed from document corpora via information extraction. with encouraging results of preliminary experiments  we showed how this approach can induce accurate predictive rules despite the heterogeneityof automatically extracted textual databases.
acknowledgements
this research was supported by the national science foundation under grant iri-1.
references
 agrawal and srikant  1  rakesh agrawal and ramakrishnan srikant. fast algorithms for mining association rules. in proceedings of the 1th international conference on very large databases  vldb-1   pages 1- 1  santiago  chile  september 1.
 ahonen et al.  1  helena ahonen  oskari heinonen  mika klemettinen  and a. inkeri verkamo. applying data mining techniques for descriptive phrase extraction in digital document collections. in proceedings of the ieee forum on research and technology advances in digital libraries  pages 1  santa barbara  ca  april 1.
 ahonen-myka et al.  1  helena ahonen-myka  oskari heinonen  mika klemettinen  and a. inkeri verkamo. finding co-occurring text phrases by combining sequence and frequent set discovery. in ronen feldman  editor  proceedings of the sixteenth international joint conference on artificial intelligence  ijcai-1  workshop on text mining: foundations  techniques and applications  pages 1  stockholm  sweden  august 1.
 bayardo jr. and agrawal  1  roberto j. bayardo jr. and rakesh agrawal. mining the most interesting rules. in proceedings of the fifth international conference on knowledge discovery and data mining  kdd-1   pages 1  san diego  ca  august 1.
 borgelt  1  christian borgelt. apriori version 1. http://fuzzy.cs.uni-magdeburg.de/ borgelt/  1.
 cohen  1  william w. cohen. fast effective rule induction. in proceedings of the twelfth international conference on machine learning  icml-1   pages 1  san francisco  ca  1.
 cohen  1  william w. cohen. providing database-like access to the web using queries based on textual similarity. in proceedings of the 1 acm sigmod international
conference on management of data  sigmod-1   pages 1  seattle  wa  june 1.
 domingos  1  pedro domingos. unifying instancebased and rule-based induction. machine learning  1-1  1.
 feldman and hirsh  1  ronen feldman and haym hirsh. mining associations in text in the presence of background knowledge. in proceedings of the second international conference on knowledge discovery and data mining  kdd-1   pages 1  portland  or  august 1.
 feldman  1  ronen feldman  editor. proceedings of the sixteenth international joint conference on artificial intelligence  ijcai-1  workshop on text mining: foundations  techniques and applications  stockholm  sweden  august 1.
 fellbaum  1  christiane d. fellbaum. wordnet: an electronic lexical database. mit press  cambridge  ma  1.
 ghani et al.  1  rayid ghani  rosie jones  dunja mladenic＞  kamal nigam  and sean slattery. data mining on symbolic knowledge extracted from the web. in dunja mladenic＞  editor  proceedings of the sixth internationalconference on knowledge discovery and data mining  kdd-1  workshop on text mining  pages 1  boston  ma  august 1.
 hearst  1  marti hearst. untangling text data mining. in proceedings of the 1th annual meeting of the association for computational linguistics  acl-1   pages 1  college park  md  june 1.
 mccallum  1  andrew kachites mccallum. bow: a toolkit for statistical language modeling  text retrieval  classification and clustering. http://www.cs.cmu.edu/ mccallum/bow 1.
 mladenic＞  1  dunja mladenic＞  editor. proceedings of the sixth international conference on knowledge discovery and data mining  kdd-1  workshop on text mining  boston  ma  august 1.
 nahm and mooney  1a  un yong nahm and raymond j. mooney. a mutually beneficial integration of data mining and information extraction. in proceedings of the seventeenth national conference on artificial intelligence  aaai-1   pages 1  austin  tx  july 1.
 nahm and mooney  1b  un yong nahm and raymond j. mooney. using information extraction to aid the discovery of prediction rules from texts. in proceedings of the sixth international conference on knowledge discovery and data mining  kdd-1  workshop on text mining  pages 1  boston  ma  august 1.
 peterson  1  james l. peterson. computation sequence sets. journal of computer and system sciences  1 :1- 1  august 1.
 quinlan  1  j. ross quinlan. c1: programs for machine learning. morgan kaufmann  san mateo ca  1.
 salton  1  gerard salton. automatic text processing: the transformation  analysis and retrieval of information by computer. addison-wesley  1.
 wagner and fisher  1  robert a. wagner and michael j. fisher. the string to string correction problem. journal of the association for computing machinery  1-1  1.
 yang  1  yiming yang. an evaluation of statistical approaches to text categorization. journal of information retrieval  1/1 :1  may 1.
1 for our purpose is a max-norm for a uniform weightvector  .
1 colors of agents and cubes are not related in this problem.
1 it is true if only depends on this agent's policy  which is not the case in a mas.
1 in a step  all agents make one move simultaneously.
1 at this point  we will follow mitchell's terminology  because he works with two dual borders  a set of maximally general solutions and a set of maximally specific ones  . in data mining  one
	typically only works with the	-set.
1 the positive and negative examples in concept-learning.
1 proofs are provided in the full version of this paper available at: robotics.stanford.edu/ stong/papers/tong koller ijcai1 full.ps
1 in general  information from observational data can easily be incorporated into our model simply by setting to be the empty set for each of the observational data instances. by theorem 1  the update rule for these instances is equivalent to standard bayesian updating of the model.
　　　1 more precisely  throughout our implementation we use map estimates with dirichlet priors.
1 a session is like a trail but relaxes the requirement of coherence in space.
1 however aswewillexplainlater theremaybefewervaluesin caseofmissingvalues.
1 for large datasets  this could be a disadvantage. however  as our algorithm fits within the learning from interpretations approach  similar arguments hold when part of a database is on disk.
1 experiments were carried out on a sun enterprise 1x1mhz ultrasparc1 cpu w/1mb e-cache 1gb ram.
1 the size may be different from the original data set to ensure computationally manageable experiments. in that case  subsets of the original data sets have been selected randomly. for www pages research  the size is small due to the fact that there are very few relevant documents.  
1 described in  and is available at www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-1/www/data/. 
1 well known data sets available at http://www.research.att.com/~lewis/reuters1.html. 1 the ling-spam anti-spam filtering corpus  available at: http://www.iit.demokritos.gr/~ionandr/lingspam public.tar.gz. 1 www pages manually classified according to their relevance to a subject  the tv show the prisoner. 
1 www pages manually classified according to their relevance to a subject  beethoven biographies.  1 usenet articles collected from soc.history and soc.religion.christians newsgroups. only the first message of a thread of discussion has been retrieved. see  scott and matwin  1  for explanations about this choice.  
1 it is worth noting that hits is typically described as running on a small collection of articles  say retrieved in response to a query   while pagerank is described in terms of the entire web. either algorithm can be run in either setting  however  and this distinction plays no role in our analysis. 1 kleinberg  discusses several other heuristics regarding issues such as intra-domain 