
this paper presents a new method for extracting meaningful relations from unstructured natural language sources. the method is based on information made available by shallow semantic parsers. semantic information was used  1  to enhance a dependency tree kernel; and  1  to build semantic dependency structures used for enhanced relation extraction for several semantic classifiers. in our experiments the quality of the extracted relations surpassed the results of kernel-based models employing only semantic class information.
1 introduction
with the advent of the internet  more and more information is available electronically. most of the time  information on the internet is unstructured  generated in textual form. one way of automatically identifying information of interest from the vast internet resources is by recognizing relevant entities and meaningful relations they share. examples of entities are
persons  organizations and locations. examples of relations are role  near and social.
　in the 1s  the message understanding conferences  mucs  and the tipster programs gave great impetus to research in information extraction  ie . the systems that participated in the mucs have been quite successful at recognizing relevant entities  reaching near-human precision with over 1% accuracy. more recently  the automatic content extraction  ace  program focused on identifying not only relevant entities  but also meaningful relations between them. if success in recognizing entities with high precision was attributed to the usage of finite-state transducers  in the last three years the dominant successful technique for extracting relations was based on kernel methods.
　kernel methods are non-parametric density estimation techniques that compute a kernel function to measure the similarity between data instances.  introduced the formalization of relation extraction in terms of tree kernels  which are kernels that take advantage of syntactic trees.  extended the work by using dependency trees that describe the grammatical relations between the words of a sentence. furthermore  each word of a dependency tree is augmented with lexicosemantic information  including semantic information available from the wordnet database . the average precision of relation extraction using dependency trees was reported in  to be 1%. since meaningful relations between relevant entities are of semantic nature  we argue that additional semantic resources should be used for extracting relations from texts. in this work  we were interested in investigating the contribution of two shallow semantic parsing techniques to the quality of relation extraction.
　we explored two main resources: propbank and framenet. proposition bank or propbank is a one million word corpus annotated with predicate-argument structures. the corpus consists of the penn treebank 1 wall street journal texts  www.cis.upenn.edu/゛treebank . the propbank annotations were performed at university of pennsylvania  www.cis.upenn.edu/゛ace . to date propbank has addressed only predicates lexicalized by verbs  proceeding from the most to the least common verbs while annotating verb predicates in the corpus. the framenet project  www.icsi.berkeley.edu/゛framenet  produced a lexico-semantic resource encoding a set of frames  which represent schematic representations of situations characterized by a set of target words  or lexicalized predicates  which can be verbs  nouns or adjectives. in each frame  various participants and conceptual roles are related by case-roles or theta-roles which are called frame elements or fes. fes are local to each frame  some are quite general while others are specific to a small family of lexical items. framenet annotations were performed on a corpus of over three million words. recently  semantic parsers using propbank and framenet have started to become available. in each sentence  verbal or nominal predicates are discovered in relation to their arguments or fes. our investigation shows that predicatearguments structures and semantic frames discovered by shallow semantic parsers play an important role in discovering extraction relations. this is due to the fact that arguments of extracted relations belong to arguments of predicates or to fes.
　to investigate the role of semantic information for relation extraction we have used two shallow semantic parsers  one trained on propbank and one on framenet. we used the semantic information identified by the parsers in two ways. first it was used to enhance the features of dependency kernels. second  it was used to generate a new representation  called semantic dependency structure. the results of the experiments indicate that not only the precision of kernel methods is improved  but also relation extraction based on shallow semantics outperforms kernel-based methods.
　the remainder of this paper is organized as follows. section 1 describes the semantic parsers. section 1 details the kernel methods and their enhancement with semantic information. in section 1 we show how relation extraction based on semantic information is produced. section 1 details the experimental results and section 1 summarizes our conclusions.
1 shallow semantic parsing
shallow semantic information represented by predicates and their arguments  or frames and their fes  can be identified in text sentences by semantic parsers. the idea of automatically identifying and labeling shallow semantic information was pioneered by . semantic parsers operate on the output of a syntactic parser. when using the propbank information  the semantic parser  1  identifies each verbal predicate and  1  labels its arguments. the expected arguments of a predicate are numbered sequentially from arg1 to arg1. additionally  the arguments may include functional tags from treebank  e.g. argm-dir indicates a directional  argm-loc indicates a locative and argm-tmp stands for a temporal. when using the framenet information  the semantic parser  1  identifies the target words;  1  disambiguates the semantic frame for each target word; and  1  labels the fes that relate to the target word based on the frame definition. for example  in
framenet the employment frame has the fes: employee  employer  purpose  compensation  manner  place  position  time  field  duration and task.
s

task 1: the chippewas 	hired eckstein  as a lobbyist task 1:	arg1	predicatearg1argmtask 1: the chippewas 	hired eckstein  as a lobbyist task 1:	employer	targetemployeepositionsemantic
parser
 propbank 
semantic
parser
 framenet 
figure 1: sentence with labeled semantic roles.
　figure 1 illustrates the output of both semantic parsers when processing a sentence. the parsing consists of two tasks:  1  identifying the parse tree constituents corresponding to arguments of each predicate or fes of each frame; and  1  recognizing the role corresponding to each argument or fe. each task is cast as a separate classifier. for example  task 1 identifies the two nps and the pp as arguments and fes respectively. the second classifier assigns the specific roles: arg1  arg1 and argm given the predicate  hired  and fes employer  employee and position given the target word  hired . in the case of semantic parsing parsed on framenet  a third task of finding the frame corresponding to the sentence is cast as a third classification problem.
  phrase type  pt : this feature indicates the syntactic type of the phrase labeled as a frame element  e.g.
np for employer in figure 1.  parse tree path  path : this feature contains the path in the parse tree between the predicate phrase and the target word  expressed as a sequence of nonterminal labels linked by direction symbols  up or down   e.g.
np    s    vp    vp for employer in figure 1.  position  pos    indicates if the constituent appears  before or after the predicate in the sentence.  voice  voice    this feature distinguishes between active or passive voice for the predicate phrase.  head word  hw    this feature contains the head word of the evaluated phrase. case and morphological information are preserved.  governing category  gov    this feature applies to noun phrases only  and it indicates if the np is dominated by a sentence phrase  typical for subject arguments with active voice predicates   or by a verb phrase  typical for object arguments .  target word   in our implementation this feature consists of two components:  1  word: the word itself with the case and morphological information preserved; and  1  lemma which represents the target normalized to lower case and infinitive form for the verbs or singular for nouns. figure 1: feature set 1  fs1 .
  content word  cw    lexicalized feature that selects an informative word from the constituent  different from the head word.part of speech of head word  hpos    the part of speech tag of the head word.part of speech of content word  cpos   the part of speech tag of the content word.named entity class of content word  cne    the class of the named entity that includes the content wordboolean named entity flags   a feature set comprising: 
  neorganization: set to 1 if an organization is recognized in the phrase
  nelocation: set to 1 a location is recognized in the phrase
  neperson: set to 1 if a person name is recognized in the phrase
  nemoney: set to 1 if a currency expression is recognized in the phrase
  nepercent: set to 1 if a percentage expression is recognized in the phrase
  netime: set to 1 if a time of day expression is recognized in the phrase
  nedate: set to 1 if a date temporal expression is recognized in the phrase figure 1: feature set 1  fs1 .
　several classifiers were previously tried for this problem: decision trees   and support vector machines  svms  . based on the results of the conll and senseval-1 evaluations1  we selected an implementation based on svms  using the svmlight package 1. for the semantic parser based on propbank  we combined feature sets: fs1  illustrated in figure 1  introduced in   fs1  illustrated in figure 1  introduced in   fs1  illustrated in figure 1  introduced in . for the semantic parser based on framenet we have also added the feature set fs1  illustrated in figure 1 . the semantic parser using framenet also required the disambiguation of the frame. to disambiguate the frame  we used the bayesnet algorithm implemented in the weka learning package 1. the features that were used are  1  the target word;  1  the part-of-speech of the target word;  1  the phrase type of all the fes  e.g. np  vp  pp ; and  1  the grammatical function. the grammatical function is defined in figure 1. to learn the grammatical function  we again used the svm with the features fs1  fs1  the features human and target-type from fs1 and only the feature pp-prep from fs1.
human: this feature indicates whether the syntactic phrase is either
 1  a personal pronoun or
 1  a hyponym of sense 1 of person in wordnetsupport verbs that are recognized for adjective or noun target words have the role of predicate for the fes. for example  if the target = clever  in the sentence  smith is very clever  but he's no einstein   the fe = smith is an argument of the support verb 'is' rather than of the target word. the values of this feature are either  1  the pos of the head of the vp containing the target word or  1  null if the target word does not belong to a vptarget type: the lexical class of the target word  e.g. verb  noun or adjectivelist constituent  fes : this feature represents a list of the syntactic consituents covering each fe of the frame recognized in a sentence. for the example illustrated in figure 1  the list is:  np  np  pp grammatical function: this feature indicates whether the fe is:
  an external argument  ext 
  an object  obj 
  a complement  comp 
  a modifier  mod 
  head noun modified by attributive adjective  head 
  genitive determiner  gen 
  appositive  appos list grammatical function: this feature represents a list of the grammatical functions of the fes recognized in the sentence.number fes: this feature indicates how many fes were recognized in each sentence.frame name: this feature indicates the name of the semantic frame for which fes are labeledcoverage: this feature indicates whether there is a syntactic structure in the parse tree that perfectly covers the fecore: this feature indicates whether the fe is one that instantiates a conceptually necessary participant of a frame. for example  in the revenge frame  punishment is a core element. the values of this feature are:  1  core;  1  peripheral and  1  extrathemathic. fes that mark notions such as time  place  manner and degree are peripheral. extratematic fes situate an event against a backdrop of another event  by evoking a larger frame for which the target event fills a role.sub corpus: in framenet  sentences are annotated with the name of the subcorpus they belong to. for example  for a verb target word  v swh represents a subcorpus in which the trget word is a predicate to a fe included in a relative clause headed by a wh word.figure 1: feature set 1  fs1 .
1 dependency tree kernels
in  the relation extraction problem was cast as a classification problem based on kernels that operate on dependency trees. kernels measure the similarity between two instances of a relation. if x is the instance space  a kernel function is a mapping k:x〜x★ 1 ±  such that given two instances
parse tree path with unique delimiter    this feature removes the direction in the path  e.g. vbn vp advppartial path   this feature uses only the path from the constituent to the lowest common ancestor of the predicate and the constituentfirst word   first word covered by constituentfirst pos   pos of first word covered by constituentlast word   last word covered by the constituentlast pos   pos of last word covered by the constituentleft constituent   left sibling constituent labelleft head   left sibling head wordleft pos head   left sibling pos of head wordright constituent   right sibling constituent labelright head   right sibling head wordright pos head   right sibling pos of head wordpp prep   if constituent is labeled pp get first word in ppdistance   distance in the tree from constituent to the target wordfigure 1: feature set 1  fs1 .
x and y  k x y  = pi φi x φi y  = φ x  ， φ y   where φi x  is some feature function over the instance x. the instances can be represented in several ways. first  each sentence where a relation of interest occurs can be viewed as a list of words. thus  the similarity between two instances represented in this way is computed as the number of common words between the two instance sentences. all words from instances x and y are indexed and φi x  is the number of times instance x contains the word referenced by i. such a kernel is known as bag-of-words kernel. when sentences are represented as strings of words  string kernels  count the number of common subsequences in the two strings and weight their matches by their length. thus φi x  is the number of times string x contains the subsequence referenced by i. if the instances are represented by syntactic trees  more complex kernels are needed. a class of kernels  called convolution kernels  was proposed to handle such instance representations. convolution kernels measure the similarity between two structured instances by summing the similarity of their substructures. thus  given all possible substructures in instances x and y  φi x  counts not only the number of times the substructure referenced by i is matched into x  but also how many times it is matched into any of its substructures.
　given a training set t={x1 ...xn}  kernel methods compute the gram matrix g such that gij=k xi xj . g enables a classifier to find a hyperplane which separates instances of different classes. g enables classifiers to find a separating hyperplane that separates positive and negative examples. when a new instance y needs to be classified  y is projected into the feature space defined by the kernel function. classification consists of determining on which side of the separating hyperplane y lies. support vector machines svms  formulate the task of finding the separating hyperplane as a solution to a quadratic programming problem.therefore  following the solution proposed by   they are used for classifying relation instances in texts.

figure 1:  a  syntactic parse;  b  head word propagation;  c  dependency tree;  d  semantic dependency tree.to measure the similarity between two instances of the same extraction relation both  and  relied on kernels that operate on trees expressing syntactic information. to build such a tree we have used the collins syntactic parser. when using this parser  for each constituent in the parse tree  we also have access to a dependency model that enables the mapping of parse trees into sets of binary relations between the head-word of each component and its sibling words. for example  figure 1 a  describes the parse tree of a sentence. for each possible constituent in the parse tree  rules first described in  identify the head-child and propagate the head word to the parent. figure 1 b  illustrates the propagation for the parse tree illustrated in figure 1 a . when the propagation is over  head-modifier relations are extracted  generating a dependency structure. figure 1 c  illustrates the dependency structure of the sentence that was analyzed syntactically in figure 1 a . the nodes of this dependency structure were augmented with features  to enable the calculation of the kernel. figure 1 lists the features that were assigned to each node in the dependency tree. two sets of features were used: f1  the features proposed in  and f1  a new set of features that we have added. feature set f1 contains:  1  the word;  1  the part-of-speech  pos   1 values ;  1  a generalized pos  1 values ;  1  the tag of the nonterminal from the parse tree having the feature word as a head;  1  the entity type  as defined by the ace guidelines;  1  the entity level  e.g. name ;  1  the relation argument  e.g. rel-a ; and  1  a wordnet hypernym. the new set of features f1 contains:  1  the predicate argument number provided by the semantic parser when using propbank;  1  the predicate target for that argument;  1  the fe provided by the semantic parser when using framenet;  1  the target word for the frame;  1  the grammatical function;  1  the frame;  1  the wordnet domain;  1  the wordnet concept that expressed the type of relation in which the word may belong within the domain;  1  a set of other related wordnet concepts  e.g. direct hypernym ;  1  the probbank concepts that most frequently occur as arguments for the predicate;  1  the framenet concepts that most frequently occur in fes for the frame. the features and their values for the node  attorneys  are listed in figure 1.
feature set f1	example	feature set f1	example
1 word	attorneys	 1 predicate argument number	arg1
1 part of speech	nn	 1 predicate target	represent
1 general pos	noun	 1 fe	employee
1 syntactic chunk tag	np	 1 target word	represent
1 entity type	person	 1 gramatical function	ext
1 entity level	nominal	 1 frame	employment
1 relation argument	rel a	 1 wordnet domain	jurisprudence
1 wordnet hypernym	individual	 1 wordnet relation concept	lawyer client
1 wordnet semantic concepts	professional paralegal
1 propbank wn  concepts	banker  lawyer  share
1 framenet wn  concepts	relation  men
figure 1: sets of features assigned to each node in the dependency tree.
　the features are used by a tree kernel function k t1 t1  that returns a similarity score in the range  1 . we preferred the more general version of the kernel introduced in  to the kernel described by . this kernel is based on two functions defined on the features of tree nodes: a matching function m ti tj  （ {1} and a similarity function s ti tj  （  1 ± . the feature vector of a tree node φ ti  = {v1 ...vd} consists of two possibly overlapping subsets φm ti    φ ti  and φs ti    φ ti . as in   φm ti  are used by the matching function and φs ti  are used by the similarity function. the two functions are defined by:
1	if φm ti  = φm tj  otherwise
and
         s ti tj  = xs i r（xs j c vq vr  vq（φ  t   v φ  t   where c vq vr  is some compatibility function between the two feature values. for example  in the simplest case where
1	if vq = vr otherwise
s ti tj  returns the number of feature values in common between the feature vectors φs ti  and φs tj . for two dependency trees t1 and t1 with root nodes r1 and r1 the tree kernel is defined as:
  1  =	s r1 r1  + kc r1 c  r1 c  	otherwiseif m r1 r1 =1
where kc is a kernel function over the children of the nodes r1 and r1. let a and b be sequences of indices such that a is a sequence a ＋ a ＋ ... ＋ an and likewise for b. let
be the length of a. then for
every ti （ t1 and tj （ t1  we have
kc ti c  tj c   = x λd a λd b k ti a  tj b  
a b l a =l b 
where λ（ 1  represents a decay factor that penalizes matching subsequences that are spread out within the child sequences. the definition of kc  the kernel function over children  assumes that the matching function used in the definition of the tree kernel k ti a  tj b   operates not only on single nodes  but also on node sequences ti a  or tj b . if all the nodes in the sequence are matched  m ti a  tj b   = 1. for each matching pair of nodes  ai bj  is a matching subsequence  we accumulate the result of the similarity function s ai bj  and then recursively search for matching subsequences of their children.
　as in   we implemented two types of tree kernels: a contiguous kernel and a sparse kernel. a contiguous kernel only matches children subsequences that are uninterrupted by nonmatching nodes. therefore d a  = l a . a sparse tree kernel  by contrast  allows non-matching nodes within matching subsequences.
1 relation extraction
when analyzing the dependency kernels  we noticed that only few nodes bear semantic information derived by the semantic parsers. we also noticed that these nodes are clustered together in the dependency tree. for example  figure 1 d  illustrates the cluster of nodes from the dependency tree that contains semantic information. instead of using the entire dependency tree to compute similarities  we selected sub-trees that contain nodes having values for the features from set f1  illustrated in figure 1 . typically these nodes correspond to target predicates and their arguments or fes. this allowed us to compare trees of the form sdt r1    attorneys ★ represented ○ woodward   and sdt r1   intern ★ dismissed ○ lawyer  . we called such trees semantic dependency trees since they are characterized by semantic features present in the nodes of dependency trees. semantic dependency trees  sdts  are binary trees containing three nodes: a verbal predicate that is the root of the tree; and two children nodes  each an argument of the predicate. to measure the similarity of two sdts simple kernel:
k	1	if m c t1  c t1  =1
 s r t1  r t1   + sp t1 t1  otherwise where the matching function is performed only on the children. the matching features that were used comprised φ1m={general-pos  entity-type  relationargument}  φ1m={fe} and φ1m={predicate-argument number}. m c t1  c t1  =1 if there is a combination of the pair of arguments that has the same matching features. for example  in the case of sdt r1  and sdt r1   the combination is {  attorneys    lawyer   and   intern    woodward  } when using φ1m“φ1m  since both attorney and lawyer are nouns  persons  rel-a and they are both covered by the fe employee in their respective frames. to measure the similarity between the verbal predicates  the function s r t1  r t1   measures the semantic compatibility of the predicates. the features used for measuring similarity of predicates are φps ={frame  predicate target  wordnet domain}. the compatibility measures assigned to each feature are: 1 if the same predicate target  1 if both predicates are targets of the same frame  1 if both predicates are targets of frames from the same event structure  1 if both predicates belong to the same wordnet domain and 1 if the predicates are not covered in framenet  but have the same arguments in propbank as other predicates from the same frame. the predicate similarity brings forward semantic frames that characterize a type of extraction relation. for example  in the case of role client relation  such frames were  a  employment end and its subframes from the event structure; employment start; and  b  commerce sell or  c  commercebuy.
　the sp t1 t1  similarity focuses on the combination of fes or predicate-arguments that are identified for the children of the sdts. the similarity features that are used are {fe  predicate-argument number  grammatical function  wordnet domain  wordnet relation concept}. for example  when the similarity sp sdt r1  sdt r1   is computed  since we have an {employer employee} relation between the fes in both sdts  the confidence assigned based on identical fe-fe relation is 1. if we have identical predicateargument numbers  the confidence is 1. for identical wordnet domains  1 and for the same wordnet relation concept  we assign the confidence 1.
　one limitation of the sdts stems from the fact that this formalism cannot capture extraction relations that are expressed in the same noun phrase  e.g.  our customers    his urologist  or  george's high school . to recognize such relations we consider that they relate to some arguments of  unspecified  predicates. in the case when nps contain pronouns  they are resolved by a successful coreference resolution algorithm   and the pronoun is substituted by a pair  referent  category . for example  the pronoun  our  from  our customers  is replaced by   ibm   organization 
　by measuring the semantic similarity between the pair  nphead  category  and any pair of arguments of a predicate from the training corpus  a plausible predicate can be found. for the noun phrase  our customers   the sdt  shop ○ billed ★ customers   was deemed the most relevant  assigning the predicate  billed  to the two arguments: organization and  customers . the semantic similarity was measured by enhancing the wordnet  similarity measure is publicly available resource  www.d.umn.edu/゛tpederse/pubs/aaai1pedersent.pdf   to handle semantic classes identified for entities  e.g. organization  disease.
1 experimental results
to evaluate the role of shallow semantics provided by semantic parsers on relation extraction we have used the automatic content extraction  ace  corpus available from ldc  ldc1 . the data consists of 1 annotated text documents gathered from various newspapers and broadcasts. five entity types have been annotated  person  organi-
zation  geo-political entity  location  facility  along with the 1 types of relations. the relations are listed in figure 1.
	at located	at residence	role affiliate	role founder
	role staff	near relative loc	role client	soc associate
	role member	role citizen	role owner	soc spouse
	role mgmt	soc professional	role other	soc sibling
	part part of	part subsidiary	soc relative	part other
at based in soc parent soc personal soc grandparent figure 1: extraction relations evaluated in ace.
　we implemented the same five kernels as : k1=sparse kernel  k1=contiguous kernel  k1=bag-of-words kernel and k1=k1+k1 and k1=k1+k1 and used first only the feature set f1 from figure 1 and then both feature sets f1 and f1. the comparison of the kernel performance of the two experiments is listed in figure 1.
	avg. precision	avg. recall	avg. f score
	kernel	features	features	features
	f1	f1+f1	f1	f1+f1	f1	f1+f1
	k1.1.1.1.1.1.1
	k1.1.1.1.1.1.1
	k1.1.1.1.1.1.1
	k1.1.1.1.1.1.1
	k1.1.1.1.1.1.1
figure 1: kernel performance comparison.
　we used each kernel within an svm  we augmented the svmlight implementation to include our kernels . we choose to train on all 1 relations  not only on the first 1-high level relation types as was done in . the results indicate that on average  for k1  the best performing kernel  we obtained an increase of 1% in the f-score when features provided by the semantic parsers were added. when relying on sdts  the average precision that was obtained was 1%  the recall was 1%  thus an f-score of 1%  when using the same data as . figure 1 illustrates f-score obtained when using several other machine learning techniques available in the weka package. in figure 1 p indicates results for relations involving a predicate  np indicates results for relations within an np and c indicates the combined results. the results show that frame semantics produce an enhancement of 1% over previous state-of-art results in relation extraction. furthermore  they show that semantic representations such as frames or predicate-argument structures have a wider impact on classification performance than the classification technique.
	netsbayes bayesnaive	adaboost bagging stacking	id1	j1	randomforest	randomtree	svm	on sdtskernels
	p 1 1	1	1	1	1 1 1	1	1	1
np 1 1	1	1	1	1 1 1	1	1	1
c 1 1 1 1 1 1 1 1 1 1 1 figure 1: results of relation extraction.
　in the ace data 1% of the training/testing data could be cast into sdts. the semantic similarity between arguments of a relation within the same np and arguments present in sdts allowed the extraction with an average f1-score of 1%. the quality of the extraction results depend on the quality of the semantic parsers  that obtained f-scores of over 1% in recent senseval evaluations.
1 conclusions
in this paper we have introduced a new dependency structure that relies on semantic information provided by shallow semantic parsers. this structure enabled the extraction of relevant relations with better performance than previous state-ofthe-art kernel methods. furthermore  the semantic features enabled similarly good results to be obtained with a few other learning algorithms. we also used compatibility functions that made use of semantic knowledge. this framework could be extended to allow processing of idiomatic predicates  e.g.  person  lobbying on behalf of   organization   and combined predications.
