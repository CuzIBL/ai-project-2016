
we consider applying hierarchical reinforcement learning techniques to problems in which an agent has several effectors to control simultaneously. we argue that the kind of prior knowledgeone typically has about such problems is best expressed using a multithreaded partial program  and present concurrent alisp  a language for specifying such partial programs. we describe algorithms for learning and acting with concurrent alisp that can be efficient even when there are exponentially many joint
　　　　　　　　　　　　　　　　　　　　　　　　　　　figure 1: a resource-gathering subgame within stratagus. peasants choices at each decision point. finally  we show recan move one step in the n  s  e  w directions  or remain stationary
sults of applying these methods to a complex com-

puter game domain.
1 introduction
hierarchical reinforcement learning  hrl  is an emerging subdiscipline in which reinforcement learning methods are augmented with prior knowledge about the high-level structure of behaviour. various formalisms for expressing this prior knowledge exist  including hams  parr and russell  1   maxq  dietterich  1   options  precup and sutton  1   and alisp  andre and russell  1 . the idea is that the prior knowledge should enormously accelerate the search for good policies. by representing the hierarchical structure of behaviour  these methods may also expose additive structure in the value function  dietterich  1 .
　all of these hrl formalisms can be viewed as expressing constraints on the behavior of a single agent with a single thread of control; for example  alisp extends the lisp language with nondeterministic constructs to create a singlethreaded partial programming language  see section 1 . experience with programming languages in robotics suggests  however  that the behaviour of complex physical agents in real domains is best described in terms of concurrent activities. such agents often have several effectors  so that the total action space available to the agent is the cartesian product of the action spaces for each effector.
　consider  for example  the resource domain shown in figure 1. this is a subproblem of the much larger stratagus domain  see stratagus.sourceforge.net . each peasant in the resource domain can be viewed as an effector  part of a multibody agent. controlling all the peasants with a single control
 all transitions are deterministic . they can pick up wood or gold if they are at a forest or goldmine and drop these resources at the base. a reward of 1 is received whenever a unit of a resource is brought back to the base. a cost of 1 is paid per timestep  and an additional cost of 1 is paid when peasants collide. the game ends when the player's reserves of gold and wood reach a predefined threshold.
thread is difficult. first  different peasants may be involved in different activities and may be at different stages within those activities; thus  the thread has to do complex bookkeeping that essentially implements multiple control stacks. second  with n peasants  there are 1n possible joint actions  so action selection is combinatorially nontrivial even when exact value functions are given. finally  there are interactions among all the peasants' actions-they may collide with each other and may compete for access to the mines and forests.
　to handle all of these issues  we introduce concurrent alisp  a language for hrl in multieffector problems. the language extends alisp to allow multithreaded partial programs  where each thread corresponds to a  task . threads can be created and destroyed over time as new tasks are initiated and terminated. at each point in time  each effector is controlled by some thread  but this mapping can change over time. prior knowledge about coordination between threads can be included in the partial program  but even if it is not  the threads will coordinate their choices at runtime to maximize the global utility function.
　we begin by briefly describing alisp  section 1  and its theoretical foundations in semi-markov decision processes  smdps . section 1 defines the syntax of concurrent alisp and its semantics-i.e.  how the combination of a concurrent alisp program and a physical environment defines an smdp. we obtain a result analogousto that for alisp: the optimal solution to the smdp is the optimal policy for the original environment that is consistent with the partial program. section 1 describes the methods used to handle the very large smdps that we face: we use linear function approximators combined with relational feature templates  guestrin et al.  1   and for action selection  we use coordination graphs  guestrin et al.  1 . we also define a suitable smdp q-learning method. in section 1  we show experimentally that a suitable concurrent alisp program allows the agent to control a large number of peasants in the resourcegathering domain; we also describe a much larger stratagus subgame in which very complex concurrent  hierarchical activities are learned effectively. finally  in section 1  we show how the additive reward decomposition results of russell and zimdars  may be used to recover the three-part qdecomposition results for single-threaded alisp within the more complex concurrent setting.
1 background
alisp  andre and russell  1  is a language for writing partial programs. it is a superset of common lisp  and alisp programs may therefore use all the standard lisp constructs including loops  arrays  hash tables  etc. alisp also adds the following new operations :
   choose choices  picks one of the forms from the list choices and executes it. if choices is empty  it does nothing.
   action a  performs action a in the environment.
   call f args  calls subroutine f with argument list args.
   get-state  returns the current environment state.
　figure 1 is a simple alisp program for the resource domain of figure 1 in the single-peasant case. the agent executes this program as it acts in the environment. let ω =  s θ  be the joint state  which consists of the environment state s and the machine state θ  which in turn consists of the program counter  call stack  and global memory of the partial program. when the partial program reaches a choice state  i.e.  a joint state in which the program counter is at a choose statement  the agent must pick one of the choices to execute  and the learning task is to find the optimal choice at each choice point as a function of ω. more formally  an alisp partial program coupled with an environment results in a semi-markov decision process  a markov decision process where actions take a random amount of time  over the joint choice states  and finding the optimal policy in this smdp is equivalent to finding the optimal completion of the partial program in the original mdp  andre  1 .
1 concurrent alisp
now consider the resource domain with more than one peasant and suppose the prior knowledge we want to incorporate is that each individual peasant behaves as in the singlepeasant case  i.e.  it picks a resource and a location  navigates to that location  gets the resource  returns to the base and drops it off  and repeats the process.
 defun single-peasant-top   
 loop do
 choose
'  call get-gold   call get-wood     
 defun get-wood   
 call nav  choose *forests*  
 action 'get-wood 
 call nav *home-base-loc* 
 action 'dropoff  
 defun get-gold   
 call nav  choose *goldmines*  
 action 'get-gold 
 call nav *home-base-loc* 
 action 'dropoff  
 defun nav  l 
 loop until  at-pos l  do
 action  choose ' n s e w rest     figure 1: alisp program for resource domain with a single peasant.
 defun multi-peasant-top   
 loop do
 loop until  my-effectors  do
 choose '    
 setf p  first  my-effectors   
 choose
 spawn p #'get-gold     list p  
 spawn p #'get-wood     list p     figure 1: new top level of a concurrent alisp program for resource domain with any number of peasants. the rest of the program is identical to figure 1.
　to incorporate such prior knowledge  we have developed a concurrent extension of alisp that allows multithreaded partial programs. for the multiple-peasant resource domain  the concurrent alisp partial program is identical to the one in figure 1  except that the top-level function is replaced by the version in figure 1. the new top level waits for an unassigned peasant and then chooses whether it should get gold or wood and spawns off a thread for the chosen task. the peasant will complete its assigned task  then its thread will die and it will be sent back to the top level for reassignment. the overall program is not much longer than the earlier alisp program and doesn't mention coordination between peasants. nevertheless  when executingthis partial program  the peasants will automatically coordinate their decisions and the q-function will refer to joint choices of all the peasants. so  for example  they will learn not to collide with each other while navigating. the joint decisions will be made efficiently despite the exponentiallylarge numberof joint choices. we now describe how all this happens.
1 syntax
concurrentalisp includes all of standardlisp. the choose and call statements have the same syntax as in alisp.
threads and effectors are referred to using unique ids. at any point during execution  there is a set of threads and each thread is assigned some  possibly empty  subset of the effectors. so the action statement now looks like
 action e1 a1 . . . en an  which means that each effector ei must do ai. in the special case of a thread with exactly one effector assigned to it  the ei can be omitted. thus  a legal program for single-threaded alisp is also legal for concurrent alisp.
the following operations deal with threads and effectors.
   spawn thread-id fn args effector-list  creates a new thread with the given id which starts by calling fn with arguments args  with the given effectors.
   reassign effector-list thread-id  reassigns the given effectors  which must be currently assigned to the calling thread  to thread-id.
   my-effectors  returns the set of effectors assigned to the calling thread.
　interthread communication is done through condition variables  using the following operations :
   wait cv-name  waits on the given condition variable  which is created if it doesn't exist .
   notify cv-name  wakes up all threads waiting on this condition variable.
　every concurrent alisp program should contain a designated top level function where execution will begin. in some domains  the set of effectors changes over time. for example  in stratagus  existing units may be destroyed and new ones trained. a program for such a domain should include a function assign-effectors which will be called at the beginning of each timestep to assign new effectors to threads.
1 semantics
we now describe the state space  initial state  and transition function that obtain when running a concurrent alisp program in an environment. these will be used to construct an smdp  analogous to the alisp case; actions in the smdp will correspond to joint choices in the partial program.
　the joint state space is   = {ω =  s θ } where s is an environmentstate and θ is a machine state. the machine state consists of a global memory state μ  a list Ψ of threads  and for each ψ （ Ψ  a unique identifier ι  the set e of effectors assigned to it  its program counter ρ  and its call stack σ. we say a thread is holding if it is at a choose  action  or wait statement. a thread is called a choice thread if it is at a choose statement. note that statements can be nested  so for example in the last line of figure 1  a thread could either be about to execute the choose  in which case it is a choice thread  or it could have finished making the choice and be about to execute the action  in which case it is not.
　in the initial machine state  global memory is empty and there is a single thread starting at the top level function with all the effectors assigned to it.
　there are three cases for the transition distribution. in the first case  known as an internal state  at least one thread is not holding. we need to pick a nonholding thread to execute next  and assume there is an external scheduler that makes this choice. for example  our current implementation is built on allegro lisp and uses its scheduler. however  our semantics will be seen below to be independent of the choice of scheduler  so long as the scheduleris fair  i.e.  any nonholding thread will eventually be chosen . let ψ be the thread chosen by the scheduler. if the next statement in ψ does not use any concurrent alisp operations  its effect is the same as in standard lisp. the call statement does a function call and updates the stack; the spawn  reassign  my-effectors  wait  and notify operations work as described in section 1. after executing this statement  we increment ψ's program counter and  if we have reached the end of a function call  pop the call stack repeatedly until this is not the case. if the stack becomes empty  the initial function of the thread has terminated  ψ is removed from Ψ  and its effectors are reassigned to the thread that spawned it.
　for example  suppose the partial program in figure 1 is being run with three peasants  with corresponding threads ψ1  ψ1  and ψ1  and let ψ1 be the initial thread. consider a situation where ψ1 is at the dummy choose statement  ψ1 is at the third line of get-wood  ψ1 is at the call in the first line of get-gold  and ψ1 is at the last line of get-gold. this is an internal state  and the set of nonholding threads is
{ψ1 ψ1}. suppose the scheduler picks ψ1. the call statement will then be executed  the stack appropriately updated  and ψ1 will now be at the top of the nav subroutine.
　the second case  known as a choice state  is when all threads are holding  and there exists a thread which has effectors assigned to it and is not at an action statement. the agent must then pick a joint choice for all the choice threads1given the environment and machine state. the program counters of the choice threads are then updated based on this joint choice. the choices made by the agent in this case can be viewed as a completion of the partial program. formally  a completion is a mapping from choice states to joint choices. in the example  suppose ψ1 is at the dummy choose statement as before  ψ1 and ψ1 are at the choose in nav  and ψ1 is at the dropoff action in get-wood. this is a choice state with choice threads {ψ1 ψ1 ψ1}. suppose the agent has learnt a completion of the partial program which makes it choose {   n rest} here. the program counter of ψ1 will then move to the top of the loop  and the program counters of ψ1 and ψ1 will move to the action statement in nav.
　the third case  known as an action state  is when all threads are holding  and every thread that has effectors assigned to it is at an action statement. thus  a full joint action is determined. this action is performed in the environment and the action threads are stepped forward. if any effectors have been added in the new environment state  the assign-new-effectors function is called to decide which threads to assign them to. continuing where we left off in the example  suppose ψ1 executes the my-effectorsstatement and gets back to the choose statement. we are now at an action state and the joint action {n dropoff rest} will be done in the environment. ψ1 and ψ1 will return to the until in the loop and ψ1 will die  releasing its effector to top.
　let Γ be a partial program and s a scheduler  and consider the following random process. given a choice state ω and joint choice u  repeatedly step the partial program forward as described above until reaching another choice state ω1  and let n be the number of joint actions that happen while doing this. ω1 and n are random variables because the environment is stochastic. let pΓ s ω1 n|ω u  be their joint distribution given ω and u.
　we say Γ is good if pΓ s is the same for any fair s.1 in this case  we can define a corresponding smdp over  c  the set of choice states. the set of  actions  possible in the smdp at ω is the set of joint choices at ω. the transition distribution is pΓ s for any fair s. the reward function r ω u  is the expected discounted reward received until the next choice state. the next theorem shows that acting in this smdp is equivalent to following the partial program in the original mdp.
theorem1 given a good partial program Γ  there is a bijective correspondence between completions of Γ and policies for the smdp which preserves the value function. in particular  the optimal completion of Γ corresponds to the optimal policy for the smdp.
　here are some of the design decisions implicit in our semantics. first  threads correspond to tasks and may have multiple effectors assigned to them. this helps in incorporating prior knowledge about tasks that require multiple effectors to work together. it also allows for  coordination threads  that don't directly control any effectors. second  threads wait for each other at action statements  and all effectors act simultaneously. this prevents the joint behaviour from depending on the speed of execution of the different threads. third  choices are made jointly  rather than sequentially with each thread's choice depending on the threads that chose before it. this is based on the intuition that a q-function for such a joint choice q u1 ... un  will be easier to represent and learn than a set of q-functions q1 u1  q1 u1|u1  ... qn un|u1 ...un 1 . however  making joint choices presents computational difficulties  and we will address these in the following section.
1 implementation
in this section  we describe our function approximation architecture and algorithms for making joint choices and learning the q-function. the use of linear function approximation turns out to be crucial to the efficiency of the algorithms.
1 linear function approximation
we represent the smdp policy for a partial program implicitly  using a q-function  where q ω u  represents the total expected discounted reward of taking joint choice u in ω and acting optimally thereafter. we use the linear approximation
  where each fk is a feature
that maps  ω u  pairs to real numbers. in the resource domain  we might have a feature fgold ω u  that returns the amount of gold reserves in state ω. we might also have a set of features fcoll i j ω u  which returns 1 if the navigation choices in u will result in a collision between peasants i and j and 1 otherwise.
　now  with n peasants  there will be o n1  collision features  each with a correspondingweight to learn. intuitively a collision between peasants i and j shouldhave the same effect for any i and j. guestrin et al.  proposed using a relational value-functionapproximation  in which the weights for all these features are all tied together to have a single value wcoll. this is mathematically equivalent to having a single  feature template  which is the sum of the individual collision features  but keeping the features separate exposes more structure in the q-function  which will be critical to efficient execution as shown in the next section.
1 choice selection
suppose we have a partial program and set of features  and have somehow found the optimal set of weights w~. we can now run the partial program  and whenever we reach a choice state ω  we need to pick the u maximizing q ω u . in the multieffector case  this maximization is not straightforward. for example  in the resource domain  if all the peasants are at navigation choices  there will be 1n joint choices.
　an advantage of using a linear approximation to the qfunction is that this maximization can often be done efficiently. when we reach a choice state ω  we form the coordination graph  guestrin et al.  1 . this is a graph containing a node for each choosing thread in ω. for every feature f  there is a clique in the graph among the choosing threads that f depends on. the maximizing joint choice can then be found in time exponential in the treewidth of this graph using cost network dynamic programming  dechter  1 .
　in a naive implementation  the treewidth of the coordination graph might be too large. for example  in the resource domain  there is a collision feature for each pair of peasants  so the treewidth can equal n. however  in a typical situation  most pairs of peasants will be too far apart to have any chance of colliding. to make use of this kind of  contextspecificity   we implement a feature template as a function that takes in a joint state ω and returns only the component features that are active in ω-the inactive features are equal to 1 regardless of the value of u. for example  the collision feature template fcoll ω  would return one collision feature for each pair of peasants who are sufficiently close to each other to have some chance of colliding on the next step. this significantly reduces the treewidth.
1 learning
thanks to theorem 1  learning the optimal completion of a partial program is equivalent to learning the q-function in an smdp. we use a q-learning algorithm  in which we run the partial program in the environment  and when we reach a
choice state  we pick a joint choice according to a glie exploration policy. we keep track of the accumulated reward and number of environment steps that take place between each pair of choice states. this results in a stream of samples of the form  ω u r ω1 n . we maintain a running estimate of w~  and upon receiving a sample  we perform the update


figure 1: learning curves for the 1 resource domain with 1 peasants  in which the goal was to collect 1 resources. curves averaged over 1 learning runs. policies were evaluated by running until 1 steps or termination. no shaping was used.
1 shaping
potential-based shaping  ng et al.  1  is a way of modifying the reward function r of an mdp to speed up learning without affecting the final learned policy. given a potential function Φ on the state space  we use the new reward function r  s a s1  = r s a s1  + γΦ s1    Φ s . as pointed out in  andre and russell  1   shaping extends naturally to hierarchical rl  and the potential function can now depend on the machine state of the partial program as well. in the resource domain  for example  we could let Φ ω  be the sum of the distances from each peasant to his current navigation goal together with a term depending on how much gold and wood has already been gathered.
1 experiments
we now describe learning experiments  first with the resource domain  then with a more complex strategic domain. full details of these domains and the partial programs and function approximation architectures used will be presented in a forthcoming technical report.
1 running example
we begin with a 1-peasant  1 resource domain. though this world seems quite small  it has over 1 states and 1 joint actions in each state  so tabular learning is infeasible. we compared flat coordinated q-learning  guestrin et al.  1  and the hierarchical q-learning algorithm of section 1. figure 1 shows the learning curves. within the first 1 steps  hierarchical learning usually reaches a  reasonable  policy  one that moves peasants towards their current goal while avoiding collisions . the remaining gains are due to improved allocation of peasants to mines and forests to minimize congestion. a  human expert  in this domain had an average total reward around  1  so we believe the learnt hierarchical policy is near-optimal  despite the constraints in the partial program. after 1 steps  flat learning usually learns to avoid collisions  but still has not learnt that picking up resources is a good idea. after about 1 steps  it

figure 1: learning curves for the 1 resource domain with 1 peasants  in which the goal was to collect 1 resources. the shaping function from section 1 was used.

figure 1: structure of the partial program for the strategic domain
makes this discovery and reaches a near-optimal policy.
　our next test was on a 1 world with 1 peasants  with learning curves shown in figure 1. because of the large map size  we used the shaping function from section 1 in the hierarchical learning. in flat learning  this shapingfunctioncannot be used because the destination of the peasant is not part of the environment state. to write such a shaping function  the programmer would have to figure out in advance which destination each peasant should be allocated to as a function of the environment state. for similar reasons  it is difficult to find a good set of features for function approximation in the flat case  and we were not able to get flat learning to learn any kind of reasonable policy. hierarchical learning learnt a reasonable policy in the sense defined earlier. the final learnt allocation of peasants to resource areas is not yet optimal as its average reward was about  1 whereas the human expert was able to get a total reward of  1. we are working to implement the techniques of section 1  which we believe will further speed convergencein situations with a large number of effectors.
1 larger stratagus domain
we next considered a larger stratagus domain  in which the agent starts off with a single town hall  and must defeat a single powerful enemy as quickly as possible. an experienced human would first use the townhall to train a few peasants  and as the peasants are built  use them to gather resources. after enough resources are collected  she would then assign a peasant to construct barracks  and reassign the peasant to resource-gathering after the barracks are completed . the barracks can then be used to train footmen. footmen are not very strong  so multiple footmen will be needed to defeat the enemy. furthermore  the dynamics of the game are such that it is usually advantageous to attack with groups of footmen rather than send each footman to attack the enemy as soon as he is trained. the above activities happen in parallel. for example  a group of footmen might be attacking the enemy while a peasant is in the process of gathering resources to train more footmen.
　our partial program is based on the informal description above. figure 1 shows the thread structure of the partial program. the choices to be made are about how to allocate resources  e.g.  whether to train a peasant next or build a barracks  and when to launch the next attack. the initial policy performed quite poorly-it built only one peasant  which means it took a long time to accumulate resources  and sent footmen to attack as soon as they were trained. the final learned policy  on the other hand  built several peasants to ensure a constant stream of resources  sent footmen in groups so they did more damage  and pipelined all the construction  training  and fightingactivities in intricate and balancedways. we have put videos of the policies at various stages of learning on the web.1
1 concurrent learning
our results so far have focused on the use of concurrent partial programs to provide a concise description of multieffector behaviour  and on associated mechanisms for efficient action selection. the learning mechanism  however  is not yet concurrent-learning does not take place at the level of individual threads. furthermore  the q-function representation does not take advantage of the temporal q-decomposition introduced by dietterich  for maxq and extended by andre and russell  for alisp. temporal qdecomposition divides the complete sequence of rewards that defines a q-value into subsequences  each of which is associated with actions taken within a given subroutine. this is very important for efficient learning because  typically  the sum of rewards associated with a particular subroutine usually depends on only a small subset of state variables-those that are specifically relevant to decisions within the subroutine. for example  the sum of rewards  step costs  obtained while a lone peasant navigates to a particular mine depends only on the peasant's location  and not on the amount of gold and wood at the base.
　we can see immediately why this idea cannot be applied directly to concurrent hrl methods: with multiple peasants  there is no natural way to cut up the reward sequence  because rewards are not associated formally with particular peasants and because at any point in time different peasants may be at different stages of different activities. it is as if  every time some gold is delivered to base  all the peasants jump for joy and feel smug-even those who are wandering aimlessly towards a depleted forest square.
　we sketch the solution to this problem in the resource domain example. implementing it in the general case is ongoing work. the idea is to make use of the kind of functional reward decomposition proposed by russell and zimdars   in which the global reward function is written as r s a  = r1 s a +...+rn s a . each sub-reward function rj is associated with a different functional sub-agent- for example  in the resource domain  rj might reflect deliveries of gold and wood by peasant j  steps taken by j  and collisions involving peasant j. since there is one thread per peasant  we can write r s a  = pψ（Ψ rψ s a  where Ψ is a fixed set of threads. this gives a thread-based decomposition of the q-function q ω u  = pψ（Ψ qψ ω u . that is 
qψ refers to the total discounted reward gained in thread ψ.
　russell and zimdars deriveda decomposedsarsa algorithm for additively decomposed q-functions of this form that converges to globally optimal behavior. the same algorithm can be applied to concurrentalisp: each thread receives its own reward stream and learns its component of the q-function  and global decisions are taken so as to maximize the sum of the component q-values. having localized rewards to threads  it becomes possible to restore the temporal q-decomposition used in maxq and alisp. thus  we obtain fully concurrent reinforcement learning within the concurrent hrl framework  with both functional and temporal decomposition of the q-function representation.
1 related work
there are several hrl formalisms as mentioned above  but most are single-threaded. the closest related work is by mahadevan's group  makar et al.  1   who have extended the maxq approach to concurrent activities  for many of the same reasons we have given . their approach uses a fixed set of identical single-agent maxq programs  rather than a flexible  state-dependent concurrent decomposition of the overall task. however  they do not provide a semantics for the interleaved execution of multiple coordinating threads. furthermore  the designer must stipulate a fixed level in the maxq hierarchy  above which coordination occurs and below which agents act independently and greedily. the intent is to avoid frequent and expensive coordinated decisions for low-level actions that seldom conflict  but the result must be that agents have to choose completely nonoverlapping highlevel tasks since they cannot coexist safely at the low level. in our approach  the state-dependent coordination graph  section 1  reduces the cost of coordination and applies it only when needed  regardless of the task level.
　the issue of concurrently executing several high-level actions of varying duration is discussed in  rohanimanesh and mahadevan  1 . they discuss several schemes for when to make decisions. our semantics is similar to their  continue  scheme  in which a thread that has completed a high-level action picks a new one immediately and the other threads continue executing.
1 conclusions and further work
we have described a concurrent partial programming language for hierarchical reinforcement learning and suggested  by example  that it provides a natural way to specify behavior for multieffector systems. because of concurrent alisp's built-in mechanisms for reinforcement learning and optimal  yet distributed  action selection  such specifications need not descend to the level of managing interactions among effectors-these are learned automatically. our experimental results illustrate the potential for scaling to large numbers of concurrent threads. we showed effective learning in a stratagus domain that seems to be beyondthe scope of singlethreaded and nonhierarchical methods; the learned behaviors exhibit an impressive level of coordination and complexity.
　to develop a better understanding of how temporal and functional hierarchies interact to yield concise distributed value function representations  we will need to investigate a much wider variety of domains than that considered here. to achieve faster learning  we will need to explore model-based methods that allow for lookahead to improve decision quality and extract much more juice from each experience.
