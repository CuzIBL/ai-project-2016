
ai has had notable success in building highperformance game-playing programs to compete against the best human players. however  the availability of fast and plentiful machines with large memories and disks creates the possibility of a game. this has been done before for simple or relatively small games. in this paper  we present new ideas and algorithms for solving the game of checkers. checkers is a popular game of skill with a search space of possible positions. this paper reports on our first result. one of the most challenging checkers openings has been solved - the white doctor opening is a draw. solving roughly 1 more openings will result in the game-theoreticvalueof checkersbeing determined.
1 introduction
high-performancegame-playingprogramshave been a major success story for ai. in games such as chess  checkers  othello  and scrabble  mankind has been humbled by the machine. however  although these programs are very strong  they can still lose a game to a human. they are strong  but not perfect. perfection requires one to  solve  a game. allis defines three levels of solving  allis  1 :
1. ultra-weakly solved. the game-theoretic value for the game has been determined. an ultra-weak solution is mainly of theoretical interest. for example  hex is a first player win  but no one knows the winning strategy.
1. weakly solved. the game is ultra-weakly solved and a strategy is known for achieving the game-theoreticvalue from the opening position  assuming reasonable computing resources. several well-known games have been weakly solved  including connect four  allis  1   qubic  allis  1   go moku  allis  1   and nine men's morris  gasser  1 .
1. strongly solved. for all possible positions  a strategy is known for determining the game-theoretic value for both players  assuming reasonable computing resources.

　　the support of nserc  icore and the university of alberta is acknowledged.strongly solved games include 1-piece chess endgames and awari  romein and bal  1 .
note the qualification on resources in the above definitions. in some sense  games such as chess are solved since the minimax algorithm can in principle determine the game-theoretic value  given a large enough amount of time. resource constraints preclude such impractical  solutions .
　how difficult is it to solve a game  there are two dimensions to the difficulty  allis et al.  1 : decision complexity  the difficulty required to make correct decisions  and space complexity  the size of the search space. checkers is considered to have high decision complexity and moderate space complexity. all the games solved thus far have either low decision complexity  qubic; go-moku   low space complexity  nine men's morris  size ; awari  size   or both  connect-four  size  .
　checkers  or draughts  is popular in the british commonwealth  present and past  and in the united states. the rules are simple  pieces move one square at a time diagonally  and can capture by jumping  and the number of piece types is small  kings and checkers   yet the game is quite challenging. checkers has high decision complexity  more complex than go and the play of bridge hands  on par with backgammon  but less complex than chess  and moderate space complexity   positions versus unsolved games such as backgammon  positions  and chess  positions . the best checkers programs are stronger than the best human players  e.g.  chinook won the world man-machine championship  schaeffer  1  .
　the number of possible placings of checkers pieces on the board is roughly  chinook  1 . this number is misleading since it includes positions that are not legally reachable from the start of the game. for example  although there are possible positions with 1 pieces on the board  only a small fraction can be reached in a game  e.g.  1 kings versus 1 kings is not possible .
　our effortto solve the gameof checkersbeganin 1! after almost 1 years of research and development  and countless thousands of computer hours  we are pleased to report the first major milestone in our quest. the white doctor opening  shown in figure 1  has been proven to be a draw.1 this is a

figure 1: the white doctor: white to play and only draw
difficult opening  as black begins the game with a huge positional disadvantage-so large that humans consider black's best play to be to sacrifice a checker early on to relieve the pressure. notably  this opening was played in the decisive game of the 1 tinsley-chinook world championship match  schaeffer  1 .
this paper has the following contributions:
1. a new hybrid combination of best-first and depth-first search results to the constructionof minimax proof trees. heuristic value iteration is introducedas a means of concentrating search effort where it is most needed.
1. a solution to the white doctor opening. achieving this result took years of computing  involving the precomputation of values  endgame databases   and building the proof tree which took a further roughly positions. this is an important first step towards weakly solving the game of checkers.
1. the largest game-tree proof to date. this was made possible not only by our new hybrid search approach  but also by the integration of several state-of-the-art algorithms and enhancements into one coherent system.
1 algorithm overview
a formal proof of a game's value can be done by depth-first alpha-beta     or best-first proof number search  pns   allis  1 . for the large and complex checkers search space  neither turned out to be effective. instead we propose a hybrid approach of the two: heuristically guided proofs.
the proof procedure has four algorithm/data components:
1. endgame databases  backward search . computations from the end of the game backward have resulted in a database of positions   pieces on the board  for which the game-theoretic value has been computed  strongly solved .
1. proof tree manager  search management . this component maintains a tree of the proof in progress  traverses it  and generates positions that need to be explored to further the proof's progress.
1. proof solver  forward search . given a position to search  this componentuses two programs  and pns  to determine the value of the position.

are randomly selected. each starting position is played twice  with sides switched for the second game.

figure 1: the checkers search space  log scale horizontally 
1. seeding  expert input . from the human literature  a single line of  best play  is identified. this is fed to the proof tree manager as the initial line to explore.
　figure 1 illustrates this approach. it plots the number of pieces on the board  vertically  versus the logarithm of the number of positions  using data from  chinook  1 ; the the widest point is with 1 pieces  positions . the endgame database phase of the proof is the shaded area  all positions with 1 or fewer pieces.
　seeding the proof process with an initial line of play is critical to the prover's performance; it is shown in figure 1 as a sequence of solid bold lines. the line leads from the start of the opening into the endgame databases. seeding is not necessary to the proof-the prover is capable of doing the proof with no expert guidance. however  this single line of play allows the proof process to immediately start doing work that is likely to be relevant to the proof. without it  we have seen the prover spend considerable effort flailing about while it tries to find the key lines of play.
　the inner oval area in figure 1 illustrates that only a portion of the search space is relevant to the proof. positions may be irrelevant because they are unreachable in this opening  or are not required for the proof. the small circles illustrate positions with more than 1 pieces for which a value has been proven. each of the circles represents the root of a search tree which has been solved. for clarity  the solved subtree below that root is not shown in the diagram.
1 endgame databases
endgame databases were pioneered in chess  thompson  1   and were instrumental in the success of the chinook program  schaeffer  1 . using retrograde analysis  sub-
sets of a game with a small number of pieces on the board can be exhaustively enumerated to compute which positions are wins  losses or draws. whenever a search reaches a database position  instead of using an inexact heuristic evaluationfunction  the exact value of the position can be retrieved.
　we have computed all endgame databases up to 1 pieces on the board  for any combination of kings and checkers  van den herik et al.  1 . the total database size is 1 trillion positions  roughly . although this sounds impressive  it represents a paltry one ten-millionth of the total search space  chinook  1 !
　how valuable are the databases  unfortunately  because of the win/loss/draw construction  a good measure of the difficulty of 1-piece endgames is not available-although it is presumed to be very high. anecdotal evidence includes:
1. perfect play databases for the 1-piece endgames have been computed  with the longest winning line being 1 ply  trice and dodgen  1 .
1.  schaefferet al.  1  report that some of their database computations had winning lines in excess of 1 ply- just to force a checker advance or a piece capture.
1. in the 1-king versus 1-king and 1-checker endgame  the longest winning line is 1 ply  gilbert  1 .
1. draws are much harder to prove than wins/losses.
this evidence strongly suggests that the databases are removing hundreds of ply from the search depth of the proof tree.
1 search algorithm
the proof search is split into a front-end proof tree manager and a back-end prover. the front-end manager maintains a large persistent proof tree  and selects  stores  and organizes the search results. proofs of individual positions are done in parallel by a farm of back-end provers.
1 proof tree manager  front end 
the proof tree manager starts off with an empty proof tree. the proof is  seeded  by being fed an initial line of play. this line is considered by humans to be best play by both sides. the proof manager starts at the last position in the line and solves it  then backs up one ply  solves that sub-tree  and so on until the root of the line is reached.
　the proof is done iteratively  as shown in figure 1. in minimax search  we have seen iterative algorithms based on search depth  iterative deepening  and confidence in the root value  as in conspiracy numbers  mcallester  1  . our algorithm is novel in that it iterates on the range of relevant heuristic values for the proof.
　classical iterative deepening in game-playing programs is used for several reasons  one of which is to improve search efficiency by using the tree from iteration as the basis for starting iteration +1. the assumption is that increasing the depth does not cause substantial changes to the tree structure; each iteration refines the previous iteration's solution. in contrast  without iterative deepening  the search can go off in the wrong direction and waste lots of effort before stumbling on the best line of play.
　our iterative value solution works for similar reasons. a position may require considerable effort to formally determine its proven value. however  the higher/lower the score of the position  the more likely it is a win/loss. rather than invest the effort to prove such a position  this work is postponed until it has been shown to be needed. we do this by introducing two new values to the proof tree. to win  loss  draw and unknown  we add likely win and likely loss.
　all searches are done relative to a heuristic threshold. the back-end prover returns an assessment of a position. any position exceeding the threshold is considered a likely win; any falling below the negativethreshold is a likely loss. the proof manager repeatedly traverses the proof tree  identifies nodes to be searched  sends them off for assessment by the prover  and integrates the results. any node with a score outside the thresholdhas a likely result and is not expandedfurtherwithin this threshold; it is effectively treated as if it is proven. the thresholds can be loosely thought of as imposing a symmetric search window on the best-first proof manager. a value outside the window in means the result is irrelevant; a value outside the heuristic threshold means that we postpone the proof until we know the result will be relevant.
　once the prooftree is complete for a given heuristic threshold  the threshold is increased and the search restarted to construct a proof for the new threshold. any position that was previously resolved to a real proven value remains proven  and positions that have a heuristic score that remain outside the new heuristic threshold remain likely wins/losses. only non-proven positions with heuristic values inside the new threshold are now considered  and they must be re-searched with the new heuristic limit. this iterative process continues until the heuristic threshold exceeds the largest possible heuristic value. at this point all likely wins/losses have been resolved  and the proof tree is complete.
　ideally  after the first likely proof tree is constructed  the work would consist solely of trying to prove that the likely wins/losses are even more likely wins/losses. in practice  the heuristics we use are occasionally wrong  they are  after all  only heuristics   and a proof tree for a position may grow large enough that other previously less desirable positions become a more attractive alternative for expansion.
　for the white doctor  we used settings of min = 1  a checker is worth 1 points  and inc = 1 in figure 1. these might not be the best choices; we are experimenting with different values on other checkers openings.
　the proof manager uses pns  trying to answer two questions about the value of the proof tree:  is it at least a likely win   and  is it at most a likely loss.  a likely win is assigned a proof number of 1 for the is-likely-win question  a likely loss is assigned a proof number of 1 for the is-likelyloss question  and a draw is assigned a disproof number of 1 for both questions. the prover tries to investigate the two questions simultaneously by selecting nodes for consideration that  ideally  contribute to answering both.
　the proof tree manager traverses the proof tree and identifies nodes that are needed or likely to be needed in the proof. while a traditional pns algorithm expands one node at a time  our manager expands many promisingnodes  possibly hundreds of them   to have enough work to keep multiple back-end prover processors busy.
　when a node is selected for expansion  it is added to a work-to-do file that is used as input to the prover pro-
proofmanager  seeding   {
// seeding: seeding line for the proof
// seeding  1   is the opening position
// min: starting proof heuristic threshold
// win: ending proof threshold
// inc: heuristic increment between likely proofs
// iterate on heuristic threshold for   thresh = min; thresh  = win; thresh += inc   // iterate over the seeded line for   move = length  seeding  ; move   1; move -= 1   {
pos = seeding  move  ;
// solve current position to threshold repeat {
worklist = pnsearch  pos  thresh  ; if  notempty  worklist     {
startprovers  worklist  results  thresh  ;
waitforresults  worklist  results  ;
updateprooftree  pos  results  thresh  ;
}
} until   isempty  worklist    ;
}
}
}
figure 1: proof manager
cesses. the back-end is given information for the generation of heuristic proof numbers and any proven information that is known about the position  e.g.  it might already be known that it is not a loss .
　the proof manager saves all back-end search results so that they can be re-used for searches with a different heuristic threshold  or from a different opening . a persistent b-tree is used to store information on every position examined in every opening  including both the back-end results and the backed-up values computed by the front end.
　in the cases where the back-end proof search does not provide enough information to answer the front end's question of interest  the heuristic score from the back end is used to generate heuristic proofand disproofnumbers. chinook is used to provide the heuristic scores. if the search result is small enough to initiate the proof search  then the result is used to help initialize the proof numbers. the proof number for a win is a function of: 1  the difference between the heuristic score and the heuristic threshold  a high score is more likely to be a win than an even score   and 1  a penalty is added for positions where both sides have a king  as these tend to be much harder to prove.
　the disproof number for a loss is the same as the proof number for a win  with one exception. we use one additional statistic from chinook. if the principal variation of the search result depends on a draw from a database lookup  we decrease the disproof number. proof numbers for a loss and disproof numbers for a win are defined similarly. as a final modification  the numbers that depend on the minimum of their child values  as opposed to the sum  are divided by the average branching factor.
1 proof solver  back end 
pseudo-code for the back-end prover is shown in figure 1. it is a novel combinationof a traditional heuristic search and a proof search. the  cheap  heuristic search value is used to determine whether the current position is relevant to the cur-
prover   { repeat {
getwork  pos  thresh  ;
// heuristicsearch returns alpha-beta heuristic value heur = heuristicsearch  pos  thresh  ;
// if value inside threshold range  do pnsearch lower = loss; upper = win; if  -thresh*1   heur && heur   1*thresh   {
// this is a simplified version of the actual code
}// check for win  at most draw  or unknown result = dfpnsearch  pos  is win  ; if  result ==	proven   { lower = win ; } else if  result == disproven   { upper = draw; }
if  lower != upper && time remaining   { // check for loss  at least draw  or unknown result = dfpnsearch  pos  is loss  ; if  result ==	proven   { upper = loss; } else if  result == disproven   { lower = draw; }
}saveresult  pos  thresh  heur  lower  upper  ;}
figure 1: proof solver process
rent proof threshold. the  expensive  proof search attempts to formally prove the result of the current position. the latter is only done if the former indicates it is needed.
　each back-end position is first searched by an searcher using a heuristic evaluation function and the endgame databases. the search is limited to 1 seconds  reaching a depth of 1 to 1 ply plus search extensions. the heuristic value returned is compared to the search threshold. values outside the threshold range are considered as likely wins/losses by the proof tree manager. if the heuristic value is more than twice the size of the threshold  then no further processing is done; the formal proof search is postponed until later in the proof when we will have more confidence that we really need this result.
　the prover uses nagai's df-pn algorithm  nagai  1   a depth-first search variant of best-first pns. df-pn was preferred over pns  in part  because of memory concerns. the algorithm's space requirements are limited by the size of the transposition table. in contrast  pns needs to store the entire tree in memory. this gives df-pn an important advantage  since memory is also needed to reduce the frequencyof costly disk i/o caused by endgame database accesses.
　the time limit for the df-pn component is 1 seconds per search. most of the search time is spent trying to answer the  easy  question  the opposite result to that of the heuristic search. for example  if the heuristic result indicates that the side to move has a large advantage  a possible win   the prover first tries to prove or disprove the position as a loss. this was done because this was usually the easier question to answer and a disproof of a loss or win may be a sufficient answer for the proof manager given that we expect the value of the proof to be a draw. if a disproof for the question is found in less than the maximum time allotted  another proof search is started to answer the other proof question.
　proving the value of a node implies finding a path to the endgame databases  or a draw by repetition . clearly  the fewer pieces on the board  the  closer  one is to a database position and the easier it should be to  dis prove. in addition  it should be much easier to prove a win or disprove a loss if one side has more pieces than the other side. we combine these two factors to initialize the proof and disproof numbers  giving more attention to fewer pieces and favorable positions. furthermore  because the standard df-pn algorithm has a fundamental problem of computing  dis proof numbers when position repetitions are present  the techniques in  kishimoto and mu：ller  1  are incorporated into the solver.
1 graph history interaction
a forward-search-based proof cannot be correct unless the graph history interaction problem  ghi  is properly addressed. ghi can occur when a search algorithm caches and re-uses a search result that depends on the move history. in checkers  repeated positions are scored as a draw. it is possible that the value of a search is influenced by a draw-byrepetition score  and this result is saved in the proof tree or in a transposition table. if this cached result is later reached by a different move sequence  it might not be correct-we do not know if the draw-by-repetition scores are correct.
　in the back-end prover  ghi is correctly handled by utilizing the new technique described in  kishimoto and mu：ller  1 . ghi is avoided in the boundary between the frontend tree and the back-end searches by not communicating any move history information to the back-end. finally  in the front-end manager  ghi is avoided by not saving any value in the proof tree that is unsafely influenced by a draw-byrepetition score somewhere in its subtree. this may cause the manager to spend extra time searching the tree  but no extra prover searches need be performed  as the prover results stored in the tree are path independent.
1 implementation insights
the effort to turn likely wins and likely losses into proven values takes the vast majority of search effort. this holds even for a fairly high initial heuristic threshold  where likely wins/losses are positions that human checkers experts agree are  game over . the major reason for this is postponing moves; a non-winningside always takes the pathof maximum resistance. for example  the losing side in a position is able to make terrible moves  such as sacrificing a checker for no apparent gain  to postpone reaching the endgame databases; the unknown aspect of a position with more than 1 pieces on the board  no matter how hopeless the position  is preferable to a known database loss. these lopsided positions must still be chased down to reach a proven conclusion.
　the proof process is computationally demanding. some of the performance issues include:
1. the proof process spends most of its time accessing the endgame databases. the cost can be reduced by locality of disk accesses and caching. smart organization of the endgame databases and use of a 1gb machine  most of the memory going to i/o caching   means that 1% of all endgame database positions do not require i/o.
1. despite the above performance optimizations  the proof process is still i/o intensive-keepingthe i/o subsystem at maximum capacity. it is rare to see the cpu usage beyond 1%. many machine architectures do not have the hardware reliability for this sustained i/o demand.
1. the massive i/o means that we can only use local i/o; network i/o is too slow. that limits the machines that we can add to the computation to those that have 1 gb of available local disk.
1. i/o errors. our programs are instrumented to detect and recover  if possible  from most types of i/o errors.
1 results
the white doctor opening was chosen as the first candidate to solve because of its importance in tournament practice to the checkers-playing community. the opening is one of the most difficult of the three-move ballots  with black starting off with a large disadvantage. many decades of human analysis determined that black's desperate situation meant that a checker had to be sacrificed to relieve the pressure. in a typical game  black is down material for most of the game  barely hanging on to salvage a draw. is the human analysis correct 
　the white doctor proof began in november 1 and a heuristic proof for a threshold of 1 was completed one month later. for the next eight months  computer cycles were spent increasing the heuristic threshold to a win and verifying that there were no bugs in the proof.1 in august 1  we thought the proof was complete  but an inspection showed that some trivial positions with an advantage of 1 or more checkers had been eliminated from the proof  an accidental  historical error . this was corrected and in january 1  we completed the last of the unresolved positions.
　is the proof correct  the endgame databases are being verified by ed gilbert  the author of the kingsrow checkers program. he has independently verified that our 1-piece databases and 1% of our 1-piece databases are correct  his computation is ongoing . the front-end white doctor proof tree is consistent and correct  assuming that all the back-end proofs returned the right answer. many of the back-end values have been re-verified by a different solver developed as part of this project  increasing our confidence that the proof is indeed correct. the proof is available online at http:
//www.cs.ualberta.ca/ chinook. several strong players have looked at the proof lines and found no errors. some interesting statistics on the proof include:
1. number of positions given to the back-end to solve  excluding work that was used for experiments or shown to have bugs : 1.
1. number of positions in one minimal prooftree: 1. the minimal proof tree is not a minimum proof tree. we did not try to maximize the usage of transpositions or choose the branches leading to the smallest subtree in generating the proof tree.
1. longest line searched in the front-end proof tree: 1 ply.
1. longest line in our minimal proof tree: 1 ply. the leaf node in the tree is the result of a 1 second proof number search  to a variable depth  possibly as much as 1 ply deep   terminating in an endgame database position  possibly worth 1s of plys of search .
1. number of nodes in a back-end search  1 seconds of clock time for each chinook search and 1 seconds for a proof number search : averaging 1 1 positions  slow because of extensive i/o .
1. average number of processors used: seven  with 1 to 1 gb of ram .
1. total number of nodes searched in the proof: roughly positions. the num-
ber of unique positions is considerably smaller.
　so  how does the computer proof stack up against human analysis  we used fortman's classic basic checkers as our guide and compared its analysis to that of the proof  fortman  1 . of the 1 positions given in the book for this opening  there were no disagreements in the game-theoretic result of a position. this is a bit misleading since some of the positions were not part of the proof  and for others the proof only had a bound  less than or equal to a draw  whereas the book had a result  draw . nevertheless  this result provides further evidence that the computer proof is correct. more impressive  however  is the quality of the human analysis. the analysis of this opening evolved over 1 years  had many human analysts contributing  requires analysis of lines that are over 1 ply long  and requires extensive insight into the game to assess positions and identify lines to explore.
　the checker sacrifice in the white doctor is correct according to our proof. this line is sufficient to prove that the opening is a draw. we have not invested computer cycles to find out if alternate non-sacrifice defences also lead to a draw. we have made substantial progress on two more checkers openings. of interest is that one of these openings appears to be roughly 1 times more difficult than the white doctor  whereas the other one seems to be half as difficult.
1 solving checkers
when will checkers be weakly solved  the white doctor is solved  and two other proofs are well under way  both appear to be draws . there are 1 three-move checkers openings and  ideally  we would like to solve all of them. however  to solve checkers for the initial starting position  with no moves made  roughly 1 openings need to be computed   cutoffs eliminate most of the openings . solving subsequent openings will not take as long as the first one did. the computations will speed up for several reasons:
1. algorithm efficiency. we will switch from experimental mode  trying different performance parameters  to production mode  fixed parameters .
1. more endgamedatabases. a new algorithmwill allow us to compute parts of the 1-piece database  getting 1% of the benefits for 1% of the computing effort.
1. data reuse. openings transpose into each other. each additional opening increases the chances for reusing the results of one opening for another.
the only obstacle remaining is access to computing resources. computer cycles are easy to obtain; the bottleneck is large local data storage and i/o speeds. with enough resources  the game could be weakly solved within a year.
