
we introduce our research on learning browsing behavior models for inferring a user's information need  corresponding to a set of words  based on the actions he has taken during his current web session. this information is then used to find relevant pages  from essentially anywhere on the web. the models  learned from over one hundred users during a fiveweek user study  are session-specific but independent of both the user and website. our empirical results suggest that these models can identify and satisfy the current information needs of users  even if they browse previously unseen pages containing unfamiliar words.
keywords: machine learning  web mining
1 introduction
while the world wide web contains a vast quantity of information  it is often difficult for web users to find the relevant information they are seeking. while modern search engines have now made search over billions of web pages an everyday occurrence  this approach still requires the user to explicitly initiate a search and to formulate a specific search query. the alternative approach  which involves surfing the web by following appropriate links  is also challenging as users cannot always determine which links are most likely to lead to the relevant information. our goal is a client-side web recommender system that simply observes the user's browsing behavior  then uses this information to suggest relevant pages from anywhere on the web  without requiring the user to provide any additional input.
　historically  information filtering approaches avoid the need for explicit queries  but they assume the user has stable long-term interests that can be used to pick out relevant material from an ongoing  dynamically changing flow of content  lewis and knowles  1 . in this case  a user's interests can be determined by statistical inference over extensive historical samples of the content that the user preferred. alternatively  information assistants are designed to respond effectively to transient short-term information needs that vary from task to task using content stored in large stable repositories. in contrast  we present a method to determine the user's current information need without either explicit queries nor a large sample of past content.
1 model
for the purposes of our present work  we focus on users using the web to obtain some information. our goal is to automatically provide the user with additional relevant content beyond what would be immediately revealed by either links on their current page  or results from a search query they might initiate. we assume that this activity can be separated into distinct sessions in which the user is pursuing a single specific information need.
　as the user browses the web  he visits pages and takes actions on these pages  such as following a link  backing up to a previous page  or bookmarking a page. we view the user's web browsing actions as implicit judgments on the content of the pages  with respect to his current information need . presently  we assume that the user is looking for text-based resources. in this case  the content of the pages is represented by words. we theorize that the actions the user takes while browsing provide evidence about the degree to which words on visited pages represent the user's current information need.
　this intuition has appeared in previous work in special cases: for instance  the idea that words appearing in the anchors of hyperlinks followed by the user are probably more representative of user's needs than words that do not. indeed  the letizia  lieberman  1  agent and the recommender created by watson  budzik and hammond  1  use observations of user behavior to help them locate pages  but they are both based on a limited set of hand-coded heuristic rules. alternatively  correlation-based recommenders  agrawal and srikant  1  tend to be tied to a specific site. moreover  such systems typically suggest that the current user go to pages that other similar users have visited  without knowing whether previous users actually found the page helpful. our  behavior-based inference  differs from content-based systems  billsus and pazzani  1   as we do not require any previous experience with the actual words in the hyperlink anchor; moreover  our models are explicitly trained to identify pages that satisfy any particular information need.
　for example in our models  following any hyperlink provides evidence for the relevance of those words appearing in the hyperlink anchor  independent of what words are actually present in the hyperlink. consequently the technique is not restricted to a subset of indexed sites. indeed  we attempt to predict the words that best capture the user's information need with a combination of basic text-based features  e.g.  the overall frequency of the word in the user's current session  etc.   as well as a generalized set of behavior-based features  e.g.  the presence of the word in anchor text  the use of the word on a page from which the user backed up  and the order in which a list of links using the word were selected  etc. .
　since the precise significance of a word appearing in a followed hypertext anchor  or being in some font  etc.  is difficult to state a priori  we use machine learning methods to learn the weight of each of our features  towards identifying which word will be relevant. this is based on data collected from a pool of calibration subjects. once the weights have been calculated  we can apply our user-independent models to new users without retraining. in fact  since the models are trained on behaviors  e.g.  the value of appearing in a followed hyperlink  etc.  instead of the information viewed  i.e.   the user is interested in 'baseball'     the models can also be applied to new domains without retraining  and can respond to new user needs as they arise during browsing.
　to apply machine learning to this domain  we need a training set. the input to the machine learning process consists of the various text-based and behavior-based features associated with each word in the current context  webic  1   plus a target label describing whether or not the word is representative of the user's interests. we have developed three methods to produce this label using different sources of information  each based on the subjects in a calibration pool.
ic-wordmodel: each subject hand-labels pages that contain desired information content as ic-pages  otherwise the pages are considered  ic-pages by default . we then assume that all nonstopwords appearing on an  ic-page  are representative of the user's current information need  whereas words that do not are unrepresentative.
ic-relevantmodel: each subject explicitly chooses words from the current browsing session that they felt best represented their current information need. all other words are assumed to be unrepresentative.
ic-querymodel: each subject hand-labels pages that contain desired information content as ic-pages  otherwise the pages are considered  ic-pages by default . but while the ic-word model viewed all words in the ic-page as important  this model is more selective  as it includes only the subset of non-stopwords that enable a search engine  e.g.  google  to find that page. in a separate sub-project  we estimate a search-engine-specific function that  given a page  here an ic-page  p  and a list of words w  estimates whether giving w as a query to the search engine will produce p. using this function  we label only the k highest-scoring words from the session as representative.
　given a set of browsing sessions  we can form a matrix where each row corresponds to a word that appears  and each column to one of the browsing features. we then label each row using one of the models described above  and run a standard learning algorithm  e.g.  c1  to construct 1 different classifiers. each classifier predicts whether a word is representative of the current information need from the text-based and behavior-based features extracted from the user's current browsing session.
1 lilac study
the goals of the lilac  learn from the internet: log  annotation  content  study are to evaluate the browsing behavior models presented above  section 1   and to gather data for future research  from people working on their day-to-day tasks.
1 overview of webic
webic  figure 1  is a client-side  internet explorer-based web browser and recommender system. it uses information from the current browsing session to recommend a page  from anywhere on the web  that it predicts the user will find useful.

figure 1: webic - an effective complete-web recommender system
　to make a prediction  webic first computes the browsing features for all stemmed non-stopwords that appear in any page of the current browsing session. it then determines which of these words to submit as a query to a search engine  using one of the models learned previously. webic recommends the top page returned from the query  as the page it considers most likely to satisfy the user's current information need.
　we modified webic for the lilac study. here  whenever the user requests a recommendation by clicking the  suggest  button  webic will select one of its models randomly to generate a recommendation page. as one of the goals of the lilac study is to evaluate our various models  this specialized version of webic will therefore ask the user to evaluate this proposed page.
　in addition  we also instructed the participants to click  markic  whenever they found an ic-page. after marking an ic-page  webic will recommend an alternative web page as before  excluding the ic-page   as if the user had clicked  suggest  here. once again  this specialized version of webic will then ask the user to evaluate this recommended page.
　as part of this evaluation  the user is asked to  tell us what you feel about the suggested page   to indicate whether the information provided on the suggested page was relevant to his/her search task. there are two categories of relevance evaluations: related and not related at all.
　in addition  the user was also asked to select informative  descriptive keywords  from a short list of words that webic predicted as relevant. the information collected here will be used to train the ic-relevant model described above.
1 experiment design
to participate in the lilac study  the subjects were required to install webic on their own computer  and then use it when browsing their own choice of non-personal english language web pages. they were told to use another browsing engine when dealing with private issues  such as banking  e-mail  or perhaps chat-rooms  etc.
　webic kept track of all the interactions  including a record of the pages the user visited  as well as the evaluation data for the pages that webic recommends.
　lilac considered four models: the three described in section 1 - ic-word  ic-relevant  and ic-query- and  followed hyperlink word   fhw   which is used as a baseline. fhw collects the words found in the anchor text of the followed hyperlinks in the page sequence. as such  there is no training involved in this model. this is similar to  inferring user need by information scent  iunis   model  chi et al.  1 . in all models  words are stemmed and stopwords are removed prior to prediction.
　we used the data collected during the study to weekly retrain each of our ic-models. that is  the users initially used the ic-word1  ic-relevant1 and ic-query1 models  which were based on a model obtained prior to the study. on the 1nd week  they used the ic-word1  ic-relevant1 and ic-query1 models  based on the training data obtained from week 1  as well as the prior model. we repeated this process throughout the study.
1 overall results
a total of 1 subjects participated in the five-week lilac study  from both canada  1  and the us  1 ; 1% are female and 1% are male  over a range of ages  everyone was at least 1  and the majority were between 1 . the subjects visited 1 web pages  clicked  markic  1 times and asked for recommendations by clicking the  suggest  button 1 times. as these two conditions are significantly different  we analyzed the evaluation results for  suggest  and  markic  separately.

figure 1: how often the user rated a recommended page as  related   after  suggest 
　figure 1 indicates how often the user considered the  suggest ed page to be  related . we see that each of our 1 icmodels works much better than the baseline model - each was over 1%  versus the 1% for fhw.
　figure 1 shows the evaluation results for the recommended pages after the user clicked  markic . we again observe that our ic-models work better than fhw. the scores for ic-word and ic-relevant remained at around 1%  roughly the same values they had for the  suggest  case  while the ic-query

figure 1: how often the user rated a recommended page as
 related   after  markic 
model increased from 1% to 1%. we also observed that fhw increased by almost 1%. as an explanation  we speculate the following: if the subject is able to find an ic-page  then the links followed in the current session appear to provide a very strong hint of what constitutes the relevant information content; and fhw benefits significantly from this hint.
1 conclusion and future work
we have demonstrated a practical and extensible framework for a behavior-based web recommender system. the basic framework covered here was able to find relevant pages 1% of the time and consistently outperformed a common baseline method  fhw . our method can easily be extended to include additional features  e.g.  timing  multiple search engines   and to employ personalized training models for custom feature weights. in addition  we anticipate our results  achieved using c1  could be improved by more sophisticated learning techniques. extensive results from our user study suggest that behavior-based recommendation is a promising approach for automatically finding the pages  from anywhere on the web that address the user's current information need  without requiring the user to provide any explicit input.
