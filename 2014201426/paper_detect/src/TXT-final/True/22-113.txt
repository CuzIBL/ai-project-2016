ion space: 
a form of constructive induction 

george drastal and gabor czako 
siemens research and technology laboratories 
1 college rd 
princeton  nj 1 
abstract 
we report on a learning system miro which performs supervised concept formation in an abstraction space. given a domain theory  the method constructs this abstraction space by deduction over instances  and then performs induction in it rather than the initial space defined by instances alone. it is also possible to regard miro as a variant of constructive induction. the vapnik-chervonenkis model suggests that learning in an abstraction space can result in a substantial speedup  and we provide empirical studies which validate this proposition. we also show that learning in an abstraction space can reduce the number of 
false negative and false postive classifications because coincidental patterns are filtered by the deduction process. the method is able to extend an incomplete domain theory represented as at tribute-value pairs with a set of rules that represent a disjunctive concept derived from a batch of training instances. 
1. introduction 
   the concept of abstraction has played an important and well-known role in artificial intelligence since the mid 1's. the difficulty in making use of abstraction has always been the construction of an explicit mapping between the problem definition in the initial space and its definition in an abstraction space . in this paper we report on the learning system miro which performs supervised concept formation in an abstraction space which is constructed by the process of deduction on rules composed of attributevalue pairs. the mapping is exactly the set of proof structures used in this construction. it then performs induction over a language defined by the process of deduction rather than the language defined by the instances  to yield a new characteristic concept description. the set of descriptors  or predicates  which are consequences of this deduction is viewed as the abstraction space  the set of descriptors used to describe the instances is viewed as the initial space  and the set of proof structures is viewed as the mapping between the two spaces. 
stan raatz 
department of computer science 
rutgers university 
new brunswick  nj 1 
   this method can also be regarded as a form of construe tive induction in which  useful  patterns are encoded in the domain theory. however  we propose a strong form of deductive bias as a means of limiting the number of constructed descriptors. while the concept description formed is not justifiable in the sense of explanation-based generalization  1   the descriptors from which it is composed are justifiable.  the method is able to extend an incomplete domain theory with a set of rules that represent a disjunctive concept derived from a batch of training instances. these inductively derived rules may be used in the same way as the original domain theory  when a new batch of instances is presented  and can support incremental refinement of the concept. 
   we present empirical studies which yield evidence for the following conjectures about learning in an abstraction space  as opposed to learning in the initial space . first  such learning can be more efficient because the abstraction space can be by construction more compact than is the initial space. both the vapnik-chervonenkis characterization  1  of the complexity of learning from examples and our own studies suggest typical problems exist in which substantial speedup is expected. second  learn ing in mi abstraction space can reduce the number of false negative and false postive classifications because coincidental patterns are filtered by the deduction process. we show evidence that a significant decrease in the number of misclassifications can be expected. the empirical studies mentioned are based on controlled  randomized  and exhaustive testing of many thousands of trials. this paper is a summary version  prepared for this conference  of reference  which is available by request. 
1. construction of an abstraction space 
   in essence  the basic idea presented here is a two stage process: first  construct an abstraction space  and then apply an induction method over this space. we will define the method for domain theories of rules composed of descriptor-values and an induction method similar to the aq algorithm   but it is important to emphasize that the concept of induction in an abstraction space is 

1 	machine learning 

	drastal  czako and raalz 	1 

description language and an element of g covers a subset of pos including the seed. in general  this is a partial concept description that covers a subset of pos  the positive training set. in case an empty g set is returned  one non-maximal descriptor is selected by a heuristic similar to quinlan's decision tree heuristic  and added to l. this may be repeated until a nonempty g is returned  and a single conjunction c is selected that covers a subset of the positive instances. selection from the g set depends on a heuristic measure of credibility  which balances the number of instances covered by a conjunction with an extra-evidential component that measures the amount of domain knowledge in the proof structure of each instance. 
   in order to obtain a characteristic partial description  each discriminant partial description is specialized by the addition of descriptors from an augmented language into the conjunction. the specialization entails a search guided by the same heuristic measure used to select an element from the g set. the instances covered by each partial characteristic description are then removed from the set pos. a complete concept description is a disjunction of partial concept descriptions. since we require a complete description  candidate elimination is applied repeatedly in order to construct a disjunction of terms. a new seed is used and one partial concept description is selected from the g set on each cycle. the algorithm terminates when either a complete description is found  or some positive instances remain which have been tried as seeds and cannot be discriminated from the set neg. we summarize the method in pseudo-code as follows. 

1 observable  structural features. many of the interesting classes that are represent able in this language share functional properties  e.g. insulated against heat  that are not included in the instance language. we invented a domain theory of 1 rules that represents such a functional property as the consequent of a rule  or chain of rules grounded in the instance language. these included four rules that could be interpreted as defining a class  glass  cup  plate  cooking vessel   but did not include any rules defining the target class for our learning experiments. 
   training instances were created by pseudorandom mechanical generation of structure and features  in order to preclude the introduction of unconscious bias by the investigators. several thousand rather bizarre objects resulted  and these were screened and mechanically sorted into training classes by using a classification rule base that was unknown to the learning program. a total of 1 usable instances resulted  of which 1  spoons1 were chosen for a pool of positive training instances. all remaining instances were used in the negative pool. an average instance of  spoon  has 1 features  and an average instance of any type has 1 features. the target concept is exactly represented by an 1-term-dnf expression in instance space. 
   a single learning trial consists of choosing k  1   k   1  positive and k negative training instances from each pool  running miro to give a set of classification rules  i.e. a characteristic concept description   and then testing that 
concept against 1 positive and 1 negative instances chosen randomly from each pool excluding training instances 

1 	machine learning 

used in that trial. by varying k from 1 through 1 by 1 we obtain a series. each point in an error rate curve reported here is an average of 1 series. figure 1 presents the false negative  solid line  and false positive  dotted line  error rates obtained in the initial feature space  using no domain theory. here the false negative rate has not stabilized after 1 positive and negative instances  beyond which these trials often could not be completed due to exhausting lisp virtual memory. this explosion of memory use is clearly visible from figure 1  which plots growth of the version space g set during candidate elimination. the independent axes are the number of training instances  horizontal  and the number of negative instances eliminated  projection of axis perpendicular to the page . the dependent vertical axis shows the average size of g. we see clearly that induction is very under-constrained  owing to the highly disjunctive nature of the target concept when represented in the initial feature space. 

figure 1 initial space g set growth 
   the following experiment defines the performance baseline for miro in our synthetic domain  using a concept description language that was constructed as described in section 1 to exclude any initial descriptors. the resulting error rates in figure 1  now extended through 1 training instances  reach low plateaus far more rapidly. an average concept was observed to converge at 1-term 1-dnf around 1 training instances  and essentially no cases of g set collapse were observed. the effect of fully exploiting deductive bias is more visible in figure 1  which shows 
1. relationship to other work 
   the work reported in this paper is directly related to constructive induction as mentioned in the introduction  and to recent attempts to integrate explanation-based and inductive learning. however  there are important differences. reference  also contains numerous other comparisons to work that is indirectly related  such as learning by analogy and various extensions to and uses of explanationbased learning  such as . lebowitz  develops a system unimem which searches a database of voting records for empirical generalizations  and verifies these generalizations via a domain theory. he proposes an inductive 
	drastal  czako and raatz 	1 
method that is used to control the search space for a version of a deductive method. this work can be considered the dual of the work reported in this paper  in the sense that we use a deductive technique to formulate a situation for induction. however  the work is very different in details  is presented by example and explanation  and neither includes an algorithm nor empirical studies. pazzani  dyer and flowers  describe a system occam in which causal theories are preferred to correlational or inductive information in forming generalizations  which are subsequently used to suggest additional causal and intentional relationships. the work is also not directly related  since occam does not have a uniform language that is used in both the deductive and inductive stages  that is  it is not actually a form of constructive induction. flann and dietterich  propose a general learning architecture that uses a multiple representation strategy: the system translates task training examples into a ''natural  representation for induction and then translates the learned concept back into the appropriate task representation. the paradigm is illustrated by the system wyl which learns concepts in board games such as checkers and chess. it is similar to the work reported in this paper because in wyl all possible board positions consistent with a functional description of a concept in logic are constructively generated  but the architecture upon which wyl is based appears to be more general. 
1. conclusions 
   we have presented evidence that induction defined over an abstraction space constructed via the process of deduction can result in substantial speed up for some induction problems  and that it is also possible for the number of false negative and false positive classifications to be reduced. in addition  it can be shown  that even with injection of 1% of attribute noise into the training set  the method presented here is able to construct a  corrected  characteristic concept description  and that with certain  ad hoc  assumptions  the method can accept domain theories in the horn subset of first-order logic. 
1. 