
the case-based learning paradigm relies upon memorizing cases in the form of successful problem solving experience  such as e.g. a pattern along with its classification in pattern recognition or a problem along with a solution in case-based reasoning. when it comes to solving a new problem  each of these cases serves as an individual piece of evidence that gives an indication of the solution to that problem. in this paper  we elaborate on issues concerning the proper combination  aggregation  of such pieces of evidence. particularly  we argue that cases retrieved from a case library must not be considered as independent information sources  as implicitly done by most case-based learning methods. focusing on the problem of prediction as a
performancetask  we proposea new inference principle that combines potentially interacting pieces of evidence by means of the so-called  discrete  choquet-integral. our method  called cho-k-nn  takes interdependencies between the stored cases into account and can be seen as a generalization of weighted nearest neighbor estimation.
1	introduction
case-based or instance-based learning algorithms have been applied successfully in fields such as  e.g.  machine learning and pattern recognition during the recent years  aha et al.  1; dasarathy  1 . the case-based learning  cbl  paradigm is also of central importance in case-based reasoning  cbr   a problem solving methodology which goes beyond the standard prediction problems of classification and regression  riesbeck and schank  1; kolodner  1 .
¡¡as the term suggests  in cbl special importance is attached to the concept of a case. a case or an instance can be thought of as a single experience  such as a pattern  along with its classification  in pattern recognition or a problem  along with a solution  in cbr.
¡¡rather than inducing a global model  theory  from the data and using this model for further reasoning  as inductive  model-based machine learning methods typically do  cbl systems simply store the data itself. the processing of the data is deferred until a prediction  or some other type of query  is actually requested  a property which qualifies cbl as a lazy learning method  aha  1 . predictions are then derived by combining the information provided by the stored cases  primarily by those which are similar to the new query.
in fact  the concept of similarity plays a central role in
cbl. the major assumption underlying cbl has already been expressed  amongst others  by the philosopher david hume   hume  1   page 1 :  in reality  all arguments from experience are founded on the similarity ... among natural objects. ... from causes  which appear similar  we expect similar effects.  this commonsense principle  that we shall occasionally call the  similarity hypothesis   rendell  1   serves as a basic inference paradigm in various domains of application. for example  in a classification context  it translates into the assertion that  similar objects have similar class labels   and in cbr it suggests that  similar problems have similar solutions .
¡¡the similarity hypothesis  which is apparently of a heuristic nature  has been put into practice by means of different inference schemes that combine similarity and frequency information in one way or the other. for example  the well-known nearest neighbor  nn  classifier first selects a neighborhood around the query  consisting of the k most similar cases  and then counts the occurrence of the different class labels. despite of their simplicity  inference methods of such kind have proved to be quite successful in practical applications.
¡¡in this paper  we suggest a further improvement based on the idea of taking interdependencies between neighbored cases into account. in fact  in standard nn methods  these cases are implicitly considered as independent information sources. we argue that a corresponding assumption of independence is not always justified and propose a new inference principle that combines interacting pieces of evidence in a more thorough way. this principle  which makes use of non-additive measures for modeling interaction between cases and employs the so-called  discrete  choquet-integral as an aggregation operator  can be seen as a generalization of weighted nn estimation.
¡¡by way of background  section 1 gives a concise review of the nn principle  which constitutes the core of the family of cbl algorithms. in section 1  we discuss the problem of interaction between cases in cbl. a new approach to cbl  which takes such interactions into account  is then introduced in section 1 and evaluated empirically in section 1.
1	nearest neighbor estimation
the well-known nearest neighbor  nn  estimation principle is applicable to both classification problems  prediction of class labels  and regression  prediction of numeric values .
¡¡consider a setting in which an instance space x is endowed with a similarity measure sim : x ¡Á x ¡ú  1 . an instance corresponds to the description x of an object  usually in attribute-value form . in the standard classification framework  each instance x is assumed to have a  unique  label y ¡Ê l. here  l is a finite  typically small  set comprised of m class labels {`1 ...`m}  and hx yi ¡Ê x ¡Ál is a labeled instance  case .
¡¡the nn principle originated in the field of pattern recognition  dasarathy  1 . given a sample s consisting of n labeled instances hx  y i  1 ¡Ü   ¡Ü n  and a novel instance x1 ¡Ê x  a query   this principle prescribes to estimate the label y1 of the yet unclassified query x1 by the label of the nearest  most similar  sample instance. the k-nearest neighbor  k-nn  approach is a slight generalization  which takes the k ¡Ý 1 nearest neighbors of x1 into account. that is  an estimation is derived from the set nk x1  of the k nearest neighbors of x1  usually by means of a majority vote. besides  further conceptual extensions of the  k- nn principle have been devised  such as distance weighting  dudani  1 :
		 1 
hx yi¡Ênk x1 
where ¦Øx is the weight of the instance x and i ¡¤  the standard {true false} ¡ú {1} mapping.  throughout the paper  we assume the weights to be given by ¦Øx = sim x x1 . 
¡¡the nn principlecan also be used forregressionproblems  i.e.  for realizing a  locally weighted  approximation of realvalued target functions x 1¡ú y = f x   in this case  l = r . to this end  one reasonably computes the  weighted  mean of the k nearest neighbors of a new query point:
		 1 
1	interaction between cases in cbl
some case-based approaches completely rely on the  supposedly  most relevant piece of evidence  namely the observation which is most similar to the query. in cbr  for example  it is common practice to retrieve just a single case from the case library  and to adapt the corresponding solution to the problem under consideration. on the one hand  ignoring all but the most similar observation is conceptually simple and computationally efficient. on the other hand  this approach does of course come along with a loss of information  because only a very small part of the past experience is utilized.
¡¡if several instead of only a single case are retrieved  as e.g. in k-nn  an importantquestion arises: how should the pieces of evidence coming from the different cases be combined  in k-nn classification  the evidences in favor of a certain class label are simply added up  see  1  . likewise  in regression the estimation is a simple linear combination of the observed

figure 1: different configurations of locations in twodimensional space.
outcomes  see  1  . thus  the neighbored cases are basically considered as independent information sources.
¡¡this assumption of independence between case-based evidence can thoroughly be called into question  hu¡§llermeier  1 . indeed  it is not even in agreement with the similarity hypothesis itself! namely  if this hypothesis is true  then two neighbored cases that are not only similar to the query case but also similar among each other will probably provide similar information regarding the query. in other words  when taking the similarity hypothesis for granted  the information coming from the neighboredcases is at least not independent.
¡¡in particular  from a problem solving perspective  one should realize that a set of cases can be complementary in the sense that the experiences represented by the individual cases complement or reinforce each other. on the other hand  cases can also be redundant in the sense that much of the information is already represented by a smaller subset among them. and indeed  as we said before  the similarity hypothesis suggests that similar cases are likely to be redundant.
¡¡to illustrate this point by a simple example  consider the problem of predicting student peter's grade in computer science  knowing that he has an a in french. the latter information  i.e. the case hfrench ai  clearly suggests that peter is an excellent student and  hence  supports predicting an a or maybe a b. yet  one cannot be sure that this prediction is correct. in this situation  the additional information that peter has a b in mathematics is probably more valuable than the information that he has an a in spanish as well. in fact  the cases hfrench ai and hspanish ai are partly redundant  since the two subjects are quite similar by themselves. as opposed to this  the case hmathematics bi is complementary in a sense  as it suggests that peter is not only good in languages.
¡¡now  suppose that you know all three grades  mathematics  spanish  french . is a or b the more likely grade  of course  mathematics is more similar to computer science than spanish and french. however  depending on the concrete specification of similarity degrees between the various subjects  it is quite possible that the weighted k-nn rule favors grade a since the two moderately similar a's compensate for the more similar b. of course  this result might be judged critically  and one might wonder whether the grade a should really count twice. in fact  one reasonable alternative is to consider the two a's as only a single piece of evidence  telling something about peter's achievements in languages  instead of two pieces of distinct information. in any case  the close connection between the two a's should not be ignored when combining the three observations.
¡¡as a second example  consider the problem of predicting the yearly rainfall at a certain location  city . for instance  given the rainfall y  at location x     = 1 1   what about the rainfall at location x1 in the two scenarios shown in fig. 1  the important point to notice is that even though the individual distances between x1 and the x  are the same in both scenarios  the y  should not be combined in the same way. in this example  this is due to the different arrangements of the neighbors  zhang et al.  1 : simply predicting the arithmetic mean  y1 + y1 + y1 /1 seems to be reasonable in the left scenario  while the same prediction appears questionable in the scenario shown in the right picture. in fact  since x1 and x1 are closely neighboredin the latter case  informationabout the rainfall at these locations will be partly redundant. consequently  the weight of the  joint  evidence that comes from the observations hx1 y1i and hx1 y1i should not be twice as high as the weight of the evidence that comes from hx1 y1i.
¡¡the above examples show the need for taking interdependencies between observed cases into account and  hence  provide a motivation for the method that will be proposed in the following section. before proceeding  let us make two further remarks: first  the above type of interaction between cases seems to be less important if the sample size is large and even becomes negligible in asymptotic analyses of nn principles. in fact  strong results on the performance of nn estimation can be derived  dasarathy  1   but these are valid only under idealized statistical assumptions and arbitrarily large sample sizes. roughly speaking  if the sample size n tends to infinity  the distance between the query and its nearest neighbors becomes arbitrarily small  with high probability . this holds true even if the size k of the neighborhood is increased too  as a function k n  of n  provided that k n /n ¡ú 1 for n ¡ú ¡Þ. moreover  if the individual observations are independent and identically distributed in a statistical sense  the neighborhood becomes  well distributed . under these assumptions  it is intuitively clear that interdependencies between observations will hardly play any role. on the other hand  it is also clear that statistical assumptions of such kind will almost never be satisfied in practice.
¡¡the second remark concerns related work. in fact  there are a few methods that fit into the cbl framework and that allow for taking certain types of interaction between observations into account. particularly  these are methods that make assumptions on the statistical correlation between observations  depending on their distance  lindenbaum et al.  1 . for example  in our rainfall example one could employ a method called kriging  which is well-known in geostatistics  oliver and webster  1 . usually  however  such methods are specialized on a particular type of application and  moreover  make rather restrictive assumptions on the mathematical  metric  structure of the instance space. our approach  to be detailed in the next section  is much more general in the sense that it only requires a similarity measure sim ¡¤  to be given. we do not make any particular assumptions on this measure  such as symmetry or any kind of transitivity   apart from the fact that it should be normalized to the range  1 . from an application point of view  this seems to be an important point. in cbr  for example  cases are typically complexobjects that cannot easily be embedded into a metric space.
1	the cho-k-nn method
1	non-additive measures
let x be a finite set and ¦Í ¡¤  a measure 1x ¡ú  1 . for any a   x  we interpret ¦Í a  as the weight or  say  the degree of relevance of the set of elements a.
¡¡a standard assumption on the measure ¦Í ¡¤   which is made e.g. in probability theory  is additivity: ¦Í a ¡È b  = ¦Í a  + ¦Í b  for all a b   x s.t. a ¡É b =  . unfortunately  additive measures cannot model any kind of interaction between elements: extending a set of elements a by a set of elements b always increases the weight ¦Í a  by the weight ¦Í b   regardless of a and b.
¡¡suppose  for example  that the elements of two sets a and b are complementary in a certain sense. to illustrate  one might think of x as a set of workers in a factory  and of a weight ¦Í a  as the productivity of a team of workers a. in that case  complementarity means that the output produced by two teams a and b is higher if they cooperate. formally  this can be expressed as a positive interaction: ¦Í a ¡È b    ¦Í a  + ¦Í b . likewise  elements can interact in a negative way. for example  if two sets a and b are partly redundant or competitive  then ¦Í a ¡È b    ¦Í a  + ¦Í b .
¡¡the above considerations motivate the use of non-additive measures  also called fuzzy measures  which are simply normalized and monotone  sugeno  1 :
  ¦Í    = 1  ¦Í x  = 1
  ¦Í a  ¡Ü ¦Í b  for a   b
¡¡in order to quantify the interaction between subsets of a set x  as induced by a fuzzy measure ¦Í ¡¤   several indices have been proposed in the literature. for two individual elements x  x  ¡Ê x  the interaction index is defined by
 
where |a| denotes the cardinality of a and
    ¦Í  a  =df ¦Í a ¡È {x  x }    ¦Í a ¡È {x }      ¦Í a ¡È {x }  + ¦Í a .
the latter can be seen as the marginal interaction between x  and x  in the context a  murofushi and soneda  1 . the above index can be extended from pairs x  x  to any set u   x of elements  grabisch  1 :
 
where
.
1 modeling interaction in case-based learning now  recall our actual problem of combining evidence in case-based learning. in this context  the set x of elements corresponds to the neighbors of the query case x1:
	x = nk x1  = {x1 x1 ...xk}	 1 
our comments so far have shown that fuzzy measures can principally be used for modeling interaction between cases. the basic question that we have to address in this connection is the following: what is the evidence weight or simply the weight  ¦Í a   of a subset a of the neighborhood  1   first  the boundary conditions ¦Í    = 1 and ¦Í x  = 1 should of course be satisfied  expressing that the full evidence is provided by the complete neighborhood x. moreover  according to our comments on the similarity hypothesis  section 1   the evidence coming from a set of cases a   x should be discounted if these cases are similar among themselves. likewise  the weight of a should be increased if the cases are  diverse   hence complementary  in a certain sense.1 to express this idea in a more rigorous way  we define the diversity of a set of cases a by the average pairwise dissimilarity: div 
  x 
 by definition  the diversity is 1 for singletons and the empty set.  we furthermore define the relative diversity by rdiv 
now  the idea is to modify the basic  additive  measure
	¦Ì a  =df c 1 x sim x1 x   	 1 
x ¡Êa
where c = px ¡Êx sim x1 x    by taking the diversity of a into account. of course  this can be done in different ways. here  we used the following approach:
	¦Í¡¥ a  =df ¦Ì a  ¡¤  1 + ¦Árdiv a  	 1 
as can be seen  the original measure ¦Ì a  of a set of cases a is increased if the diversity of a is relatively high  otherwise it is decreased. the parameter ¦Á ¡Ý 1 controls the extent to which interactions between cases are taken into consideration: ¦Ì a  can be modified by at most  1¦Á %. for ¦Á = 1  interactions are completely ignored and the original measure ¦Ì ¡¤  is recovered.
¡¡for the measure ¦Í¡¥ ¡¤  as defined in  1   the monotonicity condition does not necessarily hold. to remedy this problem  we simply enforce this property by setting
	¦Í a  =df max¦Í¡¥ b .	 1 
b a
finally  the boundary conditions are guaranteed by dividing the measure thus obtained by ¦Í x .
¡¡to illustrate  consider again the rainfall example in the right picture of fig. 1 and suppose that sim x  x1  = .1    = 1 1   sim x1 x1  = .1  sim x1 x1  = sim x1 x1  = 1  with ¦Á = 1 in  1   we obtain the following weights:
ax1x1x1x1 x1x1 x1x1 x1¦Í a .1.1.1.1.1.1¡¡as can be seen  the joint weight of {x1 x1} is relatively low  reflecting the partial redundance of the two cases.
¡¡before going on  let us comment on the derivation of the measure ¦Í ¡¤  fromthe similarity functionsim ¡¤ . firstly  even though the measure  1  captures our intuitive idea of decreasing  increasing  the evidence weight of cases that are  dis similar by themselves  we admit that it remains ad hoc to some extent  and by no means we exclude the existence of better alternatives. for example  an interesting idea is to derive the measure in an indirect way: first  the interaction indices i¦Í ¡¤  from section 1 are defined  again based on the similarity between cases. these indices can be seen as constraints on the measure ¦Í ¡¤   and the idea is to find a measure that is maximally consistent with these constraints.
¡¡secondly  even though the assumption that similar cases provide redundant information is supported by the similarity hypothesis  one might of course argue that the similarity between the predictive parts of two cases  x  and x   is not sufficient to call them redundant. rather  the associated output values y  and y  should be similar as well. indeed if y1 differs drastically from y1  the first two measurements in our rainfall example might better be considered as non-redundant.  in that case  the two measurements in conjunction suggest that there is something amiss ...  this conception of redundancy can easily be represented by deriving ¦Í ¡¤  from an extended similarity measure sim1 ¡¤  which is defined over x ¡Á l.1
¡¡anyway  the important point of this section is not so much the specification of a particular evidence measure  but rather the insight that non-additive measures can in principle be used for modeling the interaction between cases in cbl.
1	aggregation of interacting pieces of evidence
so far  we have a tool for modeling the interaction between different pieces of evidence in case-based learning. the next question that we have to address is how to combine these pieces of evidence  i.e.  how to aggregate them in agreement with the evidence measure ¦Í ¡¤ .
¡¡for the time being we focus on the problem of regression. recall that in the standard approach to nn estimation  an aggregation of the output values f x   = y  is realized by means of a simple weighted average:
	 	 1 
where. interestingly   1  is nothing else than the standard lebesgue integral of the function f : x ¡ú r with respect to the additive measure  1 :

in order to generalize this estimation  an integral with respect to the non-additive measure ¦Í ¡¤  is needed: the choquet integral  a concept that originated in capacity theory  choquet  1 .
¡¡let ¦Í ¡¤  be a fuzzy measure and f ¡¤  a non-negative function.1 the choquet integral of f ¡¤  with respect to ¦Í ¡¤  is then defined by

where  f   t  = {x|f x    t}. the integral on the righthand side is the standard lebesgue integral  with respect to the borel measure on  1 ¡Þ  . in our case  where x is a finite set  we can refer to the discrete choquet integral which can be expressed in a rather simple form:

permutation of {1...k} such that 1 ¡Ü
  ¦Ð 1  	...	 x¦Ð k    and a  = {x¦Ð 1  ...x¦Ð   }.
¡¡the discrete choquet integral  1  can be seen as a special type of aggregationoperator  namely a generalized arithmetic mean. indeed   1  coincides with  1  if ¦Í ¡¤  is an additive measure  i.e. if ¦Í ¡¤  = ¦Ì ¡¤  . otherwise  it is a proper generalization of the standard  weighted  nn estimation.
¡¡coming back to our running example  suppose that we have measured the following rainfalls: y1 = 1  y1 = 1  y1 = 1. according to  1   we then obtain the estimation  joint weight of the two locations with
less rainfall  x1 and x1  has been decreased  this estimation is higher  closer to y1  than the standard weighted nn estimation given by.
¡¡so far  we have focused on the problem of regression. in the case of classification  the choquet integral cannot be applied immediately  since an averaging of class labels y  does not make sense. instead  the choquet integral can be derived for each of the indicator functions f` : y 1¡ú i y = `  
` = `1 ...`m. as in  1   the evidence in favor of each class label is thus accumulated separately. now  however  the interaction between cases is taken into account. as usual  the estimation is then given by the label with the highest degree of accumulated evidence.
1	empirical validation
in order to validate the extension of nn estimation as proposed in the previous section  we have performed several experimental studies using benchmark data sets from the uci repository1 and the statlib archive.1
¡¡experiments were performed in the following way: a data set is randomly split into a training and a test set of the same size. for each example in the test set  a prediction is derived using the training set in combination with weighted k-nn resp. cho-k-nn. in the case of regression  an estimation y1est is evaluated by the relative estimation error   and the overall performance of a method by the mean of this error over all test examples. in the case of classification  we
data setkweighted k-nncho-k-nnauto-mpg1.1.1 1 1 1.1.1 1 1 bolts1.1.1 1 1 1.1.1 1 1 housing1.1.1 1 1 1.1.1 1 1 detroit1.1.1 1 1 1.1.1 1 1 echomonths1.1.1 1 1 1.1.1 1 1 pollution1.1.1 1 1 1.1.1 1 1 table 1: estimation of expected relative estimation error and its standard deviation.
simply took the misclassification rate as a performanceindex. moreover  we derived statistical estimations of the expected performance of a method by repeating each experiment 1 times.
¡¡for the purpose of similarity computation  all numeric attributes have first been normalized to the unit interval by linear scaling. the similarity was then defined by 1 distance for numeric variables and by the standard 1-measure in the case of categorical attributes. the overall similarity sim ¡¤  was finally obtained by the average over all attributes. as the purpose of our study was to compare -under equal conditions- weighted k-nn with cho-k-nn in order to verify whether or not taking interactions into account is useful  we refrained from tuning both methods  e.g. by including feature selection or feature weighting  even though it is wellknown that such techniques can greatly improve performance  wettschereck et al.  1  . results have been derived for neighborhood sizes of k = 1 and k = 1; the parameter ¦Á in  1  has always been set to 1.
¡¡the application of cho-k-nn for regression has shown that it consistently improves weighted k-nn  sometimes only slightly but often even considerably. some results are shown in table 1. in particular  it seems that the smaller the size of the data set  the higher the gain in performance. this finding is intuitively plausible  since for large data sets the neighborhoods of a query tend to be more  balanced ; as already said  the neglect of interaction is likely to be less harmful under such circumstances.
¡¡for classification problems  it is also true that cho-k-nn consistently outperforms weighted k-nn; see table 1. usually  however the gainin classification accuracyis onlysmall  in many cases not even statistically significant. again  this is especially true for large data sets  and all the more if the classification error is already low for standard k-nn. nevertheless  one should bear in mind that  in the case of classification  the final prediction is largely insensitive toward modifications of the estimated evidences in favor of the potential labels. in fact  in this study we only checked whether the final prediction is correct or not and  hence  used a rather crude quality measure. more subtle improvements of an estimation such as  e.g.  the enlargement of an example's margin  schapire et
data setkweighted k-nncho-k-nnglass1.1.1 1 1 1.1.1 1 1 wine1.1.1 1 1 1.1.1 1 1 zoo1.1.1 1 1 1.1.1 1 1 ecoli1.1.1 1 1 1.1.1 1 1 balance1.1.1 1 1 1.1.1 1 1 derma1.1.1 1 1 1.1.1 1 1 table 1: estimation of expected classification error and its standard deviation.
al.  1   are not honored by this measure. for the future  we therefore plan to complement our results by more sophisticated experimental comparisons.
1	concluding remarks
this paper has motivated the consideration of mutual dependencies between cases that represent past experience in casebased learning. the basic idea is that a combination of two or more cases can provide complementary but also redundant evidence. in orderto model this type of interactionin a formal way  we have proposedthe use of non-additivemeasures. the aggregation of different pieces of evidence can then be accomplished by means of the choquet integral. the inference scheme thus obtained  referred to as cho-k-nn  is a direct extension of the standard weighted nn estimation  which is recovered in the case of an additive measure .
¡¡our experimentalresults have shown that cho-k-nn consistently outperforms standard  weighted  k-nn on publicly available benchmark data. sometimes  only marginal improvements can be achieved  particularly in the case of classification and all the more for large   well-distributed  data sets  but for many problems cho-k-nn is considerably better than k-nn. all things considered it can be said that taking interaction between cases in cbl into account is worthwhile by any means  mostly it helps  at worst it remains ineffective .
¡¡as already said  the derivation of a non-additive measure ¦Í ¡¤  from the similarity function sim ¡¤  as outlined in section 1 is only a first attempt which leaves scope for development. in particular  the degree to which a set of cases is complementary resp. redundant might not only depend on their mutual similarity but also on other aspects.
¡¡we have explained how to use the choquet integral as an aggregation operator in both regression and classification problems. an interesting question concerns the extension of our approach to more general problem solving tasks as they are typically found in case-based reasoning. even though it is obvious that the approach cannot be transferred immediately  the basic ideas and concepts might still be useful. in any case  the concrete solution will strongly depend on the particular type of application.
