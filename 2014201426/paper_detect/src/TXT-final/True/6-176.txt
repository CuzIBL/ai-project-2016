 
recent algorithms like rtdp and lao* combine the strength of heuristic search  hs  and dynamic programming  dp  methods by exploiting knowledge of the initial state and an admissible heuristic function for producing optimal policies without evaluating the entire space. in this paper  we introduce and analyze three new hs/dp algorithms. a first general algorithm schema that is a simple loop in which 'inconsistent' reachable states  i.e.  with residuals greater than a given c  are found and updated until no such states are found  and serves to make explicit the basic idea underlying hs/dp algorithms  leaving other commitments aside. a second algorithm  that builds on the first and adds a labeling mechanism for detecting solved states based on tarjan's strongly-connected components procedure  which is very competitive with existing approaches. and a third algorithm  that approximates the latter by enforcing the consistency of the value function over the likely' reachable states only  and leads to great time and memory savings  with no much apparent loss in quality  when transitions have probabilities that differ greatly in value. 
1 	introduction 
heuristic search algorithms have been successfully used for computing optimal solutions to large deterministic problems  e.g.  ikorf  1  . in the presence of non-determinism and feedback  however  solutions are not action sequences but policies  and while such policies can be characterized and computed by dynamic programming  dp  methods  bellman  1; howard  1   dp methods take all states into account and thus cannot scale up to large problems  boutilier et al  
1 . recent algorithms like rtdp ibarto et al  1 and lao*  hansen and zilberstein  1   combine the strength of heuristic search and dynamic programming methods by exploiting knowledge of the initial state sq and an admissible heuristic function  lower bound  h for computing optimal policies without having to evaluate the entire space. in this paper we aim to contribute to the theory and practice of heuristic search/dynamic programming methods by formulating and analyzing three new hs/dp algorithms. the first algorithm  called flnd-and-revlse is a general schema 
search 
that comprises a loop in which states reachable from s1 and the greedy policy that have bellman residuals greater than a given 1  we call them 'inconsistent' states   are found and updated until no one is left. find-and-revise makes explicit the basic idea underlying hs/dp algorithms  including rtdp and lao*. we prove the convergence  complexity  and optimality of find-and-revise  and introduce the second algorithm  hdp  that builds on it  and adds a labeling mechanism for detecting solved states based on tarjan's efficient strongly-connected components procedure  tarjan  1 . a state is solved when it reaches only 'consistent' states  and solved states are skipped in all future searches. hdp terminates when the initial state is solved. hdp inherits the convergence and optimality properties of the general flnd-andrevise schema and is strongly competitive with existing approaches. the third algorithm  hdp i   is like hdp  except that while hdp computes a value function by enforcing its consistency over all reachable states  i.e.  reachable from s1 and the greedy policy   hdp i  enforces consistency over the likely' reachable states only. we show that this approxima-
tion  suitably formalized  can lead to great savings in time and memory  with no much apparent loss in quality  when transitions have probabilities that differ greatly in value. 
　our motivation is twofold: to gain a better understanding of hs/dp methods for planning with uncertainty  and to develop more effective hs/dp algorithms for both optimal and approximate planning. 
1 	preliminaries 
1 model 
we model non-deterministic planning problems with full feedback with state models that differ from those used in the classical setting in two ways: first  state transitions become 
probabilistic; second  states are fully observable. the resulting models are known as markov decision processes  mdps  and more specifically as stochastic shortest-path problems ibertsekas  1   and they are given by:1 


m1. positive action costs 1  and 
m1. fully observable states. 
due to the presence of full feedback  m1  and the standard 
markovian assumptions  the solution of an mdp takes the form of a function  mapping states s into actions a a s . such a function is called a policy. a policy assigns a probability to every state trajectory starting in state so which is given by the product of the transition probabilities  if we further assume that actions in goal states have no costs and produce no changes  the expected cost 
associated with a policy r starting in state sq is given by the weighted average of the probability of such trajectories times their cost  an optimal solution is a policy 
1r* that has a minimum expected cost for all possible initial states. an optimal solution is guaranteed to exist provided the following assumption holds  bertsekas  1: 
m1. the goal is reachable from every state with non-zero probability. 
since the initial state s1 of the system is fixed  there is in principle no need to compute a complete policy but a partial policy prescribing the action to take in the states that can be reached following the policy from s1. traditional dynamic programming methods like value or policy iteration compute complete policies  while recent heuristic search dp methods like rtdp and lao* compute partial policies. they achieve this by means of suitable heuristic functions h s  that provide admissible estimates  lower bounds  of the expected cost to reach the goal from any state s. 
1 	dynamic programming 
any heuristic or value function h defines a greedy policy 1t/t: 
		 1  
where the expected cost from the resulting states  s' is assumed to be given by  the greedy action 
in s for the value function h. if we denote the optimal  expected  cost from a state s to the goal by  it is well known that the greedy policy 1r/t is optimal when h is the optimal cost function  i.e. h =  
　while due to the possible presence of ties in  1   the greedy policy is not unique  we will assume throughout the paper that these ties are broken systematically using an static ordering on actions. as a result  every value function v defines a unique greedy policy and the optimal cost function k* defines a unique optimal policy we define the relevant states as the states that are reachable from sq using this optimal policy; they constitute a minimal set of states over which the optimal value function needs to be defined.1 
　value iteration  vi  is a standard dynamic programming method for solving mdps and is based on computing the optimal cost function v* and plugging it into the greedy policy  1 . this optimal cost function is the only solution to the fixed 
　　1 this definition of 'relevant states* is more restricted than the one in  barto et al  1  that includes the states reachable from so by any optimal policy. 
point equation: 
		 1  
also known as bellman's equation. for stochastic shortest path problems like m1-m1 above  the border condition v s  = 1 is also needed for goal states s  g. value iteration solves  1  by plugging an initial guess for v* in the right-hand side of  1  and obtaining a new guess on the lefthand side. in the form of vi known as asynchronous value iteration  bertsekas  1  this operation can be expressed as: 
 1  
where v is a vector of size |s| initialized arbitrarily  normally to 1  and where the equality in  1  is replaced by assignment. the use of expression  1  for updating a state value in v is called a state update or simply an update. in standard  synchronous  value iteration  all states are updated in parallel  while in asynchronous value iteration  only a selected subset of states is selected for update at a time. in both cases  it is known that if all states are updated infinitely often  the value function v converges eventually to the optimal value function. from a practical point of view  value iteration is stopped when the bellman error or residual defined as the difference between left and right in  1 : 

over all states s is sufficiently small. in the discounted mdp formulation  a bound on the policy loss  the difference between the expected cost of the policy and the expected cost of the optimal policy  can be obtained as a simple expression of the discount factor and the maximum residual. in stochastic shortest path models  no similar closed-form bound is known  although such bound can be computed  bertsekas  1 . thus  one can execute value iteration until the maximum residual becomes smaller than a given  then if the bound on the policy loss is higher than desired  the same process can be repeated with a smaller and so on  see  hansen and zilberstein  1  for a similar idea . for these reasons  we will take as our basic task below  the computation of a value function v s  with residuals no greater than a given parameter  
　one last definition and a few known results before we proceed. we say that cost function v is monotonic iff 
 1  
for every s s. notice that a monotonic value function never decreases when updated  and moreover  must increase by more than when updated in a state whose residual is greater than as in the deterministic setting  a nonmonotonic cost function can be made monotonic by simply taking the value v s  to be the max between v s  and the right-hand side of bellman's equation. the following results are well known. 
theorem 1 a  the optimal values v*  s  of a model mj-m1 are non-negative and finite; b  the monotonicity and admissibility of a value function are preserved through updates. 

1 	search 

start with a lower bound function repeat 
find a state s in the greedy graph gv with  
     revise v at s until no such state is found return v 
algorithm 1: flnd-and-revlse 
1 find-and-revise 
the flnd-and-revlse schema is a general asynchronous vi algorithm that exploits knowledge of the initial state and an admissible heuristic for computing optimal or nearly optimal policies without having to evaluate the entire space. let us say that a value function v -consistent  inconsistent  over a state  s when the residual over s is no greater  greater  than f  and that v itself is -consistent when it is -consistent over all the states reachable from s1 and the greedy policy  then flnd-and-revlse computes an -consistent value function by simply searching for inconsistent states in the greedy graph and updating them until no such states are left; see alg. 1. 
　the greedy graph  refers to the graph resulting from the execution of the greedy policy  starting in s1; i.e.  s1 is the single root node in gv and for every non-goal state s in 1vi its children are the states that may result from executing the action in s. 
　the procedures find and revise are the two parameters of the flnd-and-revlse procedure. for the convergence  optimality  and complexity of flnd-and-revlse  we assume that find searches the graph systematically  and revise of v at s updates v at s  and possibly at some other states   both operations taking 1 |s|  time. 
theorem 1  convergence  for a planning model mi-m1 with an initial value function h that is admissible and monotonia flnd-a/w/-revlse yields an  value function in a number of loop iterations no greater than  where each iteration has time com-
theorem 1  optimality  for a planning model ml -ms with an initial admissible and monotonia value function  the value 
function computed by find-and-revlse approaches the op-
timal value function over all relevant states as e goes to 1. 
1 labeling 
we consider next a particular instance of the general findand-revise schema in which the find operation is carried out by a systematic depth-first search that keeps track of the states visited. in addition  we consider a labeling scheme on top of this search that detects  with almost no overhead  when a state is solved  and hence  when it can be skipped in all future searches. a state s is defined as solved when the value function v is e-consistent over s and over all states reachable from s and the greedy policy  clearly  when this condition holds no further updates are needed in s or the states reachable from 1. the resulting algorithm terminates when the initial state s1 is solved and hence when an e-consistent value function has been obtained. 
search 

figure 1: a graph and its strongly-connected components. 
　due to the presence of cycles in the greedy graph  bottomup algorithms common in ao* implementations cannot be used. indeed  if s is reachable  in the greedy graph  from a descendant s' of s  then bottom-up approaches will be unable to label either state as solved. a labeling mechanism that works in the presence of cycles is presented in  bonet and geffner  1  for improving the convergence of rtdp. basically  after each rtdp trial  an attempt is made to label the last unsolved state .s in the trial by triggering a systematic search for inconsistent states from .s. if one such state is found  it is updated  and a new trial is executed. otherwise  the state s and all its unsolved descendants are labeled as solved  and a new cycle of rtdp trials and labeling checks is triggered. here we take this idea and improve it by removing the need of an extra search for label checking. the label checking will be done as part of the find  dfs  search with almost no overhead  exploiting tarjan's linear algorithm for detecting the strongly-connected components of a directed graph itarjan  1   and a correspondence between the strongly-connected components of the greedy graph and the minimal collections of states that can be labeled at the same time. 
　consider the  directed  greedy graph 	and the relation  between pairs of states s and 	that holds when 
or when s is reachable from and is reachable from s in gv. the strongly-connected components of gv are the equivalence classes defined by this relation and form a partition of the set of states in gv. for example  for the greedy graph in fig. 1  where 1 and 1 are terminal  goal  states  the components are 	and 
c  - 	tarjan's algorithm detects the strongly-
connected components of a directed graph in time 
while traversing the graph depth-first  where n stands for the number of states  in gv  and e for the number of edges. 
　the relationship between labeling and strongly-connected components in gv is quite direct. let us say first that a component c is e-consistent when all states s  c are consistent  and that a component c is solved when every state s  c is solved. let's then define as the graph whose nodes are the components of and whose directed edges are when some state in is reachable from some state in c. clearly  is an acyclic graph as two components which are reachable from each other will be collapsed into the same equivalence class. in addition  
1. a state s is solved iff its component c is solved  and furthermore  

1. a component c is solved iff c is consistent and all components are solved. 
the problem of labeling states in the cyclic graph gy can thus be mapped into the problem of labeling the components in the acyclic graph gy  which can be done in bottom up fashion. 
　from fig. 1 is easy to visualize the component graph associated to the greedy graph. thus  if 1 is the only inconsistent state  for example  we can label the components c  and ci as solved  while leaving c1 and c1 unsolved. 
　the code that simultaneously checks in depth-first fashion the consistency of the states and the possibility of labeling them is shown in alg. 1. we call the resulting algorithm  hdp. hdp inherits its convergence and optimality properties from the find-and-revise schema and the correctness of the labeling mechanism. 
　we do not have space to explain hdp code in detail  yet it should be clear to those familiar with tarjan's algorithm; in particular  the use of the state visit number  s.idx  and the 'low-link' number  s.low  for detecting when a new component has been found. the flag flag and the  normal  propagation of the visit numbers prevent a component from being labeled as solved when it is inconsistent or can reach an inconsistent component. 
theorem 1  correctness  the value function computed by hdp for a planning model m1-m1  given an initial admissible and monotonic value function  is t-consistent. 
1 	experimental results 
we now evaluate the performance of hdp in comparison with other recent heuristic search/dp algorithms such as the second code for lao* in  hansen and zilberstein  1   that we call improved lao*  ilao*   and labeled rtdp  lrtdp   a recent improvement of rtdp that accelerates its convergence  bonet and geffner  1 . we use parallel value iteration as the baseline. we've implemented all these algorithms in c++ and the experiments have been run on a sun fire-1r with 1 mhz and 1gb of ram. 
　the domain that we use for the experiments is the racetrack from  barto et al  1 . the states are tuples that represent the position and speed of the car in the x  y dimensions. the actions are pairs a -  ax  ay  of instantaneous accelerations where  uncertainty in this domain comes from assuming that the road is 'slippery' and as a result  the car may fail to accelerate or desaccelerate. more precisely  an action a =  ax  ay  has its intended effect with probability 1 - p  while with probability p the action effects correspond to those of the action 
 =  1 . also  when the car hits a wall  its velocity is set to zero and its position is left intact  this is different than in  barto et al.  1  where for some reason the car is moved to the start position . 
　we consider the track l a r g e - b from  barto et al  1   h- t r a c k from  hansen and zilberstein  1  1 and five other tracks  squares and rings of different size . information about these instances can be found in the first three rows 

of table 1  including number of states  optimal cost  and percentage of states that are relevant. 
　as heuristic  we follow  bonet and geffner  1   and use the domain independent admissible and monotonic heuristic hmini obtained by replacing the expected cost in bellman equation by the best possible cost. the total time spent computing heuristic values is roughly the same for the different algorithms  except vi   and is shown separately in the fifth row in the table  along with its value for s1. the experiments are carried with three heuristics: and h = 1. 
　the results are shown in table 1. hdp dominates the other algorithms over all the instances for  while lrtdp is best  with one or two exceptions  when the weaker heuristics and 1 are used. thus  while hdp seems best for exploiting good heuristic information over these instances  lrtdp bootstraps more quickly  i.e.  it quickly computes a good value function . we hope to understand the reasons for 

1 	search 


table 1: problem data and convergence time in seconds for the different algorithms with different heuristics. results for e - 1 -1 and probability p = 1. faster times are shown in bold. 

these differences in the future. 
1 	approximation 
hdp  like find-and-revise  computes a value function v by enforcing its consistency over the states reachable from so  and the greedy policy ny. the final variation we consider  that we call hdp i   works in the same way  yet it enforces the consistency of the value function v only over the states that are reachable from so and the greedy policy with some minimum likelihood. 
　for efficiency  we formalize this notion of likelihood  using a non-negative integer scale  where 1 refers to a normal outcome  1 refers to a somewhat surprising outcome  1 to a still more surprising outcome  and so on. we call these measures plausibilities  although it should be kept in mind  that 1 refers to the most plausible outcomes  thus 'plausibility greater than f  means 'a plausibility measure smaller than or equal to i.y 
　we obtain the transition plausibilities ka s' s  from the corresponding transition probabilities by the following discretization: 
 1  
with thus formalized': the most plausible next states have always plausibility 1. these transition plausibilities are then combined by the rules of the k calculus ispohn  1  which is a calculus isomorphic to the probability calculus  e.g.  goldszmidt and pearl  1  . the plausibility of a state trajectory given the initial state  is given by the sum of the transition plausibilities in the trajectory  and the plausibility of reaching a state  is given by the plausibility of the most plausible trajectory reaching the state. 
　the hdp i  algorithm  for a non-negative integer z  computes a value function v by enforcing its consistency over the states reachable from so with plausibility greater than or equal to i. hdp i  produces approximate policies fast by pruning certain paths in the search. the simplest case results 
search 
from i = 1  as the code for hdp o  corresponds exactly to the code for hdp  except that the possible successors of a state a in the greedy graph are replaced by the plausible successors. 
　hdp i  computes lower bounds that tend to be quite tight over the states that can be reached with plausibility no smaller than i. at run time  however  executions may contain 'surprising* outcomes  taking the system 'out' of this envelope  into states where the quality of the value function and its corresponding policy  are poor. to deal with those situations  we define a version of hdp i   called hdp t j   that interleaves planning and execution as follows. hdp i  j  plans from s = so by means of the hdp i  algorithm  then executes this policy until a state trajectory with plausibility measure greater than or equal to j  and leading to a  non-goal  state s' is obtained. at that point  the algorithm replans from s' with hdp i   and the same execution and replanning cycle is followed until reaching the goal. clearly  for sufficiently large j  hdp i j  reduces to hdp i   and for large i  hdp i  reduces to hdp. 
　table 1 shows the average cost for hdp i j  for i = 1  i.e.  most plausible transitions considered only   and several values for j  replanning thresholds . each entry in the table correspond to an average over 1 independent executions. we also include the average cost for the greedy policy with respect to  as a bottom-line reference for the figures. memory in the table refers to the number of evaluated states. as these results show  there is a smooth tradeoff between quality  average cost to the goal  and time  spent in initial planning and posterior replannings  as the parameter 
j vary. we also see that in this class of problems the hmtn heuristic delivers a very good greedy policy. thus  further research is necessary to assess the goodness of hd  i j  and the hmin heuristic. 
1 	related work 
we have built on ibarto et al.  1  and  bertsekas  1   and more recently on  hansen and zilberstein  1  and  bonet and geffner  1 . the crucial difference between 

h - t r a c k square- -1 r i n g - :   r i n g - 1 algorithm time quality memory time quality memory time quality memory time quality memory hdp hmin  1 1 1 1 1 1 1 1 1 1 1 1 hdp 1  1 1 1 1 1 1 1 1 1 1 1 1 hdp 1  1 1 1 1 1 1 1 1 1 1 1 1 hdp 1  1 1 1 1 1 1 1 1 1 1 1 1hdp 1  1 1 1 1 1 1 1 1 1 1 1 1 grccdy hmi   n/a 1 1 n/a 1 1 n/a 1 1 n/a 1 1 table 1: results of hdp 1 y  for j = 1 1 and greedy policy with respect to 1andp = 1. each value is the average over 1 executions. n/a in time for the greedy policy means  not applicable  since there is no planning. 

find-and-revise and general asynchronous value iteration is the focus of the former on the states that are reachable from the initial state so and the greedy policy. in rtdp  the find procedure is not systematic and is carried out by a stochastic simulation that may take time greater than 1 |1|  when the inconsistent states are reachable with low probability  this explains why rtdp final convergence is slow; see  bonet and geffner  1  . lao*  on the other hand  keeps track of a subset of states  which initially contains .s1 only  and over which it incrementally maintains an optimal policy through a somewhat expensive revlse  full dp  procedure. this is then relaxed in the second algorithm ir.  hansen and zilberstein  1   called improved lao* here. the use of an explicit envelope that is gradually expanded is present also in idean et ai  1  and  tash and russell  1 . interestingly  these envelopes are expanded by including the most likely' reachable states not yet in the envelope. the algorithm hdp i  exploits a similar idea but formulates it in a different form and has a crisp termination condition. 
1 	summary 
we have introduced and analyzed three hs/dp algorithms that exploit knowledge of the initial state and an admissible heuristic function for solving planning problems with uncertainty and feedback: find-and-revise  hdp  and hdp i . find-and-revise makes explicit the basic idea underlying hs/dp algorithms: inconsistent states are found and updated  until no one is left. we have proved its convergence  complexity  and optimality. hdp adds a labeling mechanism based on tarjan's scc algorithm and is strongly competitive with current algorithms. finally  hdp i  and hdp i  j  offer great time and memory savings  with no much apparent loss in quality  in problems where transitions have probabilities that differ greatly in value  by focusing the updates on the states that are more likely to be reached. 
acknowledgements: we thank eric hansen and shlomo zilberstein for making the code for lao* available to us. blai bonet is supported by grants from nsf  onr  afosr  dod 
mur1 program  and by a usb/con1t fellowship from venezuela. hdctor geffner is supported by grant tic1-c1 from mcyt  spain. 
