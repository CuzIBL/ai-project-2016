 
a class of concept learning algorithms cl augments standard similarity-based techniques by performing feature construction based on the sbl output. pagallo and hausslcr's f r i n g e   
pagallo's extension symmetric f r i n g e  symfringe  and a refinement we call dcfringe are all instances of this class using decision trees as their underlying representation. these methods use patterns at the fringe of the tree to guide their construction  but dcfringe uses limited construction of conjunction and dis-
junction. experiments with small d n f and c n f concepts show that dcfringe outperforms both the purely conjunctive f r i n g e and the less restrictive symfringe  in terms of accuracy  conciseness  and efficiency. further  the gain of these methods is linked to the size of the training set. we discuss the apparent limitation of current methods to concepts exhibiting a low degree of feature interaction  and suggest ways to alleviate it. this leads to a feature construction approach based on a wider variety of patterns restricted by statistical measures and optional knowledge. 
1 introduction 
supervised concept learning is the problem of finding a description of an unknown class of objects for which we are given a set of training examples. the most common approaches to this problem can be collectively referred to as similarity-based learning  sbl   including agglomerative learning systems and splitting algorithms  breiman et a/.  1; quinlan  1 . 
   the effectiveness of most sbl algorithms is influenced by concept difficulty  variously measured as feature interaction  devijver and kittler  1; rendell and seshu  1   concept dispersion  rendell and cho  1   cross entropy  ragavan and rendell  1   and term and literal complexity  ehrenfeucht et a/.  1 . these studies 
   *the authors gratefully acknowledge the support of nsf grant iri 1. blix is supported by the royal norwegian research council for science and humanities and university of illinois cs fellowships. 

figure 1: decision tree representation for 	x x1 	vx1. 
show that sbl concept difficulty affects the accuracy and conciseness of machine learning systems. 
   concepts used to test similarity-based learning systems are often not difficult  because their corresponding feature set is selected with the aid of human experience. however  for many interesting and important concepts the appropriate level of abstraction is not directly available. a shift of inductive bias through feature construction may help construct an instance space at a proper level of abstraction for conventional sbl system to work  utgoff  1; matheus  1 . this paper discusses a general approach to feature construction and compares specific approaches based on the f r i n g e algorithm. 
   f r i n g e  pagallo and haussler  1; pagallo  1  is a feature construction algorithm for decision-tree learning designed to combat the replication problem. the replication problem is the duplication of a sequence of tests in different branches of a decision tree  fig. 1   leading to an inconcise representation that also tends to have low predictive accuracy. although replications are inherent to the decision tree representation rather than the concept itself  the replication problem is a manifestation of underlying concept characteristics that is accentuated for difficult concepts  yang et a/.  1 . 
   section 1 presents a generalized algorithm schema based on f r i n g e that utilizes the output of similaritybased learning for feature construction. we present some specific approaches captured by this algorithm schema. section 1 summarizes and interprets the results of experiments with three feature construction algorithms. finally  section 1 discusses inherent limitations and research directions. 
yang  rendell  and blix 

concept learning algorithm cl 
let prm be the set of primitive user-given attributes. 
input the data expressed using prm  each with its class-member ship value. 
initialize active feature set act  - prm. 
repeat until some stopping criterion is met: 
1. convert data from ground prm to active form act; 1. perform sbl act  data  tree  
 build a decision tree using splitting algorithm ; 
1. if required  call fc to construct new feature s  new to add to act; 
1. if act is large  prune to eliininate less useful features. 
figure 1: concept learning with feature construction. 
1 feature construction based on sbl output 
fringe  pagallo  1  and citre  matheus  1  are two members of a class of concept learning algorithms cl that incorporate feature construction. given a set of primitive attributes and corresponding data annotated with class-membership values  cl learns a concept by iterating two basic steps:  1  attempt to learn the concept through some standard induction method  and  1  use the output to construct new features and repeat the process. this algorithm schema also summarizes other approaches to feature construction  e.g.   devijver and kittler  1; schlimmer  1; seshu et a/.  1  . 
　figure 1 shows more detail. although step 1 could use any attribute-based induction algorithm  we confine this discussion to sbl methods that output decision trees  e.g.   breiman et a/.  1; quinlan  1  . to detect whether feature construction fc is appropriate  step 1 might base its decision on accuracy or the size of tree  matheus  1; pagallo  1; rendell and seshu  1   but in our simplified approach  the loop's stopping criterion is failure to produce new features. step 1 is important for difficult problems  devyver and kittler  1; pagallo  1; seshu et al  1   but is not part of the algorithms we test. 
　henceforth  the acronym fc refers to feature construction based on decision tree output of sbl. since feature construction is complex  appropriate bias is essential. the following sections present increasingly involved attempts to utilize the structure of tree to produce useful new features. 
1 	first a t t e m p t s at effective constraint 
one scheme for construction is to form exactly one feature for each positive branch in tree. each branch produces a conjunction  e.g.  figure 1 would give three conjunctions  one of which is this sort of construction is not very useful. 
　a more interesting technique is to retain the branch oriented approach but to restrict the conjunction to a binary operation. this restriction effectively allows multiple conjunction  but only after further iteration of the sbl procedure. on a variety of problem domains  matheus  tried three choices of operands for binary 

figure 1: binary operations used by fc. 
conjunction: the two nodes at the root of a branch  the two at the fringe  the leaf and its predecessor  pagallo  1    and any two adjacent nodes. the best accuracy was usually attained using the fringe method  although in some cases adjacent had slightly better accuracy  but poorer efficiency . 
1 	f r i n g e 
pagallo  1; 1  and pagallo and haussler  reported results using a different set of schemes  but again the best choice was fringe  fig. 1 .  pagallo applies this name to her entire algorithm analogous to cl; we use fringe to denote the feature construction scheme only.  construction proceeds by conjoining the parent and grandparent nodes of all positive fringes in the tree; the position of the leaf node relative to the parent and grandparent determines whether the parent and grandparent occur negated in the conjunction  fig. 1 . 
　fringe alleviates the replication problem. in her analysis  pagallo  noted that even a simple boolean concept such as causes replication. for example if we traverse the decision tree for c in figure 1  we find  replication causes inconciseness and inaccuracy because h is divided into more disjunctive components than necessary  thus dispersing the training examples. in contrast  fringe 

symfringe  tree  new = nil for every leaf at depth  1 in tree 
feature = conjoin  leaf  
　　new = new + feature return  new  
figure 1: symfringe  symmetric fringe . 
dcfringe tree  
new = nil for every leaf at depth 1 in tree if leaf is a positive leaf then 
if  sibling of leaf is a negative leaf  and 
 parent's sibling of leaf is a positive leaf  then 
　　feature = disjoin leaf  else 
feature - conjoin leaf  
　　new = new + feature return new;  
figure 1: dcfringe  improved feature construction. 
prevents dispersion by constructing conjunctive features such as  which tend to be selected early in another round of sbl. 
　pagallo  demonstrated the value of fringe for several dnf expressions  most of which used 1 to 1 attributes with 1 to 1 terms of length 1 to 1  and two of which had more terms. but she also noted limited success on parity concepts  and anticipated problems with cnf-type concepts. cnf-type refers to concepts whose cnf representations are more compact than the corresponding dnf-type concept expression. 
1 	pagallo's improvements to f r i n g e 
pagallo  proposed that the cnf problem can be attacked with a dual heuristic for negative leaves  using disjunction instead of conjunction. later  pagallo  implemented an algorithm symmetric fringe that combines the constructions of fringe and dual fringe. 
　figure 1 shows the symmetric version  whose name we abbreviate to symfringe. technically  symfringe differs from symmetric fringe in that symfringe performs conjunction for both positive and negative leaves. however  the two versions produce the same features  modulo negation. conjunction of positive leaves in symfringe is identical to the fringe component of symmetric fringe; conjunction of negative leaves in symfringe is equivalent to negation of disjunctions in the dual fringe component. pagallo  showed that dual fringe and symmetric fringe give better accuracy on two difficult cnf-type concepts that have more than a thousand terms when expressed in dnf form. 
1 	an i m p r o v e d construction m e t h o d 
shown in figure 1  dcfringe is like symfringe except that dcfringe guides construction using more detailed properties of the tree output by sbl. whereas symfringe chooses disjunction or conjunction according to 

figure 1: patterns near the fringe of a decision tree. right  left  branch implies feature tests true  false . 
the sign of a single leaf node  dcfringe makes its choice based on additional information. to understand the difference between symfringe and dcfringe  consider first a simpler relationship between dcfringe and the original fringe. 
　figure 1 depicts all possible patterns that can occur near the fringe of a binary decision tree. the crucial observation is that patterns reveal useful ways to combine features. although both fringe and dcfringe use this observation  dcfringe considers more of the context in which the pattern occurs before deciding which feature to construct. fringe performs the construction shown in figure 1a regardless of the node types of the sibling and the parent's sibling. in contrast  dcfringe constructs a disjunctive feature when the sibling is a leaf and the parent's sibling is a positive leaf  i.e.  for patterns typical of trees representing cnf-type concepts as depicted in figure 1b. otherwise dcfringe performs the same conjunction operation as fringe. the signs within the new conjunction or disjunction depend on whether the current node lies in the left  false  branch or the right  true  branch of its parent node  and also the relative position of the parent node to the grandparent node  fig. 1 . 
　compared with dcfringe  symfringe is less selective because it forms all conjunctions and disjunctions  regardless of the tree structure. symfringe produces more features  which can proliferate in multiple iterations. 
1 	summary and discussion of methods 
all our fc algorithms perform the binary operations of conjunction and/or disjunction  applying them to nodes in the decision tree output by sbl. the more interesting implementations are summarized in table 1. matheus'  results demonstrate another interesting point. in addition to root and adjacent  he also tested heuristics such as root-fringe  which is the combination of root and fringe. the features generated by these variations have the following relationships: fringe  root root-fringe adjacent. intuitively  adjacent should be most accurate since it generates a superset of the other heuristics' features. however  matheus' results are not consistent with this intuition. for most of the cases  adjacent is inferior to fringe. unnecessary features confuse sbl evaluation and cause overfitting  which sug-
yang  rendell  and blix 

table 1: four binary feature construction schemes. 
name construction 
method application condition s  adjacent conjunction anywhere in branch nodes adjacent on branch fringe conjunction at leaves positive leaves only symfringe conjunction positive and negative leaves dcfringe conjunction or disjunction choice depends on pattern gests that since dcfringe  symfringe  dcfringe may work better than symfringe. in section 1 we present and analyze experiments on this and other relationships among f r i n g e   symfringe and dcfringe. 
1 experimental results and analysis 
we compare the behavior of three feature construction algorithms  by varying size of training sample and type of concept. we also discuss the strengths and weaknesses of the approaches. 
1 	e x p e r i m e n t a l d e s i g n 
the three systems  f r i n g e   symfringe  and dcfringe  were run on 1 randomly generated concepts over 1 attributes. more precisely  1 concepts each were generated from 1 pre-determined classes  each class a fourdimensional boolean choice. the boolean dimensions were d n f versus c n f   monotone versus non-monotone  m versus non-m  and 1 versus an expression is monotone if all its literals are positive. a concept is m-
d n f  if each attribute occurs in at most one term  clause  of its d n f  cnf  expression  ehrenfeucht et ai  1 . 
   for each of the 1 target concepts  n training examples and 1 testing examples were generated such that no two examples were the same. the values of n were determined empirically to be within the range where dcfringe is most effective  yang  1l ; this gave values of n = 1  1  and 1 the underlying sbl system was p l s 1   which performs much like id1  rendell and cho  1 . although their splitting criteria differ  this factor has been shown to have a minor effect on the behavior of a decision tree learner  breiman et a/.  1; mingers  1; rendell and cho  1 . 
   in the following three sections we present and discuss results from our experiments with the three feature construction schemes. first we compare their overall performance; the criteria are predictive accuracy  learning efficiency  and tree conciseness. then we investigate the 
1
    we use the notation k/l to signify a dnf expression having exactly k terms of exactly i literals  or a cnf expression having k clauses of / literals. note that ' is equivalent t o non-   a n d i s equivalent t o non   1
　　in continuing experiments  the size of the training sample is being varied over a wider range. 
table 1: behavior of three construction algorithms. 

effect of training set size on accuracy improvement. finally  we show how learning behavior depends on concept //-ness  which leads to a discussion of the limitations of the fc algorithms studied. 
1 	g e n e r a l u t i l i t y o f t h e d i f f e r e n t a l g o r i t h m s 
table 1 provides a comparison of the overall performance of each algorithm. values are averaged over all 1 runs for 1 training data. each entry represents a 1% confidence interval for the corresponding algorithm. the first row shows final predictive accuracy  separate test sample  after convergence  several rounds of sbl and fc . the second row gives the difference between this value and the basic sbl algorithm. the third row indicates the tree conciseness as measured by the number of leaves in the final tree. the fourth and fifth rows indicate learning efficiency as the number of new features and the number of iterations before convergence. 
   f r i n g e generates conjunctive features only  and is incapable of learning cnf-type concepts  yang  1l . since half of the 1 target concepts are cnf-type  
f r i n g e cannot perform well on average. symfringe and dcfringe generate appropriate disjunctive features in addition to conjunctive features. they both perform better than f r i n g e . dcfringe is even more accurate than symfringe  the t value for improvement in accuracy is 1  which implies a confidence level of 1% . 
   in terms of efficiency  dcfringe is better than symfringe. dcfringe generates significantly fewer features than symfringe and uses almost the same number of iterations. f r i n g e sacrifices accuracy and conciseness to be faster than dcfringe. 
   in terms of conciseness  symfringe and dcfringe are significantly better than f r i n g e . dcfringe and symfringe have the smallest final trees. a reduction in tree size not only facilitates human comprehensibility  but also provides better statistical support for splitting decisions since the sample size within a node increases. 
1 	a c c u r a c y i m p r o v e m e n t vs. s a m p l e size 
table 1 shows how training set size influences accuracy for each algorithm. here we consider the 1 m concepts only. the results for f r i n g e are retained mainly as a baseline for comparison. the t values listed in the last column compare dcfringe with symfringe. the entries under the algorithm names show the 1% confidence interval for predictive accuracy improvement beyond the basic sbl algorithm. 
   this table indicates that dcfringe significantly outperforms symfringe in terms of accuracy. dcfringe 

table 1: variation of accuracy improvement with sample 

extended feature construction procedure fc/1. 
using the sbl tree  and possibly knowledge  create 


has consistently better accuracy across training set size.  the t value of 1 in the second row corresponds to a significance level of about 1%  and the consistent trend substantiates the claim further.  
   the extra features generated by symfringe confuse the feature selection mechanism in the underlying sbl system. this effect and possible overfit cause symfringe to perform less well than might be expected. 
1 	a c c u r a c y vs. c o n c e p t d i f f i c u l t y 
figure 1 shows the accuracy improvement obtained by dcfringe and symfringe  as a function of concept / i ness. dcfringe attains significantly better accuracy im-
	no. of training examples 	no. of training examples 
figure 1: accuracy improvement with symfringe and dcfringe for concepts  a  and concepts  b  
provement than symfringe for all training sets on /x concepts  fig. 1a . but the tendencies are less clear for non-fi concepts  fig. 1b . the superiority of dcfringe over symfringe has vanished  and the relationship even seems to reverse  although not significantly. 
   our conjecture to explain this phenomenon is that hard concepts have degrees of concept difficulty. from the perspective of the decision tree output of sbl  the replication problem is just one manifestation of difficulty. when concepts are  the diffi-
culty is restricted in two ways. first  the number of literals in a w concept is limited to the dimensionality of the instance space  whereas concepts become more difficult for sbl roughly as the number of literals  attribute occurrences  increases  ehrenfeucht et a/.  1; rendell and seshu  1 . second  our preliminary experiments indicate that concepts often have replications  whereas non- concepts exhibit less replication  although they have obscure patterns of attribute duplication . on non-/i concepts  dcfringe is not more 
1
　　accuracies relative to sbl are shown as percentage point differences. 
a set of patterns; 
grouping these patterns by class-membership values and other constraints  form candidate pattern classes. 
convert stronger pattern classes into new features to add to new. 
return new to cl. 
figure 1: refined construction from a decision tree. 
accurate than symfringe because the features generated by dcfringe help replications only. 
   moreover  all three fc methods we evaluated improve accuracy only marginally with concepts. this elucidates the limitation of the basic f r i n g e approach. the fc methods we tested are unlikely to generate useful features if the decision tree does not exhibit replication. 
1 discussion and future work 
our experimental analysis has raised several issues. one is the general utility of fringe-like algorithms. if they primarily help concepts  these algorithms have limited practical value. of all possible boolean concepts  the proportion of concepts is large  and increases with the number of attributes. in hard practical problems  concepts  or their non-boolean analogues  often occur since low-level primitive attributes tend to participate repeatedly in many high-level features. to understand more precisely when fringe-like algorithms work  we need to address the problem of characterizing concepts. although concept characterization is itself problematic  ragavan and rendell  1   the basic intuition is that concepts become more difficult as the numbers of literals and terms increase  ehrenfeucht et al.  1   i.e.  as feature interaction worsens  rendell and seshu  1 . 
   given a suitable measure of concept difficulty for sbl  and after using that measure to ascertain the limits of current feature construction  we anticipate several possible improvements to fc. for non-m concepts  the tree regularities may be more complex than the replications enjoyed by dcfringe  requiring more subtle feature construction. figure 1 outlines  an extended version of fc  for boolean and non-boolean problems for which we may have some knowledge or hunches. 
	analogous 	to 	the 	conjunctive 	construction 	of fc  
step 1 of fc/1 forms conjunctions as patterns. one way to obtain patterns is to conjoin attributes that occur frequently in multiple branches of the tree: a candidate pattern is favored if its attributes appear in many branches. a second way to decide good patterns may be combined with the first: given a pattern proposed by the branch popularity method  one or more conjuncts may be dropped from or added to that initial pattern for various reasons. one reason for dropping a conjunct is to respond to overfit. another reason for dropping or adding specific conjuncts is to match available knowledge or patterns from other branches  to make a more 
yang  rendell  and blix 

coherent set of patterns. 
   for hard problems with much feature interaction  correct patterns are difficult to determine because greedy sbl is limited even when aided by fringe-like algorithms  pagallo  1 . we need to study the tradeoffs between relaxation of sbl greediness  breiman et al.  1  chapter 1  and overall learning behavior. 
   step 1 of fc/1 forms pattern classes  which are potential disjunctions. a pattern class is a set of similar patterns. t w o patterns are similar if their class-membership values are alike  and if they share some syntactic or semantic commonality to improve their coherence  i.e.  unifying principle  smith and medin  1l  . in realworld problems  pieces of knowledge are often available that can help form pattern classes  matheus  1; rendell and seshu  1 . 
   especially when little knowledge is available for hard problems  fc/1 will encounter extreme problems involving large numbers of patterns and pattern classes. since large numbers of features aggravate problems of overfit and evaluation  we need sensitive measures of feature utility  ragavan and rendell  1 . step 1 of fc/1 should create new features only after they attain credibility in terms of support from data  pattern coherence  and general and specific knowledge. we also need effective means to combine multiple measures of support  gunsch  1 . 
acknowledgments 
discussions with tom dietterich  chris matheus  eduardo perez  harish ragavan  raj seshu  and larry watanabe contributed to this paper. 
