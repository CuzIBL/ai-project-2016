interactive spoken simulation control and conversational tutoring 
karl schultz  brady clark  elizabeth owen bratt  stanley peters  
heather pon-barry  pucktada treeratpituk  zack thomsen-gray 
center for the study of language and information - stanford university stanford  california 1 
{schultzk bzack ebratt peters ponbany pucktada ztgray} csli.stanford.edu 
david c willdns  david m. fried  eugene grois 
beckman institute - university of illinois 
urbana  illinois 1 
{dcw fried e-grois} uiuc.edu 

introduction 
we describe how spoken dialogue interfaces make a simulation-based trainer and an intelligent tutoring system into a powerful package for naval damage control education. an advanced simulator gives a student real-time experience that would otherwise be extremely costly to provide. the dialogue interface to the simulator uses a simple finite-state script to allow the user to issue orders and requests in a realistic way. subsequently  the reflective tutor communicates to the student about their experience with the simulator entirely through a 'conversationally intelligent* dialogue system  with capabilities like topic management and coordination of multi-modal input and output. 
dialogue systems 
dialogue systems can range from very simple finite-state scripts defining the bounds of interaction  to complicated plan-oriented agents that can have very complex conversational intelligence  allen 1 . the two systems discussed below are among the extremes  one being very simple and the other containing more advanced forms of conversational intelligence  but both serving their purposes with a good degree of success. 
dc-train 1 dialogue manager 
the shipboard damage control trainer  or dc-train  is a simulator of damage control aboard large navy ships. there is a physical simulator of fire  flood  and smoke  as well as simulated shipboard damage control personnel. a student is placed in front of the simulator and presented with one or more damage control crises to solve. this involves giving orders to the various simulated agents  which should occur verbally not only for training purposes  
intelligent systems demonstrations but also to recreate the stressful nature of the job as accurately as possible. 
　to handle such interaction  it was possible to simulate the command-receiving agents using the simplest kind of dialogue system  a finite-state scripted system. 
furthermore  only 1 states are needed: the agent is ready to receive a new command  the agent has an incomplete message and is awaiting a missing value s   or the agent is actually issuing the command to the personnel that will carry it out. 
　the emphasis for the spoken interface to the simulator is on providing a realistic experience. other benefits include not having to learn locations and organizations of simulator graphical menus when the spoken commands are already familiar  and gaining the speed of verbal interaction. we use clarification sub-dialogues to provide a way to address speech recognition problems  and to allow for building up complex commands incrementally. 
scot dialogue manager 
the spoken conversational tutor  scot  is on the opposite end of dialogue-supporting capabilities from the simple finite-state script used in dc-train. there is a more robust conversational intelligence  ci  at work in scot's dialogue manager which is necessary to facilitate a prolonged conversation about damage control doctrine. since the utterances here are not simply mapped into commands and parameters the dialogue structure becomes much more important. topic management and coordination of multi-modal input and output  gestures  are among the improvements over a simple finite-state script to reach a more human-like level of conversation. 
　an annotated record of the student's performance with the dc-train simulator is fed into scot  from which an initial plan is created. this plan is made by the tutoring component of scot  which is completely separate from the dialogue manager side. this extra 'planner' is one of the necessities with ci of topics and goals. the dialogue 
1 
manager will only have limited abilities to manipulate the plan and attach a user utterance to the relevant place. it understands that there are topics and sub-topics  but the dialogue manager obviously has no knowledge of what the tutorial goals are or how they relate to each other. dealing with these concerns is the job of an outside agent the separation of the ci from the tutoring knowledge allows the dialogue manager to be generic and usable for any domain  provided an outside agent contains the semantic understanding of the goals and topics. 
　the ability to reshape the current and future dialogue threads is a major advantage over a finite-state model. it creates an interactive style which comes much closer to humans  where topics can explored in more or less detail  or put aside for later  new topics can spring up at whim and old topics can never be brought back if it becomes unnecessary. the tutor agent monitors the conversation and makes these changes as it sees fit to suit the overall tutoring strategies for teaching and specific tactics applied to get points across. 
　this dialogue manager can also handle creating gestural output to the user and timing that with the speech  as well as interpreting gestural input from the user  in the form of mouse clicks . gestures are a large part of typical humanto-human interaction  clark 1   and the more the student is able to interact with a common workspace which scot shares  the more natural-feeling and effective the interaction will be. currently the student can only point at designated times  but ideally all mouse movement would be analyzed for pointing  just as all hand movements in conversation could be looked at for gestural meaning. 
voice interaction 
of the major technical difficulties to overcome in creating a robust dialogue system the input and output is obviously one of the more pertinent. the concerns of good speech recognition are one of the limiting factors to how much variance in input is allowed. the audio side of speech output is somewhat less restrictive  but that varies based on how realistic the voice needs to be. however  both the input and output are governed by an underlying grammar which gives the system the ability to intelligently parse user input and create complex output using semantic constructs  or logical forms  lfs   rather than using canned phrases or sentences. 
gemini 
the gemini nlp system  dowding et al. 1  uses a single unification grammar both for parsing strings of words into logical forms  lfs  and generating sentences from lf inputs. this enables us to give precise and reliable meaning representations which allow us to identify the discourse move types  e.g.  a question  given a linguistic input or output; e.g.  the question  what happened next   has the lf:  ask wh  past happen    . 
nuance 
the nuance speech recognition server takes a user utterance and a recognition model  which we compile directly from a gemini grammar  and attempts to turn the utterance into text. the grammar plays a key role in defining the bias of the recognizer towards words and phrases within the current domain  as well as assuring that every recognized utterance has a corresponding lf. limiting what a user can say has the bonus of significantly better recognition of expected utterances  but has the undesired effect of sometimes turning out-of-grammar phrases into in-grammar phrases  which would obviously cause problems. this is where a well engineered grammar plays an important role. 
festival and festvox 
the festival text-to-speech system turns any text into audio output. how usable a speech-enabled system will be depends highly on how understandable the output is. there are a variety of voices to choose from when using festival  but they are 'computer-sounding' and lack the clarity and subtle inflections of a real human voice. one solution to this is to use the festvox add-in to festival  which allows one to create a voice which sounds exactly like the person who creates it. this process involves recording a large number of phrases covering the range of words in the desired domain. these recordings are then analyzed along with the corresponding text  and a voice is compiled which uses the discovered human-sounding words to generate the audio output. 
acknowledgments 
this work is supported by the department of the navy under research grant n1  a multidisciplinary university research initiative on natural language interaction with intelligent tutoring systems. 
