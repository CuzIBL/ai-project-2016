 
in this paper we discuss poligon  a skeletal system for the development of concurrent blackboard based applications  its architecture and the motivation for its design. a number of experiments have been performed in order to evaluate the performance of poligon. some of these are detailed and the results are shown. lessons learned in the development of poligon are given and conclusions about the performance of similar systems are drawn. 
1. 	introduction 
it is often said that future ai applications will make significantly greater computational demands than the present generation. the advanced architectures project of stanford university's heuristic programming project  rice  1b  is investigating this issue  since it has as its objective achieving computational speed-up for expert systems through the use of parallel hardware and new  advanced software architectures. this requires the development of everything from designs for parallel hardware  which might be appropriate for the execution of future symbolic programs  through operating system and language concepts to problemsolving frameworks and eventually mounting applications on them in order to test the new designs. 
　poligon  rice  1  is one of the problem-solving frameworks developed as part of the advanced architectures project. in section 1  we discuss poligon's architecture as a design for a high-performance  concurrent blackboard system aimed particularly at the problem domain of soft real-time problems  and what motivated this design. section 1 discusses the applications mounted on the poligon framework and experiments performed on the poligon system to measure its performance. section 1 presents the results of these experiments and an interpretation of them. we conclude in section 1 with a number of the lessons we have learned in the process and pointers for future research. 
     the author gratefully acknowledges the support of the following funding agencies for this project; darpa/radc  under contract f1-c-1; nasa  under contract number ncc 1; boeing computer services  under con tract number w-1. 
parallel and distributed processing 1 . 	the poligon architecture 
in this section we briefly discuss the architecture of the 
poligon system. a more detailed description of the design rational for poligon can be found in  nii et al.  1 . because of space constraints  it will be assumed that the reader is conversant with the terminology of blackboard systems  engclmore and morgan  1   though no deep knowledge will be assumed. 
　when we started the advanced architectures project we had a hunch that the blackboard problem-solving architecture might offer a basis for the efficient exploitation of concurrent hardware. this was because the blackboard model appeared to have concurrency built into it. why this is  in fact  not the case is explained in  rice  1a . the primary reasons why the blackboard model of a collection of simultaneously cooperating experts cannot develop the parallelism that one might expect is that the blackboard model itself assumes effectively infinite bandwidth with which the experts can see any part of the blackboard that might be of interest. it also assumes that experts do not get into one another's way whilst solving the problem. in practice a knowledge source can only see a small segment of the blackboard at any one time without degrading the performance of the system unacceptably. similarly  the experts are dependent on one another  they must often wait for the results deduced by other agents and can be confused by updates being posted at unexpected times or in surprising orders. we are  however  unaware of a better architecture for concurrent problem-solving than that of blackboard systems. 
　although a number of other research efforts have looked at concurrent blackboard systems  these have concentrated primarily on either the aspects of distributed  concurrent problem-solving  such as  lesser and corkill  1  or on coarse grained parallel systems  such as  fcnnell and lesser  1  aiello  1  ensor and gabbe  1 . poligon is a finer grained system than these  directed particularly at gaining speed-up through parallel execution. 
　the normal  serial implementations of the blackboard metaphor use a scheduling mechanism to cause one rule to fire after another. in parallel systems it is crucial that the programmer eliminate serial components  since this limits 

speed-up.1 the main motivation of the poligon system was to find a way to eliminate the serializing aspects of the blackboard model. we viewed this as doing the following: 
  eliminating the scheduling mechanism and finding ways to support concurrent rule activation all across the blackboard. 
  optimizing the design for distributed-memory  message-passing hardware  which should be able to deliver the best performance for large numbers of processors  of the order of hundreds to thousands.  
  distributing the knowledge base over the blackboard so that there would be no serialization in the access to the blackboard from the executing knowledge. 
  designing the system so as to allow it to be highly com-pilable. it was clear from the outset that a considerable portion of the expense of existing ai systems is due to the fact that they are optimized for easy modification and debugging  rather than high run-time performance. the resulting system  therefore  had to be designed so as to be able to be compiled efficiently yet still be intelligible and debuggable during the development cycle. 
　as these ideas progressed we developed the notion of a blackboard consisting of active nodes  tightly associated with the knowledge relevant to them. 
　a very simple scheme was developed for invoking the knowledge that had been distributed to the blackboard nodes: rules arc activated as daemons as a result of modifications to the slots of a node  see figure 1 . 
the distributed-memory hardware model  on which the 
poligon system was to operate had the property that each processor was effectively a uniprocessor system. this 
   ' speed-up can be viewed as the ratio of the system's speed using n processors to its speed using only one. 
meant that if we viewed the blackboard with a  node as a process/processor  model then we would lose potential parallelism due to being able to execute only one piece of code  rule  at a time for any given node. 
　what we needed  therefore  was a mechanism to allow the activation of multiple rules for any given blackboard node. this caused us to develop a model of poligon which was as follows:  a blackboard node is a process on a processor  surrounded by a collection of processors able to service its requests to execute rules.  it can easily be seen that this model is very close to a distributed object system model. this is by no means a coincidence. the underlying hardware system on which poligon was implemented was a concurrent  distributed object oriented system  delagi et al.  1 . 
　the model expressed above is not without problems. in order to minimize the probability of a node being locked for a long period  which would delay remote access to it  as much processing is done in the remote rule invocations as possible1. this means that  when the rules execute  they have to do so in the context of a snap-shot of the solution state as it was when the rule was invoked  see figure 1 . remote reads to other nodes  even the invoking node  are expensive and one cannot guarantee that things haven't changed by the time that the result of the read has been returned. 

　this led to the development of the idea of a poligon node as being an agent capable of evaluating its own performance. mechanisms had to be included so as to allow the system to be able to assess any request to modify its local state and to decide whether to perform the update  or what else to do instead  on the basis of its own view of how it is progressing towards its goal of solving the problem. 
1 . 	experiments on poligon 
in this section we briefly describe the experiments performed on the poligon system to date. two applications have been mounted on poligon: elint  a soft real-time situation assessment problem and parable  a diagnostic application for particle accelerator beam-lines  selig  1. the experiments with the elint application have now been completed  whereas those on the parable system are in their infancy  so only the elint application will be considered here. a more detailed treatment of the elint experiments can be found in  niietal.  1 . 
　the elint application encodes knowledge used to interpret the radar emissions made by planes that are received by ground-based tracking stations distributed across the country. because these tracking sites are passive devices  they can only detect the bearing and spectral characteristics of the radar emissions. between them  it is their responsibility to deduce a position  course  identity and intention for any aircraft traveling through the monitored airspace. the elint application simulates a central machine that integrates reports from these detection sites in order to achieve the overall goals mentioned. 
the important characteristics of the elint problem were: 
  a continuous stream of input data. 
  no a priori knowledge of the behavior or number of the aircraft being tracked. 
  the need to emit periodic reports capturing the system's evolving view of the solution. 
　the elint problem was chosen both because it was nontrivial and was in a class of problems  for which blackboard systems had already been used  and also because it was hoped that parallelism would be readily available. it was anticipated that parallelism could be extracted from the concurrent execution of knowledge on any given part of the solution space and from the potentially large number of independent elements in that solution space  i.e. aircraft. 
　the application was taken from a serial implementation and was not restructured so as to be better suited for parallel execution. the blackboard was  however  composed of three distinct layers in the abstraction hierarchy. data flowing from one level to the next allowed pipes to be formed that were three stages long. 
　perhaps the most important lesson that we learned from performing these experiments was to find a way to measure the relative performance of concurrent real-time systems. the best way that we found to do this was to pump data into the system at a given rate  which was under the control of the user  and examine the system's output over time. there is a measurable time between the time that data comes into the system and the time that any associated reports come out of the system. if this time difference increases on average over the course of a run then the system was not able to 
parallel and distributed processing 
keep up with the rate at which data was being pumped into it. the experiment was then performed again with the data rate turned down until the report latency did not increase. this gave us a measure for the system's throughput  which we took to denote its peak performance. 
　the experiments that were performed were intended to measure a number of different aspects of the system's performance: 
  the speed-up that the poligon system could deliver. 
  the peak throughput of the system. 
  the ability of the system to exploit large knowledge bases. 
  the granularity of the system. 
experiments to measure these are described in section 1. 
1. 	experimental results 
the space available for this paper does not allow a full explanation of the experimental results  so the interested reader is again advised to refer to  nii et al.  1  for more details. it is hoped that the treatment here will be sufficient to give the gist of what we have learned. 
　it should be noted here that wherever reference is made to absolute times  these are measured in terms of the performance of the simulated hardware on which the poligon system runs  delagi  1 . each processing element of this machine is of about the performance of a ti explorer tm  ii+ processor. 

1 . 1 . measurement of speed-up and throughput 
　in this experiment two different data sets were used. one was designed to allow the poligon system only to create one pipe in the solution space  the second allows poligon to 
explorer is a trade mark of texas instruments corpora-
t i o n . 

create four pipe-lines; it was four times as dense1. the combination of these two results allows us to do the following: 
  measure the peak throughput for the larger data set. 
  determine the contribution to speed-up due simply to pipe-line parallelism. 
  compare the results from the two data sets so as to be able to get a measure of the ability of the system to exploit parallelism in the source data  i.e. data parallelism. 
the results from the two data sets are shown in figure 1. in this experiment we learned the following: 
  the peak speed-up shown in this application due to pipe-line parallelism was 1. this showed that although the length of the pipe was three  speed-up was greater than three because of the concurrent execution of rules by the different stages of the pipe. 
  the peak throughputs measured from the two data sets were not significantly different. this indicates that poligon was able to achieve an almost linear increase in speed-up as the problem size of the data set increased  an important result. 
  the peak throughput for the system as measured from the larger data set was about 1us per signal data record. because of the linear increase in performance with data set complexity it is assumed that with more complex problems higher performance could be achieved. by comparison the elint application  when coded to run in the age blackboard system took about 1 seconds to process each piece of signal data. 

number of rules 
figure 1. a graph showing application throughput slow-down plotted against the number of rules being fired for each rule-invoking event 
　1. measurement of poligon's ability to exploit large knowledge bases 
　in this experiment the poligon system was tested using the small data set used above. the poligon framework was modified so that  whenever a rule was invoked  n rules would be invoked  rather than just one. n -1 of these rules had the special characteristic that they performed almost all of the processing required except for any blackboard modifying updates. this gave a measure of the system load if the knowledge base was n times larger  whilst still giving the right behavior for this application. 
the results from this experiment are shown in figure 1. 
　in this experiment  if the system were able to exploit parallelism in the knowledge base to the full  one would expect that the system would not slow down at all as new knowledge was added  i.e. the line shown in figure 1 would be horizontal. if  on the other hand  the system bogged down completely as more knowledge was added one would expect that the result would be worse than linear slow-down  that is the plot would appear above the  linear slow-down  line. as can be seen easily from the graph  poligon's performance was better than linear. in order to perform four times as much work it took only 1 times as long. this means that  as long as there are sufficient computational resources  the poligon system delivers good performance for a knowledge base whose size is at least up to four times that of the 
elint applications 
　1. measurement of the granularity of poligon's rules 
　in this experiment some of the internal mechanisms within poligon were timed in order to get some empirical measure of the granularity of the system. 
　within a blackboard system a number of mechanisms are of crucial importance to the performance of the system. amongst these arc slot reads  slot writes and rule invoca-
tion.1 
　in order to determine the costs of these operations they were performed repeatedly in a manner which allowed the individual costs to be measured with some precision. 
　the results of these experiments are as follows. it should be noted that all of these results neglect any communication overhead  so they are only representative for local operations. 
  slot reads take 1 + 1n us  where n is the number of slots being read at once  poligon supports a form of multiple slot read operation. 
  slot updates lake 1 + 1.in us  where n is the number of slots being written. poligon allows arbitrary user code to be executed during the slot update operation  so this is 

a representative figure taken from the elint application. this is for the case of no rules being associated with the slots being updated. 

  the overhead cost of starting up a rule's execution is about 1ms per rule invoked. 
　a substantial part of the time taken performing these operations could be optimized considerably. for instance  a figure of about 1us for rule invocation could relatively easily be achieved in a real system and more than this improvement could be expected for a system which allowed specialized microcode or similar efficiency tuning. this shows that there is a lower bound to the granularity that the user can expect to achieve. for computations taking less than a few milliseconds it may not be worth starting up a rule to perform the computation  the cost of parallel execution would be in excess of the serial execution time. 
1 . 	what we have learned 
we have learned a number of lessons from this project  some of which were counter to our intuitions. 
  our intuition told us that programming a concurrent blackboard system would not be too hard because of the assumed implicit asynchrony in serial blackboard systems. we found this not to be the case. we found the programming task to be difficult and  we believe  a reconccptualization of existing problems will be required in order to port them for efficient parallel execution. the difficulty of implementation of applications is due largely to the divergence of implementations of serial blackboard systems from the pure blackboard model in order to make implementation and programming more manageable as was mentioned in section 1 and is covered more thoroughly in  rice 1a . 
  we found that the poligon system and architecture itself performed fairly well. although programming the system was not trivial  the poligon system provided a useful abstraction model that allowed the development of an application in a blackboard-like manner that still gave the correct answers and acceptable performance. 
  we had thought that parallelism in the knowledge base would be crucial to the achievement of high performance. in the applications that we used  knowledge proved to be sparse and the pipe-line parallelism that resulted from it delivered only a factor of three in speed-up. the small amount of speed-up from pipe-line parallelism was due to the short length of the pipes  the lack of applicable knowledge and the difficulty in balancing the pipes. most of the parallelism seen in the applications implemented in the advanced.architectures project was derived from the data  not the code. the limit to the length of the pipes derived from the application was not one that resulted from the structure of the problem itself  but rather came from the fact that the application was reimplcmcntcd for poligon from the age implementation  not reformulated. 
  when we started the project our intuition told us that the significantly greater cost of communication relative to computation would bias the programmer in favor of doing as much as possible locally before a message was sent. it turned out that doing this increased the granularity of the system and restricted parallelism. we found that  although communication is expensive  as long as data keeps flowing along a pipe the price that is payed is in latency  not in speed-up. the fact that processes are not held up 
parallel and distributed processing 
by communication is a result of the non-blocking message sending ability of the hardware. thus  fine-grained systems are likely to be significant for achieving good performance from large multiprocessors but the increased latency due to distributing the work could have an adverse effect on real-time performance. 
  we learned that simulation of multiprocessors is expen-sive. a number of projects are interested simply in the difficulties caused by the asynchronous behavior of concurrent systems. such projects are able to use a simple model for their implementations on existing hardware. we  on the other hand  wanted to measure the performance of our software on the hardware we were developing precisely in order to refine both our hardware and software designs. this is computationally a very expensive task and has proved to be a major limiting factor on the work that we have done. having said this  however  it should be noted that we arc confident that we have achieved better results and have gained deeper insights than we would have done if we had concentrated on building real hardware. 
  resource allocation was found to be a significant factor in delivering high performance. the fact that blackboard nodes are often very long-lived means that an even load balance can easily be disrupted by a few busy nodes. in the experiments reported here the allocation of processes to processors was done randomly. other experiments in the advanced architectures project have shown that  compared to the ideal  perfect load balanced state1  even with very careful site allocation the elint application lost 1% in efficiency and delivered 1% less speed-up than in the perfectly load balanced case. this could not be recovered through the use of more processors |dclagi and saraiya  1 . 
1. 	conclusions 
in this paper we have described poligon  a blackboard framework designed to operate on distributed-memory multiprocessors. we have described experiments performed on it  shown the results and discussed the conclusions that can be drawn from them and mentioned some lessons that were learned along the way. 
　we have shown that the poligon system can deliver a speed-up for the elint application of nearly twelve  with near linear speed-up gain with increasing problem complexity. we have also shown significantly better than linear slowdown as a result of increasing knowledge base complexity. we are confident  therefore  that given a larger problem poligon could deliver significantly more speed-up than this. 
　from our work we can conclude that data parallelism is likely to be the most important source of parallelism in the foreseeable future  at least until truly huge knowledge bases are developed. this requires that concurrent problem-solving systems should be not only able to exploit data parallelism but be able to do so in a manner which allows the rapid development  easy maintenance and modification of knowledge bases and encourages the development of software that is not brittle when knowledge is added or removed or when the system meets circumstances that were not anticipated by the programmer. poligon is a possible first step in this direction. 
