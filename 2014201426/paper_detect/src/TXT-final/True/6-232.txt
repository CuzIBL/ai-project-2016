 
ensemble learning schemes such as adaboost and bagging enhance the performance of a single classifier by combining predictions from multiple classifiers of the same type. the predictions from an ensemble of diverse classifiers can be combined in related ways  e.g. by voting or simply by selecting the best classifier via cross-validation - a technique widely used in machine learning. however  since no ensemble scheme is always the best choice  a deeper insight into the structure of meaningful approaches to combine predictions is needed to achieve further progress. in this paper we offer an operational reformulation of common ensemble learning schemes - voting  selection by crossvalidation  x-val   grading and bagging - as a stacking scheme with appropriate parameter settings. thus  from a theoretical point of view all these schemes can be reduced to stacking with an appropriate combination method. this result is an important step towards a general theoretical framework for the field of ensemble learning. 
1 	introduction 
we apologize that space restrictions arc forcing us to be terse. 
for a more detailed version  see  seewald  1 . we will first explain how stacking works in order to lay the foundations for our functional definitions of meta classifiers later on. 
fig. 1 shows stacking on a hypothetical dataset. 
　during training  all base classifiers arc evaluated via crossvalidation on the original dataset. each classifier's output is a class probability distribution for every example  see fig. 1 b  the concatenated class probability distributions of all base classifiers  followed by the class value  forms the meta-level training set for stacking's meta classifier  see fig. 1 c  after training the meta classifier  the base classifiers are retrained on the complete training data. 
　for testing  the base classifiers are queried for their class probability distributions. these form a meta-example for the meta classifier which outputs the final class prediction. 
1 	definitions 
an arbitrary training dataset with n examples and k classes  and a single test instance  is given. n arbitrary base classifiers have been cross-validated on this dataset  and afterwards retrained on the whole dataset. all base classifiers output class probability distributions  i.e. estimated probabilities for each class instead of deciding on a single class. 
　then   refers to the probability given by classifier i for class j on example number k during is the class probability distribution of classifier i on instance k. refers to 

figure 1: illustration of stacking on a dataset with three classes 
 a  b and c   n examples and tv base classifiers.  refers to the probability given by classifier i for class j on example number k. 
the class prob.dist. for classifier i on the unknown test instance. a fixed arbitrary order on the class values and n base classifiers is assumed. classk is the true class for instance k.  is the attribute vector of instance k. n is the num-
ber of instances. all indices are zero-based  e.g. the instance id k satisfies the equation 1 	k 
as we mentioned  we assume that all ensemble learning schemes return class probability distributions. if predictions are needed  the position of the maximum class probability in the vector - i.e. the predicted class - is easily obtained via formula  1  trivially  stacking with predictions can also be simulated by this variant; simply by transforming the class distributions meta-dataset to predictions via  1  prior to applying the meta classifier. 
　we can now characterize every ensemble learning scheme by what features it extracts from the meta-dataset during training and how these features define the mapping from meta-dataset to final class probability distribution. 
1 	voting 
voting  a straight-forward extension of voting for distribution classifiers  is the simplest case. during training  no features are extracted from the meta-dataset. in fact voting does not even need the internal crossvalidation. during testing  voting determines the final class probability distribution as follows. 
		 1  

poster papers 	1 

thus  it can be easily seen that the meta-classifier defined by just computing the mean probability distribution of the base classifiers - as above - simulates voting with probability distributions. voting with predictions can be mapped similarity. in this case  we use instead of in formula  1  is the vector of p.  for all j  where 
		 1  
1 	x-val 
for x-val  which chooses the best base classifier on each fold by an internal ten-fold cv  we first determine the accuracy per classifier  which can be computed directly from the metalevel dataset  see  1  then  we derive as feature the id of the classifier with the maximum accuracy. thus  the value of bestc corresponds to our learned model. 
 1   1  
 1  
1 grading 
for grading  seewald and furnkranz  1   the case is difficult. while we cannot simulate the original grading  we will simulate a competitive variant equivalent to accuracyweighted voting. more details see  seewald  1 . 
　during training  the accuracies of base classifiers are extracted using formula  1  the accuracies of all our base classifiers correspond to our learned model. during testing  we build the combined class probability distribution similar to voting using predictions but with an additional weight namely the accuracy we extracted earlier. 
	 1  
1 bagging 
bagging  breiman  1  is another common ensemble technique. here  the same type of classifier is repeatedly trained on new datasets  which have been generated from the original dataset via random sampling with replacement. afterwards  the component classifiers are combined via simple unweighted voting. 
　thus  we use same meta-classifier as for voting with predictions. the number of base classifiers is equal to the iteration parameter of bagging - each base classifier for stacking corresponds to one instantiation of the base learner for bagging. in order to simulate the random sampling from the training set  the base learner's training sets have to be modified before training  via formula  1  
  1  
during training  formula  1  is used to create - for each base classifier separately - a training set of the same size as the original training set via random sampling from the original training set  exactly as in bagging. these training sets are then used to train the base classifiers. this approach can also be seen as a probabilistic wrapper around each base classifier. no features are extracted from the meta-level dataset during training  as for voting. 
　during testing  each base classifier gives a prediction. these predictions are then voted to yield the final prediction  exactly as for voting with predictions  i.e.  1  modified via  1  - for more details refer to subsection 1. concluding  we have shown the equivalence of bagging and stacking. 
1 	discussion & conclusion 
by definition stackingc  seewald  1   can also be mapped onto stacking via a special meta classifier. in fact  the available implementation is a specialized subclass of a common meta classifier  mlr. another recent variant  smm1  dzeroski and zenko  1   is also implemented via a special meta classifier and thus amenable to the same kind of mapping. however  adaboost  freund and schapire  1  cannot be simulated by stacking because of its sequential nature. 
　while the given formal definitions of meta classifiers are mainly intended to enable further theoretical work  a direct implementation is also feasible. the cost penalty for the simulation is small  since training the meta classifiers usually contributes little to the total runtime. 
　concluding  we have shown that stacking is equivalent to common ensemble learning schemes  namely selection by crossvalidation  x-val   voting of either class probability distributions or predictions  a competitive variant of grading  and bagging. recent variants such as stackingc  seewald  1  and smm1  dzeroski and zenko  1  arc also equivalent. thus we conclude that our approach offers a unique viewpoint on stacking which is an important step towards a theoretical framework for ensemble learning. 
acknowledgements this research is supported by the austrian fonds zur forderung der wissenschafdichen forschung  fwf  under grant no. p1-inf. the austrian research institute for artificial intelligence is supported by the austrian federal ministry of education  science and culture. 
