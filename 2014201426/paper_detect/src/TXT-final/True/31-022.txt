 
the residue-driven architecture presented here is a model of auditory stream segregation from input sounds. a subsystem to extract auditory streams by using some sound attributes is called an agency and the design of each agency is based on the residue-driven architecture. this architecture consists of three kinds of agents: an event-detector  a tracergenerator  and tracers. the event-detector calculates a residue by subtracting the predicted input from the actual input. when a residue exceeds a threshold value  tracer-generator generates a tracerthat extracts an auditory stream from the residue and returns a predicted input of the next time frame to the event-detector. this approach improves the performance of segregation and the resulting system can segregate a woman's voiced stream  a man's voiced stream  and a noise stream from a mixture of these sounds. binaural segregation is also designed by the architecture. 
1 introduction - what is computational auditory scene analysis  
　ai research on the understanding of sounds has a rich history dating back to the arpa speech understanding project in the 1's. while a great deal has been learned  systems that can understand general acoustic signals  e.g. voiced speech  music and/or other sounds  from real-world environments have not been built. there are systems that understand clean speech well in relatively noiseless laboratory environments but that cannot in more realistic  noisier environments. at a crowded party  one can attend one conversation and then switch to another. this phenomenon is known as the cocktailparty effect and it shows that humans can selectively attend to sound from a particular source even when it is mixed with other sounds. computers also need to decide which parts of a mixed acoustic signal are relevant to a particular purpose - which part should be interpreted as speech  for example  and which should be interpreted as a door closing  an air conditioner humming  or another person interrupting. 
　a number of researchers have therefore concluded that research on speech understanding and on nonspeech understanding need to be unified within a general framework. one such framework is suggested by bregman's book  auditory scene analysis  bregman  1   which discusses the psycoacoustic aspects. this work has inspired a number of attempts to model what is known about the human auditory system. it has also encouraged researchers to explore more general models of the structure of sounds in order to deal with more realistic acoustic environments. researchers have also begun trying to understand computational auditory frameworks as parts of larger perception systems whose purpose is to give a computer integrated information about the real world. to discriminate the ai and computer science approach from the psychoacoustic approach  it has been called computational auditory scene analysis  hereafter  casa   cooke et a/.  1; nakatani  okuno  and kawabata  1 . 
　research topics related to casa include modeling issues  sound understandings issues  architectural issues  control issues  representational issues and applications. they also include research on how different sensors can be integrated with models of how the human's auditory apparatus works in concert with vision and other kinds of sensation. here  we focus on system architecture based on the multi-agent paradigm. the multi-agent system was recently proposed as a new modeling technology in artificial intelligence  brooks  1; maes  1; minsky  1; okuno  1 . we take minsky's view that an agent has a limited capability  although an agent in distributed artificial intelligence is much more powerful and more like a human being than ours is. 
in this paper we present a new architecture called 
residue-driven architecture. the rest of this paper is organized as follows: section 1 discusses the related works on casa and identifies the issues. section 1 introduces the residue-driven architecture and discusses intra-agency and inter-agency interactions. section 1 presents the design and implementation of two agencies based on harmonics and localization  the direction of sound source . evaluations and the conclusions are respectively given in section 1 and 1. 
	nakatani  okuno  and kawabata 	1 
1 previous works on computational auditory scene analysis 
　one of goals in computational auditory scene analysis is to understand acoustic events  or the sources of sounds  bregman  1 . an acoustic event is represented by auditory streams  hereafter  simply streams  each of which 
is a group of acoustic components that have consistent attributes. since acoustic events are represented hierarchically  e.g.  as an orchestra   auditory streams have also a hierarchical structure. the process that segregates auditory streams from a mixture of sounds is called auditory stream segregation. since acoustic components need 
to be organized into auditory streams  the segregation of auditory streams requires the exclusive allocation of sound to a particular stream. auditory stream segregation is performed at various levels of perception. some streams are very simple and are extracted according to simple attributes  while others are extracted by grouping streams segregated at earlier stage of processing. 
　bregman proposed two mechanisms of auditory stream segregation: simultaneous  spectral  grouping  followed by sequential grouping  bregman  1 . in the simultaneous grouping  streams are extracted from a mixture of sounds; and in the sequential grouping  streams from the same acoustic event are grouped together. no algorithms of grouping for computer implementations  however  have been proposed. nakatani et al. also showed that bregman's approach failed in segregating man's and woman's voiced speech in their experiments  nakatani  okuno  and kawabata  1 . 
　in extracting acoustic attributes  some systems assume the human's auditory model of primary processing and simulate the cochlear processing  bodden  1; brown  1; slaney  naar and lyon  1 . brown and 
cooke designed and implemented the system that builds various auditory maps for input sounds and integrates them to segregate speech from input sounds  brown  1; brown and cooke  1 . an auditory map represents acoustic attributes such as onset  offset  am and fm modulations  and formants. since the integration process becomes complicated when treating a mixture of sounds in real-world environments  the blackboard architecture  erman et a/.  1  is used to simplify this integration process  cooke et a/.  1 . the algorithm building an auditory map is executed in batch in the sense that any part of the input should be available to the algorithm at any time. batch algorithms  however  are not suitable for providing a wide variety of system responses. if the system needs a reflective response  that is  it may react immediately without deliberate consideration   it cannot wait for auditory maps to be built: some fragmentary information may be enough to decide its behavior. additionally  it is not easy to incorporate schema-based segregation and grouping of streams into such a system  since it does not support a mechanism for extending capabilities. 
　to design a more flexible and expandable system  control mechanisms are needed. ipus  integrated processing and understanding signals   lesser et a/.  1  in-
tegrates signal processing and signal interpretation into the blackboard system. ipus has a small set of frontend signal processing algorithms  spas . it chooses the correct parameter setting for each spa and computes the correct interpretation by dynamic spa reconfiguration. ipus views this reconfiguration as a diagnosis of discrepancy between top-down search for spa parameter settings and bottom-up search for the correct interpretation. ipus has various interpretation knowledge sources which understand actual sounds such as hair driers  footsteps  telephone rings  fire alarms  and waterfalls. ipus may have problems in scaling up  because when the number of spas increases it may fail to choose the correct parameter settings. and to support a reflective response  another system may be needed to compute required information. 
　nakatani et al. took a multi-agent approach to auditory stream segregation  nakatani  okuno  and kawabata  1 . the hbss  harmonic-based stream segregation  system was designed and developed using a multi-agent system. it uses the fourier transformation instead of the auditory model because it is easy to implement and its properties are well known. since the hbss uses only harmonics as a cue of segregation and retains only information of the previous time frame  it extracts auditory streams incrementally. although its mechanism is simple  it can segregate two streams from a mixture of man's and woman's voiced speech. the hbss can in principle segregate any number of harmonic sounds that have no fundamental frequencies in common  but the hbss fails in scaling up the segregation because of its imperfect exclusive sound allocation caused by its poor prediction of next inputs and because of its crude mechanism for checking the consistency of streams. the main cause of these problems is that each agent does not use any temporal information about streams. spatial information may be also used to cope with the problems. another error is due to background noise. the definition of noise is relative  because a noise is simply something that cannot be classified by the focused attributes. 
　in this paper we present a new architecture to cope with the following problems of the hbss: 
 1  imperfect exclusive allocation  1  usage of temporal and spatial information 
 1  noise treatment. 
1 	residue-driven architecture 
　auditory stream segregation systems must  1  determine that streams appear   1  trace the streams   1  determine that the streams have ended  and  1  resolve interference between simultaneous streams. the residuedriven architecture  figure 1  consists of subsystems comprising three kinds of agents: an event-detector  a tracer-generator  and tracers. a subsystem extracting auditory streams by using some auditory attributes is called on agency. an agency based on the residuedriven architecture extracts streams as follows: 
 1  an event-detector subtracts a set of predicted inputs from the inputs and sends the residue to the tracer-generator and tracers. 
 1  if the residue exceeds some threshold value  a tracergenerator searches for the values of focused auditory 

attributes. if it finds appropriate values of the attributes  it generates a tracer to trace on the attributes. if it does not find such values  it generates a noise-tracer. 
 1  each tracer extracts a stream fragment by tracing the attributes of the stream. it also generates a predicted next input by adjusting the segregated stream fragment to the next input and sends this predicted input to the event-detector. 
　once a tracer-generator generates a new tracer that starts extracting a stream  the agency returns to a stable state because the residue becomes zero unless an input does not contain a new sound. when a new sound comes in  the residue becomes nonzero and a new tracer to extract the new sound is generated and the system returns to a stable state. if a tracer predicts that the next input will be zero and the actual input is zero  the tracer terminates by itself. if the tracer-generator fails to find an appropriate attribute  it considers that a noise comes in. since the agency treats unknown sounds as noise  the definition of noise is relative to each agency. there is only one event-detector and one tracer-generator and one noise-tracer  but the number of tracers changes according to the input. 
　a tracer extracts information and generates a stream fragment stream fragments are grouped into a stream. the important constraint in grouping is exclusive allocation  which means that each input fragment should be allocated to only one auditory stream. 
　interactions between agents can be classified as intraagency interactions and inter-agency interactions. intraagency interactions are performed between agents within the same agency and the main way these kinds of interactions occur is via the predicted next input. since tracers are of the same kind  predicted next input is of the same kind and it is easy to calculate a residue by using these predicted next inputs. a noise-tracer  however  differs from other tracers and thus its predicted next input should be given to each tracer and event-detector  figure 1 . another way of such an interaction is via shared variables. 
　one way inter-agency interactions occur is via the input/output relation. to model binaural hearing  for example  a pair of agencies behave like a pair of ears and each agency extracts stream fragments with spatial information. such stream fragments are given to a grouping agency  which constructs auditory streams according to spatial information. another way of intra-agency interaction is modeled by the subsumption architecture 
 brooks  1   a simple example of which is shown in figure 1. 
　agency 1 in this figure extracts streams from monaural inputs  and agency 1 extracts auditory stream fragments from binaural inputs. agency 1 extracts spatial information such as the direction of sound sources from binaural inputs. it also uses the information given by agency 1. agencies 1 and 1 construct streams by grouping stream fragments generated by agencies 1 and 1. agency 1 extracts voiced speech stream fragments by using information extracted by agencies 1 and 1. agency 1 may replace the output of agency 1 with its own output or simply suppresses the output. streams extracted by monaural processing may be replaced by those of binaural processing. the output of agency 1 may be replaced by a visual agency  e.g.  a video-camera tracking system . the reason that the grouping agency is separated from agency 1 or 1 is that it can be modeled by the residue-driven architecture  nakatani et al  1b . 

that gives the maximum et w  is selected by the tracergenerator and generates a tracer. if there is no active pitch watcher during a dormant period  the noise-tracer is activated. the current setting of constants is as follows: c = 1  p = 1  r = 1  and m = 1. 
harmonics tracer 
　a harmonics tracer gets the initial fundamental frequency from a pitch watcher when it is generated. at each residual input  each harmonics tracer extracts the fundamental frequency that maximizes the valid harmonic intensity e't w . it then calculates the intensity and the phase of each overtone by evaluating the absolute value and the phase of htk{w - it generates a predicted next input in a waveform by adjusting the phase of its overtones to the phase of the next input frame. the event-detector calculates a residue by subtracting the predicted inputs from the actual input  nakatani  okuno  and kawabata  1; ramalingam and kumaresan  1 . each tracer recovers its input by adding its predicted input to the residual input before calculating the fundamental frequency. if there are no longer valid overtones  or if the intensity of the fundamental overtone drops below a threshold value  it terminates itself. 
noise tracer 
　the noise tracer segregates the static noise stream according to the average spectral intensity  boll  1 . it calculates the spectral intensity time average of the residual input during the dormant period. the noise tracer sends a predicted next input to other agents by sending the spectral intensity. when a tracer receives a spectral intensity  it estimates the intensities of its sound components at each frequency by subtracting the predicted values. the predicted next input of the noise tracer inhibits the generator from generating unnecessary tracers and makes harmonics tracers robust against a nonharmonic noise. the noise tracer calculates average spectral intensity for a long-time range as well as for a short-time range  and it terminates itself when the short-time range average intensity drops below a threshold value. 
	1 	harmonics-based localizing agency 
　the harmonic agency uses monaural  single-channel  input. if multi-channel inputs from a pair of microphones or a microphone array is available  localization  or the direction of a sound source  can be also used to segregate auditory streams. in fact  binaural processing of signals or spatial hearing is said to play a critical role in the cocktail-party effect  blauert  1 . there are several ways to extract spatial information from binaural input. one common way is called coincidence model  which calculates the interaural difference in time that the same sound arrives at each of a pair of microphones  jeffress  1 . another is to use the interaural intensity difference. these two information can be extracted by calculating interaural cross-correlation  which is based on the auditory model. bodden used both information to get spatial information and to control the parameters of filters to extract one sound from a mixture of sounds  bodden  1 . some research also uses microphone array systems  stadlerand rabinowitz  1 . 

figure 1: the structure of the binaural tracer and grouping agency. interaural coordinator agent determines the parameters  such as fundamental frequency and direction  of the stream being extracted by a pair of tracers. 
　since the design of harmonics segregating agency is independent of the  human  auditory model  we take the same approach to design a localizing agency. the localizing agency consists of two agencies: harmonicsbased binaural segregating agency and binaural grouping agency  nakatani et al  1b . harmonic-based binaural segregating agency is modeled by the residue-driven architecture and is an extension of harmonics segregating agency described in the previous subsection. its eventdetector is the same as that of monaural system. the structure of its tracer-generator and tracers are shown respectively in figure 1 and 1. the binaural tracergenerator consists of a pair of tracer-generators and an interaural coordinator. its interaural coordinator takes candidates of new sounds from a pair of tracer-generator and orders them to generate a binaural tracer to extract a stream. 
　a binaural tracer consists of a pair of tracer and an interaural coordinator. its interaural coordinator takes information about harmonic structure from a pair of tracers and determines the fundamental frequency and direction of the stream being traced by calculating the interaural difference in time and the interaural intensity difference. a pair of tracers extract stream fragments with their direction  which are organized into streams by grouping according to their directions. this grouping agency is also modeled by the residue-driven architecture. in this case  the directional information is used to generate a tracer which constructs a pair of binaural streams by grouping stream fragments of the same direction. 
　the merit of using harmonics is that it is easy to calculate the two kinds of interaural differences. otherwise  we have to use spectrum for frequencies up to about 1 khz to calculate them and use the envelop of sounds instead for frequencies of more than 1 khz. 
1 evaluations 
1 evaluations of harmonics-based segregation agency 
　we evaluated the performance of the system by using a mixture of a man's voiced speech and a woman's voiced speech  both saying  a-i-u-e-o . figure 1 shows the fundamental frequency patterns of two speeches. the upper curve is that of the man's voiced speech and the lower one is of the woman's voiced speech. there is no common fundamental frequency  but there are several common overtones. we also used other four sets of mixed sounds by adding different power levels of white noise to it  see table 1 . sounds are put into the system at each time frame  1-ms frame period  with a hamming window . 
experiment 1 
　the first experiment compared the proposed system based on the residue-driven architecture and the hbss system. figure 1 shows fundamental frequency patterns 

1 1 1 1 time  x 1 ms  
 b  proposed system 
figure 1: experiment 1. comparison of fundamental frequency patterns of streams segregated from benchmark mixture 1  a  by the hbss system   nakatani  okuno  and kawabata  1   and  b  by the proposed system based on residue-driven architecture. 
	o 	1 	1 	1 	1 	1 	1 
	time 	 x 1 ins  
 b  with noise tracer 
figure 1: experiment 1. effect of noise tracer in segregation of benchmark mixture 1  where the power of white noise was the same as that of the man's voiced speech. 

of streams segregated from benchmark mixture 1 by each system. only two harmonic tracers were generated in the proposed system and thus no grouping was needed. in the hbss system  on the other hand  1 harmonic tracers were generated. in figure 1 a   a woman's voiced speech was segregated as one stream  while a man's voiced speech was segregated as two consecutive streams. the segregation is much improved by using temporal information in the proposed system. 
experiment 1 
　the second experiment evaluated the noise tracer by benchmark mixture 1 in which the power of white noise was the same as that of the man's speech. more precisely  the benchmark first contained only white noise and then a woman started to speak  a  and was followed by a man starting to speak. without the noise tracer  many tracers were generated in trying to find a harmonic structure in white noise  and the woman's voiced speech could not be segregated well. the quality of segregated streams of the man's voiced speech was also poor. with the noise tracer  the man's and the woman's voiced speeches were well segregated  although several false streams were also segregated. false harmonic tracers were generated but terminated immediately. the total numbers of harmonic tracers generated with and without the noise tracer were respectively 1 and 1. in the hbss system  the total number of harmonic tracers generated with and without the noise tracer were respectively 1 and 1. the proposed system reduces the number of harmonic tracers effectively  demonstrating that sound components are allocated exclusively. 
experiment 1 
　the results of the third experiment  evaluating the quality of segregated sound streams with regard to spectral distortion and pitch error  are shown in tables 1 and 1. spectral distortion is a square root of errors of the envelop of sounds and calculated in kepstrum distance. in evaluating spectral distortion  benchmark mixture 1 to 1 were used. their signal-noise  sn  ratios of white noise to the man's voiced speech varied from 1 db  1 db  1 db to -1 db. with noise tracer  the spectral distortion for each segregated sound was reduced by less than half. 
　pitch errors  or errors of fundamental frequency  of segregated sounds were evaluated by using all benchmark mixtures in table 1. when white noise was very small like benchmark mixture 1  the segregated man's voiced speeches were better than those segregated from benchmark mixture 1  without noise . pitch error without the noise tracer was small when the noise level was low  but error increased as the noise level became higher. these experimental evaluations showed that the noise 

tracer is effective in improving the quality of segregated streams. 
1 	evaluation of localizing agency 
　localizing agency was evaluated by using a mixture of the same woman's speech saying  a-i-u-e-o  in figure 1  synthesized by adding the first speech and the second speech starting 1 seconds after the first one. one speaker was positioned at -1 degree and the other was at 1 degree in the frontal plane. the proposed system with monaural input could not segregate two streams well as is shown in figure 1 a . in particular  the initial part of second woman's voice could not be segregated  because the harmonic structures of both sounds resemble each other. the binaural system segregated two streams well as is shown in figure 1 b   since it could use directional information to remove ambiguities of the harmonic structure between both sounds. the results of other benchmarks of different spatial settings also showed the good performance similar to figure 1 b . 

1 	conclusions and future works 
　this paper described the residue-driven architecture for segregating auditory streams in computational auditory scene analysis. the previous hbss system has several problems concerning imperfect exclusive allocation  usage of temporal and spatial information  and noise treatment. the residue-driven architecture can easily incorporate mechanisms to cope with these problems. this architecture is used to define an agency that segregates auditory streams by tracing sound attributes. two agencies  harmonic segregating agency and localizing agency  are presented. both harmonic agency with the noise tracer and localizing agency improve the quality of segregation. 
　a lot of issues remain  since auditory stream segregation is a primitive function for computational auditory scene analysis. okuno et al. proposed two essential problems  okuno  nakatani  and kawabata  1 . 
 1  the cocktail-party effect - selectively attending one conversation or sound source and then changing the focus of attention to another  okuno  nakatani  and kawabata  1 . 
 1  the prince shotoku effect - listening to several things at the same time  cooke et a/.  1 . this effect is named for prince shotoku  1  in japan  who is said to have been able to listen to seven people petitioning him at the same time. 
these problems require speech stream segregation  whose main issues are handling consonants or jumping sounds. there may be many clues in speech stream segregation  such as temporal structure  spectral structure  spatial structure and attributes of voiced speech. 

from the view-point of ai research  the representation of voiced speech including vowels and consonants is mandatory  but as far as we know  no such representation has been proposed. we are instead using the localizing agency to extract speech streams from binaural inputs. the cocktail-party effect is seldom observed when one ear is plugged or hearing is impaired  and this is because the ability to localize a sound source is damaged. speech stream segregation has many potential applications and we think that casa will contribute various aspects of social life. 
acknowledgments 
　we thank david f. rosenthal for cooperation in writing the introduction section of an earlier draft of the paper as well as for discussions on computational auditory scene analysis. we also thank makio kashino  kunio kashino  masataka goto  rikio onai and ikuo takeuchi for such discussions. we also appreciate the pointer to the literature on binaural processing given by an anonymous reviewer. 
