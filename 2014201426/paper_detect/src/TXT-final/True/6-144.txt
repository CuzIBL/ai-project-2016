 
a longstanding goal in planning research is the ability to generalize plans developed for some set of environments to a new but similar environment  with minimal or no replanning. such generalization can both reduce planning time and allow us to tackle larger domains than the ones tractable for direct planning. in this paper  we present an approach to the generalization problem based on a new framework of relational markov decision processes  rmdps . an rmdp can model a set of similar environments by representing ob-
jects as instances of different classes. in order to generalize plans to multiple environments  we define an approximate value function specified in terms of classes of objects and  in a multiagent setting  by classes of agents. this class-based approximate value function is optimized relative to a sampled subset of environments  and computed using an efficient linear programming method. we prove that a polynomial number of sampled environments suffices to achieve performance close to the performance achievable when optimizing over the entire space. our experimental results show that our method generalizes plans successfully to new  significantly larger  environments  with minimal loss of performance relative to environment-specific planning. we demonstrate our approach on a real strategic computer war game. 
1 introduction 
most planning methods optimize the plan of an agent in a fixed environment. however  in many real-world settings  an agent will face multiple environments over its lifetime  and its experience with one environment should help it to perform well in another  even with minimal or no replanning. 
   consider  for example  an agent designed to play a strategic computer war game  such as the freecraft game shown in fig. 1  an open source version of the popular warcraft game . in this game  the agent is faced with many scenarios. in each scenario  it must control a set of agents  or units  with different skills in order to defeat an opponent. most scenarios share the same basic elements: resources  such as gold and wood; units  such as peasants  who collect resources and build structures  and footmen  who fight with enemy units; and structures  such as barracks  which are used to train footmen. each scenario is composed of these same basic building blocks  but they differ in terms of the map layout  types of units available  amounts of resources  etc. we would like the agent to learn from its experience with playing some scenarios  enabling it to tackle new scenarios without significant amounts of replanning. in particular  we would like the agent to generalize from simple scenarios  allowing it to deal with other scenarios that are too complex for any effective planner. 
   the idea of generalization has been a longstanding goal in markov decision process  mdp  and reinforcement learning 

figure 1: freecraft strategic domain with 1 peasants  a barrack  a castle  a forest  a gold mine  1 footmen  and an enemy  executing the generalized policy computed by our algorithm. 
research 1; 1  and even earlier in traditional planning . this problem is a challenging one  because it is often unclear how to translate the solution obtained for one domain to another. mdp solutions assign values and/or actions to states. two different mdps  e.g.  two freecraft scenarios   are typically quite different  in that they have a different set  and even number  of states and actions. in cases such as this  the mapping of one solution to another is not well-defined. 
   our approach is based on the insight that many domains can be described in terms of objects and the relations between them. a particular domain will involve multiple objects from several classes. different tasks in the same domain will typically involve different sets of objects  related to each other in different ways. for example  in freecraft  different tasks might involve different numbers of peasants  footmen  enemies  etc. we therefore define a notion of a relational mdp  rmdp   based on the probabilistic relational model  prm  framework 1 . an rmdp for a particular domain provides a general schema for an entire suite of environments  or worlds  in that domain. it specifies a set of classes  and how the dynamics and rewards of an object in a given class depend on the state of that object and of related objects. 
   we use the class structure of the rmdp to define a value function that can be generalized from one domain to another. we begin with the assumption that the value function can be well-approximated as a sum of value subfunctions for the different objects in the domain. thus  the value of a global freecraft state is approximated as a sum of terms corresponding to the state of individual peasants  footmen  gold  etc. we then assume that individual objects in the same class have a very similar value function. thus  we define the notion of a class-based value function  where each class is associated with a class subfunction. all objects in the same class have the value subfunction of their class  and the overall value function for a particular environment is the sum of value subfunctions for the individual objects in the domain. 
a set of value subfunctions for the different classes imme-

diately determines a value function for any new environment in the domain  and can be used for acting. thus  we can compute a set of class subfunctions based on a subset of environments  and apply them to another one without replanning. 
   we provide an optimality criterion for evaluating a classbased value function for a distribution over environments  and show how it can  in principle  be optimized using a linear program. we can also  learn  a value function by optimizing it relative to a sample of environments encountered by the agent. we prove that a polynomial number of sampled environments suffice to construct a class-based value function which is close to the one obtainable for the entire distribution over environments. finally  we show how we can improve the quality of our approximation by automatically discovering subclasses of objects that have  similar  value functions. we present experiments for a computer systems administration task and two freecraft tasks. our results show that we can successfully generalize class-based value functions. importantly  our approach also obtains effective policies for problems significantly larger than our planning algorithm could handle otherwise. 
1 relational markov decision processes 
a relational mdp defines the system dynamics and rewards at the level of a template for a task domain. given a particular environment within that domain  it defines a specific mdp instantiated for that environment. as in the prm framework of   the domain is defined via a schema  which specifies a set of object classes each class 
c is also associated with a set of state variables  which describe the state of an object in 
that class. each state variable c.s has a domain of possible values doni we define sc to be the set of possible states for an object in the possible assignments to the state variables of c. 
   for example  our freecraft domain might have classes such as peasant  footman  gold; the class peasant may have a state variable task whose domain is dom peasant.task:  = 
{waiting  mining  harvesting  building}  and a state variable health whose domain has three values. in this case  speasant would have 1   1 = 1 values  one for each combination of values for task and health. 
	the schema also specifies a set of links 	= 
 for each class representing links between ob-
jects in the domain. each link c.l has a range 
for example  peasant objects might be linked to barrack objects  peasant.buildtarget  = barrack  and to the global gold and wood resource objects. in a more complex situation  a link may relate c to many instances of a class c  which we denote by for example  enemy.myjfootmen indicates that an instance of the enemy class may be related to many footman instances. 
¡¡a particular instance of the schema is defined via a world specifying the set of objects of each class; we use to denote the objects in class c  and to denote the total set of objects in  the world also specifies the links between objects  which we take to be fixed throughout time. thus  for each link c.l  and for each specifies a set of objects denoted o.l. for example  in a world containing 1 peasants  we would have 
if peasantl is building a barracks  we would have that peasant l.buildtarget = barrackl. 
   the dynamics and rewards of an rmdp are also defined at the schema level. for each class  the schema specifies an action c.a  which can take on one of sev-
eral values doni 	for example  	d	o	n	i	-
 wait  mine  harvest  build each class c is also associated with a transition model   which specifies the probability distribution over the next state of an object in class c  given the current state of the action taken on   and the states and actions of all of the objects linked to o: 
 1  
 for example  the status of a barrack  barrack.sta/z*/  depends on its status in the previous time step  on the task performed by any peasant that could build it  barrack.builtby.task   on the amount of wood and gold  etc. 
   the transition model is conditioned on the state of c.lt  which is  in general  an entire set of objects  e.g.  the set of peasants linked to a barrack . thus we must now provide a compact specification of the transition model that can depend on the state of an unbounded number of variables. we can deal with this issue using the idea of aggregation . in freecraft  our model uses the count aggregator where the probability that barrack.status transitions from unbuilt to built depends on barrack.builtby.task - built   the number of peasants in barrack.builby whose task is build. 
   finally  we also define rewards at the class level. we assume for simplicity that rewards are associated only with the states of individual objects; adding more global dependencies is possible  but complicates planning significantly. we define 
a reward function which represents the contribution to the reward of any object in c. for example  we may have a reward function associated with the enemy class  which specifies a reward of 1 if the state of an enemy object is dead:  enemy.state = dead  = 1. we assume 
that the reward for each object is bounded by 
   given a world  the rmdp uniquely defines a ground factored mdp whose transition model is specified  as usual  as a dynamic bayesian network  dbn  . the random variables in this factored mdp are the state variables of the individual objects o.s  for each and for each s thus  the state s of the system at a given point in time is a vector defining the states of the individual objects in the world. for any subset of variables x in the model  we define s x  to be the part of the instantiation s that corresponds to the variables x. the ground dbn for the transition dynamics specifies the dependence of the variables at time on the variables at time the parents of a variable  .s' are the state variables of the objects that are linked to . in our example with the two peasants  we might have the random variables peasantl.task  peasant1.task  barrackl.stato  etc. the parents of the time  variable barrackl.status' are the time  variables barrackl. status'  peasant l.task  peasant1.task  goldl amount and woodl amount. 
   the transition model is the same for all instances in the same class  as in  1 . thus  all of the o.status variables for 

1 	probabilistic planning 

thus  for reward function for the enemy class described above  our overall reward function in a given state will be 1 times the number of dead enemies in that state. 
it remains to specify the actions in the ground mdp the 
rmdp specifies a set of possible actions for every object in the world. in a setting where only a single action can be taken at any time step  the agent must choose both an object to act on  and which action to perform on that object. here  the set of actions in the ground mdp is simply the union 
¡¡¡¡dom o.i1 . in a setting where multiple actions can be performed in parallel  say  in a multiagent setting   it might be possible to perform an action on every object in the domain at every step. here  the set of actions in the ground mdp is a vector specifying an action for every object: dom o.i1 . intermediate cases  allowing degrees of parallelism  are also possible. for simplicity of presentation  we focus on the multiagent case  such as freecraft  where  an action is an assignment to the action of every unit. 
example 1  freecraft tactical domain  consider a simplified version of freecraft  whose schema is illustrated in fig.  where only two classes of units participate in the game: both the footman and the enemy classes have only one state variable each  health  with domain dom health  {healthy  wounded  the footman class contains one single-valued link:  footman.myjenemy  enemy. thus the transition model for a footman's health will depend on the health of its enemy: 
 i.e.  if 	footman's enemy is 
not dead  than the probability that a footman will become wounded  or die  is significantly higher. a footman can choose to attack any enemy. thus each footman is associated with an action footman.a which selects the enemy it is attacking.1 as consequence  
a model where an action can change the link structure in the 
   in our setting  the state space is exponentially large  with one state for each joint assignment to the random variables o.s of every object  e.g.  exponential in the number of units in the freecraft scenario . in a multiagent problem  the number of actions is also exponential in the number of agents. thus this lp has both an exponential number of variables and an exponential number of constraints. therefore the exact solution to this linear program is infeasible. 
   we address this issue using the assumption that the value function can be well-approximated as a sum of local value subfunctions associated with the individual objects in the model.  this approximation is a special case of the factored linear value function approach used in .  thus we associate a value subfunction  with every object in w. most simply  this local value function can depend only on the state of the individual object sa. in our example  the local value subfunction for enemy object enemy 1 might associate a numeric value for each assignment to the variable enemy j.health. a richer approximation might associate a value function with pairs  or even small subsets  of closely related objects. thus  the 
world requires a small extension of our basic representation. we omit details due to lack of space. 
   as for any linear approximation to the value function  the lp approach can be adapted to use this value function representation . our lp variables are now the local components of the individual local value functions: 
¡¡¡¡¡¡¡¡  1  in our example  there will be one lp variable for each joint assignment of fi.health and el.health to represent the components of similar lp variables will be included for the components of 
   as before  we have a constraint for each global state s and each global action 
this transformation has the effect of reducing the number of free variables in the lp to n  the number of objects  times the number of parameters required to describe an object's local value function. however  we still have a constraint for each global state and action  an exponentially large number. 
   guestrin  koller and parr   gkp hereafter  show that  in certain cases  this exponentially large lp can be solved efficiently and exactly. in particular  this compact solution applies when the mdp is factored  i.e.  represented as a dbn   and the approximate value function is decomposed as a weighted linear combination of local basis functions  as above. under these assumptions  gkp present a decomposition of the lp which grows exponentially only in the induced tree width of a graph determined by the complexity of the process dynamics and the locality of the basis function. 
   this approach applies very easily here. the structure of the dbn representing the process dynamics is highly factored  defined via local interactions between objects. similarly  the value functions are local  involving only single ob-
jects or groups of closely related objects. often  the induced width of the resulting graph in such problems is quite small  allowing the techniques of gkp to be applied efficiently. 
1 generalizing value functions 
although this approach provides us with a principled way of decomposing a high-dimensional value function in certain types of domains  it does not help us address the generalization problem: a local value function for objects in a world 
1 
does not help us provide a value function for objects in other worlds  especially worlds with different sets of objects. 
¡¡to obtain generalization  we build on the intuition that different objects in the same class behave similarly: they share the transition model and reward function. although they differ in their interactions with other objects  their local contribution to the value function is often similar. for example  it may be reasonable to assume that different footmen have a similar long-term chance of killing enemies. thus  we restrict our class of value functions by requiring that all of the objects in a given class share the same local value subfunction. 
¡¡formally  we define a class-based local value subfunction for each class. we assume that the parameterization of this value function is well-defined for every object o in c. this assumption holds trivially if the scope of is simply we simply have a parameter for each assignment to dom when the local value function can also depend on the states of neighboring objects  we must define the parameterization accordingly; for example  we might have a parameter for each possible joint state of a linked footman-enemy pair. specifically rather than defining separate subfunctions we define a class-based subfunction now the contribution of footmanl to the global value function will be 
¡¡¡¡¡¡ f1.health el.health . 	similarly footman! will contribute 	 f1.health  e1.health . 
   a class-based value function defines a specific value function for each world w  as the sum of the class-based local value functions for the objects in u   
		 1  
this value function depends both on the set of objects in the world and  when local value functions can involve related objects  on the links between them. importantly  although objects in the same class contribute the same function into the summation of  1   the argument of the function for an object is the state of that specific object  and perhaps its neighbors . in any given state  the contributions of different objects of the same class can differ. thus  every footman has the same local value subfunction parameters  but a dead footman will have a lower contribution than one which is alive. 
1 finding generalized mdp solutions 
with a class-level value function  we can easily generalize from one or more worlds to another one. to do so  we assume that a single set of local class-based value functions is a good approximation across a wide range of worlds assuming we have such a set of value functions  we can act in any new world  without replanning  as described in step 1 of fig. 1. we simply define a world-specific value function as in  1   and use it to act. 
   we must now optimize  in a way that maximizes the value over an entire set of worlds. to formalize this intuition  we assume that there is a probability distribution  over the worlds that the agent encounters. we want to find a single set of class-based local value functions that is a good fit for this distribution over worlds. we view this task as one of optimizing for a single  meta-level  mdp where 

nature first chooses a world and the rest of the dynamics are then determined by the . precisely  the state 
space of 	the transition model is the obvious one: from the initial state nature chooses a world according to and an initial state in w according to the initial starting distribution over the states in the remaining evolution is then done according to dynamics. in our example  nature will choose the number 
of footmen and enemies  and define the links between them  which then yields a well-defined mdp e.g.  
1 lp formulation 
the meta-mdp allows us to formalize the task of finding a generalized solution to an entire class of mdps. specifically  we wish to optimize the class-level parameters for not for a single ground mdp  but for the entire 
¡¡we can address this problem using a similar lp solution to the one we used for a single world in sec. 1. the variables are simply parameters of the local class-level value subfunctions : 	for the constraints  recall that our object-based lp formulation in  1  had a constraint for each state s and each action vector 
in the generalized solution  the state space 
is the union of the state spaces of all possible worlds. our constraint set for will  therefore  be a union of constraint sets  one for each world each with its own actions: 
 1  
where the value function for a world  	 s   is defined at 
the class level as in eq.  1 . in principle  we should have an additional constraint for the state s1. however  with a natural choice of state relevance weights a  this constraint is eliminated and the objective function becomes: 
 1  
if in some models  the potential number of objects may be infinite  which could make the objective function unbounded. to prevent this problem  we assume that the goes to zero sufficiently fast  as the number of objects tends to infinity. to understand this assumption  consider the following generative process for selecting worlds: first  the number of objects is chosen according to then  the classes and links of each object are chosen according to using this decomposition  we have that the intuitive assumption described above can be formalized as: 
	for 	some 	thus  the distribution 
over number of objects can be chosen arbitrarily  as long as it is bounded by some exponentially decaying function. 1 sampling worlds 
the main problem with this formulation is that the size of the lp - the size of the objective and the number of constraints - grows with the number of worlds  which  in most situations  grows exponentially with the number of possible objects  or may even be infinite. a practical approach to address this problem is to sample some reasonable number of worlds from the distribution  and then to solve the lp for these worlds only. the resulting class-based value function can then be used for worlds that were not sampled. 
   we will start by sampling a set of worlds according to we can now define our lp in terms of the worlds in d  rather than all possible worlds. for each world in our lp will contain a set of constraints of the form presented in eq.  1 . note that in all worlds these constraints share the variables which represent our class-based value function. the complete lp is given by: 
 1  
where 
ables in 
form as the ones in sec. 1. thus  once we have sampled worlds  we can apply the same lp decomposition techniques of gkp to each world to solve this lp efficiently. our generalization algorithm is summarized in step 1 of fig. 1. 
   the solution obtained by the lp with sampled worlds will  in general  not be equal to the one obtained if all worlds are considered simultaneously. however  we can show that the quality of the two approximations is close  if a sufficient number of worlds are sampled. specifically  with a polynomial number of sampled worlds  we can guarantee that  with high probability  the quality of the value function approximation obtained when sampling worlds is close to the one obtained when considering all possible worlds. 
theorem 1 consider the following class-based value functions  each with k parameters : obtained from the lp over all possible worlds by minimizing eq.  1  subject to the constraints in obtained from the lp with the sampled worlds in  1 ; and the optimal value function of the meta-
mdp 	for a number of sampled worlds m polynomial in the error is bounded by: 
with probability at least 1 where is the maximum per-object reward. 
the proof  which is omitted for lack of space  see online version of this paper   uses some of the techniques developed by de farias and van roy  for analyzing constraint sampling in general mdps. however  there are two important differences: first  our analysis includes the error introduced when sampling the objective  which in our case is a sum only over a subset of the worlds rather than over all of them as in the lp for the full meta-mdp. this issue was not previously addressed. second  the algorithm of de farias and van roy relies on the assumption that constraints are sampled according 

to some  ideal  distribution  the stationary distribution of the optimal policy . unfortunately  sampling from this distribution is as difficult as computing a near-optimal policy. in our analysis  after each world is sampled  our algorithm exploits the factored structure in the model to represent the constraints exactly  avoiding the dependency on the  ideal  distribution. 
1 	learning classes of objects 
the definition of a class-based value function assumes that all objects in a class have the same local value function. in many cases  even objects in the same class might play different roles in the model  and therefore have a different impact on the overall value. for example  if only one peasant has the capability to build barracks  his status may have a greater impact. distinctions of this type are not usually known in advance  but are learned by an agent as it gains experience with a domain and detects regularities. 
we propose a procedure that takes exactly this approach: 
assume that we have been presented with a set 	of worlds 
for each world 	an approximate value function was computed as described in sec. 1. in addi-
tion  each object is associated with a set of features. for example  the features may include local information  such as whether the object is a peasant linked to a barrack  or not  as well as global information  such as whether this world contains archers in addition to footmen. we can define our  training data  
   we now have a well-defined learning problem: given this training data  we would like to partition the objects into classes  such that objects of the same class have similar value functions. there are many approaches for tackling such a task. we choose to use decision tree regression  so as to construct a tree that predicts the local value function parameters given the features. thus  each split in the tree corresponds to a feature in  each branch down the tree defines a subset of local value functions in whose feature values are as defined by the path; the leaf at the end of the path is the average value function for this set. as the regression tree learning algorithm tries to construct a tree which is predictive about the local value function  it will aim to construct a tree where the mean at each leaf is very close to the training data assigned to that leaf. thus  the leaves tend to correspond to objects whose local value functions are similar. we can thus take the leaves in the tree to define our subclasses  where each subclass is characterized by the combination of feature values specified by the path to the corresponding leaf. this algorithm is summarized in step 1 of fig. 1. note that the mean subfunction at a leaf is not used as the value subfunction for the corresponding class; rather  the parameters of the value subfunction are optimized using the class-based lp in step 1 of the algorithm. 
1 	experimental results 
we evaluated our generalization algorithm on two domains: 
computer network administration and freecraft. 
1 computer network administration for this problem  we implemented our algorithm in matlab  using cplex as the lp solver. rather than using the full lp decomposition of gkp   we used the constraint generation extension proposed in 1   as the memory requirements 
1. learning subclasses: 
  input: 

 c  define a subclass for each leaf  characterized by the feature vector associated with its path. 
1. computing class-based value function: 

  algorithm: 
 a  compute the parameters for that optimize the lp in  1  relative to the worlds in 
1. acting in a new world: 
  input: 

  algorithm: repeat  a  obtain the current state s. 
 b  determine the appropriate class cording to its features. 
 c  define  according to  1 . 
 d  use the coordination graph algorithm of gkp to compute an action a that maximizes 
 e  take action 	in the world. 
figure 1: the overall generalization algorithm. 
were lower for this second approach. we experimented with the multiagent computer network examples in   using various network topologies and  pair  basis functions that involve states of neighboring machines  see  . in one of these problems  if we have n computers  then the underlying mdp has 1n states and 1n actions. however  the lp decomposition algorithm uses structure in the underlying factored model to solve such problems very efficiently  1. 
   we first tested the extent to which value functions are shared across objects. in fig. 1 a   we plot the value each ob-
ject gave to the assignment status = working  for instances of the 'three legs' topology. these values cluster into three classes. we used  to learn decision trees for our class partition. in this case  the learning algorithm partitioned the computers into three subclasses illustrated in fig. 1 b : 'server*  'intermediate'  and 'leaf. in fig. 1 a   we see that 'server'  third column  has the highest value  because a broken server can cause a chain reaction affecting the whole network  while 'leaf value  first column  is lowest  as it cannot affect any other computer. 
   we then evaluated the generalization quality of our classbased value function by comparing its performance to that of planning specifically for a new environment. for each topology  we computed the class-based value function with 1 sampled networks of up to 1 computers. we then sampled a 

1 	probabilistic planning 

	 a  	 b  	 c  	 d  	 c  
figure 1: network administrator results:  a  training data for learning classes;  b  classes learned for 'three legs';  c  generalization quality  evaluated by 1 monte carlo runs of 1 steps ;  d  advantage of learning subclasses. tactical freecraft:  e  1 footmen against 1 enemies. 

new network and computed for it a value function that used the same factorization  but with no class restrictions. this value function has more parameters - different parameters for each object  rather than for entire classes  which arc optimized for this particular network. this process was repealed for 1 sets of networks. the results  shown in fig. 1 c   indicate that the value of the policy from the class-based value function is very close to the value of replanning  suggesting that we can generalize well to new problems. we also computed a utopic upper bound on the expected value of the optimal policy by removing the  negative  effect of the neighbors on the status of the machines. although this bound is loose  our approximate policies still achieve a value close to it. 
   next  we wanted to determine if our procedure for learning classes yields better approximations than the ones obtained from the default classes. fig. 1 d  compares the maxnorm error between our class-based value function and the one obtained by replanning. the graph suggests that  by learning classes using our decision trees regression tree procedure  we obtain a much better approximation of the value function we would have  had we replanned. 
1 freecraft 
in order to evaluate our algorithm in the freecraft game  we implemented the methods in c++ and used cplex as the lp solver. we created two tasks that evaluate two aspects of the game: long-term strategic decision making and local tactical battle maneuvers. our freecraft interface  and scenarios for these and other more complex tasks are publicly available at: http://dags stanfoid.edu/freecraft/. for each task we designed an rmdp model to represent the system  by consulting a  domain expert . after planning  our policies were evaluated on the actual game. to better visualize our results  we direct the reader to view videos of our policies at a website: 
http.//lobotics.stantord.edu/~guestrin/re1earch/generalization/. 
this website also contains details on our rmdp model. it is important to note that  our policies were constructed relative to a very approximate model of the game  but evaluated against the real game. 
   in the tactical model  the goal is to take out an opposing enemy force with an equivalent number of units. at each time step  each footman decides which enemy to attack. the enemies are controlled using freecraft's hand-built strategy. we modelled footmen and enemies as each having 1  health points   which can decrease as units are attacked. we used a simple aggregator to represent the effect of multiple attackers. to encourage coordination  each footman is linked to a  buddy  in a ring structure. the local value functions include terms over triples of linked variables. we solved this model for a world with 1 footmen and 1 enemies  shown in fig. 1 e . the resulting policy  which is fairly complex  demonstrates successful coordination between our footmen: initially all three footmen focus on one enemy. when the enemy becomes injured  one footman switches its target. finally  when the enemy is very weak  only one footman continues to attack it  while the others tackle a different enemy. using this policy  our footmen defeat the enemies in freecraft. 
   the factors generated in our planning algorithm grow exponentially in the number of units  so planning in larger models is infeasible. fortunately  when executing a policy  we instantiate the current state at every time step  and action selection is significantly faster  1. thus  even though we cannot execute step 1 in fig. 1 of our algorithm for larger scenarios  we can generalize our class-based value function to a world with 1 footmen and enemies  without replanning using only step 1 of our approach. the policy continues to demonstrate successful coordination between footmen  and we again beat freecraft's policy. however  as the number of units increases  the position of enemies becomes increasingly important. currently  our model does not consider this feature  and in a world with 1 footmen and enemies  our policy loses to freecraft in a close battle. 
in the strategic model  the goal is to kill a strong enemy. 
the player starts with a few peasants  who can collect gold or wood  or attempt to build a barrack  which requires both gold and wood. all resources are consumed after each build action. with a barrack and gold  the player can train a footman. the footmen can choose to attack the enemy. when attacked  the enemy loses  health points   but fights back and may kill the footmen. we solved a model with 1 peasants  1 barrack  1 footmen  and an enemy. every peasant was related to a  central  peasant and every footman had a  buddy . the scope of our local value function included triples between related objects. the resulting policy is quite interesting: the peasants gather gold and wood to build a barrack  then gold to build a footman. rather than attacking the enemy at once  this footman waits until a second footman is built. then  they attack the enemy together. the stronger enemy is able to kill both footmen  but it becomes quite weak. when the next footman is trained  rather than waiting for a second one  it attacks the now weak enemy  and is able to kill him. again  planning in large scenarios is infeasible  but action selection can be performed efficiently. thus  we can use our generalized value function to tackle a world with 1 peasants and 1 footmen  without replanning. the 1 peasants coordinate to gather resources. interestingly  rather than attacking with 1 footmen  the policy now waits for 1 to be trained before attacking. the 1 footmen kill the enemy  and only one of them dies. thus  

probabilistic planning 	1 

we have successfully generalized from a problem with about 1g joint state-action pairs to one with over 1 pairs. 
1 discussion and conclusions 
in this paper  we have tackled a longstanding goal in planning research  the ability to generalize plans to new environments. such generalization has two complementary uses: first we can tackle new environments with minimal or no replanning. second it allows us to generalize plans from smaller tractable environments to significantly larger ones  which could not be solved directly with our planning algorithm. our experimental results support the fact that our class-based value function generalizes well to new plans and that the class and subclass structure discovered by our learning procedure improves the quality of the approximation. furthermore  we successfully demonstrated our methods on a real strategic computer game  which contains many characteristics present in real-world dynamic resource allocation problems. 
several other papers consider the generalization problem. 
several approaches can represent value functions in general terms  but usually require it to be hand-constructed for the particular task. others  1; 1; 1 j have focused on reusing solutions from isomorphic regions of sta1e space. by comparison  our method exploits similarities between objects evolving in parallel. it would be very interesting to combine these two types of decomposition. the work of boutilier et al.  ll on symbolic value iteration computes first-order value functions  which generalize over objects. however  it focuses on computing exact value functions  which are unlikely to generalize to a different world. furthermore  it relies on the use of theorem proving tools  which adds to the complexity of the approach. methods in deterministic planning have focused on generalizing from compactly described policies learned from many domains to incrementally build a first-order policy  1; 
1 . closest in spirit to our approach is the recent work of yoon et al.   which extends these approaches to stochastic domains. we perform a similar procedure to discover classes by finding structure in the value function. however  our approach finds regularities in compactly represented value functions rather than policies. thus  we can tackle tasks such as multiagent planning  where the action space is exponentially large and compact policies often do not exist. 
   the key assumption in our method is interchangeability between objects of the same class. our mechanism for learning subclasses allows us to deal with cases where objects in the domain can vary  but our generalizations will not be successful in very heterogeneous environments  where most objects have very different influences on the overall dynamics or rewards. additionally  the efficiency of our lp solution algorithm depends on the connectivity of the underlying problem. in a domain with strong and constant interactions between many objects  e.g.  robocup   or when the reward function depends arbitrarily on the state of many objects  e.g.  blocksworld   the solution algorithm will probably not be efficient. in some cases  such as the freecraft tactical domain  we can use generalization to scale up to larger problems. in others  we could combine our lp decomposition technique with constraint sampling  to address this high connectivity issue. in general  however  extending these techniques to highly connected problems is still an open problem. finally  although we have successfully applied our class-value functions to new environments without replanning  there are domains where such direct application would not be sufficient to obtain a good solution. in such domains  our generalized value functions can provide a good initial policy  which could be refined using a variety of local search methods. 
   we have assumed that relations do not change over time. in many domains  e.g.  blocksworld or robocup   this assumption is false. in recent work  guestrin et al.  1 show that context-specific independence can allow for dynamically changing coordination structures in multiagent environments. similar ideas may allow us to tackle dynamically changing relational structures. 
   in summary  we believe that the class-based value functions methods presented here will significantly further the applicability of mdp models to large-scale real-world tasks. 
acknowledgements we are very grateful to ron parr for many useful discussions. this work was supported by the dod muri program  administered by the office of naval research under grant n1-1  and by air force contract f1-1 under darpa's task program. 