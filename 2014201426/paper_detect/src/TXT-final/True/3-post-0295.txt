
to produce multimedia encyclopedic content  we propose a method to search the web for images associated with a specific word sense. we use text in an html file which links to an image as a pseudocaption for the image and perform text-based indexing and retrieval. we use term descriptions in a web search site called  cyclone  as queries and correspond images and texts based on word senses.
1 introduction
the world wide web  which contains an enormous volume of up-to-date information  is a promising source to obtain encyclopedic knowledge. it has become common to consult the web for specific keywords  instead of using conventional dictionaries and encyclopedias. however  existing web search engines often retrieve extraneous pages containing low-quality  unreliable  and misleading information.
　fujii and ishikawa  proposed an automatic method to extract term descriptions from the web and classify multiple descriptions into domains and word senses. using this method  fujii and ishikawa have built a web search site called  cyclone 1  in which a user can efficiently obtain an encyclopedic term description in a specific word sense. over 1 japanese terms are currently indexed as headwords.
　however  to explain certain headwords  specifically those related to an entity  such as devices and animals  it is effective to present a picture depicting the entity  in addition to text descriptions.
　in this paper  we propose a method to integrate images on the web and text descriptions in cyclone. we resolve the ambiguity of the meaning of an image by text analysis  so that images for a polysemous word  such as  hub  network device and center of wheel    are classified on the basis of word senses. our research is a step toward the automatic compilation of multimedia encyclopedias  such as encarta1.
1 description-based image disambiguation
existing search engines  such as google and yahoo!  use texts in html files for retrieval purposes  instead of performing content-based image retrieval. given a text query  an image file  such as gif and jpeg files  linked from an html file including one or more query terms is selected as a candidate image. in other words  text content in an html file is used as a  pseudo-caption  for a target image.
　however  the text-based image retrieval cannot distinguish images depicting different entities  if a query term is polysemous. for example  images of network devices and images of axles can potentially be retrieved together in response to the query  hub .
　to resolve this problem  we use a text description for a specific word sense. term descriptions in cyclone  which are organized on the basis of word senses  provide informative contexts for word sense disambiguation. for example  if an html file includes words  lan  and  cable   an image linked from this html file is likely to depict a network device more than an axle.
　we need to crawl the web and cache a large number of images and html files linking to those images. we shall call these html files  hyper html files . however  this process is computationally expensive. we experimentally use yahoo! japan1 to obtain pairs of an image and its hyper html file  by submitting a target term  e.g.   hub   as a query. we discard all html tags in the hyper html files and extract the text content  from which we produce a text index.
　for indexing and retrieval purposes  any best-match text retrieval method  such as the vector space model and probabilistic model  can be used. we experimentally use okapi bm1. we use content words  such as nouns  extracted from text as index terms  and perform word-based indexing. we use the chasen morphological analyzer1 to extract content words  because japanese sentences lack lexical segmentation. the same method is used to extract terms from queries.
　however  unlike a text-based image retrieval which uses well-organized captions  smeaton and quigley  1   not all words in an html file are related to an image. we use different term weights depending on the region in an html file. in principle  a decreasing function which assigns a value to each word depending on the proximity between the word and an anchor  i.e.   img  and  a  in html  to a target image can be used. in practice  we multiply the weight of a word m

figure 1: example text descriptions and images for  habu .
times  if the number of characters between the word and the anchor to the image is less than n. we shall call this method  proximity-based term weighting  pbtw  . the values of m and n are determined empirically in section 1.
　figure 1 depicts a successful example result for the japanese term  habu   which is associated with multiple word senses  such as a network device  snake in okinawa  airport  and center. in figure 1  the first two paragraphs describe device and snake  respectively. the top three image candidates are associated with each paragraph.
1 experimentation
to evaluate the accuracy of our method  we produced a test collection. first  we collected polysemous words used as test terms. for each test term  at least two word senses must be able to be depicted by image  because our purpose is to disambiguate the meaning of images. we collected 1 test terms.
　second  for each test term  we used yahoo! japan to search the web for the top one hundred images and their hyper html files. third  each image was manually annotated with a word sense  disregarding as to whether or not the sense is included in cyclone. the annotator was able to read the content of a hyper html file for the decision  if necessary. the images not associated with any word sense are annotated with  irrelevant . fourth  for each test term  a query is produced from each description in cyclone. we used the top descriptions classified into each domain. the descriptions not associated with any word sense annotated to the images were discarded. the total number of queries was 1.
we compared the following three methods:   a baseline method  which sorts candidate images for each query according to the ranking in yahoo! japan
  baseline   
  our method  the description-based disambiguation method   which uses descriptions in cyclone as queries   dbd   
  our method  which uses descriptions in cyclone as

1
1 1 1 1 1 1 1 1 1 1
rank
figure 1: rank-accuracy graphs for different methods.
queries and performs the proximity-based term weighting method   pbtw  .
for the baseline method  which did not perform image disambiguation  the same list of candidate images was associated with the different senses for each test term. through a preliminary experiment  we set m = 1 and n = 1 for
pbtw.
　figure 1 shows the accuracy for each method in different ranks. the accuracy for rank r is the number of queries for which a correct image was found in the top r candidates and the total number of queries. dbd and pbtw significantly improved on the accuracy of the baseline method  irrespective of the rank. pbtw improved on the accuracy of dbd. as predicted  the words in close proximity to an anchor for a
　target image were important to disambiguate the meaning of the image. pbtw retrieved at least one correct image in the top ten candidates for 1% of the queries.
1 conclusion
to compile multimedia encyclopedic content on the web  we proposed a method to associate text descriptions in cyclone and image files based on word senses  and showed its effectiveness by means of experiments.
