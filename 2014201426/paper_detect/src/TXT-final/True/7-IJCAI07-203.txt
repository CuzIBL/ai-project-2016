
although many powerful ai and machine learning techniques exist  it remains difficult to quickly create ai for embodied virtual agents that produces visually lifelike behavior. this is important for applications  e.g.  games  simulators  interactive displays  where an agent must behave in a manner that appears human-like. we present a novel technique for learning reactive policies that mimic demonstrated human behavior. the user demonstrates the desired behavior by dictating the agent's actions during an interactive animation. later  when the agent is to behave autonomously  the recorded data is generalized to form a continuous state-to-action mapping. combined with an appropriate animation algorithm  e.g.  motion capture   the learned policies realize stylized and natural-looking agent behavior. we empirically demonstrate the efficacy of our technique for quickly producing policies which result in lifelike virtual agent behavior.
1 introduction
an embodied virtual agent  or eva  is a software agent with a body represented through computer graphics  badler et al.  1; dinerstein et al.  1 . if given sufficient intelligence  these characters can animate themselves by choosing actions to perform  where each action is a motion . applications of these agents include computer games  computer animation  virtual reality  training simulators  and so forth.
¡¡despite the success of evas in certain domains  some important arguments have been brought against current techniques. in particular  programming the agent ai is difficult and time-consuming  and it is challenging to achieve naturallooking behavior. we address these issues by learning a policy for controlling the agent's behavior that is:
1. scalable - the policy learning must scale to problems with continuous and high-dimensional state spaces.
1. aesthetic - the virtual agent behavior must appear natural and visually pleasing.
1. simple - the policy learning must be directable by a non-technical user.
¡¡reinforcement learning  rl  is a powerful scheme and an obvious choice for policy learning. however  it does not meet our requirements - rl does not always scale well to continuous state spaces of high dimensionality  which are common for evas   and it is challenging to encode aesthetic goals into a reward structure. we take a different approach.
¡¡we present a technique for automatic learning of a policy by mimicking demonstrated human behavior. this technique not only provides a natural and simple method for the construction of a policy  but it also allows an animator  or other non-technical person  to be intimately involved in the construction of the policy. we learn a reactive policy rather than more advanced behavior because this learning is simple and nearly automatic  thereby allowing for ai creation by nontechnical users. this approach is empirically shown to be effective at quickly learning useful policies whose behavior appears natural in continuous state  and action  spaces of high dimensionality.
1 related work
designing and developing embodied virtual agents is fundamentally a multi-disciplinary problem  gratch et al.  1; cole et al.  1 . indeed the study of evas overlaps with several scientific fields  including artificial intelligence  computer graphics  computer-human interfaces  etc.
¡¡the computer graphics community has been interested in evas since reynold's seminal work on flocking behavior  reynolds  1 . one of the most well-known examples in film is the lord of the rings trilogy  duncan  1   depicting epic-scale battle scenes of humanoids. such large-scale animation would be implausible if the characters were animated manually. several notable techniques for constructing evas have been developed  e.g.   monzani et al.  1   but the explicit programming of agent ai remains a difficult task. game-oriented eva research includes the development of action selection and learning architectures  laird  1 . computational models of emotion have also been proposed  gratch and marsella  1 . recent work has examined rapid behavior adaption such that an eva can better interact with a given human user  dinerstein and egbert  1; dinerstein et al.  1 . in other work  a scheme has been developed whereby a non-embodied agent in a text-based computer game learns through interaction with multiple users  isbell et al.  1 .
¡¡as discussed in the introduction  designing and programming eva ai is often challenging. several authors have addressed this through reinforcementlearning  a survey is given in  dinerstein et al.  1  . rl is a natural choice since it has long been used for agent policy learning. however  rl does not meet our requirements in this paper. for example  it is quite challenging to achieve natural-looking behavior since these aesthetic goals must be integrated into the fitness function. also  many evas live in continuous virtual worlds and thus may require continuous state and/or action spaces  such as flocking evas  reynolds  1 . traditional rl is intractible for large or continuous state/action spaces  though recent research has begun to address this problem  ng and jordan  1; wingate and seppi  1 . nevertheless  the learning is still computationally expensive and limited. our technique learns stylized eva behaviors quickly as empirically shown later.
¡¡the agents and robotics communities have long recognized the need for simplified programming of agent ai. there has been some interesting work performed in programming by demonstration  such as  pomerleau  1; van lent and laird  1; mataric  1; kasper et al.  1; angros et al.  1; nicolescu  1   where an agent is instructed through demonstrations by a user. our technique fits in this category but is specifically designed for evas trained by non-technical users. in contrast to these existing techniques  our approach does not require significant effort from a programmer and domain expert before demonstration can be performed. the existing technique that is most related to our work is  learning to fly   sammut et al.  1 . however  our technique is unique in many aspects  in particular because it performs conflict elimination  can operate in continuous action spaces  and can be operated by non-technical users. there has been some previous work in conflict elimination through clustering  friedrich et al.  1   but that approach quantizes the training examples and culls many potentially useful cases.
contribution: we present a novel scheme for programming eva behavior through demonstration. while many of the components of our scheme are pre-existing  our overall system and application are unique. moreover  some notable and critical components of our scheme are novel. specific contributions made in this paper include:
  the application of agent programming-by-demonstration concepts to policy learning for humanoid evas.
  an intuitive interface for non-technical users to quickly create stylized and natural-looking eva behavior.
  a novel conflict elimination method  previous techniques in agents/robotics have usually ignored or blended conflicts .
1 eva programming by demonstration
learning an eva policy through demonstration involves the following steps  see figure 1 :
1. train:
 a  observe and record state-action pairs.

figure 1: the workflow of our technique.  a  the simulator  which drives the animation  provides a current state s. this is visualized for the demonstrator. the demonstrator responds with an action a  which is recorded and used to update the simulation.  b  the state-action examples are processed to eliminate conflicts and then generalized into a continuous policy function. this policy is used online to control the agent.
 b  eliminate conflicts.
 c  generalize state-action pairs into a policy ¦Ì.
1. autonomous behavior:
 a  use ¦Ì to compute decisions for the agent  once per fixed time step ¦¤t.
¡¡this is a process of learning to approximate intelligent decision making from human example. we formulate decision making as a policy:
	¦Ì : s ¡ú a	 1 
where s ¡Ê rn is a compact representation of the current state of the character and its world  and a¡Êrm is the action chosen to perform. each component of s is a salient feature defining some important aspect of the current state. if the character has a finite repertoire of possible actions  then a is quantized. a state-action pair is denoted . the demonstrator's behavior is sampled at a fixed time step ¦¤t  which is equal to the rate at which the character will choose actions when it is autonomous. thus the observed behavior is a set of discrete cases  b . each pair represents one case of the target behavior. there is no ordering of the pairs in the set. we construct ¦Ì by generalizing these cases. specifically  to ensure that the character's behavior is smooth and aesthetically pleasing  we construct ¦Ì by interpolating the actions associated with these cases. because s ¡Ê rn and a ¡Ê rm  we can theoretically use any continuous real-vectorvalued interpolation scheme. in our implementation  we either use ¦Å-regression svm or continuous k-nearest neighbor  as detailed later. if the action set is symbolic  we use voting k-nearest neighbor.
¡¡for generalization of cases to succeed  it is critical that the state and action spaces be organized by the programmer such that similar states usually map to similar actions. in other words    where ¦Á and ¦Â are small scalar thresholds and ¡¤ is the euclidean metric. certainly  this constraint need not always hold  but the smoother the mapping  the more accurate the learned policy ¦Ì will be.
1 training
in training mode  both the demonstrator and programmer need to be involved. first  the programmer integrates our technique into an existing  but thus far  brainless   character.
this integrationinvolvesdesigning the state and action spaces such that ¦Ì will be learnable. once integration is complete  the demonstrator is free to create policies for the character at will. in fact  the creation of multiple policies for one agent may be interesting to achieve different stylized behaviors  etc.
¡¡demonstration proceeds as follows. the character and its virtual world are visualized in real-time  and the demonstrator has interactive control over the actions of the character. note that the continuous  real-time presentation of state information to the demonstrator is critical to making the character training process as natural as possible  as this is analogous to how humans naturally perceive the real world. as the simulation-visualization of the character and its world proceeds in real-time  the demonstrator supplies the character with the actions it is to perform. this information is saved in the form of state-action pairs. once enough state-action examples have been collected  all conflicting examples are automatically eliminated  as described below. we now have a discrete but representative sampling of the entire policy function ¦Ì. moreover  because the demonstrator has had control of the character  she has forced it into regions of the state space of interest - therefore the density of the sampling corresponds to the importance of each region of the state space.
¡¡elimination of conflicting examples is important because human behavior is not always deterministic  and therefore some examples will likely conflict. this is an importantissue  because the machine learning schemes we utilize will  average  conflicting examples. this can result in unrealistic or unintelligent-looking behavior. note that conflict resolution might also be possible by augmenting the state with context  e.g. the last n actions performed . however  this increase in the state-space dimensionality will only make the learning ¦Ì more challenging and require additional training examples. thus we have designed our technique to directly eliminate conflicts.
conflicting examples are formally defined as:
¡¡if and  then conflict   1  where ¦Í and ¦Ô are scalar thresholds. to eliminate conflicts  we cannot arbitrarily delete cases involved in a conflict - this can lead to high frequencies in the policy. our goal is to remove those examples that represent high frequencies.
¡¡pseudo-code for our conflict elimination technique is given in figure 1. to complement this pseudo-code  we now describe our technique. in brief  each state-action pair is tested in turn to determine whether it is an outlier. first  the l neighbors  according to state  of the current example are found and their median action avm is computed  using the following vector-median method  koschan and abidi  1 :
	avm = arg	min	 1 
a
in other words  avm is equal to the action of the neighbor that is the closest to all other actions of the neighbors according to the euclidean metric. finally  if avm - a   ¦Ç  then the case is an outlier and is marked for deletion. marked cases are retained for testing the other cases  and then are all deleted as a batch at the conclusion of the conflict elimination algorithm.
for each recorded pair
find l closest neighbors of
compute median action avm of l neighbors using eq. 1
if  avm - mark
delete all marked pairs

figure 1: conflict elimination algorithm.
in our experiments  we have found that it works well to use l = 1. we use a threshold ¦Ç of about 1% of the possible range of component values in a.
1 autonomous behavior
once an adequate set of state-action pairs has been collected  we must construct the continuous policy  ¦Ì : s ¡ú a. we do this through one of two popular machine learning techniques: ¦Å-regression svm  haykin  1 or k-nearest neighbor  mitchell  1   or through a classification scheme if the actions are symbolic . we have chosen these two techniques because they are powerfuland well-established  yet have contrasting strengths and weaknesses in our application.
¡¡the primary strength of continuous k-nearest neighbor  knn  is that there are strong guarantees about its accuracy  as it merely interpolates local cases  e.g.  it can robustly handle non-smoothmappings  and the outputs will be within the convex hull of the k local cases . therefore we use k-nn whenever the demonstrated policy is rough. however  k-nn does have a large memory footprint  ¡«1 mb per policy   and more state-action cases are required than with svm due to weaker generalization.
¡¡the support vector machine  svm  is an interesting alternative to k-nn. it is a compact and global technique. as a result  it performs powerful generalization  but can struggle with highly non-smooth mappings. we have found svm to be useful when ¦Ì is somewhat smooth  especially when it is c1 orc1 continuous. we use the popular¦Å-regression scheme with a gaussian kernel to perform function approximation.
1 experimental results
these experiments were designed to cover  in a general fashion  most of the major distinguishing aspects of popular uses of embodied virtual agents. between our favorable experimental results  and the results from the agents/robotics literature on programming-by-demonstration see the related work section   we have some empirical evidence that our scheme is a viable tool for creating some popular types of eva policies.
¡¡the results we achieved in our experiments are summarized in table 1. comparisons against reinforcement learning  a*  and pegasus are summarized in table 1 and discussed in more detail later  while we do not take an rl approach in our technique  we believe it is valuable to compare against rl since it such a popular approach . the results of an informal user case study are reported in table 1. videos of the animation results are available from:
¡¡http://rivit.cs.byu.edu/a1dg/ publications.php
pre-number of state-action pairs1time to demonstrate all pairs1 min.time to eliminate conflicts1 sec.k-nncompute target function values1 ¦Ìsec.storage requirements¡«1 mbtotal decision-making time to cross field1 msec.total animation time to cross field1 sec.svmtime to train ¦Å-regression svm1 min.compute new target function values1 ¦Ìsec.storage requirements¡«1 kbtotal decision-making time to cross field1 msec.total animation time to cross field1 sec.
table 1: summary of average usage and performance results in our spaceship pilot case study  with a 1 ghz processor  1 mb ram  k ¡Ê  1  . similar results were achieved in other experiments.
1 spaceship pilot
in our first experiment  the virtual agent is a spaceship pilot  see figure 1 . the pilot's task is to maneuver the spaceship through random asteroid fields  flying from one end of the field to the other as quickly as possible with no collisions. the animation runs at 1 frames per second  with an action computed for each frame. the virtual pilot controls the orientation of the spaceship.
¡¡¦Ì is formulated as follows. the inputs are the spaceship's current orientation  ¦È ¦Õ   and the separation vector between the spaceship and the two nearest asteroids  ps   pa1 and ps   pa1 . thus the complete state vector is s =
.	there are two outputs 
which determined the change in the spaceship's orientation:
a.
¡¡we achieved good results in this experiment  as shown in table 1 and in the accompanying video. the svm and k-nn policies worked well for all random asteroid fields we tried  and the animation was natural-looking  intelligent  and aesthetically pleasing  verified in an informal user case study .
¡¡to gain a point of reference and to illuminate the interesting computational properties of our technique  we analyzed learning the policy through reinforcement learning  table 1 . this analysis helps validate why a non-rl approach is important in fulfilling our goals. it is straightforward to formulate our case study as an mdp. the state and action spaces have already been defined. we have a complete model of the deterministic environment  needed for animation   the initial state is a position on one side of the asteroid field  and the reward structure is positive for safely crossing the field and negative for crashing. however  even using state-of-the-art table-based techniques for solving large mdp's  wingate and seppi  1   it is currently implausible to perform rl with a continuous state space of dimensionality greater than 1  this size even requires a supercomputer . our mdp state space dimensionality is 1. even gross discretization is intractable:
e.g.  for 1 divisions per axis  1 states. the best result reported in  wingate and seppi  1  is a 1-dimensional state space discretized into approximately 1 states. we also experimented with learning a policy through function-

figure 1: spaceship pilot test bed. the pilot's goal is to cross the asteroid field  forward motion  as quickly as possible with no collisions.
approximation-based rl using a multi-layer neural net  but this failed  the learned policies were very poor . this type of learning is typically considered most stable when employing a linear function approximator  but this can limit the learning  tesauro  1; sutton and barto  1 . thus  for our application  demonstration-based learning is appealing since it is effective in high-dimensional  continuous state and action spaces.
¡¡we also tested a non-standard form of rl designed specifically for continuous state spaces: pegasus  ng and jordan  1; ng et al.  1 . in pegasus  a neural network learns a policy through hill climbing. specifically  a small set of finite-horizon scenarios is used to evaluate a policy and determine the fitness landscape. unlike traditional rl  pegasus succeeded in learning an effective policy in our case study  see table 1 . interestingly  the performanceof the policies learned through our scheme and pegasus are similar. however  pegasus learning was far slower than our technique  hours versus minutes . moreover  we found it extremely difficult and technical to tune the pegasus reward structure for aesthetics. thus  while pegasus is a powerful and useful technique  it does not fulfill the requirement given in the introduction of providing non-technical users with an intuitive and robust interface for programming eva ai. as a result  rl does not appear to be a good fit for our application.
¡¡we also analyzed explicit action selection through a*  table 1 . a* has empirically proven successful  but requires significantly more cpu than our policies to make decisions  sometimes produces behavior that does not appear natural  and requires the programmerto develop an admissible heuristic.
1 crowd of human characters
in our next experiment we created policies to control groups of human characters  see figure 1 . the characters milled about  then a boulder fell from the sky and they ran away. the policies controlled the decision making of the characters  i.e.   turn left    walk forward   etc   while the nuts-andbolts animation was carried out by a traditional skeletal motion system. thus the policy only specified a small set of discrete actions  the real-valued policy output was quantized to be discrete . we created several crowd animations. in each animation  all characters used the same policy  showing the variety of behavior that can be achieved. note that each policy we constructed only required a few minutes to capture all necessary state-action examples  and only a few thousand examples were required for use online.
to train agents to interact in a crowd we performed sev-
rlstorage requirements¡«1 gtime to learn policy¡«1 weekpeg.storage requirements¡«1 kbtime to learn policy¡«1 hrs.time to select actions1 msec.total decision-making time to cross field1 sec.total animation time to cross field1 sec.a*average storage requirement¡«1 mmax storage requirement¡«1 mtime to select actions1 sec.total decision-making time to cross field1 sec.total animation time to cross field1 sec.
table 1: summary of average results using reinforcement learning  pegasus  and a* in the spaceship pilot case study. compare to table 1. the purpose of this comparison to help validate why we have selected demonstration-based learning for our problem domain. reinforcement learning takes a prohibitive amount of time with static or adaptive table-based learning  and function-approximation-based rl did not learn effective policies in our experiments. pegasus successfully learns a useful policy in our case study  but requires significantly more learning time than our approach  and it is difficult to tune the learning for aesthetics. a* takes far more cpu time to make decisions than the policy learned through our technique  and requires an admissible heuristic. thus  from a computational standpoint  our technique is an interesting alternative to these traditional schemes for agent action selection.

figure 1: virtual human crowd test bed.
eral brief demonstration iterations  randomly placing some static characters in a scene. the human trainer then guided the learning agent through the scene. thus the agent learned how to behave in response to other agents around it in arbitrary positions.
¡¡we created two policies. the first was used before the boulder impact  and the second afterwards. in the first policy  pre-boulder   the environment was discretized into a 1d grid. s was formulated as the current orientation of the character  and the separation between it and the next three closest characters. decision making was computed at a rate ¦¤t of about twice per second. after the boulder hit the ground  we switched to another policy  with continuous state/action spaces  which simply directed the characters to run straight away from the boulder. this modularity made the policy learning easier.
1 discussion
our technique has a number of strong points. as has been shown empirically  policies can be learned rapidly. the aes-
test beduserinstruct. timedem. timespace shipauthorn.a.1 min.artist1 min.1 min.game player1 min.1 min.crowdauthorn.a.1 min.designer1 min.1 min.student1 min.1 min.
table 1: summary of an informal user case study. instruct. time is how long a new user needed to be tutored on the use of our system. our technique is intuitive  little instruction required  and user-directed policies can be quickly constructed. the users in this study prefered the aesthetics of their demonstrated policies over traditional policies.
thetic value of these policies was verified through informal user case studies  table 1 . also  its use is natural and straightforward  it is applicable to non-technical users  and there is empirical evidence that it is a viable tool for current uses of evas  e.g.  computer games and animation . in addition  our approach has a nearly fixed online execution time  which is a useful feature for interactive agents.
¡¡improvementsto our approach that are the subject of ongoing research include the following. first  note that a demonstrator's choice of actions may not make the charactertraverse all regions of the state space  leaving gaps in ¦Ì. this problem can be automatically solved  during training  by periodically forcing the character into these unvisited regions of the state space. another important issue is perceptual aliasing  which can result in unanticipated character behavior. however  perceptual aliasing can be minimized through effective design of the compact state space.
¡¡we have opted for learning standard policies  no task structure or rules  e.g.  nicolescu  1   because it allows for the most work to be done through programming-bydemonstration versus explicit programming. however  the scalability of policies is limited. our technique could be augmented with a task learning method from the literature and thereby gain additional scalability and planning behavior  but at the cost of greater programmer involvement. a good example is our crowd case study. pre/post conditions  e.g.  van lent and laird  1   for the discrete contexts could be learned  determining when to switch between policies.
¡¡because our current technique learns deterministic policies  there are some interesting behaviors that it cannot learn. for example  complex goal-directed behaviors that must be broken down into sub-tasks. moreover  our technique is limited to policies that are smooth state-action mappings.
¡¡conflict elimination is important because demonstrator behavior may be non-deterministic  and conflicts may arise due to varying delays in demonstrator response time. these conflicts can result in unintended behavior or temporal aliasing  dithering  in the behavior. our conflict elimination technique safely removes high frequencies in the state-action pairs  helping make ¦Ì a smooth and representative mapping. also  unlike elimination through clustering  friedrich et al.  1   our method does not quantize or eliminate examples. as an alternative to eliminating conflicts  one may consider allowing disjunctive action rules or incorporating probabilistic actions  perhaps weighted by how often each conflicting action is seen during the demonstration period.
¡¡policies can be incrementally constructed or adapted by adding new state-action pairs. if desired  older examples can be deleted  or conflict elimination can be run to cull less applicable examples.
