
the options framework provides methods for reinforcement learning agents to build new high-level skills. however  since options are usually learned in the same state space as the problem the agent is solving  they cannot be used in other tasks that are similar but have different state spaces. we introduce the notion of learning options in agentspace  the space generated by a feature set that is present and retains the same semantics across successive problem instances  rather than in problemspace. agent-space options can be reused in later tasks that share the same agent-space but have different problem-spaces. we present experimental results demonstrating the use of agent-space options in building transferrable skills  and show that they perform best when used in conjunction with problem-space options.
1 introduction
much recent research in reinforcement learning has focused on hierarchical methods  barto & mahadevan  1  and in particular the options framework  sutton et al.  1   which integrates macro-actionlearning into the reinforcementlearning framework. most option learning methods work within the same state space as the problem the agent is solving at the time. although this can lead to faster learning on later tasks in the same state space  learned options would be more useful if they could be reused in later tasks that are related but have distinct state spaces.
　we propose learning portable options by using two separate representations: a representation in problem-space that is markov for the particular task at hand  and one in agent-space that may not be markov but is retained across successive task instances  each of which may require a new problem-space  possibly of a different size or dimensionality   konidaris & barto  1 . options learned in agent-space can be reused in future problem-spaces because the semantics of agent-space remain consistent across tasks.
　we present the results of an experiment showing that learned agent-space options can significantly improve performance across a sequence of related but distinct tasks  but are best used in conjunction with problem-specific options.
1 background
1 the options framework
an option o consists of three components:

 
where πo is the option policy  a probability distribution over actions at each state in which the option is defined   io is the initiation set indicator function  which is 1 for states where the option can be executed and 1 elsewhere  and βo is the termination condition  giving the probability of the option terminating in each state  sutton et al.  1 . the options framework provides methods for learning and planning using options as temporally extended actions in the standard reinforcement learning framework  sutton & barto  1 .
　algorithms for learning new options must include a method for determining when to create an option or expand its initiation set  how to define its termination condition  and how to learn its policy. policy learning is usually performed by an off-policy reinforcement learning algorithm so that the agent can update many option simultaneously after taking an action  sutton et al.  1 .
　creation and termination are usually performed by the identification of goal states  with an option created to reach a goal state and terminate when it does so. the initiation set is then the set of states from which the goal is reachable. previous research has selected goal states by a varierty of methods  e.g.  variable change frequency  hengst  1   local graph partitioning  s ims ek et al.  1  and salience  singh et al.  1  . other research has focused on extracting options by exploiting commonalities in collections of policies over a single state space  thrun & schwartz  1; bernstein  1; perkins & precup  1; pickett & barto  1 .
　all of these methods learn options in the same state space in which the agent is performing reinforcement learning  and thus can only be reused for the same problem or for a new problem in the same space. the available state abstraction methods  jonsson & barto  1; hengst  1  only allow for the automatic selection of a subset of this space for option learning  or require an explicit transformation from one space to another  ravindran & barto  1 .
1 sequences of tasks and agent-space
we are concerned with an agent that is required to solve a sequence of related but distinct tasks  defined as follows. the agent experiences a sequence of environments generated by the same underlying world model  e.g.  they have the same physics  the same types of objects may be present in the environments  etc. . from the sensations it receives in each environment  the agent creates two representations. the first is a state descriptor sufficient to distinguish markov states in the current environment. this induces a markov decision process  mdp  with a fixed set of actions  because the agent does not change  but a set of states  transition probabilities and reward function that depend only on the environment the agent is currently in. the agent thus works in a different state space with its own transition probabilities and reward function for each task in the sequence. we call each of these state spaces a problem-space.
　the agent also uses a second representation based on the features that are consistently present and retain the same semantics across tasks. this space is shared by all of the tasks in the sequence  and we call it an agent-space. the two spaces stem from two differentrepresentationalrequirements: problem-space models the markov description of a particular task  and agent-space models the  potentially non-markov  commonalities across a sequence of tasks. we thus consider a sequence of tasks to be related if they share an agent-space.
　this approach is distinct from that taken in prior reinforcement learning research on finding useful macro-actionsacross sequences of tasks  bernstein  1; perkins & precup  1; pickett & barto  1; thrun & schwartz  1   where the tasks must be in the same state space but may have different reward functions. an appropriate sequence of tasks under our definition requires only that the agent-spacesemantics remain consistent  so each task may have its own completely distinct state space. konidaris and barto  1  have used the same distinction to learn shaping functions in agent-space to speed up learning across a sequence of tasks.
　one simple example of an appropriate sequence of tasks is a sequence of buildings in each of which a robot equipped with a laser range finder is required to reach a target room. since the laser-range finder readings are noisy and nonmarkov  the robot would likely build a metric map of the building as it explores  thus forming a building-specific problem space. the range finder readings themselves form the agent space  because their meaning is consistent across all of the buildings. the robot could eventually learn options in agent-space corresponding to macro-actions like moving to the nearest door. because these options are based solely on sensations in agent space without referencing problem-space  any individual metric map   they can be used to speed up learning in any building that the robot encounters later.
1 options in agent-space
we consider an agent solving n problems  with state spaces s1 ... sn  and action space a. we view the ith state in task sj as consisting of the following attributes:
sji =  dji cji rij  
where dji is the problem-space state descriptor  sufficient to distinguish this state from the others in sj   cji is an agentspace descriptor  and rij is the reward obtained at the state. the goal of reinforcement learning in each task sj is to find a policy πj that maximizes reward.
　the agent is also either given  or learns  a set of higherlevel options to reduce the time required to solve the task. options defined using d are not portable between tasks because the form and meaning of d  as a problem-space descriptor  may change from one task to another. however  the form and meaning of c  as an agent-space descriptor  does not. therefore we define agent-space option components as:

although the agent will be learning task and option policies in different spaces  both types of policies can be updated simultaneously as the agent receives both agent-space and problem-space descriptors at each state.
1 experiments
1 the lightworld domain
an agent is placed in an environmentconsisting of a sequence of rooms  with each room containing a locked door  a lock  and possibly a key. in order to leave a room  the agent must unlock the door and step through it. in order to unlock the door  it must move up to the lock and press it  but if a key is present in the room the agent must be holding it to successfully unlock the door. the agent can obtain a key by moving on top of it and picking it up. the agent receives a reward of 1 for leaving the door of the final room  and a step penalty of  1 for each action. six actions are available: movement in each of the four grid directions  a pickup action and a press action. the environments are deterministic and unsuccessful actions  e.g.  moving into a wall  result in no change in state.
　we equip the agent with twelve light sensors  grouped into threes on each of its sides. the first sensor in each triplet detects red light  the second green and the third blue. each sensor responds to light sources on its side of the agent  ranging from a reading of 1 when it is on top of the light source  to 1 when it is 1 squares away. open doors emit a red light  keys on the floor  but not those held by the agent  emit a green light  and locks emit a blue light. figure 1 shows an example.
　five pieces of data form a problem-spacedescriptorfor any lightworld instance: the current room number  the x and y coordinates of the agent in that room  whether or not the agent has the key  and whether or not the door is open. we use the light sensor readings as an agent-space because their semantics are consistent across lightworld instances. in this case the agent-space  with 1 continuous variables  has higher dimension than any individual problem-space.
types of agent
we used five types of reinforcement learning agents: agents without options  agents with problem-space options  agents with perfect problem-space options  agents with agent-space options  and agents with both option types.

figure 1: a small example lightworld.
　the agents without options used sarsa λ  with -greedy action selection  α = 1  γ = 1  to learn a solution policy in problem-space  with each stateaction pair assigned an initial value of 1.
　agents with problem-space options had an  initially unlearned  option for each pre-specified salient event  picking up each key  unlocking each lock  and walking through each door . options were learned in problem-space and used the same parameters as the agent without options  but used offpolicy trace-based tree-backup updates  precup et al.  1  for intra-option learning. options got a reward of 1 when completed successfully  used a discount factor of 1 per action  and could be taken only in the room in which they were defined  and in states where their value function exceeds a minimum threshold  1 . because these options are learned in problem-space  they are useful but must be relearned for each individual lightworld.
　agents with perfect problem-spaceoptions were given prelearned options for each salient event. they still performed option updates and were otherwise identical to the standard agent with options  but they represented agents with perfectly transferrable fully learned options.
　agents with agent-space options still learned their solution policies in problem-space but learned their option policies in agent-space. each agent employed three options: one for picking up a key  one for going through an open door and one for unlocking a door  with each one's policy a function of the twelve light sensors. since the sensor outputs are continuous we used linear function approximation for each option's value function  performing updates using gradient descent  α = 1  and off-policy trace-based tree-backup updates. the agent gave each option a reward of 1 upon completion  and used a step penalty of 1 and a discount factor of 1. an option could be taken at a particular state when its value function there exceeded a minimum threshold  1 . because these options are learned in agent-space  they can be transferred between lightworld instances.
　finally  agents with both types of options were included to represent agents that learn both general portable and specific non-portable skills simultaneously.
note that all agents used discrete problem-space value functions to solve the underlying task instance  because their agent-space descriptors are only markov in one room lightworlds  which were not present in our experiments.
experimental structure
we generated 1 random lightworlds  each consisting of 1 rooms with width and height of 1 cells. a door and lock were were randomly placed on each room boundary  and  of rooms included a randomly placed key. this resulted in state space with between 1 and approximately 1 state-action pairs  1 on average . we evaluated each problem-space option agent type on 1 lightworlds  1 samples of each generated lightworld .
　to evaluate the performance of agent-space options as the agents gained more experience we similarly obtained 1 samples  but for each sample we ran the agents once without training and then with between 1 training experiences. each training experience for a sample lightworld consisted of 1 episodes in a training lightworld randomly selected from the remaining 1. although the agents updated their options during evaluation in the sample lightworld  these updates were discarded before the next training experience so the agent-space options never received prior training in the evaluation lightworld.
results
figure 1 shows average learning curves for agents employing problem-space options  and figure 1 shows the same for agents employing agent-space options. the first time an agent-space option agent encounters a lightworld it performs similarly to an agent without options  as evidenced by two topmost learning curves in each figure   but its performance rapidly improves with experience in other lightworlds. after experiencing a single training lightworld the agent has a much shallower learning curve than an agent using problemspace options alone  until by 1 experiences its learning curve is similar to that of an agent with perfect problem-space options  compare with the bottom-mostlearning curve of figure 1   even though its options are never trained in the same lightworld in which it is tested. the comparison between figures 1 and 1 shows that agent-space options can be successfully transferred between lightworld instances.
　figure 1 shows average learning curves for agents employing both types of options.1 the first time such agents encounter a lightworld they perform as well as agents using problem-space options  compare with the second highest curve in figure 1   and thereafter rapidly improve their performance performing better than agents using only agentspace options  and again by 1 experiences performing nearly as well as agents with perfect options. we conjecture that this improvement results from two factors. first  the agentspace is much larger than any individual problem-space  so problem-space options are easier to learn from scratch

figure 1: learning curves for agents with problem-space options.

figure 1: learning curves for agents with agent-space options  with varying numbers of training experiences.

figure 1: learning curves for agents with agent-space and problem-space options  with varying numbers of training experiences.
than agent-space options. this explains why agents using only agent-space options and no training experiences perform more like agents without options than like agents with problem-space options. second  options learned in our problem-space can represent exact solutions to specific subgoals  whereas options learned in our agent-space are general and must be approximated  and are therefore likely to be slightly less efficient for any specific subgoal. this explains why agents using both types of options perform better in the long run than agents using only agent-space options.
figure 1 shows the mean total number of steps required
1 x 1

figure 1: total steps over 1 episodes for agents with no options  no   learned problem-space options  lo   perfect options  po   agent-space options with 1 training experiences  dark bars   and both option types with 1 training experiences  light bars .
over 1 episodes for agents using no options  problem-space options  perfect options  agent-space options  and both option types. experience in training environments rapidly drops the number of total steps required to nearly as low as the number required for an agent with perfect options. it also clearly shows that agents using both types of options do consistently better than those using agent-space options alone. we note that the error bars in figure 1 are small and decrease with experience  indicating consistent transfer.
1 the conveyor belt domain
a conveyorbelt system must move a set of objects from a row of feeders to a row of bins. there are two types of objects  triangles and squares  and each bin starts has a capacity for each type. the objects are issued one at a time from a feeder and must be directed to a bin. dropping an object into a bin with a positive capacity for its type decrements that capacity. each feeder is directly connected to its opposing bin through a conveyorbelt  which is connected to the belts above and below it at a pair of fixed points along its length. the system may either run the conveyor belt  which moves the current object one step along the belt  or try to move it up or down  which only moves the object if it is at a connection point . each action results in a penalty of  1  except where it causes an object to be dropped into a bin with spare capacity  in which case it results in a reward of 1. a small example conveyor belt system is shown in figure 1.

figure 1: a small example conveyor belt problem.
　each system has a camera that tracks the current object and returns values indicating the distance  up to 1 units  to the bin and each connector along the current belt. because the space generated by the camera is present in every conveyorbelt problem and retains the same semantics it is an agentspace  and because it is discrete and relatively small  1 states  we can learn policies in it without function approximation. however  because it is non-markov  due to its limited range and inability to distinguish between belts  it cannot be used as a problem-space.
　a problem-space descriptor for a conveyor belt instance consists of three numbers: the current object number  the belt it is on and how far along that belt it lies  technically we should include the current capacity of each bin  but we can omit this and still obtain good policies . we generated 1 random instances with 1 objects and 1 belts  each of length 1  with randomly selected interconnections  resulting in problem-spaces of 1-1 states.
　we ran experimentswhere the agents learned three options: one to move the current object to the bin at the end of the belt it is currently on  one for moving it to the belt above it  and one for moving it to the belt below it. we used the same agent types and experimental structure as before  except that the agent-space options did not use function approximation.
results
figures 1  1 and 1 show learning curves for agents employing no options  problem-spaceoptions and perfectoptions; agents employing agent-space options; agents employing both types of options  respectively.
　figure 1 shows that the agents with agent-space options and no prior experience initially improve quickly but eventually obtain lower quality solutions than agents with problemspace options  figure 1 . one or two training experiences result in roughly the same curve as agents using problem-space options but by 1 training experiences the agent-space options are a significant improvement  although due to their limited range they are never as good as perfect options. this initial dip is probably due to the limited range of the agent-space options  due to the limited range of the camera  and the fact that they are only locally markov  even for their own subgoals.
　figure 1 shows that agents with both option types do not experience this initial dip  and outperform problem-space options immediately  most likely because the agent-space options are able to generalise across belts. figure 1 shows the mean total reward for each type of agent. agents using agent-space options eventually outperform agents using problem-space options only  even though the agent-space options have a much morelimited range; agents using both types of options consistently outperform agents with either option type and eventually approach the performanceof agents using pre-learned problem-space options.
1 discussion
the concept of an agent-centric representation is closely related to the notion of deictic or ego-centric representations  agre & chapman  1   whereobjects are represented from the point of view of the agent rather than in some global frame of reference. we expect that for most problems  especially in robotics  agent-space representations will be egocentric  except in manipulation tasks where they will likely be objectcentric. in problems involving spatial maps  we expect that the difference between problem-space and agent-space will

figure 1: learning curves for agents with problem-space options.

figure 1: learning curves for agents with agent-space options  with varying numbers of training experiences.

figure 1: learning curves for agents with both types of options  with varying numbers of training experiences.
be closely related to the difference between allocentric and egocentric representations of space  guazzelli et al.  1 .
　we also expect that learning an option in agent-space will often actually be harder than solving an individual problemspace instance  as was the case in our experiments. in such situations  learning both types of options simultaneously is likely to improve performance. since intra-option learning methods allow for the update of several options from the same experiences  it may be better in general to simultaneously learn both general portable skills and specific  exact but non-portable skills  and allow them to bootstrap each other.
　the addition of agent-space descriptors to the reinforcement learning framework introduces a design problemsimilar
x 1

figure 1: total reward over 1 episodes for agents with no options  no   learned problem-space options  lo   perfect options  po   agent-space options with 1 training experiences  dark bars   and both option types with 1 training experiences  light bars .
to that of standard state space design. additionally  for option learning an agent-space descriptor should ideally be markov within the set of states that option is defined over. the agentspace descriptor form will therefore affect both what options can be learned and their range. in this respect designing agent-spaces for learning options requires more care than for learning shaping functions  konidaris & barto  1 .
　finally  we note that although we have presented the notion of learning skills in agent-space using the options framework  the same idea can be used in other hierarchical reinforcement learning frameworks  for example the maxq  dietterich  1  or hierarchy of abstract machines  ham   parr & russell  1  formulations.
1 conclusion
we introduced the notion of learning options in agent-space rather than in problem-space as a mechanism for building portable high-level skills for reinforcement learning agents. our experimental results show that such options can be successfully transferred between tasks that share an agent-space  and that this significantly improve performance in later tasks  both in isolation and in conjunction with more specific but non-portable problem-space options.
acknowledgements
we thank sarah osentoski  ozgu：r s：  ims ek  aron culotta  ashvin shah  chris vigorito  kim ferguson  andrew stout  khashayar rohanimanesh  pippin wolfe and gene novark for their comments and assistance. andrew barto and george konidaris were supported in part by the national science foundation under grant no. ccf-1  and andrew barto was supported in part by a subcontract from rutgers university  computer science department  under award number hr1-1 from darpa.
