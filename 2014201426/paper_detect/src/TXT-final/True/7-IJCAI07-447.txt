 
clustering accuracy of partitional clustering algorithm for categorical data primarily depends upon the choice of initial data points  modes  to 
instigate the clustering process. traditionally initial modes are chosen randomly. as a consequence of that  the clustering results cannot be generated and repeated consistently. in this paper we present an approach to compute initial modes for k-mode clustering algorithm to cluster categorical data sets. here  we utilize the idea of evidence accumulation for combining the results of multiple clusterings. initially  n f   dimensional data is decomposed into a large number of compact clusters; the k-modes algorithm performs this decomposition  with several clusterings obtained by n random initializations of the kmodes algorithm. the modes thus obtained from every run of random initializations are stored in a mode-pool  pn. the objective is to investigate the contribution of those data objects/patterns that are less vulnerable to the choice of random selection of modes and to choose the most diverse set of modes from the available mode-pool that can be utilized as initial modes for the kmode clustering algorithm. experimentally we found that by this method we get initial modes that are very similar to the actual/desired modes and gives consistent and better clustering results with less variance of clustering error than the traditional method of choosing random modes. 
1 introduction 
clustering is one of the most useful tasks in data mining process for discovering groups and identifying interesting distributions and patterns in the underlying data. clustering problem is about partitioning a given data set into groups  clusters  such that the data points in a cluster are more similar to each other than points in different clusters  guha et al  1 . the most distinct characteristics of clustering operation in data mining is that the data sets often contain both numeric and categorical attribute values. this requires the clustering algorithm to be capable of dealing with the complexity of the inter- and intrarelation of the data sets expressed in different types of the attributes  no matter numeric or categorical  michalski et al  1 . the k-means algorithm  jain and dubes  1  is one of the most popular clustering algorithms because of its efficiency in clustering large data sets  anderberg  1 . however  k-means clustering algorithm fails to handle data sets with categorical attributes because it minimizes the cost function that is numerically measured.  
모k-means does not guarantee unique clustering because we get different results with randomly chosen initial cluster centers  sing-tze bow  1  and hence the results cannot be relied with confidence. the k-means algorithm gives better results only when the initial partitions are close to the final solution  jain and dubes  1 . several attempts have been reported to generate k-prototype points that can be used as initial cluster centers. a recursive method for initializing the means by running k clustering problems is discussed by duda and hart . bradley et al  reported that the values of initial means along any one of the m coordinate axes are determined by selecting the k densest  bins  along that coordinate. bradley and fayyad  proposed a procedure that refines the initial point to a point likely to be close to the modes of the joint probability density of the data. mitra et al  suggested a method to extract prototype points based on density based multiscale data condensation. khan and ahmad  presented an algorithm to compute initial cluster centers for k-means clustering algorithm. their algorithm is based on two experimental observations that some of the patterns are very similar to each other and that is why they have same cluster membership irrespective to the choice of initial cluster centers. also  an individual attribute may provide some information about initial cluster center. the initial cluster centers computed by using their methodology are found to be very close to the desired cluster centers with improved and consistent clustering results.  
various clustering algorithms have been reported to cluster categorical data. ralambondrainy  presented an approach by using k-means algorithm to cluster categorical data. the approach is to convert multiple category attributes into binary attributes  using 1 and 1 to represent either a category absent or present  and treat the binary attributes as numeric in the k-means algorithm. gower and diday  used a similarity coefficient and other dissimilarity measures to process data with categorical attributes. the k-mode clustering algorithm  huang  1  extends the k-means paradigm to cluster categorical data by using a simple matching dissimilarity measure  hamming distance  for categorical objects and modes instead of means for clusters. 
모most of the above mentioned algorithms for clustering categorical data require a random selection of initial data points in addition to apriori knowledge of number of clusters  k .  this leads to the problem that clustering results are dependent on the selection of initial modes. choosing different initial modes lead to different cluster structures and hence the clustering results cannot be repetitively generated. furthermore  inappropriate choice of initial modes leads to undesirable clustering results. machine learning practitioners find it difficult to count on such clustering results.  
모zhexue huang  presented two methods of initialization for categorical data for k-mode clustering algorithm and showed that if diverse initial modes are chosen then it could lead to better clustering results. sun et al  proposed an iterative method based on initial points refinements algorithm for categorical data clustering to the setting up of the initial points so as to map the categorical data sets to clustering results that have better consistency rates. they applied bradley and fayyad's  iterative initial point refinement algorithm  bradley and fayyad  1  to the k-modes clustering to improve the accuracy and repetition of clustering results. they used sub-sampling method to carry the clustering iteratively several times so that effect of skewed distributed data should not affect the final clustering results. khan s.s. et al  khan and ahmad  1  presented an algorithm to compute initial modes using density based multiscale data condensation. they showed that by choosing initial modes this way consistent and efficient clustering results were achieved. 
모kant et al  presented an automatic and stable clustering algorithm for clustering numerical data. they showed stable clustering of data by repetitively clustering the same data with random initializations to generate stable cluster regions such that each pattern fits exactly into one of those regions and no single pattern can be fitted in two clusters regions. more recently   fred and jain  1a  used the idea of evidence clustering to combine the results of multiple clusterings  n times  into a single data partition  by viewing each clustering result as an independent evidence of data organization.  they did so by running a k-means algorithm many times with different parameters or initializations. first  the data is split into a large number of compact and small clusters; different decompositions are obtained by random initializations of the k-means algorithm. the final data partition is obtained by clustering this new similarity matrix  corresponding to the merging of cluster  fred  1 . topchy et al  presented an algorithm to combine multiple weak clusterings and formulated that combined clustering becomes equivalent to clustering a categorical data based on some chosen consensus function. they showed efficacy of combining partitions generated by weak clustering algorithms that uses random data splits. all of this research work is based on numerical data.  in this paper  we extend the idea of evidence accumulation to categorical data sets by generating multiple partitions as different data organization by seeding k-modes algorithm  every time  with random initial modes. the resultant modes are then stored in a mode pool and the most diverse set of modes were computed  which were used as initial modes.   
모the rest of the paper is organized as follows. section 1 briefly discusses the k-modes algorithm. section 1 describes the proposed approach in computing the initial modes of the data sets using evidence accumulation. section 1 presents the experimental results on applying the proposed approach to compute initial modes for different categorical data sets  uci data repository  and demonstrates improved and consistent clustering results. section 1 concludes the presentation. 
1 the k-modes algorithm for clustering categorical data 
the k-means clustering algorithm cannot cluster categorical data because of the dissimilarity measure it uses. the k-modes clustering algorithm is based on k-means paradigm but removes the numeric data limitation whilst preserving its efficiency. the k-modes algorithm extends kmeans paradigm to cluster categorical data by removing the limitation imposed by k-means through following modifications:  
  using a simple matching dissimilarity measure or the hamming distance for categorical data objects 
  replacing means of clusters by their modes 
모the simple matching dissimilarity measure  jain and dubes  1  can be defined as following. let x and y be two categorical data objects described by f categorical attributes. the dissimilarity measure d x y  between x and y can be defined by the total mismatches of the corresponding attribute categories of two objects. smaller the number of mismatches  more similar the two objects are. mathematically  we can say 
 	f
d x y =뫉붻 x j   y j    	 	 1   	j=1
      1   x j = y j    where 붻x j   y j = 
 	 1	x j 뫛 y j 
d x y  gives equal importance to each category of an attribute. 
let z be a set of categorical data objects described by categorical 	attributes  a1  a1   af   	a 	mode 	of z ={z   z   zn} is a vector q = q1 q1  qf   that minimize  	n
d z q =뫉d zi  q   	 	 1  
 	i=1
here  q is not necessarily an element of z. when the above is used as the dissimilarity measure for categorical data objects  the cost function becomes 
 	k	n f
c q  =뫉뫉뫉붻 zij  qlj     1   l= = =1i 1 j 1
where ql = ql1 ql1  qlm  뫍q 
 
모the k-modes algorithm minimizes the cost function defined in equation 1. 
the k-modes algorithm consists of the following steps: - a  select k initial modes  one for each of the cluster. 
b  allocate data object to the cluster whose mode is nearest to it according to equation 1. 
c  compute new modes of all clusters. 
d  repeat step 1 to 1 until no data object has changed cluster membership. 
1 computing initial modes using evidence accumulation 
the idea of evidence accumulation clustering  fred and jain  1  is to combine the results of multiple clusterings into a single data partition  by viewing each clustering result as an independent evidence of data organization. fred and jain  used the k-means algorithm as the basic algorithm for decomposing the data into a large number of compact clusters; evidence on pattern association is accumulated  by a voting mechanism  over n clusterings obtained by random initializations of the k-means algorithm. there are several possible ways to accumulate evidence in the context of unsupervised learning:  a  combine results of different clustering algorithms;  b  produce different results by re-sampling the data  such as in bootstrapping techniques  like bagging  and boosting;  c  running a given algorithm many times with different parameters or initializations. in this paper we take the last approach  using k-modes algorithm as the underlying clustering for creating multiple partitions of the categorical data. 
모khan and ahmad  presented that in a data set there are some data objects that do not change class membership irrespective of the choice of initial point. in other words they belong to same clusters irrespective of choice of initialization. for example  let di ={di1  di1 dif} be a dataset consisting of n data objects with f attributes. let us assume that data objects dk1  dk1  dk1  where 1뫞 k1  k1  k1 뫞 n are very similar  then they have same cluster membership whenever k-modes algorithm is executed with different initial modes. this information is quite useful to compute initial modes. the two major steps of our algorithm are 
 a  generate n independent evidences of data organizations by performing k-modes clustering using random initialization of modes and store the resultant modes of each of the n iteration in a mode-pool  pn. 
 b  find the most diverse modes for each cluster to be used as initial modes 
1 generating independent data organization 
we assume that the choice of numbers of clusters  k  is the same as the number of natural groupings present in the data set. the algorithmic steps are:   
1. set k뫸number of clusters present in the data set  
n뫸number of clusterings 
1. choose a value of number of clusters  k   i=1. 
1. while  i 뫞 n  do the following 
 a  choose random initial modes and execute k-modes algorithm; till it converges and create k partitions. 
 b  store the k modes thus obtained  from each of the 
모모k partitions  in a mode-pool  pi   c  increment i. 
1 extracting initial modes from mode-pool 
after the execution of algorithm discussed in 1  we are left with a mode-pool  pn with n k뫄f modes  f is the number of attributes of the data set . to extract the most diverse modes  employ this following consensus algorithm 1. set i=1  j=1  k=1 
1. while  i 뫞 k  do the following 
1. while  j 뫞 f  do the following 
1. while  k 뫞 n   do the following 
1. extract the most frequent mode and store it in the initial modes matrix  ii뫄j 
1. increment k 
1. increment j 
1. increment i 
 
모the modes generated by each n clusterings are mostly representative of those data objects/patterns that are less vulnerable to change cluster membership irrespective of the choice of random initial mode selection. after extracting the frequent modes  ik뫄f  from the mode-pool  pn  we shall have captured representations mostly from those patterns only. the modes thus obtained for each of the k partitions should be quite dissimilar from each other with more diversity embodied in them. 
1 experimental results 
to test our approach we use the following categorical data sets obtained from uci machine learning data repository  uci data repository  
 1  michalski soybean disease data set   michalski and stepp  1  
the soybean disease data set consists of 1 cases of soybean disease each characterized by 1 multi-valued categorical variables. these cases are drawn from four populations  each one of them representing one of the following soybean diseases: d1-diaporthe stem canker  d1charcoat rot  d1-rhizoctonia root rot and d1-
phytophthorat rot. ideally  a clustering algorithm should partition these given cases into four groups  clusters  corresponding to the diseases. 
 1  wisconsin breast cancer data 
this data has 1 instances with 1 attributes. each data object is labeled as benign  1 or 1%  or malignant  1 or 1% . in our literature  all attributes are considered categorical with values 1  1... 1. there are 1 instances in groups 1 to 1 that contain a single missing  i.e. unavailable  attribute value  denoted by    .  for data symmetry we took 1 benign case and 1 malignant cases for out analysis.  1  zoo small data 
it has 1 instances distributed into 1 categories. this data is same as zoo data but with only the important eight attributes  feathers  milk  airborne  predator  backbone  fins  leg and tail . all of these characteristics attributes are boolean except for the character attribute corresponds to the number of legs that lies in the set {1  1  1  1  1  1}  1  congressional vote data 
this data set includes votes for each of the u.s. house of representatives congressmen on the 1 key votes identified by the cqa.  the cqa lists nine different types of votes: voted for  paired for  and announced for  these three simplified to yes   voted against  paired against  and announced against  these three simplified to no   voted present  voted present to avoid conflict of interest  and did not vote or otherwise make a position known  these three simplified to an unknown disposition . all attributes are boolean with yes  denoted as y  and no  denoted as n  values. a classification label of republican or democrat is provided with each record. the dataset contains 1 records with 1 republicans and 1 democrats 
모in the presence of true labels  as in the case of the data sets we used  the clustering accuracy for measuring the clustering results was computed as follows. given the 
final number of clusters  k  clustering accuracy r was defined as:  	k
뫉 ai
	r = i=1	 
 	 	n
where n is the number of patterns in the dataset  ai is the number of data objects occurring in both cluster i and its corresponding class  which had the maximal value. in other words  ai is the number of records with the class label that dominates cluster i. consequently  the clustering 
error is defined as  
e=1 r 
low value of e suggests better clustering. 
모to conduct experimental comparison and to verify the efficacy of our proposed method  we supplied initial random modes to the k-modes algorithm as suggested by huang . table 1 compiles the clustering results on the categorical data sets  described above  using random initial modes and the modes supplied by our proposed approach using evidence accumulation. results presented for our approach are based on combination of n=1 k-modes clusterings  a considerable high value to ensure that convergence of the method is ensured. the reported clustering error and standard deviation is average of 1 executions of the whole process. it can be seen that the clustering results have improved with less standard deviation in error when the modes were chosen by our proposed method in comparison to the random selection of initial modes.  
모figure 1 and 1 represents these results graphically. figure 1 show the clustering error and its standard deviation when initial modes were randomly chosen. figure 1 shows the same statistics when initial modes were picked up using our proposed method based on evidence accumulation and were fed to the k-mode clustering algorithm. a reduced clustering error with less variance can be seen from figure 1.  
모one important observation was that the initial modes computed by our proposed approach were quite similar to the actual/desired modes for these data sets and therefore better clustering and fast convergence was achieved. and since k-mode is executed large number of times  n=1   the weak clustering results were eliminated and most of the time we get representations from those patterns that are less susceptible to random selection of modes and therefore we get repetitive initial modes that leads to consistent clustering results with less variance in clustering error. 
 
data set random initialization of modes proposed method of 
initialization using 
evidence accumulation avg. 
clustering error standard deviation avg. 
clustering error standard deviation soybean 1 1 1 1 wisconsin 
breast 
cancer 1 1 1 1 zoo small 1 1 1 1 congressional vote 1 1 1 1  
table 1. clustering error and standard deviation comparison using random initialization of modes and modes supplied using the proposed approach 
 

figure 1. graphical representation of clustering error and 
standard deviation using random selection of modes  
 
 

figure 1. graphical representation of clustering error and standard deviation using proposed approach for initial mode 
computation 
1 conclusions 
k-modes algorithm suffers from the drawback of choosing random initial modes which may lead to formation of non-repetitive clustering structures that are undesirable for analysis. in this paper  we have presented an approach to compute the initial modes for k-modes clustering algorithm for clustering categorical data using evidence accumulation. the procedure is motivated by the observation that some data objects do not change their class membership even when subjected to different random initial conditions  modes . we utilized the idea of evidence accumulation for combining the results of multiple k-mode clusterings. the resultant modes of each of these runs were stored in a mode-pool. the most diverse set of modes were extracted from the mode pool as the initial modes for the k-mode algorithm. the computed modes were majorly being representative of those patterns that are less susceptible to random selection of initial modes. also  the modes computed using this method were found to be quite similar to the actual/desired modes of the datasets. therefore  consistent clustering with fast convergence was achieved with less variance in clustering error.  
acknowledgements 
the authors are thankful to dr. p.k. saxena  director of sag for his encouragement to pursue this research under his kind patronage. the authors are also highly indebted to dr. michael madden  lecturer  department of information technology  nui galway for his support to publish this research work. 
