 
     this paper presents a new model of on-line inference processes during text understanding. the model  called atlast  integrates inference processing at the lexical  syntactic  and pragmatic levels of understanding  and is consistent with the results of controlled psychological experiments. atlast interprets input text through the interaction of independent but communicating inference processes running in parallel. the focus of this paper is on the initial computer implementation of the atlast model  and some observations and issues which arise from that implementation. 
1 introduction 
     this paper describes a new theory of inference processing developed at the irvine computational intelligence project  and an initial computer implementation of that theory. the research described here integrates inference processing at the lexical  syntactic  and pragmatic levels  and is consistent with the results of controlled psychological experiments. the theory centers upon a parallel-process model of text understanding which explains inference behavior at the different levels as the result of interactions between three independent but communicating inference processes. though there are three processes operating at three different levels of language understanding  there is no direct correspondence between the levels and the processes. inference decisions at all levels are made through the combined actions of the three processes running in parallel. we call this model atlast  a three-level language analysis system . 
     atlast represents a real departure from most previous models of language understanding and inference processing  e.g.  schank  1; cullingford  1; wilensky  1; dejong  1   though there are models which integrate some of the levels of inference processing. for example  ipp  lebowitz  1  and boris  dyer  1  integrate the syntactic and pragmatic levels  while the model of small  cottrel   and shastri  integrates lexical access and syntactic parsing. finally  charniak's model  as 
¡¡this research was supported in part by the national science foundation under grant ist-1 and by the naval ocean systems center under contracts n1-c-1 and nc1-c-1. 
does atlast  seeks to integrate lexical  syntactic  and pragmatic inference processing  chamiak  1   though his model differs from atlast in other respects. 
1 background: the theory in brief 
the theory behind atlast is described in detail in 
 granger  eiselt  & holbrook  1   but a brief review of the theory is provided here to aid in understanding the program. 
     atlast is a direct descendant of earlier work on inference decision processes at the pragmatic level. specifically  it came about as an attempt to address word-sense ambiguity problems which arose during research into different pragmatic inference strategies used by human subjects while reading text  and the development of a program  called strategist  which modelled that behavior  granger  eiselt  & holbrook  1; granger & holbrook  1 . as we worked on strategist  we observed that lexical and pragmatic inference processes appeared to have much in common. many pragmatic inferences seemed to be triggered by individual words. this is hardly new news  of course  as there exist integrated models of language understanding in which higher-level inferences are directly activated by input text  frump  dejong  1  and ipp  lebowitz  1  are notable examples . we believed  though  that the relationship was even closer than described by previous models that the inference decision mechanisms themselves were in some way interdependent at the very least. for example  in the text 
the cia called in an inspector to check for bugs. the secretaries had reported seeing roaches. 
the first sentence alone has an unambiguous interpretation: the  hidden microphone  sense of  bugs  is more appropriate than the  insect  sense. upon reading the second sentence  the  insect  sense is obviously more appropriate  and the initial choice of word-sense for  bugs  must be supplanted. to explain this process  we theorize that  cia  triggers a pragmatic inference about espionage which in turn influences the choice of the  hidden microphone  word-sense for  bugs . later  the word  roaches  generates higher-level inferences which suggest that the  insect  sense of  bugs  is correct and that the central intelligence agency  in this case at least  is more appropriately 1 k. eiselt 
viewed as a generic employer trying to rid itself of pests rather than as an espionage agency protecting its secrets. thus  the context generated by  cia  determined the selection of a word-sense  for  bugs   while the context generated by  roaches  resulted in an entirely new interpretation of  bugs  and a slightly modified interpretation of  cia . because of this interdependence between inference levels  theories about pragmatic inference mechanisms must include theories about lexical access processes. 
     lexical access is the process by which a word's meaning is extracted from its written  or spoken  form. recent research into lexical access has led to the counter-intuitive conclusion that when an ambiguous word is presented in context  i.e.  a sentence or phrase   all meanings of the word are initially accessed  and context is subsequently consulted to determine the most appropriate meaning  swinney & hakes  1g; tanenhaus  leiman  & seidenberg  1; lucas  1; granger  holbrook  & eiselt  1 . this happens regardless of the syntactic category of the word  or whether the context is biased toward one meaning or another. 
     if the lexical access process does in fact work as described above  and if individual words trigger the higherlevel pragmatic inferences  then it is likely that the pragmatic inference decision process is much the same as the lexical inference decision process. work on atlast goes under the assumption that  when more than one interpretation  i.e.  pragmatic inference  of an input text is possible  all possible interpretations are pursued in parallel  and those interpretations which do not fit well with the existing context are ''de-activated  or inhibited. 
1 how atlast works 
1 memory 
     atlast is built around a high-level episodic memory structure which contains two kinds of memory organization packets  mops   schank  1; kolodner  1 . for each word in atlast's vocabulary there is a mop which represents that word. most lexical-entry mops contain a one-way link to one or more word-senses directly associated with that word  and syntactic information about the word-senses. function words  such as  a  and  the   are not linked to other mops and serve only to aid in syntactic decisions. the word-senses are an example of the other kind of mop in atlast s memory: those which represent events or objects. these mops are interconnected through a network of two-way links which serve to define the relationships between the mops. these mops can be  but are not necessarily  directly linked to lexical entries. 
     the inference decisions in atlast are carried out by three primary components: the capsulizer  the proposer  and the filter. theoretically  these processes run in parallel. however  atlast is written in uci-lisp on a decsystem-1  so the parallelism which is so important to the theory is necessarily simulated in its implementation. this simulation is accomplished by repeatedly cycling through the three processes. thus  the capsulizer runs for a pre-determined amount of time  followed by the proposer  then the filter  then the capsulizer again  and so on. the amount of time each process is allocated is an important issue with respect to the accuracy of the model. this issue has not yet been fully explored. 
1 capsulizer 
     the capsulizer contains the first stage of a two-stage syntactic analysis process similar in some respects to that described by frazier and fodor . the capsulizer makes intra-phrasal  as opposed to inter-phrasal  syntactic decisions about the words in the input text  again  see  granger  eiselt  & holbrook  1  for a discussion of the theory behind two-stage syntactic analysis . as the capsulizer encounters each new word in the input text  it retrieves the syntactic category information associated with that word  e.g.   this word can be used as a noun and a verb   and activates any word-senses associated with that word. the word-senses are not used in any decisions made by the capsulizer  though pointers to the word-senses are retained. the activated word-senses serve as a starting point for the search carried out by the proposer  which is described below. 
     as the capsulizer processes the input words  it accumulates the syntactic information it retrieves and makes initial decisions about syntactic relationships within the phrases of the input text. these intra-phrasal decisions  along with the pointers to the word-senses which comprise the phrases  are passed along to the filter as  capsules  of information. the filter then makes decisions about the syntactic relationships between the phrases  i.e.  interphrasal syntax . if an input word activates more than one word-sense  i.e.  a word-sense ambiguity   the pointers to the multiple word-senses are all passed on to the filter  which will eventually select the  best  word-sense. this process is also described in more detail below. 
1 proposer 
the proposer gets its name from the idea that it 
 proposes  possible inference paths which might explain the input text. essentially  it is a search mechanism which employs spreading activation to traverse the links between the mops in memory and find connections between wordsenses which have been activated by the capsulizer. 
     the proposer maintains pointers to the most recently activated mops in memory  and to the word-senses which are the origins of the spreading activation search. each time the proposer is invoked  it traverses the links leading away from the recently activated mops  activates the adjacent mops at the end of those links  and updates its list of pointers. if the spread of activation from one point of origin intersects the spread of activation from some other point of origin  then the proposer has found some plausible relationship  by way of links and mops  between two  and possibly more  of the word-senses activated by the input text. the proposer then passes information about this newly-discovered pathway to the filter; in this way  the proposer  proposes  possible inference paths for evaluation by the filter. 
     spreading activation has been employed in a number of models  e.g.  quillian  1; fahlman  1; anderson  1; charniak  1; norvig  1; riesbeck k martin  1 . spreading activation allows atlast to pursue multiple inference paths in parallel. were this process allowed to continue unchecked  it would lead to a combinatorial explosion of inference paths. to prevent this from happening in atlast  the third major process  the filter  constantly evaluates or  filters  inference paths and inhibits pursuit of those which appear to be poor explanations of the input text. though the idea of beginning pursuit on all inference paths instead of just the  appropriate  ones may seem both counter-intuitive and counter-productive  there are two arguments for using this approach. one is that it would seem impossible to determine which inferences may be appropriate without first evaluating all inference possibilities. the other is that this approach is consistent with experimental studies of human behavior  tanenhaus  leiman  & seidenberg  1; seidenberg  tanenhaus  iyeiman  k bienkowski  1; granger  holbrook  & eiselt  1 . 
     the proposer is implemented in atlast as a separate process  but from a theoretical perspective it might be more appropriately viewed as an emergent property of a human memory organization. computer memory seems to work somewhat differently than human memory  though  so it was necessary to provide a separate process to make the spreading activation possible. 
1 filter 
     the filter performs two functions; the first is that of inter-phrasal syntax. as capsules are passed from the capsulizer to the filter  the filter makes decisions about the relationships between the phrases represented by the capsules. inter-phrasal syntax rules enable the filter to fill the actor  action  and object slots  for example. future work on the atlast program will add rules about modifying phrases  keeping track of referents across phrases  and agreement of tense  number  and gender  among other rules. 
     the filter's other function is the evaluation of inference paths. when two competing inference paths are proposed  e.g.  different paths connecting the word-senses of two words from the input text   the filter attempts to select the more appropriate path through the application of three inference evaluation metrics. 
     first  the filter evaluates the inference paths according to the specificity metric  wilensky  1 . if one path is determined to be less specific than the other  the less specific path is inhibited; that is  the spread of activation 
k. eiselt 1 
from nodes on the path is stopped  and that path is no longer considered as a plausible explanation for the input text. specificity is determined by the links in the path: a path which includes a  viewed-as  link  from the  view  relationship defined in  wilensky  1   is less specific than a path which does not contain such a link. in the example of section 1  the cia is a special case of a spy agency  but a 
spy agency can also be viewed as an employer; an inference path which describes the cia only as a spy agency is more specific than one which explains it as a spy agency and an employer. 
     if the specificity metric fails to make a decision between two competing paths  the filter applies two variations of the parsimony metric  granger  1 . the first of these variations  the  length  metric  gives precedence to the inference path with fewer links. failing this  the filter applies the other variation of the parsimony metric  the  explains more  metric   which examines the  intermediate  mops  those which are not the endpoints  of the two competing paths. this variation then selects the inference path that contains more intermediate mops which are intersection points with other inference paths. in other words  the intermediate mops can be either endpoints or intermediate mops of paths other than the two being evaluated. 
     it is with the filter that the implementation of atlast diverges most from the theory. in some sense  this is to be expected  since the filter is the most complex of the three processes. in theory  atlast should be able to evaluate and inhibit pursuit of apparently implausible inference paths almost as soon as pursuit has begun  thus preventing problems of combinatorial explosion. atlast would accomplish this by comparing the multiple  possibly incomplete  inference paths which begin with a specific word-sense to the context it has built up to that point in the processing of the input text  and determining which of the paths fit  best  with that context. this would be in agreement  with experimental results in lexical access research  tanenhaus  leiman  & seidenberg  1; lucas  1; granger  holbrook  & eiselt  1 . at this time  the atlast model can only evaluate complete inference paths  i.e.  those which connect two or more word-senses activated by the capsulizer  without regard to the existing context. though this simple inference evaluation mechanism seems to work for sentences such as the one presented in the following example  it will not be sufficient to properly interpret longer  more complicated texts. this problem will be rectified in the near future. 
1 an example 
     what follows is actual  though abbreviated  annotated run-time output from the atlast prototype program. this example illustrates primarily how atlast disambiguates between two possible meanings of the word  bugs  in the text   the cia checked for bugs.  in the interest of brevity and clarity  we use a very short text and 1 k. eiselt 
just enough of a knowledge base to process this example. due to space limitations  we will concentrate primarily on the operation of the filter. also  we have abbreviated the names of some of the memory structures  again due to space limitations. the following legend should make the program trace more readable: 
gen-employer - generic-employer 
get-secrets - get-others-secrets 
mphone = microphone 
p-healthy-envt - preserve-healthy-environment p-secrets - preserve-own-secrets 
plant-listen-dev = plant-own-listening-device rem-health-hzrd   remove-health-hazard 
rem-listen-dev - remove-others-listening-device 
     after processing  the cia  and activating associated memory structures  atlast processes  checked   which terminates the noun phrase and begins a verb phrase. capsulizer sends a capsule consisting of the word-senses initially activated by the noun phrase  i.e.  c-i-a  to filter. filter  looking for an actor for this sentence  fills the slot with this noun-phrase capsule. atlast then processes  for : 
filter: 
new path discovered: ipatho 
path from c-i-a to search 
c-i-a is special case of spy-agency spy-agency has goal p-secrets 
p-secrets has plan rem-listen-dev 
rem-listen-dev is special case of remove 
remove has precondition search 
action slot filled by search 
     the preposition  for  does not activate any new memory structures  but it does begin a modifying prepositional phrase. capsulizer sends the verb component of the verb phrase  search  to filter  which then assigns the capsule to the action slot. 
     proposer  looking for intersections among the  wavefronts  of spreading activation  finds a connection  or inference path  ipatho   between c-i-a and search  and notifies filter. filter knows of only one inference path at this time  so there is no basis for comparison and evaluation of inference paths yet. atlast then moves on to  bugs : 
filter. 
new path discovered: ipath1 
path from c-i-a to search 
c-i-a is special case of spy-agency 
spy-agency can be viewed as gen-employer 
gen-employer has goal p-healthy-envt 
p-healthy-envt has plan rem-health-hzrd 
rem-health-hzrd is special case of remove 
remove has precondition search 
new path discovered. ipath1 
path from c-i-a to insect c-i-a is special case of spy-agency spy-agency can be viewed as gen-employer gen-employer has goal p-healthy-envt 
p-healthy-envt has plan rem-health-hzrd 
rem-health-hzrd has role-filler insect 
new path discovered: ipath1 
path from c-i-a to mphone 
c-i-a is special case of spy-agency 
spy-agency has goal get-secrets 
get-secrets has plan plant-listen-dev 
plant-listen-dev has role-filler mphone 
new path discovered: ipath1 
path from c-i-a to search c-i-a is special case of spy-agency spy-agency has goal get-secrets 
get-secrets has plan plant-listen-dev 
plant-listen-dev has role-filler mphone 
mphone is role-filler of rem-listen-dev 
rem-listen-dev is special case of remove 
remove has precondition search 
new path discovered: ipath1 
path from search to mphone 
search is precondition of remove 
remove has special case rem-listen-dev 
rem-listen-dev has role-filler mphone 
new path discovered: ipath1 
path from search to insect 
search is precondition of remove 
remove has special case rem-health-hzrd 
rem-health-hzrd has role-filler insect 
new path discovered: ipath1 
path from mphone to c-i-a 
mphone is role-filler of rem-listen-dev 
rem-listen-dev is plan of p-secrets 
p-secrets is goal of spy-agency 
spy-agency has special case c-i-a 
parsimony: ipath1 explains more than ipath1 
specificity: ipath1 more specific than ipath1 parsimony: ipatho shorter than ipath1 
     capsulizer reads the ambiguous word  bugs   which results in the activation of two word-senses: insect and mphone. proposer's search has uncovered several new inference paths. when two different inference paths connect the same two word-senses  filter applies inference evaluation metrics to the two paths to determine which of the two provides the better explanation of the input text. the rejected paths are de-activated until later text results in activating that path again. finally  atlast encounters the end of the text: 
filter: 
object has competing slot f i l l e r s : insect vs. mphone 
specificity: ipath1 more specific than ipath1 parsimony: ipath1 explains more than ipatho lexical ambiguity resolution: mphone vs. insect 
all paths through insect de-activated 
ambiguity resolved: mphone selected 
object slot f i l l e d by mphone 
     capsulizer sends to filter a capsule containing the word-senses activated by the prepositional phrase. filter determines that the capsule contains the object of the action search  and that this object is ambiguous. filter attempts to resolve this ambiguity by applying the inference evaluation metrics to the remaining active inference paths. because mphone and insect are now known to be competing word-senses  filter treats ipath1 and ipath1 as competing inference paths. that is  although ipath1 connects mphone to c-i-a and ipath1 connects insect to c-i-a  the two different paths are evaluated as if they connected the same two word-senses because insect and mphone were activated by the same lexical entry   bugs  . for this same reason  ipath1 is evaluated against ipath1. this evaluation results in the two remaining inference paths containing insect to be de-activated  so filter resolves the ambiguity in favor of mphone. below is the active memory structure after all processing has ended  followed by the pointers into the structure. 
processing completed active memory structure: 
path from mphone to c-i-a 
mphone is role-filler of rem-listen-dev 
rem-listen-dev is plan of p-secrets 
p-secrets is goal of spy-agency 
spy-agency has special case c-i-a 
path from search to mphone 
search is precondition of remove 
remove has special case rem-listen-dev 
rem-listen-dev has role-filler mphone 
path from c-i-a to search 
c-i-a is special case of spy-agency 
spy-agency has goal p-secrets 
p-secrets has plan rem-listen-dev 
rem-listen-dev is special case of remove 
¡¡¡¡remove has precondition search pointers to memory structure: 
actor: c-i-a 
action: search 
object: mphone 
1 an observation on the ordering of inference metrics 
     while testing the atlast program  it became apparent that the order of application of the pragmatic inference metrics affected atlast's eventual interpretation of the input text. as mentioned earlier  atlast applies its specificity metric first  followed by the  length  metric  and then the  explains more  metric. for the example of section 1  this ordering of the inference metrics results in the interpretation that the cia was looking for hidden microphones. on the other hand  if the order of application of the two parsimony metrics is reversed  atlast arrives at a different  nonsensical interpretation. 
k. eiselt 1 
     though this observation does not lead us to any meaningful conclusions at this time  it provides an example of how atlast can serve not only as a  proving ground  for theories  but also as a source of new and interesting ideas worthy of further investigation. 
1 open questions and future work 
     the initial implementation of atlast raised a myriad of implementation issues  many of which are yet to be resolved. more importantly  the implementation again raised some open questions which have been encountered by other researchers. 
     one question has to do with the timing of the three inference processes running in parallel. we do not yet know how much work each of the three processes should do in a given cycle  though we have made arbitrary initial decisions. for the proposer in particular  there are issues which have been addressed by some of the previous models utilizing spreading activation  quillian  1; fahlman  1; anderson  1 : how far does activation spread  does activation decay with time  is there reinforcement when paths intersect  though we do not have answers to the questions now  atlast is designed to allow us to change timing parameters easily  possibly enabling us to  tune  the model for cognitive accuracy as work proceeds. 
another question is concerned with the content of at-
last's memory. currently  atlast runs with a highlevel abstraction of episodic memory: the relationships between the mops are fairly well defined  but the details of the episodes themselves are almost non-existent. thus  information is stored in the links  not in the nodes. the eventual addition of lower-level detail to the episodes will require the application of yet unknown qualitative  as opposed to quantitative  inference metrics. 
     additionally  there is the issue of memory organization. whenever researchers assume that specific concepts are organized in specific ways in human memory  i.e.   this mop is connected to that mop by this relationship    it is nothing more than an educated guess. currently  atlast's metrics depend more on the specific organization of memory  rather than on the content of memory  for correct operation. if the memory had been organized differently  so that there were a different number of links between certain mops  for example  atlast's interpretation of the input text would have been different. this particular realization of the metrics is not necessarily inaccurate  nor does the metrics' reliance on a particular organization of memory invalidate atlast  any more than similar educated guesses invalidate any other models of human understanding. this issue does remind us  however  that our implementation decisions can have as great an impact as our scientific theories on the perceived accuracy of our cognitive models  and that we should remain aware of where theoretical issues end and implementation issues begin. 

1 k. eiselt 
     obviously  much work remains to be done on atlast. the current implementation has been applied only to short texts. in the future  we will process longer texts  and different types of texts  in order to discover additional rules for inference processing. the atlast model provides a framework for testing theories  as well as for making predictions which can be verified experimentally. 
1 conclusion 
1 summary 
     to some extent  atlast is a unification and refinement of ideas from previous models of human inference processes at the lexical  syntactic  and pragmatic levels. yet  while atlast shares common features with each of these models  in many ways it is different from each of these same models. the features which distinguish the atlast model from others are discussed in greater detail in 
 granger  eiselt  & holbrook  1 . a brief summary of those features follows: 
  atlast unifies inference processing at three distinct levels: the lexical  syntactic  and pragmatic levels. 
  the separation of intra-phrasal and inter-phrasal syn-tactic analysis enables atlast to process texts which humans understand and to make the same mistakes a human understander makes. 
  the use of a spreading-activation memory model al-lows atlast to pursue competing inference paths simultaneously until syntactic or semantic information suggests otherwise. previous models of inference decision processes either left a loose end or chose a default inference when faced with an ambiguity  dejong  1; granger  1; lebowitz  1; granger  1; dyer  1; wilensky  1 . 
  the concurrent operation of atlast's capsulizcr  proposer  and filter permits pragmatic interpretations to be evaluated independently of syntactic decisions. this parallel organization also allows immediate evaluation and inhibition of competing inference paths  thus minimizing combinatorial explosion effects. 
  atlast conforms to the results of controlled exper-iments on human subjects. 
1 final comment 
     this paper describes how atlast attempts to understand only a five-word sentence. at first glance  this hardly seems like progress when one considers  for instance  that earlier systems understood hundreds of newspaper stories; in fact  it might even appear that work in natural language understanding is going backwards  at least from a performance perspective. what is really indicated by this phenomenon  though  is that we are becoming more aware of the great quantity of knowledge and the complexity of the processes which language understanders  both human and otherwise  must bring to bear in understanding even the simplest text. in this light  we should not measure the validity of any model of understanding in terms of how many stories it understands  how many words are in its vocabulary  or how fast it runs. more appropriately  we should ask such questions as: is the model extensible  does it compare favorably with experimental data  is it learnable  does it make testable predictions  in other words  cognitive models should be evaluated on the robustness of the theory which they embody. only when that metric is satisfied will the engineering issues become relevant. from this perspective  it is safe to say that atlast is a step in the right direction. 
1 acknowledgments 
     special thanks to rick granger and jen holbrook  whose efforts have made this work possible. 
1 