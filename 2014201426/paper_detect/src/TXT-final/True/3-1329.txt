
forward models enable a robot to predict the effects of its actions on its own motor system and its environment. this is a vital aspect of intelligent behaviour  as the robot can use predictions to decide the best set of actions to achieve a goal. the ability to learn forward models enables robots to be more adaptable and autonomous; this paper describes a system whereby they can be learnt and represented as a bayesian network. the robot's motor system is controlled and explored using 'motor babbling'. feedback about its motor system comes from computer vision techniques requiring no prior information to perform tracking. the learnt forward model can be used by the robot to imitate human movement.
1	introduction
the purpose of any robot is to interact with its environment. there is  howevera separation between the effects a robot can produce in the environment and its mechanisms for producing them  its motor commands. to overcome this  inverse and forward models can be used  jordan and rumelhart  1 . in robotics  forward models enable a robot to predict the consequences of its actions: given the current state of the robot they predict how a set of motor commands will influence the state of the robot or its environment  or how the observations will be perceived by the robot's sensors  as shown in figure 1. conversely  inverse models are used to output the necessary motor commands to achieve or maintain a desired goal. both these internal models have been hypothethised to be used in human motor control  wolpert et al.  1 . the ability to predict the consequence of actions has several uses in robotics  such as allowing mental rehearsal of possible actions or imitating the actions of humans or other robots  demiris  1 .
　practically any environment a robot works in will change  or have properties which cannot be modelled beforehand. even if the environment is assumed to be completely predictable  endowing the robot with this knowledge may be beyond the abilities or desires of its programmer. a truly autonomous robot  therefore  needs to be able to learn and adapt

figure 1: a forward model
its own forward models. forward models of dynamic and uncertain environments  and how they can be learnt  is the topic which is investigated in this paper. although forward models are used in a different context  there are similarities here to work on calibrating action-sensor models in robotics e.g.  roy and thrun  1 . a robot learning about itself and its environment is in the position of being an active part of the system it is trying to learn about; this situation draws interesting parallels with learning in human infants  with mechanisms such as babbling  issuing random motor commands  used by infants to learn how to control their own motor system  meltzoff and moore  1 .
　a system is presented here that enables a robot to autonomously learn a forward model with no prior knowledge of its motor system or the external environment. information about the effects of the robot's actions are captured via a vision system that clusters low-level image features to automatically find and track moving objects in a scene. the robot sends out random motor commands to its motor system and receives information back from the vision system. this set of evidence is used to learn the structure and parameters of a bayesian network  which represents the forward model. this model can then be used to enable the robot to predict the effects of its actions. by observing the gestures of a human with its own vision system  the robot can imitate the human gesture using the forward model it has learnt of its own motor system.
1	representing forward models with bayesian networks
bayesian networks  pearl  1  are an ideal method for representing forward models that can be learnt. they offer a way of representing the causal nature of a robot's control system within a rigorous probabilistic framework. the system here is aimed at learning and representing the causal associations between the robot's motor commands  the robot's state  and the observations of the robot's state that are received from the vision system. the motor commands and state of the robot are represented as random variables in the bayesian network  and the causal relationships between them are shown with arcs. the bayesian networkrepresents a learnt probabilitydistribution across the previous n motor commands  m1:n t d   the p current states s1:p t   and the p observations of the state  o1:p t . the variable d represents the delay between a motor command being issued and robot's state changing; in real robotic systems it cannot be assumed that the effect of a motor command will occur after just one time-step  so this is a parameter that must be modelled and learnt. figure 1 shows this structure.

figure 1: the 'template' bayesian network of a forward model. the question marks represent the part of the structure which needs to be learnt: the causal association between the motor commands and the state of the robot  and which observations are associated with each object in the scene. rectangular nodes correspond to observed nodes.
　the n motor random variables represent specific low-level motor commands of the robot. they could be discrete  e.g. 'open' or 'close' robot's gripper  or continuous  e.g. to set the height of the gripper. when the robot is issuing these commands there is no uncertainty as to their value  so they can be modelled as observed nodes. alternatively  when the robot is observing the actions of another robot  or a human  whilst using its own forward model  they will be hiddennodes because the bayesian network will receive no direct evidence of their value.
　the p observations of the state-space o t  are represented with square nodes descended from s t  on figure 1. the model treats o t  as noisy observations of the actual state space so that noise from the vision system and other uncertainty can be modelled. evidence about the actual state of the robot therefore enters the system here. how the state space  s t   of the robot will be represented depends on the information received from the computer vision system. the state variables are all hidden nodes  because the robot's knowledge of itself only comes from noisy observationsof the state  o t .
1	how a robot can learn and use forward models
the structure for a forward model described in section 1 require several aspects to be learnt. firstly  a representation of the state of the robot or the environment is needed. the richest source of this information comes from the robot's own vision system. however  this is also the most challenging source: information comes in the low-level format of pixels. furthermore  if the robot is seeking to learn its forward models it has no prior information about what to track  because the environment is unknown. to extract information from a scene  a method is needed of automatically initialising and tracking the objects in it. the system presented here works by clustering tracked low-level features in the image according to their position and dynamics. this allows the position of objects in the scene to be tracked without any knowledge of the objects having to be supplied.

figure 1: the vision system finds moving objects in the scene by clustering together optical flow points.
　the low-level point features used are tracked using the lucas-kanade optical flow algorithm  lucas and kanade  1 . the tracked points are clustered using the k-means algorithm according to their position and velocity. one drawback of this method of clustering is that the number of clusters must be known beforehand. this is overcome by first excluding stationary points and extracting increasing numbers of clusters. the error of a particular number of clusters is then taken to be the average squared distance of the optical flow points from the centre of the cluster they are part of. the cluster number used is the one at which the curvature of this error with respect to the number of clusters is minimised  hoppner et al.  1 . the positions  velocities and sizes of each of these clusters can now be used as the observations 
o t   of objects for the forward model learning system. the

figure 1: how the robot can learn about its environment. the stages 1 involved in the experiment are outlined in the text.
actual state of the motor system  s t   is taken to be a hidden discrete node for each object in the scene found by the vision system  and this will be connected to a continuous observation.
　with all the possible nodes in the bayesian network specified  the final task is to learn how the motor commands interact with the state  and train the parameters of the bayesian network so that its predictions are as accurate as possible. to train a bayesian network  a set of evidence is required: a data set containingactual observedvalues of the observednodes in the network. this is a set of n motor commands executed at each time step  m1:n t d   together with the p observations of the state o1:p t   the size  position or velocity of an object. the problem now is how to use this data to learn the structure and parameters of the bayesian network. an inspiration for solving the problem of how to learn this bayesian network can be taken from developmental psychology. gopnik and meltzoff compare the mechanism an infant uses to learn to that of a scientist: scientists form theories  make observations  perform experiments  make generalisations and revise existing theories  gopnik and meltzoff  1 . the notion of forming theories about the world is already dealt with here: a forward model can be seen as a theory of the robot's own world. the process a robot uses to obtain its data and infer a forward model from it is its experiment. an outline of what a robot's experiment involves is as follows  and is highlighted in figure 1:
1. a particular motor command  m1 t  is chosen;
1. a value for this motor command is sent to the robot's motor system;
1. the vision system returns a set of observations of the scene  o1:p t+d   for example  moving forward v ms-1 in direction x;
1. this training set of data  m1 t  and o1:p t+d   is used to train the bayesian network to develop a forward model;
1. the whole process is repeated  either with a different value for the current motor command  or a different motor command altogether.
the open issues here relate to the last two stages. firstly  if the structure of the bayesian network is not completely known  how should the data be used to learn the structure and parameters of the network  most methods for learning structure from data are based on performing a search through the space of possible structures  with the goal of finding the one that maximises the log-likelihood of the model given the data  friedman  1; neapolitan  1 ; the higher the log likelihood  the more accurate a particular network is at predicting the data. this is subject to a constraint on the complexity of the model  as the structure with the highest likelihood will be the one with connections between every node. in this paper the network structure is learnt as part of the experiment by performing a search through the set of possible structures and choosing the one which maximises the log-likelihood. this search is performed online by training and evaluating simultaneously a set of candidate models. the search space is the set of models with different delays in motor commands and different observation nodes for each possible object.
　the second issue is how particular motor commands and their values should be chosen so as to learn the most accurate forward model. unlike most machine learning situations  with a forward model the robot is an active part of the bayesian network structure it is attempting to learn. this situation  referred to as active learning  has received relatively little attention in bayesian networks literature. active learning for bayesian networks has been discussed for unknown parameters and unknown structure  tong and koller  1; 1 . the method of choosing motor commands in the experiment in this paper was inspired by the idea of babbling in infant motor skill learning  meltzoff and moore  1 ; the motor commands were chosen from a random walk through a markov model.
1	experiments to learn a forward model
an experiment was carried out to show how forward models can be autonomously learnt and used. the robot used was an activmedia peoplebot  as shown in figure 1. the task was to learn the forward model for the movement of the robot's grippers; the robot needed to learn to predict the effects of the motor commands. no prior information about the appearance of the grippers or the nature of the motor commands was specified. the robot was to perform an experiment: it babbled with its motor commands  observed what happened  and then learnt the relationship between the motor command and the observation using a bayesian network; this process was repeated until an accurate model had been learnt.
　the motor command in this case was limited to just one degree of freedom: the robot's grippers  which could be either opened  closed or halted. the motor commands were decided at random using a random walk through the markov model shown in figure 1. motor commands were sent according to the current state of the markov model  and the state would change each time-step accordingto the transition probabilities shown on the figure. the parameters of the markov model were chosen so that the motor system stays in one babbling state just long enoughto get enoughinformationto train its forward model before moving to the next state.
　when the motor commands were sent  the grippers moved; the vision system correctly calculated and tracked the posi-

figure 1: an activmedia peoplebot.

figure 1: markov model of gripper babbling. the values on the arcs represent the transition probabilities of going from one state to the next; self transitions are not shown.
tions of the moving grippers in the scene: figure 1 shows the two grippers being located and tracked. the tracking system worked well in this situation: black objects are automatically being tracked even against the black background of the robot's base.
　the vision system provided the random variables for the states s t  and observations o t  in the bayesian network for the forward model. in this situation the two objects  the grippers  were automatically added to the bayesian network. each gripper was represented as a discrete node for the state  and a set of continuous nodes for the observation  both of which were represented as a gaussian distribution. the set of possible observations  o t   was the velocity  vel t   the position  pos t   or size  size t  of the tracked objects. the basic template for the structure of the forward models to be learnt is shown by the bayesian network in figure 1.
　even though the forward model of the robot's gripper was relatively simple  this example highlighted how much the robot needed to learn: all the parameters and two important properties of the structure needed to be learnt. firstly  it is not known how long the delay is between the motor command being issued and the gripper changing state. this is shown in figure 1 as the multiple possible motor commands connected to each state; only the four previous commands are shown to keep the diagram simple. secondly  it was not known which observation from the vision system is the best

figure 1: the tracking system working on the grippers. the system has correctly identified two moving objects  which are identified as grouped primitives shown with the same colour.

figure 1: the template bayesian network for the gripper forward model. the vision system has supplied the information that two objects can be interacted with: the two grippers. the robot still has to learn what the delay  d  is from action to effect and which of the observations the change of state of each object represents  a translation or a change in velocity  and all the parameters that specify the bayesian network.
one to use as part of the forward model: the velocity  position or size. the task of the structure learning algorithm here is therefore to maximise the log-likelihood of the data given the model  which in this case is log p  m o | g    where m is the set of motor commands over the experiment  o the set of corresponding observations and g is the directed acyclic graph representing the structure of the model  together with the parameters of the model. this value is calculated as part of the parameter learning process  as a particular model is trained to the data.
figure 1: evaluating the predictive abilities of a learnt for-　for each model  the parameters of the bayesian network were learnt using the expectation maximisation  em  algorithm  with the inference stage being performed with the junction-tree algorithm  pearl  1 . in this experiment  the search space is small enough for the robot to consider each possible candidate structure simultaneously. if the previous 1 motor commands are considered and 1 possible combinations of observation variables  vel t   pos t  and figure 1: how the log-likelihoods of the bayesian networks using vel t  as the observation node evolve over the course of a typical experiment. the maximum log-likelihood is for the bayesian network with a motor delay of 1 time steps  or 1 ms
size t   for each gripper   the search space contains 1 possible models. figure 1 shows how the log-likelihoods for the bayesian networks with varying motor delays and the velocity observation change over time. as the figure shows  the motor delay which maximise the log-likelihood  d  was 1 time-steps  which is about 1 ms. following the search  the learnt forward model for the grippers is shown in figure 1. the motor command was learnt to be m t-1   and the best observation was learnt to be the velocity of the grippers. this was correct since the motor commands do control the grippers' velocities.

figure 1: the learnt forward model of the grippers.
1	using the bayesian network as a forward model
once the bayesian network in figure 1 has been learnt  it can be used as a forward model to give a prediction of the consequences of the motor command. for the forward models of the grippers  the predicted output is the velocity of the grippers  calculated from the conditional probability distribution: p vel t |m t-1 =m . because the observations here are modelled as gaussians  the most likely values will be the ward model. the predicted gripper movements  dashed  are close to the actual ones  solid . the differences are principally because the variation in gripper velocities for each of the three motor commands are modelled as noise in the forward model.

figure 1: using the learnt bayesian network as an inverse model. evidence is supplied to the observations or the state  and the task is to infer the probability distribution of motor commands.
means of the observations  vel1 t  and vel1 t . the inference algorithm used to calculate this marginal distribution was the junction-tree algorithm  pearl  1 . to evaluate the quality of the learnt forward model the grippers were babbled in a similar way as before  using a different markov model. figure 1 shows how the most likely predicted observation compares with the actual observation. only the x-coordinate predictions are plotted because the grippers' movements are predominantly in this plane. the prediction is shown to be very accurate.
　the learnt bayesian network can also be used as an inverse model by calculating the most likely motor command given the current velocity observation for each time-step  as shown in figure 1. one use of this inverse model is for imitation: by replacing the robot's observations of its own movement with its observations of a human's hand movements in the inverse model  the robot is able to reproduce the motor commands that would be most likely to recreate that human movement in its own gripper system. the same principle is used by demiris to switch from imitation to recognition  albeit in a multiple paired inverse and forward model system  demiris  1 . figure 1 shows the results of the imitation experiment: the robot is able to imitate a simple hand waving motion. the human's hand movements are tracked and recognised using the vision system  which is able to correctly track the two hands. the observations are used as evidence for the bayesian network already learnt by the robot. this network is then used as an inverse model to predict the most likely motor commands that would produce this observation. these motor commands are sent to the robot's motor system enabling it to perform the action that best replicates the observed movement.

figure 1: imitationusing a single inverse model. the top images are corresponding frames from the demonstration  left  and imitating  right  sequences. the graphs show the trajectory of the demonstrating hands  and the corresponding imitating trajectory of the grippers.
1	conclusion
forward models relate a robot's motor commands to their effect on its motor system and the robot's environment. the system presented here allows a robot to autonomously learn forward models of its motor system  using feedback from its vision system  by babbling with its motor commands. by representing the forward model with a bayesian network  the uncertainty in its prediction can also be represented. an experiment was implemented showing how a robot can learn a forward model of its gripper system. the forward model was subsequently used to allow the robot to imitate a simple 'waving hand' gesture of a human. this imitation occurred without any prior information having to be supplied by a human: the robot learns how to control its motor system  then uses this information to imitate a human gesture. future work will involve extending the system to learn forward models with more degrees of freedom and interaction with objects.
1	acknowledgements
this work has been supported through a doctoral training award from the uk's engineering and physical sciences research council  epsrc   and through a bursary from the bbc research & development department. the authors wish to thank gavin simmons  bassam khadhouri  matthew johnson and paschalis veskos of the biologically inspired autonomous robots team  bioart  at imperial college. 