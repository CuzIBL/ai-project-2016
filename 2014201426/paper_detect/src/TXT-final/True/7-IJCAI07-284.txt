
recently  significant progress has been made on learning structured predictorsvia coordinated training algorithms such as conditional random fields and maximum margin markov networks. unfortunately  these techniques are based on specialized training algorithms  are complex to implement  and expensive to run. we present a much simpler approach to training structured predictors by applying a boosting-like procedure to standard supervised training methods. the idea is to learn a local predictor using standard methods  such as logistic regression or support vector machines  but then achieve improved structured classification by  boosting  the influence of misclassified components after structured prediction  re-training the local predictor  and repeating. further improvement in structured prediction accuracy can be achieved by incorporating  dynamic  features-i.e. an extension whereby the features for one predicted component can depend on the predictions already made for some other components. we apply our techniques to the problem of learning dependency parsers from annotated natural language corpora. by using logistic regression as an efficient base classifier  for predicting dependency links between word pairs   we are able to efficiently train a dependency parsing model  via structured boosting  that achieves state of the art results in english  and surpasses state of the art in chinese.
1 introduction
recently  a significant amount of progress has been made on developing training algorithms for learning structured predictors from data  tsochantaridis et al.  1; altun et al.  1; taskar et al.  1 . structured prediction learning extends the standard supervised learning framework beyond the univariate setting  where a single output variable is considered  to the multivariate setting  where complex  non-scalar predictions y  must be produced for inputs x. the challenge is that each component y i of y  should not depend only on the input x  but instead should take into account correlations between y i and its neighboring components y j （ y .
it has been shown in many application areas that structured prediction models that directly capture the relationships between output components perform better than models that do not directly enforce these relationships  lafferty et al.  1; tsochantaridis et al.  1; altun et al.  1; taskar et al.  1; 1 . in particular  these ideas have started to have a significant impact in the field of natural language parsing  mcdonald et al.  1; mcdonald and pereira  1; corston-oliver et al.  1   where state of the art results have recently been achieved through the use of structured training algorithms. parsing is a large-scale structured prediction problem where multiple predictions must be coordinated to achieve an accurate parse for a given input sentence. parsing is a particularly challenging and important task for machine learning  since it involves complex outputs  parse trees  and limited training data  manually parsed treebanks   and yet machine learning methods have proved to provide the best approach to obtaining robust parsers for real data.
　one drawback with current structured prediction training algorithms  however  is that they involve new  specialized parameter optimization algorithms  that are complex  nontrivial to implement  and usually require far more computation than standard classification learning methods  lafferty et al.  1; taskar et al.  1 . the main reason for increased complexity is the fact that some form of structured prediction algorithm  such as a parser or a viterbi decoder  must be considered in the underlying training principle  which causes the structured inference of output predictions to be tightly coupled with the parameter optimization process during training. in this paper  we demonstrate the somewhat surprising result that state of the art performanceon natural language parsing can be achieved through the use of conventional  local classification methods. in particular  we show how a simple form of structured boosting can be used to improve the training of standard local classification methods  in the structured case  without modifying the underlying training method. the advantage of this approach is that one can use off-the-shelf classification techniques  such as support vector machines or logistic regression  to achieve competitive structured prediction results with little additional effort. we achieve this through the use of two simple ideas. first  we introduce a very simple form of  structured boosting   where a structured predictor  such as a parser  is used to modify the predictions of the local  weak learning algorithm  which then influences

	investors	continue to	pour cash	into money funds
figure 1: a dependency tree
the example weightings and subsequent hypotheses  implicitly encouraging more accurate structured predictions. second  we exploit an old idea from natural language parsing  but highlight it here more generally  that  dynamic  features can be used in the local classifiers  which take into account the previous classifications of a restricted set of neighboring examples.
　we demonstrate these ideas concretely on the problem of learning natural language dependency parsers from labeled  treebank  data. although dependency parsing is a very complex problem  we are able to achieve state of the art results by training a local  link predictor  that merely attempts to predict the existence and orientation of a link between two words given input features encoding context-without worrying about coordinating the predictions in a coherent global parse. instead  a wrapper approach  based on structured boosting  is used to successively modify the training data so that the training algorithm is implicitly encouraged to facilitate improved global parsing accuracy.
　the remainder of the paper is organized as follows. first  in section 1 we briefly describe the dependency parsing problem and introduce some of our notation and terminology. we then explain the relationship between structured prediction learning and traditional classification learning in section 1  and point out opportunities for connecting these problems. next  in section 1 we briefly explain  in general terms  the idea of  dynamic  features in classification learning and how these can be used to improve structured prediction. then in section 1 we introduce the main technique proposed in this paper  structured boosting  and explain its relation to standard boosting approaches. finally  in sections 1 and 1  we describe our experiments in learning dependency parsers from treebank data  and show how competitive results can be obtained through the use of standard learning methods. in fact  our results surpass state of the art accuracy in chinese parsing  and are competitive with state of the art in english. section 1 concludes the paper with a discussion of future work.
1 dependency parsing
since our main application involves learning dependency parsers from labeled data  we briefly introduce the problem and some of the issues it creates. in natural language parsing  a dependency tree specifies which words in a sentence are directly related. that is  the dependency structure of a natural language sentence is a directed tree where the nodes are the words in the sentence and links represent the direct dependency relationships between the words; see figure 1. generally speaking  a dependency tree is much easier to understand and annotate than constituency trees that involving part of speech and phrase labels  e.g.  figure 1 . consequently  there has been a growing interest in dependency parsing in recent years. dependency relations have been playing important roles in machine translation  fox  1;

figure 1: a constituency tree
cherry and lin  1; ding and palmer  1   coreference resolution  bergsma and lin  1   and information extraction  culotta and sorensen  1 .
　we consider the problem of automatically learning a dependency parser for a given language from a treebank: a collection of manually parsed sentences. treebanks usually consist of constituency parses  but a dependency parse can be automatically extracted from a constituency parse by a few simple rules  bikel  1 . thus  the training data consists of a set of sentences  each annotated with a directed spanning tree over words  figure 1 . for most languages  like english and chinese  this tree is planar  often called  projective  .
　although a dependency tree is a complex object  parsing can conceptually be reduced to a set of local classifications: each word pair in a sentence can be classified into one of three categories; no link  left link  or right link. one aspect of the problem is local: for a given pair of words in a given sentence context  what should their link label be  the other aspect of the problem is global: how should the local link predictions be coordinated globally to produce a consistent and accurate dependency tree  both aspects of the problem-local link prediction versus global link coordination-are crucial to achieving a state of the art dependency parser. locally  the problem is amenable to standard classification learning approaches. globally  the problem requires coordination-that is  interaction with a parsing algorithm.
1 from local to coordinated training
the problem of learning a structured predictor can generally be formulated as follows. we are given a set of annotated objects  s1 t1  ...  st tt   where each object si consists of a composite observation  i.e. a sentence consisting of a word string  and each annotation ti is a complete labeling of the relevant subcomponents in si  i.e. a link label for every pair of words in the sentence . typically  a single composite example  si ti  can be broken down into a  coordinated 
set of local examples  si 1 ti 1 | si ti  ... si n ti n | si ti   where each label tij （ ti is an atomic classification of a subcomponent si j （ si  taken in context  si ti . for example  a sentence si = wi 1...wi n and a dependency tree labeling ti = ti 1...ti n 1 n can be decomposed into local examples
 wi 1 wi 1;ti 1 | si ti  ... wi n 1 wi n;ti n 1 n | si ti   consisting of arbitrary  not necessarily adjacent  word pairs and their link label  none  left  right  in context  si ti . the context is important because accurately predicting a component label  even locally  requires the consideration of more than just the subcomponent  word pair  itself  but also the surrounding components  and possibly even the labels of some of the surrounding components.  we will discuss the latter point in more detail below. 
　this decomposition facilitates a purely local approach to the learning problem. given the original composite data  s1 t1  ...  st tt   e.g. sentences and their parses  one can first break the data up into local exam-
ples  w1 w1;t1 1 | s1 t1  ... wi j wi k;ti j k | si ti  ...  ignore the relationships between examples  and use a standard supervised learning algorithm to learn a local predictor  in context  si ti . for example  if we restrict attention to linear predictors  support vector machines or logistic regression models   we only need to learn a weight vector θ over a set of features defined on the local examples f wi j wi k ti j k;si ti . here  each feature fm computes its value based on the component  wi j wi k   the label ti j k  in their context  si ti . in this case  a multiclass support vector machine  crammer and singer  1  or logistic regression model  hastie et al.  1  could be trained in a conventional manner to achieve an accurate local prediction model.
　the only question that remains is how to perform valid structured prediction on composite test objects using a local prediction model. the problem is that structured classification requires that constraints be respected between the classifications of different local components. for example  in dependency parsing  the predicted word pair labels  no link  left link  right link  must form a valid directed spanning tree  which in the case of some languages like english and chinese  should also be a planar tree. this form of global consistency is achieved in practice simply by combining a local link classifier with a parsing algorithm. a dependency parsing algorithm is  in effect  a dynamic programming algorithm that has the goal of producing a maximum weight spanning tree subject to the constraints  eisner  1 . the output of the local predictor can be used as a numerical weight to score a potential link  in context  which the parser can then use to make decisions about which link labels to choose. in this sense  the role of a local predictor is just to supply a learned scoring function to a pre-existing parsing algorithm. exactly this approach to combining local link predictors with dependency parsing algorithms has been tried  with some success  by many researchers-using support vector machines yamada and matsumoto  1   logistic regression  aka. maximum entropy models   ratnaparkhi  1; charniak  1   and generative probability models  collins  1; wang et al.  1 -to learn local scoring functions.
　unfortunately  these simple local learning strategies have an obvious shortcoming. the problem is that the training loss being minimized during local parameteroptimization has nothing directly to do with the parser. although it is true that an accurate local predictor is a prerequisite for an accurate parse prediction  the parameters of the local model are not being trained to directly optimize the global accuracy of the parser. that is  a far better choice of parameters might exist within the given space defined by the features that leads to better global parsing accuracy. this is where the advent of recent training algorithms for learning structured predictors has been helpful. the main idea behind these training algorithms has been to explicitly incorporate the effects of the structured predictor directly into the training algorithm. that is  parameter optimization of a local predictor is performed by directly considering the implied effects on the structured  global rather than local  prediction error. the extension to structured training loss has been developed for both the large margin training principle of support vector machines  tsochantaridis et al.  1; altun et al.  1; taskar et al.  1  and the maximum conditional likelihood principle of logistic regression  lafferty et al.  1 . subsequently  training algorithms based on these principles have been applied to parsing  taskar et al.  1; wang et al.  1   and recently resulted in state of the art accuracy for english dependency parsing  mcdonald et al.  1; mcdonald and pereira  1 . unfortunately  the main drawback with these structured training techniques is that they are specialized  non-trivial to implement  and require a great deal of refinement and computational resources to apply to a significant task like parsing  mcdonald et al.  1; corston-oliver et al.  1 .
　in this paper  we pursue a simpler  more general approach that can be applied to any local prediction learning strategy  without requiring that the underlying training algorithm be modified  while still ensuring that the training outcome is directly influenced by the resulting accuracyof the global structured predictor  the parser .
1 dynamic features
before describing our generalstructured boostingmethod  we first briefly describe a simple but useful idea for improving the structured classification accuracy of local predictors. the idea is to use so-called  dynamic  features; that is  features that take into account the labels of  some  of the surrounding components when predicting the label of a target component. in particular  when predicting the label of a target component si j （ si from a composite object si  one can assume that the labels for some other components  say si j 1 （ si  have already been computed. the only constraint is that the neighboring labels used must always be available before attempting to label si j. for supervised training  this is never a problem  because the labels are always available for every component. however  the real issue arises during testing-that is  during the structured classification of a test object. here the labels for each component have to be determined in a systematic order that ensures the required features are always available when the next component needs to be labeled.
　the easiest way to illustrate the concept is in a sequential labeling task  like part of speech tagging: given a sentence s = w1...wn  the goal is to predict the corresponding tag sequence t = t1...tn. here the preceding tags can be used as features for the current word under consideration-e.g. in a maximum entropy markov model  memm   mccallum et al.  1 -while still permitting an efficient viterbi decoding algorithm to be used for structured prediction. alternatively  one could use the following tags as features or use both the preceding and following tags  toutanova et al.  1 . the idea of dynamic features  however  is more general than sequence labeling and maximum conditional likelihood training.
　for dependency parsing  dynamic features can also be easily employed. for example  when considering a possible link label that connects a head word to a subordinate word  one will always have access  in any standard parsing algorithm  to the existing children of the head that occur between the two words under consideration. in this case  the number and types of pre-existing subordinate children are valid features that can be used to predict whether the new head-subordinate link should occur  which turns out to be a very informative feature for link prediction in parsing  collins  1; wang et al.  1 .
　although the idea of using dynamic features is not new in some fields like parsing  collins  1; magerman  1   it is still not as widely appreciated as perhaps it should be. dynamic features are often a trivial way to improve structured predictors without requiring any modification of the underlying training methods. in fact  even still the possibility is not always used in parsing  mcdonald et al.  1   only to yield immediate improvements when subsequently reintroduced  mcdonald and pereira  1 . below we find that simple dynamic features easily improve structured prediction performance.
1 structured boosting
even though dynamic features can significantly improve the structured prediction performance of local training algorithms  local training is still myopic  and the parameter optimization process remains uninfluenced by the global behavior of the structured predictor. in fact  even though dynamic features capture some weak aspects of global prediction  we still expect proper structured training algorithms to yield significant global accuracy improvements in structured problems like parsing. for example  in simpler sequence labeling tasks  it has already been well observed that structured training algorithms  like conditional random fields  lafferty et al.  1   outperform their myopic counterparts  maximum entropy models  mccallum et al.  1   because memm training still fails to consider global prediction accuracy  lafferty et al.  1 . however  by attempting to incorporate the global predictor directly into local parameter optimization  one is inevitably led to design new  complex training algorithms that require standard local training methods to be replaced. the problem we seek to avoid is to complicate the training process in this manner.
　we now introduce our main proposal  structured boosting  that provides a straightforward way to combine global structured prediction  parsing  with local parameter optimization  without modifying the underlying local training algorithm. in fact  the procedure is a trivial variant of standard boosting algorithms  freund and schapire  1; schapire and singer  1; collins et al.  1   altered to incorporate a structured prediction algorithm during the classification phase. the procedure is as follows.
  first  a standard predictor is trained on the local labeled components  as discussed in sec. 1  to produce a  weak  local predictor  e.g. a local link predictor for parsing .
  then the global structured predictor is used to classify the data using the current weak local predictor. for example  a parsing algorithm is used to re-predict the training labels  in a coordinated global fashion  using the learned link predictor as an internal scoring function.
  based on the resulting misclassifications of the structured predictor  i.e. the parser output   the ensemble weight for the current weak local predictor is calculated  and the local example weights are updated  according to any standard boosting method; for example  either exponential loss adaboost  freund and schapire  1; schapire and singer  1  or logistic regression loss boosting  collins et al.  1 .
  the above steps are then repeated for some number of boosting rounds.
the resultingensemble of weak local predictorsthen provides a combined local predictor that can be used for subsequent global structured prediction on composite test examples.
　the advantage of this approach is its simplicity and generality. it can be applied to any standard local training method without requiring any modification of the underlying algorithm  yet via structured boosting  the local learning algorithm is forced to respond to the behavior of the global predictor. in effect  it is a simple training wrapper  where local examples are reweighted  not based on the predictions of a current hypothesis  but instead on the predictions that the local hypothesis forces the global structured predictor to make. below we find that a structured boosting method of this form can improve the quality of dependency parsers learned from treebank data. note that only a few boosting rounds are ever feasible in our application  because each round requires the entire corpus to be re-parsed and the local prediction model re-trained. nevertheless  we still witness some useful improvements and achieve state of the art results.
1 experimental design: dependency parsing
we applied the above ideas for learning structured predictors to the challenging problem of learning a dependency parser from treebank data. in particular  we considered two languages  english and chinese.
　data sets we used the english penn treebank 1  marcus et al.  1  and the chinese treebanks 1  palmer et al.  1  and 1  palmer et al.  1  for our experiments. for english  we converted the constituency structures to dependency trees using the same rules as  yamada and matsumoto  1  and adopted the standard training/development/test split used throughout the literature. the development and test sets were tagged with the part of speech tagger of  ratnaparkhi  1 . for chinese  we used the rules in  bikel  1  for conversion and created the same data split as  wang et al.  1 on the chinese treebank 1 data set  and the same data split as  corston-oliver et al.  1  on the chinese treebank 1 data set. chinese treebank 1 contains chinese treebank 1 as a subset  but adds approximately 1 sentences  1 words  of taiwanese chinese text.
　static features for both english and chinese we used a common set of feature templates. in particular  for the static features  we used the same set of features described in  mcdonald et al.  1   except the  in between pos features . given a target word pair and their context  these static features consisted of indicators of the individualwords  their part of speech tags  and also the part of speech tags of words in the surroundingcontext. in addition to the indicator features used in  mcdonald et al.  1  we also added a distance feature that simply measures how far apart the two words are in the sentence  which is highly predictive of link existence  since most links in a dependency parse are short range.
　dynamic features for dynamic features  we used the number of previous children of a candidate head word  and an indicator of the part of speech  if any  of the previous child word on the same side of the candidate head word. for english  we used one special dynamic feature to try to capture prepositional phrase attachment preference: if a candidate child is tagged pp  then we use a feature that indicates the tag and word of the first grandchild  first child of the child .
　local training for the local training algorithm we used a standard logistic regression model  aka maximum entropy model   hastie et al.  1  to attempt to learn a predictor for one of three word pair labels  no link  left link  right link   given the features described above. to regularize the parameters in the model  we used the linear  non-negativeregularizer described in  goodman  1 . the regularization parameter  λ  was set to 1 in our experiments. this parameter was selected with some tuning on the english development set  and then used without modification on the other data sets. unfortunately  the number of features and number of local examples were both so large that training the logistic regression model  even once  took more than a day. so to accelerate the training process  we employed a further trick: we partitioned the set of local examples  determined by word pairs in each sentence  according to the part of speech tags of the pair. within each equivalence class  the number of features could then be further reduced by dropping those features that became constant within the class. this partitioning dropped the overall training cost to a few hours on a few computers  since the separate partitions could then be trained in parallel. interestingly  the quality of the learned model was not significantly affected by this training procedure. this suggests that the part of speech tags of the word pair  which are used to create the partitions  are the most essential piece of information in deciding the existence and orientation of a dependency link.
　parser there are many dependency parsing algorithms available with differing computational cost. they range from eisner's o n1  time parser  where n is the length of the sentence  to an o n1  time cky parser  mcdonald et al.  1 . the difference between these algorithms has to do with the global constraints they can enforce and the types of features they can use during dynamicprogramming. faster algorithms enforce fewer global constraints and need to use a more restricted class of features. in our experiments  we used a standard cky parser  jurafsky and martin  1 . which allowed us to use all of the features described above  while also enforcing the planarity constraint.
　boosting method we experimented with a few boosting methods  including a simplified variant where the weights of table 1: boosting with static features
iterdaenglish
racmchinese  treebank 1 
	da	ra	cm1.1.1.1.1.1.11111111.1.1.1.1.1.1111111each mis-parsed local example were simply increased by an additive constant  with other weights kept the same  and only the last hypothesis is kept. in fact  in our experiments below we obtain state of the art results just using this simplified procedure  and so we focus on these initial results here. comparisons to standard boosting algorithms  such as adaboost m1  m1  freund and schapire  1  and the logistic regression form of boosting described in  collins et al.  1  are still in progress.
1 results
to determine the effectiveness and generality of our approach we conducted a number of experiments on each of the data sets  english and chinese . these results were achieved using only the simplified boosting procedure mentioned above  additive weight updates  keeping only the last hypothesis .
　first  to determine the effectiveness of the basic structured boosting idea  we started with a simple local prediction model  static features only  and measured parsing accuracy on the held out test set as a function of the number of boosting rounds. table 1 shows that parsing accuracy is improved in each round of boosting with static features  on both english and chinese  using chinese treebank 1 . to explain the improvementsmore carefully  note that da  dependency accuracy  indicates how many word pairs are correctly linked; ra  root accuracy  measures how many sentence roots are correctly identified; and cm  complete match  is the number of sentences where the entire tree is correct. our focus in this work is on improving the dependency accuracy scores  da   rather than the root accuracy and complete match scores. this fact is reflected in the boosting procedure  since instance reweighting is based only on whether each candidate link is predicted correctly  not whether the root is labeled correctly  nor whether the complete sentence is matched correctly.
　not surprisingly  table 1 shows that the dependency accuracy  da  improves on each round of boosting for english  and improves on most rounds  and improves overall  for chinese; while the ra and cm results fluctuate somewhat. note that although the improvements appear small  the observed da differences are all statistically significant. for english  the test corpus consists of 1 instances  word pairs occurring in a sentence   and differences of 1 in the percentages shown in the tables are statistically significant with greater than 1% confidence. for chinese  the test corpus consists of 1 instances  and differences of 1 in the percentages shown in the tables are statistically significant with greater than 1% confidence.
second  to determine the effectiveness of dynamic featable 1: boosting with dynamic features
iterdaenglish
racmchinese  treebank 1 
	da	ra	cm1.1.1.1.1.1.11111111.1.1.1.1.1.1111111tures  we added these additional features to the local prediction model and repeated the previous boosting experiment. table 1 shows a significant further improvement in parsing accuracy over just using the static features alone  table 1 . once again  however  boosting provides further improvement over the base model on both english and chinese with respect to dependency accuracy. in each case the improvement is significant.
　finally  we compare the final results we were able to achieve to the state of the art. table 1 shows the best results achieved by our method and other researchers on english and chinese data. once again  all of the results on english are obtained on the same standard training and test set splits on the english penn treebank 1  marcus et al.  1 . the results on chinese are obtained on two different data sets  chinese treebank 1  palmer et al.  1  and chinese treebank 1  palmer et al.  1  as noted. in the table  y&m1 refers to  yamada and matsumoto  1   n&s1 refers to  nivre and scholz  1   wsl1 refers to  wang et al.  1   mira1 refers to  mcdonald et al.  1   mira1 refers to  mcdonald and pereira  1   bpm1 refers to  corstonoliver et al.  1 . from table 1 we can see that on english  the results we are able to achieve through the simple boosting method are competitive with the state of the art  but are still behind the best results of  mcdonald and pereira  1 . however  perhaps surprisingly  table 1 shows that the technique we have proposed in this paper actually achieves state of the art accuracy on chinese parsing for both treebank collections.1
　computationalcomplexity clearly  there is some computational overhead associated with training by boosting  since each round requires the base learning algorithm to be retrained on the re-weighted training data. the training cost scales up proportional to the number of boosting iterations however  and reasonable improvements can be achieved with a small number of rounds. interestingly  we have found for test complexity  the computational cost of using a composite hypothesis for scoring the local predictions does not add much overhead to the parsing complexity  although we report only single hypothesis results here .
table 1: comparison with state of the art
modeldaenglish
ra	cmdachinese
racmy&m1.1.1.1---n&s1.1.1.1---wsl1---1*--mira1.1.1.1---mira1.1-1---bpm1.1.1.1.1 1 1 our1111 1 1 model1*1*1*  obtained with chinese treebank 1 using the data split reported in  wang et al.  1 .
  obtained with chinese treebank 1 using the data split reported in  corston-oliver et al.  1 .
1 conclusion
we have addressed the problem of learning structured predictors by using a simple form of boosting to augment the training of standard local predictors. the procedureis general  and allows one to improve performance at structured prediction without modifying the underlying training algorithm  nor implementing a complex training algorithm. further improvements in structured prediction accuracy are easily obtained by using dynamic features that consider the labels of some neighboring examples. although our results are very promising  and in fact provide the new state of the art result in chinese parsing  there remain many directions for future work. one obvious direction is to investigate the effect of using alternative boosting algorithms  and also to investigate the theoretical nature of applying these algorithms to the structured boosting case: under what circumstances do the algorithms converge  and what guarantees can be made about their performance. we would also like to explore further ideas about useful features for parsing  and additional smoothing and regularization techniques for local training.
1 acknowledgments
research supported by the alberta ingenuity centre for machine learning  nserc  mitacs  cfi and the canada research chairs program.
