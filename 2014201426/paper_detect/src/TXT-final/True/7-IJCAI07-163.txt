
error bounds for decision trees are generally based on depth or breadth of the tree. in this paper  we propose a bound for error rate that depends both on the depth and the breadth of a specific decision tree constructed from the training samples. this bound is derived from sample complexity estimate based on pac learnability. the proposed bound is compared with other traditional error bounds on several machine learning benchmark data sets as well as on an image data set used in content based image retrieval  cbir . experimental results demonstrate that the proposed bound gives tighter estimation of the empirical error.
1 introduction
computational learning theory  colt  deals with the characterization of the difficulty of learning problems and the capabilities of machine learning algorithms  vapnik  1; vidyasagar  1 . the probably approximately correct  pac  framework is formulated to characterize classes of hypotheses that can be reliably learned from a reasonable  polynomial  number of randomly drawn training examples with a reasonable amount of computation. sample complexity specifies the number of training examples needed for a classifier to converge  with a high probability  to a successful hypothesis. within the pac framework  it is possible to derive bounds on sample complexity by combining the expression for the size of the hypothesis space and the empirical error  mitchell  1 . thus characterizationof the hypothesisspace is an important issue in colt.
모in order to understand the representational capability of various classifiers  we ran several algorithms such as perceptron  linear and polynomial support vector machines  svm   bayes maximumlikelihood ml  decision tree  dt   oblique decision tree  and k-nearest neighbor  knn  classifiers on the standard iris benchmark dataset. iris is a four-dimensional dataset comprising three classes  of which two classes are not linearly separable. figure 1 shows a two-dimensional projection of the decision regions described by the trained classifiers on the benchmark dataset. perceptron  linear svm and dt define elementary decision regions comprising linear and axis-parallel rectangular surfaces. oblique dt  polynomial svm  bayesian ml and knn describe increasingly more complex decision regions and hence have richer hypothesis representation capability. it is to be noted that although limited empirical characterization of classifiers is possible as depicted in figure 1  theoretical characterization of classifiers  in general  is difficult. in this paper we shall consider dts for further theoretical and empirical analysis.
모several techniques have been proposed for estimating the future error rate of a decision tree classifier  kaariainen and langford  1; mansour  1 . there are two primary classes of error estimation methods available. the first class of methods utilize the empirical error on the training samples in predicting the future error. empirical error could be based on the training set or on the test set or on both. for example  we can use k-fold cross-validationon a dataset and then transform the cross-validation estimate into an estimate or heuristic confidence interval of the error of the final decision tree learned from all examples. another approach is the sample compressionboundwhich considers the test set after labelling it  that is  it estimates the next label based on the training set and the labelled previous test set  langford  1 . there are several other methods such as microchoice bound  langford and blum  1   test set bound and occam bounds  langford  1 . microchoice bound inherently depends on the structure of the dt by calculating the choice spaces at every node of the dt. test set bound is a test set-based bound and is entirely characterized by the errors on the labelled test set. as the test set bound incorporates test set error directly  its estimate is usually good. occam bound assumes the underlying distribution to be binomial and computes an estimate based on the empirical errors observed on the training dataset.
모the second class of methods utilize structural aspects of the classifier in order to arrive at an estimate. for example  in the case of dts  one could consider the depth or breadth of the dt constructed on the training samples in the estimation of future error.

figure 1: decision regions of classifiers: comparison of the hypotheses learned by various classification methods on the iris모in this paper we propose a structure-based error bound for dts using the pac learning framework. the new error bound considers both depth and breadth of the dt learned from training examples. we conducted several experiments on benchmark datasets to compare the results of various error bound estimation methods and found that the proposed estimation method works well. the rest of the paper is organized as follows. firstly  we describe both the structure-based and machine learning benchmark dataset.
empirical error-based estimation methods. experimental results are presented subsequently. discussion of the results is followed by conclusions.
1 classifier structure-based error bounds
in this section  before introducing classifier structure based bounds  we will describe pac learning in detail and explore how generalization bounds could be formulated based on pac learning framework.
1 pac learning and error bounds
in this section we review the relation between probably approximately correct  pac  learning framework and generalization error bounds  valiant  1; mitchell  1 . pac tackles the questions related to the number of examples and the amount of computation required to learn various classes of target functions. pac learning framework has two major assumptions. one that assumes that the classification error is bounded by some constant  붼  that can be made arbitrarily small. the second assumption requires that the classifier's probability of failure is bounded by some constant  붻  that can be made arbitrarily small. in short  we require that the classifier probably learns a hypothesis that is approximately correct - hence it is called the probably approximately correct  pac  learning framework. defining the error on entire instance distribution  t  of data as true error  errort   and the error on training set  d  as empirical error  errord   pac learnability can be defined formally as follows.
모definition  mitchell  1 :	consider a concept class c defined over a set of instances x of length n and a learner l using hypothesis space h. c is pac-learnable by l using h if for all c  c  distribution t over x  붼 such that 1  
  and 붻 such that   learner l will with probability at least  1   붻  output a hypothesis h 뫍 h such that errort h  뫞 붼  in time that is polynomial in 1붼  1붻  n  and size c . this can also be represented mathematically as follows.
p errort h    붼  뫞 붻
	 or 	 1 
p errort h    붼  뫟  1   붻 
모pac-learnability concept can be used to derive a general lower bound on the size m of the training set required by a learner. there are two possible scenarios. in the first scenario  we assume that the learner is consistent  that is  the learner outputs hypotheses that are consistent with the target concept on the training set. the probability that a single hypothesis having true error greater than 붼 and is consistent with the training set is at most  1   붼 . and the probability that this hypothesis will be consistent with m individuals is at most  1 붼 m. if  instead of one  there are k such hypotheses whose true error is greater than 붼 and are consistent with the training set  then the upper bound on the probability that at least one of these k hypotheses will be consistent with all the m randomly drawn individuals is  k  1 붼 m. using the fact that k 뫞 |h|  we can write down the following inequality in equation 1.
	p errort  h    붼  뫞 h    1   붼 m 뫞 붻	 1 
모we can rewrite this as shown in equation 1 and solving for m  the number of training examples needed by the learning algorithm  results in the inequality shown in equation 1. p errort  h    붼  뫞 h   e  m붼  뫞 붻	 1 
		 1 
모in the second scenario when the learner is agnostic  that is  the true error on the training set is not necessarily zero  then we can use chernoff approximation to estimate the error bound in the pac learning framework  mitchell  1 . analogous to equation 1  the error bound for agnostic learner can be derived and is shown in equation 1.
		 1 
모now  we can use equation 1 to derive the generalization error bound as shown in equation 1.
		 1 
모we can write this in terms of true error  errort h   and training error  errord h   as shown in equation 1.
errort h  뫞 errord h  +
모coming back to the original problem of deriving error bounds for dts  equation 1 can be used if only we know the size of the hypothesis space  |h|. thus for pac based error bounds  we need an estimate of |h|. there are three ways of estimating |h| for dts. the first approach is to estimate based on the depth of dt  the second method is to estimate based on the breadth of dt  and the third possibility is to use both of these measures. we propose a bound based on the third approach wherein recursive estimation of |h| is done. in the subsequent sub-sections  we describe the three structure-based error bounds for dts.
1 depth-based error bound for dt  pac i 
computation of |h| is based on depth of the tree and the approach is akin to the one shown in the lecture notes of guestrin  guestrin  1 . we assume that the dts are binary trees and use the idea that every k-depth decision tree will have  k   1 -depth decision trees as its children.
approximation of |h| for pac i
given n attributes 
hk=number of decision trees of depth kh1=1hk+1= choices of root attribute  
 possible left subtrees  
 possible right subtrees hk+1=n   hk   hkhk=1k   n1k 1if lk=log1hk and l1 = 1lk+1=log1n + 1lkso  lk= 1k   1  1 + log1n  + 1 1 
모if we substitute this intoequation1  then we get the expression for error bound for decision tree using the depth feature as shown in equation 1.
errort h  뫞 errord h +

1 breadth-based error bound for dt  pac ii 
in this method  computation of |h| is based on breadth of the tree and the approach is akin to the one shown in the lecture notes of guestrin  guestrin  1 . we assume that the dts are binary trees and use the idea that every k-leaves decision tree will have  k   1 -leaves decision trees in it.
approximation of |h| for pac ii
given n attributes 
	 1 
모the last step in equation 1 is a rough  crude  approximation to hk from the previous step. if we substitute this into equation 1  then we get the expression for error bound for decision tree using the breadth feature as shown in equation 1. the crude approximation will be replaced by a better approximation obtained by recursive estimation in pac iii . errort h  뫞 errord h +

1 depth and breadth-based error bound for dt  pac iii 
we propose this new approach for calculating error bounds for dts. this approximation is very much similar to pac ii although the resulting bound will be tighter as we will show empirically. the simple idea behind this approach is that it is important to consider several structural features of the dt to get a closer approximation of |h| which in turn leads to a tighter estimate for the error bound.
approximation of |h| for pac iii
given n attributes 

 1 
모we have to substitute the expression for h into equation 1 to get error bound for dt using this approximation.
1 empirical error-based bounds
in this section  we will describe generalization bounds formulated based on the empirical error observed on the dataset. unlike the structure-based error bounds discussed in the previous section  bounds reviewed in this section explicitly incorporate empirical errors observed on the datasets - training and test sets.
1 microchoice bound
microchoice approach tries to estimate an a priori bound on the true error based on the sequential trace of the algorithm  langford and blum  1 . when a learning algorithm is applied on training set of n instances  the algorithm successively makes a sequence of choices c1 ... cn from a sequence of choice spaces  c1 ... cn finally producing a hypothesis  h 뫍 h. specifically  the algorithm looks at the choice space c1 to produce choice c1. the choice c1 in turn determines the next choice space c1. it is to be noted here that at every stage the previous choices are eliminated from consideration at the next stage. the algorithm again looks at the data to make the next choice c1 뫍 c1. this choice then determines the next choice space c1  and so on. these choice spaces can be thought of as nodes in a choice tree  where each node in the tree corresponds to some internal state of the learning algorithm  and a node belonging to some choice space ci.
모we can apply this microchoice bound to decision tree by taking into account the choice spaces available at every node of the decision tree and then using pac bound technique to get the error bound as shown in equation 1.

 1 
1 test set bound
in this bound assumptions about the error distribution are made. in particular  classification error distribution is modelled as coin flips distribution or the binomial distribution as shown in equation 1.
		 1 
모the expression in equation 1 computes the probability that m examples  coins  with error rate cd producek or fewer errors. we can interpret the binomial tail as the probability of an empirical error greater than or equal to mk . but we are interested in error bound for a classifier given the probability of error붻 and m  so we define binomial tail inversionas given in equation 1 which gives the largest true error such that the probability of observing or more errors is at least 붻.

모now  given the number of test instances m  test set bound can be formulated as shown in equation 1.
test set bound :

     p errort h  뫞 bin errortest 붻   뫟 1   붻  1  one serious drawback of the test set bound is that it is possible that the test set and training sets are incompatible and thereby introduce inaccuracies in the error bound estimation  kaariainen and langford  1 .
1 occam's razor bound
this is can be termed as a training set-based bound as it takes the training set performance into consideration. this is reasonable as many learning algorithms implicitly assume that the train set accuracy behaves like the true error. additionally  in this bound we need to know the prior probability of the hypotheses.
occam's razor bound:
for all priors p  h   over all classifiers h  for all 붻 뫍  1  

	p errort h  뫞 bin errord h  붻p  h    뫟 1   붻	 1 
모we obtain the occam's razor bound by negating the above equation 1.

	p errort h  뫟 bin errord h  붻p  h      붻	 1 
모it is very important to notice that the prior p  h  must be selected before looking at the instances. we can relax the occam's razor bound with the entropy chernoff bound to get a somewhat more tractable expression  langford  1 .
chernoff occam's razor bound:
for all priors p h   over all classifiers h  for all 붻 뫍  1  

모the application of the occam's razor bound is somewhat more complicated than the application of the test set bound.

table 1: results of experiments for error bound calculating on 1 different data sets.
data setsp i p ii p iii mcocctestempattbdeweather1.1.1.1.1.1.1.111yellow-small-1.1.1.1.1.1.1.111adult-1.1.1.1.1.1.1.111adult+1.1.1.1.1.1.1.111yellow-small+1.1.1.1.1.1.1.111contact lenses1.1.1.1.1.1.1.1111labor1.1.1.1.1.1.1.1111monk11111111111monk111111111.1.1.1monk111111111.1.1.1voting-records1.1.1.1.1.1.1.1111crx1.1.1.1.1.1.1.1111tic-tac-toe1.1.1.1.1.1.1.1111segment1.1.1.1.1.1.1.1111cbir1.1.1.1.1.1.1.111s: size; p i : pac i ; p ii : pac ii ; p iii : pac iii ; mc: microchoice; occ: occam's razor; test: test set; emp: empirical error; att: number of attributes; b: average breadth of dts; d: average depth of dts; e: average count of misclassification errorsfigure 1: example dt: corresponding to adult- dataset where breadth and depth are 1 and the quantity in parenthesis denotes the size of the choice space at every node.
1 illustrative example

                                              1  here we consider an example experimental dataset  namely  the adult- dataset. the dt structure obtained from one of the ten experiments on adult- dataset is chosen to illustrate how various error bounds are computed. in this case the breadth and depth are one each and the choice space size at each node is also indicated in figure 1. the number of attributes  n  is 1.
모the h value computed for various error bounds in equation 1 are now substituted in equation 1 to obtain the final theoretical estimates of the error bound under different schemes. taking m = 1 and 붻 = 1  we obtain the following values for error bounds: pac i  = 1; pac ii  = 1; and pac iii  = 1. for the microchoice bound  we need to take equation 1 and substitute the choice-space sizes from the dt in figure 1. then the microchoice error works out to be 1.
1 empirical results
we have done experiments on 1 different machine learning benchmark datasets from the uci repository. the results reported are averaged over 1 experiments conducted on randomly chosen subsets of the datasets. training was done on 1% of the data and testing on the remaining 1%. in the case of image dataset from content based image retrieval  cbir  system  due to the complexity of the experiment only one run was conducted. the results are summarized in table 1. table reports the observed error rates  on training set and test set  as empirical error. the goal of the experimentation is to see if the theoretical bounds estimated from various methods come close to the observed empirical error. the bold-faced entries in table 1 correspond to the best estimate of the true error. from the table  as expected we can see that pac iii  always gives better estimate than pac i  and pac ii . depth based measures overestimate the size of the tree since they assume a complete binary tree and in reality  the dt may be far from complete. thus pac i  estimates will be inferior to those of pac iii . this can be clearly observed in the table where the pac i  estimates become worse with increasing depth. in pac ii   breadth of dt is considered. however  since a crude approximation of the actual value of h is considered in pac ii  as discussed in section 1  pac ii  does not give a better estimate compared to pac iii .
모from table 1 we can also observethat pac iii always outperforms the microchoice and occam's razor bounds. microchoice bound is an estimate of h based on choice spaces available at every node of dt and the choice space sizes can lead to overestimation of h. test set and occam's bounds rely on binomial distribution based measures which take total number of instances and observed errors into consideration. conceptually  it is not straight forward to compare pac iii  and binomial theory based bounds. however it can be observed that whenever empirical error is low  we tend to get good estimation from occam and test set bounds. test set bound performed well in five out of the fifteen experiments whereas pac iii  fared well in the remaining 1 experiments. it appears that in the majority of these five cases  empirical  test  error was low and the average breadth of the dt was high. these may be the possible reasons for poorerestimation by the pac iii  approach. further  it is easy to see that these empirical results seem reasonable from theoretical considerations also when different expressions for |h| are compared for the three pac-based bounds.
모results in table 1 suggest a possible subset relation among the various error bounds studied in this paper. pac i  뫟 pac ii  뫟 pac iii 
micro choice 뫟 pac iii  occam 뫟 pac iii 
1 conclusion
in this paper  we proposed a bound for error rate that depends both on the depth and the breadth of a specific decision tree constructed from the training samples. pac leaning framework is used to derive this bound. the proposed bound is compared with other traditional error bounds on several machine learning benchmark data sets and on an image data set. experimental results demonstrate that the proposed bound gives tighter estimation of the empirical error. the bound we have obtained here is considerably tighter than previous bounds for decision tree classifiers. we arrived at a possible subset relations among various structure-based and empirical error-based bounds.
