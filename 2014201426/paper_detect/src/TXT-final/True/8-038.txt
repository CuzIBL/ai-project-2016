 
     this paper presents and describes a pattern recognition program with a relatively simple and general basic structure upon which has been superimposed a rather wide variety of techniques for learning  or self-organization. the program attempts to generalize n-tuple approaches to pattern recognition  in which an n-tuple is a set of individual cells or small pieces of patterns  and each n-tuple is said to characterize an input pattern when these pieces match it  as specified. 
     the program allows n-tuples to match when only some of their parts match  and it allows these parts to match even though they are not precisely positioned  see uhr  1b  for some simple example programs . it further learns  in a variety of ways: it searches for good weights on its characterizers' implications  byre-weighting as a function of feedback. it generates and discovers new characterizers  and can therefore begin with no characterizers at all   and discards characterizers that prove to be poor  see uhr and vossler  1  and prather and uhr  1 . it also uses a set of characterizers of characterizers  to search for good parameter values that newlygenerated characterizers should have. 
     a detailed flow-chart-like  precis  description of the program is given  along with an actual listing. it is thus possible to examine exactly what the program does  and how it does it  and therefore to see how a wide variety of learning mechanisms have been implemented in a single pattern recognition program. but because it was coded in a  high-level  patternmatching and list-processing language the program runs too slowly for extensive tests to be practicable. therefore only a brief listing of output is given  to show that the program  works and begins to learn. 
descriptors: learning  self-organization  induction  discovery  pattern recognition  learning to learn  n-tuple recognition  characterizing characterizers. 
introduction 
     programs that have used n-tuples as their characterizers appear to perform with the very best of pattern recognition programs  for discussions  see uhr  1  1a; for a good recent example  see andrews  atrubin  and hu  1 . this is not surprising  for n-tuples are easily handled by the digital computer. 	and although they may appear simple  any possible characterizer can be described as a sufficiently complex and detailed n-tuple. what we don't know is whether the n-tuple description of sufficiently powerful characterizers would avoid being overly cumbersome and ridiculously wasteful of storage space and processing time. 
     programs that use n-tuples either have them designed by human beings and pre-programmed in  e.g.  andrews  atrubin  and hu  1   or randomly generate a fixed set of fixed-n-size ntuples  e.g.  bledsoe and browning  1 . an interestingly simple generalization of this is the following: let the program begin with no characterizers  but have it generate new characterizers that are as simple as possible  and only when needed. thus the program might start by generating one 1-tuple  continue generating more 1-tuples as it finds itself continuing to choose wrong names to assign to input patterns  and at some point begin generating n+1-tuples. it further should be assessing how well each characterizer is working  by in effect conducting a running experiment that examines its successes and failures. this information should be used a  to weight the importance of this characterizer1 s implications in combining them for the decisions as to names to choose  b  to decide whether a characterizer is good and should therefore be used  or is bad and should therefore be discarded  to be replaced by another  and c  to gather information about general types of characterizers  so that new characterizers are generated that are similar in important parameter values to characterizers that have proved themselves good. 
     this paper describes a program that is a first approximation to this simple  but hazy  scheme of generating as few characterizers as needed  keeping them as simple as possible  but using what has been learned about characterizers to direct the generation of new characterizers  so that they will be similar in their characteristics to good characterizers that have been generated in the past. 
     the program has a second general purpose to puch deeper into techniques for learning characterizers. 
     the basic structure of this program seems to us extremely simple - the generation  when needed  of the best new specific n-tuple of the best general type possible  and the learning of as much as possible. 	but when the program is described or given in detail  as in the following 

- 1 1 -

pages  it inevitably sounds more complex - for indeed it is more complex when forced to the level of code for a discrete digital computer. in order to get flexibility into our n-tuples so that they need not be precisely positioned and can be considered to match even though all parts do not always match  extra details must be added to the code. these in turn suggest additional learning mechanisms that w i l l search for good values for this allowed wobbling and threshold matching. 
　　　there are also several points at which we simply evade quite subtle decisions that should be made by the program: should the program spend more time adjusting the weights of its present set of characterizers  or should it generate one or more new characterizers   this we handle by having the program generate one new characterizer per pattern  up to a fixed maximum  also discarding characterizers found to be bad  to make room for more . when the program generates a new characterizer  should it be of the same size n  or of size n + 1  this we handle by treating n-size as just another parameter  so that  as described below  the program w i l l choose the n for a new tuple as a function of the goodness of the tuples of different n-size that have been generated so far. thus n is initialized to equal 1; the program w i l l keep tabs on the goodness of each n-size and w i l l generate tuples with an n-size that reflects this goodness  but with some probability will occasionally generate a new tuple of size n + 1. this procedure is used for all parameters of characterizers. 
precursors 
     as an introduction to the structure of our program  let us consider the bledsoe-browning pattern recognition program  1   which was among the first to use n-tuples randomly selected from the input grid to recognize typed or handwritten characters. 	for each n-tuple  the possible pattern names having the same state 
as the unknown input pattern are added into a comparison tally. 	after all tuples are considered  the name that matches the unknown pattern most closely   i . e .   having the highest sum of same-state n-tuples  is chosen as the name of the input pattern. 
     using the string manipulation language snobol  uhr  1b  coded a somewhat extended version of the bledsoe-browning program. uhr's short program uses weighted implications  rather than merely tallying them  and it allows varying sizes for the n-tuples and for the individual pieces of the n-tuples. 
     there are several weaknesses in this type of program: it does not learn  so its performance remains only as good as the n-tuple characterizers it starts with. 	n-tuples are r i g idly positioned  and must match exactly and entirely. 
a basic n-tuple pattern recognition and learning program 
　　　let's try to generalize the basic n-tuple program. for example  the characterizer tuples w i l l be looked for one part at a time  instead of all at once. each tuple piece of the characterizer n-tuple w i l l contain pertinent information about its expected location within the pattern grid  its size  and the specific configuration that should be found. optionally  a tuple part w i l l have no particular position specified  signaling the program to look anywhere  presently meaning from its current position on down  for this tuple part. if a characterizer is matched  its implied pattern names are put on a list of found implications. when the same name is implied by several characterizers  the separate weights of implications are added together. the tuples are allowed to be non-exclusive  so that grid points in important locations  such as  perhaps  the left edge of the grid  may reappear in several characterizers. 
　　　this basic program will also have the a b i l i ty to learn from its experience  by comparing its chosen answer with the feedback giving the correct pattern name. if the program gave the right answer  the memory is left as i s   since it produced satisfactory results. but a wrong answer calls for reweighting of implications in the characterizers whose tuple configurations were matched. the weights of implications of the wrongly chosen name are decreased  and implications of the feedback name are increased. if the answer was wrong the program will also generate a new characterizer using this wrongly named input pattern. to do this  a random ntuple is extracted  n is chosen to reflect the distribution of weights attached to the generated values of n  from the input and assembled into a characterizer which implies the correct feedback name. each run has an upper limit to the number of characterizers generated  to prevent saturation of memory or unnecessary slowing of processing time. poor characterizers are d i s carded  making room for new ones  when the weights of all their implications fall below a minimal acceptable level. 
characterization over variations 
     presented with only standard  non-varying instances  in a single type font  perhaps  of its repertoire of patterns  it is no great problem for a pattern recognition program to learn to recognize a set of characters. but if patterns can vary  even slightly  in position or shape from time to time  then problems mushroom. our 

-1-

program tries to handle this in several ways. 
wobbly patterns 
     each part of a tuple is allowed to wobble a given horizontal distance to either side.  a somewhat more limited capability for handling vertical wobbling is the  anywhere  search mentioned previously  plus the fact that all tuple part addresses are given relative to the last position  wherever that may be. uhr  1b  presents programs that also allow vertical wobble.  in each characterizer tuple part there is an explicitly given wobble which tells the program just how big a hunk of the grid row it can look in for the desired configuration. this allowable wobble may vary from tuple piece to piece  as learning has indicated was needed for good performance. thus if a desired configuration was not found within the specified wobble  but would have been found were the wobble slightly larger  then the program remembers how it almost found this characterizer. when feedback shows it chose the wrong name  if the program finds that this almost-matched characterizer would have implied the right answer  it increases the wobble allowance to improve performance . 
threshold characterizers that can partially match 
     suppose that three parts of a 1-tuple were found  but the other part was not. we would like to allow use of the implications of this nearly matched characterizer even though the program did not find a perfect match. in order to do this the program uses threshold matching  where each part of a tuple has its own weight to add into the tuple's sum of  foundness.  each implication of the characterizer is preceded by a threshold requirement which must be met by the tuple sum before the implication may be merged into the list of possible pattern names. thus one implication may require all but one part of the tuple's configuration to be found  
where another implication of the same characterizer might require a perfect match of all parts. 
compound characterizers 
     besides having a primitive sort of tuple consisting of a set of 1 configurations to be looked for at certain points on the pattern grid  our program can also use compound characterizers  where one or more of the tuple parts is itself the name of another characterizer. the program looks in the stated position  or else 'anywhere   for the name of the desired component characterizer and treats this tuple part just as any other. now the program must add the names of found characterizers to the input that it is processing. 
     compound characterizers are currently generated from primitive characterizers that are on the list of characterizers found for this input. there must be two or more such component characterizers in order to generate a compound characterizer. when the program decides to generate a compound characterizer  it chooses the maximum number of parts to give the tuple. then the parts are pulled off the list of characterizers found in this input  and the characterizer is assembled as initially implying only the feedback. 
     these compound characterizers are more general than primitive characterizers in that a more sophisticated set of pattern characteristics can be represented by one tuple. indeed  with compound characterizers we approach a method for learning stroke or feature recognition  where primitive characterizers might represent the various primary curves and lines  and the compound characterizers could form the desired combinations of strokes to imply the various patterns. for example  if char1 is the tuple describing a small open-left curve  char1 is a long vertical line  and char1 is a large open-left curve  then char1 compounding char1 and char1 could imply the pattern  p  and char1 coupling char1 and char1 could imply  d . 
parameters that characterize characterizers 
     an important part of learning in humans is generalization. in order to enable our program to  in effect   generalize  on what it has learned and thus perform better  we have given it an expandable set of parameters or characterizer traits. for each trait  such as the number of parts  or their closeness  or their maximum horizontal spread   a value can be computed for every characterizer. with every characterizer there is associated a list of this characterizer's value for each trait. in addition we keep a common traits list of all traits and all values that have been generated and used for them. a weight is associated with each value for each trait. for example  suppose the program gives a wrong name for an input pattern on the basis of found characterizer n. then after the implication weight of the wrong name in char n is decreased  the program goes through the trait list of char n and  for every trait  downweights char n's value for that trait in the common trait list. in particular  if char n's value for vertspred  vertical point spread  is   1     then our program will look for the value   1   under the trait vertspred in the general characterizer traits list  and decrease the  goodness weight  of the value   1   . 
     when upweighting a good characterizer's trait values  the program also enters  if not 

-1-

already there  on each trait value list a slightly larger parameter value. in this way it broadens the range of parameter values that will be used to generate new characterizers. the value and goodness weight information in the general traits list is used when a new characterizer is generated. the program tries to generate the new characterizer tuple within the framework of 
what the program has already learned; currently it uses the three traits necessary to control the basic generation  tuple size  piece size  wobble  plus a fourth chosen randomly from the 
other possible traits  currently  these are horizontal spread  vertical spread  average closeness of parts  number of parts on the edge of the grid  and compound or primitive . a desired value for the new characterizer is chosen 
with a probability that reflects the weights associated with the various possible values of the 
trait. 	the program tries several times to find a randomly positioned tuple which will have this same value. 	thus the program generalizes on 
what it has learned  in that if a value of  1  for vertspred has been upweighted several 
times  the program may decide that this is a good value to try for in a new characterizer. 
 for further details  see functions traitwt and probchoose and the section labeled primitive in the precis and the code.  
the complete program 
     the preceding sections describe independent features  any or all of which could be added to a basic learning program to create a complete program. the final program containing all the features is described at the end of this section. as might be expected  the characterizers for this final program have become fairly complex. 
as an example  detailed description of program 
*precis for ntuple 
learning program. 
initize 
initialize memory  can be null   any characterizers and their trait lists  and the general characterizertraits value list  
par 
initialize parameters  values for 
	increment  	decrement  initial 
threshold  initial weight  good  bad  probability of compound characterizer generation   edgedots to allow     wobbling  room around i n -
put pattern. 
in 
read in the matrix  row by row  putting an edge on each side to 
allow for the maximum current wobble  and maintaining the current columnsize. 
read in the feedback  marked by 
'***'  and any parameter change  marked by ' $ '     if given. if no more cards  go to end. 
recognize 
initialize found implication list  foundcharacterizers list  a copy of memory  rowsize gridsize  
rl 
blank out implist of implications whose threshold requirements are met. 
get the next characterizer and its cnumber from mem. if no 
more characterizers  go to deny. 
get the description of char  its statement 
number 
1 
1--1 
1--1 
1--1 
1 
1--1 
1--1 charo implied patterns  and the com-
pounds that char is part of. 
	r1 	1--1 
means the following: 
 description=at row 1  column 1 look in the next 1 position for the string  1   adding 1 to the tuple sum of weights on success; 1 rows down and 1 cols  over look in the next 1 positions for the string   1     adding 1 to the tuple pick off this piece  its weight  and its position from descr. if no more parts  go to r1. 
rr1 
if pos includes relativedrow and 
dcol numbers and a mask size  1--1 sum on success/lmplications=if sum 1 then compute the absolute drow locaimply i with weight 1; if sum 1 imply t with tion and go to rr1 to look for 
weight l/charo is ♀art of the compound char1/trait list name=tro/last tuple part's absolute address is row 1  col. 1 /   . 
     an outline of the program's operation follows. this positioned piece. 
rr1 
otherwise look 'anywhere' for 
this  starting at the current begin row. 	if find this  go to r1r. 1--1 -1-

statement 
number deny 
erase from foundchars any characterizers whose implications are denied by a compound characterizer. 
imply 
merge all the remaining implications from foundchars into a found list. 
choose 
choose as hiname the name on found with the highest weight. 
out 
print out hiname. 
if there was no feedback or if hiname was right  go to i n . 
otherwise  answer was wrong. 
reweight 
pick off the next characterizer from the fcopy of foundchars. if no more chars  go to adjust wobbles. 
if the wrong hiname is implied by this char  downweight the implication or erase it if the reduced weight is bad. 
c1 
for each trait  give this bad 
characterizer's trait value a decrement on the main character traits list. 
if all implications are erased for this char  erase it from memory and go to reweight. 
c1 
if feedback was not implied by this found char  add it to implications and go to reweight. otherwise  
c1 upweight the implication of fbk in char. for each trait  give this good char's value an increment on the main chtraits list. 
go to reweight. 
adjust 
pick off the next characterizer which was almostfound. if no more  go to generate a new characterizer. 
if this characterizer impued fbk but not the wrong hiname  enlarge the mask for the parts 
which would have given a match. 
make sure this characterizer*s trait list contains the maximum wobble value of its parts. statement number 
1 
1 
1 1 
1 
1 
1 
1 
1 1 
1 
1 
1 r1r 
if the anywhere search fails  make the next tuple part apply anywhere  too  by erasing its position . go to r1. 
r1r 
set this row as the begin row for future search. 
if this is followed by bcolumn and weight information at the right  we are working with a compound characterizer; go to rr1. 
otherwise compute the begincol for future search and go to r1. 
rr1 
if the weight of this piece meets the threshold weight requirement for this compound characterizer part  add the weight to the sum of implications. go to r1. 
rr1 
finish computing the absolute 
dcolumn position to look at for 
this piece. 	look first for this as a compound tuple part with its dcolumn and weight information; if don't find it  go to rr1. 
if dc was within the wobble allow- ance  reset brow and bcol  add wgt to sum  and go to r1. otherwise  if a little bigger wobble allowance would have given a match  record this nearmiss. go to rr1. 
rr1 
look for this positioned primitive tuple part. if no match  see if a little more wobble gives a nearmiss. 
rr1 
set new brow and bcol for next search and go to r1. 
r1 
make implist of implications whose threshold weight requirements were met. 
r1a 
record nearmisses on almostfound characterizers list. 
r1 
if implist is not empty put on foundcharacterizers this char and its information. 1 
1 
1 
1 
1 
1 1 
1 
1 1 
1 
1 if char is part of a larger compound  1 mark it found in the input matrix and put the compounds on mem to look at later. go to r l . -1-

statement 
number statement 
number generate 
if there are already enough characterizes   i . e . no more togenerate   go to i n . 1 get the number of the new charac- terizer and decide if it should be compound. 	if not  go to primitive generation. 1 compound 
choose the number of parts the new characterizer should have. make an fcopy of foundchars to choose from. 1 cgi 
get the next found characterizer from fc. if no more  go to cg1. keep tup count of how many parts are got from fc. 1 get the lastpart location of ch and insert it in order into the description of the new compound characterizer. 1 add to the new implication list the denial of this component 
	characterizer's 	implications. 1 keep a list of the primitive parts in order to later insert this new characterizer in their compounds l i s t s . 1 keep the largest wobble value of any part of the compound stored as wbl. 1 keep the largest piece size of any part of the compound stored as pc. 1 cg1 
if there were less than 1 components in fc  go to primitive. 1 cg1 
assemble the final description with relative positions  noting the lastpart. 1 initialize trait list for new com- pound characterizer. assemble the characterizer. 1 cg1 
mark each component characterizer as part of this compound characterizer. go to g1. 1 primitive 
make a rearranged copytraits of chtraits so as to cycle 
through traits used to influence primitive tuple generation. 1 according to the value probabili- ties in copytraits  choose the necessary values for characterizer 1 generation  tuple size  piece size  wobble . initialize the new trait list with these values. 1 get a desired tryvalue for another trait. w i l l try to generate a tuple with the same value. 1 gentup 
create a relatively-ordered random tup-tuple ordered by rows  and calculate its value for the chosen trait. 
if val equals the tryval  or if 1 trys failed  go to g1. 	otherwise go to gentup and try again. 1 g1 
assemble the primitive characterizer. 1 g1 
complete the new characterizer's trait list  computing values for all other traits. 1 add the new characterizer to memory. go to i n . 1  begin routines to calculate the various trait values for the relativelyaddressed tuple in descr. 
horospred 	1 1 calculates the maximum number of columns between the leftmost and 
rightmost parts of the tuple. 
vertspred 	1 calculates the maximum number of rows between the topmost and bottommost parts of the tuple. 
gridedge 	1 calculates the number of parts in the tuple which lie on the edge of the input pattern. val is normalized over 1. 
proxim 	1 calculates the sum of absolute 
differences between corresponding digits in all possible pairs of tuple parts. 
compound 	1 returns 'yes' or 'no'  according to whether the tuple is a com-
pound or primitive characterizer. 
discussion      this paper briefly describes the various features of-our program. it then gives a detailed flow-chart-like  precis  that refers by number to the actual program statements being 
-1-

described. 	the program itself is given in the appendix. 	thus the reader can examine exactly what has been done to implement any of the aspects of the program about which he is curious. 	this seems to us of crucial importance: if the program can be used to document itself there is no need for lengthy and usually misleading descriptions and discussions. 
     the program listing is too long and complex to be followed with ease  even by someone who knows snobol; but it should give an idea of what's going on to the casual observer  and those parts in which the reader is interested enough to make some effort should become understandable. snobol is a very simple language in its basic conception  for its programs are built up from sets of production and replacement statements  of the sort  let a = b; look for c on a and  if it's found  replace it by b   tied together by labels and gotos. a brief description of snobol is given in the appendix. 
     this program was written to examine whether a wide variety of learning methods could be implemented together in a single pattern recognition program. using the language snobol allowed us to code a relatively powerful  yet short  program. however  the program runs too slowly to make extensive tests of its abilities to learn and achieve interesting asymptotic performance levels. we therefore give only a brief listing of a short run  to indicate that the program works  and that it at least begins to learn. the program will be recoded in a faster language if we decide to make more extensive tests. 
     further developments might be to have the program try to learn good weights of characterizer tuple parts and the thresholds required to imply a pattern name. we would also like it to generate new parameters with which to characterize its characterizers  see uhr  1b . 
summary 
     the program described in this paper attempts to combine a very simple basic pattern recognition scheme with a wide variety of powerful learning mechanisms. the program attempts 1  to generate its own n-tuple characterizers as needed  and to adjust their weights as a function of feedback  1  to decide what type of characterizer to generate  and 1  to learn what are good general characteristics of characterizers. it can further decide 1  whether and how to modify any particular characterizer that it is evaluating. these decisions are all made within the framework of a program that tries to recognize patterns with as small a set of characterizers that are as simple as possible. 
it therefore starts out with no characterizers  and generates other characterizers which are as simple as it has been able to get away with and which fall within the range of what the program conjectures to be optimal values for the characteristics of characterizers. in terms of characterizer size  this means the program starts out generating 1-tuples and then  to the extent that feedback indicates that it must improve upon its performance  1-tuples  1-tuples  and n+1-tuples. 
acknowledgements 
     this research has been supported in part by nih grant mh-1 and nsf grant gp-1. bibliography 
1. andrews  d.r.  atrubin  a.j.  and hu  k. c  the ibm 1 optical page reader: 
part iii: recognition logic development. ibm i. research and development  1  1  1. 
1. bledsoe  w.w. and browning  i.  pattern recognition and reading by machine. proc. eastern joint comp. conf.  1  1. 
1. prather  rebecca and uhr  l.  discovery and learning techniques for pattern recognition. proc. 1th annual meeting of the acm  1. 
1. uhr  l.  pattern recognition computers as models for form perception. psychol bull.  1  1.  1. 
1. uhr  l. & vossler  c  a pattern recognition program that generates  evaluates  and adjusts its own operators. proc. 
western joint computer conf.  1  1. 
1. uhr  l.  a tutorial description of pattern recognition programs.  submitted for 
publication  1a . 
1. uhr  l   pattern recognition  problem-solving and learning. 1b  in preparation  . 
appendix 
a brief description of snobol 
     snobol is a  pattern matching  language that turns out to be quite convenient for handling list structures and networks of information  using push-down stacks  indirection  and recursive programming. 	its syntax is extremely simple  as follows: 
     snobol programs are built up of two basic types of statements: 
1  assignment statements that assign a name to a pattern of strings  
e.g. 	description = '1' 
characterizer = description ' = ' 
impiieds '/' 

-1-

1  replacement statements that find patterns on a string and  if they are found  replace them by another pattern  e . g . 
these statements have several components: 
a  the  name  of the string to be processed  b  the  pattern  which is a sequence of 1   names   e.g. impueds  found  this  which refer to and stand for their contents  1   literals  e.g.  ' = '   which stand for themselves  and 1   variable names   e.g.   * s u m *       which are assigned contents during the execution of the statement  if the program succeeds in matching the pattern somewhere in the named string. a variable name can be subscripted with a number that fixes its length  e.g. 
 this/size* where size contains an integer . 
     in the two examples of assignment statements above  description is made the name of the string whose literal contents are '1'  and then 1 is put at the beginning of the string named characterizer  since the name description refers to its contents. if another 
assignment statement   were coded  then the indirect reference symbol dollarsign  $  preceding the name $ description 
would put edge  not 1  on characterizer. 
     the end of the pattern-to-be-matched is marked by the equal sign  without quotes around it  and this also marks the beginning of the replacement pattern. the first string of a statement is always the name; all subsequent strings up to the equal sign form the patternto-be-matched  and all strings after the equal sign form the replacement pattern  if the lefthand pattern succeeded . 
     a statement can be surrounded by  labels  and 'tjotos  which control the flow of the program. a 'label  is a string that always begins in column 1. a  goto  comes after the statement  is signaled by a slash  and is of the form / input  or /s input  or /f input  or /s input f process   where s means transfer on success  f means transfer on failure  and no letter after the slash means unconditional transfer.  the goto must  of course  always refer to a label.  when there are no gotos  the program goes to the next statement in sequence. 
     arithmetic is performed within these statements by using  and /  and ** for exponentiation . numbers must be referred to either as literals or as contents of lists  as in sum + ' 1 ' above  which w i l l add one to the number stored in sum . a number of built-in functions can be used to test for inequalities: 
. g t   a   b     .lt a b  .ge a b   .le a b   .eq a b  and equals a b   which is stringmatching equality . the command  .read  will read in one data card  and  .print =  w i l l 
print out the pattern that follows. 
     an asterisk  *  in column 1 denotes a comment card  which the compiler w i l l ignore. a period  .  in column 1 indicates that this card continues the statement on the preceding card  statements can use only 1 columns  whereas the data cards that follow the program can use all 1 columns . a program ends with an end card  end starts in column 1  that also contains the label of the first statement to be executed. 
     the basic pattern match goes from left to right. the compiler looks for the next match of each element of the pattern  ignoring variable names  which w i l l be assigned to the strings that lie between the matched elements - the literals and names with contents . if no match is found  it backtracks to break the assignment of the previously matched element  and looks for its next match  continuing this until either the last element matches or the first element fails. success or failure in the gotos is contingent upon either this match or one of the functions. the programmer can define and code his own functions  and do a number of other powerful things not discussed here. 
     a simple program for information retrieval follows. 
 example program. a simple program to   do 'information retrieval  follows: 
go 	documents = 'rivers=d1 d1 d1 /' 	ml 
'lakes=d1  d1  /spain= d1  d1  dl 1   ' 
 d1 /' 
in 	.read *query* ' 	1f end  	1 
ask 	query  descriptors '   ' = / f   i n   	1 
	documents descriptor ' = ' 	1 
 pertinent* ' / ' /f ask  
	.print = descriptor ' is ' 	1 
'discussed in ' pertinent/ ask  
end 	go 
rivers  spain  mountains  	dl 
  query to be input  on data card   
  precis - an english description of 
 above information retrieval program. 
go 	let documents contain the de- 	ml 
scriptors  followed by pertinent 
documents. 
in 	read in the next query  which is 	1 
a list of descriptors  
ask 	get the next descriptor from the 	1 
	query. 	 if no more  fail to ask.  
	from documents  get pertinent 	1 
ones if the descriptor is found. 
	print out the descriptor and the 	1 
pertinent documents. 
end 	go 

-1-















-1-



































