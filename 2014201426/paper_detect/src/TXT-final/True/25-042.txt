 
testing the satisfiability of a boolean formula over linear constraints is not a simple matter. existing ai systems handle that kind of problems with a general proof method for their boolean parts and a separate module for combining linear constraints. on the contrary  traditional operations research methods need the problem to be transformed  and solved with a mixed integer linear programming algorithm. 
both approaches appear to be improvable if no early separation is introduced between the logical and numerical parts. in this case  combinatorial explosion can be dramatically reduced thanks to efficient looking-ahead techniques and learning methods. 
indeed  propagating bounds following the initial formula gives precious information. besides  an especially tight linear relaxation can be driven from the formula  and allows a simplex algorithm to make a good test for satisfiability. finally  these two looking-ahead methods can be easily coupled for more efficiency and completed by local enumeration. moreover  discovering a good failure explanation is relatively easy in the proposed framework. by  learning  these explanations  it is possible to prune important redundant parts of the search tree. 
   area/subarea: automated reasoning  constraint satisfaction 
1 	introduction 
it is now widely accepted that general theorem provers or problem solvers cannot be efficiently implemented without relying on modules specialized in solving some well delimited problem classes  lassez  1 . in particular  the most natural formalization of many concrete problems often relies on arithmetics over a continuous domain  as real or rational numbers  which cannot be handled directly by general theorem-proving algorithms such as resolution. in such a case  it is often proposed to still use a general theorem-proving algorithm which handles arithmetic constraints as uninterpreted propositions  and to 
1 	constraint satisfaction problems 
control this proof by calling a constraint solving module able to check the compatibility between arithmetic constraints. this is  for example  what is done with the clp r  language. 
   unfortunately  such a strong decomposition limits drastically the performance of the resulting algorithm. this is the reason why we propose here to solve directly problems combining logical and arithmetic constraints. more precisely  this paper deals with the satisfiability problem for formulas built with boolean operators  boolean variables and linear constraints. note that this is a language powerful enough to model many physical systems by piecewise linear approximation  for example electronic circuits or mechanical systems. 
   after describing precisely the problem to solve  we will present the main components of the proposed algorithm. this algorithm  based on implicit enumeration  is complete. moreover  it succeeds in reducing combinatorial explosion in two ways. firstly  it  looks ahead  efficiently by both testing the satisfiability of a  tight  linear relaxation of the overall problem  and by inferring the value of some decision variables early thanks to bounds propagation. secondly  it is able to learn at low cost from failure analysis  and then avoids many redundant computations. 
1 	the problem 
the problem to be solved is the satisfiability of formulas having the following syntax: 

this language allows many problems to be described. in particular  it can be used to model physical systems by piecewise linear approximation. thus  in the domain of analog electronic circuit diagnosis the following formulas about a transistor's good behavior have been successfully used in  dague et al.  1 : 
1 a basic and complete solving algorithm: enumeration 
a basic but always useful algorithm for solving combinatorial problems is implicit enumeration. its principle is to explore systematically the search space by successively dividing it  i.e.  choosing alternatives  until the problem becomes simple enough to be directly solved. 
¡¡thus  it is possible to solve the above problem by a so called   depth-first backtrack search   see for example 
 dechter  1  . after making all the possible choices  elements in disjunctions   a simple conjunction of linear constraints is obtained. such a system is then easily tested for solvability by an efficient algorithm such as the simplex. 
   as the simplex is a complete algorithm  if all the possible choice combinations are tried  the overall algorithm is complete. however  it may spend a time proportional to the exponential of the number of disjunctions. fortunately  this can be dramatically improved by lookingahead and learning techniques as presented below. 
1 	looking-ahead techniques 
during the search  each time a subproblem is generated  even if the resulting formula is still not directly solvable by a simplex because of remaining disjunctions  it is possible to analyze it and reduce a priori the remaining search space by removing some alternatives. sometimes  the analysis will detect  without combinatorial search  that the subproblem is unsatisfiable. two complementary analysis methods may be used: constraint propagation and linear relaxation. especially interesting methods of these two kinds are presented below. 
1 	b o u n d p r o p a g a t i o n 
bound propagation based on linear constraints can easily be implemented. indeed  given a single linear constraint and some initial bounds on its variables  it is quite simple to compute how the bounds of each variables may be changed: the constraint may be rewritten as  expr or x   expr for any of its variables x. given a conjunction of linear constraints  bounds may be propagated by using each constraint after the other until stability is reached  or some stopping criterion is verified . it is to be noted that this process is fairly incomplete. as an example  with the constraint set: x = y and x = - y and x and y in  -1   this method fails to infer any stricter bound  though x and y are necessarily 1. 
   in order to apply bound propagation to any formula  it suffices to work bottom up in the formula tree computing bounds which may be inferred from each terminal conjunctions of linear constraints  then grouping the results for the disjunctions by selecting the less tightening bounds and so on until new bounds are computed for the root  i.e.  for the whole problem. for example  if the formula  is considered where 
ai and bi are the triangles shown in figure 1  the first step of bound propagation yields the bounds represented by the box enclosed in the dashed lines. this example demonstrates that bound propagation can last forever: 

its solution is  1  and at each step the bounds are only reduced by a factor of 1. 

   in order to draw the best of such a method  it is important to propagate any new bound inferred at one level of the formula tree in its subtrees  i.e.  to propagate bounds under hypotheses in the part of the formula which holds under the same hypotheses . this may be done with the algorithm described below. the recursive function propagate.bounds takes as arguments a set of initial bounds for the variables  and a formula. it returns either a set of bounds implied by formula  or the information unsolvable if the formula is discovered incompatible with the initial bounds. 
propagate bounds bounds  formula  
if formula is a single linear constraint for each variable x in this constraint update its bound in bounds as described above and return the result 
 which may be unsolvable  if formula is a conjunction formula 1 and formula 1 repeat 
old bounds : = bounds 
bounds := propagate.bounds bounds  formula   
   bounds := propagate.bounds  bounds  formula 1   until bounds = unsolvable or old-bounds = bounds 
    or after n iterations  return bounds 
if formula is a disjunction formula  v formula1 bounds  = propagate bounds{bounds  formula   
bounds 1 - propagate' .bound s  bounds  formulay  if  bounds 1 = unsolvable  return bounds 1 if  bounds1 - unsolvable  return bounds 1 else return bounds  u bounds1 
 union of each intervals  
¡¡the propagation is started by calling this function with bounds containing no restriction and formula being the whole formula to be tested for satisfiability. 
   the practical implementation of this algorithm is in fact more incremental. at each disjunctive node of 
1 	constraint satisfaction problems 
the tree  the bounds  and bounds 1 sets are cached. then  whenever propagation of bounds is requested for this node  only the new information contained in bounds compared to bounds 1  or bounds 1   is propagated through formula 1  respectively formula 1  . note that  at the end of the algorithm  bounds 1  respectively bounds 1   may be regarded as the bounds which hold under the hypothesis that the first  respectively second  alternative of the disjunction have been chosen  and therefore that the whole path leading to this disjunction has been selected . 
   despite its incompleteness  bound propagation updated at each level of the search tree often reduces the combinatorial explosion in an important manner by discovering soon that some alternatives may be suppressed. 
1 	linear r e l a x a t i o n 
solving a simple conjunction of linear constraints is an easy matter thanks to algorithms such as the simplex. even if a problem of the class studied here contains disjunctions  it is possible to derive from it a new problem containing only conjunctions and whose solution set includes every solution to the initial problem. this is called a linear relaxation. testing the solvability of a relaxation already gives important information: if the relaxation is unsolvable  then so is the initial problem. 
   obviously  a tight relaxation  i.e.  one whose solution set is as small as possible  is of great interest. in geometrical terms  a set expressed using disjunctions of linear constraints is not convex  whereas one expressed using only conjunctions is convex. the idea is then to use the convex hull of a disjunction for its relaxation. the linear constraints defining this convex hull can be computed symbolically  by using a variable elimination algorithm  lassez and lassez  1 . this is however to be avoided  because the number of constraints defining the convex hull may grow exponentially with respect to the number of constraints appearing in the disjunction. 
   nevertheless  there exists an interesting construction adapted from balas  balas  1  which allows the convex hull of a disjunction of linear constraint sets to be represented compactly by only adding new variables. it is presented here for a disjunction with two elements  but the extension to more elements is straightforward. 

   in the definition of rel d   the positivity constraints on the 1t can actually be removed. this is one difference between balas' result and the following theorem. moreover  we have demonstrated it even for unbounded polyedron. 

¡¡now  any problem in normal form can be relaxed into a conjunction of constraints  using balas' relaxation to remove each disjunction. please note that  because of the conjunctions connecting different disjunctions with each other  it is not the convex hull of the solution of the global problem which is obtained  but instead a less tight constraint set. 
¡¡the example in figure 1 illustrates how from the inconsistent formula  the space of 
possible solutions is well reduced by the relaxation without succeeding in discovering the inconsistency. note that on this example bound propagation alone succeeds in proving inconsistency. however if the drawing is rotated by 1¡ã  bound propagation does not find anything  while balas' relaxation will be as precise as without rotation. 

¡¡this linear relaxation is only one possible among many. it is however the only known one which is so tight without needing any complex computation. relaxations for a disjunction of constraints which are currently used in mixed integer programming  see for example  williams  1   introduce less new variables in the formulation. however  they do not succeed in approaching the convex hull of the disjunctions. balas' relaxation used here certainly adds many new variables  but because the average complexity of the simplex is only proportional to the logarithm of the number of variables  this is not a real problem. moreover  as the added constraints are only equalities  they can be easily erased by substitution. therefore  balas' relaxation is both computationally convenient and very precise. 
1 	coupling bound propagation and relaxation 
balas' relaxation is perfect  i.e.  represents the convex hull of the solutions  only when applied to one disjunction of constraint sets. however  it is in theory possible to build a convergent series of relaxations by: 
  computing explicitly the projection on the initial x variables of the first balas' relaxation  that suppose each variable and are removed by an algorithm such as the one in  lassez and lassez  1  . 
  adding the result to each leaf of the formula tree 
  and iterating  i.e.  recompute a balas' relaxation... 
¡¡balas has demonstrated in  balas  1  that such a series converges  in possibly infinite time  toward the convex hull of the solutions of the initial formula. this is completely impractical since just computing the pro-
jection may give an exponentially big result  which is to be duplicated and re-projected. even so  this theoretical result is useful in showing the interest of  re-injecting  in the leaf of the tree the information about the global solution space. the only manageable information for this purpose is the bounds on the variables. now  let us consider the bounds computed by the propagation for one leaf of the formula tree. by definition  these bounds can be added as explicit constraints to the corresponding formula leaf without removing any potential solution. then  if balas' relaxation is consequently updated  it may become much tighter. 
¡¡finally  relaxation can be used to perform a  local enumeration . this means verifying that each  may be fixed to 1. if some  cannot be set to 1 then they are necessarily zero and the corresponding alternative should be suppressed. after such suppressions it is worth to restart the looking ahead phase at its beginning  i.e.  bounds propagation. 
1 	learning 
by learning  we mean inferring general boolean constraints over hypotheses  in order to reduce a priori unexplored paths of the search tree. such constraints can be detected using two different methods: incompatibilities 
from bounds  and failure analysis. in both cases  these constraints are of the form  during the constraint solving process  many of them can be inferred. they are stored in a symbolic boolean constraint solver  or better in an atms  de kleer  1 . such an algorithm is able to detect early that hypotheses are fixed  in the case of an atms  one-element nogoods   and manages efficiently the constraints in store. therefore  after many such constraints have been inferred  the possibilities for the hypotheses can be reduced drastically. for example  if and are known to be incompatible  if one of them is fixed to 1  then the choice corresponding to the other one will automatically be avoided. 
1 	learning from bounds 
each bound computed by the bound propagation algorithm can be labeled by the set of hypotheses yielding to it. when bounds for the same variable are incompatible 

1 	failure analysis 
relaxing a problem yields a set s of linear constraints over continuous variables and  variables. if it is unsolvable  it is interesting to know why. this can easily be analyzed as being due to a subset of constraints in s which are incompatible together. this subset will be called a conflict. as each constraint is related to one hypothesis   a conflict may be easily translated in terms of hypotheses. 
¡¡let us first explain how a conflict in terms of constraints may be built. the solvability of the set s where a is a matrix  can be tested by computing minx o on where i is the identity matrix  is a vector of slack variables  is a positive variable  and 1 is a column vector whose coordinates are all equal to 1. s is solvable if and only if minx o = 1. 
¡¡when s is unsolvable  the simplex reaches a positive minimum value for x1 with some variables in the ba-
sis  i.e.  being not necessaraly stuck to 1 . constraints whose slack variables are not in this basis are the limits on which the minimization stumbled. they form a minimal conflict of s. as the reader can see  the detection of this conflict is done a posteriori  without any modification of the core constraint satisfaction algorithm  i.e.  at no cost. for a detailed presentation of this result see  de backer and beringer  1 . 
¡¡once a cause of inconsistency has been found in terms of constraints  it can be translated in terms of the corresponding hypotheses. it can be added as a nogood in the atms  so that it can be used to further reduce the possibilities on the hypotheses. please note that the minimality of a conflict in terms of constraints does not guarantee at all that the corresponding conflict in terms of hypotheses is minimal. however  it is usually not far from being minimal. 
1 	comparison with other approaches 
many ai systems just use some kind of bound propagation to draw information from any kind of arithmetic constraints. depending on the application  the incompleteness of such a method is more or less disturbing. for example  in a diagnosis application such as  dague et al.  1   it may forbid to detect some failure cause. in any case  the hereabove tools may be fruitfully added to these systems when more completeness is necessary. 
1 	mixed integer linear programming 
there exist many commercial packages able to solve 
mixed integer linear programming  milp  problems  i.e.  problems described by a simple conjunction of lin-
ear constraints in which some variables are forced to take 
1 	constraint satisfaction problems 
integer values. the milp formalism is powerful. in particular  the satisfiability of boolean formulas over linear constraints may be transformed into a milp problem. 
¡¡good milp packages solve such problems by an implicit enumeration method controlled by a simplex and bound propagation. so even without any learning  they normally perform not too badly. however  when applied to problem naturally expressed in the disjunctive form studied here  they have an important limitation: they are not able to deal directly with the disjunctive formulation. the user is then forced to reformulate completely the problem in milp this means that the user is in charge of generating the relaxation used by the algorithm. 
   now  though the relaxation proposed here is the most precise one without being computationally expensive  it cannot be used as such with milp packages. indeed  even after adding the constraints that the  variables are equal to either 1 or 1  this relaxation still has solutions which do not correspond to solution of the initial formula. to obtain this essential property  many extra artificial constraints have to be added to enforce xi to be zero when  is zero. this is done by constraining each variable x  with two artificial bounds and  where m is a large enough positive number. the resulting formulation has so many constraints that the simplex may perform poorly on it. this is perhaps the reason why  despite its qualities  balas relaxation has been hardly ever used. 
¡¡moreover  whatever milp reformulation is chosen by the user  it hides the initial structure of the problem in such a way that bound propagation applied to this reformulation makes much less inferences than when it is applied directly to the initial formulation as proposed here. 
¡¡finally  no milp package seems to use a learning technique similar to the one proposed here. 
¡¡as a result  on difficult problems naturally expressed as boolean formulas over linear constraints  our algorithm should perform better than any milp package running on any milp reformulation. 
1 	constraint logic programming 
languages as clp r   heintze et ai  1  which extend the prolog language with linear constraints may be used to express the satisfiability problem considered here  and even to solve it. however  used as such  the resolution algorithm handles very poorly the disjunctions: they are not considered in any way before branching on them. 
¡¡but note that  when used to solve combinatorial problems  clp languages are more programming languages than ready-to-use solving algorithms. as a matter of fact  the authors are currently testing and implementing the methods presented here using claire  a prototypal constraint extension of ibm prolog. 
1 	conclusion 
solving boolean formulas containing linear constraints is a difficult combinatorial problem. as for any combinatorial problem  there does not exist an  ideal  algorithm 
which would be better than the others on every problem. success in combinatorial problem solving lies in the adequacy of the solving strategy to the restricted problem set to be solved. in particular  the user must have a way to specify heuristics able to guide the search. indeed  success of constraint logic programming over finite domains  hentenryck  1  may be explained by the ability to combine easily efficient algorithm thanks to a high level programming language. 
   the present work intended to provide basic procedures useful to design efficient algorithms while taking adavantage of the problem specificities. the different procedures proposed here are not only separately efficient  but are also perfectly complementary. thus  learning by failure analysis often gets an information which is not easily found by a priori analysis; balas' relaxation is locally perfect but does not integrate long distance influences; and  on the contrary  bound propagation is a rough approximation which takes into account the links in the overall problem. 
   the very first experiments with the prototype written in c l a i r e have shown good results. further work will include the benchmarking of the method on problems of good size. 
