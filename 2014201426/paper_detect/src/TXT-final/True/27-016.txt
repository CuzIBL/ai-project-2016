 
we review accuracy estimation methods and compare the two most common methods crossvalidation and bootstrap recent experimen-
tal results on artificial data and theoretical re cults m restricted settings have shown that for selecting a good classifier from a set of classifiers  model selection   ten-fold cross-validation may be better than the more expensive ka p one-out cross-validation we report on a largescale experiment-over half a million runs of c1 and anaive-bayes algorithm-loestimale the effects of different parameters on these al gonthms on real-world datascts for crossvalidation we vary the number of folds and whether the folds arc stratified or not  for bootstrap  we vary the number of bootstrap samples our results indicate that for real-word datasets similar to ours  the best method lo use for model selection is ten fold stratified cross validation even if computation power allows using more folds 
1 	introduction 
　　　it can not be emphasized enough that no claim whatsoever 1 being made in this paper that alt algorithms are equivalent in practice in the real world in particular no claim is being made that ont should not use cross validation in the real world 
- wolpcrt  1a.  
   estimating the accuracy of a classifier induced by supervised learning algorithms is important not only to predict its future prediction accuracy  but also for choosing a classifier from a given set  model selection   or combining classifiers  wolpert 1  for estimating the final accuracy of a classifier  we would like an estimation method with low bias and low variance to choose a classifier or to combine classifiers  the absolute accuracies are less important and we are willing to trade off bias 
　a longer version of the paper can be retrieved by anony mous ftp to starry htanford edu pub/ronnyk/accest-long ps for low variance  assuming the bias affects all classifiers similarly  e g eslimates are   % pessimistic  
　in this paper we explain some of the assumptions made by ihe different estimation methods and present concrete examples where each method fails while it is known that no accuracy estimation can be corrert all the time  wolpert 1b schaffer 1j we are inter ested in identifying a method that ib well suited for the biases and tn rids in typical real world datasets 
　recent results both theoretical and experimental  have shown that it is no! alwa s the case that increasing the computational cost is beneficial especiallhy if the relative accuracies are more important than the exact values for example leave-one-out is almost unbiased  but it has high variance leading to unreliable estimates  efron 1  lor linear models using leave-one-out cross-validation for model selection is asymptotically in consistent in the sense that the probability of selecting the model with the best predictive power does not converge to one as the lolal number of observations approaches infinity  zhang 1  shao 1  
　this paper  s organized as follows section 1 describes the common accuracy estimation methods and ways of computing confidence bounds that hold under some assumptions section 1 discusses related work comparing cross-validation variants and bootstrap variants sec lion 1 discusses methodology underlying our experiment the results of the experiments are given section 1 with a discussion of important observations we conelude with a summary in section 1 
1 	methods for accuracy estimation 
a classifier is a function that maps an unlabelled instance to a label using internal data structures an inducer or an induction algorithm builds a classifier from a given dataset cart and c 1  brennan  friedman olshen &. stone 1  quinlan 1  are decision tree inducers that build decision tree classifiers in this paper we are not interested in the specific method for inducing classifiers  but assume access to a dataset and an inducer of interest 
let v be the space of unlabelled instances and y the 
	kohavi 	1 

set of possible labels be the space of labelled instances and   i n   be a dataset 
 possibly a multiset  consisting of n labelled instances  
where 	a classifier c maps an unla-
beled instance 	' 1 a l a b e l a n d an inducer 
maps a given dataset d into a classifier c the notation  will denote the label assigned to an unlabelled in-
stance v by the classifier built  by inducer x on dataset dt 
 we assume that there exists a 
distribution on the set of labelled instances and that our dataset consists of 1 d  independently and identically distributed  instances we consider equal misclassificalion costs using a 1 loss function  but the accuracy estimation methods can easily be extended to other loss functions 
　the accuracy of a classifier c is the probability of correctly clasaifying a randoml  selected instance  i e for a randomly selected instance where the probability distribution over the instance space 1 the same as the distribution that was used to select instances for the inducers training set given a finite dataset we would like to custimate the future performance of a classifier induced by the given inducer and dataset a single accuracy estimate is usually meaningless without a confidence interval  thus we will consider how to approximate such an interval when possible in order to identify weaknesses  we also attempt 
o identify cases where the estimates fail 
1 	holdout 
the holdout method sometimes called test sample estimation partitions the data into two mutually exclusive subsets called a training set and a test set or holdout set it is lommon to designate 1/ 1 of the data as the training set and the remaining 1 as the test set the training set is given to the inducer  and the induced classifier is tested on the test set formally  let   the holdout set  be a subset of d of size h  and let the 
holdout estimated accuracy is defined as where that the inducer s accuracy increases as more instances are seen  the holdout method is a pessimistic estimator because only a portion of the data is given to the inducer for training the more instances we leave for the test set  the higher the bias of our estimate however  fewer test set instances means that the confidence interval for the accuracy will be wider as shown below 
　each test instance can be viewed as a bernoulli trial correct or incorrect prediction let s be the number of correct classifications on the test set  then s is distributed bmomially  sum of bernoulli trials  for reasonably large holdout sets  the distribution of s/h is approximately normal with mean ace  the true accuracy of 
1 	learning 
the classifier  and a variance of ace *  1 - acc hi thus  by de moivre-laplace limit theorem  we have 
where z is the quanl lie point of the standard normal distribution to get a ioo1 percent confidence interval  one determines 1 and inverts the inequalities inversion of the inequalities leads to a quadratic equation in ace  the roots of which are the low and high confidence points 

the above equation is not conditioned on the dataset d  if more information is available about the probability of the given dataset it must be taken into account 
　the holdout estimate is a random number that depends on the division into a training set and a test set in random sub sampling the holdout method is repeated k times and the eslimated accuracy is derived by averaging the runs th  slandard deviation can be estimated as the standard dewation of the accuracy estimations from each holdout run 
　the mam assumption that is violated in random subsampling is the independence of instances m the test set from those in the training set if the training and test set are formed by a split of an original dalaset  then an over-represented class in one subset will be a under represented in the other to demonstrate the issue we simulated a 1  1 /1 split of fisher's famous ins dataset and used a majority inducer that builds a classifier pre 
dieting the prevalent class in the training set the iris dataset describes ins plants using four continuous features  and the task is to classify each instance  an ins  as ins setosa ins versicolour or ins virginica for each class label there are exactly one third of the instances with that label  1 instances of each class from a total of 1 instances  thus we expect 1% prediction accuracy however  because the test set will always contain less than 1 of the instances of the class that was prevalent in the training set  the accuracy predicted by the holdout method is 1% with a standard deviation of 1%  estimated by averaging 1 holdouts  
　in practice  the dataset size is always finite  and usually smaller than we would like it to be the holdout method makes inefficient use of the data a third of dataset is not used for training the inducer 
1 	cross-validation  leave-one-out  and stratification 
in fc-fold cross-validation  sometimes called rotation estimation  the dataset v is randomly split into k mutually 
exclusive subsets  the folds   of approximately equal size the inducer is trained and tested 

　the cross-validation estimate is a random number that depends on the division into folds complete cross-validation is the average of all  possibil ities for choosing m/k instances out of m  but it is usually too expensive exrept for leave-one-one  rc-fold 
cross-validation   which is always complete  fc-fom crossvalidation is estimating complete k-foid cross-validation using a single split of the data into the folds repeating cross-validation multiple limes using different spills into folds provides a better monte c arlo estimate to 1 he complele cross-validation at an added cost in stratified cross-validation the folds are stratified so thai tlicy contain approximately the same proportions of labels as the original dataset 
　an inducer is stable for a given dataset and a set of perturbal ions if it induces classifiers thai make the same predictions when it is given the perturbed datasets 
proposition 1  variance in a fold cv  given a dataset and an inducer if the inductr is stable under the pei tur bations caused by deleting the instances for thr folds in k fold cross-validatwn the cross validation   stnnate will be unbiastd and the tail ance of the estimated accuracy will be approximately accrv  1 -   / n when n is the number of instances 
in the datasi t 
proof if we assume that the k classifiers produced make the same predictions  then the estimated accuracy has a binomial distribution with n trials and probabihly of success equal to  he accuracy of the classifier | 
　for large enough n a confidence interval may be computed using equation 1 with h equal to n  the number of instances 
　in reality a complex inducer is unlikely to be stable for large perturbations unless it has reached its maximal learning capacity we expect the perturbations induced by leave-one-out to be small and therefore the classifier should be very stable as we increase the size of the perturbations  stability is less likely to hold we expect stability to hold more in 1-fold cross-validation than in 1-fold cross-validation and both should be more stable than holdout of 1 the proposition does not apply 
to the resubstitution estimate because it requires the inducer to be stable when no instances are given in the dataset 
　the above proposition helps  understand one possible assumption that is made when using cross-validation if an inducer is unstable for a particular dataset under a set of perturbations introduced by cross-validation  the accuracy estimate is likely to be unreliable if the inducer is almost stable on a given dataset  we should expect a reliable estimate the next corollary takes the idea slightly further and shows a result that we have observed empirically there is almost no change in the variance of the cross validation estimate when the number of folds is varied 
corollary 1  variance m cross-validation  given a dataset and an inductr if the inducer is stable undfi the } tituibuhoris  aused by deleting the test instances foi the folds in k-fold cross-validation for various valuts of k then tht vartanct of the estimates will be the same 
proof the variance of a-fold cross-validation in proposition 1 does not depend on k | 
　while some inducers are liktly to be inherently more stable the following example shows that one must also take into account the dalaset and the actual perturba 
 ions 
example 1  failure of leave-one-out  lusher s ins dataset contains 1 instances of each class leading one to expect that a majority indu er should have acruraov about j  % however the eombmation of this dataset with a majority inducer is unstable for the small perturbations performed by leave-one-out when an instance is deleted from the dalaset  its label is a minority in the training set  thus the majority inducer predicts one of the other two classes and always errs in classifying the test instance the leave-one-out estimated accuracy for a majont  inducer on the ins dataset is therefore 1% moreover all folds have this estimated accuracy  thus the standard deviation of the folds is again 1 %giving the unjustified assurance that 'he estimate is stable | 
　the example shows an inherent problem with crossvalidation th-t applies to more than just a majority inducer in a no-infornirition dataset where the label values are completely random  the best an induction algorithm can do is predict majority leave-one-out on such a dataset with 1% of the labels for each class and a majontv ind'-cer  the best  possible inducer  would still predict 1% accuracy 
1 	b o o t s t r a p 
the bootstrap family was introduced by efron and is fully described in efron &. tibshirani  1  given a dataset of size n a bootstrap sample is created by sampling n instances uniformly from the data  with replacement  since the dataset is sampled with replacement  the probability of any given instance not being chosen after n samples is  the 
	kohavi 	1 

expected number of distinct instances from the original dataset appearing in the teat set is thus 1n the eo accuracy estimate is derived by using the bootstrap sample for training and the rest of the instances for testing given a number b  the number of bootstrap samples  let e1  be the accuracy estimate for bootstrap sample i the 1 bootstrap estimate is defined as 
		 1  
where ace  is the resubstitution accuracy estimate on the full dataset  i e   the accuracy on the training set  the variance of the estimate can be determined by com puting the variance of the estimates for the samples 
   the assumptions made by bootstrap are basically the same as that of cross-validation  i e   stability of the algorithm on the dataset the 'bootstrap world  should closely approximate the real world the b1 bootstrap fails  o give the expected result when the classifier is a perfect memonzer  e g an unpruned decision tree or a one nearest neighbor classifier  and the dataset is completely random  say with two classes the resubstitution accuracy is 1%  and the eo accuracy is about 1% plugging these into the bootstrap formula  one gets an estimated accuracy of about 1%  far from the real accuracy of 1% bootstrap can be shown to fail if we add a memonzer module to any given inducer and adjust its predictions if the memonzer remembers the training set and makes the predictions when the test instance was a training instances  adjusting its predictions can make the resubstitution accuracy change from 1% to 1% and can thus bias the overall estimated accuracy in any direction we want 
1 	related work 
some experimental studies comparing different accuracy estimation methods have been previously done but most of them were on artificial or small datasets we now describe some of these efforts 
　efron  1  conducted five sampling experiments and compared leave-one-out cross-validation  several variants of bootstrap  and several other methods the purpose of the experiments was to 'investigate some related estimators  which seem to offer considerably improved estimation in small samples ' the results indicate that leave-one-out cross-validation gives nearly unbiased estimates of the accuracy  but often with unacceptably high variability  particularly for small samples  and that the 1 bootstrap performed best 
　breiman et al  1  conducted experiments using cross-validation for decision tree pruning they chose ten-fold cross-validation for the cart program and claimed it was satisfactory for choosing the correct tree they claimed that  the difference in the cross-validation estimates of the risks of two rules tends to be much more accurate than the two estimates themselves   
1 	learning 
　jain  dubes fa chen  1  compared the performance of the t1 bootstrap and leave-one-out cross-validation on nearest neighbor classifiers using artificial data and claimed that the confidence interval of the bootstrap estimator is smaller than that of leave-one-out weiss  1  followed similar lines and compared stratified cross-validation and two bootstrap methods with nearest neighbor classifiers his results were that stratified two-fold cross validation is relatively low variance and 
superior to leave-one-out 
　breiman fa spector  1  conducted a feature subset selection experiments for regression  and compared leave-one-out cross-validation  a:-fold cross-validation for various k  stratified k-fold cross-validation  biascorrected bootstrap  and partial cross-validation  not discussed here  tests were done on artificial datasets with 1 and 1 instances the behavior observed was  1  the leave-one-out has low bias and rms  root mean square  error whereas two-fold and five-fold crossvalidation have larger bias and rms error only at models with many features   1  the pessimistic bias of ten-fold cross-validation at small samples was significantly reduced for the samples of size 1  1  for model selection  ten-fold cross-validation is better than leave-one-out 
 bailey fa elkan  1  compared leave-one-out crossahdation to 1 bootstrap using the foil inducer 
and four synthetic datasets involving boolean concepts they observed high variability and little bias in the leave-one-out estimates  and low variability but large bias in the 1 estimates 
　weiss and indurkyha  weiss fa indurkhya 1  conducted experiments on real world data lo determine the applicability of cross-validation to decision tree pruning their results were that for samples at least of size 1 using stratified ten-fold cross-validation to choose the amount of pruning yields unbiased trees  with respect to their optimal size  
1 	m e t h o d o l o g y 
in order to conduct a large-scale experiment we decided to use 1 and a naive bayesian classifier the c1 algorithm  quinlan 1  is a descendent of id1 that builds decision trees top-down the naive-bayesian classifier  langley  iba fa thompson 1  used was the one implemented in   kohavi  john  long  manley fa pfleger 1  that uses the observed ratios for nominal features and assumes a gaussian distribution for continuous features the exact details are not crucial for this paper because we are interested in the behavior of the accuracy estimation methods more than the internals of the induction algorithms the underlying hypothesis spaces-decision trees for c1 and summary statistics for naive-bayes-are different enough that we hope conclusions based on these two induction algorithms will apply to other induction algorithms 
because the target concept is unknown for real-world 

concepts  we used the holdout method to estimate the quality of the cross-validation and bootstrap estimates to choose & set of datasets  we looked at the learning curves for c1 and najve-bayes for most of the supervised classification dataaets at the uc irvine repository  murphy & aha 1  that contained more than 1 instances  about 1 such datasets  we felt that a minimum of 1 instances were required for testing while the true accuracies of a real dataset cannot be computed because we do not know the target concept  we can esti mate the true accuracies using the holdout method the  true' accuracy estimates in table 1 were computed by taking a random sample of the given size computing the accuracy using the rest of the dataset as a test set  and repeating 1 times 
　we chose six datasets from a wide variety of domains  such that the learning curve for both algorithms did not flatten out too early that is  before one hundred instances we also added a no information dalastt  rand  with 1 boolean features and a boolean random label on one dataset vehicle  the generalization accuracy of the naive-bayes algorithm deteriorated hy more than 1% as more instances were g;iven a similar phe nomenon was observed on the shuttle dataset such a phenomenon was predicted by srhaffer and wolpert  schaffer 1  wolpert 1   but we were surprised that it was observed on two real world datasets 
　to see how well an accuracy estimation method per forms we sampled instances from the dataset  uniformly without replacement  and created a training set of the desired size we then ran the induction algorihm on the training set and tested the classifier on the rest of the instances lei the dataset this was repeated 1 times at points where the learning curve was sloping up the same folds in cross-validation and the same samples m bootstrap were used for both algorithms compared 
1 	results and discussion 
we now show the experimental results and discuss their significance we begin with a discussion of the bias in the estimation methods and follow with a discussion of the variance due to lack of space  we omit some graphs for the naive-bayes algorithm when the behavior is approximately the same as that of c 1 
1 	t h e bias 
the bias of a method to estimate a parameter 1 is defined as the expected value minus the estimated value an unbiased estimation method is a method that has zero bias figure 1 shows the bias and variance of k-fold cross-validation on several datasets  the breast cancer dataset is not shown  
　the diagrams clearly show that k-fold cross-validation is pessimistically biased  especially for two and five folds for the learning curves that have a large derivative at the measurement point the pessimism in k-fold cross-

figure   c'1 the bias of cross-validation with varying folds a negative k folds stands for leave k-out error bars are 1% confidence intervals for  he mean the gray regions indicate 1 % confidence intervals for the true ac curaries note the different ranges for the accuracy axis 
validation for small k s is apparent most of the estimates are reasonably good at 1 folds and at 1 folds they art almost unbiased 
　stratified cross validation  not shown  had similar behavior  except for lower pessimism the estimated accuracy for soybe an at 1 fold was 1% higher and at five-fold  1% higher for vehicle at 1-fold  the accuracy was 1% higher and at five-fold 1% higher thus stratification seems to be a less biased estimation method 
　figure 1 shows the bias and variance for the b1 bootstrap accuracy estimation method although the 1 bootstrap is almost unbiased for chess hypothyroid  and mushroom for both inducers it is highly biased for soybean with c'a 1  vehicle with both inducers and rand with both inducers the bias with c1 and vehicle is 1% 
1 	the variance 
while a given method may have low bias  its performance  accuracy estimation in our case  may be poor due to high variance in the experiments above  we have formed confidence intervals by using the standard deviation of the mean accuracy we now switch to the standard deviation of the population i e   the expected standard deviation of a single accuracy estimation run in practice  if one dots a single cross-validation run the expected accuracy will be the mean reported above  but the standard deviation will be higher by a factor of v1  the number of runs we averaged in the experiments 
	kohavi 	1 


table 1 true accuracy estimates for the datasets using c1 and naive-bayes classifiers at the chosen sample sizes 


figure 1 c1 the bias of bootstrap with varying samples estimates are good for mushroom hypothyroid  and chess  but are extremely biased  optimistically  for vehicle and rand  and somewhat biased for soybean 
　in what follows  all figures for standard deviation will be drawn with the same range for the standard deviation 1 to 1% figure 1 shows the standard deviations for c1 and naive bayes using varying number of folds for cross-validation the results for stratified cross-validation were similar with slightly lower variance 
figure 1 shows the same information for 1 bootstrap 
cross-validation has high variance at 1-folds on both 
c1 and naive-bayes on c1  there is high variance at the high-ends too-at leave-one-out and leave-twoout-for three files out of the seven datasets stratification reduces the variance slightly  and thus seems to be uniformly better than cross-validation  both for bias and vanance 
1 	learning 
figure 1 cross-validation standard deviation of accuracy  population  different  line styles are used to help differentiate between curves 
1 	s u m m a r y 
we reviewed common accuracy estimation methods including holdout  cross-validation  and bootstrap  and showed examples where each one fails to produce a good estimate we have compared the latter two approaches on a variety of real-world datasets with differing characteristics 
　proposition 1 shows that if the induction algorithm is stable for a given dataset  the variance of the crossvalidation estimates should be approximately the same  independent of the number of folds although the induction algorithms are not stable  they are approximately stable it fold cross-validation with moderate k values  1  reduces the variance while increasing the bias as k decreases  1  and the sample sizes get smaller  there is variance due to the instability of the training 

1 igure 1 	1 bootstrap 	standard deviation in accrat y  population  
sets themselves leading to an increase in variance this is most apparent for datasets with many categories  such as soybean in these situations  stratification seems to help  but -epeated runs may be a better approach 
　our results indicate that stratification is generally a better scheme both in terms of bias and variance when compared to regular cross-validation bootstrap has low  variance but extremely large bias on some problems we recommend using stratified len fold cross-validation for model selection 
acknowledgments we thank david wolpert for a thorough reading of this paper and many interesting discussions we thank tom bylander brad efron jerry friedman  rob holte george john pat langley hob tibshiram and sholom weiss for their helpful com nients and suggestions dan sommcrfield implemented lhe bootstrap method in wlc++ all experiments were conducted using mlc++ partly partly funded by onr grant n1-1 and nsf grants iri 1 and iri-1 
