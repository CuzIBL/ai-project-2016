
developing scalable algorithms for solving partially observable markov decision processes  pomdps  is an important challenge. one approach that effectively addresses the intractable memory requirements of pomdp algorithms is based on representing pomdp policies as finitestate controllers. in this paper  we illustrate some fundamental disadvantages of existing techniques that use controllers. we then propose a new approach that formulates the problem as a quadratically constrained linear program  qclp   which defines an optimal controller of a desired size. this representation allows a wide range of powerful nonlinear programming algorithms to be used to solve pomdps. although qclp optimization techniques guarantee only local optimality  the results we obtain using an existing optimization method show significant solution improvement over the state-of-the-art techniques. the results open up promising research directions for solving large pomdps using nonlinear programming methods.
1 introduction
since the early 1s  markov decision processes  mdps  and their partially observable counterparts  pomdps  have been widely used by the ai community for planning under uncertainty. pomdps offer a rich language to describe situations involving uncertainty about the domain  stochastic actions  noisy observations  and a variety of possible objective functions. pomdp applications include robot control  simmons and koenig  1   medical diagnosis  hauskrecht and fraser  1  and machine maintenance  eckles  1 . robots typically have sensors that provide uncertain and incomplete information about the state of the environment  which must be factored into the planning process. in a medical setting  the internal state of the patient is often not known with certainty. the machine maintenance problem  one of the earliest application areas of pomdps  seeks to find a costeffective strategy for inspection and replacement of parts in a domain where partial information about the internal state is obtained by inspecting the manufactured products. numerous other pomdp applications are surveyed in  cassandra  1b .
모developing effective algorithms for mdps and pomdps has become a thriving ai research area  cassandra  1a; feng and hansen  1; feng and zilberstein  1; hansen  1; littman et al.  1; meuleau et al.  1; poupart and boutilier  1 . thanks to these new algorithms and improvements in computing power  it is now possible to solve very large and realistic mdps. in contrast  current pomdp exact techniques are limited by high memory requirements to toy problems.
모pomdp exact and approximate solution techniques cannot usually find near-optimal solutions with a limited amount of memory. even though an optimal solution may be concise  current exact algorithms that use dynamic programming often require an intractable amount of space. while work has been done to make this process more efficient  feng and zilberstein  1   the time and space complexities of pomdp algorithms remain serious challenges. pomdp approximation algorithms can operate with a limited amount of memory  but as a consequence they provide very weak theoretical guarantees  if any. in contrast  we describe a new approach that bounds space usage while permitting a principled search based on the optimal solution.
모current techniques used to find optimal fixed-size controllers rely solely on local information  meuleau et al.  1; poupart and boutilier  1 . we present a formulation of an optimal fixed-size controller which represents an optimal pomdp policy of a given controller size. to illustrate some of its benefits  we employ a standard nonlinearly constrained optimization technique. nonlinearly constrained optimization is an active field of research that has produced a wide range of techniques that can quickly solve a variety of large problems  bertsekas  1 . the quadratic constraints and linear objective function of our formulation belong to a special class of optimization problems for which many robust and efficient algorithms have been developed. while these techniques only guarantee locally optimal solutions  our new formulation may facilitate a more efficient search of the solution space and produces high-quality results. moreover  the optimization algorithms employ more advanced techniques than those previously used  meuleau et al.  1; poupart and boutilier  1  to avoid convergence at certain suboptimal points.
모the rest of the paper is organized as follows. we first give an overview of the pomdp model and explain how a solution can be represented as a stochastic controller. we briefly discuss some of the previous work on solving pomdps using these controllers and then show how to represent an optimal controller using a quadratically constrained linear program  qclp . we conclude by demonstrating for a set of large pomdps that our formulation permits higher valued fixedsize controllers to be found than those generated by previous approaches while maintaining similar running times. our results suggest that by using our qclp formulation  small  high-valued controllers can be efficiently found for a large assortment of pomdps.
1 background
a pomdp can be defined with the following tuple:
  with
  s  a finite set of states with designated initial state distribution b1
  a  a finite set of actions
  p  the state transition model:  is the transition probability of state s if action a is taken in state s
  r  the reward model: r s a  is the expected immediate reward for taking action a in state s
  붲  a finite set of observations
  o  the observation model:is the probability of observing o if action a is taken  resulting in state s
모we consider the case in which the decision making process unfolds over an infinite sequence of stages. at each stage the agent selects an action  which yields an immediate reward  and receives an observation. the agent must choose an action based on the history of observations seen. note that because the state is not directly observed  it may be beneficial for the agent to remember the observation history. the objective of the agent is to maximize the expected discounted sum of rewards received. because we consider the infinite sequence problem  we use a discount  1 뫞 붺   1  to maintain finite sums.
모finite-state controllers can be used as an elegant way of representing pomdp policies using a finite amount of memory. the state of the controller is based on the observation sequence  and in turn the agent's actions are based on the state of its controller. these controllers address one of the main reasons for intractability in pomdp exact algorithms by not remembering whole observation histories. we also allow for stochastic transitions and action selection  as this can help to make up for limited memory  singh et al.  1 . the finitestate controller can formally be defined by the tuple  where q is the finite set of controller nodes  뷍 : q 뫸 붟a is the action selection model for each node  mapping nodes to distributions over actions  and 붾 : q 뫄 a 뫄 o 뫸 붟q represents the node transition model  mapping nodes  actions and observations to distributions over the resulting nodes. the value of a node q at state s  given action selection and node transition probabilities  is given by:

table 1: the linear program for bpi. variables
represent for a given node  q.

this equation is referred to as the bellman equation.
1 previous work
while many pomdp approximation algorithms have been developed  we will only discuss controller-based approaches  as that is the focus of our work. these techniques seek to determine the best action selection and node transition parameters for a fixed-size controller.
모poupart and boutilier  have developed a method called bounded policy iteration  bpi  that uses a one step dynamic programming lookahead to attempt to improve a pomdp controller without increasing its size. this approach alternates between policy improvement and evaluation. it iterates through the nodes in the controller and uses a linear program  shown in table 1  to examine the value of probabilistically taking an action and then transitioning into the old controller. if an improvement can be found for all states  the action selection and node transition probabilities are updated accordingly. the controller is then evaluated and the cycle continues until no further improvement can be found. bpi guarantees to at least maintain the value of a provided controller  but it is not likely to find a concise optimal controller.
모a heuristic has also been proposed for incorporating start state knowledge and increasing the performance of bpi in practice  poupart and boutilier  1 . in this extension  termed biased bpi  improvement is concentrated in certain node and state pairs by weighing each pair by the  unnormalized  occupancy distribution  which can be found by solving the set of linear equations:

for all states and nodes. the variables used are those previously defined and the probability  bp1  of beginning in a node state pair. a factor  붻  can also be included  which allows value to decrease by that amount in each node and state pair. this makes changes to the parameters more likely  as a small amount of value can now be lost in note state pairs. as a result  value may be higher for the start state and node  but it

figure 1: simple pomdp for which bpi and ga fail to find an optimal controller
could instead decrease value for that pair. while these heuristics may increase performance  they are difficult to adjust and not applicable in all domains.
모meuleau et al.  have proposed another approach to improve a fixed-size controller. the authors use gradient ascent  ga  to change the action selection and node transition probabilities and increase value. a cross-product mdp is created from the controller and the pomdp by considering the states of the mdp to be all combinations of the states of the pomdp and the nodes of the controller and actions of the mdp to be based on actions of the pomdp and deterministic transitions in the controller after an observation is seen. the value of the resulting mdp can be determined and matrix operations allow the gradient to be calculated. unfortunately  this calculation does not preserve the parameters as probability distributions. this further complicates the search space and is less likely to result in a globally optimal solution. the gradient can be followed in an attempt to improve the controller  but due to the complex and incomplete gradient calculation  this method can be time consuming and error prone.
1 disadvantages of bpi and ga
both bpi and ga fail to find an optimal controller for very simple pomdps. this can be seen with the two state pomdp with two actions and one observation in figure 1. the transitions are deterministic  with the state alternating when action a1 is taken in state 1 or action a1 is taken in state 1. when the state changes  a positive reward is given. otherwise  a negative reward is given. since there are no informative observations  given only a single node and an initial state distribution of being in either state with equal likelihood  the best policy is to choose either action with equal probability. this can be modeled by a one node stochastic controller with value equal to 1. notice that this example also shows that with limited memory  one node   a stochastic controller can provide arbitrarily greater total discounted reward than any deterministic controller of that same size.
모if the initial controller is deterministic and chooses either action  say a1  bpi will not converge to an optimal controller. the value of the initial controller in state 1 is r 붺r/ 1 붺  and  r/ 1   붺  in state 1. for 붺   1  which is common  value is negative in each state. based on a one step lookahead  assigning any probability to the other action  a1  will raise the value for state 1  but lower it for state 1. this is because the node is assumed to have the same value after a new action is taken  rather than calculating the true value of updating action and transition probabilities. since bpi requires that there is a distribution over nodes that increases value for all states  it will not make any improvements. biased bpi sets equal weights for each state  thus preventing improvement. allowing value to be lost by using a predetermined 붻 does not guarantee controller improvement  and for many chosen values quality may instead decrease.
모likewise  the gradient calculation in ga will have difficulty finding an optimal controller. because meuleau et al. formulate the problem as unconstrained  some heuristic must adjust the gradient to ensure proper probabilities are maintained. for the example problem  some heuristics will improve the controller  while others remain stuck. in general  no heuristic can guarantee finding the globally optimal solution. essentially  this controller represents a local maximum for both of these methods  causing suboptimal behavior. one premise of our work is that a more general formulation of the problem  which defines an optimal controller  facilitates the design of solution techniques that can overcome the above limitation and produce better stochastic controllers. while no existing technique guarantees global optimality  experimental results show that our new formulation is advantageous.
모in general  the linear program used by bpi may allow for controller improvement  but can easily get stuck in local maxima. while the authors suggest heuristics for becoming unstuck  our nonlinear approach offers some advantages. using a single step or even a multiple step backup to improve a controller will generally not allow an optimal controller to be found. while one set of parameters may appear better in the short term  only the infinite lookahead defined in the qclp representation can predict the true change in value.
모ga also gets stuck often in local maxima. meuleau et al. must construct a cross-product mdp from the controller and the underlying pomdp in a complex procedure to calculate the gradient. also  their representation does not take into account the probability constraints and thus does not calculate the true gradient of the problem. techniques more advanced than gradient ascent may be used to traverse the gradient  but these shortcomings remain.
1 optimal fixed-size controllers
unlike bpi and ga  our formulation defines an optimal controller for a given size. this is done by creating a set of variables that represents the values of each node and state pair. intuitively  this allows changes in the controller probabilities to be reflected in the values of the nodes of the controller. to ensure that these values are correct given the action selection and node transition probabilities  quadratic constraints  the bellman equations for each node and state  must be added. this results in a quadratically constrained linear program. although it is often difficult to solve a qclp exactly  many robust and efficient algorithms can be applied. our qclp has a simple gradient calculation and an intuitive representation that matches well with common optimization models. the more sophisticated optimization techniques used to solve qclps may require more resources  but commonly produce much better results than simpler methods.

table 1: the qclp defining an optimal fixed-size controller. variable represents   variable y q s  represents v  q s   q1 is the initial controller node and ok is an arbitrary fixed observation.모the experimental results suggest that many pomdps have small optimal controllers or can be approximated concisely. because of this  it may be unnecessary to use a large amount of memory in order to find a good approximation. as our approach optimizes fixed-size controllers  it allows the algorithm to scale up to large problems without using an intractable amount of space. in the rest of this section  we give a formal description of the qclp and prove that its optimal solution defines an optimal controller of a fixed size.
1 qclp formulation
unlike bpi  which alternates between policy improvement and evaluation  our quadratically constrained linear program improves and evaluates the controller in one phase. the value of an initial node is maximized at an initial state distribution using parameters for the action selection probabilities at each node p a|q   the node transition probabilities   and the values of each node in each state v  q s . to ensure that the value variables are correct given the action and node transition probabilities  nonlinear constraints must be added to the optimization. these constraints are the bellman equations given the policy determined by the action selection and node transition probabilities. linear constraints are used to maintain the proper probabilities.
모to reduce the representation complexity  the action selection and node transition probabilities are merged into one 
and
모this results in a quadratically constrained linear program. qclps may contain quadratic terms in the constraints  but have a linear objective function. they are a subclass of general nonlinear programs that has structure which algorithms can exploit. this produces a problem that is often more difficult than a linear program  but possibly simpler than a general nonlinear program. the qclp formulation also permits a large number of algorithms to be applied.
모table 1 describes the qclp which defines an optimal fixed-size controller. the value of a designated initial node is maximized given the initial state distribution and the necessary constraints. the first constraint represents the bellman equation for each node and state. the second and last constraints ensure that the variables represent proper probabilities  and the third constraint guarantees that action selection does not depend on the resulting observation which has not yet been seen.
theorem 1 an optimal solution of the qclp results in an optimal stochastic controller for the given size and initial state distribution.
proof the optimality of the controller follows from the bellman equation constraints and maximization of a given node at the initial state distribution. the bellman equation constraints restrict the value variables to valid amounts based on the chosen probabilities  while the maximum value is found for the initial node and state. hence  this represents an optimal controller.
1 methods for solving the qclp
constrained optimization seeks to minimize or maximize an objective function based on equality and inequality constraints. when the objective and all constraints are linear  this is called a linear program  lp . as our formulation has a linear objective  but contains some quadratic constraints  it is a quadratically constrained linear program. unfortunately  our problem is nonconvex. essentially  this means that there may be multiple local maxima as well as global maxima  thus finding globally optimal solutions cannot be guaranteed.
모nevertheless  a wide range of nonlinear programming algorithms have been developed that are able to efficiently find solutions for nonconvex problems with many variables and constraints. locally optimal solutions can be guaranteed  but at times  globally optimal solutions can also be found. for example  merit functions  which evaluate a current solution based on fitness criteria  can be used to improve convergence

figure 1: hallway domain with goal state designated by a star
and the problem space can be made convex by approximation or domain information. these methods are much more robust than gradient ascent  while retaining modest efficiency in many cases. also  the quadratic constraints and linear objective of our problem often permits better approximations and the representation is more likely to be convex than problems with a higher degree objective and constraints.
모for this paper  we used a freely available nonlinearly constrained optimization solver called snopt  gill et al.  1  on the neos server  www-neos.mcs.anl.gov . the algorithm finds solutions by a method of successive approximations called sequential quadratic programming  sqp . sqp uses quadratic approximations which are then solved with quadratic programming  qp  until a solution to the more general problem is found. a qp is typically easier to solve  but must have a quadratic objective function and linear constraints. in snopt  the objective and constraints are combined and approximated to produce the qp. a merit function is also used to guarantee convergence from any initial point.
1 experiments
in this section  we compare the results obtained using our new formulation and the snopt solver with those of bpi and biased bpi. ga was also implemented  but produced significantly worse results and required substantially more time than the other techniques. in the interest of saving space  we omit the details of ga and focus on the more competitive techniques  bpi and biased bpi  which we implemented according to their original descriptions in  poupart and boutilier  1  and  poupart and boutilier  1 . choices of the loss parameter  붻  in biased bpi often decreased performance in the second domain studied. because of this  the best value for 붻 was found experimentally after much trial and error. this search centered around the value suggested by poupart and boutilier   maxs ar s a  mins ar s a  /1 붺 . note that our goal in these experiments is to demonstrate the benefits of our formulation when used in conjunction with an  off the shelf  solver such as snopt. the formulation is very general and many other solvers may be applied. in fact  we are currently developing a customized solver that would take further advantage of the inherent structure of the qclp and increase scalability.
모two domains are used to compare the performance of the qclp and the two versions of bpi. each method was initialized with ten random deterministic controllers and we report mean values and times after convergence. to slightly increase the quality of the qclp produced controllers  upper and lower bounds were added. these represent the value of taking the highest and lowest valued action respectively for an infinite number of steps. while the experiments were

figure 1: hallway domain: the mean values using bpi and the qclp for increasing controller size
conducted on different computers  we expect that this affects solution times by only a small constant factor in our experiments. mean time for biased bpi with or without a delta is not reported as the time for each was only slightly higher than that for bpi.
1 hallway benchmark
the hallway domain  shown in figure 1 was introduced by littman  cassandra and kaelbling  and is a frequently used benchmark for pomdp algorithms. it consists of a grid world with 1 states  1 observations and 1 actions. there are 1 squares in which the robot may face north  south  east or west  and a goal square. the robot begins in a random location and orientation and must make its way to a goal state by turning  going forward or staying in place. the start state is never the same as the goal state and the observations consist of the different views of the walls in the domain. both the observations and transitions are extremely noisy. only the goal has a reward of 1 and the discount factor used was 1.
모the results for this domain are shown in table 1 and figure 1. we see that for all controller sizes the mean value produced by the qclp is greater than those produced by each version of bpi. biased bpi  and in particular biased bpi with a 붻 of 1  improves upon bpi  but the quality remains limited. the value of an optimal pomdp policy is not known. however  the value of an optimal policy for the underlying mdp  which represents an upper bound on an optimal pomdp policy  was
sizeqclphalbpihalqclpmacbpimac1  1 min  1 min  1 min1 mins1  1min  1 min  1 min1 mins1  1 min  1 min1 mins1 mins1.1 mins1 mins1 mins1 mins1.1 mins1mins1 mins1 mins1.1 mins1mins1 mins1 minstable 1: mean running times for the qclp and bpi on the hallway and machine problems
found to be 1. the qclp provides a very good solution to the pomdp that is significantly closer to this bound than that found by bpi.
모the time taken to produce these controllers remains mostly similar  but the difference increases as controller size grows. while in this case the significantly higher quality results produced using qclp required more computation time  it is not generally the case that the qclp is less efficient. also  it can be noted that a four node controller improved by the qclp produces a higher quality solution than any bpi method for any controller size while using very little time.
모poupart and boutilier also report results for bpi on a 1 node controller. in their implementation  an escape technique which attempts to avoid local maxima  was also used. after 1 hours  bpi produced a controller with a value of 1. our qclp formulation generates a 1 node controller with 1% higher value in 1% of the time with 1% of the size.
1 machine maintenance
in order to test performance on a larger problem  we consider a machine maintenance domain with 1 states  1 actions and 1 observations  cassandra  1a . there are four independent components in a machine used to manufacture a part. each component may be in good  fair or bad condition as well as broken and in need of a replacement. each day  four actions are possible. the machine can be used to manufacture parts or we can inspect  repair  or replace the machine. the manufacture action produces good parts based on the condition of the components. good components always produce good parts  and broken components always produce bad parts. components in fair or bad condition raise the probability of producing bad parts. the condition of the resulting part is fully observed. inspecting the machine causes a noisy observation of either good or bad for each component. components in good or fair condition are more likely to be seen as good and those in fair or broken are more likely to be seen as bad. repairing the machine causes parts that are not broken to improve one condition with high probability. the replace action transitions all components to good condition. components may degrade each day unless repaired or replaced. rewards for each action are: 1 for manufacturing good parts for the day  -1 for inspecting  -1 for repairing and -1 for producing bad parts. a discount factor of 1 was used.
모for this problem  again the value of an optimal pomdp policy is not known  but the value of an optimal policy for the underlying mdp is known to be 1. we see in figure 1 that very small controllers produced by the qclp provide solutions with values which are exceptionally close  1%  to this upper bound. the qclp generates higher quality controllers for all sizes and we are encouraged to see particularly significant performance improvement for small controller sizes. again  a four node controller produced with the qclp representation has a higher mean value than any bpi approach.
모running times  as seen in table 1  are more favorable for our fomulation in this problem. the qclp required less time than bpi for small controller sizes and remains reasonable for larger controller sizes. while more time is required for larger controller sizes  these results suggest that the qclp repre-

figure 1: machine domain: the mean values using bpi and the qclp for increasing controller size
sentation not only permits controllers to be found that outperform those provided by bpi  but finding solutions can also be more efficient for small controller sizes. a real strength of the qclp approach seems to be finding very compact high valued controllers.
1 conclusions
we introduced a new approach for solving pomdps using a nonlinear programming formulation of the problem which defines an optimal fixed-size stochastic controller. this allows a wide range of powerful nonlinear optimization algorithms to be applied. we showed that by using a standard nonlinear optimization algorithm  we can produce higher valued controllers than those found by the existing state-of-theart approach  bpi. our approach provided consistently higher values over a range of controller sizes even when the best possible heuristic settings were used for bpi in each domain. we also showed that better solutions can be found with significantly smaller controllers by using the qclp. these results suggest that concise optimal or near-optimal controllers could be found for large pomdps  making our approach very useful in a range of practical application domains. we are particularly encouraged by the ability of our approach to generate a very small controller with 1% higher value in 1% of the time used by bpi on a large  1 node  controller.
모this work opens up new promising research directions that could produce further improvement in both quality and efficiency. a better understanding of the relationship between the nonlinear formulation and various optimization algorithms may improve performance. different representations may better match current nonlinearly constrained optimization methods and may thus produce better solutions. one goal is to identify subclasses of pomdps for which globally optimal solutions can be efficiently found. also  we are currently working on a customized qclp solver that can take advantage of the specific structure inherent in pomdps. this new approach shows promise for scaling up the size of controllers that are efficiently solvable with our formulation.
모the qclp technique is competitive with the state-of-theart and in the future  we will conduct a more comprehensive evaluation of our approach with respect to a wide range of approximation techniques and domains. bpi has already been shown to be competitive with other leading pomdp approximation techniques such as perseus  spaan and vlassis  1  and pbvi  pineau et al.  1 . this suggests that the qclp approach compares favorably with these techniques as well. we are confident that our experiments will verify this as well as provide insight into the strengths and weaknesses of the qclp approach in relation to other techniques.
모finally  we have begun to study the applicability of the qclp approach to problems involving multiple agents modeled as decentralized pomdps  amato et al.  1 . the bpi technique has already been generalized successfully to the multi-agent case  bernstein et al.  1 . based on our recent experiments  we are confident that the qclp approach can also be used to find concise near-optimal fixed-size controllers for a set of agents in large multi-agent domains.
1 acknowledgments
we would like to thank the anonymous reviewers for their helpful comments. support for this work was provided in part by the national science foundation under grant no. iis1 and by the air force office of scientific research under agreement no. fa1-1.
