 
　　　　diagnosing a system requires the identification of a set of components whose abnormal behavior could explain the faulty system behavior. previously  model-based diagnosis schemes have proceeded through a cycle of assumptions -* predictions observations assumptions-adjustment  where the basic assumptions entail the proper functioning of those components whose failure is not established. here we propose a scheme in which every component's status is treated as a variable; therefore  predictions covering all possible behavior of the system can be generated. remarkably  the algorithm exhibits a drastic reduction in complexity for a large family of system-models. additionally  the intermediate computations provide useful guidance for selecting new tests. 
　　　　the proposed scheme may be considered as either an enhancement of the scheme proposed in  de kleer  1  or an adaptation of the probabilistic propagation scheme proposed in  pearl  1  for the diagnosis of deterministic systems. 
i introduction 
　　　　the diagnosis of a system exhibiting abnormal behavior consists of identifying those subsystems whose abnormal behavior could produce the manifested behavior. in model-based diagnosis  the system is treated as an idealized structure of components whose local behaviors interact to produce overall system behavior. previous diagnostic schemes have tackled the task by partitioning the problem into two phases. first  making use of a set of observations and assuming the proper local behavior of components  predictions arc generated about the behavior of unobserved points. in the second phase  the assumptions underlying those predictions contradicted by observations are identified  and a set of hypotheses about the individual component's behavior which  best  explains the manifested system behavior is assembled. 
while many proposed schemes fit within this paradigm 
 reiter 1  genesereth 1   the work of  de kleer et al  1  has especially focused on algorithms for performing these two tasks efficiently. in their scheme  predictions are generated by constraint propagation  stallman et al.1   while the process of identifying the  minimal  set s  of assumptions underlying a contradicted prediction  conflict sets  is facilitated by the atms machinery  de kleer 1 . the diagnoses chosen as  best  are those minimal sets of assumptions  candidates  which  if removed  render the model behavior compatible with the manifested behavior. these are assembled from the conflict sets by a set-covering algorithm. 
 this work was supported in part by the national science foundation grant dsr 1. 
　　　　the diagnostic algorithm we propose including here consists of a single task: instead of predicting only those behaviors which assume the proper functioning of components  the proposed algorithm generates explanations for all possible behaviors  including  in particular  the optimal diagnoses that account for each new observation. the algorithm takes advantage of the fact that there is usually a small set of hypotheses compatible with the current set of observations that will best explain any single new observation. the resulting scheme 
  diagnoses failures due to multiple faults; 
  is incremental; and  in contrast to two-phase approaches  
  fully exploits the topology of the system model under diagnosis. 
　　　　the diagnostic algorithm reported here grew out of the analysis and comparison between the probabilistic scheme proposed in  pearl 1   and applied to diagnosis in  geffher et al. 1   and the non-probabilistic approach exemplified by  de kleer et al. 1 . as it will be discussed below  the resulting scheme can be viewed either as an enhancement of the latter or as a non-probabilistic simplification of the former. 
　　　　we shall first introduce some notation and present the propagation algorithm for singly-connected constraint networks models  section ii . its application to the diagnosis of a simple digital circuit is discussed in section iii. we then extend the propagation scheme to properly handle both multiply-connected networks as well as component models with different prior probabilities of failure  section iv . we conclude with a discussion of related work  section v   and a summary of the main results  section vi . 
ii the proposed scheme 
　　　　for the purpose of illustrating the ideas underlying the proposed scheme  let us first introduce some convenient notation. let l sjo  denote a labeling that assigns a status to each component of a system s  compatible with a set of observations 1. for the time being  we shall assume that the merit of a given diagnosis is determined by the number of faulty components involved; so  every labeling l s o  will be assigned a figure of  de merit denoted by n s  1   representing the number of s -components labeled  faulty  

　　　　let us now consider two complex system circuits  s 1 and s1  with outputs x 1 and x1* and observation sets 1 and 1. respectively  as depicted in figure 1. let us also assume that  for each of these circuits  we have computed a set of diagnoses accounting for both their observation sets and each of the possible values of their output variables  xx and x 1 . in other words  two sets of labelings  are available  each 
	geffner and pearl 	1 

figure 1 
we can now consider the composite circuit formed by an adder a 
with output y and inputs xx and x 1 . we are interested in finding the best labelings over the components of s compatible with the set of observations  and each of the possible values yofy. 
in other words  we want to determine the labeling. for each value y of y. 
　　　　the 	main 	point 	to 	note 	is 	that  	to 	determine we need not reconsider all the combinations of 
labelings over the individual circuits s 1 and is made up of labelings that were already found optimal overs  and s1 for some values of x1 and x1. in particular  assuming that the adder a is working properly  denoted by a ＊ ok t the weight associated with the optimal labeling will be given by: 
	 1  
where x1 and x1 range over the possible values of x1 andx1  respectively. thus  if the minimum is achieved at values x1 and x1  the optimal labeling for y=y under the assumption a =1k can be constructed from: 
	 1  	 1  
to find the overall optimal labeling  we must also include the possibility that a is not working properly  denoted by a = -ok  yielding: 
the constant 1  stands for the penalty associated with component a being faulty  which in turn removes the constraint between the values of  notice that the terms required to compute  1  and  1  depend only on the subcircuits  and were assumed to be available. 
　　　　the overall best iabeling s  compatible with y =y can now be obtained simply by comparing the weight computed from  1  with the one computed from  1  and choosing the labeling s  corresponding to the lowest one. if y =y is now observed  would emerge as the labeling defining the best diagnosis. 
　　　　a question remains  however  of how to minimize over sets of values that  in principle  may have infinite cardinality. its solution is based on the fact that there will be labelings which are compatible with all the values a variable can take. for example  in the simple adder circuit depicted in figure 1  we have only two labelings  
  the best labeling compatible with x =1 is  a mok  while  for any other value of x  {a =-ok; is the best and  in fact  the only compatible labeling. moreover  {a =-ok} is the best labeling compatible with any value of x except x = 1 . we will take advantage of this fact and will use the symbol  as a place 
1 	reasoning 
figure 1 - best labelings far x 
holder for all those  possible infinite  values of x which share the same optimal labelings. refeming to the example above  we will say that {a m=-ok} is the best labeling compatible with   meaning that it is the best  and probably only  labeling compatible with any value of x. the introduction of these  special  values will allow us to keep the state representation of the problem concise; the algorithm  however  should behave as though an explicit representation were used for each possible value. 
　　　　the example above illustrates two important properties on which our algorithm is based: first  that the problem of determining the optimal labeling for a variable can be solved from information associated with neighboring variables  making feasible a distributed diagnostic inference engine in the form of constraint propagation; and secondly  that it is possible to incrementally predict all the possible model behaviors by encoding a finite  and usually small  set of  explanations   allowing the constraint propagation algorithm to accomplish the global identification task without appealing to nonlocal set-covering procedures. 
　　　　a critical assumption implicit in the example of figure 1 is that circuits s 1 and s 1 are only connected via a or  more generally  that the constraint network induced by the composite circuit s is singly-connected. in section iv this assumption will be relaxed and we shall generalize the algorithm to deal with multiply-connected constraint networks. these extensions will provide interesting points of comparison with  de kleer  1 . 
a. notation 
     we assume that a component can be in either of two states: it is working properly  denoted by or it is failing  denoted by . since we are going to treat components as any other variable in the model  we will usually draw them as nodes as in figure 1b. rather than depict them as blocks as in figure 1a.  where  stands for the name of the variable 
corresponding to the component status and j denotes the undirectional constraint among the component inputs   its output s  and its status. 

	a. 	b. 
figure 1   components as nodes of a constraint network 
the resulting network is composed of 
  nodes  or variables   denoted by upper-case letters 
 x  y  w .... for observable system points and c   cj ...  for components ; 
  constraints  denoted by small letters  ij ... ; and 

  links connecting nodes to constraints. 
　　　　the set of nodes linked to a constraint k will be denoted by pk. a more useful set is / which reads as the port k of node x  and stands for the set of all other nodes different than x linked to constraint k.  will refer to the set of constraints linked to x. lower-case letters x  y  w  ... c'  cj ...  will refer to the values of variables x  y  iv ...  c' c ;   and subindices  wil be used to differentiate among the values of a single variable. we will extend this convention to refer to px as the values of the variables in the port px. for example  the port px in figure 1 is the set of variables {y z cj}  while px stands for their associated values 
 as stated before  the  value  fy will serve as a place holder for all those values of node y that are not explicitly represented. 
         in constraint-propagation algorithms such as the one used here  the ports of a variable contain enough information for the variable to update its state. this updating is usually done incrementally  whenever a new state of a variable can be deduced from a neighbor constraint and its associated ports. we will use messages  denoted as  where node x is the recipient and port the origin  to 
encode the influence of the values of the variables in px upon x through the constraint k. 
b. the algorithm 
     messages: messages m px x  are built from a vector of sub-messages  where the x  's refer to values x may take. each of these submessages is composed of three fields: x   and p   where the last parameter stands for the set of 
values of px which would  best  support the potential observation x =x - and the second one for the weight associated with such support  i.e.  the minimum number of faulty components  in the subnetwork behind the port   necessary to account for in short  a message can be expressed as: 

i 
  for some value x  of x  we simply remove x  from the set of values x that needs to be considered. 
     message combination: the first requirement for message combination is that every node save the last message received from each of its ports. new messages override old messages from any one port. from this set of messages  every node computes its state m x . that is  if we denote the combination of messages by the symbol then  for each of its values xt  x computes the states: 
 1  
where 
 1  
is called the weight associated with x  and represents the minimum number of faulty components necessary to account both for the current set of observations and the instantiation x = xi;  and 
 1  
the support of xi  encodes the state s  of the neighborhood of x that will best account for all the observations and the instantiation upon observing x = xi;  the set of optimal diagnoses can be easily retrieved by tracing the support associated with represents the number of faults involved in any of these hypothetical diagnoses. 
	message assembling: 	let port px contain the variables y1  
　　　then the parameters of the message are computed from: 
and where the y; 's range over the values of yj  and denotes a predicate indicating the compatibility of its arguments according to constraint k. the substraction of the component nk y  from the weight n y  in  1  and  1  amounts to subtracting the contribution to n y  that originated in the port pk. this  orthogonalization  is required to avoid counting the same fault more than once within a single diagnosis. 
initialization: the propagation algorithm requires that nodes be properly initialized. a node  c  representing a component status will be initialized to: 

while any other variable y will have an initial state: 

observations: the observation x =x is codified as a message from a virtual port ox of x: 
		 1  
where absence of sub-messages corresponding to other values of x is interpreted as for any x  # x. for the purpose of applying formulas for any observable node x  virtual ports ox are taken to be members of the set of ports linked to x. 
control of the propagation process: while an orderly and incremental propagation scheme would be the most efficient implementation in serial machines  the algorithm can also work under distributed control  in which each node inspects the state of its ports at its own discretion. the final state reached at equilibrium would be the same. 
　　　　we now proceed to illustrate the working of the algorithm on a simple circuit discussed in  de kleer 1    oenesereth 1  and  davis 1 . 
ill example 
let us consider the circuit depicted in figure 1: components m.m1 and m 1 are multipliers  while a 1 and a 1 are adders. the former will correspond to constraints c 1 c1 and c1  and the latter to constraints c1 and c1. initially  all inputs are known and propagated through the rest of the network  generating the pattern of messages shown in figure 1 a . figure 1 b  displays the new states of nodes x and f. 
geffner and parl 1 

support of x 
1 	reasoning 

       the computations required by the algorithm also provide useful information for selecting points to test. not only do the resulting data structures encode the best diagnoses accounting for any potential observation at any point in the circuit  but they also encode  through the supports attached to component nodes  the network state most compatible with any component status. this turns out to be especially useful when we want to select test points to discriminate between different hypotheses. 
       let us also note here that the scheme proposed does not guarantee rinding all irredundant diagnoses  peng 1   but only those with a minimum number of faults  i.e.  the optimal diagnoses. 
iv enhancements 
       in this section we will discuss some modifications to the scheme introduced in section ii to enhance the types of models with which the algorithm can deal. we first discuss how to extend the message-passing algorithm to handle constraint networks containing loops and then propose a slight modification to accommodate models with specified component-failure probabilities. 
a. handling constraint networks with loops 
       the strength of the proposed scheme lies in its ability to decompose global optimization problems into local ones. to achieve this  we assumed that messages received by a node from different ports carried independent information  i.e.  do not emanate from common observations. it is easy to find systems in which this assumption is violated  as in the example of figure 1. 

figure 1 - a circuit containing a loop. 
clearly  the information carried by both inputs of c1 will depend on the status of c1. thus  knowing m  y  and m  w  no longer suffices to compute m z . the reason is simply that  for some pairs of values y and w of y and w  respectively  m  y  and m  w  might involve a common set of faulty components; so  the weight associated with a value z of z supported by y and w will not necessarily be the sum of the weights of its supporters. in addition  m y  and m w  may involve incompatible labelings of x; so  we must ensure that they do not appear in a same support set. 
　　　　the solution that we will pursue is not new  peart 1.a   dechter & peari 1  and rests on the idea of treating a  loopy  network as a family of singly-connected networks in which some of the variables  usually those corresponding to a cycle cutset and referred to here as  assumption  nodes  have been assigned a fixed value  figure 1 . a fixed-value node can be duplicated into a set of identical nodes  each connected to one and only one of its original ports while still preserving the overall behavior. to illustrate the modifications needed to handle loops  let us consider constraint network s  figure 1  in which the instantiation of assumption node a decomposes it into a pair of singly-connected networks  s  and 
s1. let oi denote the set of observations gathered in the subnetwork j. t   f o 	represent 	the complete set of 
observations. 	recalling the notation introduced in section ii  
n s 1  stands for the number of faulty components needed to ac-

fig. 1 - loopy network treated as a family of singly-connected networks. 

figure 1 - breaking loops by introducing assumptions. 
count for the set of observations 1 in system s.n y  was then used as a shorthand for we shall now use the abbreviations: 

and 
		 1  
　　　　notice that since the s  's are singly-connected  the measure wa  x   i.e. the minimum number of faults in 1   needed to account for the observations   can be computed according to the procedure discussed above. we are interested  however  in computing the weight  i.e.  the total number of faults in s needed to account for the entire set of observations  without any assumption about the value of a. 
this can be obtained by writing: 
		 1  
where 
		 1  
thus  if the minimum is achieved at a = a *  the optimal set of diagnoses will be obtained from the supports associated with x=x under the assumption a =a* and from those associated with a =a* itself. from  1  it is clear that we will be able to compute n y  for any node y if we can compute wa  x  for every value a of a and every value x of the nodes x in s. 
b. the algorithm 
	geffner and petri 	1 

　　　　each instantiation of the cutset variables identifies a set of singly-connected networks. we shall refer to the instantiated nodes as assumption nodes  to their instantiations as assumptions or tags  and to the states of the net compatible with a given set of assumptions as contexts. the manner in which tags are treated in this modified scheme bears a strong resemblance to the way assumptions are treated in the atms  de kleer 1 . 
     as with node values  the message-passing algorithm will not explicitly represent each of the possible values but will appeal  instead  to the special values introduced in section ii. now  however  tags containing values will be required to specify the range of values for which any -value stands. for that purpose  we will use i as the place holder for values of x other than this will render a tag containing the instantiation incompatible with any tag containing the instantiation the 
　-values assigned to non-assumption nodes are not required to keep this extra information explicit; so  for them  we use our previous  simpler notation. messages will now have the form : 

where a represents the tag of the message  and each submessage 

where 
 represents the component of w1 x  originated in the ik-th 
port of x; denotes its underlying support and 
 stands for the contribution   as received from the it -th port. 
　　　　note that  does not depend on the value of x. we have included it among the sub-messages in order to simplify the presentation. 
　　　　the state of a node is computed by combining all messages with compatible tags received by the node from its different ports. for a value x of node x  the resulting state m  x  is given by: 
		 1  
where are computed as usual  within the context is computed from: 
 1  
　　　　messages are now assembled as in the case of singlyconnected networks  with the additional constraints imposed by the tags. 
when a new observation  y =y  is obtained  the optimal weight can be computed according to eq. 1 : 
		 1  
where the weight n a  is available at node a  and both ny a  and 
1 	reasoning 
wa  y  are available at y.   and the minimum is achieved at a = a *   the set of optimal diagnoses can be obtained by tracing the supports . additionally  the message posted by the observation y=y will have the form: 
		 1  
where the last component amounts to setting n' a  to the current value of wa  y   as specified by eqs. lo  and  1 . 
     another problem generated by the presence of loops in constraint networks is the inability of unaided local constraintpropagation methods to enumerate  in advance  all the distinguished instantiations of the cutset variables. for instance  if we regard the components in figure 1 as adders  and we happen to observe z=1 instead of the predicted z=1  the only value of x compatible with 
　　　　　　　　　　to arrive at this conclusion  we need either to solve a linear equation or to step sequentially through all the values in the domain of x. one way to obtain these solutions would be to permit the engine to propagate symbolic values  stallman 1 . this approach seems suitable for implementation in the scheme proposed here and corresponds to viewing the i values of assumption nodes as symbolic values. 
　　　　for an illustration of how the proposed algorithm would handle the diagnosis of a circuit like the one depicted in fig. 1  the reader might refer to  gefrher etal. 1 . 
c. varying failure rates 
　　　　the criterion of minimizing the number of faulty components is reasonable in situations where there are no reasons to believe that different components fail with significantly different rates. if such is not the case  information about component failure rates could be easily integrated in the scheme proposed. one needs only to change the initial states of the component nodes. for example  instead of initializing component node c   to: 
j 
we can initialize it at: 
where pi is the probability of failure in component ct  and k is any suitable constant 
　　　　for those cases  the majority  in which the components* failures are independent  the set of diagnoses obtained with this slight modification  will be those with the highest probability. 
v related work 
　　　　the diagnostic algorithm presented here grew out from the analysis and comparison of the scheme proposed in  de kleer et ai 1  and the one reported in  geffner et al. 1 . the former is a typical two stage algorithm. predictions that assume the proper functioning of components whose failure was not established  are matched against the observations. a mismatch identifies a set of assumptions  conflicts  which cannot simultaneously hold. these sets are obtained from the tags attached to the predictions refuted. from these sett  a non-local set-covering algorithm constructs the set of minimal diagnosis  i.e. minimal sets of violated assumptions which explain the observations. 

　　　　the diagnostic scheme reported in  geffher et al. 1  is a probabilistic scheme in which both  right  and  wrong  behavior are considered on the same basis. knowledge of the network topology  the singly-connectedness of the network is embedded in the algorithm  and a set of probabilistic goodness measures attached to hypotheses  allow copious prunning  avoiding the combinatorial explosion characteristic of unaided multi-hypothesis diagnostic algorithms. on the other hand  the scheme seems unnecessarely expensive for many applications  and it also requires the assessment of the probability distribution associated with the component's i/o which  generally  is not available. 
　　　　the scheme proposed here borrows from the latter what we consider to be its two main features : knowledge of the network topology  and numeric qualification of candidate hypotheses. the algorithm however can be interpreted as embedding two further assumptions : very small failure rates and uniform probability over the possible output values of failed components. the resulting diagnostic scheme can be thought then  as the non-probabilitic version of  oeffner et al. 1   in the sense that it can only accommodate a narrow set of probabilistic models. on the other hand  it only requires to propagate one type of message  it does not require the assessment of probabilities and it is sensibly less computationally expensive. compared to  de kleer et al. 1  and  reiter 1   it does not require a non-local set covering procedure for identifying the culprits. moreover in the singly-connected case  the knowledge of the network topology permits to tag messages only with a numeric measure rather than with the identity of the assumption set in the multiplyconnected case however  tags identifying the cutset assumptions are needed. in this respect  the scheme reported in  de kleer et al. 1  which does not presuppose any knowledge about the type of network  appears to be treating all variables as cutset variables. for sparse networks however  the preprocessing step needed to identify a cut set may be worthwhile. 
vi conclusions 
　　　　we have introduced a distributed diagnostic algorithm which fully exploits the topology of the network of the system being diagnosed. the algorithm has linear complexity for singly-connected networks and a worst-case complexity of exp   cycle-cutset i  for multiply-connected networks. 
　　　　the proposed scheme departs from previous work by treating each component status as a variable  thus facilitating the prediction of all possible model behaviors. this allows the messagepassing algorithm to perform the diagnostic task without appealing to non-local set-covering procedures. it also simplifies probabilistic approaches like  pearl 1  by taking advantage of the deterministic nature of the models analyzed. 
　　　　the intermediate computations generated by the algorithm provide information useful for the selection of new tests. additionally  information about component failure rates can easily be accommodated. 
