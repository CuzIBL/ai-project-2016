 
we investigate the problem of non-covariant behavior of policy gradient reinforcement learning algorithms. the policy gradient approach is amenable to analysis by information geometric methods. this leads us to propose a natural metric on controller parameterization that results from considering the manifold of probability distributions over paths induced by a stochastic controller. investigation of this approach leads to a covariant gradient ascent rule. interesting properties of this rule are discussed  including its relation with actor-critic style reinforcement learning algorithms. the algorithms discussed here are computationally quite efficient and on some interesting problems lead to dramatic performance improvement over noncovariant rules. 
1 introduction 
much recent work in reinforcement learning and stochastic optimal control has focused on algorithms that search directly through a space of policies rather than building approximate value functions. policy search has numerous advantages: domain knowledge may be easily encoded in a policy  the policy may require less representational power than a value-function approximation  there are simple extensions to the multi-agent domain  and convergent algorithms are known. furthermore  policy search approaches have recently scored some encouraging successes  bagnell and schneider  1    baxter et al  1 including helicopter control and game-playing. 
　one interesting aspect of existing gradient search algorithms  however  is that they are non-covariant; that is  a simple re-parameterization of the policy typically leads to a different gradient direction.  not that found by applying the jacobian of the re-parameterization to the gradient . this is a odd result; it is intuitively difficult to justify the gradient computation as actually indicating the direction of steepest descent. this problem is well recognized in the patternrecognition and statistics communities and is a very active area of research.  kakade  1  was  to the best of our knowledge the first to identify this problem in reinforcement learning and to suggest that techniques from information geometry may prove valuable in its solution. 
　inspired by the work of  amari and nagaoka  1   kakade proposed a  natural gradient  algorithm. in particular   kakade  1  proposed a scheme for generating a metric on parameter space that has interesting theoretical properties. most convincingly  kakade showed strong empirical evidence for the algorithm's usefulness. in particular   kakade  1  applies the algorithm to the tetris problem of  bertsekas and tsitsiklis  1 . this problem is particularly interesting as value function methods  as for example described in  bertsekas and tsitsiklis  1    demonstrate non-monotone performance; first policy iteration improves the policy dramatically  but after a small number of iterations the policy gets much worse. normal gradient methods  including second-order and conjugate methods also prove very ineffective on this problem; even after a tremendous number of rounds they only mildly increase the performance of the game-player. the method presented in  kakade  1  shows rapid performance improvement and achieves a significantly better policy than value-function methods  at their peak  in comparable time. 
　however  despite recognizing an interesting defect in the general approach and intuiting an apparently powerful algorithm  kakade concludes that his method also fails to be covariant leaving the problem open. we present here what we believe to be an appropriate solution. 
　in kakade's work  there is no proper probability manifold  but rather a collection of such  one for each state  based on the policy. as such  kakade must rely on an ad-hoc method for generating a metric. here instead we take a proper manifold  the distribution over paths induced by the controller  and compute a metric based on that. in the special case appropriate to the average reward rl formulation  kakade's scheme is shown to give rise to a bona-fide natural metric  despite the observation in the paper that the learning was non-covariant. we believe this is an artifact- perhaps of step-length. further  we note that parametric invariance does not require this metric- there are numerous covariant metrics. rather  a stronger probabilistically natural invariance demands the metric used. we describe this result to motivate our method. importantly  the notion of the metric on the pathdistribution allows us to provide very simple and natural extensions to kakade's algorithm that cover the finite horizon  discounted start-state  and partially-observed reinforcement learning problems as well as the average reward one. fi-

nally  for completeness  we describe the result discovered by kakade relating the invariant metric and compatible actorcritic methods  sutton et al  1 . 
1 	problem setup and notation 
a stochastic control problem consists of paths  also called system trajectories  in a space  a distribution over pathspace  that is a function of a sequence of  finite  controls indexed by time  from a space a. throughout this paper we will be considering partially observed markov decision problems  in which there are state-variables of the system that compose a path and render the past and future independent. in a pomdp  is defined by an initial state and next state transition probabilities 
we also typically have a sequence of outputs  observations  measurable with respect to . a controller  or policy   usually parameterized by is a feedback-type strategy that maps the history of observations to a distribution over controls  most of the derivations will be given in terms of memoryless stochastic controllers that map the current state to a distribution over actions  although they can be easily extended to finite window or recurrent representations that operate on only the observations. the goal in a control problem is to maximize the expected reinforcement  with respect to  the sum is 
always assumed to exist for all controllers. the reward function on paths in a pomdp is additive in time  or in the infinite time case  discounted or averaged   and a function r of state  although the algorithms discussed here are not necessarily predicated on that assumption. reinforcement learning is the adaptive version of the control problem where we attept to maximize the expected reinforcment by sampling tra-
jectories and then improving our policy. we use the notation  throughout to denote where the parameter should be 
clear from context. for vectors and we use the notation  to denote the inner product with respect to metric m. the notation indicates the expected value of the function / with respect to the distribution 
1 covariance and riemannian manifolds 
1 meaning of steepest ascent 
for direct policy search methods  we are interested in finding the direction of steepest ascent with respect to our current parameters. that is  we wish to maximize the reward function  subject to an infinitesimal. if we reparameterize our controller in terms of  e.g.  and express the same effective infinitesimal policy change in terms of these parameters  will of course remain the same. however  if we measure lengths using the naive dot product  the same effective change will not have the same size. it is this problem that leads to the odd behavior of  steepest descent  rules. 
1 non-covariant learning rule 
to demonstrate the problem we first consider a simple two state  two action mdp described in  kakade1j.  figure 1  in state 1 executing action 1 returns to the same state and gets a reward of 1. executing action 1 get no reward but transits to 

figure 1: a two-state mdp. each state has two controls one which self transitions  and earns the reward that labels the state  and another that simply transitions to the other state. rewards occur only on the transitions between different states. 
figure 1: log odds of actions for policies logp 
1 	on the two-state mdp  horizontal axis corresponds to state 1 . different curves correspond to varying 	notice the non-covariant policy behavior. 
state 1. in state 1  action 1 returns to state 1 and gets reward 1. action 1 transits to state 1 and achieves no reward. 
consider parameterized probabilistic policies of the form is an arbitrary scale 
parameter that we use to demonstrate that even mildly different parameterization lead to dramatically different behavior. below we plot the resulting track through the space of policies using the log ratio of probabilities of the actions for each of the possible states. we start from a policy that makes it somewhat more likely to choose action 1 in state 1 and action 1 in state 1. we then scale  to 1  .1  and 1 and plot the log odds of the policy from state 1  figure 1 . 
the non-covariant behavior is quite clear in this graph. 
further  we note that it is very difficult to achieve a good policy using this algorithm from this starting point as the wrong action becomes overwhelmingly likely to be chosen from state 1. if sampling were used to compute the gradient  we would nearly never get samples from state 1- which we need to improve the policy. 
1 	path distribution manifolds 
the control problem is essentially a coupled one of optimization and integration over the space of possible paths   this motivates the idea that instead of considering the  arbitrary  distance in terms of parameterization of the policy  we may consider distance in terms of the change in distribution over paths resulting from the policy change. we may view the distribution over paths as a parameterized manifold 
 nominally embedded in of dimension  this takes some work to visualize; as an example consider a space of three possible paths. all possible distributions over these three paths can be smoothly represented with two parameters. in figure 1  left  we see one visualization with the embedding 

1 	probabilistic planning 


figure 1: an example probability manifold over paths. each axis represents the log probability of one of the three possible paths. the manifold is formed as we vary the parameters defining it throughout their domain. on the right we attach the tangent space at a particular point in parameter space. 
　with parameters  assuming no redundancy  we generate an dimensional manifold. in the case pictured  it is the set of all distributions on 1 paths  but in general  the dimensionality of the path probability manifold will be tremendously less than the number of possible paths. it is important to understand the manifold under consideration is that of the probability distribution over paths and not of paths themselves. 
　the study of parameterized manifolds like that pictured above is the domain of differential geometry. here we are interested in establishing a riemannian structure on the manifold of paths. by that we mean we wish to establish a metric on the tangent space  the local linear approximation to the manifold at a point so we can measure small parameter changes. we do this with the metric on the tan-
gent space  which is spanned by the partials with respect to each parameter  as where g is a positive definite matrix. this is a very natural thing to do- instead of just the standard dot product  we have a dot product that allows us to represent rotations and scalings  exactly what a positive definite matrix can represent  and that can vary throughout the manifold. in figure 1  right  we depict the tangent space at a point in our parameterization as the local linear approximation at that point. 
1 	steepest ascent on riemannian manifold 
there are two questions that then naturally arise- what is steepest descent on a function defined over a riemannian space  and is there a riemannian metric on the manifold of the paths that is in some sense natural. we quickly answer the first question  and pursue the second in the next two sections. a lagrange multiplier argument  given schematically here  makes it easy to see the form of the steepest descent direction. 
 1  
 1  form the lagrangian: 

 1  
and take derivatives with respect to each   then set to zero 
to solve for the optimal direction: 

　this implies that  since g is positive definite and hence invertible   giving us: 
		 1  
　that is  the direction of steepest descent is simply the normal gradient times the inverse of the metric evaluated at the point of tangency. we call this the  natural gradient  for the reimannian manifold.  amari and nagaoka  1 
1 invariance and chentsov's theorem 
some confusion seems to surround the choice of metric on probability manifolds. there is  unique answer for this metric  even under the supposition of parametric invariance as suggested by some authors. the issue is rather more subtle. any metric that transforms under parameter change according to the jacobian of the function connecting parameters meets this requirement. this type of parametric covariance is a minimum requirement for us. it is natural to suggest a metric that preserves essential probabilistic aspects of the manifold. consider  for example  functions q  markov mappings  congruent cmbeddings  or sufficient statistics  depending on your viewpoint  that carry one distribution over paths to another in a natural and recoverable way. for example  consider the mapping between the two distributions p paths  and p paths' given by congruent embedding q depicted below: 

paths' 
　the mapping q above interchanges the role of two paths and splits one path into two  with probability 1 for each split . in a probabilistically fundamental way  the manifolds p path  and p path'   where we imagine p path  is a smoothly parameterized set of distributions  are similar to each other. for each path' we can uniquely recover the original probability. in this way  a parameterized distribution over paths can be embedded into a different path space.  this equivalence can actually be phrased in a category theoretic way where the morphisms are these congruent embeddings.  ichentsov  1  general congruent embeddings can 

be thought of as simply generalizations of the example depicted above to allow arbitrary permutations and arbitrary probabilities for different paths stemming from a single path  as well as compositions of these two. in the control problem this could arise by simple permutation of the state variables or change of co-ordinates to ones with redundant information. it is natural to require that with our metric the congruent embedding is an isometry on the tangent space   i.e. preserves the length of the vectors . that is  if we make a change of size  under the metric to the distribution  paths then carrying that change through we should measure the same change c using . adding the requirement of invariance with respect to congruent embeddings leads to a unique  up to scale  metric on the manifold.  chentsov  1  this metric is well-known in statistical inference as the fisher-rao metric and it can be written as the fisher information matrix  degroot  1: 

　another way to derive the metric is to think about  distances  on probability spaces. the kl-divergencc  or relative entropy  between two distributions is a natural divergence on changes in a distribution. it is also manifestly invariant to re-parameterization. if we think about derivatives as small changes in parameters  differentials  then we discover that we also get a unique metric as the second-order taylor expansion of the kl-divergence. this too agrees with the fisher information  up to scale . we note that the direction of the kl-divergence is irrelevant as to second-order famari and nagaoka  1 . 
1 fisher-rao metric on the path-space manifold 
the issue now becomes how to derive the fisher metric on the space of path distributions. it turns out in the case of processes with underlying markovian state to be rather easy  and involves only computations we already make in the likelihood ratio approach standard in gradient-based reinforcement learning. 
1 	derivation of finite-time path metric 
the fisher information 	metric 	involves computing 
. fortunately  this is easy to 
do. the essential algorithm within gradient methods like 
reinforce and gpomdp is a clever method of computing the expected score function. thus  while 
the expected score is the gradient  the correlation of the score is the fisher matrix. the following simple algorithm is unbiased and converges almost surely  under the same regularity conditions as in  baxter et al  1  as the number of sample paths goes to infinity: 
algorithm 1  finite-horizon metric computation  for i in 1 to m: 
1 
- compute zj = i- z + jdjlog1r a x;1  
return g 
　we use the markov property and the invariance of the transition probabilities  given the actions  in the algorithm above to compute 1jogp ♀;1  simply. these details can be extracted from the proofs below  as well as other  potentially better ways to sample. 
　to compute the natural gradient  we simply invert this matrix and multiply it with the gradient computed using standard policy search methods. 
1 	limiting metric for infinite horizon problems 
a well-known result in statistics  degroot  1  gives a different form for the fisher metric under appropriate regularity conditions. we quickly derive that result  and then show how this gives us a simple form for the path probability metric as 

  1  
　the third line follows from the second by integrating by parts and the fifth follows by observing that the total probability is constant. 
　now we show that in the limit this leads to a simple metric for the infinite horizon case. to get a convergent metric  we must normalize the metric  in particular by the total length of the path denoted t. since the metric is defined only up to scale  this is perfectly justified. 
theorem 1  infinite-horizon metric  for an ergodic markov process the fisher information matrix limits to the expected fisher information of the policy for each state and control under stationary distribution of states and actions. proof: we use to indicate the t-step finite-horizon metric. 
 1  
for a markov process we can write the likelihood ratio 
probabilistic planning 

where is the limiting distri-
bution of states. thus the infinite horizon  undiscounted  and start-state metrics give essentially the same metric  with only the effective weighting by states differing. 
1 metrics for partially observed problems for policies that map the observation space of a partiallyobserved markov decision process into distributions over actions  it is just as easy to derive appropriate metric using our approach. the tuple is also a markov chain  and with only subtle changes to the arguments above we end up with the same metric except using the limiting distribution of 
observations instead of states. 
1 relation to compatible value function actor critic 
　kakade noticed a fascinating connection between the limiting metric given in theorem 1 and the improvement direction computed by a class of actor-critic methods that use a special where the second equality follows by noting that the like- compatible function approximation technique. sutton et al.  
lihood ratio is independent of transition probability matrix- 1; konda and tsitsiklis  1 following kakade we let given the action probability  and by application of the ergodic and let the compatible function aptheorem. the last line follows  with i the fisher informa- proximator be linear in tion for the policy at a given state  as the second term in line 
1 vanishes since the total probability is constant with respect 

to 
　it is also interesting to consider what happens in the startstate  discounted formulation of the problem. in this case we would like our metric  using the general definition over path distributions given above  to naturally weigh the start start more than it necessarily is in the infinite horizon average case. it is well-known that a discount factor is equivalent to an undiscounted problem where each trajectory terminates with probability 1 - at each step. we can use this fact to derive a metric more appropriate for the discounted formalism. 
theorem 1  start-state metric  for a discounted markov process the fisher information matrix equals the fisher in-
formation of the policy for each state and control under the limiting distribution of states and actions. proof: the proof is very similiar to the infinite horizon case and so we simply sketch it: 
　this type of value function approximator was initially suggested as it may be used to compute the true gradient. in practice  it has not been clear what advantage this brings over the gradient estimation routines of  baxter et al  1 . however   folk-wisdom  has it that performing an that infinitesimal policy iteration  moving in the direction of the best policy according to using this approximation has very good properties and often significantly outperforms standard gradient methods. the natural gradient provides insight into this behavior. let minimize the squared value-function error: 

where  is the exact advantage function.  sutton et al  1j it is easy to check that   kakade  1   theorem that is  the direction of maximum policy improvement is exactly the natural gradient direction. this can be seen by simply differentiating to minimize it with respect to and noting the result of  sutton et al  
1  that 

1 demonstration 
as a consequence of the results of section 1 and ikakade  1   experimental results already exist demonstrating the effectiveness of the natural gradient method. that is  all results using policy improvement with compatible function approximators are implicitly computing this result. as a demonstration  we computed analytically the natural gradient for the 


figure 1: performance comparison of covariant  nat and scaled  and non-covariant  grad  on the 1-state mdp. the covariant learning algorithm dramatically outperforms the standard gradient algorithm. the standard method achieves j=1 after more than 1 iterations. 

figure 1: path through policy space  showing log-odds of the actions with state 1 along the horizontal axis  of covariant  nat and scaled  and non-covariant  grad  algorithms on the 1-state mdp. note that  as required  the path is independent of scaling for the covariant algorithm. 
problem given in section 1. this problem is interesting as it reveals some of the properties of the natural gradient method. first it is easily checked that for a complete boltzmann policy in an mdp the natural gradient computes: 

　this means that it is very similiar to the standard gradient except that it removes the weighting of states by the stationary distribution. rather  each state is treated equally. this leads to much more reasonable results in the problem discussed as the partial derivative component for state 1 does not shrink as the policy changes initially. 
　in figure 1 we plot the performance of the natural gradient  using two different scalings  and the standard gradient methods in optimizing the policy for this problem. 
　it is interesting to note that in this graph the behavior of the natural gradient descent algorithm appears to be noncovariant. this is simply due to the step size heuristic not computing equivalent steps in the policy space. the direction however is constant as illustrated in figure 1. 
ing. unfortunately  working in the space of policies  it was difficult to generate such an algorithm. here we proposed considering the induced path-distribution manifold and used notions from information geometry to propose a natural covariant algorithm. this leads to interesting insight and a practical algorithm. fortunately  it agrees with the heuristic suggested by kakade  despite the suggestion in the paper that the algorithm there was actually not covariant  in the infinite horizon case and extends to cover new problems.  peters et ai  1  independently developed theory related to ours  in particular the theorems in section 1  and presented results in the context of robot dynamic control. 
　further work will provide more investigation into the experimental behavior of the algorithms presented. future effort may also yield deeper insight into the relationship between the method presented here and value-function approximations. 
