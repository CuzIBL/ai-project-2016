 
the role of parallel processing in heuristic search is examined by means of an example  cryptarithmetic addition . a problem solver is constructed that combines the metaphors of constraint propagation and hypothesize-and-test. the system is capable of working on many incompatible hypotheses at one time. furthermore  it is capable of allocating different amounts of processing power to running activities and changing these allocations as computation proceeds. it is empirically found that the parallel algorithm is  on the average  more efficient than a corresponding sequential one. implication* of this for problem solving in general are discussed. 
1. introduction 
many ai systems that perform a  heuristic search   i.e. they can be thought of as searching some space of possibilities for an answer  are based upon one or both of two programming techniques known as constraint propagation and hypothesize-and-test. 
in a system based on constraint propagation  internal data structures represent  implicitly or explicitly  potentially acceptable points in the search space. computation proceeds in narrowing down these possibilities by employing knowledge of the domain in the structure of the computation. there is not enough space here to properly introduce the concepts involved in constraint propagation. the reader is referred to some systems described in the literature  1  ii  for an introduction. one point we wish to emphasize about pure constraint propagation is that at any time the internal data structures will be consistent with any solution to the problem. thus  if more than one solution is possible  pure propagation of constraints will be unable to select only one of them. further  even if a unique solution exists  a constraint propagation system may not be able to find it. 
the hypothesize-and-test methodology allows the program to make assumptions that narrow the size of the search space; there is no guarantee that the assumption is consistent with any solution to the problem. the program continues to make hypotheses until a solution is located  or it has been determined that no solution is possible with the current set of assumptions. there is no requirement that any hypothesis be correct and so mechanisms must be available that prevent commitment to any hypothesis until it has been demonstrated to be acceptable. the most commonly available mechanism is known as backtracking. backtracking allows the program to return to an environment that would exist had that assumption not been made. 
as long as the search space is enumerable  a very weak assumption ' hypothesize-and-test can be easily seen to be logically more powerful. if there are several consistent solutions  a pure constraint propagation system has no way to establish preference for one of them. even if only one solution is possible a constraint propagation system will not necessarily find it; this will be demonstrated later by example. the proponents of constraint propagation point out that hypothesize-and-test is grossly inefficient in situations where constraint propagation can function  see for example waltz  . the example in this paper bears out this claim  though one recent study  suggests there are situations in which pure backtracking is more efficient than constraint propagation. 
one can  however  imagine a composite system that has aspects of both constraint propagation and hypothesize-and-test. in such a system  constraint propagation can be used to prune the search space  yet allowing hypothesize-and-test to continue the search where constraint propagation is not able to. a constraint language that can support the creation of such systems has been constructed by steele . steele allows assumptions to be made and backtracking performed. the current work discusses another such system in which the hypothesize-and-test methodology allows more than one assumption to be pursued concurrently. it is an extension of earlier work discussing parallel problem solving systems  1  1  and a language  ether  for implementing these systems. here we examine one particular kind of search problem  cryptarithmetic addition  of the sort used by newell and simon  1} we study this problem  not because it is interesting in itself  but because it is well-defined and test cases are relatively easy to come by. this allows us to test the efficiencies of algorithms empirically. we have constructed a parallel problem solver for doing these cryptarithmetic problems. 
there are two main points we wish to make: 
1. that a system combining both constraint propagation and parallel hypothesize-and-test methodologies can be constructed. the code is simple to read  write  and understand. example code is presented. 
1. that  on the average  a parallel program for solving these puzzles can be constructed that requires less average run time when the parallel program is executed by time-slicing on a single processor than a sequential program executed on the same processor. obviously  it matters which sequential and which parallel program we compare: the benchmarks for this comparison will be explained later and are  i think  quite reasonable. the speedup we are talking about here is not large  but is noticeable. the important point is that it is present at all. a similar effect has been noticed in other suidies for various problems  1  1j. it suggests that concurrency may be a useful for the design of heuristic search algorithms whether or not the programs are executed on concurrent hardware or a conventional sequential computer. 
the remainder of this paper consists of a discussion of the problem being solved and the nature of the parallel solution. we show how the efficiency of the parallel program depends on the use of heuristic information for allocating resources of the parallel program. we then develop a series of allocation strategies  each one improving on the previous one. we finally discuss the importance of this experiment for a general theory of problem solving. we show how the allocation strategies represent a use of what has been celled mcta-lefcl knowledge in the literature  i.e. knowledge about 
1 

how to guide the search process to gain efficiency. in this study  concurrency is necessary to make use of this mete-level knowledge. 
1. the problem 
we ere given three strings of letters  t.g.  donald    okrald   and 
 robkm  that represent integers when substitutions of digits are made for each of the letters. there is at least one possible assignment of digits for letters so that the numbers represented by the first two   donald  and   o r r a h /     when added  yield the number represented by the third   rottt-rt  . any one of these assignments is a solution. in the problems we will be looking at  each will contain exactly ten letters. a solution consists of a mapping from these ten letters onto the ten digits 1 through 1. 
1. a constraint propagation solution 
in our construction of the constraint network wt will use the actor model of computation.. we find it a very natural formalism for building these sorts of systems. in this formalism nodes of the network are implemented as actors. constraint propagation between nodes is implemented by sending of messages containing the new constraints to the node being constrained. for our cryptarithmetic problem solver we have three kinds of nodes: letters  digits  and columns. they are arranged as shown: 
arcs in the diagram indicate flow of constraints. thus column nodes can constrain their left and right neighbor columns and certain letter nodes  the ones representing letters contained within the column . letter nodes can constrain digit nodes and column nodes that contain their respective letters. digit nodes can constrain letter nodes. in the initial configuration  before constraint propagation begins  we store at each letter node a list of possible digits that contains all ten digits. similarly  each digit node contains    possible letters list  containing all ten letters. we will give a short description of what each node has to do when it receives a message informing it of a new constraint. 
column!. a column can receive messages informing it of new constraints on letters it contains and on possible values for its carry-in and carry-out if a column node receives any such messages  it computes possible new constraints on its letters  carry-in  and carry-out. if any one of these has no possible values a contradiction is asserted. when a contradiction is asserted the code implementing hypothesize-and-test is invoked to take an appropriate action. new constraints on letters are sent to the respective letter nodes. new constraints on carry-in and carry-out are sent to the right and left neighbor columns respectively. 
letter.. letters receive messages subsets of the digits 
1 through 1 that they can possibly be. if they learn of digits that they cannot be  nodes representing those digits are sent messages. also  each column that contains the letter receives a new message informing it of the new restrictions on the value of the particular letter. if the set of possible digits becomes null  a contradiction is asserted. 
digits. these receive messages from letter nodes indicating that they are or are not the respective letter. if the set of possible letters is reduced to a singleton  a message is sent to the particular letter. if the set of possible letters is reduced to null  a contradiction is asserted. 
we can observe some things about the ability of this system to satisfactorily derive a unique solution. first  if there is more than one possible solution it will not find any of them. since the letter and digit assignments of each possible solution are certainly possible assignments  they will appear on the possibility lists attached to each node. even if there is only one possible solution  or no possible solutions  the system may not find it  or discover that no solutions exist . for example  the  donald  +  okrald  - and  rodf.rt  puzzle has only one solution: the constraint network described will quiesce before finding it. nevertheless  the knowledge can be said to be  present  in the network: if the nodes of the network are instantiated with an assignment of leters to digits  the network will assert a contradiction iff the assignment is not a solution. our constraint network  then  needs the ability to make assumptions and test them if it is to be able to solve these puzzles. 
1. hypothesize and test in ether 
the constraint network and hypothesize-and-test methodologies were written in the ether language  1  1 . we will only give enough details about the implementation to support the ensuing discussion. the interested reader is referred to  for a more detailed discussion of the implementation. 
the primitive operations of the ether languages are based around the notion of an assertion rather than message passing. rather than coding in a message passing formalism  send the node for the letter d that is 1  we instead say  assert that d is 1  and a process of compilation turns this assertionai code into a message passing implementation. for certain problems this process of compilation is important because certain ideas can be expressed quite naturally in the assertionai form that compile into very complex message passing code. these issues will be discussed in  1  
because we are interested in the possibility of pursuing more than one instantiation of the constraint network in parallel  we need the ability to have more than one available for processing. for this we introduce the notion of a viewpoint. each viewpoint tags a mutually 
compatible collection of assumptions about the possible values of letters and digits together with the constraints that derive via propagation from these assumptions   i.e. a viewpoint is one particular instantiation of the network . viewpoints are related to each other by an inheritance mechanism. the viewpoint in which a is assumed to be 1 and b is assumed to be 1 might be a subviewpoint of the one in which a is assumed 1 and no other assumptions have been made. viewpoints are the repositories of assumptions and facts derived from these assumptions. 
in order to be able to hypothesize and test we need to introduce some control primitives. these primitives are built around a construct known as an activity. all processing that happens during execution happens under the auspices of some activity. there are language constructs for conveniently grouping parts of a related task into a single activity. for example  we can create an activity  make a new assumption in a viewpoint  and cause all further work within the viewpoint  i.e. all further constraint passing in the instance of the 
1 
network defined by the sumption  to be part of the activity. 
activities are of interest because they give us ways to control quantities of system resources available for the execution of alternative explorations. if we itiflt an activity  all execution with the activity stops; a stifled activity cannot be restarted. we also have the ability to control the rates that non-stifled activities run. different activities can be assigned different amounts of processing power  the total amount of cpu time an activity will get during an interval of time is proportional to its processing power. the processing power of tn activity can be altered by the system asynchronously with the running of the activity. 
systems using hypothesize-and-test can be constructed in ether by using viewpoints to represent assumptions made  and octititics to control which parts of the search space are explored  and with what vigor. 
1. a simple parallel solution 
in this treatment we will ignore many details of how both the ether system and the crypt arithmetic system implemented within it are constructed. if we wish to  create a new instance of the constraint network  that inherits from another  we create a new viewpoint  using the new-viewpoint construct . to add an assertion about a letter being associated with a digit within the context of this viewpoint  we execute  assert  one-of -letter  -digit    where letttr and digit are bound to the respective letter and digit which we want to assume are identified in this viewpoint. the second argument to one-or is a list of possible digits that the letter can be. so  for example  we could execute  essert  one-of s  l j s ; 1    to indicate that s is odd. ether syntax makes use of a quasi-quote convention in which symbols prefaced by the character v are substituted with the values of the associated symbols. if letter were bound to  d  and digit were bound to t  the item actually asserted would be  one-of d  s  . if the titert is executed within the context of a certain activity  then all. work propagating constraints that follow from that assertion will happen within that activity. 
the implementation described in this section is quite simple. it first creates a viewpoint in which no assumptions are made and continues propagating constraints within this viewpoint until it has quicsccd  i.e. no more propagation can happen. when this state has been achieved  if each letter does not have a unique digit that it can be identified with  it is determined which letter has the least number of possible digits that it can be  excluding those letters that already have a unique assignment . for each one of these digits  a new viewpoint and a new activity are created. within these  in parallel   the letter is asserted  assumed  to be the digit and propagation of constraints continues. if quiescence is reached in this new activity and the problem has not been solved  we recurse. 
the function shown below takes a letter  a list of alternative digits  and a viewpoint. it uses the environment contained in the viewpoint to create new subviewpoints in which the letter is assumed to be each of the alternative digits. we first check to see if there is at least one possible digit. if not  there cannot be a possible solution to this problem consistent with the parent viewpoint and so we assert that there is a contradiction within the parent viewpoint. otherwise we iterate over each digit in the alternatives list and for each one we create a new viewpoint whose parent is the parent viewpoint and a new activity with parent start-act and assert the letter is the particular digit; this initiates propagation of constraints. if we discover there is a contradiction within the viewpoint  this is accomplished by the code  fragment beginning with 
  when   contradiction !   we assert within the parent viewpoint  that the letter cannot be the particular digit. we are justified in doing this because the only difference  in terms of assumptions made  between the current viewpoint and the parent viewpoint is the one assumption of the totter being identified with   particular digit that was a possible alternative in the parent viewpoint: if this assumption leads to a contradiction  we know that this is not a possible identification for the letter. in addition we stifle  stop from executing  the activity that was pursuing the now known to be invalid assumption. we further check to see if the activity quiesces in the section of code beginning with   when   euitictnt *    . if this has occurred  we first check to see if the problem has been solved. if so we are done: otherwise we determine the letter in the viewpoint with the least number of possible digits  but greater than i  and recursively call parallel-seiva on this. 

when a new activity has been created  and has something to do  it immediately begins executing concurrently with already existing activities. the default allocation of processing power  when no explicit allocation has been done  is such that each running activity gets approximately equal servicing  in terms of cpu seconds  by the scheduler. 
1. alternative parallel program 
the simple parallel program described might well be reasonable if we had a large number of processors. with a small number of processors  in particular  only one processor  the case actually studied  it is considerably less efficient in terms of average total run time than some other solutions. all the solutions we wilt examine are elaborations of  or simple modifications to the basic parallel program already presented. 
we observe that a traditional depth-first search  with backtracking  is but a trivial modification of the code above. when new alternative digits are proposed for a letter  instead of starting them up in concurrent viewpoints as was done above  they are placed on a list. only the activity for the first one on the list is given any processing power. if it quiesces we recursively call parallel-solve. if it is discovered that the viewpoint is contradictory  the next one is begun  if a next one exists ; otherwise  the parent viewpoint is asserted to be inconsistent. asserting that it u inconsistent will trigger the 
1 
activity monitoring the next higher viewpoint to pick the next possibility on its list. depth-first b a degenerate case of parallel anarch in which only one activity at a time b given non-xero processing power. 
1 	heurlftlc information to control itaoarce allocation 
a simple elaboration wt can make to the parallel implementation presented that preserves to parallel character is to vary the processing power based on an assessment of how likely the assumptions we have made within its associated viewpoint are to lead to useful information  either leading to a solution or determining that the viewpoint is contradictory . we base the quantity of processing power allocated to the activity doing the exploration on the numerical value of this judgement. for this particular problem  we are more likely to learn in a short period of time whether a viewpoint contains a valid solution or is contradictory if it is already fairly well constrained  le. if the letters in the viewpoint only have a few possible digits that they could be. after some experimentation we came upon the following formula for determining relative processing power allocations for the various different activities participating in the search: 

where each ni is the number of possible digit assignments for the letter i in the viewpoint if the letters tend to have fewer possible digit possibilities  the sum terms   l o   n j   will tend to be large. squaring this number  and squaring the final sum serves to accentuate the relative differences between the different viewpoints. when the system is first set up  a separate activity known as the manager activity continually monitors each of the other running activities and evaluates this function for each associated viewpoint. the processing power allocations to these activities are adjusted in proportion to the numerical value of this formula. the ether command we use for modifying the processing power allocations of an activity is called support-in-rat tot. it takes three arguments: an activity  a list of activities  that are children of the first  and a list of non-negative numbers with the same number of elements as the list of activities. the processing power assigned to the parent activity is  re divided among the children activities in proportion to the numbers in this list thus  if a factor for a given activity is 1 the activity gets no processing power; if the factor associated with the activity is twice the factor associated with another  then the former activity gets twice as much processing power as the latter. the allocator described is implemented as follows: 
we create a separate activity at top-level called the nenaaer-activity and execute the following to have the allocation strategy continually called asynchronously with the activities doing the actual search: 

this scheme gives considerably better performance than the simple parallel solution. it does better than the backtracking solution on some examples with a single processor implementation  although on the average the back trie king solution is more efficient. it is important to understand the source of this improvement. we have a scheme for estimating the likelihood that a running activity will return useful information in a short period of time. we allocate more resources to those activities that we estimate will supply us with information for the least amount of resource expenditure. assuming our heuristic is reasonable  the average time to complete the search is reduced. 
there are three more improvements we have made to the processing power allocation strategy before reaching the final strategy for which we have collected data in the next section. each will be described in turn. 
	1 	concurrency factors 
we have observed in the allocation strategy discussed thus far that even though activities are running with different amounts of processing power that wt related to our estimate of the utility of getting useful information back from them  there still seems to be so many activities running that they tend to thrash against one another. we would like to limit the amount of concurrency so that the running activities can get something done. for this purpose we introduce the notion of a concurrency factor. instead of letting all runnable activities run  we pick the n most promising activities  using the metric above   where n is the concurrency factor  and give only those activities processing power and in the ratios defined by the metric. the optimal value for the concurrency factor is picked experimentally and is discussed below. 
the value of the concurrency factor that yields the best result is a reflection of two aspects of the problem: the quality of our heuristic knowledge and the distribution of computational expense for picking bad branches in the search. obviously if our heuristic knowledge were perfect  i.e. it could always point to the correct branch to explore next  the optimal concurrency factor would be i it should simply explore this best branch. if we are less sure we are about which is the best  more branches should be explored. also  if the computational cost of exploring a bad branch is always small  a small concurrency factor would be appropriate. if  however  the cost of a bad branch can bo vary largo wo would want to use a larger concurrency factor. with a small concurrency factor we increase the probability that the problem solver will become stuck for a very long time. a limiting case of this is with a search space that is infinite  introducing the possibility of a bad branch that never runs out of possibilities  and a concurrency factor of i. if the problem solver happens to pick one of these branches it will diverge. 
hayes-roth has noted an analogy with portfolio theory  the purpose of which is to pick an investment strategy that will yield the greatest expected capital appreciation. uncertainty about the future performance of certain industries and volatility in the market place argue for greater diversification of the portfolio. 
1 
1 eettmatiiig which assumptions are moat valuable 
our strategy so far has been to use hypothesize-andtest on one tetter only in each viewpoint. we sprout one new viewpoint and activity to test the hypothesis that that letter is each one of the digits it could possibly be in the parent viewpoint this is not necessarily the best strategy. by hypothesizing a letter is a certain digit we may learn a lot or a little. we have  learned a lot  if we  i  discover quickly that a viewpoint is contradictory  or  1  cause a lot of constraint propagation activity that significantly increases our evaluation of the new viewpoint. one thing we have observed is that the amount we learn from assuming a letter is a particular digit does not significantly depend on which digit we use. in other words  if we assume the letter n is 1 and discover a contradiction  then we are likely to either discover a contradiction or signficantly constrain our solution by assuming n is any other digit on its list of alternatives. to take advantage of this phenomenon the program remembers what happened when it makes particular assumptions. when it creates a new viewpoint to study the result of assuming a letter is a particular digit the result is recorded in the parent viewpoint when it has completed. there are two possible results. if it led to a 
contradiction this fact is recorded. if it led to a quiescent  but consistent  state it records the difference of the evaluation metric applied to the parent viewpoint and the evaluation metric on the quiescent viewpoint - our estimate of the amount of reduction that is likely to be obtained by assuming this letter to be a digit our new evaluation metric attempts to take this information into consideration. when assuming a letter l is a specific digit we use the old evaluation metric if we do not have have never assumed l to be a particular digit from this viewpoint: otherwise  we use the average of the evaluations for each of the resultant viewpoint*. we then multiply this figure by the factor i 1 .1   n where n is the number of letters that we have assumed l to be and determined that they lead to contradictions. 
now that we have a mechanism for taking advantage of information learned by making different assumptions we would like to ensure that a variety of choices are tried at each branching point we will slightly modify the technique for picking the activities to be run at any given time  in accordance with the concurrency factor . where c is the concurrency factor  we. use the following algorithm to pick the c activities to run at a given time: 
1. the activity with the highest evaluation is scheduled. 
1. if n   c activities have been selected for running  the n+lst activity is  a  the one with the highest metric if it does not duplicate any of the first n activities in terms of which letter it is making an assumption about for a given viewpoint  or  b  the highest rated non-duplicated activity unless the highest rated activity has a rating at least three times higher in which case we use the highest rated activity. the factor three was picked experimentally and is based on the following argument. there is a certain advantage in having a diversity of letters being tested because this gives us a greater chance to discover assumptions that will cause significant shrinkage by constraint propagation. however  there is also an advantage to running the activity that we have estimated will give us the best result. the factor three is the ratio of estimates for expected gain for which we would rather run the higher estimated test than one that will increase our diversity. 
1. an experiment 
in order to test for the existence of a speed-up with concurrency we timed 1 problems using the final parallel algorithm described above for several concurrency factors. the problems tested are: 

they were picked by   trial-end-error process of selecting possible problems and then running them to we if they have a solution. it is not known whether they have one or more than one solution. the program finishes when it has found one solution. these tests were run on the mit lisp machine  a single user machine designed for efficient execution of lisp programs. the times represent processor run time only and are adjusted for time lost due to paging. the manager activity  which continually monitors the state of the search activities and readjusts processing power accordingly  receives a processing power allocation of .1. we tested with concurrency factors between i and 1. numbers 1 through 1 each gave some improvement with 1 being the best. here we report the results for 
concurrency factors i and 1. times reported are in seconds: 

with a concurrency factor of i the algorithm becomes  functionally  a depth-first search. a concurrency factor of 1 represents the value which yields least average run time for the problems examined. concurrency factors larger and smaller yield higher average values. we caution the reader not to take the numbers too seriously. we only wish to demonstrate that the parallel algorithm runs with some improvement of efficiency ovtt the sequential algorithm. 
some interesting facts can be learned by examining the data. 
although the parallel solution beat out the sequential solution in only 1 of the 1 cases  these six cases are the ones for which the sequential solutions take the longest. in particular  problems 1 and 1 have show by far the longest times for the sequential solution and the time saving of the parallel solution is considerable. similarly  for the cases in which the sequential solution finished quickly  the parallel solution tended to take longer. this phenomenon is fairly easy to explain. the parallel solution supplies  insurance  against picking bad branches in the search space. if the sequential solution happened to pick a bad branch  or several bad branches  there was no recourse but to follow it through. similarly  if the sequential program found a relatively quick path to the solution  the extra efficiency of the parallel solution was not needed. 
b  conclusions 
we have demonstrated that cryptarithmetk puzzles can be solved with a certain increase in average efficiency by the parallel algorithm described over a more traditional depth-first search solution. while this result in and of itself is of little use it does demonstrate a tool that may be of great use in heuristic programming - the use of parallelism to control a heuristic search. several writers have pointed to the use of mcto-letd knowledge  1. 
1 
davis  1j  in controlling a march  mete-level knowledge is knowledge about how to use the problem solving loob at hand in a way that increases overall search efficiency. the allocation strategies we have examined are mete-level knowledge for cryptarithmetic problems. by allowing a few to run in parallel  and with controllable amounts of processing power we art able to increase the efficiency of the search. although the increase we gained is not dramatic there is reason to suspect that it would be more significant in more interesting problems. the silt of the search space in these problems is relatively quite small. thus picking a  bad branch  in the search can't be too catastrophic. with a search space that is much larger  and possibly infinite  as is the case with many interesting problems   a bad branch using a parallel search can only do. a bounded amount of harm  bounded by the quantity of processing power allocated to it. very similar results have been obtained in speech understanding research projects  1  in which competing hypotheses are used to control the allocation of resources for further investigation. 
we introduced several concepts that were used in the construction of the allocation strategy. processing power b allocated in proportion to an. estimate of how likely we are to get useful information out of the exploration of a branch. concurrency factors have been introduced to keep the problem solver reasonably focused. a certain amount of diversity b incorporated in the algorithm to increased the likelihood of discovering assumptions that can be made that will lead to valuable information quickly. although the only problem we have examined b cryptarithmetic  there is nothing about these general strategies that b specific to cryptarithmetic. they contribute to a general theory of parallel problem solving. 
the form of the code is quite simple to write and understand. the algorithm consists of a mixture of constraint propagation and parallel hypothesize-and-test. 	the 	programs 	involve 	asynchronous  concurrent activities processing different sets of assumptions. furthermore  the resources allocated to these activities can be altered asynchronously with the execution of the activities. 
we have demonstrated that introducing concurrency in the search process does actually increase overall efficiency  in particular it does no harm. this lends support to efforts to design a computing system for message passing languages that involves many intercommunicating autonomous processors  e.g. hewitt  1  . it suggests there is inherent concurrency in search problems that could be gainfully run on multiple processors. we are interested in generalizing the control notions we have developed  such as processing power and quiescence  to be implementable on truly parallel architectures. 
1. acknowledgements 
beppe atfardi. roger duffey  carl hewitt kurt konotige  david 
levitt  reid smith  and barbara white were kind enough to read earlier drafts of this work and have substantially aided the presentation. 
i offer my sincere thanks to the lisp machine development group at mit. without the superb computing environment available on the lisp machine the program development necessary to carry out this research would have been impossible. 
1. reference* 
 i  bowing  alan  thinghb -- a cousteoiut-orkutod smuktion laboratory. xerox parc report ssl-1  july 1. 
1 
