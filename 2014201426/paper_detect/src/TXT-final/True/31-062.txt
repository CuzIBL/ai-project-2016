 
this paper reports on an implementation of kanerva's sparse distributed memory for the 
connection machine. in order to accomplish a modular and adaptive software library we applied a plain object-oriented programming style to the common lisp extension *ltsp. some variations of the original model  the selected coordinate design  the hyperplane design  and a new general design  as well as the folded sdm due to kanerva are realized. it has been necessary to elaborate a uniform presentation of the theoretical foundations the different designs are based on. we demonstrate the simulator's functionality with some simple applications. runtime comparisons are given. we encourage the use of our simulation tool when outlining research topics of special interest to sdm. 
1 	introduction 
uncertainty  vagueness and associativity have been a challenge for symbolic knowledge processing in ai. apart  from efforts to model these properties inside symbolbased ai  the development of hybrid systems by integrating ai methods with connectionist memory models can be of certain benefit for e.g. inference or natural language processing. the advantages of both approaches should be combined: symbolic ai makes use of explicit methods and knowledge and thus leads to a certain amount of comprehensibility  whereas neural networks are capable of modeling knowledge which is difficult to express explicitly. in addition they offer an easy way to apply case-inherent statistics. the connectionist paradigm  in particular  could gain new significance by this confluence. 
　one of the most clear and sound connectionist models is the sparse distributed memory  as introduced by pentti kanerva  1; 1 . sdm is rooted in a mathematical idea complying with a convenient deduction of quantitative properties. it has also been described as a general random access memory  cf.  kanerva  1   ch. 1   such that an easy hardware realization could be 
* previously at univ. of erlangen-niirnberg. 
expected. on this basis a further description of sdm could be developed  using formal neurons. 
　from results of physiologically motivated brain research it has been argued that sdm models the physical structure of parts of the cerebellar cortex of mammals  as well as its functionality  the control of learned motor activity. sdm was designed to resemble some specific qualities of human cognition: dynamic recollection  distinct degrees of uncertainty in concept recognition  associativity in recalling memory contents  and the faculty for building abstractions. 
　sdm's technical properties  e.g. capacity  recognition rate  and convergence  have been investigated well. now even the hardware restrictions  which for some time made the realization of sdm simulators of appropriate time efficiency impossible  have disappeared 
　we introduce a tool for experiments with sdm  running on a connection machine 1  hillis  1 . compared to other simulators it has some specific advantages: 
  there are in principle no hardware restrictions on the size of simulated sdms  because data can be swapped between main and background memory of the cm 1. 
  data types are chosen in order to optimize execution time and reduce memory waste  sec. 1  1 . 
  the software is engineered: it is modular  adaptive and extensible. 
  modules for the most important architectures of sdm-like memory models are predefined. 
　flexible investigation tools are required to compare the behavior of differently designed sdms. we found some attempts to develop simulators for sdm of which the closest one to our approach is that of rogers . the main reason for providing an implementation of our own has been that all of them are neither capable to simulate sdms of an appropriate size nor in a tolerable time  matz  1 . 
1 	the memory model 
the sdm memory model is based on a small subset of the space {1  l}n. elements of this subset  the so-called hard locations  can be regarded as registers. the sdm is built up from three major parts concerning storage  the addressing mechanism and control  fig. 1 : the address 
	turk and gorz 	1 

module  containing address matrix and hamming register  the contents module  selection register  contents matrix and summation register  and external registers for address  input and output. all of them employ binary counters  except the contents matrix which is built from integer counters. counters can be considered as  weighted  connections in a neural-network-like representation of sdm. sdm's functionality is given by three operations: the addressing of hard locations  and the writing and reading of data. addressing serves to mark a subset of registers  rows  of the contents matrix as active. the input register is added to the active memory locations by writing     s cause the particular counters lected coordinates  when compared to the address register. this is to simulate a sparse matrix. the intermediate design also consists of sets of selected coordinates  but addressing is done as in basic sdm. thus kanerva's sdm and the selected coordinates sdm both can be specified from the more general intermediate design. 
　another class of sdm is introduced by the hyperplane design. addresses and data are assumed to contain  is  with a probability of 1. this small proportion of active connections is physiologically motivated. as before  a set of selected coordinates exists for each register  but now this set is meant to enumerate exactly all activated connections   is  in the address-register . thus all selected coordinates of activated locations must reference a  1  in the corresponding coordinate of the address register instead of matching  1 s and  l s. this makes an 
explicit address matrix redundant. 	* 
　the hyperplane design can be regarded as a counterpart to the selected coordinates design. a corresponding variant of the basic sdm is easily defined: the adapted kanerva design expects a probability of 1 for neural activity in address matrix and data. no further changes are made. finally the intermediate hyperplane design functions as a counterpart to the intermediate design comprising hyperplane and adapted kanerva. 
　to reduce complexity we have defined the generalized design  comprising all of the sdm variants mentioned above. for this purpose it was necessary to formalize the different models in a uniform manner  so that all of them can be derived by an appropriate choice of parameters  turk  1 : 
　addressing operations serve to activate a precisely predictable number of registers in a reproducible way. close addresses must effect a larger intersection of active registers than distant ones. though jaeckel  1a; 1b  based the measure on the logical and  we will here use measures1 based on the logical nand  the zero distance 

to increase   1 s to decrease by one . the reading operation stores the column-wise sums of active registers in the summation register which in turn is matched against an external threshold. resulting values are stored in the output register   1  if greater than or equal to   1  if less than the threshold . 
　with selected coordinate design  hyperplane design and several intermediate designs certain modifications of the original sdm have been accomplished  jaeckel  
1a; 1b . in principle these variants differ from kanerva's basic model only in the methods of selecting sets of active registers. in kanerva's sdm the address matrix is a uniform randomly initialized  constant binary matrix of dimensions m x n. during addressing for each row of the address matrix the hamming distance  defined below  to the address register has to be calculated. locations that settle within a given radius are selected. 
　choosing coordinates in {1}  corresponds to the selection of columns from the address matrix. the selected coordinates sdm provides  for each row of the address 
matrix  a different set of constant size containing randomly selected coordinates. activation is effected for those locations that have matching values in their se-
1 	connections models 


	turk and gorz 	1 


1 	connections models 

of folded sdms into account this seems to be a more intuitive partitioning. fig. 1 can now be viewed as one  of many  inner sdm combined with the external registers  dashed box . the particular sdm partitions are independent with one exception: reading out an overall sum requires one extra step to add up the inner summation registers. 
　in conjunction with the capability of swapping  memory partitioning allows for sdms of arbitrary size. the package big contains functions necessary to handle sets of inner sdms. global load-operate-store cycles are built  managing step-by-step access to an array of single sdm objects. thus it is possible to simulate sdms of a size kanerva  stated as necessary to achieve certain theoretical properties  orthogonality of memory contents  convergence of sequential reading  memory capacity . the collection of different sdm types into a large hybrid sdm is also provided. this permits a sdm which employs different addressing methods at the same time  collecting sets1 of similarly addressed registers  only if these sets are of a relevant size. experiments in the combination of the advantages of different sdm models still remain to be carried out. 
　one further package  fold  provides the simulation of folded sdms  sec. 1  using similar means of combination as above. 
　the usefulness of a sdm simulation depends very much on a correct value for the actual activation radius. even small deviations from the optimal value may result in pathological states with all or none of the registers activated. the package prob contains a collection of functions for an approximative calculation of the cumulative binomial distribution of large numbers. they should be applied to calculate appropriate radii in advance. 
　a detailed description of functions as well as application examples can be found in  turk  1 . 
　1 simulation examples 
the intended system behavior can be demonstrated by two simple examples. a few intuitive patterns have been predefined such that similarity can be seen immediately. we have made no attempt to meet statistical properties  e.g. parity of  l s and  1 s  or to maximize orthogonality. the icons are called caterpillar  pine and the roman numbers one to six. 
the first experiment employs an sdm of type n = 
1 dimensions  m = 1 hard locations  k - 1 selected coordinates  an activation radius of h - 1 and a probability of pone = 1 for the appearance of  l s in address matrix and data. the metric in use is defined by the zero distance. we added 1% of random noise to ten instances of the caterpillar icon. the resulting patterns are stored auto-associatively into the sdm: 

　1 there is no order in the implementation of hard locations  since they are randomly selected in all sdm variants mentioned above. 

what we obtained can be defined as the generation of an abstraction  since the original pattern caterpillar was in fact unknown. 
　the second experiment demonstrates the discrimination of sequences in a 1-folded sdm. the characteristics are n = 1 dimensions  m = 1 hard locations per fold  k - 1 selected coordinates  h - 1 as the activation radius and a probability for a single 
 1  of 
　　　pone = 1 in address matrix and data. we define the hamming distance as the metric in use and hence are dealing with a sdm of type k. the heteroassociative loading of both sequences  caterpillar  one  two  three  four  and  pine  one  two  three  five  to an unfolded sdm results in inseparable memory contents. reading four times gradually at the address caterpillar leads to nearly the same pattern as it does for the address pine: a combination of four and five. the 1-folded sdm we employ here  is capable of discriminating the above sequences correctly. this can be seen from the results obtained: 

 turk  1  provides complete traces. 
　the cm 1 at our disposal was equipped with 1 k onebit processors  each with 1 k bit of local storage. tab. 1 shows timing measurements of different sdm sizes wrt. allocation  addressing  write- and read-access. in the first row timing values for an sdm running on the *lisp simulator  a cm 1 simulator for sparc stations  can be found. our experiences show that the *lisp simulator is inadequate even for development purposes. sorted by sdm magnitude  compiled   comp   and non-compiled code is compared. it can be seen that compilation results in shorter execution times  up to a factor of five . execution times of the front-end which runs the common lisp process  decrease even more  not contained . meticulous declarations are necessary to provide the *ltsp compiler with sufficient information.1 
　rogers  tested his simulator with a k'-type sdm of size n = 1 and m = 1. we obtained slightly 
   1 we would have liked to present a comparison to the cm 1  but unfortunately we could not manage to get access to a cm 1 running *lisp. 
	turk and gorz 	1 

better results than the reported three write-read-cycles per second. 
1 	related work 
palm  described another associative memory model  the binary associative memory  bam . this model was designed to enable easy capacity estimations  but it also forms the basis of the pan iv simulator  cf. 
 palm  1  .1 bam can be compared directly with sdm  because its architecture is similar but simpler. bams consist of an initially empty binary contents matrix the counters of which can be only activated. no address decoding is performed  the address register indicates activation directly. thus the number of hard locations is exactly the same as the dimension of the address space. bam is an instance of sdm through definite activation of a subset of registers in combination with a slightly modified write operation. 
　instead of the distributed representations in sdm  bam stores exactly one representation for each association. as a result bam has a higher storage capacity  which has been proved to be optimal  at the price of less robustness: permanent noise on input leads to an almost completely filled contents matrix during training phases. sdm instead manages to generate an abstraction  as demonstrated in sec. 1. bam requires sparse coding even in normal use  whereas sdm input may or may not be coded sparsely  kanerva vs. hyperplane designs . the performance of sdm subsumes bam in the fields of e.g. pattern completion  pattern recognition  and the storage of sequences  but sdm possesses additional capabilities in building abstractions. the application of sdm to concrete programming problems is consequently more adequate than the use of bam. it is a matter for further research to compare storage capacities under sparse coding for sdm. 
　in this context the encoding problem should be mentioned: the convenience of metrics like the hamming distance does not come for free. applications of connectionist memory models often involve an appropriate encoding of world knowledge. an isomorphic transition of relations of the conceptual domain into internal distances has to be found. the problem of variable binding that occurs  when neural processes are used to model log-
   pan iv is used in the wina project - wissensverarbeitung in neuronaler architektur - at the univ. of ulm. 
1 	connectionist models 
ical inference  is an example as pointed out by dorffner . simple lexical coding has turned out to be insufficient. one can expect harder problems from the essential transfer of functional dependencies from world level to representational level  than from simple storage. this is caused by apparent incompatibilities between the necessity of unambiguous representations in certain cases and the desire for involving vagueness. the role of knowledge encoding will become a major research topic in the future. we suggest reducing similarity measurements to contexts to gain an extended comparability of data. contexts are defined by subspaces in form of non-randomly selected coordinates. the effect is similiar to the use of different measures. 
　encoding may also serve as an alternative way to handle overlapping sequences. folding requires a set of separated sdms  whereas an architecture-independent representation of sequences of arbitrary length is desirable. encoding of system history as in  jordan  1  combines the adequacy of one single unfolded sdm with a more plausible handling of overlapping sequences: long overlaps are harder to discriminate than short ones. experiments on encoding are currently in preparation. 
1 future work on the implementation of symbolic structures with sdm 
one of the major challenges facing connectionist approaches in typical computer science and artificial intelligence applications is the representation and processing of data structures  e. g. of  dynamic  sequences or sets. fodor and pylyshyn  have argued that connectionist representations lack combinatorial syntactic and semantic structure. as kanerva    ch. 1  pointed out  sdm is suited to store and retrieve sequences which in turn provide a basis for the implementation of complex data structures - as is common practice in lisp programming. therefore the next research step will be the specification and implementation of sequences  sets and temporal relations on top of the basic sdm storage model. with these means it will be possible to realize devices like finite automata. a particular challenge will be to provide the basic structures and operations for constraint-based natural language processing which has been a traditional domain for symbolic representations. whereas most of those do not provide appropriate facilities to deal with vagueness and under-specificatiori  these issues are a particular strength 

of sdm. slack showed in a series of papers  1; 1  how to represent the basic structures and operations of a chart parser and of constraint-based grammar formalisms in a distributed memory model which closely resembles sdm. he even argues that  in doing so  it is not only possible to explain certain linguistic phenomena like unbounded dependency  but also that the use of distributed representations based on connectionist principles might influence theories developed at the level of symbolic representation. the investigation of the possibility of implementing a chart parser for context free grammars or even an augmentation with a constraintbased grammar formalism within sdm will not only provide valuable insights into bridging the gap between subsymbolic and symbolic representations  but also demonstrate a close integration of both approaches. 
acknowledgments 
we are grateful to the gesellschaft fur mathemattk und 
datenverarbeitung  gmd   sankt augustin  germany  which provided access to the connection machine 1.  palm  1  g. palm. on associative memory. biol. cybern.  1-1  1. 
 palm  1  g. palm. the pan system and the win a project. univ. of ulm  internal memo  1. 
 rogers  1  david rogers. kanerva's sparse distributed memory: an associative memory algorithm well-suited to the connection machine. international journal of high speed computing  l 1 :1  1. 
 slack  1  j.m. slack. distributed memory: a basis for chart parsing. in proceedings of col1ng-1  pages 1  bonn  1. 
 slack  1  j.m. slack. unbounded dependency: tying strings to rings. in proceedings of coling-1  pages 1  helsinki  1. 
 turk  1  andreas turk. parallele implemetation eines verteilten assoziativen speichers. immd 1  univ. of erlangen-niirnberg  june 1. 

