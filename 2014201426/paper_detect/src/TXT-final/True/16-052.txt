 
　　this paper describes the work on using vision verification within an object level language for robot assenbly  rapt . the framework which handles vision data is discussed in detail. the framework enables us to combine a verification vision facility with an object level language in an intelligent way. it can also handle other kinds of sensory data. 
1. introduction 
　　there have been up to now two major approaches available for programming robots . one is teach-mode programming: this is well known as programming by showing. the other is off-line programming and is referred to as programming by using some formal language. since the off-line programming method has many advantages over the teach-mode one  there is a trend for robot users to program industrial robots by using robot languages. 
　　it is also believed that if sensory information is used in conjunction with the robot  then the robot can do a better job. in this paper we will discuss the work of combining a special kind of vision  verification vision  with a particular robot language - rapt  1 1 . however  the author believes that the principle of the work can 
also be used to deal with some other kinds of sensory data. 
1. the current rapt language and 	its new vision commands 
1. the current rapt language 
　　rapt is a model-based object level robot command language  1 . object level languages allow the human user to describe the task that he wants the robot to perform by describing the objects that are to be handled and the things he wants done with them in terms that are natural to him rather than to the robot. this information has to be converted by some computational system into run time commands that the robot can obey. in rapt  the environment of the robot is modelled by an incomplete geometrical modelling system. each ob-
* this research i supported by a chinese government studentship it is also supported by the university of edinburgh. 
ject in the environment is represented by those of its features which are to be used in the associated rapt program. these features  such as planes  edges  points  etc.  may be finite or infinite. in rapt programs  the objects which are to be manipulated by the robot are explicitly represented  and the programmer specifies intermediate states and actions on objects. these are transformed by the rapt system into movements of the robot which will bring about the desired goal state. 
　　rapt is designed mainly for doing automatic assembly and allows the user to specify a set of bodies  the spatial relationships that are to hold between their features in each goal state  and their movements between one situation and the next. spatial relationships constrain the relative positions of bodies. if multiple spatial relationships hold between two bodies then new  more restrictive relationships may be deduced to hold between the bodies. this ability is a result of spatial relationship reasoning. 
　　the output of normal rapt is a series of positions of objects in each situation. these are only planned positions in as much as they have been determined taking no account of the inaccuracies inherent in the real world. the purpose of the current work is to allow statements to be ad-
ded to the rapt system which will allow use to be made of vision data to modify the planned positions . 
　　the work of the rapt system is divided into two parts: compile time reasoning and run time execution. at compile time  the system performs off-line reasoning about the positions of the objects and the actions of the robot. the results of the compile time reasoning will guide the actions of the robot at run time. 
　　readers who are not familiar with rapt are recommended to refer to  1 1  in order to understand the paper better. 
1. verification vision and vision task specification in rapt 
　　a wide assortment of devices and systems exist for robot sensing of the environment. they fall into two generic classes: contact and non-contact . vision is one of the most important noncontact sensing methods. verification vision  as a type of visual information processing  is quite 

different from other types of vision system  and is very useful in conjunction with robots. the 
main characteristics that distinguish it from others are  1  : 
 1  the system has a great deal of prior knowledge about the type  placement and appearance of the objects that form the scene  
 1  the goal is to verify and refine the location of one or more objects in the scene rather than to recognize them. 
　　before the verification system can be used  it must be told exactly what to look for and approximately where to look. the error between the expected   nominal   positions and the actual ones should not be too big. 
　　since rapt reasons about spatial relationships between body features  we may ask the vision system to verify the positions of some special features  e.g. edges  and then send the vision 
data to the rapt reasoning system so that the 	actual 	position of 	the body to be verified can be deduced. 	since images of edges are easy to detect we prefer to use edges in the system. 
　　to introduce verification vision facilities into the rapt system  a number of vision commands must be added to the rapt language. they are the look statement  the inviolate statement  the tolerance statement and the combine statement. 
some 	auxiliary 	commands are also needed in order to specify cameras and some low level 	vision 	details. 
　　the look statement. is used to specify the features to be verified by the vision system. its 
main effect is to form symbolic features and relationships in the rapt reasoning network. it also sends the necessary information to the low level vision system to enable it to find the expected edge image at run time. 
	when a camera has been used to find the 	image 
of an object feature in the scene  the rapt system creates a new virtual feature for the camera and establishes a spatial relationship between this new feature and the observed feature of the object. for example  it may establish an  against  between a newly created plane feature of the camera and an edge of the object. since the relevant vision data is not available at compile time  the new feature of the camera  and therefore the new spatial relationship  will have symbolic forms during the compilation phase. 
　　the inviolate statement specifies a constraint on the position of the object to be verified in terms of a relationship that must hold between the object and the world. for example  inviolate can be used to indicate that the bottom of the object must be against the top of the table no matter what the inaccuracies of the placing of the object are. from the view point of geometric reasoning  it provides a reliable relationship in the relationship network. this will enable the geometric reasoning system to explain vision data in a correct way. 
b. yin 1 
　　the tolerance statement specifies the maximum translational error along all the three axes of the body coordinate system. the rotation error tolerance is not discussed in this paper. the restriction of the translation error should indicate the range in which the feature is likely to be found. 
　　the combine statement provides a package for the vision task. it invokes the symbolic reasoning facility to deduce the symbolic position of the object by use of all the information included in the package. it checks whether the statements in the package are compatible or not. it also combines the information given by the inviolate and tolerance statements in order to deduce real restrictions on the position of the object to be 
verified. 
　　the format of the package of the vision commands is like this: 
combine; 
violate/ against  bottom of bodyl  top of table; 
look/ edgel of bodyl   cameral; 
look/ edge1 of bodyl  camera1; 
tercom; 
where tercom terminates the combine package. detailed discussion of these vision statements can be found in f 1 . 
1. the symbolic reasoning facility 
　　the symbolic reasoning facility is required in the rapt system for dealing with vision data at compile time. during compile time  the positions of features which are created by look statements have only a symbolic form and cannot be evaluated until run time  when the vision data is acquired. in this case  the rapt inference system must deal with the symbolic form of the feature positions rather than their actual values. the result of the process will be evaluated during run time when the real feature positions have taken the places of the symbolic ones. therefore  a symbolic reasoning facility is essential for combining sensory data with the rapt system. 
　　the symbolic reasoning system works in a similar way to the current rapt cycle finder . being given two relationship chains between two objects  it will produce a constrained new relationship between the objects by means of a set of rewrite rules. the difference between the symbolic reasoning system and the cycle finder is that the features in both the input and output of the symbolic reasoning system may be symbolic and their parameters may be symbolic expressions. detailed discussion of the symbolic reasoning facility can be found in . 
1. the framework for handling vision information 
　　so far we have discussed how to specify a vision task and how to reason about vision information symbolically. now we will discuss how to provide a framework to handle the symbolic posi-

tion expressions caused by 	introducing 	verification vision. 
1. outline of the framework 
　　it is connonly the case that vision data will verify not only the position of the specified body at the current situation  body instance   but also some other body instances whose positions are relevant to or deduced from this body instance. the verification vision indicates discrepancies between an expected position and an actual position in a particular situation and it will be necessary to nodifv subsequent positions in the light of this information. for example  suppose a robot moved a block to a specified position and then moved away a fixed amount waiting for the vision system to operate. 1 the verified position shows that the block is not exactly at the specified position  then we are sure that the robot hand is also not at the position where we supposed it to have been. 
　　wo want to avoid  as much as possible  reasoning which involves the symbolic expressions representing verified positions. otherwise these expressions  or parts of then  would appear in a 
　　large number of places in the run time code  and the evaluation of each of them would take too much time. on the other hand  if we can find a method of using the verified position only at run time  then the run time system will work faster. here we outline a method of using verified positions at run tine only. we will enploy two reasonable assumptions in the following discussion. they are: 
1. the nominal position of a body is assumed to be accurate unless there is some evidence  e.g. vision data  to the contrary. 
 .. the movement of a robot arm is assumed to be accurate over small distances. 
1.1. analysis of expressions of the body instance position 
　　we first need to establish how the actual position of a body instance is related to its nominal position and the vision verification data  even after some movements have been made. a movement of a body  b  in one situation  i  to the 
next  i-l  can be represented by a matrix tb i+1  such that 
	pnb i+l  = pnbi * tb i+1  	 1  
where pnbi and pnb i+1  are matrices representing the -.nominal positions of the body in situations i and i+1. if we consider that the body b makes a virtual movement from its nominal to its virtual position then the movement can be represented by a 
matrix fmbi: 
	fmbi * pnbi = pvbi 	 1  
	and we call fmbi the modifying factor 	of 	the 
body instance pnbi. 
we know from  1  that: 
	f !bi = pvbi * pnbi ' 	 1  
　　how suppose that the nominal position of the body b in situation j  pnbj  is produced by a sequence of specified movements from its nominal position in situation i  then it can be seen that 
	phbj = pnbi * tb i1l  j  	 1  
where 
 tb i+l j  = tb i+1  * ... * tbj  s  and j   i . 
the actual position of the body in situation j  the actual position is not a  verified position  since we have not verified it by vision commands  but we consider that it is equivalent to a verified position in our discussion  will be related to the actual position of the body in situation i bv exactly the same sequence of movements  and therefore 
pvbj = pvbi * tb i+1   j  
- pvbi * pnbi 1* pnbi * tb i+l j  
- pvbi * pnbi-' * pnbj 
	= fmhj * pnbj 	 f   
where 	fllbj - pvbi * 	pnbi-' 	 /  
fmbj is referred! to as the modifying factor of the body instance pnbj. 
　　we can see from the discussion above that the actual position of a body instance is determined by two parts: the nominal position and a modifying factor  and it is onlv the modifying factor that is affected by the vision data. when a body instance is verified  its modifying factor is defined by the vision data and by the nominal position of the body instance. furthermore  if we know the modifying factor of a body instance and the subsequent nominal movements of that bodv  then we can determine the modifying factors for the instances of that body in the subsequent situations. as we can obtain the nominal position for each body instance by using the current rapt cycle finder system  we can determine the nominal movement of a body instance between any situations. therefore  we can obtain the modifying factor for every body instance by working forward from the modifying factor of the verified body instance. we can see from equations  1  and  1  that we can also assume that once the actual position of a body has been found to be different from its nominal one  accurate positions of the body in subsequent situations will bear the same relationship to their nominal positions until a new modifying factor is found either by new vision data or by a specified action  see sec. 1.1 . therefore  in our vision verification system we may deduce the nominal position of each body instance at compile time in the usual way  then evaluate the modifying factors at run time  and get the actual positions by matrix multiplication. 
　　the introduction of modifying factors will affect the actions of bodies. we will refer to the 

b.yin 1 
tor array can he divided into two stapes  the reasoning stage and the simplifying stage. in the reasoning stage  all the modifying factors for the body instances in the initial  situation will he assigned an identity natrix symbol   1  . in each of the following situations  if a bodv is verified in that situation the corresponding modifying factor will be assigned the symbolic position expression pvbi   p    otherwise a pointer or a set of pointers which points to another modifying factor will be assigned to the modifying factor according to rules which will be discussed later. 
　　in the simplifying stage  the modifying factor array is simplified according to a set of rules. this process makes the array more compact. and speeds up the run time evaluation of the array. 
.'1.1. modifying factors in symbolic reasoning 
　　as well as influencing the symbolic reasoning   the introduction of modifying iactors will also influeuce the way vision commands are used. the modifying factors from previous vision commands may enable us to make better predictions for positions of bodies in subsequent vision commands. 
　　in a vision command package  some inviolate statements may be used to indicate the most reliable relationships holding between the object to be verified and some other objects. the modifying factors for the body instances of these reference objects may not be identity matrices. this means that they may not be at their nominal positions  though the relationships mentioned in the inviolate statements still hold. discussion of how to deal with these two points can be found in 
. 
1.1. pules for setting; the pointers 
　　the rules discussed below are used for setting the pointers in the modifying  factor array for bo-
dies according to their status in the rapt program such as being moved   tied and so on. for convenience  we will refer to the rules for making the pointers as linking rules. 
1.1. actions 
	there are two kinds of 	action 	statements 	in 
rapt: move statements and turn statements. for making a modifying factor array  however  we are more concerned with spatial relationship specifications about bodies and the association between the relationship specifications and action statements. if the position of a body instance is restricted by some spatial relationship specifications  then we refer to the position of the body instance as a specified position. on the other hand  if there are no explicit spatial relationship specifications constraining the position of the body instance  then we refer to the position as an unspecified position. 
　　the following are the linking rules for the body instances which are moved  i.e. moved or turnd  directly by action statements rather than by the effects of ties and subassemblies. the 

linking rules for ties and subassemblies will 	be 
discussed in sec. 1.1 and sec. 1.1. 
al if a body is noved to an unspecified position  then the pointer of the body instance points to its modifying factor in the previous situation. 
a1 if a body is moved to a specified position  then the pointer of the body instance points to the modifying factor for the body instance of the body between which the relationships hold  reference body  in the current situation unless the condition mentioned in rule  a1  is net. 
a1. if the reference body in a specified position is tied to the bodv to be moved or belongs to the same subassembly as the body to be moved then the pointer of the body instance points to its modifying factor in the previous situation. 
1.1. ties 
　　in the rapt language bodies can be tied together during an action  and this means that they maintain the same relative position before and after the action. therefore  any descriptions of the motion of one body which is tied to another must apply to the motions of that other. tles are made and revoked by tied and untied statements. 
　　we can see  from the definition of tie  that the modifying factors for bodies which have been tied together must keep the same relations to each other throughout the existence of the tic  except when local vision commands  see  for details  are used. this means that a change in the modifying factor of one member of a tie must be applied to that of the other member. moreover  this effect will continue after the two bodies are untied until one of them is moved by specified actions or both of them are verified by global vision com-
mands individually  see  for details . 
　　let us consider an example in order to understand how to keep the same relationship between two modifying factors when one of them is changing. suppose bodies a and b are tied together with different modifying factors mal and mbl respectively. if mal is changed to ma1 then the relative change for modifying factor mal is tr. we have 
mal * tr = ma1 tr = mal-'* ma1 	 1  
and the new modifying factor mb1 for b is 
mb1 = mbl * tr 
	= mbl * mal-1 * ma1 	 1  
　　the discussion above can be expanded to cover a tie linking more than two bodies and the circumstances when this large tie has been broken. suppose n bodies bl  ...  bn are tied together by n-1 tie statements. for convenience we will call the result a  super  tie in the following discuss ion. 
　　the linking rules for tie statements can be summarized as follows: 
tl. in a super tie  if body bj is moved in situation i by a specified action which brings about some relationships between body bj and body c  then the pointer of body bj will point to the modifying factor for body c in situation i while the modifying factors of other bodies in the super tie are pointer triples. the triples have the form 
 pl p1 p1  where pi points to the modifying factor for the same body in situation  i-1   p1 to the modifying factor for bj in situation  i-1  and p1 points to the modifying factor for bj in situation i. 
t1. in a super tie  if body bj is verified by a set of global vision commands in situation i  then its modifying factor will be assigned a symbolic position expression  p  while that of other bodies in the super tie are pointer triples. the triples have the same form and contents as in rule  tl . 
t1. in a super tie  if body bj is verified by a 
set of local vision commands in situation i  then its modifying factor will be assigned a symbolic position expression  p  while that of other bodies in the super tie will refer to their modifying factor in situation  i-1 . 
　　five similar rules are applied after a super tie has been broken. details can be found in 
. 
1.1. subassembly 
　　the subassembly is a set of bodies between whose features certain specified relationships hold for the duration of the existence of the subassembly . subassembli es differ from ties in that there may be more tha n two bodies within a subassembly and the com ponents of a subassembly can move with respect to e ach other during the existence of the subassembl y  provided that the relations remain valid. tre atment of subassemblies is similar to that for hes. detailed discussion can be found in . 
　　the following simple example shows how the linking rules are used. 

therefore the modifying factor for body a in the new situation is ma1 while mb1 can be represented by a pointer triple  pl p1 p1  in which pl points to mbl  p1 points to mal and p1 points to ma1. at 
run time the triple will be evaluated by the equation 
	 pl  *  p1 -'*  p1  	 1  
remark bodies bl b1 b1 have been defined; 
verify/b1; 	remark abbreviation for vision command package  now in situation i; 
move/bl; 
fixed/bl   b1; remark abbreviation for a set of relations which completely defines the position of bl with b1  sit i+1 ; 

tied/bl b1; verify/b1; 	remark sit i+1; nove/b1; 
	fixed/b1 b1; 	remark sit i+1; 

1. the position of the camera 
　　the camera is defined in rapt in the usual way as an ordinary body with a .specified focal length. this means that as with any other body it can be operated on by the system  being noved   tied and so on. it can also be mounted on a robot hand.  n most cases the position of a camera body can be modified by the modifying factor array according to the linking rules. for example  the position of one camera can be verified by another camera. however  if the result of a global vision command package will affect the position of the camera to 
be used  according to the linking rules listed in see. 1  then the vision command package will he dealt with as a local one. in this case  only the position of the body to be verified is subject to modification while the position of the camera is 
unchanged. 
1. the generalily of the framework 
　　so far we have discussed how to use the framework to handle verification vision information. in fact  the framework also has the capability to handle other kinds of sensor information for the purposes of verification. generally speaking  in rapt  when we use a sensor to detect a feature of an object  we create a new relationship between that object and the world  as the absolute position of the sensor in the world is usually known when the sensor is used. the nature of the relationship depends entirely on the type of sensor. since sensor data is available at run time  at compile time we can only obtain symbolic relationships. when the symbolic relations are sufficient to  fix  the object  we can use a suitable symbolic reasoning system to deduce the symbolic position of the object. we can then evaluate the symbolic position expression at run time when the corresponding sensor data is available. in order to introduce a new kind oi sensor  we need only to provide a set of commands to specify how to use the new sensor  and a new symbolic reasoning system which is capable of dealing with the new relationships created by this sensor. the framework handles new kinds of sensor information in the same way as it does vision data. 
　　now let. us consider an example. suppose we use a touch sensor to detect a plane feature pi of a body b. when we detect a point on pl  we create 
b. yin 1 
a relationship between the v/orld and the body  the  against  relationship between a sphere feature  a vertex is considered as a sphere with radius 1 in rapt  of the world and the plane pl. if we detect some carefully chosen planes of the body b with the touch sensor and so obtain enough against relationships  then we will be able to deduce the position of the body b. the symbolic position expression of b can be stored in the modifying factor array and can be used in exactly the same way 
as verification vision data. 
　　some authors  e.g.   have suggested that vision sensors have poor precision and therefore it is better to use both vision and contact sensors. vision sensors can be used for coarse sensing while contact sensors can be used for fine resolution. in this case  the generality of the framework to handle both kinds of sensor information is very important. 
1. discussion 
　　there are two main problems in 	using 	sensory information in programming and controlling robots. the first is how to get accurate sensory 	information and the second is how to process and use the information in the system. 	the latter problem  as harmon points out   seems the harder of the two and has more implications for the theoretical 	use o f sensors. 
　　some work on using vision data in robot programming  languages has been reported  such as val  1  1 and al . in the val system  vision is used to recognize objects in a picture taken by a tv camera  and to locate them. the recognition is done by comparing the blobs in the picture with trained characteristics of given prototypes in an effort to find a match. the vision technique used in the val system is of the first generation    i.e. it uses a binary picture and produces a 1 explanation. in the al system  vision is used to f u l f i l l the verification task . both the languages mentioned above are classified as endeffector level languages by koutsou   i.e. only the end-effector of the robot appears explicitly in the program. 
　　some object level languages have the ability to use some touch sensor information. for example  in autopass  force or torque sensor information is used to provide special threshold 
values in order to constrain or terminate some actions. 	in the lama system  1  the force sensor is used to detect whether a specified action 	ter-
minates at a correct position or not. in both the above systems  sensory information is used as a condition in a decision tree and no further explanation of the information is made. 
　　in contrast to these systems  the rapt language with the verification vision facility provides a method of specifying vision tasks and reasoning about the vision data symbolically offline in an object level language. it makes no assumptions about the relative positions of objects and sensors as a teach-method does. it allows partial information about positions to be combined 

with sensor information in a general way. at run time  when the vision data is available  the results of symbolic reasoning are evaluated and are used to improve the system's knowledge about its environment. this method uses grey level pictures and produces a 1-d interpretation rather than a 1-d one. the framework described in this paper enables us to combine the verification vision facility with an object level language in an intelligent way. the framework which is used by the system for handling vision data has some generality. it is not only able to handle the vision data  but also able to deal with some other kinds of sensory information  such as touch sensor information. using the framework to implement the vision facility within rapt requires few changes in the current rapt system. 
　　in this paper we have only discussed the theoretical framework. the system has been imple-
mented within rapt and used with simulated vision data  but as yet no link has been made with a real vision system. this will  of course  provide a real test  and we anticipate some problems in ensuring the correctness of match between the model features and their images. 
acknowledgement 
　　i am grateful to a. p. ambler and r. 	j. 	popplestone for their supervision. 	i would also like to thank p. brna  d. corner  s. cameron  r. featherstone  	r. fisher and j. hallam  for their helpful criticisms of my work. 
