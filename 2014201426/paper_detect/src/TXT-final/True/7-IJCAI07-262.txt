
this paper describes work on the detection of anomalous material in text. we show several variants of an automatic technique for identifying an 'unusual' segment within a document  and consider texts which are unusual because of author  genre  biber  1   topic or emotional tone. we evaluate the technique using many experiments over large document collections  created to contain randomly inserted anomalous segments. in order to successfully identify anomalies in text  we define more than 1 stylistic features to characterize writing  some of which are well-established stylistic determiners  but many of which are novel. using these features with each of our methods  we examine the effect of segment size on our ability to detect anomaly  allowing segments of size 1 words  1 words and 1 words. we show substantial improvements over a baseline in all cases for all methods  and identify the method variant which performs consistently better than others.
1 introduction
anomaly detection refers to the task of identifying documents  or segments of text  that are unusual or different from normal text. previous work on anomaly detection  sometimes called novelty detection  has assumed the existence of a collection of data that defines  normal   e.g. allison and guthrie  1; markou and singh  1   which was used to model the normal population  and methods were developed to identify data that differs significantly from this model. as an extension to some of that work  this paper describes a more challenging anomaly detection scenario  where we assume that we have no data with which to characterize  normal  language. the techniques used for this task do not make use of any training data for either the normal or the anomalous populations and so are referred to as unsupervised.
　in this unsupervised scenario of anomaly detection  the task is to find which parts of a collection or document are most anomalous with respect to the rest of the collection. for instance  if we had a collection of news stories with one fictional story inserted  we would want to identity this fictional story as anomalous  because its language is anomalous with respect to the rest of the documents in the collection. in this example we have no prior knowledge or training data of what it means to be  normal   nor what it means to be news or fiction. as such  if the collection were switched to be mostly fiction stories and one news story then we would hope to identify the news story as anomalous with respect to the rest of the collection because it is a minority occurrence.
　there is a very strong unproven assumption that what is artificially inserted into a document or collection will be the most anomalous thing within that collection. while this might not be true in the general case  every attempt was made to ensure the cohesiveness of the collections before insertion to minimize the chance of finding genuine  unplanned anomalies. in preliminary experiments where a genuine anomaly did exist  for example  a large table or list   it was comforting to note that these sections were identified as anomalous.
　the focus of this paper is the identification of segments in a document that are anomalous. identifying anomalies in segments is difficult because it requires sufficient repetition of phenomena found in small amounts of text. this segment level concentration steered us to make choices and develop techniques that are appropriate for characterizing and comparing smaller segments.
　there are several possibilities for the types of anomaly that might occur at the segment level. one simple situation is an off-topic discussion  where an advertisement or spam is inserted into a topic specific bulletin board. another possibility is that one segment has been written by a different author from the rest of the document  as in the case of plagiarism. plagiarism is notoriously difficult to detect when the source of the plagiarism can be found  using a search engine like google or by comparison to the work of other students or writers.  in addition  the plagiarized segments are likely to be on the same topic as the rest of the document  so lexical choice often does not help to differentiate them. it is also possible for a segment to be anomalous because of a change in tone or attitude of the writing. the goal of this work is to develop a technique that will detect an anomalous segment in text without knowing in advance the kind of anomaly that is present.
　unsupervised detection of small anomalous segments can not depend on the strategies for modeling language that are employed when training data is available. with a large amount of training data  we can build up an accurate characterization of the words in a document. these languagemodeling techniques make use of the distribution of the vocabulary in a document and  because language use and vocabulary are so diverse  it is necessary to train on a considerable amount of data to see the majority of cases  of any specific phenomenon  that might occur in a new document. if we have a more limited amount of data available  as in the segments of a document  it is necessary to characterize the language using techniques that are less dependent on the actual distribution of words in a document and thus less affected by the sparseness of language. in this paper we make use of techniques that employ some level of abstraction from words and focus on characterizing style  tone  and classes of lexical items.
　we approach the unsupervised anomaly detection task slightly differently than we would if we were carrying out unsupervised classification of text  oakes  1; clough  1 . in unsupervised classification  or clustering  the goal is to group similar objects into subsets; but in unsupervised anomaly detection we are interested in determining which segments are most different from the majority of the document. the techniques used here do not assume anomalous segments will be similar to each other: therefore we have not directly used clustering techniques  but rather developed methods that allow many different types of anomalous segments within one document or collection to be detected.
1 characterizing segments of text
the use of statistical methods with simple stylistic measures  milic  1  1; kenny  1  has proved effective in many areas of natural language processing such as genre detection  kessler et al.  1; argamon et al.  1; maynard et al.  1   authorship attribution  mcenery and oakes  1; clough et al.  1; wilks 1   and detecting stylistic inconsistency  glover and hirst 1; morton  1  mccolly  1; smith  1 . determining which stylistic measures are most effective can be difficult  but this paper uses features that have proved successful in the literature  as well as several novel features thought to capture the style of a segment of text.
　for the purposes of this work  we represent each segment of text as a vector  where the components of the vector are based on a variety of stylistic features. the goal of the work is to rank each segment based on how much it differs from the rest of the document. given a document of n segments  we rank each of these segments from one to n based on how dissimilar they are to all other segments in the document and thus by their degree of anomaly.
　components of our vector representation for a segment consist of simple surface features such as average word and average sentence length  the average number of syllables per word  together with a range of readability measures  stephens  1  such as flesch-kincaid reading ease  gunning-fog index  and smog index  some vocabulary richness measures such as: the percentage of words that occur only once  percentage of words which are among the most frequent words in the gigaword newswire corpus  1 years of newswire   as well as the features described below. all segments are passed through the rasp  robust and accurate statistical parser  part-of-speech tagger developed at the universities of sussex and cambridge. words  symbols and punctuation are tagged with one of 1 part-ofspeech tags from the claws 1 tagset. we use this markup to compute features that capture the distribution of part of speech tags. the representation of a segment includes the
i  percentages of words that are articles  prepositions  pro-
nouns  conjunction  punctuation  adjectives  and adverbs
ii  the ratio of adjectives to nounsiii  percentage of sentences that begin with a subordinating or
coordinating conjunctions  but  so  then  yet  if  because  unless  or... 
iv  diversity of pos tri-grams - this measures the diversity in
the structure of a text  number of unique pos trigrams divided by the total number of pos trigrams 
　texts are also run through the rasp morphological analyzer  which produces words  lemmas and inflectional affixes. these are used to compute the
i  percentage of passive sentences ii  percentage of nominalizations.
1 rank features
authors can often be distinguished by their preference for certain prepositions over others or their reliance on specific grammatical constructions. we capture these preferences by keeping a ranked list sorted by frequency of the:
i 	most frequent pos tri-grams list ii 	most frequent pos bi-gram list iii  most frequent pos list iv  most frequent articles list
v 	most frequent prepositions list vi  most frequent conjunctions list vii  most frequent pronouns list
　for each segment  the above lists are created both for the segment and for the complement of that segment. we use 1 - r  where r is the spearman rank correlation coefficient  to indicate the dissimilarity of each segment to its complement.
1 characterizing tone
the	general	inquirer	dictionary
 http://www.wjh.harvard.edu/~inquirer/  developed by the social science department at harvard  contains mappings from words to social science content-analysis categories. these content-analysis categories attempt to capture the tone  attitude  outlook  or perspective of text and can be an important signal of anomaly. the inquirer dictionary consists of 1 words mapped into 1 categories with most words assigned to more than one category. the two largest categories are positive and negative which account for 1 and 1 words respectively. also included are all harvard iv-1 and lasswell categories. we make use of these categories by keeping track of the percentage of words in a segment that fall into each category.
1 method
all experiments presented here are performed by characterizing each segment as well as characterizing the complement of that segment  the union of the remaining segments . this involves constructing a vector of features for each segment and a vector of features for each segment's complement as well as a vector of lists  see rank features section  for each segment and its complement. so  for every segment in a document we have a total of 1 vectors:
v1 - feature vector characterizing the segment
   v1 - feature vector characterizing the complement of the segment
v1 - vector of lists for all rank features for the segment
   v1 - vector of lists for all rank features for the complement of the segment
we next create a vector of list scores called v1 by computing the spearman rank correlation coefficient for each pair of lists in vectors v1 and v1.  all numbers in v1 are actually 1-spearman rank coefficient so that higher numbers mean more different . we next sum all values in v1 to produce a rank feature difference score.
　finally  we compute the difference between two segments by taking the average difference in their feature vectors plus the rank feature difference score.
1 standardizing variables
while most of the features in the feature vector are percentages  % of adjectives  % of negative words  % of words that occur frequently in the gigaword  etc.  some are on a different scale  such as the readability formulae. to test the impact of different scales on the performance of the methods we also perform all tests after standardizing the variables. we do this by scaling all variables to values between zero and one.
1 experiments and results
in each of the experiments below all test documents contain exactly one anomalous segment and exactly 1  normal  segments. whilst in reality it may be true that multiple segments are anomalous within a document  there is nothing implicit in the method which looks for a single anomalous piece of text; for the sake of simplicity of evaluation  we insert only one anomalous segment per document.
　the method returns a list of all segments ranked by how anomalous they are with respect to the whole document. if the program has performed well  then the truly anomalous segment should be at the top of the list  or very close to the top . our assumption is that a human wishing to detect anomaly would be pleased if they could find the truly anomalous segment in the top 1 or 1 segments marked most likely to be anomalous  rather than having to scan the whole document or collection.
　the work presented here looks only at fixed-length segments with pre-determined boundaries  while a real application of such a technique might be required to function with vast differences between the sizes of segments. once again  there is nothing implicit in the method assuming fixedlength sizes  and the choice to fix certain parameters of the experiments is to better illustrate the effect of segment length on the performance of the method. one could then use either paragraph breaks as natural segment boundaries  or employ more sophisticated segmentation techniques.
　we introduce a baseline for the following experiments that is the probably of selecting the truly anomalous segment by chance. for instance  the probability of choosing the single anomalous segment in a document that is 1 segments long by chance when picking 1 segments is 1 + 1 + 1 or 1%.
1 authorship tests
for these sets of experiments we examine whether it is possible to distinguish anomaly of authorship at the segment level. we test this by taking a document written by one author and inserting in it a segment written by a different author. we then see if this segment can be detected using our unsupervised anomaly techniques.
　we create our experimental data from a collection consisting of 1 thousand words of text written by each of 1 victorian authors: charlotte bronte  lewis carroll  arthur conan doyle  george eliot  henry james  rudyard kipling  alfred lord tennyson  h.g. wells.
　test sets are created for each pair of authors by inserting a segment from one author into a document written by the other author. this creates 1 sets of experiments  one for each author inserted into every author.  for example we insert a segment  one at a time from bronte into carroll and anomaly detection is performed. likewise we insert segments one at a time from carroll into bronte and perform anomaly detection. our experiment is always to test if we can detect this inserted paragraph.
　for each of the 1 combinations of authors we insert 1 random segments from one into the other  one at a time. we performed 1 pairs * 1 insertions each = 1 sets of insertion experiments. for each of these 1 insertion experiments we also varied the segment size to test its effect on anomaly detection. we then count what percentage of the time this paragraph falls within the top 1  top 1  top 1  and top 1 segments labeled by the program as anomalous. the results shown here report the average accuracy for each segment size  over all authors and insertions . the average percent of time we can detect anomalous segments in the top n segments varies according to the segment size  and as expected  the average accuracy increases as the segment size increases. for 1 word segments  anomalous segment was found in the top 1 ranked segments about 1% of the time  1% in the top ten  1% of the time in the top 1 and 1% of the time in the top three segments . for 1 word segments  average accuracy ranged from 1% down to 1% and for 1 word segments it ranged from 1% down to 1%.
top n
segmentspercentage of the time foundpercentage ofthe time found  standardized features chancesegment size: 1 words1.1.111.1.111111.1.11segment size: 1 words1.1.111.1.111.1.111.1.11segment size: 1 words1.1.111.1.111.1.111.1.11table 1: average results for authorship tests
1 fact versus opinion
in another set of experiments we tested whether opinion can be detected in a factual story. our opinion text is made up of editorials from 1 newspapers making up a total of 1 words.
　our factual text is newswire randomly chosen from the english gigaword corpus  graff  1  and consists of 1 different 1 word segments one each from one of the 1 news wire services.
each opinion text segment is inserted into each newswire service one at a time for at least 1 insertions on each newswire. tests are performed like the authorship tests using 1 different segment sizes.
　results in this set of experiments were generally better and the average accuracy for 1 word segments  top 1 ranking was 1%  1% in the top 1 ranked segments.  small segment sizes of 1 words also yielded good results and found the anomaly in the top 1  1% of the time  although only 1% of the time in the top 1.
top n
segmentspercentage of the time foundpercentage of the time found  standardized features chancesegment size: 1 words1.1.1.11111.1.1.1111segment size: 1 words1.1.1.11111.1.1.1111segment size: 1 words11.11111.111table 1: average results for fact versus opinion
1 newswire versus chinese translations
in this set of experiments we test whether english translations of chinese newspaper segments can be detected in a collection of english newswire. we used 1 thousand words of chinese newspaper segments that have been translated into english using google's chinese to english translation engine. english newswire text is the same randomly chosen 1 word segments from the gigaword corpus. again  tests are run using the 1 different segment sizes.
　for 1 word segments  the average accuracy for detecting the anomalous document among the top 1 ranked segments is 1% and in the top 1 ranked segments 1% of the time.
top n
segmentspercentage of the time foundpercentage of the time found  standardized features chancesegment size: 1 words1.1.1.11111.1.1.1111segment size: 1 words1.1.1.11111.1.1.1111segment size: 1 words1.1.1.1111111111table 1: average results for newswire versus chinese translations
1 conclusions  
there experimental results are very positive and show that  with a large segment size  we can detect the anomalous segment within the top 1 segments very accurately. in the author experiments where we inserted a segment from one author into 1 segments by another  we can detect the anomalous author's paragraph in the top 1 anomalous segments returned 1% of the time  for a segment size of 1 words   which is considerably better than chance.  if you randomly choose 1 segments out of 1 then you only have a 1% chance of correctly guessing the segment.  other experiments yielded similarly encouraging figures.  the task with the best overall results was detecting when a chinese translation was inserted into a newswire document  followed surprisingly by the task of detecting opinion articles amongst facts.  
　on the whole  our experiments show that standardizing the scores on a scale from 1 to 1 does indeed produce better results for some types of anomaly detection but not for all the tasks we performed.  we believe that in all cases where it performed worse than the standard raw scores  were cases where the genre distinction was very great. we performed an additional genre test not reported in this paper where anarchist's cookbook segments were inserted into newswire.  many of the readability formulas  for instance  distinguish these genre differences quite well but their effects on anomaly detection are greatly reduced when we standardize these scores. 
