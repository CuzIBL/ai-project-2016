
we propose an efficient framework for humanaided morphological annotation of a large spontaneous speech corpus such as the corpus of spontaneous japanese. in this framework  even when word units have several definitions in a given corpus  and not all words are foundin a dictionary or in a training corpus  we can morphologically analyze the given corpus with high accuracy and low labor costs by detecting words not found in the dictionary and putting them into it. we can further reduce labor costs by expanding training corpora based on active learning.
1 introduction
the  spontaneous speech: corpus and processing technology  project sponsored the construction of a large spontaneous japanese speech corpus  the corpus of spontaneous japanese  csj   maekawa et al.  1 . the csj is a collection of monologues and dialogues  the majority being monologues such as academic presentations and simulated public speeches. the simulated public speeches are short speeches presented specifically for the corpus by paid non-professional speakers. the csj includes transcriptions of the speeches as well as audio recordings of them. one of the goals of the project is to detect two types of word segments and corresponding morphological information in the transcriptions. the two types of word segments were defined by the members of the national institute for japanese language and are called short words and long words. the term short word approximates an item found in an ordinary japanese dictionary  and long word represents various compounds. the length and morphological information of each are different  and every short word is included in a long word  which is shorter than a japanese phrasal unit  a bunsetsu. for example  the short and long words in  keitaisokaiseki ni tsuite ohanasi itashimasu   i would like to talk about morphological analysis  are represented as shown in table 1. in this table  each line indicates a short word  and ten short words and four long words are shown.
　approximately 1 million short words were detected in the csj  which makes it the largest spontaneous speech corpus in the world. on the other hand  there were fewer long words because each long word consists of one or more short words. approximately one-eighth of the words have been manually detected  and morphological information such as part-of-speech  pos  categories and conjugation types have been assigned to them. human annotators tagged every morpheme in the one-eighth of the csj that had been tagged  and other annotators checked them. the human annotators discussed their disagreements and resolved them. the accuracies of the manual tagging of short and long words in the one-eighth of the csj were both approximately 1%. the accuracies were evaluated by random sampling. because it took over two years to tag one-eighth of the csj accurately  tagging the remainder with morphologicalinformation would take about twenty years. therefore  the remaining seveneighths of the csj were tagged semi-automatically. in this paper  we describe methods for detecting the two types of word segments and corresponding morphological information. we also describe how to accurately tag a large spontaneous speech corpus. we propose an efficient framework for human-aided morphological annotation of a large spontaneous speech corpus.
　we collectively call short and long words morphemes. we use the term morphological analysis for the process of segmenting a given sentence into a row of morphemes and assigning to each morpheme morphological attributes such as a pos category.
1 framework of morphological annotation
we call a series of processes for morphological analysis and maintenance of a corpus morphological annotation. our framework of morphological annotation is illustrated in figure 1. the purpose of this framework is  given an annotated corpus and a large raw corpus  to improve the quality of the annotated morphological information in both corpora with low labor costs. in this framework  a corpus-based morphological analyzer is used because the definition of a set of partof-speech categories and that of word units are often changed in the middle of constructing a corpus  and a morphological analyzer must be robust to such changes.
　the framework of morphological annotation consists of three parts: maintenance of an annotated corpus  analysis of a large raw corpus  and enhancement of linguistic resources. they are described in more detail in the following sections.

table 1: example of morphological analysis results.
short wordlabellong wordot	dicform lemma	ptpos conjtype conjform otherot	dicform lemma	pos conjtype conjformotherkeitai	keitaikeitai	 form 	ke tainounbakeitaiso- keitaiso- keitaisokaiseki nounso	soso	 element  sosuffixikaiseki	kaiseki	 morphologicalkaiseki kaisekikaiseki  analysis  kaisekinouniaanalysis ni	nini	nipppcase markerbnitsuite	nitsuite	nitsuite	pppcase markertsui	tsukutsuku	 relate 	tsuiverb ka-gyo adfeuphonic changei about & compoundte	tetetepppconjunctiveiwordo	oooprefixbohanashi- ohanashi- ohanashiitasu verb sa-gyo adfhanashi hanasuhanasu talk hanashi verb sa-gyo adfiaitashi	itasu	 talk itashi	itasuitasuitashi	verb sa-gyo adfiamasu	masumasumasu	aux	ending formbamasu	masu	masu	aux	ending formot: orthographic transcription  dicform: dictionary form written in kanji and hiragana characters  lemma: lemma written in kanji and hiragana characters  pos: part-of-speech  pt: phonetic transcription written in katakana characters  conjtype: conjugation type  conjform: conjugation form. ppp: post-positional particle  aux: auxiliary verb  adf: adverbial form.figure 1: framework of morphological annotation.
1 maintenance of an annotated corpus
in general  when an annotated corpus or a training corpus has many errors  a corpus-based analyzer easily overfits the errors  and the analysis accuracy often decreases. therefore  the errors should be detected and corrected to avoid overfitting them. the simplest way to detect and correct the errors in a training corpus is to examine the difference between the original annotated corpus and the corpus automatically annotated by a corpus-based analyzer trained by using the original annotated corpus. in previous studies  a boosting method and an anomaly detection method were applied to detect errors in a corpus  abney et al.  1; eskin  1   and a method for detecting and correcting errors in a corpus was proposed  murata et al.  1 . in the method for detecting and correcting errors  the difference between the annotated labels of the original corpus and those of the automatically annotated corpus is first detected. for each difference  the probabilities of the labels in the original corpus and the automatically annotated corpus are calculated by a model trained using the original annotated corpus. then  the labels in the original corpus whose probabilities are lower than those in the automatically annotated corpus are replaced with the corresponding labels in the automatically annotated corpus. this method was initially applied to the csj  and the morphological information replaced by the method was manually examined.
1 analysis of a large raw corpus and enhancement of linguistic resources
enhancement of the dictionary
when a given raw corpus including unknown words that are in neither the dictionary nor the training corpus  detection errors tend to occur on the unknown words and words to the left and right of the unknown words. in many cases  not a single unknown word but a series of unknown words are found  or an unknown word may consist of known words and unknown characters. in this case  the accuracy of morphological information assigned to a raw corpus can be increased by detecting and putting the unknown words into the dictionary and manually examining words whose probability is estimated as low  uchimoto et al.  1 .
　the cost of manual examination is high when word segments and their pos categories have several definitions. because the csj has two types of word definition  the cost would double. however  when one type of word segment consists of compounds of other types of words  the manual examination of the shortest word segments improves the morphological analysis accuracy for other types of words if we use a method based on a chunking model  uchimoto et al.  1 .
　besides the above methods  in the morphological analysis of the csj  the number of unknown words was further reduced by expanding conjugational words in a dictionary based on a conjugation chart developed by the members of the national institute for japanese language.
active sampling
in general  a corpus-based morphological analysis system requires a large training corpus. however  the accuracy does not improve in proportion to the simple increase of the training corpus. this is because a model for morphological analysis usually considers the relationshipbetween adjacent words  and the simply supplemented data rarely includes relationships that were not found in the initial training corpus. therefore  when expanding a training corpus  data must be selected that include as many sequences of words as possible that are difficult for the morphological analysis model to analyze. the expansion should be done so that a big improvement can be achieved with as small a supplement as possible. argamon-engelson et al. reported that data that can usefully be added to training data can be selected by extracting sentences whose analysis results obtained by using randomly selected models include a great amount of consistency  argamon-engelson and dagan  1 . however  they assumed that word boundaries were given and that the data had no unknown words. their method cannot be simply applied to japanese sentences when word boundaries are inconsistent because no blank spaces are used between words in japanese sentences. in our research  we assumed that word boundaries were not given and that the data had unknown words. this section describes an active sampling method using a single statistical model for expanding a training corpus.
　the active sampling is conducted as follows  under three assumptions. under these assumptions  a set of sentences that minimizeis extracted  and words in the extracted sentences whose probabilities are below a threshold are examby the morpheme model for all words in a set of sentences. ined. here  p is the product of the probabilities estimated
the three assumptions are as follows.
1. similar errors tend to appear in the same speech.
this is because specific words or sequences of words may only appear in a certain speech. therefore  the data should be compiled from as varied speeches as possible to avoid examining the same erroneous words. that is why x% of words in each speech were examined. the maximum number of examined words was chosen to be  when the number of words in a speech was n.
1. the sentence will have more errors if the product ofprobabilities estimated for its words are low.
therefore  we preferred sentences with low probabilities.
1. any word whose probability is over a certain thresholdcan be considered correct.
therefore  words whose probabilities are over a threshlated. the threshold should be set rather high to reduce old are ignored when p mentioned above is calcu-
the chance that errors will be ignored. in our preliminary experiments  the threshold was set at 1% because the accuracy obtained using this threshold was 1% or higher.
1 models and algorithms for morphological analysis
one of the most important problems in morphological analysis is that posed by unknown words  which are words found in neither a dictionary nor a training corpus. two statistical approaches have been applied to this problem. one is to find unknown words from corpora and put them into a dictionary  e.g.   mori and nagao  1    and the other is to estimate a model that can identify unknown words correctly  e.g.   kashioka et al.  1; nagata  1  . uchimoto et al. used both approaches. they proposed a morphological analysis method based on a maximum entropy  me  model  uchimoto et al.  1 . their method uses a model that estimates how likely a string is to be a morpheme as its probability  and thus  it has the potential to overcome the unknown word problem. therefore  we used their method to morphologically analyze the csj.
　in this paper  we assume that two types of word segments  short and long words  are defined and every long word consists of one or more short words  as in the csj.
method based on a morpheme model
given a tokenized test corpus  namely  a set of strings  the problem of japanese morphological analysis can be reduced to that of assigning one of two tags to each string in a sentence. a string is tagged with a 1 or a 1 to indicate whether it is a morpheme. when a string is a morpheme  a grammatical attribute is assigned to it. a tag of 1 is thus assigned one of a number  n  of grammatical attributes assigned to morphemes  and the problem becomes assigning an attribute  from 1 to n  to every string in a given sentence.
　we define a morpheme model that estimates the likelihood that a given string is a morpheme and has a grammatical attribute  i 1 ＋ i ＋ n . we implemented this model within an me modeling framework  jaynes  1; 1; berger et al.  1 . the model is represented by eq.  1 :

where a  called a  future   is one of the categories for classification  and it can be one of  n + 1  tags from 1 to n; b  called a  history   is the contextual or conditioning information that enables us to make a decision among the space of futures; and zλ b  is a normalizing constant determined by the requirement that for all b. the computation of pλ a|b  in any me model is dependent on a set of  features   which are binary functions of the history and future. for instance  one of our features is
               1 : if has b fj  = 1 & a = ai gi j a b  = f =  pos  1  major  : verb
.
here  has b fj   is a binary function that returns 1 if the history  b  has feature fj. the features used in our experiments are described in section 1.
　given a sentence  the probabilities of n tags from 1 to n are estimated for each length of string in that sentence by using the morpheme model. from all possible divisions of morphemes in the sentence  an optimal one is found by using the viterbi algorithm or a branch and bound method. each division is represented as a particular division of morphemes with grammatical attributes in a sentence  and the optimal division is defined as a division that maximizes the product of the probabilities estimated for each morphemein the division.
　in the csj  transcriptions consist of basic forms and pronunciations. the pronunciationis transcribed separately from the basic form written in kanji and hiragana characters.
speech sounds are faithfully transcribed in katakana characters as the pronunciation and represented as basic forms in kanji and hiragana characters. the text we targeted for morphological analysis is the transcription in the csj. when all possible divisions of morphemes in a sentence are obtained  they are matched with the pronunciation part in each line representing a bunsetsu by using a dynamicprogramming method. then  morphemes whose phonetic transcription candidates do not match the aligned pronunciation part are eliminated before searching for the optimal division of the morphemes.
method based on the chunking model and transformation rules
long word segments and their pos information are detected by using a method described below after detecting short word segments and their pos information by using a morpheme model.
　given the two types of word segments  the longer of which consists of compounds of the shorter  the problem of detecting long word segments and their pos information can be reduced to the problem of assigning one of four labels  as explained below  to each short word. we call the model that estimates the likelihood of the four labels a chunking model. we implemented this model within me or support vector machine  svm  based modeling frameworks. the four labels are as follows.
ba: the beginning of a long word  and the pos information of the long word agrees with that of the short word.
ia: the middle or end of a long word  and the pos information of the long word agrees with that of the short word.
b: the beginning of a long word  and the pos information of the long word does not agree with that of the short word.
i: the middle or end of a long word  and the pos information of the long word does not agree with that of the short word.
the label assigned to the leftmost constituent of a long word is  ba  or  b . the labels assigned to the other constituents of long words are  ia  and  i . a short word that  ba  or  ia  is assigned to has the same pos information as its corresponding long word. here  the pos information represents a set of a pos category  conjugation type  conjugation form  and other detailed information on pos  as shown in table 1. for example  in the table  the labels  in the  label  column  are assigned to the short words. if these labels are correctly detected  the pos information of the long words can be obtained from the short words to which  ba  or  ia  is assigned. in the example  the pos information can be detected for all the long words except  nitsuite . on the other hand  only the long word segment information can be assigned to  nitsuite  even if the assigned labels for its constituents are correct because it has additional pos information as a compound word that is different from the pos information of its constituents   ni    tsui   and  te . in this case  the pos information can be obtained by using the transformationrules mentionedlater.
　a given sentence is labeled from its beginning/end of to its end/beginning. when using an me-based model  the optimal set of labels is obtained by finding a division that maximizes the product of the probabilities estimated for each label assigned to each short word. the model is represented by eq.  1 . in the equation  a can be one of the four labels. the optimal set of labels is found by using the viterbi algorithm or a branch and bound method. on the other hand  svm is a binary classifier. therefore  we expanded it to a multi-class classifier by using multi-class methods such as a pairwise method and a one-versus-rest method. the optimal label determined using an svm model is deterministically assigned to each short word. the features used in our experiments are described in section 1.
　the transformation rules are automatically acquired from the training corpus by extracting long words with constituents  namely  short words  that are labeled only  b  or  i . a rule is constructed by using the extracted long word and the adjacent short words on its left and right. for example  the rule shown in figure 1 was acquired from the example shown in table 1. this rule indicates that when the labels  b    i   and  i  are assigned to  ni   post-positional particle    tsui   verb   and  te   post-positional particle   respectively  the combination  nitsuite  is transformed into a long word having the morphologicalinformation  a post-positional particle  case marker  and compound word. if several different rules have the same antecedent part  only the rule with the highest frequency is chosen. if no rules can be applied to a long word segment  rules are generalized in the following steps.
1. delete the posterior context
1. delete the anterior and posterior contexts
1. delete the anterior and posterior contexts and lexicalinformation such as orthographic transcriptions  dictionary forms  lemmas  and phonetic transcriptions
if no rules can be applied to a long word segment in any step  the pos category of the leftmost constituent of the long word is assigned to the long word.
　the dictionary form and lemma of a long word are usually generated by concatenating those of short words. as for a spoonerism  for example  in case of a long word  ipponbari  consisting of three short words   ichi    hon   and  hari   information on the phonetic transcriptions of the short words is used to generate the dictionary form and the lemma of the long word when concatenating the short words.
1 experiments and discussion
1 experimental conditions
in ourexperiments  we used 1short wordsand 1 long words for training and 1 short words and 1 long words for testing. those words were extracted from the one-eighth of the csj that already had been manually tagged. the training corpus consisted of 1 speeches and the test corpus consisted of 1 speeches.
　in our experiments  we used the basic forms and pronunciations of transcriptions as the input for morphological analysis.
kaiseki kaiseki kaiseki
noun
iani ni ni
ppp
case marker
btsui tsuku tsuku
verb
ka-gyo adf
euphonic change
ite te te
ppp
conjunctive
io o o
prefix
b nitsuite nitsuite nitsuite
ppp
case marker  compound word　　　　　anterior context	before transformation	posterior context	after transformation ot
dicform
lemma
pos
conjtype
conjform
other
label
	antecedent part	consequent part

figure 1: example of transformation rule.　because the sentences in the corpus do not have boundaries between them  we selected the places in the csj that were automatically detected as pauses of 1 ms or longer and designated them as sentence boundaries. in addition to these  we used utterance boundaries as sentence boundaries. these were automatically detected at places where short pauses  between 1 and 1 ms  follow the typical sentence-ending forms of predicates such as verbs  adjectives  and copulas.
　in the csj  bunsetsu boundaries  which are the boundaries of japanese phrasal units  were manually detected. fillers and disfluencies were marked with the labels  f  and  d . in the experiments  we eliminated the fillers and disfluencies  but we did use their positional information as features. we also used as features bunsetsu boundaries and the labels  m    o    r   and  a   which were assigned to particular morphemes such as personal names and foreign words. thus  the input sentences for training and testing were character strings without fillers and disfluencies  and both boundary information and various labels were attached to them. candidate morphemes that crossed bunsetsu boundaries were ignored because morphemes do not cross them. the output was a sequence of morphemes with grammatical attributes  as shown in table 1. the candidate morphemes whose pos information correspondedto that of the correct morphemes were used as positive examples  and the others were used as negative examples. we used the pos categories in the csj as grammatical attributes. we obtained 1 major pos categories for short and long words. therefore  a in eq.  1  can be one of 1 tags from 1 to 1 for short words.
　next  we describe the features used with the morpheme models in our experiments. each feature consists of a type and a value  and it corresponds to j in the function gi j a b  in eq.  1 . as the featurefunctions  we used the combinations of features and futures that appeared three times or more in the training corpus. the features used in our experiments are basically the same as those that uchimoto et al. used uchimoto et al.  1 . the main differences are as follows:
strings following to the right: one- and two-character strings directly to the right of the target word  their character types  and the combinations of them and the dictionary information on the target word were used as features.
fine-grained categories of conjugation types and forms: conjugation types and forms were divided into finegrained categories according to the context.
　in our experiments  svm-based models were used to analyze long words because better results were obtained using svm-based models than me-based models in our preliminary experiments. a svm-based chunker  yamcha kudo and matsumoto  1   was used to assign the four chunking labels.
　we selected the following parameters for yamcha based on our preliminary experiments.
  degree of polynomial kernel: 1rd
  analysis direction: right to left
  dynamic features: two preceding chunk labels
  multi-class method: one-versus-rest
we used the following information as features of target words:
  morphological information: orthographic transcription  dictionary form  lemma  phonetic transcription  pos category  conjugation type  conjugation form  and other detailed pos information
  boundary information: bunsetsu and various labels such as  filler .
  the same information as for the target word for the four closest words  the two on the left and the two on the right of the target word.
morphological information for approximately 1% of the long words was generated by applying transformation rules.
1 results and discussion
the results of the morphological analysis obtained by using morpheme models are shown in table 1. in the table  oov indicates the out-of-vocabulary rates  which were calculated as the proportion of word and morphological information pairs that were not found in a dictionary to all pairs in the test corpus. the morphological information included orthographic transcriptions  dictionary forms  lemmas  pos categories  conjugation types  conjugation forms  and other detailed pos information. recall is the percentage of morphemes in the test corpus for which the segmentation and all morphological information were identified correctly. precision is the percentage of all morphemes identified by the system that were identified correctly. the f-measure is defined as the harmonic mean of recall and precision.
　table 1  except the bottom line  shows the results obtained when the output of a short word analysis was used as the input table 1: accuracies of morphological analysis.
wordrecallprecisionfoovshort11%11%long11%11%long11%of a long word analysis. the bottom line shows the results obtained when the long words were analyzed after correcting the errors of the output of the short word analysis. this indicates that all morphemes of the csj could be analyzed accurately if no unknown words were in the data. the accuracies obtained after detecting unknown words  examining them  and putting them into a dictionary is shown in table 1. in the initial state  1 words were unknown  and these words were of 1 types. the unknown words were detected  examined  and registered in the dictionary through the following steps.
1. examine the short words that are not found in the dictionary  put them into the dictionary  and reanalyze the corpus using the amended dictionary.
after conducting this step twice  all the words obtained in the short word analysis were found in the dictionary  and 1 words were registered in the dictionary. the first and second lines in table 1 show the accuracies of a short word analysis done after the first registration in this step  and the third and fourth lines show the accuracies after the second registration.
1. examine the short words whose probabilities are withina window size  put them into the dictionary  and reanalyze the corpus using the amended dictionary.
this step was conducted twice. the window size was set from 1 to 1 in the first examination and from 1 to 1 in the second examination. a total of 1 words were registered in the dictionary  which is approximately 1% of the initial number of unknown words. the fifth and sixth lines in table 1 show the accuracies of short word analysis done after the first registrations in this step  and seventh and eighth lines show the accuracies after the second registration.
　in the above steps  a total of 1 short words were examined  which is close to the number of unknown words in the initial state. these results show that the small number of examinations in this experiment can dramatically reduce the number of unknown words and improve the accuracy to close to that obtained without unknown words.
　table 1 shows the results obtained by applying the active sampling mentioned in section 1. here  we assumed all words were known 1. in this table  the examined word rate is the x described in section 1  and the extracted word rate table 1: accuracies obtained after unknown words were registered in dictionary.
wordrecallprecisionfnoe  ratio short11  1% long1short11  1% long1short11  1% long1short11  1% long1noe: # of examination.
table 1: accuracies obtained after active sampling.
wordrecallprecisionfext  exm short11%	 1% long1short11%	 1% long1short11%  1% long1ext: extracted word rate  exm: examined word rate.
 ext  is the percentage of words in the sentences extracted so that the percentage of examined words was x%. for example  this table shows that the accuracies of the short and long words were approximately 1 and 1 in f-measure when about 1% were examined. here  we assumed that all errors in the extracted short words were corrected. actually  the percentage of words that needed correction was close to that of the examined words. in the csj  the examined word rate  exm  was about 1%. therefore  the final accuracies of the short and long words are expected to be approximately1 and 1 in f-measure.
1 conclusion
we proposed an efficient framework for human-aided morphological annotation of a large spontaneous speech corpus such as the corpus of spontaneous japanese  csj . we demonstrated that within this framework  even when word units in a given corpus have several definitions and not all words are found in a dictionary or in a training corpus  the corpus can be morphologically analyzed with high accuracy and low labor costs by detecting words not found in the dictionary and putting them into it. we also demonstrated that labor costs can be further reduced by expanding the training corpus based on active learning. we applied this framework to the csj. the final accuracies of short and long words are expected to be approximately 1 and 1 in f-measure.
acknowledgments
the authors would like to thank prof. sadaoki furui  mr. kazuma takaoka  dr. chikashi nobata  dr. atsushi yamada  prof. satoshi sekine  dr. masaki murata  and the members of the national institute for japanese language  especially dr. masaya yamaguchi  dr. hideki ogura  mr. ken'ya nishikawa  dr. hanae koiso  and dr. kikuo maekawa  for their beneficial comments during the progress of this work. the authors also would like to thank anonymous reviewers for their helpful comments.
