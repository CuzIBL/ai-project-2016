 
we are interested in the problem of determining a course of action to achieve a desired objective in a non-deterministic environment. markov decision processes  mdps  provide a framework for representing this action selection problem  and there are a number of algorithms that learn optimal policies within this formulation. this framework has also been used to study state space abstraction  problem decomposition  and policy reuse. these techniques sacrifice optimality of their solution for improved learning speed. in this paper we examine the suboptimality of reusing policies that are solutions to subproblems. this is done within a restricted class of mdps  namely those where non-zero reward is received only upon reaching a goal state. we introduce the definition of a subproblem within this class and provide motivation for how reuse of subproblem solutions can speed up learning. the contribution of this paper is the derivation of a tight bound on the loss in optimality from this reuse. we examine a bound that is based on bellman error  which applies to all mdps  but is not tight enough to be useful. we contribute our own theoretical result that gives an empirically tight bound on this suboptimality. 
1 	introduction 
selecting a course of action in a complex environment is a difficult task. it has been examined in a number of frameworks  including classical planning  probabilistic planning  and markov decision processes. in all of these formulations it is a goal to solve difficult problems through simplifying techniques  such as finding and exploiting levels of state abstraction  hauskrecht et al.  1; parr  1   decomposing into smaller subproblems  dean and lin  1   and reusing previously discovered solutions  veloso  1; thrun and schwartz  1 . all of these techniques make locally optimal  decisions  and therefore will almost certainly find a suboptimal solution. the tradeoff is to sacrifice optimality of the solution for efficiency in finding it. 
　in this paper we examine this tradeoff in the context of markov decision processes  mdps . we assume the reader has a basic understanding of mdps  kaelbling et al  1   though we will give a brief overview emphasizing the theory that is needed for this paper. we will formalize a notion of a subproblem and what it means to use a subproblem. we will then provide examples of the use of subproblem solutions  and also the tradeoff between suboptimality and efficiency. the focus of the paper is the introduction of a quantitative bound on this loss of optimality. we will examine a bound based on bellman error  which does not give us a tight bound. we contribute our own theoretical result and show empirical results that this bound is tight using our presented examples. 
1 markov decision processes 
a markov decision process  is a tuple where 
1 is a set of states  a is a set of actions  t is a transition function and is a reward function 
                       the transition function defines a probability distribution over next states as a function of the current state and the agent's action. the reward function defines the reward received when selecting an action from the given state. the goal is to find a policy  which determines the agent's actions so as to maximize discounted future reward  with discount factor  
　　for any policy  the value of a state  is the expected discounted future reward of following the policy from that state. we say  is the optimal policy and is the value function of the optimal policy. a standard result from the theory of dynamic programming  bertsekas  1  states that the optimal value function is the unique solution to the simultaneous equations for all states   
these equations are called the bellman equations. for any value function  v  not necessarily optimal  we can define the bellman residual or bellman error as  

so the optimal value function has a bellman error of 1  and reinforcement learning techniques  in general  can be viewed as trying to minimize the bellman error. 

1 	uncertainty and probabilistic reasoning 

　　in this paper we will examine a restricted class of mdps  called goal state mdps. mdps in this class have a set of goal states. reaching a goal state ends the trial  so all actions from a goal state simply transition to the same state with zero reward. other transitions into goal states receive a reward of 1  and all other transitions receive a reward of 1. using this class we can replace the reward function with the goal set  since this uniquely defines the reward function. throughout this paper we will refer to mdps in this class as the tuple  
1 	subproblems 
before we can proceed we need to formalize the notion of a subproblem within the frameworks of our restricted class of mdps. we also need to define what it means to use a subproblem. these notions hinge on the definition of the boundary of a set of states. 
definition 1 for a set of states   let the boundary of denoted be the set of all states not in s with a non-zero 
probability transition from a state in  
in other words the boundary of  are all the states not in that are reachable from  in a single time step. 
we can now define our notion of a subproblem. 
definition 1 a subproblem  	of an mdp  	= 
this definition induces a new mdp over the states  the state transitions remain the same as in but the reward function is zero for all transitions except those entering states in where the reward is one. 
　given this notion of a subproblem we want to investigate the effects of using a subproblem solution. so we define a formal notion of using a subproblem. 
definition 1 for a subproblem  of a problem  we say that a policy  for uses s if and only if restricted to states in is optimal for the subproblem  s. additionally  the optimal policy for that uses is denoted  
1 examples 
given this definition of subproblems and how they are used  we want to examine the tradeoffs of reusing subproblem solutions. we explored this in the context of some simple stochastic maze problems  which are within our class of goal state mdps. the two problems discussed here are representative of more complex mdps. 
1 	hallway example 
figure 1 depicts a simple grid environment that was first investigated by thrun and schwartz  within the context of sharing policies. the agent has actions that move it in any of the eight grid directions  including diagonal moves   but with probability 1 its action is ignored and it is moved in a random grid direction. there is a single goal state labeled in the diagram. the agent receives a reward of 1 for a transition into the goal state  and a reward of 1 otherwise. the agent is trying to maximize its discounted future reward with a discount factor   
figure 1: hallway example. each grid position represents a 
state. 's' denotes the start state  and *g' the goal state. the shaded regions mark off two different subproblems. 
the shaded regions in the diagram denote subproblems. 
for example the subproblem of exiting the left room has the dark shaded region as its state space   the five states in the rightmost half of the doorway into the middle room are the boundary of and in this case  another subproblem is exiting both the left and middle rooms. in this case the states    would be both shaded regions  and the goal states would be the five rightmost states of the doorway to the room on the right. 
　we explored how using the solution to these subproblems might speed learning of the complete problem. we fixed the policy to always select the subproblem's optimal action while within the subproblem's state space  i.e. the shaded regions.  we then use q-leaming  watkins  1  with a decreasing learning factor in order to learn the optimal policy given this constraint. 
　the results of this learning can be seen in figure 1. when we use the subproblem of exiting the left and middle rooms  we have a huge boost in learning speed. after visiting 1 states  it performs approximately six times better than learning from scratch. notice that this sacrifices convergence to the optimal solution. the more conservative approach of using the subproblem of exiting just the left room has a more conservative learning improvement. after visiting 1 states it performs almost three times better than learning from scratch. although the improvement isn't as pronounced  it appears to converge to an optimal solution. 
1 	doorways example 
a second example that we explored consisted of a similar grid world but with multiple doorways  where  this is depicted in figure 1. the actions and transitions remain the same as was described for the hallways example. like that 
	bowling and vel1 	1 


figure 1: results from the hallway example. 	figure 1: results from the doorways example. 

example  this also has intuitive subproblems  such as reaching a subset of the three doorways. we explored this domain using a number of these subproblems. results are shown in figure 1 for learning when using the subproblem of reaching the right doorway  reaching either the right or middle doorway  and learning from scratch. 

figure 1: doorways example. 's' denotes the start state  and 'g' the goal state. the shaded region marks off the state space of the subproblem. 
　the results are very similar to the previous example. subproblems speed learning but are likely to converge on a suboptimal solution. these results also motivate our goal  to be able to make a claim about the amount of optimality sacrificed in order to gain the improved learning speed. 
1 	bounding the suboptimality 
in order to better judge this tradeoff it is important to know the suboptimality of the solution obtained. formally  we want to find a bound on the suboptimality of  this would provide us with a quantitative measure of the trade-off of using s as a subproblem. before we discuss possible bounds we need a notion of suboptimality. 
definition 1 a policy  	is a-optimal if and only if 
　using this definition  values are in the range where large values are closer to the optimal solution* a summary of the optimality for our examples  figures 1 and 1  is given in table 1  under the column labeled  actual . these values were computed by taking the minimum over all states of the ratio between the value of the state when using the subproblem and the optimal value of the state. we can quantify our goal as finding a guaranteed lower bound for these values without requiring knowledge of the optimal policy or value. 
　we will first examine a bound based on bellman error and give some empirical results on our examples that show that this bound is not satisfactorily tight. we will then introduce a new bound based on boundary values and show its results on these same examples. 
1 	bound using bellman error 
the use of the bellman error to bound the suboptimality of a 
policy was developed by williams and baird . bellman error  as mentioned earlier  is simply the maximum error in the bellman equations  and the optimal policy would have a bellman error of zero. they proved the following for all mdps  
theorem 1 for any value function  	with bellman error  let be a policy greedy for 	then for all states   
qualitatively  this means we can bound the suboptimality of a policy by examining the bellman error of a value function that induces that policy. 
　we can use this to compute a bound by simply calculating the bellman error of  we define a value function that induces and satisfies the bellman equations for states in  because we are using an optimal solution to the subproblem  and also for states outside of  because we are acting optimally outside of  then the only states with a non-zero bellman error would be on the boundary of  
results from the examples. table 1 gives the results of using this bound in the subproblems in section 1. these values were computed by finding the optimal solution using the 

1 	uncertainty and probabilistic reasoning 

example subproblem optimality actual bellman bound boundary bound hallway left and middle rooms 1 1 1 left room 1 1 1 doorways kight and middle doorway 1 1 1 ri ht doorway 1 1 1 table 1: comparison of the suboptimality bounds computed from the bellman error and from our method  against the actual suboptimality of using the subproblem. larger values signify a more optimal solution  or a better guarantee . 

subproblem  calculating the bellman error for that value function  and applying the equation. the results are quite disappointing. even for subproblems that are nearly optimal  e.g. the left and middle rooms of the hallway problem  result in a weak constraint on the suboptimality. the computed bound does not help identify when a subproblem results in a good solution. 
1 	bound using boundary values 
the following two definitions play a crucial role in our bound on suboptimality. the first provides a quantitative measure of the stochasticity of a problem. the second provides a measure for comparing values of sets of states. 
definition 1 let  be the probability that following the optimal policy at s will transition to a state  where 
 let this captures the stochasticity of the problem  where for a deterministic problem. definition 1 for a set of states and a policy 
if 	i.e. the 
values of all the states in the set are within  
　the following theorem provides a bound on the suboptimality of using a subproblem where the goal states for the subproblem are its entire boundary. 
theorem 1 given a subproblem 	of 
v  if b s'  is a-equivalent with respect to 	then  
is optimalfor  
where 	and 	 
the value may appear cryptic but it is simply the discounted probability of ever reaching a state of lower value. the value emerges from the proof which is presented in appendix a. 
　we have attempted to generalize the theorem to all subproblems with the following conjecture  
conjecture 1 given a subproblem 	let 
　theorem 1 proves the conjecture true for deterministic mdps and the factor should provide the necessary adjustment for the non-determinism as it did in theorem 1. the proof for theorem 1 is given in appendix b. 
results from the examples. table 1 gives the results of using this bound in our example subproblems. the values were computed much like those for the bellman error; an optimal solution was learned using the subproblem  the values were calculated  and the bound computed. the results show a reasonably tight bound  especially for the single exit subproblems. thought it's not as tight on the multiple exit subproblems it is still much improved over the bound computed from bellman error. 
　this bound provides considerable information as to the loss in optimality of using a subproblem. in addition to this guarantee on the resulting policy it also helps to identify a possibly better subproblem. this is done by examining the  values. for example  in the doorways problem using the right doorway  the guaranteed optimality is fairly weak. but  by examining the  values it becomes obvious that this bound could be improved if the middle door was in the subproblem's goal states. by using the  new  subproblem we achieve a better policy and a better bound on its suboptimality. 
1 related work 
the definitive performance bound for general mdps is the one based on bellman error  williams and baird  1 . it is provably tight for the general case of any value function of any mdp. although  as seen earlier  it is not as strong at bounding subproblem use in our restricted class of mdps. 
　there has been a great deal of recent work in finding and exploiting layers of abstraction in mdps. this work is very related to subproblem reuse since we can view the operators of a high layer of abstraction as subproblem solutions of the lower layers. therefore  theoretical bounds on the suboptimality of subproblem reuse can also help to understand the suboptimality of using abstractions. 
　sutton and his colleagues provided a theoretical framework for dealing with abstraction in reinforcement learning  through a multi-time model  sutton et al.  1 . this framework defines temporally abstract options or macro-actions  which are local policies that act like subroutines. options  unlike primitive actions  can control the agent over multiple time steps  and so can work at different levels of abstraction. the theoretical model of options allows them to be treated as actions in the mdp  and can be used to possibly speed planning and still find an optimal solution. 
　hauskrecht  et al.  extend sutton's original theoretical results on planning with macro-actions to develop a hierarchical model of mdps that reduces the size of the state and action space. this is done by creating a state partition of the mdp and restricting a macro to be a local policy over one 
	bowling and veloso 	1 

of these regions. this partition and associated macros defines* a new abstract mdp whose states are the boundaries of the regions  and whose actions are the macros that move between regions. the optimal solution to this abstract mdp can then be used as a solution to the original mdp. 
　to make some guarantees on the quality of such a solution  a macro set is required that  covers  the range of values of a region's exit states. such a macro set limits the resulting bellman error and makes use of the bellman bound to guarantee some amount of optimality. unfortunately  construction of such a macro set will often be intractable. the number of required macros grows exponentially in the number of exit states for the region and in the desired bound on suboptimality. 
　parr  developed a number of improved methods for constructing macro sets  or policy caches  for an abstract mdp. he presents two algorithms that give optimality guarantees on the original mdp  which are not exponential in neither the error bound nor the size of the region's boundary. these techniques still use the bellman bound as the measure of suboptimality  and still can create a much larger macro set than is actually required. 
1 	conclusion 
in this paper we examined the reuse of subproblem solutions in the context of goal state mdps. we gave a formal definition for a subproblem  and what it means to use subproblems in this context. we also provided representative examples of the tradeoff between learning speed and loss in optimality when reusing subproblems. in order to quantify this tradeoff we examined quantitative bounds on the suboptimality of these techniques. although generally applicable  a bound based on the bellman error is often not tight enough to be useful. we contributed the derivation of an empirically tight bound that applies specifically to subproblems in our restricted class of mdps. 
　this work opens further directions to investigate  including relaxing the restriction on the applicable class of mdps  examining the effects of using multiple subproblems  and investigating if our bound may help to retrieve the most useful subproblems. 
a 	proof of theorem 1 
proof  the proof makes use of a special non-stationary policy  this policy initially selects the same action as for states in  and the same action as for states not in 
　　after reaching a state in the policy changes so that it always selects the same action as notice that 
	this is because 	uses  and 
non-stationary policies cannot do better than an optimal stationary one. 
we will first show that  is from this we will be able to show that for we can write  
we can then split the sum into the paths containing a state in and those that don't contain a state in  let  be the probability a path from following the optimal policy  contains a state in then  
notice that 
any path that contains a state in must also contain a state in so we can write the second term as a value of that state discounted by the length of the path before the state is be the number of steps in path until 
and pull it outside of the sum. the resulting sum is just the 

we can also achieve a similar result for since our non-stationary policy   acts the same as for $ then it will achieve the same value for paths not containing 
 as before  paths containing a state in must contain a state will select actions 
it outside of the sum. the resulting sum is the expected discount between sm and s when following let this value be  so we get  

since is optimal over the subproblem  it will reach a state in with no worse expected discount than hence  
and we can substitute for  and maintain the inequality. 
now we examine the ratio of the two values  


1 	uncertainty and probabilistic reasoning 

the value  is the probability of returning to a state in   discounted by the distance to that state and the distance from that state to a state in since all states in  have a smaller value than we can bound this by the discounted probability of ever reaching a state with smaller value  which 

we can now show that for any state  	we 
can decompose the value of s just as was done for let be the probability that a path from s  following   contains a 
state in  notice that if  t h e n w e letand   represent the same values with respect to s. then we can 
bound 
hence o 
b 	proof of theorem 1 
proof. since  is deterministic then  = 1 and  = 1. 
also  for any state  s  and a policy there is a unique path of states to a state in  that follows that policy. trivially  if the optimal path  path of states from following the optimal policy  from s does not contain a state in  then  = 

consider the case where the optimal path from s contains a state in  then it must contain a state in let be the last state in the optimal path where so the optimal path from cannot contain a state in   and therefore  
now  there are two cases: 
1. the path from following will reach a state in fewer steps  because is optimal at reaching 
	goal states of the subproblem.  since 	then 
and since g' is reached in fewer steps  

1. 
so  
