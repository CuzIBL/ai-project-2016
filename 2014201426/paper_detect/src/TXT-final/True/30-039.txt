 
the creation of a complex web site is a thorny problem in. user interface design. in ijcai '1  we challenged the ai community to address this problem by creating adaptive web sites. in response  we investigate the problem of index page synthesis - the automatic creation of pages that facilitate a visitor's navigation of a web site. previous work has employed statistical methods to generate candidate index pages that are of limited value because they do not correspond to concepts or topics that are intuitive to people. in this paper we formalize index page synthesis as a conceptual clustering problem and introduce a novel approach which we call conceptual cluster mining: we search for a small number of cohesive clusters that correspond to concepts in a given concept description language l. 
next  we present sgml  an algorithm schema that combines a statistical clustering algorithm with a concept learning algorithm. the clustering algorithm is used to generate seed clusters  and the concept learning algorithm to describe these seed clusters using expressions in l. finally  we offer preliminary experimental evidence that instantiations of sgml outperform existing algorithms  e.g.  cobweb  in this domain. 
1 	introduction 
designing a complex web site so that it readily yields its information is tricky first  different visitors have distinct goals. second  the same visitor may seek different information at different times. third  many sites outgrow their original design  accumulating links and pages in unlikely places. fourth  a site may be designed for a particular kind of use  but may be used in many different ways in practice; the designer's a priori expectations may be violated. too often web site designs are fossils cast in html  while web navigation is dynamic  timedependent  and idiosyncratic. 
　in  perkowitz and etzioni  1   we challenged the ai community to address this problem by creating adaptive web sites: sites that automatically improve their 
1 	challenge papers 
organization and presentation by learning from visitor access patterns. many ai advances  both practical and theoretical  have come about in response to such challenges. the quest to build a chess-playing computer  for example  has led to many advances in search techniques  e.g.   anantharaman et a/.  1  . similarly  the autonomous land vehicle project at cmu  thorpe  1  resulted not only in a highway-cruising vehicle but also in breakthroughs in vision  robotics  and neural networks. we believe that the adaptive web sites challenge will also drive ai advances. 
　much of the previous work on adaptive web sites has focused on fairly simple adaptations  e.g.  automatically creating shortcuts in the site  and on customization - personalizing  a web site to suit the needs of each individual user. in contrast  our own work has been motivated by two goals: first  we seek to demonstrate that relatively sophisticated adaptations can be generated; second  we seek to aggregate information gleaned from a population of users to transform the site - altering it to make navigation easier for a large number of users. these goals have led us to investigate the problem of index page synthesis: the automatic creation of navigational pages that consist of a comprehensive set of links to pages at the site on a particular topic  e.g.  an index page on  electric guitars  at a musical instruments web site . 
　in  perkowitz and etzioni  1   we formally defined the index page synthesis problem and presented an algorithm called pagegather for discovering candidate link sets which would form the basis for new index pages  based on visitor access logs. in that paper  we compared pagegather to classical clustering algorithms  voorhees  
1; rasmussen  1; willet  1  and to apriori  the classical data mining algorithm for the discovery of frequent sets  agrawal and srikant  1  using the  cohesiveness  of the link sets generated as the basis for comparison  see section 1 for a precise definition of our measure . we found that pagegather produced substantially better candidates in our domain. surprisingly  we also found that pagegather's candidates were better than human-authored index pages available at our experimental test site. 
　more detailed examination of the data reveals the reason for pagegather's  super-human  performance: human index page authors operate under a constraint that 

given: 
1. a data collection d  e.g.  a set of pages at a web site . 
1. a pairwise similarity measure m defined over d  e.g.  page co-occurrence frequencies derived from access logs . 
1. a conceptual language l for describing elements of d  e.g.  conjunctions of descriptive features . 
1. a description in l of each object in d. 
output all subsets c of d such that 
1. c is highly cohesive with respect to m  e.g.  the average pairwise similarity of objects in c exceeds some threshold  
1. c corresponds to a concept describable in l. 
figure 1: the conceptual cluster mining problem. given a data collection  a similarity measure  and conceptual descriptions of data objects  find sets of objects that are both similar and conceptually related. 

pagegather ignored for successful navigation  index pages typically correspond to a topic or concept that is intuitive to visitors; they cannot be composed of a cohesive  but arbitrary  set of links. for example  if the topic of the index page is  electric guitars   the page should contain no links to information about pianos or drums  and there should be no local pages about electric guitars that are not linked to from the index page. 
　pagegather relies on a statistical approach to discovering candidate link sets; its candidates do not correspond precisely to intuitive concepts  whereas humanauthored index pages do. in this paper  we present a key extension to pagegather that guarantees that pagegather will generate only link sets that correspond to topics  given a language for describing topics. our extension also constitutes a novel approach to the longstanding problem of conceptual clustering  michalski and stepp  1 . 
　this paper is organized as follows. we first define the conceptual cluster mining problem and discuss previous work. next  we present the sgml algorithm schema and discuss several instantiations. we experimentally evaluate instantiations of sgml and compare them to the cobweb conceptual clustering algorithm and to human-authored clusters. we then discuss ways to automatically generate the object descriptions that conceptual clustering algorithms take as input. we conclude with a discussion of contributions and future work. 
1 	conceptual cluster m i n i n g 
in this section  we define the conceptual cluster mining problem  describe previous work  and present our own approach. 
1 	problem definition 
in  perkowitz and etzioni  1   we presented a novel approach to clustering called cluster mining: instead of attempting to partition the entire data space into disjoint clusters  we search for a small number of cohesive  and possibly overlapping  clusters. in that paper we snowed that  in our domain  cluster mining outperformed traditional clustering algorithms. 
　in this paper we introduce conceptual cluster mining: given a collection of objects  a pairwise similarity measure over those objects  and a conceptual language for describing them  we search for a small set of cohesive clusters that correspond to concepts expressible in the language. we formalize this problem in figure 1. 
1 	previous work 
relevant previous work is of two types: statistical approaches to clustering and data mining  and conceptual clustering. statistical clustering  see  voorhees  1; rasmussen  1; willet  1   is a technique for partitioning a set of objects in a multidimensional data space into  clusters  of objects that are close in that space. cluster mining is a statistical technique for finding only cohesive clusters in a data space  instead of partitioning the entire space. pagegather  see  perkowitz and etzioni  1   is a cluster mining algorithm which has been applied to the problem of synthesizing web pages. frequent set algorithms are designed to find sets of similar items in large collections  see  agrawal and srikant  1; savasere et a/.  1; toivonen  1  . in a traditional frequent set problem  the data is a collection of market basket information. each basket contains a set of items  and the goal is to find sets of items that appear together in many baskets. the standard frequent set algorithm is a priori  agrawal and srikant  1 . all of these statistical approaches are useful for finding cohesive sets of objects in large collections of data  but make no attempt to ensure that their results correspond to a intuitive concept. 
   conceptual clustering algorithms  see  michalski and stepp  1    like their statistical counterparts  partition a data collection into clusters of similar objects. in a conceptual clustering problem  however  objects are described in a conceptual description language  and each cluster corresponds to a concept expressible in that language. these conceptual languages may be simple con-
junctions of attributes as in  michalski and stepp  1   probabilistic descriptions as in  fisher  1   or complex and cognitively inspired as in  hanson and bauer  1 . all of these are clustering rather than cluster mining algorithms; they attempt to find the best partition of the entire space rather than the best concepts. for our problem  we require an algorithm that can discover a small set of clusters that are both cohesive and  conceptual . 
1 	the scml algorithm schema 
the naive algorithm for the conceptual cluster mining problem might look like this: 
	perkowitz and etzioni 	1 

the sgml algorithm schema 
1. run statistical clustering algorithm  on the data collection d  producing a set c of clusters. 
1. for each cluster c in c 
 a  tag the objects in c as positive examples. 
 b  tag remaining objects  d - c  as negative examples. 
 c  use these as input to concept learning algo-rithm r. 
 d  find the concept v - 
 e  find the set of objects cv which is the extension of v. 
1. return all the sets  found. 
figure 1: the sgml algorithm schema  which is instantiated with a statistical cluster mining algorithm and a. concept learning algorithm. 
1. enumerate all concepts / expressible in l 
1. for each /  compute /'s cohesiveness with respect to the similarity measure m. 
1. return all / with a cohesiveness score over a certain threshold. 
of course  the number of expressions in l could be quite large. for example  when l contains conjunctions of attributes  the number of expressions in l is exponential in the number of attributes  making this algorithm impractical for real applications. how can we devise a tractable algorithm for this problem  
　our previous work has presented cluster mining  a tractable technique for finding cohesive statistical clusters; if we could efficiently transform these clusters into conceptual ones  we would have a tractable algorithm. in essence  we would like to find the conceptual cluster that best approximates each statistical one. our key insight was that the members of a  statistical cluster can be viewed as positive examples of the desired concept  and objects outside the cluster as negative examples  enabling us to apply concept learning techniques to efficiently generate the desired approximations. 
　we therefore propose the sgml algorithm schema  figure 1   which uses statistical cluster mining to find cohesive clusters and concept learning to find a concept that describes each statistical cluster. as the match between statistical and conceptual clusters will be imperfect  we will require a noise-tolerant learning algorithm. the output of this schema is a set of clusters  and their conceptual descriptions . this approach is not guaranteed to find the optimal set of conceptual clusters  but it is tractable and it enables us to leverage continuing advances in concept learning research. 
1 	scml and index page synthesis 
we have presented the general conceptual cluster mining problem. in this section we frame the index page synthesis problem as conceptual cluster mining. page synthesis is the automatic creation of web pages. an index page is a page consisting of links to a set of pages that 
1 	challenge papers 
cover a particular topic  e.g.  electric guitars . given this terminology we define the index page synthesis problem: given a web site and a visitor access log  create new index pages that contain coherent collections of links: ones that exhaustively cover specific topics at the site. we are not provided with a pre-selected set of topics. possible topics are instead described in a conceptual language  and the exponential number of expressible topics makes enumerating them impractical. instead  we must search the space of possible concepts. an access log contains one entry for each page requested of the web server. each request lists at least the origin  ip address  of the request  the url requested  and the time of the request. 
　previously  our approach to this problem has been to use statistical cluster mining  embodied in pagegather  our statistical page synthesis algorithm. briefly  pagegather creates a graph in which each node represents a page at the web site. the algorithm then processes the access logs to find pages that are often visited together by the site's users; such pages are connected by an edge in the graph. finally  the algorithm finds maximal cliques in the graph and outputs those sets of pages.1 
　we instantiate the scml schema using pagegather for  . we use three different algorithms for t. as a baseline  we wished to use a simple algorithm that would perform a greedy search from general to specific  constructing a conjunctive rule  as this approach seems welltailored to our problem. accordingly  we based our algorithm on greedy-1  pagallo and haussler  1 . the algorithm iteratively builds a conjunctive rule  choosing a conjunct that maximizes the scoring function at each iteration. 
　we use two scoring functions  producing two instantiations of scml. as positive examples in our domain are typically outnumbered by negatives  the first scoring function - called  - is a simple count of positives  with negatives used to break ties ; we use a maximum rule length to limit the complexity of rules. 
the second  is based on minimum description length  mdl   quinlan and rivest  1 . as the mdl measure includes a tradeoff between the complexity of a rule  its length in conjuncts  and its effectiveness  we do not need an explicit maximum length; the simple measure provides no such tradeoff. 
the third algorithm is  which uses ripperk 
 cohen  1  for the concept learning algorithm t ripperk: is a rule-learning algorithm designed to work well with large  noisy datasets and is competitive with the classic concept learning systems c1 and c1rules in both speed and performance. in section 1  we present results from 
1 	experiments 
in this section  we evaluate the scml algorithm schema  comparing it to pagegather  cobweb  a popular conceptual clustering algorithm   and clusters derived from human-authored pages at a web site. furthermore  we compare several instances of the scml algorithm schema. 
1
　　 the cluster size is bounded to ensure a polynomial-time algorithm. 


figure 1: 	the performance of various instantiations of 
 sgml compared with pagegather  clusters found by pagegather have substantially higher visit percentages  but do not correspond to expressible concepts . clusters found by 
　　　　　  show substantially higher average visit percentages than 

figure 1: pgrip compared with the popular conceptual clustering algorithm cobweb and clusters corresponding to human-authored pages at the web site. the best clusters found by pgrip are better to those found by cobweb; otherwise  the two algorithms are comparable. both score higher than the pre-existing clusters. 

   our experiments draw on data collected from the m u sic machines web site 1 a site devoted to information about many kinds of electronic musical instruments. the site contains approximately 1 distinct documents  including h t m l pages  plain text  images  and audio samples. music machines receives approximately 1 hits per day from roughly 1 distinct visitors. in our experiments  the training data is a collection of access logs for six months; the test data is a set of logs from a subsequent one-month period.1 our conceptual language l consists of conjunctions of attributes describing pages and musical instruments  e.g.  type of instrument  price  and type of file; see figure 1 for examples . 
　　we compare the algorithms in terms of the quality of the candidate index pages they produce. measuring cluster quality is a notoriously difficult problem. to measure the quality of a cluster as an index page candidate  we need some measure of whether the cluster captures a set of pages that are viewed by users in the same visit. if so  then grouping them together on an index page would save the user the trouble of traversing the site to find them. as an approximate measure we ask: if a user visits any one page in the cluster  what percentage of pages in the cluster is she likely to visit overall  more formally  let v be the set of user visits to the site and vc be the set of v that include at least one page from cluster c. for a particular visit the set of pages visited in r is . the average number of hits to a cluster  over 
each experiment  each algorithm outputs a ranked list of clusters. in all graphs  these clusters are sorted by average visit percentage for the sake of clarity. 
first  we compare instantiations of scml. in section 
1  we described three different learning algorithms to plug into the scml schema:  1  ripperk  pgrip ; 
 1  a greedy algorithm with a scoring function based on 
 http://machines.hyperreal.org * data sets are available from the authors. positive examples  pgpos '  and  1  the greedy algorithm with an mdl-based scoring function  pgmdl figure 1 compares these three variations. we see that 
pghip performs substantially better than either of the other algorithms. all of pgrip's clusters are better than all but one of those found by the other algorithms. 
　in figure 1  we also compare the pagegather algorithm with scml. we see in this graph that the clusters found by pagegather are substantially more cohesive than those found by scml. the best pagegather clusters have an average visit percentage of over 1%  while scml tops out below 1%. pagegather  in essence  finds the most cohesive clusters in the data  regardless of their conceptual coherence; scml must respect the conceptual constraint and so cannot necessarily return the most cohesive clusters. although pagegather produces more cohesive clusters  we feel that conceptual coherence is essential; coherence makes the pages easier to name automatically  easier to evaluate by the human webmaster  and easier to navigate by site visitors. 
   in figure 1 we compare scml with both cobweb and human-authored clusters. c o b w e b is a popular conceptual clustering algorithm  first presented in  fisher  1 . c o b w e b builds a hierarchical partition of the object space  using probabilistic representations of clusters. typically  the top level of the hierarchy is the basic level  or most appropriate level at which to parti-
tion the space. however  since our domain dictates that the clusters be in a certain size range  it is impractical to create a web page containing hundreds of links   we choose the level of the hierarchy at which the average cluster size is in the right range typically 1. the best clusters found by scml are substantially better - with average visit percentages at around 1% - than any of those found by cobweb. otherwise  both algorithms find clusters with visit percentages around 1%. 
　it is natural to ask how good our clusters really are  and how high of an average visit percentage is high enough. to attempt to answer this question we look to the  ideal  case: index pages created by a human webmaster. music machines contains many index pages 
	perkowitz and etzi1ni 	1 


figure 1: conceptual descriptions of the top five clusters found by pgrjp and cobweb. 

of different kinds. expecting all of the site's index pages to score substantially better than pagegather's output  we chose index pages pertaining to four representative topics:  1  instruments produced by a particular manufacturer  roland ;  1  documents pertaining to a particular instrument  the roland juno keyboard ;  1  all instruments of a particular type  drum machines ; and  1  all files of a particular type  audio samples . the set of outgoing links on each page  excluding standard navigation links  was treated as a  cluster  and evaluated on our test data. figure 1 shows the comparison of these clusters to the output of pgrip. although pgrip's clusters show higher average visit percentages than the human-authored pages  the human-authored pages tend to be substantially larger  often leading to lower percentage scores. if we instead compare the average number of hits to each cluster  the human-authored pages perform at least as well as pg rip's. 
　visit percentage measures only the statistical cohesiveness of our clusters with respect to the data. of course  as demanded by our problem definition  we also require that these clusters correspond to intuitive concepts. excepting the original pagegather algorithm  all the algorithms we have discussed produce conjunctive concepts in l. keeping in mind that we wish to find concepts that we can express to users of the web site  these concepts should be fairly simple - i.e. without too many conjuncts. figure 1 shows the top five concepts found by both pgrip and cobweb. although both algorithms produce clusters of a reasonable size  cobweb's concepts tend to be more complex. 
1 	automatically derived conceptual descriptions 
we have presented two general algorithms for finding clusters of related pages at a web site: pagegather and scml. pagegather requires no input beyond the web site and access log  but cannot produce conceptually coherent clusters. scml  on the other hand  produces conceptual clusters but carries the strong requirement that all documents at the site be fully described in a conceptual language. we now explore the question of whether it is possible to gain some of the advantages of 
1 	challenge papers 
conceptual clustering without paying the price of generating full meta-information. 
　our approach is to derive meta-information from the structure of the web site or the content of the pages themselves  which may be done completely automatically or with some assistance from the webmaster. we describe two such approaches and present preliminary experimental results. both approaches are based upon defining a different conceptual language in which to describe web pages. 
　the first approach is based on file types. rather than describing every document at the site with a set of attributes  as was done for scml   we tag each page with its filetype. filetypes are derived solely from the file suffix; common suffixes include jpg  gif  zip  html  txt  and so on. each statistical cluster found by page-
gather is partitioned by filetype; small partitions are discarded. the remaining partitions are our final clusters. note that we only guarantee that all pages in a cluster are of the same type; we do not add all other pages of that type to the cluster as scml would. therefore  this approach can find concepts that scml could not. we have implemented this algorithm - pgtype - and tested it on our data. 
　the second approach is based on content-based clustering. pagegather clusters pages based on visitor ac-
cess patterns; perhaps by clustering them based on their content  we will discover clusters that are more conceptually coherent. we apply a clustering algorithm to the pages at the site to obtain a set of content-based clusters. next  for each cluster output by pagegather  we find the closest content-based cluster - the one with the highest overlap. these closest clusters are our final output. note that these concepts are defined with respect to keywords rather than more intuitive concepts. keywords may indicate conceptual similarity  but are not perfect due to synonyms and homonyms. we have implemented this algorithm using suffix-tree clustering  stc   zamir and etzioni  1  and tested pgstc on our data. 
　in figure 1  we compare pgrip with pgtype and pgstc. clusters found by pgtype have the high-
est average visit percentages; this is not surprising  as p g t y p e ' s output is closest to pagegather clusters. 


percentages. 
1 	conclusion and future work 
this paper has responded to the broad adaptive web 
sites challenge task by presenting and evaluating a method for index page synthesis. more precisely  the method solves the problem of discovering coherent and cohesive link sets  which can be presented to a human webmaster as candidate index pages. while our method is based on previous work  this paper introduces and evaluates a key extension which ensures that the candidate index pages generated correspond to an intuitive concept  which makes the pages easier to name automatically  easier to evaluate by the human webmaster  and easier to navigate by site visitors. we have also explored two ways of relaxing the requirement of full meta-information by deriving information from the site's structure and content. 
   the challenge has also led us to formulate the domainindependent problem of conceptual cluster mining  figure 1   and an algorithm schema for its solution. we have instantiated sgml with various concept learning algorithms and conceptual languages  and compared these instantiations experimentally. we have shown that pgrip outperforms c o b w e b  a leading conceptual clustering algorithm  in our domain. 
　in future work  we plan to investigate alternative measures of index page quality and also to carry out user studies  with both web site visitors and webmasters  to assess the impact of the suggested adaptations on users in practice. in addition  we plan to test our approach on additional web sites  including our department's web site  in the near future. finally  index page synthesis itself is a step towards the long-term goal of change in view: adaptive sites that automatically suggest re- organizations of their contents based on visitor access patterns. 
