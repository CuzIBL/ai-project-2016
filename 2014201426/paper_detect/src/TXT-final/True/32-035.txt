 
an important problem in clustering is how to decide what is the best set of clusters for a given data set  in terms of both the number of clusters and the membership of those clusters. in this paper we develop four criteria for measuring the quality of different sets of clusters. these criteria are designed so that different criteria prefer cluster sets that generalise at different levels of granularity. we evaluate the suitability of these criteria for non-hierarchical clustering of the results returned by a search engine. we also compare the number of clusters chosen by these criteria with the number of clusters chosen by a group of human subjects. our results demonstrate that our criteria match the variability exhibited by human subjects  indicating there is no single perfect criterion. instead  it is necessary to select the correct criterion to match a human subject's generalisation needs. 
1 introduction 
an important problem in clustering is how to decide what is the best set of clusters for a given data set  in terms of both the number of clusters and the membership of those clusters. in this paper we present an empirical evaluation of four criteria for measuring the quality of different sets of clusters. in particular  we demonstrate how these criteria can be applied to non-hierarchical clustering of search engine results. 
　the focus of our work is the problem of document clustering  which is a major application of clustering techniques. while the feasibility of document clustering has been demonstrated since the 1's  salton 1   in recent years there has been a growth in applications for this technology. example applications include personal document management  rus and de sands 1   sharing information between a community of users  davies et al 1   and clustering search results  leouski and croft 1%  zamir and etzioni 1 . in all of these applications  it is not known a priori how many clusters exist in the document set. one way around this problem is to find a fixed number of clusters. however  this imposes artificial constraints on the search for structure in a document set. a different approach is to search for alternative sets of clusters  and apply a quality criterion to decide which is the best partition of a document set. 
　the three main contributions of our work are as follows. we have developed four criteria for determining the quality of clustering. we have evaluated the sensitivity of'these criteria in a practical document clustering application  i.e.  clustering the results returned by a search engine. we have compared these results with the number of clusters chosen by a group of 1 human subjects. we begin in the next section by describing the problem of document clustering in more detail. in section 1  we describe the four criteria we have developed for evaluating the quality of clusters. we then provide the results of an empirical evaluation of our criteria in sections 1 and 1. finally  in section 1 we compare our approach to related work in the area of clustering. 
1 document clustering 
the context of our work is the development of tools for clustering the results returned by search engines on the internet. when a search engine is given a query  e.g.  'agents*  it responds with a set of search results ri. each search result is a short description of a web page that matches the query. in practice  a large number of results are returned  and the user is faced with the daunting task of filtering out irrelevant results. these irrelevant results arise because the query terms can appear in many different contexts  e.g.  travel agents or intelligent agents. even within a single context there can be multiple sub-topics  e.g.  intelligent agent software or intelligent agent conferences. consequently  we are interested in automatically clustering related search results so users can easily explore the underlying topics that match their query. 
　in order to solve this problem we have followed the standard model for document clustering as developed by salton  1 . this model has three main features. first  each document is represented by a vector of word frequencies  where commonly occurring words have been excluded using a stop list or heuristic feature selection techniques. second  a distance measure is defined as a function of these document vectors  so that we can quantify the similarity or distance between any pair of documents in the vector space. finally  a clustering algorithm uses this distance measure to group related documents into clusters. 
　the clustering algorithm groups the search results ri into a set of clusters cj it is important that clusters can be quickly generated  and easily scanned by the user. consequendy  we have used a non-hierarchical  single-pass clus-
	raskutti and leckie 	1 
　
tering algorithm  rasmussen 1 . we use this clustering algorithm to assign each result rj to a single cluster  so there is no overlap between clusters. therefore  the clusters form a partition of the document space  which we refer to as p. in principle  a result could be assigned to multiple clusters. however  this can make it harder to characterise and differentiate clusters. 
　the clustering algorithm proceeds as follows. the first search result r1 is used to initialise the first cluster c1. for each of the remaining results ri  we need to assign ri. to the nearest cluster  or start a new cluster if none is sufficiently close. in order to compare a result ri to a cluster cj  we represent cj by its centroid. the centroid of cj is the average of the word frequency vectors corresponding to the results that have already been assigned to cj. we can then calculate the distance between ri and the centroid of each class cj. if the distance to the closest cluster centroid is less than a threshold t  then we assign ri to that cluster and update its centroid. otherwise  all clusters are a distance greater than t from the search result  so we create a new cluster using ri. 
　using this algorithm we can generate different partitions by varying the threshold t large threshold values will result in a small number of general clusters  while small threshold values produce a larger number of more specific classes. consequently  we can explore a range of different partitions by stepping through different values of t. this raises the question of which is the best partition  i.e.  how do we judge whether one partition better reflects the inherent similarities of the search results than another  
1 four quality criteria for clustering 
in order to determine the quality of a partition  we have defined criteria that evaluate a partition with respect to the following measures described in dubes and jain  1 : 
  compactness - this is a measure of cohesion or uniqueness of objects in an individual cluster with respect to the other objects outside the cluster  e.g.  the average similarity of objects within the cluster. the greater the similarity  the greater the compactness. 
  isolation - this is a measure of distinctiveness or separation between a cluster and the rest of the world  e.g.  highest similarity to an object outside the cluster. the smaller the similarity  the greater the isolation. 
　ideally  we need to generate partitions that have compact  well-separated clusters. hence  our criteria combine the two measures to return a value that indicates the quality of the partition. the value returned is minimised when the partition is judged to consist of compact well-separated clusters  with different criteria judging different partitions as the best one. in each of our criteria  we have used a simple similarity/ distance based measure to evaluate compactness and isolation  rather than statistical tests of significance used in multivariate analysis of variance. this is done both for computational efficiency and due to the inadequacy of manova teste to provide quantitative measures of cluster validity  dubes and jain 1 . 
　in our discussion of each of the criteria  ri represents an object or search result  cj represents a cluster and cjc its centroid. gc is the global centroid.  is the similarity 1 natural language processing 
between two search results ri and rk where ri and rk are represented by their frequency vectors  and their similarity is calculated using the cosine coefficient  rasmussen 1 . 
represents the distance or dissimilarity measure and 
is calculated as 1 - 
1 minimum total distance  ci  
in mis criterion  we minimize the total of the sum of distances of objects to their cluster centroids and the sum of the distances of the cluster centroids from the global centroid. the value for each partition is computed as follows: 

　the first term is the intra-cluster distance  solid lines in figure 1  and represents the compactness of clusters. it is small when the objects are close to their cluster centroids  e.g.  when there are a few compact clusters  figure la  and increases as the number of clusters decreases and the clusters are spread out  figure lb . the maximum value is reached when the number of clusters is 1. 
　the second term is the inter-cluster distance  dashed lines in figures la and lb  and represents the isolation. it is small when there are a few large clusters  and increases to its maximum value when there is one document per cluster. 
　hence  when the threshold t is small and there is one document per cluster  the null hypothesis   the total of the two terms is simply the second term  i.e.  the sum of the distances of the objects from the global centroid. if the data set is nonrandom  the total then reduces as new clusters are formed. as clusters get large and diverse  the first term becomes larger  and the total of the two terms increases until it reaches the maximum value when there is only one cluster. the second term is then 1 and the first term is the sum of the distances of the objects from the global centroid. thus  the two boundary conditions of the null hypothesis and a single cluster return the same value. 
　in general  this criterion prefers partitions with small specific clusters that are far from the global centroid. this is because several large inter-cluster distances are then replaced by a single large inter-cluster distance and several small intra-cluster distances. when clusters are more general  the intra-cluster distances get larger  thus overwhelming the advantage of the single inter-cluster distance. 
1 separated clusters  c1  
in this criterion  we measure cluster quality by maximizing the separation of clusters relative to their compactness. compactness is computed by determining the weakest connection within the cluster  i.e.  the largest distance between two objects and within the cluster  solid lines in figure 1 . the more compact a cluster  the smaller the distance. isolation is computed by determining the strongest connection of a cluster to another cluster  i.e.  the smallest distance between a cluster centroid and another cluster centroid  dashed lines in figure 1 . the more distinct a cluster the larger die distance. we compute this criterion as follows: 

　
　this value is defined only when there is at least one cluster with more than one object  and only when there is more than one cluster in the partition. as such  this index is not defined for the boundary conditions of the null hypothesis and a single cluster. 
　as shown in figures 1a and 1b  both the numerator and denominator for each cluster increase as the threshold t increases. however  the relative change depends on the compactness and isolation of the clusters. in addition  there are more terms as the number of clusters increases. hence  this criterion is likely to be minimised when there are larger number of more specific clusters. 
1 object positioning  c1  
in this criterion  the quality of clustering is determined by the extent to which each object has been correctly positioned or classified. to compute this  for each object ri we compute the weakest connection within the cluster  i.e.  the largest distance between objects ri and rk within the cluster  solid lines in figure 1 . in addition  we compute the strongest connection of this object to the outside world  i.e.  the smallest distance between objects ri  and rm where rm belongs to a different cluster  dashed lines in figure 1 . the extent to which the object is incorrectly positioned is given by the difference between its weakest internal connection and strongest external connection. hence this criterion is given by the following equation: 

　for the null hypothesis  the internal connection for each object is 1. however  the closest external object may be very close  so the criterion does not necessarily have the smallest value. when there is only one cluster  there are no external connections  and the index attains its maximum value. 
　figures 1a and 1b show the connections for two objects when there are four clusters  figure 1a  and when there are two clusters  figure 1b . as shown pictorially  both distances increase as the number of clusters decreases. however  the increase in the second term is often larger when there are fewer clusters. hence  this criterion prefers large general groupings to small specific groupings. 
1 number of objects correctly positioned  c1  in this criterion  the quality of clustering is determined by the number of objects that have been correctly positioned or classified. the more objects that are correctly positioned  the better the quality of clustering. an object ri belonging to cluster cj is correctly positioned if its intra-cluster similarity  i.e.  average similarity to other objects in the cluster  is greater than the inter-cluster similarity  i.e.  average similarity to objects outside the cluster. intra-cluster similarity is computed using the following equation: 

the inter-cluster similarity is computed as follows: 

　for singleton clusters  i.e.  clusters with one element  the intra-cluster similarity is 1  hence  the object is always incor-
	raskutti and leck1e 	1 
　

 figure 1: normalised criteria values for different partition sizes on test query results  with a histogram of the partition sizes created by human subjects  each marked as an x  
ci - minimum total distance c1 - separated clusters c1 - object positioning c1 - no. of objects correctly positioned 

figure 1: histogram of the number of non-singleton clusters for each human subject  and top 1 partitions by each criterion o - minimum total distance  ci  o - separated clusters  c1  o- object positioning  c1  
a - number of objects correctly positioned  c1  x - human subject 
　
rectly positioned. thus  for the null hypothesis  no object is correctly positioned. when there is exactly one cluster  the inter-cluster similarity is 1. hence  all objects are perfectly positioned. thus  unlike the minimum total distances measure  this criterion prefers one boundary condition to the other. in other partitions  the number of well-positioned objects depends on each object's intracluster and inter-cluster similarity. however  since objects in singleton clusters are always counted as incorrectly positioned  this criterion prefers large clusters rather than a few compact clusters and some singleton clusters. 
1 evaluation 
our first goal was to evaluate the sensitivity of each criterion  and the spread between different criteria  on alternative partitions of the same set of search results. to study sensitivity  we consider whether one partition is clearly better than the rest  or whether there are several partitions that are all close to optimal. to study spread  we consider the extent to which different criteria agree in their choice of the optimum partition. our second goal was to compare how a group of human subjects cluster the same set of search results  in order to determine whether they exhibit a similar spread in their choice of the optimum partition. 
　our test data was generated by issuing three queries to the altavista search engine  http://www.altavista.com . 
1 	natural language processing 
these queries were 'agents'  'chips' and 'telstra'. the first two query terms are used in many different contexts  while the third query is a company name that has many sub-topics corresponding to different product lines. for each query we received 1 search results  which include the tide and url of the matching web page  as well as a short summary. 
　in order to assess how our criteria ranked different partitions of the same data  we first needed to generate a range of different partitions for each data set. this was done by repeatedly applying our clustering algorithm with increasing values of the clustering threshold parameter t  see section 1 . as t increased  the number of clusters in the partition decreased. we then assigned a value to each partition using each of our four criteria. the resulting values have been plotted in figure 1. the horizontal axis of each graph corresponds to the number of clusters in a partition  while the vertical axis indicates the value of each criterion for each partition. this enables us to compare the sensitivity of each criterion as the number of clusters varies. 
　for ease of comparison in figure 1  the values returned by the criteria have been normalised to lie in the range 1 to 1  where better partitions have higher values. note that this is the opposite of the formulas given in section 1  where the best partition produced the minimum value of the criterion. however  we found that the graphs were easier to interpret visually when the sign was reversed. it also simplifies com-
　
parison with the histogram of results from the human sub-
jects. in addition  we have smoothed these curves using a running average of length 1. this makes it easier to visually compare the underlying trend of each criterion  with little effect on their sensitivity. 
　we also gave the search results for the 1 test queries to 1 human subjects  and asked them to cluster the search results by hand. their results appear as a histogram above each graph in figure 1  where each point indicates the number of clusters found by that human subject. we can thus compare the spread between our four criteria and our human subjects. 
　in many cases  the partitions found by both the clustering system and our human subjects contained a large number of singleton classes. in order to test whether this distorted the results in figure 1  we have plotted histograms in figure 1 of the number of non-singleton clusters found by our four criteria and our human subjects. due to space restrictions  for each criterion we have plotted only die best 1 partitions. 
1 discussion 
as shown in figures 1 and s  there is a large variability in the number of clusters that human subjects generated for the same document sets. for instance  for the 'agents' query  the number of clusters ranges from 1 to 1 and the number of non-singleton clusters ranges from 1 to i1. this indicates that some subjects prefer general groupings while others prefer tight specific groupings. 
　analysis of the cluster groupings generated by human subjects indicates that subjects formed conceptual groupings that were not necessarily apparent from the words in the summary. however  different subjects generalise at different levels of granularity. for instance  when clustering the search results of the 'agents' query  1 subjects grouped software  mobile  intelligent and autonomous agents into a single cluster  which we refer to as ai agents. 1 subjects grouped ai agents into two clusters: mobile and other intelligent agents. 1 subjects split these documents into three clusters  but the cluster groupings were different. 
　this same variability is also exhibited by the four criteria for cluster quality  with different criteria preferring different levels of generalisation  figure 1 . for instance  for the 'agents' query  the number of clusters in the best partition ranges from 1  1 non-singleton  to 1  1 non-singleton   which is in line with the variability found in the human clusters. even with a single criterion  there is a whole range of partitions that are near-optimal  i.e.  the value returned is within 1% of the value for the best partition. however  the location and width of the range varies between criteria. 
　the minimum total distances criterion  ci  prefers many small specific clusters to a few large general clusters. for instance  for the 'agents' query  the best partition has 1 clusters out of which 1 are non-singleton. the ai agents group discussed earlier is split into five groups: software  intelligent  autonomous  mobile and others that did not fall into the above groups. in general  this criterion has a narrow optimal area indicating high sensitivity  e.g.  for the 'agents' query  four other partitions are near-optimal and the number of clusters for these partitions range from 1  1 non-singleton  and 1  1 non-singleton . 
　the separated clusters criterion  c1  also prefers many small specific clusters to a few large general clusters  e.g.  for the 'agents' query  the best partition has the same number of clusters  and the ai agents group is again split into five groups although the actual groupings are different. in general  the sensitivity of this criterion is data-dependent with large optimal areas for some queries such as 'telstra' and very narrow optimal areas for other queries. 
　the object positioning criterion  c1  prefers a few large general clusters to many small specific clusters. for instance  for the 'agents' query  the best partition has 1 clusters out of which 1 are non-singleton  and the ai agents group discussed earlier is split into two groups with software  autonomous and mobile agents grouped together. in general  this criterion has a narrow optimal area indicating high sensitivity  e.g.  for the 'agents' query  two other partitions are nearoptimal and the number of clusters for these partitions range from 1  1 non-singleton  and 1  1 non-singleton . 
　the number of objects correcdy positioned criterion  c1  also prefers a few large general clusters to many small specific clusters. for instance  the best partition of the 'agents' query has 1 clusters  all with more than one object. the ai agents group discussed earlier is split into two groups in the best partition  with mit software agents in one group and other intelligent agents in the other group. in general  this criterion has a very wide optimal area indicating low sensitivity  e.g.  for the 'agents' query  1 other partitions are near-optimal and the number of clusters for these partitions range from 1  1 non-singleton  and 1  1 non-singleton . 
　given the diversity of partitions generated by human subjects and our evaluation criteria  there is no single clustering methodology or cluster quality criterion that is useful for all users. however  preliminary studies into the use of clustering as an exploratory tool during retrieval indicates that clustering is useful in quickly eliminating large sets of retrieved results  zamir and etzioni 1 . the choice of clustering methodology and quality criterion is dictated by a user's preferences for generalisation  and a clustering algorithm with varying thresholds and different quality criteria is one method for catering to a user's generalisation preferences. criteria c1 or c1 may be used when users want tight specific clusters  and criteria c1 or c1 when users require a few general clusters. we have found from further testing on a wide range of queries that the above difference in behaviour between these criteria is consistent. 
　the sensitivity analysis indicates that even for the highly sensitive criteria  such as ci and c1  there are a number of partitions that are optimal. hence  for applications that require real-time response  such as clustering search results  only a few thresholds need to be explored in order to provide a near-optimal rather than the best partition. 
1 related work 
one of the main studies of clustering criteria was made by milligan and cooper  1   in which they performed an empirical evaluation of 1 different criteria. their focus was on stopping rules for hierarchical clustering. they tested these criteria on simulated data sets involving a maximum of 
1 clusters and 1 attributes. in contrast  we have focused on 
	raskutti and leckie 	1 
　
non-hierarchical clustering of practical data sets  where our data sets have 1 attributes. these practical differences motivated our interest in a sensitivity analysis of clustering criteria for such large and complex data sets. 
　our study has concentrated on distance-based clustering  which relies on a similarity function to compare vectors describing the objects to be clustered. an alternative class of clustering algorithm is known as mixture modelling  where the objects to be clustered are generated from a mixture of probability distributions of a known type. oliver et al.  1  conducted an empirical comparison between a minimum message length criterion and several other statistical criteria on simulated data. however  our experience has been that a mixture modelling approach is difficult to apply to document clustering  due to the problems in finding suitable underlying distributions for term frequencies in documents. 
　leouski and croft  1  performed an empirical evaluation of methods for clustering search results. their emphasis was the representation of documents and the performance of different clustering techniques. while they examine how to evaluate clustering techniques in terms of precision and recall  they do not provide quality criteria that can be applied as the clusters are being generated. in addition  they focused on hierarchical clustering of full-text documents. 
　the problem of clustering results from a search engine has also been studied by zamir and etzioni  1 . they use a technique called suffix tree clustering  which first clusters documents that contain common phrases. they then merge clusters based on the proportion of documents in common between two clusters. it is difficult to make a direct comparison between the quality of their clusters and ours  because they can assign a result to more than one cluster  and they used a fixed number of clusters in their tests. the aim of their experiments was to assess the relevance of clusters to the original query  based on a manually assigned value of relevance. in contrast  our aim has been to detect the number of clusters in the search results  and assess the sensitivity of numerical criteria as well as human judgement in the choice of this number of clusters. 
　macskassy et al.  1  have studied how a group of human subjects clustered the results of 1 search queries. they reached the conclusion that humans show considerable variation in how they cluster search results  which matches our own experience. we have extended this result by making a comparison between the sensitivity of humans and numerical criteria to the number of clusters. we have shown that the numerical criteria also reflect the range in the number of clusters found by our human subjects. 
1 conclusion and further work 
we have developed several alternative criteria for determining the quality of clustering. these criteria are designed so that different criteria prefer cluster sets that generalise at different levels of granularity. we have evaluated our criteria for sensitivity and spread in a practical application. in addition  we have compared the partitions chosen by our criteria with those generated by human subjects. 
　our analysis demonstrates that our criteria match the variability exhibited by human subjects  indicating there is no 
1 	natural language processing 
single perfect criterion. instead  it is necessary to select the correct criterion to match a human subject's generalisation needs. we show how this matching may be done using a clustering algorithm with varying thresholds and different quality criteria. the number of thresholds examined may be adjusted to provide the required computational efficiency. 
　our next step is to explore the behaviour of our criteria with non-document data sets and to test the suitability of our criteria as stopping rules for hierarchical clusters. in addition  we plan to extend our criteria to study overlapping clusters such as those generated by zamir and etzioni  1 . 
acknowledgements 
we hereby acknowledge the permission of the director  
technology strategy and research  telstra corporation  to publish this work. 
