
we consider the fundamental problem of monitoring  i.e. tracking  the belief state in a dynamic system  when the model is only approximately correct and when the initial belief state might be unknown. in this general setting where the model is  perhaps only slightly  mis-specified  monitoring  and consequently planning  may be impossible as errors might accumulate over time. we provide a new characterization  the value of observation  which allows us to bound the error accumulation.
the value of observation is a parameter that governs how much information the observation provides. for instance  in partially observable mdps when it is 1 the pomdp is an mdp while for an unobservable markov decision process the parameter is 1. thus  the new parameter characterizes a spectrum from mdps to unobservable mdps depending on the amount of information conveyed in the observations.
1 introduction
many real world applications require estimation of the unknown state given the past observations. the goal is to maintain  i.e. track  a belief state  a distribution over the states; in many applications this is the first step towards even more challenging tasks such as learning and planning. often the dynamics of the system is not perfectly known but an approximate model is available. when the model and initial state are perfectly known then state monitoring reduces to bayesian inference. however  if there is modelling error  e.g. the transition model is slightly incorrect   then the belief states in the approximate model may diverge from the true  bayesian  belief state. the implications of such a divergence might be dire.
　the most popular dynamic models for monitoring some unknown state are the hidden markov model  hmm   rabiner& juang  1 and extensions such as partially observable markov decision process  puterman  1   kalman filters  kalman  1  and dynamic bayesian networks  dean & kanazawa  1  - all of which share the markov assumption. naturally  one would like to provide conditions as to when monitoring is possible when modelling errors are present. such conditions can be made on either the dynamics of the system  i.e. the transition between the states  or on the observability of the system. while the true model of the transition dynamics usually depends on the application itself  the observations often depend on the user  e.g. one might be able to obtain better observations by adding more sensors or just more accurate sensors. in this paper our main interest is in quantifying when the observations become useful and how it effects the monitoring problem.
　before we define our proposed measure  we give an illustrative example of an hmm  where the value of information can vary in a parametric manner. consider an hmm in which at every state the observation reveals the true state with probabilityand with probability  gives a random state. this can be thought of as having a noisy sensor. intuitively  as the parameter  varies from zero to one  the state monitoring become harder.
　we introduce a parameter which characterizes how informative the observations are in helping to disambiguate what the underlying hidden state is. we coin this parameter the value of observation. our value of observation criterion tries to quantify that different belief states should have different observation distributions. more formally  the l1 distance between any two belief states and their related observation distributions is maintained up to a multiplicative factor  which is at least the value of observation parameter .
　in this paper we use as an update rule a variant of the bayesian update. first we perform a bayesian update given our  inaccurate  model  and then we add some noise  in particular  we mix the resulting belief state with the uniform distribution . adding noise is crucial to our algorithm as it ensures the beliefs do not become incorrectly overly confident  thus preventing the belief state from adapting fast enough to new informative information.
　our main results show that if the model is only approximate then our modified bayesian updates guarantee that the true belief state and our belief state will not diverge- assuming the value of observation is not negligible. more specifically  we show that if the initial state is approximately accurate  then the expected kl-divergence between our belief state and the true belief state remains small. we also show that if we have an uninformative initial state  e.g.  arbitrary initial belief state  we will converge to a belief state whose expected kl-divergence from the true belief state is small and will remain as such from then on. finally  we extend our results to the setting considered in  boyen & koller  1   where the goal is to compactly represent the belief state. the precision and rate of convergence depends on the value of observation.
　one natural setting with an inaccurate model is when the underlying environment is not precisely markovian. for example  it might be that the transition model is slightly influenced by some other extrinsic random variables. given these extrinsic variables  the true transition model of the environment is only a slightly different model each time step. this is a case where we might like to model the environment as markovian  even at the cost of introducing some error  due to the fact that transition model is not entirely markovian. our results apply also to this setting. this is an encouraging result  since in many cases the markovian assumption is more of an abstraction of the environment  then the a precise description.
related work. the work most closely related to ours is that of boyen and koller  1   where they considered monitoring in a hidden markov model. in their setting  the environment is  exactly  known and the agent wants to keep a compact factored representation of the belief state  which may not exactly have a factored form . their main assumption is that the environment is mixing rapidly  i.e the error contract by geometric factor after each time we apply the transition matrix operator. in contrast  we are interested in monitoring when we have only an approximate environmentmodel. both our work and theirs assume some form of contraction where beliefs tend to move closer to the truth under the bayesian updates - ours is through an assumption about the value of observation while their is through assumption about the transition matrix. the main advantage of our method is that in many applications one can improve the quality of its observations  by adding more and better sensors. however  the mixing assumption used by boyan and koller may not be alterable. furthermore  in the final section  we explicitly consider their assumption in our setting and show how a belief state can be compactly maintained when both the model is approximate and when additional error accumulates from maintaining a compact factored representation.
　particle filtering  doucet  1  is a different monitoring approach  in which one estimates the current belief state by making a clever sampling  where in the limit one observes the true belief state. the major drawback with this method is in the case of a large variance where it requires many samples. a combination of the former two methods was considered by  ng et al.  1 .
　building on the work of  boyen & koller  1  and the trajectory tree of  kearns et al.  1   mcallester and singh  1  provides an approximate planning algorithm. similar extensions using our algorithm may be possible.
　outline. the outline of the paper is as follows. in section 1 we provide notation and definitions. section 1 is the main section of the paper and deals with monitoring and is composed from several subsections; subsection 1 describes the algorithm; subsection 1 provides the main monitoring theorem; subsection 1 proves the theorem. in section 1  we show how to extend the results into the dynamic bayesian networks.
1 preliminaries
an hidden markov model  hmm  is 1-tuple   s p ob o   where s is the set of states such that |s| = n  p is the transition probability form every state to every state  ob is the observations set and o is the observation distribution in every state. a belief state b is a distribution over the states s such that b i  is the probability of being at state si. the transition probabilities of the belief states are defined according to hmm transition and observation probability  using a bayesian update.
　for a belief state b ，   the probability of observing o is o o|b   where
.
after observing an observation o in belief state b ，   the updated belief state is:

where uoo is defined to be the observation update operator. also  we define the transition update operator t as 
.
　we denote by bt the belief state at time t  where at time 1 it is b1.  we will discuss both the case that the initial belief state is known and the case where it is unknown.  after observing observation ot （ ob  the inductive computation of the belief state for time t + 1 is:
 
where we first update the belief state by the observation update operator according to the observation ot and then by the transition update operator. it is straightforward to consider a different update order. therefore  bt+1 is the distribution over states conditioned on observing {o1 o1 ...ot} and on the initial belief state being b1.
1 approximate monitoring
we are interested in monitoring the belief state in the case where either our model is inaccurate or we do not have the correct initial belief state  or both . let us assume that an tion distribution o  which have error with respect to the true algorithm has access to a transition matrix p and an observa-
models. the algorithm's goal is to accurately estimate the belief state at time t  which we denote by  bt.
　for notational simplicity  we define eo゛b = eo゛o ，|b . when are clear from the context  we define t to
be tp and t to be	. when	and o are clear from the
context  we define to be .
our main interest is the behavior of

where the expectation is taken with respect to observation sequences {o1 o1 ...ot 1} drawn according to the true model  and bt and  bt are the belief states at time t  with respect to these observation sequences.
　in order to quantify the accuracy of our state monitoring  we must assume some accuracy conditions on our approximate model. the kl-distance is the natural error measure. the assumptions that we make now on the accuracy of the model will later be reflected in the quality of the monitoring.
assumption 1  accuracy  for a given hmm model  s p ob o   an  accurate model is an hmm
 s p ob o   such that for all states s （ s 
.
next we define the value of observation parameter.
definition 1 given an observation distribution o  let mo be the matrix such that its  o s  entry is o o|s . the value of observation  γ  is defined as  and it is in  1 .
　note that if the value of observation is γ  then for any two belief states b1 and b1 
 .
where the first inequality follows from simple algebra.
　the parameter γ plays a critical rule in our analysis. at the extreme  when γ = 1 we have
. note that this definition is very similar to definition of the dobrushin coefficient  and it is widely used in the filtering literature  moral  1 . we now consider some examples.
　let γ be 1 and consider b1 having support on one state and b1 on another state. in this caseand therefore  which implies that we have a different observations from the two states. since this holds for any two states  it implies that given an observation we can uniquely recover the state. to illustrate the value observation characterization  in pomdp terminology for γ = 1 we have a fully observable mdp as no observation can appear with positive probability in two states. at the other extreme  for an unobservable mdp  we can not have a value of γ   1 since for any two belief states b1 and b1.
　recall the example in the introduction where at every state the observation reveals the true state with probabilityand with probability  gives a random state. here  it is straightforward to show that. hence  as  approaches 1  the value of observation approaches 1.
　we now show that having a value of γ bounded away from zero is sufficient to ensure some guarantee on the monitoring quality  which improves as γ increases. throughout  the paper we assume that γ   1.
1	the belief state update
now we present the belief state update. the naive approach is to just use the approximate transition matrix p  and the approximate observation distribution o . the problem with this approach is that the approximate belief state might place negligible probability on a possible state and thus a mistake may be irreversible.
consider the following update operator t . for each states
s （ s 
 
where uni is the uniform distribution. intuitively  this update operator u  mixes with the uniform distribution  with weight u  and thus always keeps the probability of being in any state bounded away from zero. unfortunately  the mixture with the uniform distribution is an additional source of inaccuracy in the belief state  which our analysis would latter have to account for.
the belief state update is as follows.
	 bt+1 = t u ot bt 	 1 
where  bt is our previous belief state.
1	monitoring the belief state
in this subsection we present our main theorem  which relates the accuracy of the belief state to our main parameters: the quality of the approximate model  the value of observation  and the weight on the uniform distribution.
 theorem 1 at time t let  bt be the belief state updated according to equation  1   bt be the true belief state  zt = be the value of observation. then
where.
	furthermore  if	then for all times t:
also  for any initial belief states b1 and  b1  and δ   1  there exists a time τ δ  − 1  such that for any t − τ δ  we have
　the following corollary now completely specifies the algorithm by providing a choice for u  the weight of the uniform distribution.
corollary 1 assume that
. then for all times t 

proof: with the choice of u  we have:
.
and 
 
which completes the proof.	
1	the analysis
we start by presenting two propositions useful in proving the theorem. these are proved later. the first provides a bound on the error accumulation.
proposition 1  error accumulation for every belief states bt and  bt and updates bt+1 = tuotbt and  bt+1 = t uot bt  we have:	
　the next proposition lower bounds the last term in proposition 1. this term  which depends in the value of observation  enables us to ensure that the two belief states will not diverge.
proposition 1  value of observation  let γ be the value of observation  b1 and b1 be belief states such that b1 s  − μ for all s. then

　using these two propositions we can prove our main theorem  theorem 1.
　proof of theorem 1: due to the fact that u  mixes with the uniform distribution  we can take. combining
propositions 1 and 1  and recalling the definition of   we obtain that:

　by taking expectation with respect to {o1 o1 ...ot 1}  we have:

where the last line follows since  by convexity 
  
which proves the first claim in the theorem.
　we proceed with the case where the initial belief state is good in the sense that . then we have that zt is always less than. the function  has derivative   which is positive when . since is mapped to  then every is mapped to a smaller value. hence the zt will always remain below.
　we conclude with the subtle case of unknown initial belief state  and define the following random variable
	 	z
  otherwise
　note that is a positive super-martingale and therefore converges with probability 1 to a random variable z. the expectation of z cannot be larger than  since whenever z is larger than  its expectation in the next timestep is strictly less than its expected value. since by definition zt then  regardless the our initial knowledge on the belief state  the monitoring will be accurate and results in error less than.	
error accumulation analysis
in this subsection we present a series of lemmas which prove proposition 1. the lemmas bound the difference between the updates in the approximate and true model.
　we start by proving the lemma 1 provided at the beginning of the subsection
lemma 1 for every belief states b1 and b1 

proof: let us define the joint distributions
. throughout the
proof we specifically denote s to be the  random  'first' state  and s to be the  random  'next' state. by the chain rule for relative entropy  we can write:

where the last line follows by assumption 1.
　letdenote the distributions of the first state given the next state  under p1 and p1 respectively. again  by the chain rule of conditional probabilities we have  entropy. putting these two results together leads to the claim.

　the next lemma bounds the effect of mixing with uniform distribution.
lemma 1 for every belief states b1 and b1

proof: by convexity 

where the last line uses the fact that the relative entropy between any distribution and the uniform one is bounded by logn.	
　combining these two lemmas we obtain the following lemma on the transition model.
lemma 1 for every belief states b1 and b1 

　after dealing with transition model  we are left to deal with the observation model. we provide an analog lemma with regards to the observation model.
lemma 1 for every belief states b1 and b1 

proof: first let us fix an observation o. we have:

where the last line uses the fact thatare constants  with respect to s .
now let us take expectations. for the first term  we have:

where the last step uses the fact that. similarly  for the third term  it straightforward to show that:

for the second term 

directly from the definition of the relative entropy. the lemma follows from assumption 1. 
now we are ready to prove proposition 1.
　proof of proposition 1: using the definitions of updates and the previous lemmas:

where the first inequality is by lemma 1 and the second is by lemma 1 this completes the proof of proposition 1.

value of observation proposition - the analysis
the following technical lemma is useful in the proof and it relates the l1 norm to the kl divergence.
lemma 1 assume that  b s    μ for all s and that. then

proof: let a be the set of states where b is greater than  b 
i.e. a = {s|b s  −  b s }. so

where we have used the concavity of the log function and that. the claim now follows using the fact that.	
　we are now ready to complete the proof. we will make use of pinsker's inequality  which relates the kl divergence to the l1 norm. it states that for any two distributions p and q
	 .	 1 
　proof of proposition 1: by pinsker's inequality and assumption 1  we have

for all states s （ s. using the triangle inequality 
 .
therefore 

where we used
	.
combing with pinsker's inequality  we have

where the before last inequality follows from and the last inequality from lemma 1.
1 extension to dbns
in many application one of the main limitations is the representation of the state which can grow exponentially in the number of state variables. dynamic bayesian networks  dean & kanazawa  1  have compact representation of the environment. however  the compact representation of the network does not guarantee that one can represent compactly the belief state.
　the factored representation can be thought as having an update of the following form
	 bt+1 = ht u o bt 	 1 
where h projects the belief state into the closest point in the factored representation space. next we adopt the following definition from  boyen & koller  1  regarding the quality of the factored representation.
definition 1 an approximation  b of  b  incurs error  if relative to true belief state b we have

　armed with these definitions one can prove an equivalent theorem to theorem 1 with respect to kl  bt||bt . due to lack of space and the similarity to the previous results we omit them.
1 conclusions and open problems
in this paper we presented a new parameter in hmm  which governs how much information the observation convey. we showed how one can do fairly good monitoring in absence of an accurate model/unknown starting state/compcat representation as long as the hmm' observations are valuable. an open question that remains is whether the characterization can be made weaker and still an agent would be able to track  for instance if in most states the observations are valuable but there are few in which they are not  can we still monitor the state  another very important research direction is that of planning in pomdps. our results show that one can monitor the belief state of the agent  however this is not enough for the more challenging problem of planning  where one should also decide which actions to take. it is not clear whether our characterization can yield approximate planning as well. the major problem with planning is that of taking the best action w.r.t to a distribution over states can lead to disastrous state and one should look into the long term implications of her actions due to the uncertainty. we leave these interesting problems to future work.
