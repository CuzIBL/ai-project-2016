
this paper presents first steps towards a simple  robust computational model of automatic melody identification. based on results from music psychology that indicate a relationship between melodic complexity and a listener's attention  we postulate a relationship between musical complexity and the probability of a musical line to be perceived as the melody. we introduce a simple measure of melodic complexity  present an algorithm for predicting the most likely melody note at any point in a piece  and show experimentally that this simple approach works surprisingly well in rather complex music.
1 introduction
melody is a central dimension in almost all music. human listeners are very effective in  unconsciously  picking out those notes in a - possibly complex - multi-voice piece that constitute the melodic line. melody is also an important aspect in music-related computer applications  for instance  in music information retrieval  e.g.  in music databases that offer retrieval by melodic motifs  weyde and datzko  1  or query by humming  birmingham et al.  1  .
　it is not easy to unequivocally define the concept of 'melody'. in a sense  the melody is the most prominent line in a polyphonic  multi-voice  piece of music. although in western music  the melody is often found among the higher notes  this is not always the case. also  even in compositions that are explicitly structured into individual lines by the composer such as  e.g.  orchestral pieces consisting of monophonic instrument voices   the melody is by no means always represented by  or 'appearing in'  the same line throughout a piece. in a way  which notes constitute the melody is defined by where the listeners perceive the most interesting things to be going on in the music  or what they sense to be the most coherent path through the complex interweaving of musical lines. thus  though our experience tells us that hearing the melody is a rather simple and intuitive task for humans  it is by no means a simple task to be formalised in a machine.
　in popular music  it is sometimes assumed that the melody does not change between the instruments present. some work has been done on predicting which one of the tracks in a midi file contains the 'main melody'  rizo et al.  1; friberg and ahlba：ck  1 . in both cases a statistical approach was taken - learning properties of melodic and nonmelodic tracks. ideas for converting polyphonic tracks into melodically meaningful monophonic sequences have also been proposed  uitdenbogerd and zobel  1 .
　this paper presents first steps towards a simple  robust computationalmodelof automaticmelodynote identification. based on results from musicology and music psychology that indicate a relationship between melodic complexity and a listener's attention  we postulate that the notes making up the melody line may be identified by calculating and integrating some measures of perceived complexity over time. we will introduce a simple  straightforward measure of melodic complexity based on entropy  present an algorithm for predicting the most likely melody note at any point in a piece  and show experimentally that this very simple approach works surprisingly well in picking out the melody notes in quite complex polyphonic music. still  the results are still far below what we can expect humans to achieve  and we will discuss a number of possible extensions of the approach towards a more comprehensive and effective computational model.
1 complexity and melody perception
the basic motivation for our model of melody identification is the observation  which has been made many times in the literature on music cognition  that there seems to be a connection between the complexity of a musical line  and the amount of attention that will be devoted to it on the part of a listener. a voice introducing new or surprising musical material will potentially attract the listener's attention. however  if the new material is constantly repeated  we will pay less and less attention to it and become habituated or accustomed to the stimulus. less attention is required from the listener and the voice will fall into the background  snyder  1 . the notion of musical surprise is also related to the concept of 'expectation' as it has been put forth in recent music theories  narmour  1; huron  1 . if we assume that the melody is the musical line that commands most attention and presents most new information  it seems natural to investigate melodic complexity measures as a basis for melody detection algorithms.
　indeed  the idea of using information-theoretic complexity measures to characterise aspects of musical development is not at all new. for instance  to cite just two  in  dubnov et al.  1   a measure of information rate computed over a piece of music was shown to correlate in significant ways with familiarity ratings and emotional force response profiles by human human subjects. in  li and sleep  1  it was shown that kernel-based machine learning methods using a compression-based similarity measure on audio features perform very well in automatic musical genre classification.
　in the current paper  we intend to show that the complexity or information content of a sequence of notes may be directly related to the degree to which the note sequence is perceived as being part of the melody. our approach is to start with a very simple  or simplistic  measure of complexity based only on note-level entropies  in order to first understand the influence of this simple parameter. in the next phases of the project  more complex complexity measures based on pattern compression and top-down heuristics derived from music theory will be added  one by one.
1 a computational model
the basic idea of the model consists in calculating a series of complexity values locally  over short-term musical segments  and for each individual voice in a piece.1 based on these series of local complexity estimates  the melody is then reconstructed note by note by a simple algorithm  section 1 .
　the information measures will be calculated from the structural core of music alone: a digital representation of the printed music score. though the model works with arbitrary midi files which may represent actual performances  it will not consider performance aspects like expressive dynamics  articulation  timbre  etc. these may well contain a lot of useful information for decoding the melody  in fact  expressive music performance is a means used by performers to elucidate the musical structure of a piece  gabrielsson  1    but we want to exclusively focus on music-structural aspects first  in order not to mix different factors in our investigation.
1 the sliding window
the algorithm operates by in turn examining a small subset of the notes in the score. a fixed length window  with respect to duration  is slid from left to right over the score. at each step  the window is advanced to where the next 'change' happens so no window will contain exactly the same set of notes  but at the same time all possibilities of placing the window resulting in different content have been examined. the first window starts at time 1. it is then moved ahead in time until the end of the piece. at each step  the next window will begin at whatever of the following two situations occurs first:
1. offset of first ending note in current window
1. onset of next note after current window
　in the first case  the change occurs since a note 'leaves' the current window; the next window will begin right after the first occurring offset of a note in the current window. if all pitch

wi 
figure 1: sliding the window
notes in the current window end at the same time  the next window will thus begin right after  which makes it possible for the window to be empty. only then will the second of the above cases apply  the change is the entry of the next note .
　figure 1 shows the positions of the window when sliding over three notes a  b  and c. window wi contains a and b  wi+1 contains only b  wi+1 is empty and wi+1 contains c.
　from the notes belonging to the same voice  instrument  in the window  we calculate a complexity value. we do that for each voice present in the window. the most complex voice is expected to be the one that the listener will focus on in this time period. the complexity measures and the melody note prediction method are explained below.
1 entropy measures in musical dimensions
shannon's entropy  shannon  1  is a measure of randomness or uncertainty in a signal. if the predictability is high  the entropy is low  and vice versa.
　let x = {x1 x1 ... xn} and p x  = pr x = x  then the entropy h x  is defined as:
		 1 
　x could for example be the set of midi pitch numbers and p x  would then be the probability  estimated by the frequency  of a certain pitch. in the case that only one type of event  one pitch  is present in the window  that event is highly predictable or not surprising at all  and the entropy is 1. entropy is maximised when the probability distribution over the present events is uniform.
　we are going to calculate entropy of 'features' extracted from the notes in monophonic lines. we will use features related to pitch and duration of the notes. a lot of features are possible: midi pitch number  midi interval  pitch contour  pitch class  note duration  inter onset interval etc.  cf.  conklin  1  . we have used the following three measures in the model presented here:
1. pitch class  c : count the occurrences of different pitch classes present  the term pitch class is used to refer the 'name' of a note  i.e.  the pitch irrespective of the octave  such as c  d  etc. ;
1. midi interval  i : count the occurrencesof each melodicinterval present  e.g.  minor second up  major third down  ... ;
1. note duration  d : count the number of note durationclasses present  where note classes are derived by discretisation  a duration is given its own class if it is not within 1% of an existing class .
　with each measure we extract events from a given sequence of notes  and calculate entropy from the frequencies of these events  hc  hi hd .
　hc and hi are thought to capture opposite cases. hc will result in high entropy when calculated on notes that form a scale  while hi will result in low entropy. in situations where there are more different intervals than pitch classes  e.g. an 'alberti bass'  or a triad chord played in arpeggio up and down   hi will produce a higher entropy than hc.
　so far rhythm and pitch are treated separately. we have also included a measure hcid weighting the above three measures:.
　entropy is also defined for a pair of random variables with joint distribution:
		 1 
　we will test two joint entropy measures: pitch class in relation to duration  hc d  and interval plus duration  hi d . these are expected to be more specific discriminators.
　as melodic intervals are meant to be calculated from successive single musical events  rather than between chords   we will apply these measures only to individual lines in the music - instrumental parts. in the few cases where it does happen that a part contains simultaneous notes  e.g.  a violin   only the top note is used in the entropy calculation.
1 an alternative: complexity via compression
the entropy function is a purely statistical measure related to the frequency of events. no relationships between events is measured - e.g. the events abcabcabc and abcbcacab will result in the same entropy value. humans  however  would probably describe the first string as three occurrences of the substring abc - we infer structure. according to snyder  we perceive music in the most structured way possible  snyder  1 . to take this into account  complexity measures based on compression could be considered. music that can be compressed a great deal  in a lossless way  can then be considered less complex than music that cannot be compressed. methods exist that substitute recurring patterns with a new event  and store the description of that pattern only once  e.g. run-length encoding or lzw compression  ziv and lempel  1 . this idea has been discussed in several musical application contexts  e.g.  in music information retrieval  li and sleep  1  or in automated music analysis and pattern discovery  lartillot  1  . however  these compression algorithms are not well suited for compressing the short sequences that we are dealing with. special algorithms for compressing short sequences will be needed.
　shmulevich and povel  shmulevich and povel  1  have examined methods for measuring the complexity of short rhythmic patterns which are supposed to repeat infinitely. tanguiane's measure  tanguiane  1  is based on the idea that a rhythmic pattern can be described as elaborations of windows
prediction period
    1   1   1   1   1  overlapping windows figure 1: prediction periods and windows overlapping them.
simpler patterns. before turning to these more complex measures and the assumptions they make  we would like to investigate the power of a simple and well-understood measure like entropy.
1 predicting melody notes
given the set of windows described above and an entropy value for each voice present in each window we will predict the notes expected to belong to the melody. we consider in turn the notes in the interval between the starting points of two consecutive windows - the prediction period. the prediction period pi is thus the interval between the beginning of window wi and the beginning of wi+1  see figure 1 .
　we want to mark  in each prediction period  the notes that pertain to the most complex voice. complexity measures from each window a note appears in will influence the prediction. in a prediction period pi  we will consider the set o pi  of all windows that overlap this period. for pi the set will contain all windows that have not ended at the start time of pi+1. figure 1 gives an example of prediction periods and overlapping windows - e.g. o p1  is the set {w1 w1}.
　for each voice in each window  a complexity value was calculated by the time the windows were calculated. the average complexity value for each voice present in the windows in o pi  is calculated. based on these values  we can now rank the voices according to their average complexity over o pi   and a 'winning' voice is found. every note in wi  the window where pi begins  gets its melody attribute set to true if it is part of the winning voice  and to false otherwise.
　in each prediction period  exactly the windows overlapping  or partly overlapping this period are contributing to the final score of each voice. the policy is to mark all notes in the entire window wi starting at pi. when wi+1 occurs before wi has ended  the contribution from this new window can change the prediction of wi from that point in time and onwards. a new prediction period pi+1 begins - including all windows now present at this time  and the notes' melody attribute is reset accordingly. so if a note is only contained in a single prediction period  its status will only be set once. a long note contained in a number of prediction periods will finally be judged by the period it last occurs in.
　thus the predicted notes may belong to different voices from period to period - the role as the most prominent voice may change quickly. we are not just predicting an entire voice to be the most interesting  but every note is considered. in case there are several voices that share the highest complexity value for a given prediction period  the voice with the highest pitch will be predicted  e.g.  of two voices playing in octaves  the higher one is chosen .
1 experiments
1 the musical test corpus
to perform experiments we need music which is composed for different parts  and encoded in such a way that each part is separately available. furthermore  since our complexity measure assumes monophonic music  each voice in the piece should be close to monophonic. this places some restrictions on the experiments we are able to do. a typical piano sonata  lacking the explicit voice annotation  or being one non-monophonic part   will not be an appropriate choice.
　since the model presented here does not take into account any performanceaspects but only the musical surface  score   we will use midi files generated from the musedata format  http://www.musedata.org . the durations of the notes in these files are nicely quantised. the model will run on all types of midi files  but performed music tends to bias the rhythm complexity. two pieces of music were chosen to be annotated and used in the experiment:
1. haydn  f.j.: string quartet no 1 op. 1  no. 1  in cmajor  1st movement
1. mozart  w.a.: symphony no 1 in g minor  kv 1  1st movement
　this is not an awful lot of data  but since the music will have to be annotated manually  this will have to do for our initial experiments.
1 annotating melody notes
melody annotation is non-trivial and rather tedious task. a trained musicologist was asked to serve as expert annotator. she was given a piano roll representation of the music with 'clickable' notes and simple playback options for listening to the midi sound. the annotator  who was not aware of the purpose of this task  was instructed to mark notes she would consider as 'melody'.
　the test pieces turned out to have quite distinguishable melodic lines. the annotator solved the task by marking the notes that she was humming during listening to the pieces. some immediate differences between the 'true' concept of melody and some assumptions behind our complexity-based melody prediction model became immediately clear after talking to the annotator:
　our model assumes that there is always one most complex voice present  but a melody was not found to be present at all times. furthermore  melodic lines were sometimes marked as overlapping  which the model does not allow. the annotator chose not to mark thematic lines that clearly were a  melodic  response in another voice to the melody rather than a continuation of the melody line. our model might mark both the melody and the response. another source of error is situations where the melodic line consists of long sustained tones while the accompanying notes are doing all the action. the model will erroneously predict an accompanying voice.  in
haydnmozarttotal number of notes1number of melody notes1melody note percentage1 %1 %number of voices1duration1 min1 mintable 1: the test data
these situations it is also difficult to tell what a listener will actually listen to .
　in any case  the annotations made by the musicologists were taken to be authoritative; they were adopted unchanged and used as ground truth for the evaluation. table 1 shows some information about the test data.
1 evaluation method
we can now measure how well the predicted notes correspond to the annotated melody in the score. we express this in terms of recall  r  and precision  p  values  van rijsbergen  1 . recall is the number of correctly predicted notes  true positives  tp  divided by the total number of notes in the melody. precision is tp divided by the total number of notes predicted  tp + fp  false positives  . the f-measure combines recall and precision into one value:
		 1 
　a high rate of correctly predicted notes will result in high values of recall  precision and f-measure  close to 1 .
1 results
we performed prediction experiments with four different window sizes  1 seconds  and with the six different entropy measures described above. table 1 shows recall  precision and f-measure values from all experiments with the two evaluation pieces. in addition  the columns marked p show the value of the f-measure achieved by the simple strategy of always picking the highest note  see below . the highest values in each row are printed in bold  ignoring the p columns .
　the string quartet turned out to be the less complex of the two pieces. this is not much of a surprise. it is easier to discriminate between 1 than 1 voices  and also the compositional structures of the pieces are quite different. overall the joint pitch class and duration measure hc d was found to have the greatest predictive power  generally followed by the weighted combination hcid. pitch class seems to be the single most important measure in the string quartet. in total  the joint measures perform better than the measures based on a single feature.
　we can conclude that there is indeed a correlation between melody and complexity in both pieces. the precision value of 1 in the best symphony experiment with a resulting fmeasure of 1  window size 1 seconds  tells us that 1 % of the predicted notes in the symphony are truly melody notes.
haydnmozartwinphchihdhcidhc dhi dphchihdhcidhc dhi d1sr11111111111111p11111111111111f111111111111111sr11111111111111p11111111111111f111111111111111sr11111111111111p11111111111111f111111111111111sr11111111111111p11111111111111f11111111111111table 1: recall  precision  and f-measure for melody note predictions.
　in the string quartet  starting in bar 1  see figure 1  the second violin is alternating between a single note and notes from a descending scale  making the voice very attractive  lots of different notes and intervals  while the 'real melody' figure 1: the string quartet  measures 1
in the first violin is playing fewer different notes  but has a more varied rhythm. we took a closer look at this passage. setting the window size to 1 seconds  the measures hd  hcid  and hc d recognise the upper voice as melody whereas hc  hi  and hi d suggest the lower. the measures based on intervals are naturally led astray in this case  and the measure based solely on pitch class is also.
　the reader may also ask if it would not be more effective to just always predict the highest note as the melody note - after all  that is where the melody is most often  at least in relatively simple music. we checked against this baseline. the results are shown in the columns labeled p in table 1. indeed  it turns out that in hadyn string quartet  the simple strategy of always picking the highest note outperforms all of our heuristics - at least in terms of global precision and recall  though not necessarily in terms of the local musical coherence of the resulting melodic line.
　in the more complex mozart piece  however  we witness that the highest note prediction strategy was outperformed by most of our methods. in the symphony movement the melody is not on top as often. also  the melody role is taken by different instruments at different times. figure 1 shows 1 bars  bar 1  from the symphony. in the first three  the flute  top voice  was annotated to be the melody  but the melody then moves to the first violin in bar 1. in bar 1 the first oboe enters on a high-pitched long note - above the melody. such high-pitched non-melodic notes  doubling chord tones  occur frequently in the movement. the hc d measure cor-

	f	p
figure 1: the symphony  measures 1
rectly catches the entry of the theme in bar 1  but the bassoon  fg.  is falsely predicted in bar 1 - the wind instruments play very similar lines  but because the bassoon continues a bit further than the others it becomes the most complex.
1 discussion
in our opinion  the current results  though based on a rather limited test corpus  indicate that it makes sense to consider musical complexity as an important factor in computational models of melody perception. the results of the experiments show that a simple entropy based incremental algorithm can identify parts of melodies in quite complex music. we can also turn the argument around and interpret this as empirical evidence that melody in classical tonal music is indeed partly definable by complexity - a finding that musicology may find interesting.
　clearly  recall and precision values of around 1%  as in the mozart piece  are not sufficient  neither for justifying our approach as a full model of melody perception  nor for practical applications. the next steps towards improvement are quite clearly mapped out. the notion of musical complexity must be extended to encompass structural aspects  i.e.  recognisable patterns and their recurrence . we will study various compression-based complexity measures  in particular with respect to their suitability in the context of very short sequences  windows .
　a second research direction is dictated by the musicological insight that melody  and generally the perception of what constitutes a coherent musical line  has something to do with continuity in certain parametric dimensions  e.g.  in terms of intervallic motion . such gestalt-like principles are usually difficult to formalise  due to their inherently top-down nature  but local measures of continuity and melodic coherence over short windows should be quite straightforward.
　generally  the more different aspects are integrated in a model of music perception  the more conflicts may arise between them  for instance the obvious conflict between continuity and unpredictability . good music  in fact  thrives on these kinds of conflicts - divergent cues make music interesting to the listeners  maintaining their attention at a high level. that is precisely why performers are so important in music. their task is  in part  to disambiguate different possible 'readings' of a piece through their playing. thus  eventually we will also have to take into account performance information  such as dynamics  relative loudness   articulation  etc. but we plan to proceed in stages  evaluating contributions of individual components step by step  in order to arrive at a model that not only works  but tells us something interesting about a human musical ability.
acknowledgments
this research was supported by the viennese science and technology fund  wwtf  project ci1  and by the austrian federal ministries of education  science and culture and of transport  innovation and technology.
