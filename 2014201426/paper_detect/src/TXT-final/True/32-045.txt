 
i highlight some inefficiencies of graphplan's backward search algorithm  and describe how these can be eliminated by adding explanation-based learning and dependency-directed backtracking capabilities to graphplan. i will then demonstrate the effectiveness of these augmentations by describing results of empirical studies that show dramatic improvements in run-time  w loox speedups  as well as solvability-horizons on benchmark problems across seven different domains. 
1 introduction 
graphplan  blum & furst  1  is currently one of the more efficient algorithms for solving classical planning problems. four of the five competing systems in the recent aips-1 planning competition were based on the graphplan algorithm  mcdermott  1   extending the efficiency of the graphplan algorithm thus seems to be a worth-while activity. in this paper  i describe my experience with adding explanation-based learning  ebl  and dependency-directed backtracking capabilities  ddb  to graphplan's backward search. both ebl and ddb are based on explaining failures at the leaf-nodes of a search tree  and propagating those explanations upwards through the search tree  kambhampati  1 . ddb involves using the propagation of failure explanations to support intelligent backtracking  while ebl involves storing interior-node failure explanations  for pruning future search nodes. graphplan does use a rudimentary form of failure-driven learning that it calls  memoization.  as we shall see in this paper  graphplan's brand of learning is quite weak as there is no explicit analysis of the reasons for failure. instead the explanation of failure of a search node is taken to be all the constraints in that search node. as explained in  kambhampati  1   this not only eliminates the opportunities for dependency directed backtracking  it also adversely affects the utility of the stored memos. 
   *this research is supported in part by nsf young investigator award  nyi  iri-1  arpa/rome laboratory planning initiative grant f1-1-c-1  army aasert grant daah1-1  afosr grant f1-1 and nsf grant iri1 thank maria fox  derek long  terry zimmerman  amol mali and biplav srivastava for comments on this work. 
1 	planning and scheduling 
　adding full-fledged ebl and ddb capabilities in effect gives graphplan both the ability to do intelligent backtracking  and the ability to learn generalized memos that are more likely to be applicable in other situations. technically  this involves generalizing conflict-directed back-
jumping  prosser  1   a well-known technique in csp  to work in the context of graphplan's backward search. empirically  the ebl/ddb capabilities improve graphplan's search efficiency quite dramatically-allowing it to easily solve several problems that have hither-to been hard or unsolvable. in particular  i will report on my experiments with the benchmark problems used in  kautz & selman  1   as well as four other standard test domains. the experiments show up-to 1 fold speedups on individual problems. 
　this paper is organized as follows. in the next section  i provide some background on graphplan's backward search. in section 1 discuss some inefficiencies of the backtracking and learning methods used in normal graphplan that motivate the need for ebl/ddb capabilities. section 1 describes how ebl and ddb are added to graphplan. section 1 presents empirical studies demonstrating the usefulness of these augmentations. section 1 discusses related work and section 1 presents conclusions and some directions for further work. 
1 review of graphplan algorithm 
graphplan algorithm  blum & furst  1  can be seen as a  disjunctive  version of the forward state space planners  kambhampati et. al.  1a . it consists of two interleaved phases - a forward phase  where a data structure called  planning-graph  is incrementally extended  and a backward phase where the planning-graph is searched to extract a valid plan. the planning-graph  see figure 1  consists of two alternating structures  called  proposition lists  and  action lists.  figure 1 shows a partial planning-graph structure. we start with the initial state as the zeroth level proposition list. given a k level planning graph  the extension of structure to level k + 1 involves introducing all actions whose preconditions are present in the kth level proposition list. in addition to the actions given in the domain model  we consider a set of dummy  persist  actions  one for each condition in the kth level proposition list. a  persist-c  action has c as its precondition and c as its effect. once the actions are introduced  the proposition list at level k + 1 is constructed as just the union of the effects of all the introduced actions. planninggraph maintains the dependency links between the actions at 


figure 1: the running example used to illustrate ebl/ddb in graphplan 
level k +1 and their preconditions in level k proposition list and their effects in level k +1 proposition list. the planninggraph construction also involves computation and propagation of  mutex  constraints. the propagation starts at level 1  with the actions that are statically interfering with each other  i.e.  their preconditions and effects are inconsistent  labeled mutex. mutexes are then propagated from this level forward by using two simple propagation rules. in figure 1  the curved lines with x-marks denote the mutex relations. 
　the search phase on a a: level planning-graph involves checking to see if there is a sub-graph of the planning-graph that corresponds to a valid solution to the problem. this involves starting with the propositions corresponding to goals on . further  when there is a choice in the actions  values  that can support a condition  we will consider the top actions first since there is only one choice for each of the conditions at level k  and none of the actions are mutually exclusive with each other  we select the actions a1  a1  a1 and a1 for supporting the conditions at level k. we now have to make sure that the preconditions of a1  a1  a1  a1 are satisfied at level k - 1. we thus subgoal on the conditions p1     p1 at level k - 1  and recursively start the action selection for them. we select the action a1 for p1. for p1  we have two supporting actions  and using our convention  we select a1 first/for p1  a1 is the only choice. when we get down to selecting a support for p1  we again have a choice. suppose we select a1 first. we find that this choice is infeasible as as is mutually exclusive with a1 that is already chosen. so  we backtrack and choose a1  and find that it too is mutually exclusive with a previously selected action  a1. we now are stymied as there are no other choices for p1. so  we have to backtrack and undo choices for the previous conditions. graphplan uses a chronological backtracking approach  whereby  it first tries to see if p1 can be re-assigned  and then p1 and so on. notice the first indication of inefficiency here - the failure to assign p1 had nothing to do with the assignment for p1  and yet  chronological backtracking will try to re-assign p1 in the vain hope of averting the failure. this can lead to a large amount of wasted effort had it been the case that p1 did indeed have other choices. 
　as it turns out  we find that p1 has no other choices and backtrack over it. p1 does have another choice - a1. we try to continue the search forward with this value for p1  but hit an impasse at p1-since the only value of p1  a1 is mutex with a   at this point  we backtrack over p   and continue back-

at level k  if all the goals are not present  or if they are present tracking over p1 and p1  as they too have no other remaining 1 but a pair of them are marked mutually exclusive  the search choices. when we backtrack over p1  we need to go back is abandoned right away  and planning-graph is grown an- to level k and try to re-assign the goals at that level. before other level . for each of the goal propositions  we then se- this is done  the graphplan search algorithm makes a  memo  lect an action from the level k action list that supports it  such signifying the fact that it failed to satisfy the goals p1     p1 that no two actions selected for supporting two different goals at this level  with the hope that if the search ever subgoals are mutually exclusive  if they are  we backtrack and try to on these same set of goals in future  we can scuttle it right change the selection of actions . at this point  we recursively away with the help of the remembered memo. here is the call the same search process on the k-1 level planning-graph  second indication of inefficiency- we are remembering all with the preconditions of the actions selected at level k as the the subgoals p1       p1 even though we can see that the probgoals for the k - 1 level search. the search succeeds when lem lies in trying to assign p1  p1  p1 and p1 simultaneously  we reach level 1  corresponding to the initial state . and has nothing to do with the other subgoals. if we remem-
introduced in  mittal & falkenhainer  1  . 
1 some inefficiencies in graphplan's backward search 
to motivate the need for ebl and ddb  we shall first review the details of graphplan's backward search  and pinpoint some of its inefficiencies. we shall base our discussion on the example planning graph from figure 1. assuming that 　after the memo is stored  the backtracking continues into level k - once again in a chronological fashion  trying to reassign g1 g1  g1 and g1 in that order. here we see the third indication of inefficiency caused by chronological backtracking - g1 really has no role in the failure we encountered in assigning p1 and p1 - since it only spawns the condition p1 at level k - 1. yet  the backtracking scheme of graphplan considers reassigning g1. a somewhat more subtle point is that reassigning g1 is not going to avert the failure 　previous work  kambhampati et. al.  1a  had expli-	ber {p1  p1  p1  p1} as the memo as against {p1       p1}  the cated the connections between this backward search phase of remembered memo would be more general  and would have graphplan algorithm and the constraint satisfaction problems a much better chance of being useful in the future.  specifically  the dynamic constraint satisfaction problems  as 
g1 -     g1 are the top level goals of the problem we are inter- either. although g1 requires p1 one of the conditions taking ested in solving  we start at level k  and select actions to sup- part in the failure  p1 is also required by g1 and unless g1 port the goals g1       g1. to keep matters simple  we shall ' gets reassigned  considering further assignments to g1 is not assume that the search assigns the conditions  variables  at going to avert the failure. each level from top to bottom  i.e.  g1 first  then g1 and so for this example  we continue backtracking over g1 and 
	kambhampati 	1 

g1 too  since they too have no alternative supports  and finally memoize {g1  g1  g1 g1} at this level. at this point the backward search fails  and graphplan extends the planning graph by another level before re-initiating the backward search on the extended graph. 
1 improving backward search with ebl and ddb 
i will now describe how graphplan's backward search can be augmented with full fledged ebl and ddb capabilities to eliminate the inefficiencies pointed out in the previous section. informally  ebl/ddb strategies involve explanation of failures at leaf nodes  and regression and propagation of leaf node failure explanations to compute interior node failure explanations  along the lines described in  kambhampati  1 . the specific extensions i propose to the backward search can essentially be seen as adapting conflict-directed back-
jumping strategy  prosser  1   and generalizing it to work with graphplan's backward search  which can be seen as a form of dynamic constraint satisfaction problem . the development here parallels the framework described in  kambhampati  1 . 
　the algorithm is shown in pseudo-code form in figure 1. it contains two mutually recursive procedures find-plan and assign-goals. the former is called once for each level of the planning-graph. it then calls assign-goals to assign values to all the required conditions at that level  assign-goals picks a condition  selects a value for it  and recursively calls itself with the remaining conditions. when it is invoked with an empty set of conditions to be assigned  it calls find-plan to initiate the search at the next  previous  level. 
　in order to illustrate how ebl/ddb capabilities are added  let's retrace the previous example  and pick up at the point where we are about to assign p1 at level k - 1  having assigned p1  p1 and p1. when we try to assign the value as to p1  we violate the mutex constraint between a1 and a1. an explanation of failure for a search node is a set of constraints from which false can be derived. the complete explanation for this failure can thus be stated as: 

from the explanation since the mutual exclusion relation will hold as long as we are solving this particular problem with these particular actions. further  we can take a cue from the conflict directed back-jumping algorithm  prosser  1   and represent the remaining explanation compactly in terms of  conflict sets.  specifically  whenever the search reaches a condition c  and is about to find an assignment for it   its conflict set is initialized as {c}. whenever one of the possible assignments to c is inconsistent  mutually exclusive  with the current assignment of a previous variable c'  we add c' to the conflict set of c. in the current example  we start with {p1} as the conflict set of p1  and expand it by adding p1 after we find that as cannot be assigned to p1 because of the choice of a1 to support p1. informally  the conflict set representation can be seen as an incrementally maintained  partial  explanation of failure  indicating that there is a conflict between the current value of p1 and one of the possible values of p1  kambhampati  1 . 
1 	planning and scheduling 

figure 1: a pseudo-code description of graphplan backward search enhanced with ebl/ddb capabilities. 
　we now consider the second possible value of p1  viz.  a1  and find that it is mutually exclusive with a1 which is currently supporting p1. following our practice  we add p1 to the conflict set of p1. at this point  there are no further choices for p1  and so we backtrack from p1  passing the conflict set of  as the reason for its fail-
ure. in essence  the conflict set is a shorthand notation for the following complete failure explanation  kambhampati  1  r1 

　it is worth noting at this point that when p1 is revisited in the future  with different assignments to the preceding variables   its conflict set will be re-initialized to {p1} before 
　　1we strip the first  disjunctive  clause since it is present in the graph structure  and the next two implicative clauses since they are part of the mutual exclusion relations that will not change for this problem. the conflict set representation just keeps the condition  variable  names of the last two clauses - denoting  in essence  that it is the current assignments of the variables p1 and p1 that are causing the failure to assign p1. 

considering any assignments to it 
dependency directed backtracking: the first advantage of maintaining the conflict set is that it allows a transparent way of dependency directed backtracking  kambhampati  1 . in the current example  having failed to assign   we have to start backtracking. we do not need to do this in a chronologi-
cal fashion however. instead  we jump back to the most recent variable  condition  taking part in the conflict set of   - in this case . by doing so  we are avoiding considering other alternatives at and thus avoiding one of the inefficiencies of the standard backward search. it is easy to see that such back-jumping is sound since  is not causing the failure at   and thus re-assigning it won't avert the failure. 
　continuing along  whenever the search backtracks to a condition c  the backtrack conflict is absorbed into the current conflict set of c. in our example  we absorb into the conflict set of    which is currently {p1}  making the new conflict set of. 1. we now assign a1  the only remaining value  to -   next we try to assign p1 and find that its only value a1 is mutex with a1. thus  we set conflict set of  and backtrack with this con-
flict set. when the backtracking reaches p1  this conflict set 
is absorbed into the current conflict set of p1  as described 
earlier   giving rise to  as the current combined failure reason for p1. this step illustrates how the conflict set of a condition is incrementally expanded to collect the reasons for failure of the various possible values of the condition. 
　at this point  p1 has no further choices  so we backtrack over p1 with its current conflict set  . at p1  we first absorb the conflict set into p1's cur-
rent conflict set  and then re-initiate backtracking since p1 has no further choices. 
now  we have reached the end of the current level  k - 1 . 
any backtracking over p1 must involve undoing assignments of the conditions at the kth level. before we do that however  we carry out two steps: memoization and regression. 
memoization: before we backtrack over the first assigned variable at a given level  we store the conflict set of that variable as a memo at that level. in the current example  we store the conflict set  of p1 as a memo at this level. 
notice that the memo we store is shorter  and thus more general  than the one stored by the normal graphplan  as we do not include and.   which did not have anything to do with 
the failure.1 
regression: before we backtrack out of level k - 1 to level k  we need to convert the conflict set of  the first assigned variable in  level k-1 so that it refers to the conditions in level k. this conversion process involves regressing the conflict set over the actions selected at the kth level  kambhampati  1 . in essence  the regression step computes the  smallest  set of conditions  variables  at the kth level whose supporting actions spawned  activated  in dcsp terms  the conditions  variables  in the conflict set at level k - 1. in the current case  our conflict set is . we can see that p1  
　1  while in the current example  the memo includes all the conditions up to p1  which is the farthest we have gone in this level   even ' this is not always necessary. we can verify that would not have been in the memo set if  were not one of the supporters of. . 
p1 are required because of the condition at level k  and the condition p1 is required because of the condition 
　in the case of condition p1  both and are responsible for it  as both their supporting actions need p1. in such cases we have two heuristics for computing the regression:  1  prefer choices that help the conflict set to regress to a smaller set of conditions  1  if we still have a choice between multiple conditions at level k  pick the one that has been assigned1 earlier. the motivation for the first rule is to keep the failure explanations as compact  and thus as general  as possible  and the motivation for the second rule is to support deeper dependency directed backtracking. it is important to note that these heuristics are aimed at improving the performance of the ebl/ddb and do not affect the soundness and completeness of the approach. 
　in the current example  the first of these heuristics applies  since   is already required by   which is also requiring p1 and . . even if this was not the case  i.e.  g1 only required p1   we still would have selected over as the regression of   since was assigned earlier in the search. 
	the result of regressing 	over the actions 
at kth level is thus . we start backtracking at level k with this as the conflict set. we jump back to right away  since it is the most recent variable named in the conflict set. 
this avoids the inefficiency of re-considering the choices at and   as done by the normal backward search. at g1  the backtrack conflict set is absorbed  and the backtracking continues since there are no other choices. same procedure is repeated at . at this point  we are once again at the end of a level-and we memoize  as the memo at level k. since there are no other levels to backtrack to  graphplan is called on to extend the planning-graph by one more level. 
　notice that the memos based on ebl analysis capture failures that may require a significant amount of search to rediscover. in our example  we are able to discover that is a failing goal set despite the fact that there are no mutex relations between the choices of the goals and 
using the memos  ebl : before we end this section  there are a couple of observations regarding the use of the stored memos. in the standard graphplan  memos at each level are stored in a level-specific hash table. whenever backward search reaches a level k with a set of conditions to be satisfied  it consults the hash table to see if this exact set of conditions is stored as a memo. search is terminated only if an exact hit occurs. since ebl analysis allows us to store compact memos  it is not likely that a complete goal set at some level k is going to exactly match a stored memo. what is more likely is that a stored memo is a subset of the goal set at level k  which is sufficient to declare that goal set a failure . in other words  the memo checking routine in graphplan needs to be modified so that it checks to see if some subset of the current goal set is stored as a memo. the naive way of doing it - which involves enumerating all the subsets of the current goal set and checking if any of them are in the hash table- turns out to be very costly. one needs more efficient data structures  such as the set-enumeration trees  rymon  1 . indeed  koehler and her co-workers  koehler et. al.  1  have developed a  seeming variation of set-enumeration tree  data structure called ubtrees for storing the memos. the ub-tree structures can efficiently check if any subset of the current goal set has been stored as a memo. 
	kambhampati 	1 


table 1: empirical performance of ebl/ddb. unless otherwise noted  times are in cpu minutes on a spare ultra 1 with 1 meg ram  running allegro common lisp compiled for speed.  tt  is total time   mt  is the time used in checking memos and  btks  is the number of backtracks done during search. the numbers in parentheses next to the problem names list the number of time steps and number of actions respectively in the solution. avln and avfm denote the average memo length and average

number of failures detected per stored memo respectively. 
　the second observation regarding memos is that they can often serve as a failure explanation in themselves. suppose we are at some level k  and find that the goal set at this level subsumes some stored memo m. we can then use m as the failure explanation for this level  and regress it back to the previous level. such a process can provide us with valuable opportunities for further back jumping at levels above k. it also allows us to learn new compact memos at those levels. note that none of this would have been possible with normal memos stored by graphplan  as the only way a memo can declare a goal set at level k as failing is if the memo is exactly equal to the goal set. in such a case regression will just get us all the goals at level k 1- 1  and does not buy us any back-
jumping or learning power  kambhampati  1 . 
1 empirical evaluation 
we have now seen the way ebl and ddb capabilities are added to the backward search by maintaining and updating conflict-sets. we also noted that ebl and ddb capabilities avoid a variety of inefficiencies in the standard graphplan backward search. that these augmentations are soundness and completeness preserving follows from the corresponding properties of conflict-directed back-jumping  kambhampati  1 . the remaining  million-dollar  question is whether these capabilities make a difference in practice. i now present a set of empirical results to answer this question. 
　i implemented the ebl/ddb approach described in the previous section cm top of a graphplan implementation in lisp.1 the changes needed to the code to add ebl/ddb capability were minor - only two functions as sign-goals and f ind-plannneeded non-trivial changes. i also added the ub-tree subset memo checking code described in  koehler et. al.  1 . i then ran several comparative experiments on the  benchmark  problems from  kautz & selman  1   as 
   1 the original lisp implementation of graphplan was done by marie peot the implementation was subsequently improved by david smith. 
1 	planning and scheduling 
well as from four other domains. the specific domains included blocks world  rocket world  logistics domain  gripper domain  ferry domain  traveling salesperson domain  and towers of hanoi. the specifications of the problems as well as domains are publicly available. table 1 shows the statistics on the times taken and number of backtracks made by normal graphplan  and graphplan with ebl/ddb capabilities. 
runtime reductions & solvability improvements: the first thing we note is that ebl/ddb techniques can offer quite dramatic speedups - upto 1x in the seven domains i tested. we also note that the number of backtracks reduces significantly and consistently with ebl/ddb. moreover  ebl/ddb techniques push the solvability horizon as many problems were unsolvable without ebl even after 1 hours of cpu time!1 
reduction in memo length: the results also show that as expected the length of memos stored by graphplan decreased substantially when ebl/ddb strategies are employed. for example  the average memo length  the column named  avln  in table 1  goes down from 1 to 1 in logistics  and 1 to 1 in the ferry domain. furthermore  the relative reductions in memo length in different domains are well correlated with the speedups seen in those domains. specifically  we note that blocks world domain  which shows a somewhat lower  ~1x  speedup also has lower memo-length reduction  from 1 to 1 . similarly  the fact that the average length of memos for rocket-ext-a problem is 1 with ebl  and 1 without ebl  shows in essence that normal graphplan is re-discovering an 1-sized failure embedded in many many possible ways in a 1 sized goal set - storing a new memo each time  incurring both increased backtracking and matching costs ! it is thus no wonder that normal graphplan performs badly compared to graphplan with ebl/ddb. 
utility of stored memos: since ebl/ddb store more gen-
   1 for our lisp system configuration  this amounted to about 1 hours of real time including garbage collection time. 

eral  smaller  memos than normal graphplan  they should  in theory  generate fewer memos and use them more often. the columns labeled  avfm  give the ratio of the number of failures discovered through the use of memos to the number of memos generated in the first place. this can be seen as a rough measure of the average  utility  of the stored memos. we note that the utility is consistently higher with ebl/ddb in all the solved problems. as an example  in rocket-ext-b  we see that on the average an ebl/ddb generated memo was used to discover failures 1 times  while the number was only 1 for the memos generated by the normal graphplan. 
the c vs. lisp question: given that most existing implementations of graphplan are done in c with many optimizations  one nagging doubt is whether the dramatic speedups due to ebl/ddb are somehow dependent on the moderately optimized lisp implementation i have used in my experiments. thankfully  the ebl/ddb techniques described in this paper have also been  re implemented by maria fox and derek long on their stan system. stan is a highly optimized implementation of graphplan that fared well in the recent aifs planning competition. they have found that ebl/ddb resulted in the same dramatic speedups on their system too  fox  1 . 
1 related work 
in their original implementation of graphplan  blum and furst experimented with a variation of the memoization strategy called  subset memoization . in this strategy  they keep the memo generation techniques the same  but change the way memos are used  declaring a failure when a stored memo is found to be a subset of the current goal set. since complete subset checking is costly  they experimented with a  partial  subset memoization where only subsets of length n and n - 1 are considered for an n sized goal set. 
as we mentioned earlier  koehler and her co-workers 
 koehler et. al.  1  have re-visited the subset memoization strategy  and developed a more effective solution to complete subset checking that involves storing the memos in a data structure called ub-tree  instead of in hash tables. the results from their own experiments with subset memoization  as well as my replication of those experiments  kambhampati  1b   are mixed at best  and indicate that the improvements are nowhere near those achievable through ddb and ebl. the reason for this is quite easy to understand - while they improved the memo checking time with the ub-tree data structure  they are still generating and storing the same old long memos. in contrast  the ebl/ddb extension described here supports dependency directed backtracking  and by reducing the average length of stored memos  increases their utility significantly  thus offering dramatic speedups. 
　 kambhampati  1  describes the general principles underlying the ebl/ddb techniques and sketches how they can be extended to dynamic constraint satisfaction problems.  kambhampati  1a  discusses the relations between graphplan and dynamic csp problems. the development in this paper can be seen as an application of the ideas from these two papers. 
1 conclusion and future work 
in this paper  i motivated the need for adding ebl/ddb capabilities to graphplan's backward search  and described the changes needed to support these capabilities in graphplan. i also presented and analyzed empirical results to demonstrate that ebl/ddb capabilities significantly enhance the efficiency of graphplan on benchmark problems in seven different domains. based on these results  and the fact that i have not encountered any problems where ebl/ddb turned out to be a significant net drain on efficiency  i conclude that ebl/ddb extensions are very useful for graphplan. 
　there are several ways in which this work can be  and is being  extended. recently  i have added forward checking and dynamic variable ordering capabilities on top of ebl & ddb in graphplan  and initial results show further improvements  up to a factor of 1  in run time  kambhampati  1b . the success of ebl/ddb approaches in graphplan is in part due to the high degree of redundancy in the planning graph structure. in  zimmerman & kambhampati  1   we investigate techniques that are even more aggressive in exploiting this redundancy to improve planning performance. finally  we are also studying the utility of memo-forgetting strategies such as relevance based learning  bayardo & schrag  1   as well as inter-problem memo transfer. 
