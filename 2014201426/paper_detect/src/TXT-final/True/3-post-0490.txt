
in this research we investigated user's behavior while facing a system coping with common knowledge about keywords and compared it with not only classic word-spotting method but also with random text-mining. we show how even a simple implementation of our idea can enrich the conversation and increase the naturalness of computer's utterances. our results show that even very commonsensical utterances are more natural than classic approaches and also methods we developed to make a conversation more interesting. for arousing opinion exchange during the session  we will also briefly introduce our idea of combining latest nlp achievements into one holistic system where the main engine we want to base on commonsense processing and affective computing.
1 introduction
during one of the nlp community meetings in japan we attended a discussion about evaluating the freely  meaning  in the open domain   talking computer systems. the participants agreed that such systems are almost ignored by computer scientists while so called  chatter-bots  as parry  colby  1  or eliza  weizenbaum  1  seem to be very often the only known yield of ai for other scientists and nonscholars. but today  even if passing the turing test is still questionable  we have faster and faster access to massive corpora  which we believe may incline researchers to rethink the need of naturally talking systems and we claim they might be done without much labor and cost. such renaissance would not only be important for the public recognition of the field but also could bring more young scholars fascinated by the fact that internet has a chance to replace programmer's will and change the program's behavior depending on the facts discovered in millions of web-pages. already rzepka et al.    has noticed such possibility proposing an automatically developing average personality of  mr. internet  and has confirmed that commonsense knowledge can be retrieved from the www. we want to go a step further and show that using even simple usage of commonsensical  s vo-then-v  verb-object and following verb  and  s vo-if- s vo  subjects vary depending on the conversation processing so ignored here  phrases text-mined from homepages improve the user's acceptation for talking systems.
1 commonsense knowledge retrieval
1 collected data structure
for the set of experiments we performed  we used only nouns as for decades they were usually the main part of opendomain chat programs  although we claim that te commonsensical context is the future of dialogue processing. using a nouns list and larbin robot we created 1 1 sentences www corpus which was a base for a verb  a noun and vo n-gram dictionaries. the noun and verb dictionaries consist of 1 verbs and 1 nouns extracted with help of chasen analyzer. for creating vo phrases automatically  our system had to search for the relationships between verbs and nouns and also between verbs. in this step  we used the verbs and nouns which had the highest occurrence and are common  as they are used in everyday live  for example  pour/drink/melt -water   listen/switch on/enjoy -music or  go to/buy at/enter convenient store. we used japanese language  which has useful grammar features like rentaikei where the verb suffix te usually joins verbs in a time sequence e.g. gohan wo tabe-te neru  to sleep after having a dinner  or tara  eba and to  if  forms which are able to distinguish different causal connotations. by these useful grammar features we are able to web-mine commonsensical knowledge as  it is usual that some people buy sweets at convenient store even if they didn't wanted . until now such data had to be collected manually singh  1  but full automatizing of such knowledge collecting brings new opportunities not only for dialogue but also for storytelling  question answering  machine translation and many other fields.
1 architecture overview
basically  our system's architecture for creating commonsensical data can be summarized into the following processing steps:
a  a noun of is assigned for a keyword;
b  the system uses our web corpus for frequency checkto retrieve 1 most frequent verbs following the keyword noun;
c  the most frequent particle between noun keyword and 1
most frequent verbs is discovered;
d  for creating bi-gram the system retrieves a list of mostfrequent verbs occurring after the previously chosen verb;
e  by using yahoo search engine  the system checks if thenoun-particle unit occurs with new verb-verb unit for time-sequence actions and verb-if unit for casual dependencies;
f  if yes - the vo-then-v and vo-if-vo units are stored:
vothenv=n+pmax+vmax1+vmax1
n:triggering noun  keyword ;
pmax:most frequent particle joining noun and verb;
vmax1:most frequent verb occurring after the n;
vmax1:most frequent verb occurring after vmax1;
voifv=n1+p1max+v1max+if+n1+p1max+v1max
n1:triggering noun  keyword ;
p1max:most freq. particle joining first noun with a verb;
v1max:most freq. verb after the n1;
n1max:most freq. noun after n1maxv1max and  if ;
p1max:most freq. particle joining n1 and v1;
v1max:most freq. verb after n1;
1 experiments
in order to see user's perception of the basic commonsense knowledge included in a utterance  we performed a set of experiments basically using four kinds of utterances following input with one noun keyword:
  eliza's output  eli   input sentence structure changing to achieve different outputs 
  www random retrieval output  wrr   a shortest of 1 sentences retrieved by using keyword and query pattern  did you know that   
  www commonsense retrieval output  high   cs1   sentences using common knowledge of highest usualness  most frequent mining results 
  www commonsense retrieval output  low   cs1   sentences using common knowledge of the lowest usualness  least frequent mining results .
typical eliza answer is  why do you want to talk about smoking  if the keyword is  smoking . for the same keyword wrr retrieved a sentence  did you know that people wearing contact lenses have well protected eyes when somebody is smoking  . an example of cs1 is  you will get fat when you quit smoking  and cs1 is  smoking may cause mouth  throat  esophagus  bladder  kidney  and pancreas cancers . we selected 1 most common noun keywords of different kinds  water  cigarettes  subway  voice  snow  room  clock  child  eye  meal  not avoiding ones often used in japanese idioms  voice  eye  to see if it influences the textmining results. 1 referees were evaluating every set of four utterances in two categories -  naturalness degree  and  will of continuing a conversation degree  giving marks from 1 to 1 in both cases.

figure 1:	naturalness level evaluation:	cs1.1% 
cs1%  wrr:1%  eli: 1%
1 results and conclusions
in  continuation will degree  eliza unsurprisingly achieved 1 points out of 1 for four systems  only 1% . but the performance of commonsenseical utterances was surprisingly high  cs1.1%   cs1.1%  which suggest that interlocutor prefers a machine saying  smoking is bad  than one naturally asking questions. the highest result of wrr  1%  tells us how simple tricks can help on keeping up the conversation. on the contrary  the naturalness degree results  see fig.1  show that the  tricks  of eliza and wrr and information overload of cs1 are less natural than the ordinary truth statements. due to the lack of space  more specific results analysis and graphs we are going to provide during the poster session.
