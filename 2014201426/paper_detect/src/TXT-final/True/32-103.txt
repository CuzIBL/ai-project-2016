 
boosting is a general method for improving the accuracy of any given learning algorithm. this short paper introduces the boosting algorithm adaboost  and explains the underlying theory of boosting  including an explanation of why boosting often does not suffer from overfitting. some examples of recent applications of boosting are also described. 
background 
boosting is a general method which attempts to  boost  the accuracy of any given learning algorithm. boosting has its roots in a theoretical framework for studying machine learning called the  pac  learning model  due to valiant ; see kearns and vazirani  for a good introduction to this model. kearns and valiant  1  1  were the first to pose the question of whether a  weak  learning algorithm which performs just slightly better than random guessing in the pac model can be  boosted  into an arbitrarily accurate  strong  learning algorithm. schapire  came up with the first provable polynomial-time boosting algorithm in 1. a year later  freund  developed a much more efficient boosting algorithm which  although optimal in a certain sense  nevertheless suffered from certain practical drawbacks. the first experiments with these early boosting algorithms were carried out by drucker  schapire and simard  on an ocr task. 
adaboost 
the adaboost algorithm  introduced in 1 by freund and schapire   solved many of the practical difficulties of the earlier boosting algorithms  and is the focus of this paper. pseudocode for adaboost is given in fig. 1. the algorithm takes as input a training set where each  belongs to some domain or instance space x  and each label  is in some label set y. for most of this paper  we assume later  we discuss extensions to the multiclass case. adaboost calls a given weak or base learning 
algorithm repeatedly in a series of rounds  with error 
 choose  update: where zt is a normalization factor  chosen so that dt+1 will be a distribution . 
output the final hypothesis: 

figure 1: the boosting algorithm adaboost. 
one of the main ideas of the algorithm is to maintain a distribution or set of weights over the training set. the weight of this distribution on training example on round t is denoted  initially  all weights are set equally  but on each round  the weights of incorrectly classified examples are increased so that the weak learner is forced to focus on the hard examples in the training set. 
   the weak learner's job is to find a weak hypothesis  appropriate for the distribution dt. the goodness of a weak hypothesis is measured by its error 

notice that the error is measured with respect to the 
	schapire 	1 


figure 1: error curves and the margin distribution graph for boosting c1 on the letter dataset as reported by schapire et al. . left  the training and test error curves  lower and upper curves  respectively  of the combined classifier as a function of the number of rounds of boosting. the horizontal lines indicate the test error rate of the base classifier as well as the test error of the final combined classifier. right the cumulative distribution of margins of the training examples after 1 and 1 iterations  indicated by short-dashed  long-dashed  mostly hidden  

and solid curves  respectively. 
distribution dt on which the weak learner was trained. 
　in practice  the weak learner may be an algorithm that can use the weights dt on the training examples. alternatively  when this is not possible  a subset of the training examples can be sampled according to dt  and these  unweighted  resampled examples can be used to train the weak learner. 
　　once the weak hypothesis has been received  adaboost chooses a parameter as in the figure. intuitively  at measures the importance that is assigned to ht. note that at   which we can assume 
without loss of generality   and that at gets larger as et gets smaller. 
　the distribution dt is next updated using the rule shown in the figure. the effect of this rule is to increase the weight of examples misclassified by ht  and to decrease the weight of correctly classified examples. thus  the weight tends to concentrate on  hard  examples. 
　the final hypothesis h is a weighted majority vote of the t weak hypotheses where at is the weight assigned to ht. 
　　schapire and singer  show how adaboost and its analysis can be extended to handle weak hypotheses which output real-valued or confidence-rated predictions. that is  for each instance x  the weak hypothesis ht outputs a prediction whose sign is the predicted label and whose magnitude  gives a measure of  confidence  in the prediction. 
analyzing the training error 
the most basic theoretical property of adaboost concerns its ability to reduce the training error. let us write the error  since a hypothesis that guesses each instance's class at random has an error rate of 1  on binary problems   thus measures how much 
1 	invited speakers 
better than random are  predictions. freund and schapire  prove that the training error  the fraction of mistakes on the training set  of the final hypothesis 
h is at most 
		 i  
thus  if each weak hypothesis is slightly better than random so that  for some  then the training error drops exponentially fast. 
　a similar property is enjoyed by previous boosting algorithms. however  previous algorithms required that such a lower bound  be known a priori before boosting begins. in practice  knowledge of such a bound is very difficult to obtain. adaboost  on the other hand  is adaptive in that it adapts to the error rates of the indi-
vidual weak hypotheses. this is the basis of its name -  ada  is short for  adaptive.  
　the bound given in eq.  1   combined with the bounds on generalization error given below prove that adaboost is indeed a boosting algorithm in the sense that it can efficiently convert a weak learning algorithm  which can always generate a hypothesis with a weak edge for any distribution  into a strong learning algorithm  which can generate a hypothesis with an arbitrarily low error rate  given sufficient data . 
generalization error 
fveund and schapire  showed how to bound the generalization error of the final hypothesis in terms of its training error  the size m of the sample  the vcdimension d of the weak hypothesis space and the num-


figure 1: comparison of c1 versus boosting stumps and boosting c1 on a set of 1 benchmark problems as reported by freund and schapire . each point in each scatterplot shows the test error rate of the two competing algorithms on a single benchmark. the y-coordinate of each point gives the test error rate  in percent  of c1 on the given benchmark  and the x-coordinate gives the error rate of boosting stumps  left plot  or boosting c1  right plot . all error rates have been averaged over multiple runs. 

ber of rounds t of boosting.  the vc-dimension is a standard measure of the  complexity  of a space of hypotheses. see  for instance  blumer et al. .  specifically  they used techniques from baum and haussler  to show that the generalization error  with high probability  is at most 

where denotes empirical probability on the training sample. this bound suggests that boosting will overfit if run for too many rounds  i.e.  as t becomes large. in fact  this sometimes does happen. however  in early experiments  several authors  1  1  1  observed empirically that boosting often does not overfit  even when run for thousands of rounds. moreover  it was observed that adaboost would sometimes continue to drive down the generalization error long after the training error had reached zero  clearly contradicting the spirit of the bound above. for instance  the left side of fig. 1 shows the training and test curves of running boosting on top of quinlan's c1 decision-tree learning algorithm  on the  letter  dataset. 
	in 	response 	to 	these 	empirical findings  
schapire et al.   following the work of bartlett   gave an alternative analysis in terms of the margins of the training examples. the margin of example  x  y  is defined to be 
it is a number in which is positive if and only if h correctly classifies the example. moreover  the magnitude of the margin can be interpreted as a measure of confidence in the prediction. schapire et al. proved that larger margins on the training set translate into a superior upper bound on the generalization error. specifically  the generalization error is at most 
for any with high probability. note that this bound is entirely independent of t  the number of rounds of boosting. in addition  schapire et al. proved that boosting is particularly aggressive at reducing the margin  in a quantifiable sense  since it concentrates on the examples with the smallest margins  whether positive or negative . boosting's effect on the margins can be seen empirically  for instance  on the right side of fig. 1 which shows the cumulative distribution of margins of the training examples on the  letter  dataset. in this case  even after the training error reaches zero  boosting continues to increase the margins of the training examples effecting a corresponding drop in the test error. 
　attempts  not always successful  to use the insights gleaned from the theory of margins have been made 
	schapire 	1 


figure 1: comparison of error rates for adaboost and four other text categorization methods  naive bayes  probabilistic tf-idf  rocchio and sleeping experts  as reported by schapire and singer . the algorithms were tested 
on two text corpora - reuters newswire articles  left  and ap newswire headlines  right  - and with varying numbers of class labels as indicated on the x-axis of each figure. 

by several authors  1  1  1 . in addition  the margin theory points to a strong connection between boosting and the support-vector machines of vapnik and others  1  1  1  which explicitly attempt to maximize the minimum margin. 
　the behavior of adaboost can also be understood in a game-theoretic setting as explored by freund and schapire  1  1   see also grove and schuurmans  and breiman 1  . in particular  boosting can be viewed as repeated play of a certain game  and adaboost can be shown to be a special case of a more general algorithm for playing repeated games and for approximately solving a game. this also shows that boosting is related to linear programming. 
multiclass classification 
there are several methods of extending adaboost to the multiclass case. the most straightforward generalization   called adaboost.ml  is adequate when the weak learner is strong enough to achieve reasonably high accuracy  even on the hard distributions created by adaboost. however  this method fails if the weak learner cannot achieve at least 1% accuracy when run on these hard distributions. 
　　for the latter case  several more sophisticated methods have been developed. these generally work by reducing the multiclass problem to a larger binary problem. schapire and singer's  algorithm adaboost.mh works by creating a set of binary problems  for each example x and each possible label of the form:  for example x  is the correct label or is it one of the other labels   freund and schapire's  algorithm adaboost m1  which is a special case of schapire and 
singer's  adaboost mr algorithm  instead creates binary problems  for each example with correct label and each incorrect label of the form:  for example . is the correct label or  
1 	invited speakers 
　these methods require additional effort in the design of the weak learning algorithm. a different technique   which incorporates dietterich and bakiri's  method of error-correcting output codes  achieves similar provable bounds to those of adaboost.mh and adaboost.m1  but can be used with any weak learner which can handle simple  binary labeled data. schapire and singer  give yet another method of combining boosting with error-correcting output codes. 
experiments and applications 
practically  adaboost has many advantages. it is fast  simple and easy to program. it has no parameters to tune  except for the number of round t . it requires no prior knowledge about the weak learner and so can be flexibly combined with any method for finding weak hypotheses. finally  it comes with a set of theoretical guarantees given sufficient data and a weak learner that can reliably provide only moderately accurate weak hypotheses. this is a shift in mind set for the learning-system designer: instead of trying to design a learning algorithm that is accurate over the entire space  we can instead focus on finding weaking learning algorithms that only need to be better than random. 
　on the other hand  some caveats are certainly in order. the actual performance of boosting on a particular problem is clearly dependent on the data and the weak learner. consistent with theory  boosting can fail to perform well given insufficient data  overly complex weak hypotheses or weak hypotheses which are too weak. boosting seems to be especially susceptible to noise . 
　adaboost has been tested empirically by many researchers  including  1  1  1  1  1  1  1 . for instance  freund and schapire  tested adaboost on a set of uci benchmark datasets  using c1  as a weak learning algorithm  as well as an algorithm which 


figure 1: a sample of the examples that have the largest weight on an ocr task as reported by freund and schapire . these examples were chosen after 1 rounds of boosting  top line   1 rounds  middle  and 1 rounds 
 bottom . underneath each image is a line of the form  where d is the label of the example  l  and l1 are the labels that get the highest and second highest vote from the combined hypothesis at that point in the run of the algorithm  and w1  w1 axe the corresponding normalized scores. 
finds the best  decision stump  or single-test decision tree. some of the results of these experiments are shown in fig. 1. as can be seen from this figure  even boosting the weak decision stumps can usually give as good results as c1  while boosting c1 generally gives the decision-tree algorithm a significant improvement in performance. 
　in another set of experiments  schapire and singer  used boosting for text categorization tasks. for this work  weak hypotheses were used which test on the presence or absence of a word or phrase. some results of these experiments comparing adaboost to four other methods are shown in fig. 1. in nearly all of these experiments and for all of the performance measures tested  boosting performed as well or significantly better than the other methods tested. boosting has also been applied to text filtering  and  ranking  problems . 
　a nice property of adaboost is its ability to identify outliers  i.e.  examples that are either mislabeled in the training data  or which are inherently ambiguous and hard to categorize. because adaboost focuses its weight on the hardest examples  the examples with the highest weight often turn out to be outliers. an example of this phenomenon can be seen in fig. 1 taken from an ocr experiment conducted by freund and schapire . 
references 
 peter l. bartlett. the sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. ieee transactions on information theory  1 :1  march 1. 
 eric bauer and ron kohavi. an empirical comparison of voting classification algorithms: bagging  boosting  and variants. machine learning  to appear. 
 eric b. baum and david haussler. what size net gives valid generalization  neural computation  1 :1 1. 
 anselm blumer  andrzej ehrenfeucht  david haussler  and manfred k. warmuth. leamability and the vapnik-chervonenkis dimension. journal of the association for computing machinery  1 :1  october 1. 
 bernhard e. boser  isabelle m. guyon  and vladimir n. vapnik. a training algorithm for optimal margin classifiers. in proceedings of the fifth annual acm workshop on computational learning theory  pages 1 1. 
 leo breiman. arcing the edge. technical report 1  statistics department  university of california at berkeley  1. 
 leo breiman. prediction games and arcing classifiers. technical report 1  statistics department  university of california at berkeley  1. 
 leo breiman. arcing classifiers. the annals of statistics  1 :1  1. 
 corinna cortes and vladimir vapnik. supportvector networks. machine learning  1 :1  september 1. 
 thomas g. dietterich. an experimental comparison of three methods for constructing ensembles of decision trees: bagging  boosting  and randomization. machine learning  to appear. 
 thomas g. dietterich and ghulum bakiri. solving multiclass learning problems via error-correcting output codes. journal of artificial intelligence research  1-1  january 1. 
 harris drucker and corinna cortes. boosting decision trees. in advances in neural information processing systems 1  pages 1  1. 
 harris drucker  robert schapire  and patrice simard. boosting performance in neural networks. 
international journal of pattern recognition and artificial intelligence  1 :1  1. 
 yoav freund. boosting a weak learning algorithm by majority. information and computation  1 :1  1. 
 yoav freund  raj iyer  robert e. schapire  and yoram singer. an efficient boosting algorithm for combining p