 
domain-specific search engines are becoming increasingly popular because they offer increased accuracy and extra features not possible with general  web-wide search engines. unfortunately  they are also difficult and timeconsuming to maintain. this paper proposes the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific search engines. we describe new research in reinforcement learning  text classification and information extraction that enables efficient spidering  populates topic hierarchies  and identifies informative text segments. using these techniques  we have built a demonstration system: a search engine for computer science research papers available at www.cora.justrcsettrch.com. 
1 introduction 
as the amount of information on the world wide web grows  it becomes increasingly difficult to find just what we want. while general-purpose search engines such as altavista and hotbot offer high coverage  they often provide only low precision  even for detailed queries. 
　when we know that we want information of a certain type  or on a certain topic  a domain-specific search engine can be a powerful tool. for example  www.cmnpsearch.com allows complex queries over summer camps by age-group  size  location and cost. performing such searches with a traditional  generalpurpose search engine would be extremely tedious or impossible. for this reason  domain-specific search engines are becoming increasingly popular. unfortunately  building these search engines is a labor-intensive process  typically requiring significant and ongoing human effort. 
　this paper describes the ra project-an effort to automate many aspects of creating and maintaining domainspecific search engines by using machine learning techniques. these techniques allow search engines to be created quickly with minimal effort  and are suited for re-use across many domains. this paper presents machine learning methods for effident topic-directed spider-
1 	machine learning 
ing  building a browsable topic hierarchy  and extracting topic-relevant substrings. these are briefly described in the following three paragraphs. 
　every search engine must begin with a collection of documents to index. when aiming to populate a domain-specific search engine  a web-crawling spider need not explore the web indiscriminantly  but should explore in a directed fashion to find domain-relevant documents efficiently. we frame the spidering task in a reinforcement learning framework  kaelbling et a/.  1   allowing us to mathematically define  optimal behavior.  our experimental results show that a simple reinforcement learning spider is three times more efficient than a 
　spider using breadth-first search. 
　search engines often provide a browsable topic hierarchy; yahoo is the prototypical example. automatically adding documents into a topic hierarchy can be posed as a text classification task. we present extensions to the naive bayes text classifier  e.g.  mccallum et a/.  1   
that use no hand-labeled training data  yet still result in accurate classification. using only unlabeled data  the hierarchy and afew keywords for each category  the algorithm combines naive bayes  hierarchical shrinkage and expectation-maximization. it places documents into a 
1-leaf computer science hierarchy with 1% accuracy- performance approaching human agreement levels. 
　extracting topic-relevant pieces of information from the documents of a domain-specific search engine allows the user to search over these features in a way that general search engines cannot. information extraction  the process of automatically finding specific textual substrings in a document  is well suited to this task. we approach information extraction with techniques used in statistical language modeling and speech recognition  namely hidden markov models  rabiner  1 . our algorithm extracts fields such as title  authors  and affiliation from research paper headers with 1% accuracy. 
　we have brought all the above-described machine learning techniques together in cora  a publicly-available search engine on computer science research papers  www.cora.justresearch.com . an intelligent spider starts from the home pages of computer science departments and laboratories and collects links to postscript documents. these documents are converted to plain text 


figure 1: a screen shot of the query results page of the 
cora search engine. note the topic hierarchy and the extracted paper titles  authors and abstracts. 
and further processed if they are determined to be research papers  e.g. by having abstract and reference sections . important identifying information such as the title and author is then extracted from the head of each paper  as well as the bibliography section. the extracted results are used to group citations to the same paper together and to build a citation graph. phrase and keyword search facilities over the collected papers are provided  as well as a computer science topic hierarchy which lists the most-cited papers in each research topic. figure 1 shows the results of a search query as well as the topic hierarchy. our hope is that  in addition to providing a platform for machine learning research  this search engine will become a valuable tool for other computer scientists. the following sections describe the new research that makes cora possible; more detail is provided in mccallum et al. {1  and by other papers available at cora's web page. 
1 efficient spidering 
in cora  efficient spidering is a significant concern. many of the pages in cs department web sites are about courses and administration  not research papers. while a general-purpose search engine should index all pages  and might use breadth-first search to collect documents  cora need only index a small subset. avoiding whole regions of departmental web graphs can significantly improve efficiency and increase the number of research papers found given a finite amount of time. 
for a formal setting in which to frame the problem 
of efficient spidering  we turn to reinforcement learning. reinforcement learning is a framework for leaning optimal decision-making from rewards or punishment  kaelbling et at.  1 . the agent learns a policy that maps states to actions in an effort to maximize its reward over time. we use the infinite-horizon discounted model where reward over time is a geometrically discounted sum in which the discount  1   r   1  devalues rewards received in the future. a q-function represents the policy by mapping state-action pairs to their expected discounted reward. policy decisions are made by selecting the action with the largest q-value. 
　as an aid to understanding how reinforcement learning relates to spidering  consider the common reinforcement learning task of a mouse exploring a maze to find several pieces of cheese. the agent receives immediate reward for finding each piece of cheese  and has actions for moving among the grid squares of the maze. the state is both the position of the mouse and the locations of the cheese pieces remaining to be consumed  since the cheese can only be consumed and provide reward once . note that in order to act optimally  the agent most consider future rewards. 
　in the spidering task  the on-topic documents are immediate rewards  like the pieces of cheese. an action is following a particular hyperlink. the state is the set of on-topic documents remaining to be consumed  and the set of hyperlinks that have been discovered. the key feature of topic-specific spidering that makes reinforcement learning the proper framework is that the environment presents situations with delayed reward. 
　the problem now is how to practically apply reinforcement learning to spidering. the state-space is enormous and does not allow the spider to generalize to hyperlinks that it has not already seen. hence  we make simplifying assumptions that  1  disregard state and  1  capture the relevant distinctions between actions using only the words found in the neighborhood of the corresponding hyperlink. thus our q-function becomes a mapping from a  bag-of-words  to a scalar. 
　we represent the mapping using a collection of naive bayes text classifiers  see section 1   and cast this regression problem as classification. we discretize the discounted sum of future reward values of our training data into bins  place each hyperlink into the bin corresponding to its q-value  calculated as described below   and use the text in the hyperlink's anchor and surrounding page as training data for the classifier. at test time  the estimated q-value of a hyperlink is the weighted average of each bin's average q-value  using the classifier's probabilistic class memberships as weights. 
　other systems have also studied spidering  but without a framework defining optimal behavior. for example  arachnid  menczer  1  does so with a collection of competitive  reproducing and mutating agents. additionally  there are systems that use reinforcement learning for non-spidering web tasks. web watcher  joachims et a/.  1  is a browsing assistant that uses a combination of supervised and reinforcement teaming to rec-
mccallum  nigam  rennie  and seymore 	g1 


figure 1: average performance of two reinforcement learning spiders versus traditional breadth-first search. 
ommend relevant hyperlinks to the user. laser uses reinforcement leaming to tune the search parameters of a search engine  boyan et at  1 . 
1 experimental results 
in august 1 we fully mapped the os department web sites at brown university  cornell university  university of pittsburgh and university of texas. they include 1 documents and 1 hyperlinks. we perform four test/tram splits  where the data from three universities is used to train a spider that is tested on the fourth. the target pages  for which a reward of 1 is given  are computer science research papers  identified separately by a simple hand-coded algorithm with high precision. 
　we currently train the agent off-line. we find all target pages in the training data  and calculate the q-value associated with each hyperlink as the discounted sum of rewards that result from executing the optimal policy  as determined fay full knowledge of the web graph . the agent amid also learn from experience on-line using  a spider is evaluated cm each test/train split by having it spider the test university  starting at the department's home page. in figure 1 we report results from traditional breadth-first search as well as two different reinforcement learners. immediate uses  = 1  and represents the q-function as a binary classifier between immediate rewards and other hyperlinks. future uses = 1  and represents the q-function with a more finely-discriminating 1-bin classifier that uses future rewards. 
　at all times during its progress  both reinforcement learning spiders have found more research papers than breadth-first search. one measure of performance is the number of hyperlinks followed before 1% of the research papers are found. both reinforcement learners are significantly more efficient  requiring exploration of less than 1% of the hyperlinks; in comparison  breadth-first requires 1%. this represents a factor of three increase in spidering efficiency. 
note that the future reinforcement learning spider 
i l l 	machine learning 
performs better than the immeadiate 
alternative branches  none of which give immediate reward. on average the immediate spider takes nearly three times as long as future to find the first 1  1%  of the papers. however  after the first 1% of the papers are found  the immediate spider performs slightly better  because many links with immediate reward have been discovered  and the immediate spider recognizes them more accurately. in ongoing work we are improving the accuracy of the classification when there is future reward and a larger number of bins. we have also run experiments cm tasks with a single target page  where future reward decisions are more crucial. in this case  the future spider retrieves target pages twice as efficiently as the immediate spider  rennie and mccallum  1 . 
1 classification into a hierarchy by bootstrapping with keywords 
topic hierarchies are an efficient way to organize and view large quantities of information that would otherwise be cumbersome. as yahoo has shown  a topic hierarchy can be an integral part of a search engine. for cora  we have created a 1-leaf hierarchy of computer science topics  the top part of which is shown in figure 1. creating the hierarchy structure and selecting just a few keywords associated with each node took about three hours  during which an expert examined conference proceedings and computer science web sites. 
　a much more difficult and time-consuming part of creating a hierarchy is placing documents into the correct topic nodes. yahoo has hired many people to categorize web pages into their hierarchy. in contrast  mar chine learning can automate this task with supervised text classification. however  acquiring enough labeled training documents to build an accurate classifier is often prohibitively expensive. 
　in this paper  we ease the burden on the classifier builder by using only unlabeled data  some keywords and the class hierarchy. instead of asking the builder to hand-label training examples  the builder simply provides a few keywords for each category. a large collection of unlabeled documents are then preliminarily labeled by using the keywords as a rule-list classifier  searching a document for each keyword and placing it in the class of the first keyword found . these preliminary labels are noisy  and many documents remain unlabeled. however  we then bootstrap an improved classifier. using the documents and preliminary labels  we initialize a naive bayes text classifier from the preliminary labels. then  expectation-maximization  dempster et al  1  estimates labels of unlabeled documents and re-estimates labels of keyword-labeled documents. statistical shrinkage is also incorporated in order to improve parameter estimates by using the class hierarchy. in this paper we combine for the first time in one document classifier both em for unlabeled data and hierarchical shrinkage. 

we use the framework of multinomial naive bayes text classification. the parameters of this model are  for each class the frequency with which each word wt occurs  and the relative document frequency of each class  given estimates of these parameters and a document di  we can determine the probability that it belongs in class cj by bayes' rule: 
 1  
where is the word wt that occurs in the the pesition of document di. training a standard naive bayes classifier requires a set of documents  d  and their class labels. the estimate of a word frequency is simply the smoothed frequency with which the word occurs in training documents from the class: 

where n wt  di  is the number of times word wt occurs in document is an indicator of whether document di belongs in class cj  and is the number of words in the vocabulary. similarly  the class frequencies  are smoothed document frequencies estimated from the data. 
　when a combination of labeled and unlabeled data is available  past work has shown that naive bayes parameter estimates can be improved by using em to combine evidence from all the data  nigam et a/.  1 . in our bootstrapping approach  an initial naive bayes model is estimated from the preliminarily-labeled data. then 
em iterates until convergence  1  labeling all the data 
 equation 1  and  1  rebuilding a model with all the data  equation 1 . the preliminary labels serve to provide a good starting point; em then incorporates the unlabeled data and re-estimates the preliminary labels. 
　when the classes are organized hierarchically  as is our case  naive bayes parameter estimates can be improved with the statistical technique shrinkage  mccallum et of.  1 . new word frequency parameter estimates for 
a class are calculated by a weighted average between the class's local estimates  and estimates of its ancestors in the hierarchy  each formed by pooling data from all the ancestor's children . the technique balances a tradeoff between the specificity of the unreliable local word frequency estimates and the reliability of the more general ancestor's frequency estimates. the optimal mixture weights for the weighted average are calculated by em concurrently with the class labels. 
1 	experimental results 
now we describe results of classifying computer science research papers into our 1-leaf hierarchy. a test set was created by hand-labeling a random sample of 1 research papers from the 1 papers formerly comprising the entire cora archive. of these  1 did not fit into any category  and were discarded. in these experiments  we used the title  author  institution  references  and abstracts of papers for classification  not the full text. 
　traditional naive bayes with 1 labeled training documents  tested in a leave-one-out fashion  results in 1% classification accuracy. however  less than 1 documents could have been hand-labeled in the 1 minutes it took to create the keyword-lists; using this smaller training set results in only 1% accuracy. the ralelist classifier based on the keywords alone provides 1%. we now turn to our bootstrap approach. when these noisy keyword-labels are used to train a traditional naive bayes text classifier  1% accuracy is readied on the test set. the full algorithm  including em and hierarchical shrinkage  achieves 1% accuracy. as an interesting comparison  human agreement between two people on the test set was 1%. 
　these results demonstrate the utility of the bootstrapping approach. keyword matching alone is noisy  but when naive bayes  em and hierarchical shrinkage are used together as a regularize  the resulting classification accuracy is close to human agreement levels. automatically creating preliminary labels  either from keywords or other sources  avoids the significant human effort of hand-labeling training data. 
　in future work we plan to refine our probabilistic model to allow for documents to be placed in interior hierarchy nocks  documents to have multiple class assignments  and multiple mixture components per class. we are also investigating principled methods of re-weighting the word features for  semi-supervised  clustering that will provide better discriminative training with unlabeled data. 
1 information extraction 
information extraction is concerned with identifying phrases of interest in textual data. in the case of a search engine over research papers  the automatic extraction of informative text segments can be used to  1  allow searches over specific fields   1  provide useful effective presentation of search results  e.g. showing title in bold   and  1  match references to papers. we have investigated techniques for extracting the fields relevant to research papers  such as title  author  and journal  from both the headers and reference sections of papers. 
　our information extraction approach is based on hidden markov models  hmms  and their accompanying search techniques that are widely used for speech recognition and part-of-speech tagging  rabiner  1 . discrete output  first-order hmms are composed of a set of states q  which emit symbols from a discrete vocabulary  and a set of transitions between states  a 
common goal of learning problems that use hmms is to recover the state sequence  that has the highest probability of having produced some observation sequence 
mccallum  nigam  rennie  and seymore 	g1 


figure 1: illustrative example of an hmm for reference extraction. 

is the probability of state qk emitting output symbol xk. the viterbi algorithm  viterbi  1  can be used to efficiently recover this state sequence. 
　hmms may be used for extracting information from research papers by formulating a model in the following way: each state is associated with a class that we want to extract  such as title  author  or affiliation. each state emits words from a class-specific unigram distribution. in order to label new text with classes  we treat the words from the new text as observations and recover the mostlikely state sequence. the state that produces each word is the class tag for that word. an illustrative example of an hmm for reference extraction is shown in figure 1. 
　our weak with hmms for information extraction focuses on learning the appropriate model structure  the number of states and transitions  automatically from data. other systems using hmms for information extraction include that by leek   which extracts information about gene names and locations from scientific abstracts  and the nymble system  bikel et at  1  for named-entity extraction.. these systems do not consider automatically determining model structure from data; they either use one state per class  or use hand-built models assembled by inspecting training examples. 
1 	experiments 
the goal of our information extraction experiments is to investigate whether a model with multiple states per class  either manually or automatically derived  outperforms a model with only one state per class for header extraction. we define the header of a research paper to be all of the words from the beginning of the paper 
hi 	machine learning 
up to either the first section of the paper  usually the introduction  or to the end of the first page  whichever occurs first. a single token  either +intro+ or +page+  is added to the end of each header to indicate the case with which it terminated. likewise  the abstract is automatically located and substituted with the single token +abstract+. a few special classes of words are identified using simple regular expressions and converted to tokens such as +email+. all punctuation  case and newline information is removed from the text. the target classes we wish to identify include the following fifteen categories: title  author  affiliation  address  note  email  date  abstract  introduction  phone  keywords  web  degree  publication number  and page. 
manually tagged headers are split into a 1-header  
1 word token labeled training set and a 1-header  1 word token test set. unigram language models are built for each class and smoothed using a modified form of absolute discounting. each state uses its class unigram distribution as its emission distribution. 
　we compare the performance of a model with one state per class  baseline  to that of models with multiple states par class  m-merged  v-merged . the multi-state models are derived from training data in the following way: a maximally-specific hmm is built where each word token in the training set is assigned a single state that only transitions to the state that follows it. each state is associated with the class label of its word token. then  the hmm is put through a series of state merges in order to generalize the model. first   neighbor merging  combines all states that share a unique transition and have the same class label. for example  all adjacent title states are merged into one title state. as two states are merged  transition counts are preserved  introducing a self-loop on the new merged state. the neighbor-merged model is used as the starting point for the two multi-state models. manual marge decisions are made in an iterative manner to produce the m-merged model  and an automatic forward and backward v-merging procedure is used to produce the v-merged model. v-merging consists of merging any two states that share transitions from or to a commcm state and have the same label. transition probabilities for the three models are set to their maximum likelihood estimates; the baseline model takes its transition counts directly from the labeled training data  whereas the multi-state models use the counts that have been preserved during the state merging process. 
　model performance is measured by word classification accuracy  which is the percentage of header words that are emitted by a state with the same label as the words' true label. extraction results are presented in table 1. hidden markov models do well at extracting header information; the best performance of 1% is obtained with the m-merged model. both of the multi-state models outperform the baseline model  indicating that the richer representation available through models derived from data is beneficial. however  the automaticallyderived v-merged model does not perform as well as the manually-derived m-merged model. the v-merged model 

model number of states number of transitions accuracy | baseline 1 1 	1 	| m-merged 1 1 1 | v-merged 1 1 1 table 1: extraction accuracy  %  for the baseline  mmerged and v-merged models. 
is limited in the state merges it can perform  whereas the m-merged model is unrestricted. we expect that more sophisticated state merging techniques  as discussed in  seymore et al.  1   will result in superior-performing models for information extraction. 
1 related work 
several related research projects are investigating the automatic construction of special-purpose web sites. the new zealand digital library project  witten et al.  
1  has created publicly-available search engines for domains from computer science technical reports to song melodies using manually identified web sources. the citeseer project  bollacker et al  1  has also developed a search engine for computer science research papers that provides similar functionality for matching references and searching. the webkb project  craven et al.  1  uses machine learning techniques to extract 
domain-specific information available on the web into a knowledge base. the whirl project  cohen  1  is an effort to integrate a variety of topic-specific sources into a 
single domain-specific search engine using html-based extraction patterns and fuzzy matching for information retrieval searching. 
1 conclusions 
the amount of information available on the internet continues to grow exponentially. as this trend continues  we argue that  not only will the public need powerful tools to help them sort though this information  but the creators of these tools will need intelligent techniques to help them build and maintain these tools. this paper has shown that machine learning techniques can significantly aid the creation and maintenance of domainspecific search engines. we have presented new research in reinforcement learning  text classification and information extraction towards this end. in future work  we will apply machine learning to automate more aspects of domain-specific search engines  such as creating a topic hierarchy with clustering and automatically identifying seminal papers with citation graph analysis. we will also verify that the techniques in this paper generalize by applying them to a new domain. 
