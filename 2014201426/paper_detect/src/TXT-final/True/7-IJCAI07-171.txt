
in this paper  we propose a graph-based construction of semi-supervised gaussian process classifiers. our method is based on recently proposed techniques for incorporating the geometric properties of unlabeled data within globally defined kernel functions. the full machinery for standard supervised gaussian process inference is brought to bear on the problem of learning from labeled and unlabeled data. this approach provides a natural probabilistic extension to unseen test examples. we employ expectation propagation procedures for evidence-based model selection. in the presence of few labeled examples  this approach is found to significantly outperform cross-validation techniques. we present empirical results demonstrating the strengths of our approach.
1 introduction
practitioners of machine learning methods frequently encounter the following situation: large amounts of data can be cheaply and automatically collected  but subsequent labeling requires expensive and fallible human participation. this has recently motivated a number of efforts to design semisupervised inference algorithms that aim to match the performance of well-trained supervised methods  using a small pool of labeled examples and a large collection of unlabeled data. the scarcity of labeled examplesgreatly exaggeratesthe need to incorporate additional sources of prior knowledge  to perform careful model selection  and to deal with noise in labels. a bayesian framework is ideally suited to handle such issues. in this paper  we construct semi-supervised gaussian processes  and demonstrate and discuss its practical utility on a number of classification problems.
¡¡our methods are based on the geometric intuition that for many real world problems  unlabeled examples often identify structures  such as data clusters or low dimensional manifolds  whose knowledge may potentially improve inference. for example  one might expect high correlation between class labels of data points within the same cluster or nearby on a manifold. these  respectively  are the cluster and manifold assumptions for semi-supervised learning.
¡¡we utilize the graph-based construction of semi-supervised kernels in  sindhwani et al.  1 . the data geometry is modeled as a graph whose vertices are the labeled and unlabeled examples  and whose edges encode appropriate neighborhood relationships. in intuitive terms  in the limit of infinite unlabeled data we imagine a convergence of this graph to the underlying geometric structure of the probability distribution generating the data. the smoothness enforced by regularization operators  such as the graph laplacian  over functionson the vertices of this graphis transferredonto a reproducing kernel hilbert space  rkhs  of functions defined over the entire data space. this gives rise to a new rkhs in which standard supervised kernel methods perform semisupervised inference. in this paper  we apply similar ideas for gaussian processes  with an aim to draw the benefits of a bayesian approach when only a small labeled set is available. the semi-supervised kernel of  sindhwani et al.  1  is motivated through bayesian considerations  and used for performing gaussian process inference. we apply expectation propagation  ep   see  rasmussen and williams  1  and references therein  for approximating posterior processes and calculating evidence for model selection. we point out some aspects of the proposed method:
¡¡a  our method  hereafter abbreviated as ssgp  can be seen as providing a bayesian analogue of laplacian support vector machines  lapsvm  and laplacian regularized least squares  laprls  proposed in  sindhwani et al.  1; belkin et al.  1  and semi-supervised logistic regression proposed in  krishnapuram et al.  1 . in this paper  we empirically demonstrate that when labeled examples are scarce  model selection by evidence in ssgp is a significant improvement over cross-validation in lapsvm and laprls.
¡¡b  several graph-based bayesian approaches incorporating unlabeled data have recently been proposed. methods have been designed for transductive bayesian learning using a graph-based prior in  zhu et al.  1; kapoor et al.  1 . since the core probabilistic model in these methods is defined only over the finite collection of labeled and unlabeled inputs  out-of-sample extension to unseen test data requires additional follow-up procedures. by contrast  our approach possesses a gaussian process model over the entire input space that provides natural out-of-sample prediction.
¡¡c  while the focus of this paper is to derive ssgp and verify its usefulness on binary classification tasks  we wish to comment on its potential in the following respects:  1  ssgp also provides a general framework for other learning tasks  such as regression  multiple-class classification and ranking.  1  ssgp also produces class probabilities and confidence estimates. these can be used to design active learning schemes.  1  efficient gradient-based strategies can be designed for choosing multiple model parameters such as regularization and kernel parameters.
¡¡we begin by briefly reviewing gaussian process classification in section 1 and then propose and evaluate ssgp in the following sections.
1 background and notation
in the standard formulation for learning from examples  patterns x are drawn from some space x  typically a subset of rd and associated labels y come from some space y. for ease of presentation  in this paper we will focus on binary classification  so that y can be taken as {+1  1}. we assume there is an unknown joint probability distribution
px¡Áy over the space of patterns and labels. a set of pat is drawn i.i.d from the marginal px x  =
; the label yi associated with xi is drawn from the conditional distribution py y|xi  for the first l patterns. we are interested in leveraging the unlabeled patterns  for improved inference.
¡¡to set our notation for the discussion ahead  we will denote xl = {xi}li=1 as the set of labeled examples with associated labels as the set of unlabeled patterns. we further denote all the given patterns as xd  i.e.  xd = xl ¡È xu. other patterns that are held out for test purposes  or have not been collected yet  are denoted as xt. the set of all patterns  labeled  unlabeled and test  is denoted as x  i.e.  x = xd ¡Èxt  but we will often also use x to denote a generic dataset.
¡¡in the standard gaussian process setting for supervised learning  one proceeds by choosing a covariance function
. with any point x ¡Ê x  there is an associated latent variable fx. given any dataset x  the latent variables fx = {fx}x¡Êx are treated as random variables in a zero-mean gaussian process indexed by the data points. the covariance between fx and fz is fully determined by the coordinates of the data points x and z  and is given by k x z . thus  the prior distribution over these variables is a multivariate gaussian  written as: p fx  = n 1 ¦²xx  where ¦²xx is the covariance matrix with elements k x z  with x z ¡Ê x.
¡¡the gaussian process classification model relates the variable fx at x to the label y through a probit noise model such as y = sign fx + ¦Î  where ¦Î ¡« n 1 ¦Òn1 . given the latent variables fl associated with labeled data points  the class labels yl are independent bernoulli variables and their joint likelihood is given by:  where ¦µ is the cumulative density function
of the normal distribution.
¡¡combining this likelihood term with the gaussian prior for the latent variables associated with the labeled dataset xl  we obtain the posterior distribution: p fl|yl  =  where we use ¦²ll as a
 shorthand for ¦²xlxl. the normalization factor p yl  =  is known as the evidence for the model parameters  such as parameters of the covariance function and the noise level ¦Òn1 .
   the posterior distribution above is non-gaussian. to preserve computational tractability  a family of inference techniques can be applied to approximate the posterior as a gaussian. some popular methods include laplace approximation  mean-field methods  and expectation propagation  ep . in this paper we will use the ep procedure  based on empirical findings that it outperforms laplace approximation. in ep  the key idea is to approximate the non-gaussian part in the posterior in the form of an un-normalized gaussian  i.e. . the parameters c ¦Ì a are
obtained by locally minimizing the kullback-liebler divergence between the posterior and its approximation.
¡¡with a gaussian approximation to the posterior in hand  the distribution of the latent variable fxt at a test point xt  as given by the following integral  becomes a tractable quantity:
 
where ¦Ìt = ¦²tlt¦² ll1 ¦Ì and ¦Òt1 = k xt xt  ¦²tlt  ¦² ll1  
¦² 1 a ¦² 1  ¦²lt. here  the column vector ¦²lt = and ¦²ll is the gram matrix
of k over the labeled inputs. the conditional distribution p fxt|fl  is a multi-variate gaussian with mean and covariance matrix.
¡¡finally  one can compute the bernoulli distribution over the test label yt  which for the probit noise model becomes
.	for more details on
gaussian processes  we point the reader to  rasmussen and williams  1  and references therein.
¡¡our interest now is to utilize unlabeled data for gaussian process inference.
1 semi-supervised gaussian processes
1 kernels for semi-supervised learning
a symmetric positive semi-definite function k ¡¤ ¡¤  can serve as the covariance function of a gaussian process and also as a kernel function of a deterministicreproducingkernel hilbert
space  rkhs  h of functions x ¡ú . the rkhs and the
gp associated with the same function k ¡¤ ¡¤  are closely related through a classical isometry between h and a hilbert space of random variables spanned by the gp  i.e. random variables of the form and their mean square limits 1. in this section  we review the construction of an rkhs that is adapted for semi-supervised learning of deterministic classifiers  sindhwani et al.  1 . the kernel of this rkhs is then used as the covariance function of ssgp.
¡¡in the context of learning deterministic classifiers  for many choices of kernels  the normcan be interpreted as a smoothness measure over functions f in h. the norm can then be used to impose a complexity structure over h and learning algorithms can be developed based on minimizing functionals of the form:  where v is a loss function that measures how well f fits the data. remarkably  for loss functions that only involve f through point evaluations  a representer theorem states that the minimizer is of the form so that only the ¦Ái remain to be computed. this provides the algorithmic basis of algorithmslike regularized least squares  rls  and support vector machines  svm   for squared loss and hinge loss  respectively. in a semi-supervised setting  unlabeled data may suggest alternate measures of complexity such as smoothness with respect to data manifolds or clusters. thus  if the space h contains functions that are smooth in these respects  it is only required to re-structure h by refining the norm using labeled and labeled data xd. a general procedure to perform this operation is as follows: we define h  to be the space of functions from h with the modified data-dependent inner product: g  where f and g are the vectors {f x }x¡Êxd and {g x }x¡Êxd respectively  and m is a symmetric positive semi-definite matrix. the norm induced by this modified inner product combines the original ambient smoothness with an intrinsic smoothness measure defined in terms of the matrix m. the definition of m is based on the construction of a data adjacency graph that acts as an empirical substitute of the intrinsic geometry of the marginal px. m can be derived from the graph laplacian l; for example  are popular choices for families of graph regularizers. the laplacian matrix of the graph implements an empirical version of the laplace-beltrami operator when the underlying space is a riemannian manifold. this operator measures smoothness with respect to the manifold. the space h  can be shown to be an rkhs. with the new data-dependentnorm  h  becomes better suited  as compared to the original function space h  for semi-supervised learning tasks where the cluster/manifold assumptions hold. the form of the new kernel k  associated with h  can be derived  see  sindhwani et al.  1   in terms of the kernel function k using reproducing properties of an rkhs and orthogonality arguments  and is given by:
   k  x z  = k x z    ¦²tdx i + m¦²dd  1m¦²dz  1  where ¦²dx  and similarly ¦²dz  denotes the column vector:  k x1 x  ... k xl+u x  t. laplacian svm and laplacian rls solve regularization problems in the rkhs h  with hinge and squared loss respectively.
1 data-dependent conditional prior
ssgp uses the semi-supervised kernel function k  defined above as a covariance function for gaussian process learning. thus  in ssgp the covariance between fxi and fxj not only depends on the ambient coordinates of xi and xj  but also on geometric properties of the set xd. in this section  we discuss the derivation of k  from bayesian considerations. our general approach is summarized as follows. we begin with a standard gaussian process. given unlabeled and labeled data xd  we define a joint probability distribution over the associated latent process variables fd and an abstract collection of random variables  denoted as g  whose instantiation is interpreted as realization of a certain geometry. then  conditioning on the geometry of unlabeled data through these variables  a bayesian update gives a posterior over the latent variables. ssgp is the resulting posterior process  which incorporates the localized spatial knowledge of the data via bayesian learning.
¡¡there are many ways to define an appropriate likelihood evaluation for the geometry variables g. one simple formulation for p g|fd  is given by:
	 mpd	 1 
where pd =  p y = 1|fx1  ... p y = 1|fxl+u  t =
 ¦µ fx1  ... ¦µ fxl+u  t is a columnvector of conditionallabel probabilities for latent variables associated with xd  m is a graph-based matrix such as the graph laplacian in section 1  and z is a normalization factor. p g|fd  may be interpreted as a measure of how much fd corroborates with a given geometry  computed in terms of the smoothness of the associated conditional distributions with respect to unlabeled data. this implements a specific assumption about the connection between the true underlying unknown conditional and marginal distributions - that if two points x1 x1 ¡Ê x are close in the intrinsic geometry of px  then the conditional distributions p y|x1  and p y|x1  are similar  i.e.  as a function of x  p y|x  is smooth with respect to px.
¡¡since the likelihood form  eqn 1  leads to a non-gaussian posterior  a natural substitute is to use an un-normalized gaussian form  as commonly used with ep approximations. the posterior is then approximated as:

such a form captures the functional dependency between the latent and the geometry variables while rendering subsequent computations tractable. the value of c is inconsequential in defining this approximate posterior distribution  since c cancels due to the normalizing term p g    although it is important for evidence computations. in section 1  we further comment on the role of the c and propose the use of partial evidence for model selection. the matrix m is approximated by the laplacian matrix of the data-adjacency graph  to correspond to the deterministic algorithms introduced in  sindhwani et al.  1; belkin et al.  1 . we note that c m can alternatively be computed from ep calculations  leading to a novel graph regularizer and tractable computations for the full evidence. we outline more details in this direction in  chu et al.  1 .
¡¡to proceed further  we make the assumption that given fd  g is independent of latent variables at other points  i.e.  if the data set x contains xd and a set of unseen test points xt  we have:
	p g|fx  = p g|fd 	 1 
this assumption allows out of sample extension without the need to recompute the graph for a new dataset. the posterior distribution of fx given g is: p fx|g  ¡Ø p g|fx p fx  = p g|fd p fx .
¡¡the prior distribution for fx is a gaussian n 1 ¦²xx  given by the standard gaussian processes. in the form of block matrices  the prior p fx  can be written as follows: fx where we use the shorthand d for xd and t for xt. the posterior distribution of fx conditioned on g can be written as a zero-mean gaussian distribution p fx|g  ¡Ø  where

¡¡proposition: given eqn 1  for any finite collection of data points x  the random variables fx = {f x }x¡Êx conditioned on g have a multivariate normal distribution n 1 ¦² xx   where ¦² xx is the covariance matrix whose elements are given by evaluating the following kernel function
z	 1 
for x z ¡Ê	x.	here ¦²dx denotes the column vector
 k x1 x  ... k xn x  t.
¡¡this proposition shows that the gaussian process conditioned on the geometry variable g is a gaussian process with a modified covariance function k . a proof of this result uses straightforward matrix algebra and is omitted for brevity.
¡¡ssgp is the posterior process obtained by conditioning the original gp with respect to g. note that the form of its covariance function k  is the same as in eqn 1  which is derived from properties of rkhs.
1 model selection
model selection for ssgp involves choosing the kernel parameters and the noise variance ¦Òn  see section 1 . the definition of k  is based on the choice of a covariance function k and a graph regularization matrix m. as in  sindhwani et al.  1   in this paper we restrict our attention to the following choices:
and use the kernel. the parameters ¦Ãa and ¦Ãi balance ambient and intrinsic covariance. the parameters related to the computation of the graph laplacian l are - the number of nearest neighbors nn and the graph adjacency matrix w.
  if xi and xj are adjacent
in a nn-nearest neighbor graph and zero otherwise.
¡¡denote ¦Õ as the collection of model parameters. the optimal values of ¦Õ are determined by maximizing the evidence p yl|g ¦Õ  which is available from ep computations.
¡¡model selection is particularly challenging in semisupervised settings in the presence of few labeled examples. popular model selection techniques like cross-validation  for deterministic methods like laplacian svm/rls  requiresubsets of the already-scarce labeled data to be held out from training. moreover  for graph-based semi-supervised learning  a proper cross-validation should also exclude the held out subset from the adjacency graph instead of simply suppressing labels  in order to ensure good out-of-sample extension. this adds a major expense since the semi-supervised table 1: datasets with d features  n training examples  labeled+unlabeled  and t test examples.
1	1	usps 1	1
1	1	r	-1
kernel in eqn 1 needs to be recomputed multiple times. also  since cross-validation is based on counts  it may often be unable to uniquely identify a good set of parameters. by contrast  evidence maximization neither needs to suppress expensive labels nor needs to hold out labeled examples from the graph. as a continuousfunction of many model parameters  it is more precise in parameter selection  demonstrated in section 1  and also amenable to gradient-based techniques. in addition to these benefits  we note the novel possibility of employing unlabeleddata for model selection by maximizing the full evidence p yl g|¦Õ  = p yl|g ¦Õ p g|¦Õ . the latter term may be computed asfd. given eqn 1  one can immediately derive logp g|¦Õ  =  where i is the identity
matrix. for simplicity  in this paper we focus on comparing standard evidence maximization in ssgp with standard cross-validation in laplacian svm/rls.
1 experiments
we performed experiments on a synthetic two-dimensional dataset and four real world datasets for binary classification problems. the statistics of these datasets are described in table 1.
synthetic data
the toy 1-d dataset moons  meant to visually demonstrate ssgp  is shown in figure 1 as a collection of unlabeled examples  small black dots  and two labeled examples per class  large colored points . the classification boundary and the contours of the mean of the predictive distribution over the unit square are shown for standard gp and ssgp  figure 1 a c  . these plots demonstrate how ssgp utilizes unlabeled data for classification tasks. the uncertainty in prediction  figure 1 d   respects the geometry of the data  increasing anisotropically as one moves away from the labeled examples. figure 1 b  also demonstrates evidence-based model selection.
real world data sets
for all the real world data sets  except 1vs1   we generated a fixed training/test split by randomly drawing one-fourth of the original data set into a test set of unseen examples  and retaining the rest as a training set of labeled and unlabeled examples. learning curves were plotted for varying amount of labeled data  randomly chosen from the training set. for
1vs1  we chose the exact data splits used in  lawrence and jordan  1  to facilitate comparison between algorithms.
¡¡to reducethe complexity of modelselection  we fix nn = 1 and p = 1 for 1vs1  set1 and nn = 1 and p = 1 for
	mean of the predictive distribution of supervised learning	logrithm of the evidence of semi supervised learning
1	 1
1	 1
1
1
1
1
 1 1	 1	 1	1.1	1	1.1	 1	 1	1.1	1	1.1
figure 1: moons: the mean of the predictive distribution of supervised gp is shown in graph  a . based on maximum evidence  the best settings are found at log1 ¦Òt =  1 and log1 ¦Ò =  1  shown as a cross in graph  b . the results of ssgp with the best settings are shown in graph  c  and  d .
pcmac  reuters. these values are based on experimental experience from  sindhwani et al.  1  with image and text data sets. we used weighted graphs with euclidean distance as the graph similarity measure  and gaussian weights with width  ¦Òt  set to the mean distance between adjacent vertices. the behavior of evidence-based model selection for ssgp and cross-validation  cv  based model selection for laplacian svm was investigated over a range of values for the parameters ¦Ò ¦Ãa ¦Ãi. for each dataset  we computed the mean length of feature vectors  ¦Ò1  in the training set  and probed ¦Ò in the range ; the range for
¦Ãa was  1 1 1 1  and the choices for ratio ¦Ã¦Ãai were  1 1 1   note that a choice of 1 ignored unlabeled data and reduces the algorithm to its standard supervised mode . the noise parameter ¦Òn in the probit model for class labels was set to 1 in all experiments. note that this parameter allows ssgp to potentially deal with label noise.
¡¡1. benefit of unlabeled data: in figure 1  we plot the mean error rates on the test sets for the four datasets as a function of the number of labeled examples  expressed in terms of probability of finding a labeled example in the training set  for ssgp and standard supervised gp  which ignores unlabeled data . figure 1 shows the corresponding curves for performanceon the unlabeled data. the solid curves show the mean and standard deviation of the minimum error rates for ssgp and gp over the set of parameters ¦Ò ¦Ãa ¦Ãi  ¦Ãi = 1 for gp  and the dashed curves show the mean and standard deviation of error rates at parameters chosen by maximizing evidence. the mean and standard deviations are computed over 1 random draws for every choice of the amount of labeled data. we make several observations:  a  by utilizing unlabeled data  ssgp makes significant performance improvements over standard gp on both the test and unlabeled sets  the gap being larger on the unlabeled set. this holds true across the entire span of varying amounts of labeled data for

figure 1: comparison of ssgp with gp on test data.
all datasets  except pcmac where the gap seems to converge faster.  b  performance based on parameters chosen by evidence based model selection is much closer to the optimal performance for ssgp than for gp. the performance curves for ssgp have lesser variance.

figure 1: ssgp vs gp on unlab. data  transduction .
¡¡1. evidence maximization versus cv: we next compare evidence-based model selection in ssgp with cv based model selection in laplacian svm. in figure 1  we plot the mean error rates on the test sets for the four datasets as a function of the number of labeled examples for ssgp and laplacian svm  both of which utilize unlabeled data through the same kernel. figure 1 shows the corresponding curves for performance on the unlabeled data. the solid curves show the mean and standard deviation of the minimum error rates for ssgp and laplacian svm over the set of parameters ¦Ò ¦Ãa ¦Ãi; and the dashed curves show corresponding curves at a parameter setting chosen by evidencemaximization and cv for ssgp and laplacian svm respectively. the mean and standard deviations are computed over 1 random draws for every choice of the amount of labeled data. we used 1-fold cv for each labeled set  except those with 1 or fewer examples  in which case leave-one-out cross validation was used. it is important to note that our cv protocol for laplacian svms only suppresses labels but does not exclude the labeled examples from the graph. we make

figure 1: comparison of ssgp with lapsvm on test data.
the following observations:  a  in terms of best performance over the range of parameters  i.e the solid curves   ssgp and laplacian svm return nearly identical mean error rates  except in 1vs1  where when given very few labeled examples  ssgp outperforms lapsvm.  b  by looking at the performance curves at parameters chosen by their corresponding model selection strategies  i.e the dashed curves   the superiority of evidence maximization in ssgp over cv in laplacian svm becomes quite evident. this is true for both test and unlabeled set performance.  c  the quality of cv based performance of laplacian svm is significantly better over unlabeled data as compared to that over test data  indicating that cv drives the model selection towards parameters that favor transduction at the expense of semi-supervised induction. these experiments show that evidence maximization returns significantly better performance than cv on both the test and unlabeled sets.

figure 1: ssgp vs lapsvm on unlab. data  transduction .
¡¡1. comparison with other methods: finally  in figure 1  we superimpose the ssgp performancecurves over the results of the null category noise model  ncnm   informative vector machine  ivm   svm  and transductive svm  tsvm  plotted in  lawrence and jordan  1 . the same experimental protocol and data splits were used. we are encouraged to see that ssgp outperforms all algorithms tested in this experiment.

figure 1: ssgp versus ncnm  tsvm  ivm and svm
1 conclusion
to conclude  we have presented semi-supervised gaussian process classifiers based on a data-dependent covariance function that is adapted to the geometry of unlabeled data. we have empirically demonstrated the utility of ssgp on several learning tasks  and observed the benefits of evidence-based model selection in comparison to cross-validation techniques in laplacian svm/rls.
