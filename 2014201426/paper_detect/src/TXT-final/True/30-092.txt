 
the majority of naturally sounding musical performance has musical expression  fluctuation in tempo  volume  etc. . musical expression is affected by various factors  such as the performer  performative style  mood  and so forth. however  in past research on the computerized generation of musical expression  these factors are treated as being less significant  or almost ignored. hence  the majority of past approaches find it relatively hard to generate multiple performance for a given piece of music with varying musical expression. 
in this paper  we propose a case-based approach to the generation of expressively modulated performance. this method enables the generation of varying musical expression for a single piece of music. we have implemented the proposed case-based method in a musical performance system  and  we also describe the system architecture and experiments performed on the system. 
1 	introduction 
almost all musicians play music with musical expression 
 varying of tempo  volume  etc. . they consider how the target pieces should be played  and they elaborate upon it with tempo curves and change in volume. thus  musical expression is a highly significant element in making performance pleasant and attractive. 
　many past research efforts have focused on the computerized generation of musical expression. the majority of them employ musical expression rules  which define relations between phrase characteristics and musical expression  figure 1 . past approaches have used rules of musical expression manually acquired by human researchers   fryden and sundberg  1    friberg and sundberg  1    friberg  1   and  noike et a/.  1  . here  expressively modulated performance is generated by applying these rules to the target piece. some recent research efforts have introduced learning mechanisms into the acquisition of rules   bresin et a/.  1    chafe  1    widmer  1b    widmer  1a   and  widmer  1  . these approaches extract rules of musical expression from sample performance data played by human musicians. since the above methods generate and apply rules of musical expression  they are called rule-based approaches. 
　one advantage of rule-based approaches is  once the rule set is established  it is applicable to any piece of music. another advantage is transparency in that users can 
knowledge-based 	applications 

figure 1: the basic mechanism employed by rule-based approaches 
access rules for musical expression used in the generation process. these rules are useful for cognitive research. 
　on the other hand  rule-based approaches have some drawbacks. the most serious one is that these approaches are hard to adapt to the generation of performances with different styles. 
　generally  musical expression has vast freedom and a broad range of tolerance. musical expression varies according to various factors  for instance  the performer  style  e.g.  baroque    romantic   etc.   mood  e.g.  lively    calm   etc.   and so forth. we call these factors performance conditions. 
　to generate suitable musical expression by computer  these performance conditions must be taken into consideration. however  as was seen for rule-based approaches  it is hard to introduce these factors into the process of generation. besides  performance conditions are difficult to describe in term of rules of musical expression  since they consist of various elements and each element continuously changes. thus  there is little research which has considered such factors   canazza et a/.  1  . 
　on the other hand  there is very little research which has employed non-rule-based approaches. arcos  et al. applied case based reasoning  cbr  to the generation of musical expression   arcos et a/.  1  . their approach uses a performance data set as a musical expression knowledge base. for each note in the target piece  it retrieves similar notes from the knowledge base  analyzes musical expression in these similar notes  and applies to the target piece. however  arcos  et al. do not take any kind of performance conditions into account  such that  their approach cannot generate performance variety  similarly to rule-based approaches. 
　we aim to develop a method of computerized generation of natural musical expression which incorporates a range of performance conditions. to overcome the prob-

lems faced by conventional methods  we propose a new case-based method for the generation of musical expression. the advantage of this method is that it can easily consider performance conditions  to be able to generate various kinds of musical expression for a single piece of music in accordance with performance condition settings. we have implemented the case-based method proposed in this paper in a music performance system. in the remainder of this paper  we present our case-based method for the generation of musical expression  and discuss the architecture of the performance system incorporating this method  and experiments on it. 
1 case-based method for musical expression generation 
1 	concept 
figure 1 shows a rough outline of our method. our method uses a performance data set consisting of preanalyzed musical pieces  from which an example data set is extracted for use as the musical expression knowledge base. an example data set is acquired for each inputted target piece. moreover  we evaluate the significance of each example piece to the input piece by considering the structural resemblance of the two pieces and similarity between performance conditions for the input and example piece. the resultant performance is generated based on the example data set and the various significance values. hence  even if the example pieces are fixed  the generated performance will change according to the input performance conditions. this mechanism realizes our aim of generating varying musical expression. 

figure 1: rough outline of our case-based method for musical expression generation 
1 	algorithm 
this section describes the basic architecture used in our case-based method. figure 1 shows the algorithm of our method. 
　our method requires a performance data set  which is a set of musical performance data performed by human musicians. each data component has not only a record of the event sequence  note on  note off  pedal control  etc.  but also the musical score of the performed piece and the performance conditions under which the data was figure 1: overview of the case-based method for musical expression generation 
recorded. this performance data set must be collected beforehand. 
　our method comprises the following stages: 1  input the musical score of the target piece and performance condition settings  1  extract similar parts  called the example segment set  from the performance data set  1  analyze musical expression in each example segment  1  evaluate the significance of each example segment  1  compose the musical expression pattern for the target piece  and  1  apply the musical expression pattern to the target piece. 
　input data consists of information about the target piece taken from the musical score and performance condition settings. the musical score information is not only the information about note sequence but also accompanying information  e.g. beats  bars  repetitions  pauses  etc. . the performance condition settings are parameters which decide the characteristics and mood of the generated performance. a description of the performance condition settings is presented in section 1. 
　in the extraction stage  our method divides both the target piece and each example piece in the performance 
suzuki  tokunaga  and tanaka 

data set into segments  e.g. parts  phrases  bars  etc.   see section 1 for details . then  the similarity between each segment of the target piece and all sample segments is evaluated  and similar sample segments are obtained as the example data set for the target piece. 
　in the analysis stage  our method compares the record of the performance sequence and the musical score for each example segment  and analyzes variances in tempo  volume  and so on. variance in musical expression is represented as a curve of the relative diversity  called the musical expression pattern  mep   see section 1 for details . patterns for all example data segments are stored in the mep set 
　in the evaluation stage  our method calculates a significance score for each example segment. this score indicates how useful the example data is expected to be in the generation of musical expression for the target piece. it is determined principally from similarity in musical score and performance conditions. 
   as a result of the analysis and evaluation stages  an mep set with significance scores is obtained. in the composition stage  these meps are integrated into a musical expression value for the whole target piece  see section 1 for details . the first step of this stage is the calculation of the meps for each segment of the target piece. this is achieved through the average of example meps for that segment. the average is weighted by the significance of each mep. the second step is the integration of segments meps. in this step  averaged meps for each target piece segment are unified as the integrated mep. 
　finally  in the application stage  our method applies the integrated mep to the musical score of the target piece  and outputs the resultant performance data. 
1 c o m p o n e n t technologies 
1 	segmentation of musical pieces 
generally speaking  one possible serious problem faced by the case-based method is shortfalls in the example data set. our methods extracts available example segments from the example data set  analyzes them  and applies them to the target piece. thus  if the size of available example data is insufficient  the proceeding processes will not function satisfactorily. 
   arcos et al. used single notes as their segment granularity  and introduced cognitive musical structure to relate neighboring notes. this is based on narmour's implication/realization model   narmour  1    and lerdahl and jackendoff's generative theory of tonal music   lerdahl and jackendoff  1  . this is a good way to avoid shortfalls in the example data set. however  such an approach is insufficient to generate musical expression variance over longer stretches of music. therefore  as mentioned above  our method extracts sequence of notes instead of single notes as the example data  and does not rely on the cognitive musical structure. it is obvious that the cognitive structure has a good effect on the generation of musical expression. however  since there may be individual differences between some of these structures  it is undesirable to rely solely on this knowledge type. moreover  we think that the cognitive structure can equally be acquired with a case-based method similar to that proposed here. so  in this research  we chose the more challenging path  that is the generation without cognitive musical structure. 
　in our method  the most desirable example type is performance data on the target piece. however  it is unreal-
knowledge-based 	applications 
istic to expect that such examples can be obtained  and close-fitting examples for all portions of the target piece are also rarely found. in other words  it is likely that enough examples could not be found simply by querying a piece which is similar throughout. 
to avoid this problem  as briefly mentioned above  cf. 
section 1   we divide the target piece into segments  and extract an example data set for each segment. 
　so as to extract examples extensively for all parts of the target piece  queries should be made at various granularities of division. ideally  all possible methods of division should be tried. however  the number of plausible segment lengths reaches exponential order on the number of notes which appear in the target piece  making such exhaustive computation unrealistic from the viewpoint of computational cost. hence  our method uses a number of consistent approaches to division  which are based on the musical structure described in the musical score. 
   most music pieces have a hierarchical musical structure consisting of musically meaningful segments  e.g. motives  repetitions  phrases  bars  etc. . the musical structure mentioned in this paper is not cognitive  but a combination of parts which constitute musical pieces. this structure consists of multiple layers of variably sized segmentation units. the segmentation unit at the bottorn layer is the smallest sized segments  i.e. the single note. the segmentation unit at the next layer up is one size larger  which is usually a beat. still higher layers consist of much larger segments  such as a half bar  bar  phrase  sequence of phrases  repetition  motive  and so on. the top layer is composed of the whole piece. the segmentation of a musical piece is described in the musical score to some extent  and likely to be unaffected by factors other than musical score information. in dividing the target piece into segments  the possibility of finding appropriate examples increases. 
1 	performance conditions 
this section explains performance conditions and the associated method of comparison. 
　performance conditions are described as a set of features. each feature is made up of key and value. the 
key indicates the particular feature in the form of a keyword. the value is the degree of the feature  and given as a real number in the range -1 to 1. for instance   an elegant and bright performance  has two features. one feature has the key  elegant   and the other has the key  bright . the value of each feature is between 1 and 1. in the case of  elegant and very bright performance   the value of the feature  bright  is close to 1. in contrast  in the case of  somewhat bright performance   the value of the feature  bright  is close to 1. in the case of  non-bright performance   the feature  bright  has a negative value. if the feature  bright  is not given  it is considered that this performance implicitly has the feature  bright  with value 1. 
　not only the information on the feel to the performance but also information on the performer and performance style are also described in this form. for example  performance data from musician  a  has feature  performer a . the value of this feature is 1. in the case of musician a imitating musician b  the performance conditions consist of feature  performer a  and  performer b   with values slightly closer to 1 than in the previous case. 

　now  considering the key and value of each feature as unit vector and the norm of that vector  respectively  performance conditions are the sum of vectors on a vector space which covers each unit vector key. this summation of the vectors is named the performance condi-
tion vector. equation 1 shows performance condition vector v 
		 1  
　where v is the set of keys of features which constitute the performance conditions  and ai is the value of key t. 
　by introducing the concept of the performance condition vector  similarity in performance conditions can be evaluated through the distance between the performance conditions vectors. equation 1 shows the resemblance value of performance condition vectors v and u. the numerator is the dot product of the performance vectors. the denominator is the square of the length of the larger vector  hence normalizing the degree of resemblance. 
 1 bars  or a half phrase  1 bars . the parent is decided in accordance with the segmentation strategy. in the case of tempo  the mep is calculated from the seconds/crotchet value  instead of the tempo value  since the tempo value is inconsistent in some calculations  e.g. the mean of tempo values and the average tempo value usually differ . 

figure 1: an example of mep calculation for tempo 

		 1  
1 	musical expression pattern 
this method uses musical expression patterns  meps  in the generation process. this section describes analysis and composition of meps. 
analysis of meps 
this method uses the ratio of  the musical expression value  tempo  volume  and so on  of the target example segment  to  the average value of the next segment up  parent segment   as a representation of variance in musical expression. the mep of an example segment is the set of the ratios for each type of musical expression  tempo  volume  etc. . equation 1 shows this calculation   s  is the mep of musical expression type exp  seconds/crotchet  see below   volume  etc.  for a segment s  si j is a segment of the target piece  t is the depth of the segmentation layer  c.f section 1   j and k are segment indices within the given segmentation layer  si j is a sub-phrase of si-1 k and exp s  is the musical expression value of segment s. the average mep over all segments composing one segment size up is always 1. 
		 1  
　the following example shows the calculation of mep for a performance data segment of a 1 bar phrase  figure 1 . this performance data is played at an average tempo of 1  1 seconds/crotchet . in the case of human performance  the tempo varies with musical expression  so that the tempo of most notes in the phrase will be other than 1. in this example  the average tempo of each bar is  respectively  1  1 seconds/crotchet   1  1 seconds/crotchet   1  1 seconds/crotchet   and 1  1 seconds/crotchet .  note that the average of these tempos will not be 1  since the average tempo is the reciprocal of total performance time.  as mentioned above  mep is the ratio of the expression value of the target segment to the value of the next segment up. in this case  target segments are made up of each bar  and the parent will generally be the whole phrase assuming that the next segment up is the whole phrase  the tempo mep for each segment  each bar  is the ratio of the seconds/crotchet tempo of each bar  1  1  ...  to the seconds/crotchet tempo of the whole phrase  1 . in this way  the mep for the bars are 1  1  1  and 1  respectively. 
m e p composition 
in the composition stage  these meps are integrated into a single mep for the whole target piece. as mentioned in section 1  the composition stage consists of two steps. 
the first step is the calculation of the mep for each segment of the target piece. the mep of each segment of the target piece is the weighted mean of meps of all examples for that. equation 1 is a formalization of this process. in this equation  si j refers to a segment of target piece  ei j is the overall example data set for segment si j and w s  is the weight of example segment s  which is calculated from the significance of each segment. 
		 1  
the second step is the integration of the individual 
meps. in this step  for each note of the target piece  the meps of all ancestral segments are multiplied. an ancestral segment of a note is any segment which contains that note. equation 1 shows the integrated mep for the mth note nm. si is the set of segments in tth layer  and n is number of layers  where the segmentation unit of the nth layer is a single note  i.e. sn m = nm . 
		 1  
figure 1 shows a simple example of this calculation. 
the mep for a half bar segment is the ratio of the value of the half bar to the value of full length containing bar  and the mep for a bar segment is the ratio of the bar value to the whole 1 bar phrase value. thus  the integrated mep indicates the ratio of the value of each note to the value of the whole phrase. 
suzuki  t1kunaga  and tanaka 


figure 1: an example of mep generation for a 1 bar phrase 
1 	musical expression generation system 
1 	outline 
we have been developing a musical expression generation system called kagurame  which uses the case-based method described above. kagurame phase-i  the first stage of kagurame  is intended to estimate the system capability and possibilities of our method. for the sake of simplicity  the types of musical pieces and performance conditions the system can handle have been limited. for example  the target piece and sample data are limited to single note sequences. 
1 	architecture 
figure 1 shows the architecture of kagurame phase~-i. 
the following section describes the basic mechanism and algorithm for each component. 
input 
as input  this system uses: 1  musical score information of the target piece  1  the musical structure of the target piece  and 1  performance condition settings. the musical score information is a sequence of detailed parameters for each note  e.g. position  beat length  key value  etc. . the musical structure is information on segment boundaries  used to divide the target piece into segments  cf. section 1 . the performance condition settings are given in the form of a performance condition vector  cf. section 1 . this combined information is given in an originally formatted text file. 
performance data set 
each performance data set consists of: 1  musical score information  1  musical structure  1  performance conditions  and 1  performance data. the musical score information  musical structure  and performance conditions are given in the same format as described for the system input. the performance data is a sequential record of a human performance. it is given as a standard midi format file  smf . the smf is a sequence record of note event information  which consists of the time  key value  and strength   velocity  . this format file is easily obtained from a computer and electronic keyboard. each data type is divided into segments beforehand for convenience of calculation at the extraction stage. 
extraction of examples 
in the extraction process  first of all  the target piece is divided into segments according to the musical structure information. the similarity score between a given target segment and each performance data segment is then 
knowledge-based applications 

figure 1: the system architecture of our performance system 
calculated  and high scoring segments are used as the example data set for the target segment. this extraction process is carried out for all segments of the target piece. 
evaluation of similarity 
the similarity score used at the extraction stage is calculated by the similarity evaluation module. this estimation is based on the resemblance of the characteristics of the concerned segments. the system currently uses three factors as segment characteristics: melody  harmony  and rhythm. melody is the tendency for fluctuation in the melody. it is calculated as the difference between the average pitch of the first half of the segment and that of the latter half. equation 1 shows the melody characteristic function  for segment s  where n is the set of notes in the first half of the segment  n1 is the set of notes in the latter half  and p n  is the pitch of note n. the characteristic of harmony is the chord component of the segment. this is a set of 1 values. each value is a count of given pitch note. equation 1 shows the ith value of the harmony characteristic function the characteristics of rhythm is the beat length of the segment. 


		 1  
　for each factor  the system evaluates the characteristic parameters of target segments  calculates the resemblance of these parameters  and normalizes them. resemblance of melody is the difference between cm s  for two segments. resemblance of harmony is the summation of the difference of  for each i  equation 1 . resemblance of rhythm is the ratio of beat length. if this value calculates to less than 1  the inverse is used. the summation of these resemblances is used as the similarity between segments. 
analysis of meps 
then  this system analyzes the mep of each example segment. details of this process are given in section 1. 
evaluation of significance 
the significance of each example segment is the product of similarity with the target segment  similarity with neighbor segments  similarity with ancestral segments  and resemblance of performance conditions. the similarity of target segments is calculated in the same way as for the extraction process  and likewise for the similarity of neighbor or ancestral segments. resemblance of performance conditions is the dot product of the performance condition vectors in question  cf. section 1 . 
composition of musical expression 
the application process consists of: 1  calculation of mep for the each segment of the target piece  1  integration of segment meps for the whole target piece  and 1  generation of expressive performance data for the target piece. details of the calculation and integration process are given in section 1. the weight for the calculation of mep of each segment  w s  in equation 1  is an exponential function on the significance of that segment. as a result of this process  the integrated meps for the overall target piece are generated. 
　in the generation process  the system multiplies the integrated meps by the average for each musical expression over the whole piece. for example  in the case of tempo  the average seconds/crotchet value for the piece is multiplied in its entirety with each integrated mep. the overall average value is based on example data for segments of the overall piece and notation on the musical score of target piece. all types of musical expression are generated in same way. finally  the system applies the overall musical expression to each note of the target piece  and outputs the resultant performance data as an smf file. 
handling of musical expression kagurame phase i handles three types of musical expression: local tempo  duration  and volume. local tempo is the tempo of each note. duration is the ratio of the time from note on until note off to the given length of the note. duration of 1 means the note is played for its full length   there is no pause or overlap . in the case of staccato  the duration will be close to 1  and in the case of legato  it will exceed 1. volume is a measure of the strength of sound. these parameters are easily accessible from the smf file. 
1 	evaluation 
we generated some expressive performance data with kagurame phase-i  and evaluated the resultant performance. this section describes the experiments and evaluation of the performance generated by kagurame phase-i. 
1 	experiments 
a relatively homogeneous set of 1 short etudes from czerny's  1 kurze ubungen  and  1 passageniibungen  were used for the experiment. performance data was prepared for each piece. all performance data was derived from a performance by an educated human musician  and each piece was played in two different styles: 1  romantic style and 1  classical style. the performance conditions for each piece has the single feature of  romantic  or  classical  with a value of 1. 
　out of the 1 pieces  one piece was selected as test data  and performance data for all the remaining pieces  1 pieces  was used as the performance data set. as such  the human performance data for the test piece was not included in the sample data set  i.e. evaluation is open . two styles  those described above  of performance data were generated for the test piece by kagurame phase~i based on the performance data set. the test piece was varied iteratively  similar to crossvalidation   and performance data was generated for all the pieces. all generated smf data was played with a yamaha clavinova clp 1 and recorded on an audio tape for the listening experiments. 
1 	evaluation of performance results 
we evaluated the resultant performance through a listening test and numerical comparison. in the listening test  the resultant performances were presented to several human musicians for their comments. some of them were players of sample data. in the numerical comparison  the difference between human performance and the generated performance was calculated  and rating was also made of the difference between performance data for the two styles. 
　the following are comments from the listeners. from the viewpoint of satisfaction of performance  the resultant performances sounded almost human-like  and musical expression was acceptable. there were some overly weak strokes caused by misplay in the sample data  but these misplays were not obvious in the resultant performance. it is hard to determine which performance  human or system  is better  since it relies heavily on the listener's taste. but  if forced to say one way or the other  human performance was better than the system one. 

figure 1: the tempo curve of the system and human performance of  no. 1  1 kurze ubungen  
　human listeners pointed out that the curve of the generated performance tended to be similar to that of the 
suzuki  t1kunaga  and tanaka 

human performance particularly at characteristic points.  e.g. the end of each piece . numerical comparison between the human performance and generated performance also showed that fluctuations in musical expression for the system performance resembled human performance in many respects. figure 1 shows the comparative tempo curves for the generated performance and human performance of ''no. 1  1 kurze ubungen  in the  romantic  style  of course  this is not the best resultant data but an average case . in this graph  it observable that the peaks of the graph coincide  e.g. around 1  1  the ending  and so on . in some portions  however  differences in the curve behavior are noticeable. human listeners judged some of these differences to be permissible and not critical errors. they seem to represent variance of musical expression within the same style. 
　the difference between the generated performance for the two styles was clear in each case. in the listening test  very high percentages of correct answers were obtained when listeners were asked to identify the performance style of the piece. figure 1 shows the tempo curve of the  romantic  and  classical  styles for the generated performance. the target piece is  no. 1  1 kurze ubungen . this graph also evidences differences in the generated tempo curve. the range of fluctuation for the  romantic  style is much broader than the  classical  style. since a broad range of rubato is known as a typical characteristic of the  romantic  style  the broader fluctuation seen for the  romantic  performance seems to be appropriate. based on this result  at least these two styles were discriminated in performance. 

figure 1: the tempo curve of the  romantic  and  gassical  style performances of  no. 1  1 kurze ubungen  
1 	conclusion 
this paper described a case-based method for the generation of musical expression  and detailed a music performance system based on the case-based method proposed in this paper. the advantage of the proposed method is that it can model performance conditions during the generation process. this makes it easy to generate various kinds of musical expression for a single piece of music in accordance with the performance condition settings. 
　according to a listening test  the resultant performance of the described system was judged to be almost human-like and acceptable as a naturally expressed performance. particularly  at characteristic points of the target piece  musical expression tended to be remarkably similar to human performance. by testing different styles of system performance  it was proved that our system can generate different musical expression for a given 
knowledge-based applications 
piece of music. moreover  most of the generated musical expression was judged to be appropriate for the given style. 
　as a result of these experiments on the system  the case-based method presented in this paper can be seen to be useful for the generation of expressive performance. it was also confirmed that this method can generate varying musical expression for a single piece of music through changing the performance condition settings. 
