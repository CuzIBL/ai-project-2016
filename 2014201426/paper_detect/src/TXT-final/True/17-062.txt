 
     an alternative approach to uncertain inference in expert systems is described which might be regarded as a synthesis of techniques from 
automatic induction and mathematical statistics. it u t i l i s e s a type of pattern matching in which comparisons are made between new cases  as yet 
unclassified  and a database of past cases  in which the outcome is known . the method uses a 
stepwise approach in which  at each stage  that evidence v a r i a b l e p r o v i d i n g the g r e a t e s t additional discriminating power between classes  above that already obtained  is u t i l i s e d . it avoids relying on the assumption of conditional independence. 
     a rudimentary system  predictor  which operates according to these principles has been written. various adaptations to deal with missing and uncertain evidence are described  as are additional features such as a window  a f a c i l i t y for focusing discrimination on a subset of classes and a modification to deal with subjective data. 
	ii 	an alternative approach 
     another approach to the problem of uncertain inference in expert systems might loosely be described as a synthesis of techniques from automatic induction and mathematical statistics. it u t i l i s e s data on past cases in order to make predictions or classification judgements about new ones. in this respect  it is s i m i l a r to ideas on automatic i n d u c t i o n in the ai f i e l d  e.g. michalski  1  and also to a large family of s t a t i s t i c a l techniques. however  it differs from automatic induction in that the latter is really a logi cal procedure which  by i t s very nature  
cannot handle u n c e r t a i n t y . it d i f f e r s from orthodox statistical techniques in that no model is constructed for predictive purposes - instead each new case is matched against past data on a subset of variables  by a process about to be described. other differences are that the system is capable of providing more of the customary features of expert systems than are usually found in traditional statistical approaches. 
     
	i 	introduction 
     traditional rule-based systems suffer from a 
     number of serious deficiencies when attempts are made to incorporate mechanisms for dealing with uncertainty. the present author has attempted to i l l u s t r a t e these shortcomings elsewhere  white  1 . perhaps the most serious flaw is the assumption of c o n d i t i o n a l independence in circumstances where it is not j u s t i f i e d . this particular topic has been discussed by szolovits and pauker  1 . it has also been admitted by members of the prospector team in their f i n a l report  duda et a l   1 . other d i f f i c u l t i e s relate to the peculiarities arising from the use of fuzzy logic for combining p r o b a b i l i t i e s . this has been c r i t i c i s e d both by myself  white  1  and by quinlan  1 . further problems are caused by inconsistencies in the parameters b u i l t into such systems as a consequence of t h e i r being subjective estimates. attempts to avoid these problems by not using formal p r o b a b i l i t y theory have f a l l e n v i c t i m to e x a c t l y the same d i f f i c u l t i e s   as shown by adams  1  in his discussion of the mycin system. 
     suppose that we are concerned with some problem domain in which each case possesses values on a number of binary a t t r i b u t e s   or evidence variables. let us further suppose that each case f a l l s into one of k classes that are mutually exclusive and jointly exhaustive. the f i r s t thing to do is to form a frequency table from a sample of past cases in which their class membership is known as well as t h e i r values on the various attributes.  as the number of possible patterns of evidence becomes l a r g e   the t a b l e would be expected to become increasingly sparse  i.e. would have an increasingly large proportion of evidence patterns with a l l class frequencies zero. of course  such patterns need not be stored . 
     if we now consider a new case  in which the pattern of evidence is known but the class is not  then the computation of p r o b a b i l i t i e s f o r classifying the case proceeds along the following lines. the technique yields p r o b a b i l i t i e s which 
are conditional upon some appropriate subset of the variables. 
	a. white 	1 
table for each of the evidence variables  cross-     the f i r s t step is to f i n d that evidence variable which is the most powerful discriminator between the k classes. the way that this is done in predictor is by forming a k x 1 contingency tabulating class against the evidence variable  and choosing as the best discriminator that p a r t i c u l a r v a r i a b l e showing the strongest interaction with class  as measured by the x1 test s t a t i s t i c for the k x 1 t a b l e . the e n t i r e frequency table is then conditioned upon that value of the variable found in the new case  thereby forming a sub-table. 
     this process is repeated  discriminating upon successive variables in t u r n   u n t i l no further steps can be taken. this occurs when none of the remaining independent v a r i a b l e s y i e l d s a significant x1 value when crossed with class. 
     the f i n a l step consists of collapsing the remaining sub-table into k cells  one for each class  and forming the p r o b a b i l i t i e s for class membership from these in the obvious manner  along with their confidence intervals  if required . 
     it should be noted that when each new variable is conditioned upon  its interactions with those v a r i a b l e s that have already been used are automatically taken into account by the  /ery nature of the process employed. thus there is no need to make the customary assumption of conditional independence between variables  which is so often violated in conventional expert systems. 
	i l l 	dynamic path generation 
     the approach as o u t l i n e d so f a r has s t a t i s t i c a l s i m i l a r i t i e s with the discriminant function for categorical data described by sturt  1  and also with the technique outlined by mabbett et al  1 . it resembles most closely the  probabilised  automatic induction procedure described by hart  1  but d i f f e r s from a l l these procedures as follows. 
     the techniques just mentioned a l l generate decision trees for c l a s s i f i c a t i o n purposes  as does quintan's 1 algorithm . the approach used in predictor does not do t h i s . although the process of branching on selected variables is involved  only that path necessary to classify the case under consideration is generated. although a l l the information necessary to create the f u l l decision tree is available in the database  this is not done. thus the tree remains a v i r t u a l one  apart from the path through it that is actually generated. this neatly circumvents the worst aspects of the combinatorial explosion which could otherwise be problematic  even with quite small numbers of variables. 
     the i n t e r a c t i v e v e r s i o n of predictor interrogates the user in order to obtain the values of variables that it is going to condition  i.e. branch  on. the user has the option of giving a  don't know  response to any question. the algorithm used ensures that in such cases  these variables w i l l not be conditioned upon but w i l l be collapsed over at the f i n a l stage. this f e a t u r e means that the i n f o r m a t i o n in the frequency table corresponds to a number of virtual trees  one of which w i l l have a path actualised if it is required to classify a particular case. 
iv the strategy for uncertain evidence 
     if the user is completely uncertain concerning the value of a particular piece of evidence  then it should assume its prior value and be dealt with as a missing value  as described previously. 
     however  f o r i n t e r m e d i a t e l e v e l s o f uncertainty  some other strategy is required. if a variable that has uncertainty associated with it is not conditioned upon  then no further special action need be taken because the f i n a l sub-table w i l l be collapsed over this variable  i.e. it w i l l not feature as a node in the dynamically generated path . on the other hand  if the variable is one that is branched on  then further steps are required. 
     the m o d i f i c a t i o n i s s t r a i g h t f o r w a r d . essentially  it uses both sub-tables that result from conditioning upon an uncertain variable  followed  at the f i n a l stage  by the collapse of both sub-tables and the computation of two sets of conditional probabilities. each final conditional probability is then computed as the weighted sum of the two appropriate conditional probabilities  where the weights are the subjective probabilities f o r the d i f f e r e n t values of the u n c e r t a i n variable.  in terms of the tree structure  a forked path is generated  with the uncertain variable located at the point of bifurcation . 
     a useful heuristic to save computation time is not to condition on uncertain variables when they are thrown up by the selection algorithm but to flag them for postponed conditioning at the penultimate stage  immediately before collapsing the sub-tables . 
v further features 
     one of the features judged to be important in conventional expert systems is the presence of a 1 	a. white 
 window   i.e. the user should be able to see how the system got to its present state. the approach outlined earlier can be supplied with a  window  in much the same way as any other stepwise s t a t i s t i c a l procedure. it would be quite easy to program the system to report at each stage what variables have been conditioned upon and  if required  what the k conditional p r o b a b i l i t i e s would be if the current sub-table was collapsed without further conditioning. 
     a n o t h e r idea concerns the f o c u s of d i s c r i m i n a t i o n . at any given stage in the conditioning process  the user might wish to shift the focus of discrimination away from a global consideration of a l l k classes to a p a r t i c u l a r subset - perhaps two or three - which he judges more likely to be relevant. this feature could be implemented simply by altering the x1 test used so that it would be based on the approriate subset of classes. 
     f i n a l l y   the issue of subjective data should be tackled. as predictor was conceived  it was intended to operate on objective  i.e. actual  data from past cases. however  it has recently occurred to the author that it might be feasible to use the same scheme to operate on subjective data. these data might be elicited from the domain expert  as follows. he would be asked to imagine some large number of cases  say f i f t y thousand  and would then be asked to specify how he would expect these cases to be d i s t r i b u t e d over the various evidence vectors and classes in the imaginary database. the intention is that the expert should pick out those combinations of class and evidence which are relatively common and also those which are r e l a t i v e l y rare and attempt to estimate their frequencies. having estimated these   p e a k s   and   t r o u g h s   i n the f r e q u e n c y distribution  the remaining cases would be spread evenly over the database. such a method of e l i c i t i n g subjective data neatly circumvents problems arising from getting the expert to estimate impossibly complicated likelihood ratios and ensures consistency in the estimates - thereby avoiding the problem of inconsistent priors  white  1 . however  one problem with t h i s approach remains. the statistical sensitivity  or  power  in statistical terminology  depends on the number of imaginary cases that are used. if this number is too small  the system w i l l be of l i t t l e use because it w i l l make few inference steps  or perhaps none at all  before coming to an automatic halt. if the number is too large then the system w i l l possess a spurious degree of precision. some further thought on this p a r t i c u l a r topic is obviously required. 
