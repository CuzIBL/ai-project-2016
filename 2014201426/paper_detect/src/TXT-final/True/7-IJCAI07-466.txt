
this paper describes a methodology to semiautomatically acquire a taxonomy of terms and term definitions in a specific research domain. the taxonomy is then used for semantic search and indexing of a knowledge base of scientific competences  called knowledge map. the kmap is a system to support research collaborations and sharing of results within and beyond a european network of excellence. the methodology is general and can be applied to model any web community - starting from the documents shared and exchanged among the community members and to use this model for improving accessibility of data and knowledge repositories.
1 introduction
the noe  network of excellence  interop1 is an instrument for strengthening excellence of european research in interoperability of enterprise applications  by bringing together the complementary competences needed to develop interoperability in a more global and innovative way. one of the main objectives of interop has been to build a so-called  knowledge map   kmap  of partner competences  to perform a periodic diagnostics of the extent of research collaboration and coordination among the noe members. the aim is to monitor the status of research in the field of interoperability through a web-based platform that allows the user to retrieve information according to his/her actual need in a specific situation.
the main benefits of the kmap  figure 1  for its users are:
* to be able to diagnose current interoperability research inside interop and in europe;
* to receive an overview of all european research activities on interoperability and subordinated topics;
* to receive an overview of organisations and experts as well as research results;
* to find relevant information for specific needs quickly;
* to find potential partners for collaborating in research activities.
micha l petit
university of namur  fundp 
rue grandgagnage  1 - b-1 namur belgium mpe info.fundp.ac.be
the target groups of the interop kmap system are:
* members of the kmap management team  who are in charge of producing a periodic diagnostics of current research in interoperability performed by interop partners in the first place and in europe in the second place;
* interop partners who contribute with information about their research and that of other researchers in the domain of interoperability  and retrieve knowledge about the current status of interoperability research;
* the scientific community in the field of interoperability  including universities  research institutes  researchers  companies  etc.
these objectives and targets can be considered relevant for any scientific web community in any research field.

figure 1. the interop kmap.
the kmap is a knowledge management application  exploiting recent research results in the area of semantic web  text mining  information retrieval and ontology enrichment. these techniques have been put in place to create a semantically indexed information repository  storing data on active collaborations  projects  research results  and organizations. a query interface allows users to retrieve information about partner collaborations  research results and available  or missing  competences  as well as to obtain summarized information  presented in a graphical or tabular format  on the overall degree of collaboration and overlapping competence  based on a measure of semantic similarity between pieces of information.
the paper is organized as follows: first  we provide a general picture of the knowledge acquisition value chain  and the motivations behind the adopted approach. then  we summarize the learning techniques used to bootstrap the creation of a domain taxonomy  currently evolving towards an ontology . finally  we describe the implementation and preliminary results of the semantically indexed kmap. related research and future activities are dealt with in the concluding remarks section.
1 the knowledge acquisition value chain
figure 1 schematizes the knowledge acquisition value chain adopted in interop. progressively richer knowledge structures  lexicon  glossary  taxonomy  ontology  are first bootstrapped through automatic text mining techniques  and then refined through manual validation and enrichment  supported by appropriate tools and collaborative web interfaces. each knowledge structure builds on previously acquired knowledge  e.g. automatic glossary extraction exploits knowledge on domain terminology  the lexicon   automatic identification of taxonomic relations is based on glossary parsing  etc.

figure 1. the knowledge acquisition value chain.
in short  the steps of the knowledge acquisition chain are the following1  steps marked a are automatic  steps marked m are manual  supported by web applications :
1.  a  text and documents exchanged by the members of the research community are parsed to extract a set of domain terms  constituting the domain lexicon l;
1.  a  for each term t in l  one or more definitions are automatically extracted from the available documents and from the web  constituting the glossary g;
1.  m  the lexicon and glossary are validated through a collaborative web application by all the members of the community  who are also asked to express a finegrained evaluation of the definition quality;
1.  a  definitions in the validated glossary g are parsed to extract hypernymy  kind-of  relations. additional hypernymy relations are extracted from a generalpurpose lexicalized taxonomy  wordnet  fellbaum  1   and tailored to the domain using a word sense disambiguation algorithm. the validated set of hypernymy relations is used to automatically structure the terms in l into a forest f of taxonomically ordered
sub-trees;
1.  m  a taxonomy management and validation web application is used to: i  manually create a taxonomy c of  core  domain terms ii  enrich c with the automatically created sub-trees f  and iii  allow a collaborative validation of the resulting taxonomy  t;
1.  m+a  the same application is being used  this is an in-progress activity  to let the taxonomy evolve towards the full power of an ontology. again  automatic techniques are used to start the ontology enrichment process  followed by a validation and refinement task.
the idea behind this approach is that  despite many progresses in the area of ontology building and knowledge acquisition  automated techniques cannot fully replace the human experts and the stakeholders of a semantic web application. on the other side  manual ontology building methods as for example methontology  fern¨¢ndez et al. 1  or the nasa taxonomy development framework  dutra and busch  1  are very costly and require an effort in terms of time and competences  not affordable by loosely structured web communities. in our view  automated procedures are useful to achieve a significant speed-up factor in the development of semantic resources  but human validation and refinement is unavoidable when resources are to be used in real environments and applications.
in the rest of this paper  we overview the methodologies used for bootstrapping the knowledge acquisition process. because of space restrictions  and because some of the methods have been already described in literature  see  velardi et al.  1  for steps 1 and 1 of the procedure outlined above   we provide details only on the taxonomy ordering algorithm  step 1 . as far as the validation tasks are concerned   steps 1 and 1   only the final results of the evaluation are presented and discussed; to obtain details  the interested reader is invited to access the deliverables of the project1.
1 learning a domain terminology and glossary
web communities  groups of interest  web enterprises  research communities  share information in an implicit way through the exchange of mail  best practices  white papers  publications  announcements  etc. in interop  many documents  state of arts  deliverables  workshop proceedings  etc.  have been stored on the network collaborative platform  like state of arts  deliverables  workshop proceedings  etc.. we applied to these documents a terminology extraction algorithm based on four measures: lexical cohesion  park et al.  1   domain relevance and domain consensus  navigli and velardi  1  and text layout. the algorithm puts together among the best available term extraction techniques in the literature  and proved to have a very high precision1 in different domains and applications  navigli and velardi  1 . the output of this phase is a domain lexicon l.
for each term t in l  candidate definitions are then searched in the document repository and on the web. automatic extraction of definitions relies on an incremental filtering process:
1.  a  definitions are firstly searched in existing web glossaries. if not found  simple patterns at the lexical level  e.g.  t is a y    t is defined as y   etc.  are used to extensively search an initial set of candidate definitions from web documents. let dt be the set of candidate definitions for each t in l.
1.  a  on the set dt a first statistical filtering is applied  to verify domain pertinence. a statistical indicator of pertinence is computed for each definition dt  dt  based on the number and statistical relevance of domain words  e.g. those in l  occurring in dt;
1.  a  a subsequent stylistic filtering is applied  based on fine-grained regular expressions at the lexical  part-ofspeech and syntactic level. the objective is to select  well-formed  definitions  i.e. definitions expressed in terms of genus  the kind a concept belongs to  and differentia  what specializes the concept with respect to its kind .
there are three advantages in applying the stylistic filtering criterion: i  to prefer definitions adhering to a uniform style  commonly adopted by professional lexicographers. for example  the following definition is not well-formed in the stated sense:  component integration is obtained by composing the component's refinement structures together  resulting in  larger  refinement structures which can be further used as components   ii  to be able to distinguish definitions from non-definitions  especially when candidate definitions are extracted from free texts  rather than glossaries . for example   component integration has been recently proposed to provide a solution for those issues  is not a definition; iii  to be able to extract from definitions the kind-of information  subsequently used to help taxonomic ordering of terms. for example:  in the traditional software engineering perspective  domain model is a precise representation of specification and implementation concepts that define a class of existing systems  is well-formed  and its parsing returns the hypernym: representation.
the regular expressions used for stylistic filtering are domain-general  and the patterns are learned from definitions in professional glossaries on the web.
1.1 collaborative lexicon and glossary validation during the subsequent evaluation phase  all interop partners were requested  through a collaborative voting interface1  first to validate the lexicon  rejecting inappropriate terms in l  and then to express their judgment on the definitions of the survived terms. the actual decision to reject or accept a term or definition was based on the sum of all expressed votes  cumulated vote . as far as definitions are concerned  the request was for a fine-grained voting of definition's quality. votes were ranging from +1 adequate  to -1  totally wrong . partners were also requested to add missing definitions  the coverage of the automated gloss extraction procedure was about 1%  and to manually adjust some near-good definition. table i summarizes the results of this phase. the performance is comparable with published results  e.g.  park et al. 1   but the  real life  value of the experiment increases its relevance. in the literature  evaluation is mostly performed by two or three domain experts with adjudication  or by the authors themselves. in our case  the validation was performed by an entire research community with rather variable expertise  mainly: enterprise modeling  architectures and platforms  knowledge management  and different views of the interoperability domain.
number of partners who voted the lexicon1total expressed votes1accepted terms1  1% number of partners who voted the glossary1total expressed votes1analysed definitions1accepted definitions1  1% reviewed definitions1  cumulated vote =-1 1  1% rejected definitions  cumulated vote  -1 1  1% new definitions added  terms without definition 1table i. result of collaborative lexicon and glossary evaluation.
1.1 computing the speed-up factor for the glossary
the need to use automated glossary learning techniques in interop was motivated by the absence of skilled personnel to create a high quality glossary  but most of all  by the fact that the knowledge domain of the interop community was vaguely defined  as it often happens in emerging communities   making it particularly difficult to identify the truly pertinent domain concepts. however  as already remarked  the aim of the learning procedure described so far is not to replace humans  but to significantly reduce the time needed to build lexico-semantic resources.
to our knowledge  and after a careful study of the relevant literature  no precise data are available on glossary development costs  except for  kon and hoey  1  in which a cost of 1 dollars per term is estimated  but no details are given to motivate the estimate. we consulted several sources  finally obtaining the opinion of an experienced professional lexicographer1 who has worked for many important publishers. the lexicographer outlined a three-step procedure for glossary acquisition including: i  internet search of terms ii  production of definitions iii  harmonization of definitions style. the lexicographer evaluated the average time spent in each step in terms of 1 minutes  1 min. and 1 min. per definition  respectively. notice that the creation of a list of relevant terms  lexicon  is not included in this computation. the lexicographer also pointed out that conducting this process with a team of experts could be rather risky in terms of time  however he admits that in very new fields the support of experts is necessary  and this could significantly increase the above figures  however he did not provide an estimate of this increase . starting from the professional lexicographer's figures  that clearly represent a sort of  best case  performance  we attempted an evaluation of the obtained speed-up. the glossary acquisition procedure has three phases in which man-power is requested: lexicon and glossary validation  and manual refinement of definitions. each of these phases require from few seconds to few minutes  but actions are performed both on  wrong  and  good  data  with respect to the results of table i  to obtain 1  good  definitions  1 must be inspected: 1 of them just accepted  1 to be manually adjusted  etc. . we omit for the sake of space the details of the computation that led to over 1% speed up with respect to the lexicographer's estimate. in this comparison we exclude the stylistic harmonization  step  iii  of the lexicographer's procedure   which is indeed necessary to obtain a good quality glossary. however  since this phase would be necessarily manual in both cases  it does not influence the computation of the speed-up factor.
1 learning taxonomic relations
the application of the well-formedess criterion discussed in section 1  implemented with regular expressions   allows to extract from definitions the kind-of information  as defined by the author of a definition. this information may help structuring the terms of l in taxonomic order. however  ordering terms according to the hypernyms extracted from definitions has well-known drawbacks  ide and v¨¦ronis  1 . typical problems found when attempting to extract  manually or automatically  hypernymy relations from natural language definitions  are: over-generality of the provided hypernym  e.g.  constraint checking is one of many techniques...    unclear choices for more general terms  or-conjoined hypernyms  e.g.  nonfunctional aspects define the overall qualities or attributes of a system    absence of hypernym  e.g.  ontological analysis is accomplished by examining the vocabulary that...    circularity of definitions  etc. these problems - especially over-generality - are more or less evident when analysing the hypernyms learned through glossary parsing. to reduce these problems  we defined the following procedure:
1.  a  first  terms in the lexicon l are ordered according to simple string inclusion. string inclusion is a very reliable indicator of a taxonomic relation  though it does not capture all possible relations. this step produces a forest f of sub-trees. let stint be one of such trees  for example:
integration representation integration model integration enterprise model integration
schema integration ontology integration knowledge integration data integration information integration
this  string inclusion  heuristics created a forest of 1 isolated trees out of the 1 validated terms in l  cf. table
i .
1.  m  the trees in f are manually connected to a core taxonomy1 c of high-level concepts  defined by a team of experts who basically reused wordnet and previous available work on enterprise ontologies1. let t1=cf be the resulting  fully connected  taxonomy.
in the interop domain  c includes 1 concepts and t1 includes 1 nodes in total.
1.  a  the set of multi-word concept names in t1 is decomposed in a list l' of singleton words  to which we added also the hypernyms automatically extracted from definitions. for example  if t1 is the sub-tree stint   l' is: representation  integration  model  data  ontology  specification  information  etc. terms in l' are used to search hypernymy relations in the wordnet sense inventory. for example:
representation#n#1  knowledge#n#1 scheme#n#1  representation#n#1 data#n#1  information#n#1 workflow#n#1  development#n#1
all the wordnet word senses in the above example have a lexical counterpart in l'. let rwn be the set of extracted hypernymy relations.
some of the senses in rwn are not appropriate in the interoperability domain  e.g.: architecture#n#1  activity#n#1  which refers to the  profession  sense of architecture rather than to computer architecture  sense #1 in wordnet . however  the objective is to apply these relations in a restrictive way  i.e. only to sibling terms in t1. for example  the first rule of the above list can be used to move a term starting with  representation  below a term starting with  knowledge  iff if these two terms are siblings in some subtree of t1  e.g. in stint . the number of  applicable  rules is therefore reduced to a subset rwnt1  rwn.
in our domain  l' includes 1 different words   since certain words occur many times in terminological strings   rwn includes 1 kind-of relations  but rt1wn includes only 1 relations.
1.  a  an on-line word sense disambiguation algorithm  ssi  navigli and velardi  1   is used to detect wrong senses1 in rt1wn  with respect to the domain. we use ssi to disambiguate each word in l' that appears in at least one of the kind-of relations in rt1wn. the context for disambiguation is provided by co-occurring words in each sub-tree  e.g. in stint: representation  integration  model  etc. let rssi be the relations in rt1wn survived after this step.
step 1 returned 1 sense selections  which have been manually validated by two judges. 1 sense selections  1%  were judged as correct  given the domain.
1.  a  relations in rssi are used to restructure t1. for example  according to the relations available in rssi  e.g. those in the example of step 1   stint becomes:
knowledge integration
representation integration
schema integration
model integration
enterprise model integration
information integration data integration
ontology integration
let t1 be the resulting taxonomy after step 1. following the learn-and-validate methodology adopted throughout the project  a web interface1 has been developed to allow a collaborative validation of t1. table ii provides a summary of the validation task.
number of partners who voted the taxonomy1total number of activated polls1total number of performed actions1of which:movement of single terms or term sub-trees
deleted core nodes1created core nodes1table ii. results of collaborative taxonomy validation.
in table ii   activated polls  refers to the fact that before making a change  partners need to activate a poll and receive consensus. the table shows that only 1 moves have been approved. a comparison between the number of actions performed by partners in table i and table ii suggests that domain specialists can easily perform certain tasks  i.e. lexicon pruning  but are less confident when asked to contribute in creating progressively more  abstract  representations of their domain of expertise  from glossary to taxonomy and  eventually  to an ontology . this seems to further support the use of automated techniques.
1 semantic indexing and semantic search
the taxonomy created through the procedure illustrated so far has been used to semantically index the interop kmap. figure 1 shows the screen dump of a possible query type   find all the results - papers and projects - dealing with a subset of concepts in the taxonomy  . the user can select concepts  referred to as knowledge domains  or simply domains  in the query interface  by  string search  in the taxonomy  as in the example of figure 1   they can arrange concepts in boolean expressions  and perform query expansion  including in the query all or some of the concept's hyponyms .

figure 1. taxonomy-based search of interop research results.
it is also possible to obtain  global  information  e.g. a map of member's competence similarity  or an analysis of research results similarity. figure 1 shows the screen dump of a graph in which nodes represent interop organizations and the similarity value is highlighted by the thickness of edges. the number shown on each edge is the result of a semantic similarity computation  see  velardi et al.  1  for details . in short  the information  text or data  concerning each organization and its affiliated partners  is automatically parsed  and a weighted vector pm of taxonomy concepts is associated to each member m. the well-known cosine-similarity measure is computed between vector pairs  but rather than considering only direct matches between terms  we also consider indirect matches  i.e. term pairs tx  pm1 and ty  pm1 related by direct  tx  ty  or semidirect hypernymy relations  tx  t  ty .
in the current version of the kmap  to be enhanced in the last year of the project  indirect matches represent a 1% of the total matches used to compute partner similarity.
1 related work and concluding remarks
this paper illustrated  in a forcefully sketchy way  a complete application of semantic web techniques to the task of modeling the competences of a web-based research community  interop. we are not aware of any example of fully implemented knowledge acquisition value chain  where the acquired knowledge is first  extensively validated through the cooperative effort of an entire web community  and then  put in operation  to improve accessibility of web resources. the adopted techniques are fully general and the tools and interfaces developed within interop can be applied to any other domain. for example  in the last year of the project the glossary learning procedure will be available as a web application and will be experimented by industrial partners to build glossaries in different business domains.

figure 1. competence similarity of interop members.
given the wide spectrum of methodologies used  text mining  glossary and taxonomy enrichment  semantic indexing  a complete analysis of related work is impossible for space restrictions. we concentrate on what we consider the most original part of this work  taxonomy learning. taxonomy learning is a three stage process: terminology extraction  e.g.  park et al.  1    glossary extraction  like  klavans and muresan  1    and finally  extraction of hypernymy relations between terms  among the others  the surveys in  maedche et al.  1  and  cimiano et al.  1  . while a variety of methods address specific phases of taxonomy learning  no published work addresses the complete process in all its aspects  like we do. another difference is the predominant use  in the literature  of trained machine learning methods  miliaraki and androutsopoulos  1 : the availability of training sets cannot be assumed in general  and  furthermore  preparing a training set requires professional annotators  like e.g. in trec1 contests.
the algorithms used to learn taxonomic relations are mostly based on the analysis and comparison of contextual features of terms  extracted from their occurrences in texts  see  cimiano et al.  1  for a comparison of different vectorbased hierarchical clustering algorithms . instead  we use a knowledge-based method that orders terms using kind-of relations extracted from their definitions and from a generalpurpose semantic lexicon. the main advantage is that the principles that justify the resulting term ordering are clear  consistently applied  easier to evaluate and modify. on the contrary  evaluation of clustering algorithms is difficult and the results are hardly comparable: usually  the error rate of hypernymy extraction is over 1%  caraballo  1; widdows  1; cimiano et al.  1 . furthermore  performance is evaluated with reference to the judgment of two-three human evaluators  often the authors themselves  rather than submitted to the user community  as we do. in summary  the comparison with existing literature shows that the work presented in this paper promotes some progress in the automatic enrichment and use of semantic resources for knowledge management in real-world applications.
acknowledgments
this work is partially funded by the interop noe  1   1th european union fp.
