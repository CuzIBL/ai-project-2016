 
information extraction  ie  aims at extracting specific information from a collection of documents. a lot of previous work on 1 from semi-structured documents  in xml or html  uses learning techniques based on strings. some recent work converts the document to a ranked tree and uses tree automaton induction. this paper introduces an algorithm that uses unranked trees to induce an automaton. experiments show that this gives the best results obtained so far for ie from semi-structured documents based on learning. 
1 	introduction 
information extraction aims at extracting specific information from a collection of documents. one can distinguish between ie from unstructured and from  semi-  structured texts  muslea  1 . extracting information from web documents belongs to the latter category and gains importance  levy et al  1 . these documents are not written in natural language  but rather involve explicit annotations such as html/xml tags to convey the structure of the information  making the methods tuned towards natural language unusable. 
　while special query languages exist  bry and schaffert  1; xql  1   their use is time consuming and requires nontrivial skill. as argued in  muslea et al  1; kushmerick  1   there is a need for systems that learn to extract information from a few annotated examples {wrapper induction . several machine learning techniques for inducing wrappers have been proposed. examples are multistrategy approaches  freitag  1  and various grammatical inference techniques that induce a kind of delimiter-based patterns  muslea et al  1; freitag and mccallum  1; freitag and kushmerick  1; soderland  1; freitag  
1; hsu and dung  1; chidlovskii et a/.  1 . all these methods treat the document as a string of characters. 
　structured documents such as html and xml documents  however  have an explicit tree structure. in  kosala et al  1b; 1a   it is argued that one can better exploit this tree structure and use tree automata  comon et al  1 . the document tree is converted in a ranked binary tree and ktestable tree automata  rico-juan et al.  1  are induced and then used for the extraction task. typically in a ie task from structured documents  there is some structural context close to the target. after linearisation in a string  this context can be arbitrarily far away  making the learning task very difficult for string based methods. while binarisation may also increase the distance between the context and the target  they remain closer  and the learning task should be easier. this is confirmed by the experiments in  kosala et al  1b; 1a . if distance between the relevant context and the target is indeed a main factor determining the ability to learn an appropriate automaton  then an algorithm inducing a wrapper directly from the unranked tree should perform even better. this path is pursued in the current paper. as in  kosala et al.   1b; 1a  user intervention is limited to annotating the field to be extracted in a few representative examples. string based methods require substantially more user intervention  such as splitting the document into small fragments  and selecting some of them for use as a training example  e.g.  soderland  1 ; the manual specification of the length of a window for the prefix  suffix and target fragments  freitag and mccallum  1; freitag and kushmerick  1   and of the special tokens or landmarks such as ''   or''  freitag and kushmerick  1; muslea et al.  1 . 
　the rest of the paper is organized as follows. section 1 provides some background on unranked tree automata and their use for ie. section 1 describes our methodology and introduces our unranked tree inference algorithm. results are described in section 1  related work in section 1. section 1 concludes. 
1 	preliminaries 
grammatical inference and information extraction. 
grammatical inference  also called automata induction  grammar induction  or automatic language acquisition  refers to the process of learning rules from a set of labeled examples. the target domain is a formal language  a set of strings over some alphabet  and the hypothesis space is a family of grammars. the inference process aims at finding a minimum automaton  the canonical automaton  that is compatible with the examples. there is a large body of work on grammar induction  for a survey see e.g.  murphy  1; parekh and honavar  1 . 
　in grammar induction  we have a finite alphabet formal language . given a set of examples in 

and a  possibly empty  set of examples not in   the task is to infer a deterministic finite automaton  dfa  that accepts the examples in  and rejects those in . in  freitag  1   an ie task is mapped into a grammar induction task. 
a document is converted into a sequence of tokens  from examples are transformed by replacing the token to be extracted by the special token x. then a dfa is inferred that accepts all the transformed examples. in our case  a document is converted into a tree and the token to be extracted  at a leaf  is replaced by the special token x. then a tree automaton is inferred that accepts all the transformed examples. when using the learned automaton  a similar transformation is done. each token that is a candidate for extraction is in turn replaced by x. the token replaced by x is extracted iff the transformed document is accepted by the automaton. 
unranked tree automata. some existing algorithms for string automaton induction have been upgraded to ranked tree automaton induction  e.g.  rico-juan et al  1; abe and mamitsuka  1  1. by converting documents  which are unranked  into binary trees  which are ranked   tree automaton induction can be used for ie as shown in  kosala et al  1b . the present work avoids binarisation and uses unranked trees. unranked tree automata have been studied since the late 1's  see e.g.  bruggemann-klein et al  1  for a survey. to our knowledge  algorithms for inducing them do not yet exist. this paper is a first step in this direction. 
an unranked label is a label with a variable rank  arity . 
thus the number of children is not fixed by the label. given a set v of labels in an unranked alphabet  we can define   the set of all  unranked  trees  as follows: 
  a is a tree where 
  a u1 ...  un  is a tree  where  and each  is a tree. 
an unranked tree automaton  uta  is a quadruple 
　　　　　  where v is a set of unranked labels  q is a finite set of states  is a set of final  accepting  states  and is a set of transitions where each transition is of the form 	and e is a regular expression over q. 
　a bottom up uta processes trees bottom up. when a leaf node is labeled v and there is a transition  such that e matches the empty string  then the node is assigned state q. when an internal node is labeled v  its children have been assigned states qi ...  qn  and there is a transition 
such that the string  matches the regular expression e  then the node is assigned state q. a tree is accepted if the state of its root is assigned an accepting state 
1 	approach and algorithm 
preprocessing. fig. 1 shows a representative task. for dealing with text nodes  we follow the approach described in  kosala et al.  1b : we replace most text nodes by cdata1  making an exception for so-called distinguishing context  specific text that is useful for the identification of the 
'ranked: the label determines the number of children. 
1  see fig. 1 for an example. field of interest. e.g.  the text  organization:  may be relevant for the extraction  hence this text should not be changed into cdata. in each data set  at most one text field is identified as distinguishing context. it is found automatically using the approach described in  kosala et al.  1b . 

figure 1: the fields to be extracted are the fields following the alt.name and organization fields. a document consists of a variable number of records. the number of occurrences of the fields to be extracted is variable  from zero to many . also the position is not fixed. 

figure 1: the left figure is an html tree. the right one is the same tree after abstracting the text nodes 
approach. our approach for information extraction has the following characteristics: 
  strings stored at the text nodes are treated as single la-bels. if extracted  the whole string is returned. 
  one automaton is learned for one type of field to be ex-tracted  e.g.  the field following  organization . 
  in examples used during learning  one target field  a text node  is replaced by x. a document gives rise to several examples when several targets occur. the learning phase proceeds as follows: 
  replace in the examples the target by  a:   the distin-guishing context s   if present  by  and all other text fields by cdata. 
  map examples to trees and learn a tree automaton. the extraction phase repeats for all candidate targets: 
  map the document to a tree and replace the candidate target by u z   the distinguishing context s   if present  

1 	information extraction 

by  ctx  and all other text fields by cdata. 
  run the tree automaton; if the tree is accepted  then output the original text of the node labeled with x. 
1 	local unranked tree automaton inference algorithm 
preliminaries. a partition of a set 1 is a set of disjoint nonempty subsets of s such that the union of the subsets is s. the height of a tree is defined 
the ti are called subtrees oft  and subtrees of ti are also subtrees of t. when a tree t is  cut off  at level k  this means all subtrees at level k become leaves. thus the height of the cutoff tree can be at most k. given a tree t  the set of roots is the singleton containing t cut off at a;; the set of forks contains all subtrees of t of height at least k  cut off at k  and the set ofsubtrees  contains all subtrees oft of height at most k. roughly  these sets respectively collect all subtrees of height k at the top  in the middle  and at the bottom oft. see  rico-juan et al.  1  for a formal definition. 
the algorithm. we have designed a procedure  which is shown in algorithm 1  to learn an unranked tree automaton from a set t of positive examples. the inferred automaton is  local  in the sense that it identifies a tree with its 1-forks as defined above. note that dtds for xml do the same  murata et al   1   in addition to the input t  it takes as additional parameter a positive integer which is the parameter for the k.contextual subroutine that it calls. 
　in a first for-loop  our algorithm collects all 1-forks  1subtreesand 1-roots. the latter become final states. however  before these steps  the node labels of each input tree t are rewritten using the function convert jabels t . this function  algorithm 1  rewrites node labels. the label v of the root of a subtree is changed into vx if that subtree contains a  x  and into vctx if that subtree contains a  ctx . the effect is that the special labels are remembered up to the root1. 
as a result  the tree automaton is not purely local. 

　an element  v  str  of the partition gives all positive examples for a particular label v. the regular expression e captures the regularity in these examples  the document content model in xml terminology . to obtain sufficient generalization  we decided  after some experimentation  to distinguish three cases. if all children of a vx node are long enough  we construct a dfa using the k-contextual algorithm with k-value kc  otherwise with value 1. for other labels  either a vetx label or an original label   we ignore the content of the children and accept any sequence  the regular expression 
　using a result of  muggleton  1   it is quite straightforward to make an incremental version of algorithm 1. we omit it here due to lack of space. using a basic result of  angluin  1   we can prove: 
theorem 1 every unranked tree language that is definable by a local unranked tree automaton with k-contextual regular expressions is identifiable in the limit  from positive examples only  by our algorithm. 

1 	experimental results 
we evaluated our method on two semi-structured data sets commonly used in ie research  available online from http://www.isi. edu/-muslea/rise/ : a collection of web pages containing people's contact addresses  the internet address finder  iaf  database  and a collection of web pages about stock quotes  the quote server  qs  database . for each dataset  there are two tasks; they are the extraction of alternative and organization fields in the iaf dataset and of the date and volume fields in the qs dataset. each dataset consists of 1 documents. the number of fields to be extracted is respectively 1  iaf-organization   1  iafalt.name   1  qs-date   and 1  qs-vol . we choose these datasets because they are benchmark datasets that are commonly used for research in ie; hence they allow us to compare results. in order to provide a close comparison  we use the same train and test splits as in  freitag and kushmerick  1 . in addition  they require the extraction of a whole leaf node  our algorithms are designed for that task . 
1 
moreover  the results obtained so far  muslea et al.  1; 
hsu and chang  1  indicate that they are difficult tasks. in fact one of the authors in  muslea et al.  1  has tried to build a hand-crafted extractor given all available documents from the qs dataset and achieved only 1% accuracy  or recall in our criteria below . we also test our method on a significantly reduced shakespeare xml dataset  available online from http://www.ibiblio.org/bosak/ . we use the same training and test set as in  kosala et al.  1b . the task on this dataset is to extract the second scene from one act in a particular play. 
　we apply the commonly used criteria of ie research for evaluating our method. precision p is the number of correctly extracted objects divided by the total number of extractions  while recall r is the number of correct extractions divided by the total number of objects present in the answer template. the fl score is defined as 1pr/ p + r   the harmonic mean of p and r. table 1 shows the results we obtained as well as those obtained by some current state-ofthe-art string-based methods: an algorithm based on hidden markov models  hmms   freitag and mccallum  1   the 
stalker wrapper induction algorithm  muslea et al.  1   and 
bwi  freitag and kushmcrick  1 . we also include the results of the k-testable algorithm in  kosala et al.  1b  which works on ranked trees. the results of 1mm  stalker and bwi are taken from  freitag and kushmerick  1 . all tests are performed with 1-fold cross validation following the splits used in  freitag and kushmerick  1   except in the small shakespeare dataset which uses 1-fold cross validation. each split has 1 documents for training and 1 for testing. we refer to section 1 for a description of these methods. 
　table 1 shows the best results of the unranked method with a certain kc  cross-validation on one fold of 1% random training and test examples.  as can be seen  our method is the only one giving optimal results. 
　table 1 shows the value of k and kc used respectively by the k-testable and the unranked algorithm. it is wellknown that when learning from positive examples only  there is a problem of over-generalization. our algorithm requires a cross-validation on the value of  to avoid overgeneralization. 
　algorithm 1 runs in time   where n is the total number of nodes in the training examples and c is a constant. it takes an average time between 1 and 1 ms in a 1 ghz pentium 1 pc for algorithm 1 to learn an example in the iaf and qs datasets. 
1 	related work 
the ie work for  semi-  structured texts can be divided into systems built manually using a knowledge engineering approach  e.g.  hammer et al.  1  and systems built  semi-  automatically using machine learning techniques or other algorithms. the latter are called wrapper induction methods. we briefly survey them. 
　the three systems referred to in table 1 learn wrappers based on regular expressions. bwi  freitag and kushmerick  1  uses a boosting approach in which the weak learner learns a simple regular expression with high precision but low 
information extraction 

recall. the hmm approach  freitag and mccallum  1  learns a hidden markov model; it solves the problem of estimating probabilities from sparse data by using a statistical technique called shrinkage. this model has been shown to achieve state-of-the-art performance on a range of ie tasks. stalker  muslea et al  1  induces extraction rules that are expressed as simple landmark grammars  a class of finite automata; it performs hierarchical extraction guided by a manually built embedded catalog tree that describes the structure of the fields to be extracted. 
　several techniques based on naive-bayes  two regular language inference algorithms  and their combinations for ie from unstructured texts are described in  freitag  1 . whisk fsoderland  1  learns extraction rules based on a form of regular expression patterns with a top-down rule induction technique.  chidlovskii et ai  1  describe an incremental grammar induction approach; they use a subclass of deterministic finite automata that do not contain cyclic patterns. the softmcaly system  hsu and dung  1  learns separators that identify the boundaries of the fields of interest. ihsu and chang  1  propose two classes of softmealy extractors: single pass  which is biased for tabular documents such as qs data  they reach up to 1% recall   and multi pass  which is biased for tagged-list document such as iaf data  they reach up to 1% recall . we cannot really compare results because the experimental setting is different. 
　all above methods use algorithms for learning string languages and require some manual intervention. hmms and bwi require to specify a window length for the prefix  suffix and the target fragments. stalker and bwi require to specify special tokens or landmarks such as  or  ; . softmcaly extractors in  hsu and chang  1  requires to choose between single and multi pass bias. 
　in  kosala et al  1b   the document is converted in a ranked  binary  tree and an algorithm is used that induces a k-testable tree automaton. however  as binarisation increases the distance between target and distinguishing context  large k are needed and the resulting automaton is precise but does not generalize enough  table 1 . in  kosala et al  1a   the same authors generalize the obtained automaton by selectively introducing wild-card labels. this gives some modest improvement in recall but does not solve the problem. our unranked tree automaton induction algorithm does. 
　the most apparent limitation of our method is that it can only output a whole text node. to overcome this  it could be extended with a second step where string based methods are used to extract part of the text node. for example  to extract the substring  the web  from the whole string  data on the web   fig. 1 . another limitation is that our method only output a single field  slot  in one run. 
　finally  a disadvantage that is not apparent from the results reported above  is that when the identification of target fields does not require dependencies between nodes in the tree but can rely on a local pattern  e.g.  the field to be extracted is always surrounded by specific delimiters   our tree based method needs more examples to learn the same extraction rule as methods that automatically focus on local patterns. intuitively  more variations in further-away nodes need to be observed before these variations are considered irrelevant. this is simply an instance of the well-known trade-offbetween the generality of a hypothesis space and the efficiency with which the correct hypothesis can be extracted from it. 
　some other approaches that exploit the structure of the documents are: wl1  cohen et al  1   a logic-based wrapper learner that uses multiple  string  tree  visual  and geometric  representations of the html documents. in fact  wl1 is able to extract all four tasks in the iaf and qs datasets with 1% recall; and wrappers  sakamoto et al  1  that identify a field with a path from root to leaf  imposing conditions on each node in the path. 
1 	conclusion 
we have presented an algorithm for the inference of a local unranked tree automaton with k-contextual regular expressions and have shown that it can be used for ie from structured documents. our results confirm the claim of  kosala et al  1b  that utilizing the tree structure of the documents is worthwhile for structured ie tasks. whereas the latter work transforms the positive examples into binary ranked trees  we use them directly as unranked trees. our results are optimal for the previously considered benchmarks  substantially improving upon the published results of other string and tree based methods and are a strong indication that unranked tree automata are much better suited than ranked ones for structured ie tasks. 

   possible future work includes experiments with larger and more difficult datasets  adapting tree automata for multi slot extraction in one run  and a more formal analysis of the algorithm. 
acknowledgements 
we thank nicholas kushmerick for providing us with the datasets used for bwi experiments and anonymous reviewers for their very valuable comments. this work is supported by the fwo project query languages for data mining. hendrik blocked is a post-doctoral fellow of the fund for scientific research of flanders. 
