 
the paper describes a branch and bound scheme that uses heuristics generated mechanically by the mini-bucket approximation. this scheme is presented and evaluated for optimization tasks such as finding the most probable explanation  mpe   in bayesian networks. the mini-bucket scheme yields monotonic heuristics of varying strengths which cause different amounts of pruning  allowing a controlled tradeoff between preprocessing and search. the resulting branch and bound with mini-bucket heuristic  bbmb   is evaluated using random networks  probabilistic decoding and medical diagnosis networks. results show that the bbmb scheme overcomes the memory explosion of bucket-elimination allowing a gradual tradeoff of space for time  and of time for accuracy. 
1 	introduction 
this paper proposes a new scheme for augmenting branch-and-bound search with heuristics generated automatically by the mini-bucket algorithms. mini-bucket is a class of parameterized approximation algorithms based on the recently proposed bucket-elimination framework. the approximation uses a controlling parameter which allows adjustable levels of accuracy and efficiency  dechter and rish  1 . the algorithms were presented and analyzed for deterministic optimization tasks and probabilistic tasks  such as finding the most probable explanation  mpe   belief updating  and finding the maximum a posteriori hypothesis. encouraging empirical results were reported for mpe on randomly generated noisy-or networks  on medical-diagnosis cpcs networks  and on coding problems  rish et a/.  1 . in some cases  however  the approximation was largely suboptimal  even when using the highest feasible accuracy level. 
   one way of improving the mini-bucket scheme is by embedding it in a general search algorithm. the intermediate functions created by the mini-bucket scheme can be interpreted as a heuristic evaluation function and 
1 	constraint satisfaction 
used by any heuristic search algorithm. for instance  an upper bound on the probability of the best possible extension of any partial assignment in an mpe task can be derived. the tightness of these bounds can be controlled by the accuracy parameter of the mini-bucket scheme. 
in this paper we evaluate this idea using branch-and-
bound search which searches the space of partial assignments in a depth-first manner. it expands a partial assignment only if its upper-bounding heuristic estimate is larger than the currently known best lower bound. the virtue of branch-and-bound compared to best-first search  is that it requires a limited amount of memory and can be used as an anytime scheme - when interrupted  branch-and-bound outputs the best solution found so far. in  kask and dechter. 1a  we apply this approach to best-first search and compare the two schemes. 
the resulting search algorithm  bbmb  branch and 
bound with mini-bucket heuristics  is evaluated and compared against other algorithms  such as bucket elimination  the mini-bucket scheme and iterative belief propagation   on a number of test problems  including cod ing networks  random networks  and cpcs networks. we show that the bbmb scheme is effective for a larger range of problems because of its gradual trade-off between preprocessing and search  and time and accuracy. unlike bucket elimination  bbmb does not suffer from memory explosion and is often quicker to find an optimal solution. we investigated this approach for the optimization task of finding the most probable explanation  mpe . 
　section 1 presents an overview of the relevant algorithms. in section 1 we describe our branch-and-bound scheme and its guiding heuristic function. section 1 presents empirical evaluations  while section 1 provides discussion and conclusions. for space reasons we omit all proofs. for more details see  kask and dechter  1a . 
1 	related work 
mpe appears in applications such as medical diagnosis  circuit diagnosis  natural language understanding and probabilistic decoding. for example  given data on clinical findings  mpe can postulate on a patient's probable affliction. in decoding  the task is to identify the most likely input message transmitted over a noisy channel 



figure 	1: 	belief 	network 	p f d c b a  	= 

given the observed output. researchers in natural language consider the understanding of text to consist of finding the most likely facts  in internal representation  that explains the existence of the given text. in computer vision and image understanding  researchers formulate the problem in terms of finding the most likely set of objects that explains the image. scientific theories are models that attempt to fit the given observations  and so on. 
　it is known that solving the mpe task is np-hard. complete algorithms for mpe use either the cycle rutset technique or the join-tree-clustering  pearl  1  and the bucket-elimination scheme  dechter  1 . however  these methods work well only if the network is sparse enough to allow small cutsets or small clusters. following pearl's stochastic simulation algorithms for the mpe task  pearl  1   the suitability of stochastic local search  sls  algorithms for mpe was studied in the context of medical diagnosis applications  peng and lleggia  1    peng and reggia  1  and more recently in  kask and dechter  1b . best first search algorithms were also proposed  shimony and charniak  1  as well as algorithms based on linear programming  santos  1 . 
1 	background 
1 	notation and definitions 
belief networks provide a formalism for reasoning about partial beliefs under conditions of uncertainty. they are defined by a directed acyclic graph over nodes representing random variables of interest. 
definition 1  belief networks  given a set  x of random variables over multivalued do-
mains  ...  a belief network is a pair  g. p  where g is a directed acyclic graph and p =  
 = 	are conditional probability ma-
trices associated with the set parent set of an assignment =   can be abbreviated to x = 
represents a probability 	distribution 	- 
 where   is the projection of x over 
a subset s. an evidence set e is an instantiated subset of variables. the argument set of a function h is denoted 
s{h . 
definition 1  most probable explanation  given a belief network and evidence c  the most probable explanation  mpe  task is to find an assignment such that 
definition 1  graph concepts  an ordered graph is a pair  g  d  where g is an undirected graph and d ＊= is an ordering of the nodes. the width 
of a node in an ordered graph is the number of its earlier neighbors. the width w d  of an ordering d  is the maximum width over all nodes. the induced width of an ordered graph  w* d   is the width of the induced ordered graph obtained by processing the nodes recursively  from last to first; when node x is processed  all its earlier neighbors are connected. the moral graph of a directed graph g is the undirected graph obtained by connecting the parents of all the nodes in g and then removing the arrows. an example of a belief network is given in figure la  and its moral graph in figure lb. 
1 	bucket and mini-bucket algorithms 
bucket elimination is a unifying algorithmic framework for dynamic-programming algorithms applicable to probabilistic and deterministic reasoning  dechter  1 . the input  to a bucket-elimination algorithm consists of a collection of functions or relations  e.g.  clauses for propositional satisfiability  constraints  or conditional probability matrices for belief networks . given a variable ordering  the algorithm partitions the functions into buckets  each placed in the bucket of its latest argument in the ordering. the algorithm has two phases. during the first  top-down phase  it processes each bucket  from the last variable to the first by a variable elimination procedure that computes a new function  placed in a lower bucket. for mpe  this procedure computes a product of all probability matrices in the bucket  and maximizes over the bucket's variable. during the second  bottomup phase  the algorithm constructs a solution by assigning a value to each variable along the ordering  consulting the functions created during the top-down phase. 
theorem 1  dechter  1  the time and space complexity of elim-mpe  the bucket elimination algorithm for mpe  are exponential in the induced width w*  d  of the network's ordered moral graph along ordering d.  
　　the mini-bucket elimination is an approximation scheme designed to avoid the space and time problems of full bucket elimination. in each bucket  all the functions are first partitioned into smaller subsets called mini-buckets which are then processed independently. here is the rationale. let  be the functions in bucketp. when elim-mpe processes bucketp  it computes the function  the minibucket algorithm  on the other hand  creates a partitioning where the mini-bucket contains the functions the approximation will compute clearly  thus  
	kask and dechter 	1 

figure 1: algorithm approx-mpe i  

the algorithm computes an upper bound on the probability of the mpe assignment. subsequently  in its second phase  the algorithm computes an assignment that provides a lower bound. the quality of the upper bound depends on the degree of the partitioning into mini-buckets. given a bound parameter i  the algorithm creates an partitioning  where each mini-bucket includes no more than  variables. algorithm approx-
  sometimes called mb i   is described in figure 1. the algorithm outputs not only an upper bound on the  and an assignment  whose probability yields a lower bound   but also the collection of augmented buck-
ets. by comparing the upper bound to the lower bound we can always have a bound on the error for the given instance. 
theorem 1  decider and rush  	1  algorithm 
 generates an upper bound on the exact mpe and its time and space complexity is  
　　when the bound i is large enough  i.e. when i w*   the algorithm coincides with full bucket elimination. 
example 1 figure  illustrates how algorithms 
elim-mpe and approx-mpe i  for i = 1 process the network in figure 1 a  along the ordering 
c  b . elim-mpe records new functions  
 and  during its backwards 
phase. then  in the bucket of a  it computes mpe subsequently  an mpe assignment is an 
evidence  is computed by maximizing the product functions in each bucket. namely   and so on. the approximation approx-mpe 1  splits bucket 	into two mini-buckets each containing no more than 1 vanables  and generates 	and 	an upper bound 	on the mpe value is computed in a 's bucket  a suboptimal tuple 
1 	constraint satisfaction 
1 	heuristic search with mini-bucket 
1 	notation 
given an ordered set of augmented buckets generated by the mini-bucket algorithm along  we use the following convention 
   denotes an input conditional probability matrices placed in bucket   namely  its highest-ordered variable is  
  hp  denotes an arbitrary function in bucket p generated by the mini-bucket algorithm. 
   denotes a function created by the  minibucket in bucket  
   denotes an arbitrary function in bucket  
　we denote by buckets l..p  the union of all functions in the bucket of  through the bucket of  remember that  denotes the set of arguments of function /. 
1 	the idea 
we will now show that the new functions recorded by the mini-bucket  algorithm can be used to express upper bounds on the most probable extension of any partial assignment. therefore  they can serve as heuristics in an evaluation function which guides either a best-first search or a branch-and-bound search. 
definition 1  exact evaluation function  
let 	the probability of the most 
probable extension of x   denoted 
　the above product defining f* can be divided into two smaller products expressed by the functions in the ordered augmented buckets. in the first product all the 

arguments are instantiated  and therefore the maximization operation is applied to the second product only. denoting  

and 

we get  
during search  the function can be evaluated over the partial assignment can be estimated by a heuristic function. our first proposal is to estimate by a function  which is the product functions generated by the mini-bucket algorithm 
which reside in bucket  it can be shown  using properties of mini-bucket scheme  that this product indeed provides an upper bound on    yielding an admissible heuristic. 
however  the heuristic function  is non-monotonic. 
in fact  even if provided with augmented buckets that are generated by exact full bucket elimination  this heuristic may still be nonmonotonic. a heuristic function is monotonia if the evaluation function along any path in 
the search tree is not increasing. 
　　we next modify  to make it monotone and more accurate. the modified heuristic function adds to the product of  all those  functions in buckets that were generated in buckets processed before bucket p. the rationale is that some conditional probability matrices in  are approximated by  functions that are placed below bucket   because they are not defined over  
definition 1 given an ordered set of augmented buckets  the heuristic function  is the product of all the h functions that satisfy the following two properties; 1  they are generated in buckets and 1  they reside in buckets 1 through p. namely   where  
is generated by a bucket processed before bucket p.  
theorem 1  mini-bucket heuristic  for every partial assignment 
variables  the evaluation function  is: 1  admissible - it never underestimates the probability of the best extension of  monotonia  namely 

the following proposition shows how and can be updated recursively based on and and functions residing in bucket   from now 
on we will use h to mean ho.  
proposition 1 given 	a 	partial 	assignment 	=  both  and  can be computed re-
cursively by 
		 i  
algorithm bbmb i  
input: a belief network bn -  ordering d. output: an mpe assignment  or a lower bound and an upper-bound on the mpe. 
1. initialize: run  algorithm which generates a set of ordered augmented buckets and an upper-bound on 
mpe. set lower bound l to 1. set  current variable index  to 1. 
1. search: execute the following procedure until variable a'i has no legal values left. 
  expand: given a partial instantiation  compute 
prune those assignments is smaller than the lower bound l. 
  forward: if has no legal values left  goto backtrack. otherwise let be the best extension to  according to /. then set 
and goto backtrack. otherwise remove v from the list of legal values. and goto expand. 
  backtrack: k exit. otherwise s e t a n d repeat the forward step. 
figure 1: algorithm bbmb i  
		 1  
1 	search with mini-bucket heuristics 
the tightness of the upper bound generated by the minibucket approximation depends on its /-bound. larger values of i yield better upper-bounds  but require more computation. therefore  branch-and-bound search  if parameterized by  allows a controllable tradeoff between preprocessing and search  or between heuristic strength and its overhead. 
　in figure 1 we present algorithm bbmb i . this algorithm is initialized by running the mini-bucket  algorithm  producing the set of ordered augmented buckets. it then traverses the search space in a depth-first manner  instantiating variables from first to last. throughout the search  the algorithm maintains a lower bound on the probability of the mpe assignment  which corresponds to the probability of the best full variable instantiation found thus far. it uses the heuristic evaluation function  to prune the search space. search terminates when it reaches a time-bound or when the first variable has no values left. in the latter case  the algorithm has found an optimal solution. 
　the heuristic function generated by the mini-bucket approximation can also be used to guide any  type algorithm. for a description of best-first search with mini-bucket heuristics see  ka.sk and dechter  1a . 
1 	experimental methodology 
we tested the performance of our scheme on sevkask and dechter 1 

　n c.p.k elirn 
m p e 
# time  
  
w* opt mb / 
bbmb i=1 
# time  mb / 
bbmb i = 1 
# time  mb / 
bbmb i = 1 
# time  mb / 
bbmb i = 1 
# time  mb / 
bbmb i= 1 
# time  mb / 
bbmb i=1 
#|time  mb / 
bbmb i=1 1 
1 1 1.1  1  1 1.1  
1 1  1.1  
1 1  1.1  
1.1  1.1  
1.1  1.1  
1.1  1 1  
1.1  1l l.1  
1.1  1 
1 1 1 l l   
1  1 1.1  
1.1  1.1  
1  1  1 1  
1.1  1.1  
1.1  1.1  1 1.1  
1.1  1.1  
1.1  1 
1 1 1.1  1  1  1.1  1  1.1  
1 1  1.1  
1 1  1 1  
1.1   1.1  1.1 
  1.1  
1.1  1.1  
1.1  1 
1 1 1.1  
1  1  1.1  1 1  1  1.1  1.1  
1 1  1.1  
1.1   1.1  1.1l 
  1.1  
1 1  1l 1  
1.1  1 
1 1 1.1  1  1 1 1  
1.1  1 1  1.1  1.1  
1.1  1.1  
1  1  1.1  1  1  
1  1 
1.1 1.1  
1 1 
1 1 1.1  1  1 1.1  
1.1  1.1  
1.1  1 1  
1 1  1.1  
1.1  1.1  1 l.1  1.1; 1.1 1.1  
1.1    
'fable 1: random mpe. time bound 1 sec. 1 samples. 

eral types of networks. on each problem instance we ran bucket elimination  elim-mpe   mini-bucket approximation with various i-bounds  and branch and bound with some levels of mini-bucket heuristics 

　the main measure of performance of the approximation algorithm given a fixed time bound  is the accuracy ratio  between the probability of the 
solution found by the test algorithm and the probability of the optimal solution whenever is available. we also record the running time of each algorithm. 
　　we report the distribution of problems with respect to 1 predefined ranges of accuracy :  
we recorded the 
number of problems that elim-mpe solved as well as the average induced width  of the test problems. because of space restrictions in most cases we report  only the number of problems that fall in the highest accuracy range  however  since most problems were solved by bbmb optimally  we lose only minimal information. 
　in addition  during the execution of bbmb we also stored the current lower bound  at regular time intervals. comparing the lower bound against the optimal solution  allows reporting the accuracy of bbmb as a function of time. 
1 random bayesian networks and noisy-or networks 
random bayesian networks and noisy-or networks were randomly generated using parameters  n  k  c  p   where n is the number of variables  k is their domain size  c is the number of conditional probability matrices and p is the number of parents in each conditional probability matrix. 
　the structure of each test problem is created by randomly picking c variables out of n and for each  randomly selecting p parents from preceding variables  relative to some ordering. for random bayesian networks  each probability table is generated randomly. for noisyor networks  each probability table represents an orfunction with a given noise and leak probabilities : 
　tables 1 and 1 present results of experiments with random bayesian networks and noisy-or networks respectively. in each table  parameters n  k and p are fixed  while c  controlling network's sparseness  is changing. for each c  we generate 1 problem instances. each entry in the table reports the number of instances that fall in a specific: range of accuracy  as well as the average running time of each algorithm  note that bbmb time includes the preprocessing time by mb . 
 for example  table 1 reports the results with random problems having  there are 1 hori-
zontal blocks  each corresponding to a different  value of c. each block has two rows  one for mb and one for 
bbmb  reporting the number of problems in accuracy range of 1 and the average running time. 
　the second column reports the results of elim-mpe  namely the number of instances it solved  their average  and running time. the rest of the columns report 
results of mb and bbmb for various levels of  looking at the first line  in table 1 we see that in the best accuracy range   solved only 
1 problems using 1 seconds on the average. bbmb with  solved  instances in this range while  ing 1 seconds on the average. note that elim-mpe on the other hand solved 1 instances using much more time than bbmb with any  
　each row demonstrates a tradeoff between preprocessing by mb and subsequent search by bbmb. as expected  mb can solve more instances as / increases while its average time increases. at the same time  the search time of bbmb decreases as  increases. we observe that the total bbmb time improves when  increases until a threshold point and then worsens. we have highlighted the best performance point in each row. below this threshold  the heuristic function is weak  resulting in long search. above the threshold  the extra preprocessing is not cost effective. as problems become harder bbmb achieves its best performance at higher thresholds. for example  when c is 1 and 1  the threshold is  when c is 1  1 and 1  it is  and when c is 1  it is  
　table 1 reporting on noisy-or shows similar results  based on  1 range . on this class bbmb is very 

	effective and solved all problems exactly  while elim-
1 	constraint satisfaction 

 n c p elirn 
mpe 
# time  opt mb / 
bbmb i = 1 
# time  mb / 
bbmb 
1 = 1 m b / 
bbmb i=1 
# time  mb / 
bbmb 
1 
#ltime  1 
1 1.1  1  1 1l 1  
1.1  1  1  
1.ll  1 1  
1.1  1  1  
1.1  1 
1 i  1  
1  1 1  1  1.1  1.1  1 c . 1  1  1  1 1  1  1  
1 1  1 
1 1  1 1 1  1.1  1.1  
1.1  1 1 
1.1  1.1  
1 1  1 
1 1 -  1  1 1  1  1 1  1.1  
1.1  1.1;
1.1 
   1.1  
1 1  table 1: noisy-or mpe. noise 1  leak 1. time bound 1 sec. 1 samples. 1 evidence. 
mpe solved almost none. for 	1 bbmb can solve 
all 1 instances  with threshold  
　in figures 1 and 1 we provide an alternative view of the performance of bbmb i . let  be the 
fraction of the problems solved completely by bbmb i  by time /. 	each graph in figures 1 and 1 plots  for some specific value i. figure 1 shows the 	distribution 	or 	random 	bavesian 
                                                         v networks when n=1  c=1  k=1 and p=1  corresponding to the last row in table 1   whereas figure 1 shows the distribution of  for noisy-or net-
works when n = 1  1 = 1 and p=1  corresponding to the second row in table 1 . 
　figures 1 and    display a trade off between preprocessing and search. clearly  if  then  completely dominates fbbmb j  t . 
for example  in figure 1 bbmb jo  completely dominates  when  and  intersect  they display a trade-off a.s a function of time. for example  if we have only few seconds  bbmb tj  is better than bbmb i1 . however  when sufficient time is allowed  bbmb 1  is superior to bbmb g . 
　the same pattern appears in figure 1. bbmb 1  completely dominates bbmb 1   while there is a tradeoff between bbmb 1  and bbmb 1  depending on the amount of time allowed. 
1 	random coding networks 
our random coding networks fall within the class of car block codes. they can be represented as four-layer belief networks  figure 1 . the second and third layers correspond to input information bits and parity check bits respectively. each parity check bit represents an xor function of input bits u . input and parity check nodes are binary while the output nodes are real-valued. in our experiments each layer has the same number of nodes because we use code rate of  where 
k is the number of input bits and n is the number of transmitted bits. 
　given a number of input bits k  number of parents p for each xor bit and channel noise variance  a coding network structure is generated by randomly picking parents for each xor node. then we simulate an input signal by assuming a uniform random distribution of information bits  compute the corresponding values of the parity check bits  and generate an assignment to 

figure 1: random bavesian. n=1t   c=1  k=1  p=1. 1 samples. 

table 1: random coding. time  1sec. 1 samples. 
the output nodes by adding  laussian noise to each information and parity check bit. the decoding algorithm takes as input the coding network and the observed real valued output assignment and recovers the original input bit vector by computing or approximating an mpe assignment. 
　we tested two sets of random coding networks - k=1 and k-1 input bits. table 1 reports results on random coding networks having k=1. in addition to elimmpe and bbmb. we also ran iterative belief propagation  ibp  which was recently observed as the best performing decoding algorithm. it is identical to iterative application of peaits belief updating on tree-like networks  pearl  1 . for each  we generated and tested 1 samples divided into 1 different networks each simulated with 1 different  input bit vectors. 
　in table 1 we observe a familiar pattern of preprocessing search tradeoff. for each level of noise there is an empirical optimal  balancing preprocessing cost  and search. for = 1 the threshold is 1. as noise increases  the threshold increases. also  as noise increases  the role of search becomes more significant. although ibp is superior  because it is faster  for small 
	kask and dechter 	1 


figure 1: noisy-or. n = 1  c=1  p=1. 1 samples. 

figure 1: belief network for structured  1  block code with parent set size p-1 
noise  it has a shortcoming for high noise levels since its performance cannot be improved with extra time while bbmb can. 
   fable 1 reports the bit error rate  for  and  is a standard measure 
used in the coding literature denoting the fraction of input bits that were decoded incorrectly. when compared using  is about the same as  when the noise is small  but slightly better when the noise is large. 
figure 1 shows the distribution of for 
random coding networks when k=t1 and =r 1. we observe that no algorithm completely dominates any other algorithm. however  we can see a trade-off depend-
mb / bbmb i= 1 ibp 1 1/1 1 1 1/1 1 1 1/1 1 1 1/1 1 1 1/1 1 table 1: random coding ber 
1 	constraint satisfaction 

figure 1: bbmb random coding. k = 1  1   = 1. 1 samples. 
ing on the time available. when the time allowed is small  less then 1 seconds    with i less than 1 is better. however  when more time is allowed  is clearly superior. 
1 	cpcs networks 
as another realistic domain  we used the  networks derived from the computer-based patient care simulation system  and based on  and quick 
medical reference expert systems  pradhan et al.  1 . 
the nodes in  networks correspond to diseases and findings. representing it as a belief network requires some simplifying assumptions  1  conditional independence of findings given diseases  1  noisy-or dependencies between diseases and findings  and 1  marginal independencies of diseases. for details see  pradhan et al.  1 . 
　in table 1 we have results of experiments with two binary cpcs networks  cpes1b  n = 1  c = 1  and   n = 1  c = 1   with 1 and 1 instances respectively. each instance had 1 evidence nodes picked randomly. 
　　since network is solved quite effectively by mb we see that added search time is small  serving primarily to prove the optimality of the mb solution. 
on the other hand  on can solve a third of the instances accurately when is small  and more as increases.  can solve all instances accurately for when search takes little additional time. 
1 	discussion and conclusion 
our experiments demonstrate that combining branchand-bound with mini-bucket heuristics  bbmb  provides a powerful method of attacking optimization problems such as the mpe. 

cpcs1b m b / m b / mb / mb / 1 b b m b b b m b b b m b b b m b sample i = 1 i = 1 i = i 1 i=1c  1 1.1  1.1  1 1  1 1  1.1  1.1  1 1 1   1 1.1  1.1  1.1  1 1  1 -  1 -  1 -  1 -   1 1 1  1.1  1  1  1.1  1 -  1 -  1 -  1 -   1 1.1  1.1  1 -  1 -  1 -  1 -  ol-l 1 -   1 1 -  1 -  1 -  1 -  1 -  1 -  1 -  1 -  cpcs1b mb / mb / mb / mb / 1 b b m b b b m b b b m b b b m b samples 1 = 1 1 = 1 i=1 i = 1  1 1 l  1.1  1.1  1 1  1 1  1 1  1u 1  1.1   1 1  1  1 1  1.. 1  1 1  1 1  o -  1 -     -   1 1.1  1.1  1 1  1.1     -  1 -  1 -  1 -   1 1 1  1 1  1 1  1.1  1 1  1 1  1 -  1 -   1 1 1  1.1  1 1  1 1  1 1  1 -  1 -  1 -    table 1: cpcs networks. time 1 and 1 resp. 
　　on the one hand it avoids the storage bottleneck of full bucket elimination while frequently solving problems optimally. on the other hand  it improves output quality substantially over the mini bucket approximation  especially when the former is highly suboptimal. however  the most important feature of  is the ability to control the balance between preprocessing  for heuristic generation  and search  using its bounding 
pure search  and pure bucket-elimination  are two extremes ends of that spectrum. 
　in all our experiments  including random bayesian networks. noisy oh networks  coding networks and medical diagnosis cpcs networks  we observed that optimal performance-  measured by time within a certain accuracy range  occurred at an intermediate threshold point of  weaker heuristics  lying below the threshold  and stronger heuristics  lying above the threshold  were less cost-effective. we also observed that as problems grew harder  stronger heuristics became more cost 
effective. the control of the gradual change of spacetime-accuracy tradeoff of  now makes a larger set of problem solvable. 
　although the best threshold point may not be predictable for every problem instance  a preliminary empirical analysis can be informative when given a class of problems that is not too heterogeneous. 
　we have also tested the mini-bucket heuristic function with best-first search. we found that if we are interested in the optimal solution only   is often significantly faster than  when given sufficient time and space. however  unlike bbmb   is not an anytime algorithm  kask and dechter  1a .  dechter  1  r. dechter. bucket elimination: a unifying framework for probabilistic inference algorithms. in uncertainty in artificial intelligence  ua1   pages 1'  1. 
 kask and dechter  1a  k. kask and r. dechter. on the power of mini-bucket heuristics for improved search. ucl technical report  1. 
 kask and dechter  1b  k. kask and ft. dechter. stochastic local search for bayesian networks. in workshop on ai and statistics  aistat1   1. 
 pearl  1  j. pearl. probabilistic reasoning in intelligent systems. morgan kaufrnann  1. 
 peng and reggia  1  v. 	peng and j.a. 	reggia. 
	plausability of diagnostic hypothesis. 	in national 
conference  on artificial intelligence  aaai1   pages 1 1. 
peng and reggia  1  y. peng and .l.a. reggia. a connectionist model for diagnostic problem solving. ikvje transactions on systems  man and  'ybernetir.s  1. 
pradhan et al.  1  m. pradhan  g. provan  b. middleton  and m. henrion. knowledge engineering for large belief networks. in proc. tenth. conf. on uncertainty in artificial intelligence  1. 
 rish et al.  1  1. rish  k. kask  and r. dechter. approximation algorithms for probabilistic decoding. in uncertainty in artificial intelligence  l'al-1   1. 
santos  i1.l  e. santos. on the generation of alternative explanations with implications for belief revision. in uncertainty in artificial intelligence  uai1   pages 1. 1. 
 shimony and charniak  1l  s.p. shimony and p. charniak. a new algorithm for finding map assignments to belief networks. in p. bonissone  m. henrion  l. kamal  and ./. lemmer uds. uncertainty in artificial intelligence  volume'1  pages 1  1. 

