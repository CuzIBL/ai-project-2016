 
we present a coherent framework for data clustering. starting with a hopfield network  we show the solutions for several well-motivated clustering objective functions are principal components. for minmaxcut objectives motivated for ensuring cluster balance  the solutions are the nonlinearly scaled principal components. using scaled pc a  we generalize to multi-way clustering  constructing a self-aggregation network  where connection weights between different clusters are automatically suppressed while connection weights within same clusters are automatically enhanced. 
1 	introduction 
principal component analysis  pca  is widely adopted as an effective unsupervised dimension reduction method. pca is extended in many different directions  hastie and stuetzle  1; kramer  1; lee and seung  1; scholkopf et al  1; collins et a/.  1 . 
모the main justification is that pca uses singular value decomposition  svd  which is the best low rank approximation in l1 norm to original data due to eckart-young theorem. however  this results alone is inadequate to explain the effectiveness of pca. here  we provide a new derivation of pca based on optimizing suitable clustering objective functions and show that principal components are actually cluster indicator vectors in clustering. 
모hopfield networklhopfield  1  provide a convenient framework of our study. in particular  the self-aggregation network proposed in this work uses hebb rule to encode pattern vectors. one feature of hopfield associative-memory networks is that it can be adopted to solve hard combinatorial problems haykin  1nd ed . 
모another thread of this work is the spectral graph partitioning  fiedler  1; pothen et al.  1; hagen and kahng  
1; shi and malik  1; ding et al  1a; 1b; ng et al.  1; meila and shi  1   which uses laplacian matrix of a graph. this arises naturally for balancing the clusters  see  our approach differs from others mainly in 
   * work supported by department of energy  office of science  through a lbnl ldrd  under contract de-ac1sf1. well-motivated clustering objective functions. a further point is the recognition that spectral graph clustering is embedded in the scaled pca thus leading to self-aggregation networks. 
모this paper combines above three threads and develop a coherent framework for clustering. we begin with two-way clustering in and generalize to multi-way clustering in 뫫1. 
1 two-way clustering 
we start by formulate two-way clustering as a hopfield network and derive pca as cluster indicator vectors. 
1 hopfield network and pca 
given n data points and properly defined similarity or association  1 between points i  j  we form a network  a weighted graph  g with  as the connection between nodes  we wish to partition it into two clusters cj c1. the result of clustering can be represented by an indicator vector q  where 
	 1  consider the clustering objective  

ity of c1  and analogously for s c1  c1 . 
모now we propose a min-max clustering principle: data points are grouped into clusters such that the overlap s c1  c1  between different clusters are minimized while within-cluster similarities 
 1  
these conditions can be simultaneously satisfied by maximizing the energy  objective function  j1. using hopfield model  hopfield  1   the solution is obtained by the update rule 
or in vector form if one relaxes q i  from discrete indicators to continuous values in  -1   the solution q is given by 
		 1  

learning 	1 

since the matrix entries in w are non-negative  the first principal eigenvector qi has all positive  or all negative  entries. thus the desired solution is q1. 
1 	hopfield network for bipartite graph 
here we extend the hopfield networks for clustering bipartite graph. an example of bipartite graph is a m x n termdocument association matrix   where each row represents a word  each column represents a document  and btj the counts of co-occurrence of row rt and column  we show that the solution for clustering indicators is precisely the latent semantic indexing deerwester el al  1 
모we wish to partition the r-type nodes of r into two parts r1  r1 and simultaneously partition the otype nodes of c into two parts  based on the clustering principle of minimizing between-cluster association and maximizing withincluster association. we use indicator vector f to determine how to split r into = 1  or -1 depending on 
we use g to determine how to split c into 
-1 depending on 
모 for presentation purpose  we index the nodes such that nodes within same cluster are indexed contiguously. the clustering algorithms presented are independent to this assumption. bold face lower case letters are vectors. matrices are denoted by upper case letters.  thus we may write 

it is convenient to convert the bipartite graph into an undirected graph. we follow standard procedure and combine the two types nodes to one by setting 
		 1  
this induces an undirected graph g  whose adjacency matrix is the symmetric weight matrix w. 
consider the following objective function  
 1  
between r1 and c1  and s r1 c1  is the sum of association between r1 and c1. s ri c1  and s r1 ci  should be minimized. s r1   c   is the sum of association within cluster 1  see fig.l   and s r1 c1  is the sum of association within cluster 1. s r1   c1  and s r1  c1  should be maximized. these conditions are simultaneously satisfied by maximizing j1. 
모we can write down the hopfield network update rule for q. if one relaxes q{i  from discrete indicators to continuous values  the solution q satisfies eq. 1 . now utilizing the explicit structures of w and q  we have 
		 1  
the solutions to this equation are the singular value decomposition  svd  of b: {f;} are left singular vectors and {g } are right singular vectors of the svd of 뫩   
		 1  
again  since the matrix entries in w are non-negative  the first principal components  have entries of same sign; thus the desired solutions are 1  v1. note that this is also the svd employed in lsi. we summarize these results in 
theorem 1. principal components are solutions for clustering indicators for clustering undirect graphs and bipartite graphs under appropriate hopfield network models. 
1 	principal component clustering 
in the objective j1  we may explicitly enforce a balance of 
clusters 	from eq.l  we need to minimize 
thus we consider the clustering objective  
 1  
where 	we adjust u 
the principal 
vector. 
모what is the reasonable choice of  a natural choice is to set the average of wij. with this choice  the modified weight matrix satisfies the sum-to-zero condition: 
		 1  
모this condition can be further refined by centering each column and each rows  such that 
		 1  
where 

and w are standard notations in statistics.  now the desired cluster indicator vector is the principal eigenvector of 

모the fully centered w has a useful property that all eigenvectors of w with nonzero eigenvalues have the sum-to-zero property: = 1. this is because  1  q1 = e is an eigenvector of w with  1  all other eigenvectors are orthogonal to qo  i.e  
모the sum-to-zero condition = 1 does not necessarily imply that the sizes of the two cluster  should be equal. in fact  the cluster indicator vector should be refined to 
		 1  

 in this paper  all vectors are implicitly normalized to 1 using 	to overcome this problem  we seek to prevent either 
l1 norm . correspondingly  the clustering objective becomes 	s c1 c1  or s c1 c1  become very small. we optimizelding 
which can be written as 
ple  j1 becomes 
		 1  
 1  
again  the desired solutions is the eigenvector associated with the second largest eigenvalue. note that by comparing eq. 1  with eq. 1   the net effect for cluster balance is diagonal scaling. this diagonal scaling  however  leads to an 
	are sizes of column clusters 	+ 	important feature of self-aggregation  see 뫫1 . 
column size . again  the first two terms represent 
   1 cluster balance for bipartite graphs the average within-cluster similarities which are maximized  and the last two terms represent average between-cluster sim- for balanced clustering of bipartite graphs  object function of ilarities which are minimized. eq. 1  becomes 
we note that 
using the representation w and q of eq.1  and similar derivation  ding  1; zha et al  1 j  the solution for optimization of j1 is also given by eq.1. let dr = diag be  and 
is minexp imately eq.1. 

1 	a-way clustering 
the above mainly focus on 1-way clustering. below we generalize to a'-way clustering  k   1. the generalization is based on the key observation that the solution for cluster indicator vectors  see 뫫1 and 뫫1  are scaled principal components  as we discuss next. 
1 	scaled principal components 
associations among data objects are mostly quantified by a similarity metric. the scaled principal component approach starts with a nonlinear  non-uniform  scaling of w. noting that w - d1 d'x'1wd'l'1 dl'1t we apply spectral 
decomposition on the scaled matrix 
are the k eigenvectors with eigenvalue ear combination of k step functions  i.e.  piece-wise constant function. clearly  all data objects within the same cluster have identical elements in q. the coordinate of object i in the kdim spca space is  thus objects within a cluster are located at  self-aggregate into  the same point in spac space. 
모scaled principal components are not unique  since c could be any orthogonal matrix. however  

note eq. 1  is identical to eq. 1   or eq. 1   thus scaled principal components are cluster indicator vectors in minmaxcut  see 뫫1 . 
1 	self-aggregation network 
in hopfield networks  a pattern q1 is encoded into the network as   the hebb rule ; multiple patterns are encoded additively. in our problem  a pattern is a cluster partitioning indicator vector. we define a self-aggregation network with the connection weights 
. here we highlights several important 
properties of wsa and provides several example applications.  self-aggregation is first studied in  ding et al  1 .  has an interesting self-aggregation property enforced 
by within-cluster association  connectivity . to prove  we apply perturbation analysis by writing   
where is the similarity matrix when clusters are wellseparated  zero-overlap  and accounts for the overlap among clusters and is treated as a perturbationlding et al  1a . this perturbation approach is standard in quantum physics mathews and walker  1 . 
clusters overlap 
모consider the case when overlaps among different clusters exist. the overlaps are treated as a perturbation. theorem 1. at the first order  the k scaled principal components and their eigenvalues have the form 
several features of spca can be obtained from theorem 1: corollary 1. sa network has the same block diagonal form of eq. 1   within the accuracy of theorem 1. this assures that objects within the same cluster will selfaggregate as in theorem 1. 
corollary 1. the first scaled principal component is qi = and qi 
 1  
 1  
note that this is precisely j1 of eq.1  the clustering objective we started with: our clustering framework is consistent. example 1. a dataset of 1 clusters with substantial random overlap between the clusters. all edge weights are 1. the similarity matrix and results are shown in fig.l  where nonzero matrix elements are shown as dots. the exact  and approximate  from theorem 1 are close: 

wsa is much sharper than the original weight matrix w clearly due to self-aggregation: connections between different clusters are substantially suppressed while connections within same clusters are substantially enhanced. 

figure 1: left: similarity matrix w. diagonal blocks represent weights inside clusters and off-diagonal blocks represent overlaps between clusters. right: computed wsa-
application 1. in dna micro-array gene expression profiling  responses of thousands of genes from tumor tissues are simultaneously measured. we apply spca framework to gene expression profiles of lymphoma cancer sampleslalizadeh ex al  1 . three cancer and three normal subtypes are shown in fig.1. this is a difficult case due to the large variations of cluster sizes  the number of samples in each subtype are shown in parentheses in fig.1b . self-aggregation is evident in figure 1b and 1c. the computed clusters correspond quite well to the normal and cancer subtypes identified by human experts. 
1 	dynamic aggregation 
the self-aggregation process can be repeated to obtain sharper clusters. w& is the low-dimensional projection that contains the essential cluster structure. combining this structure with the original similarity matrix  we obtain a new similarity matrix containing sharpened cluster information: 
 1  
w
iteration  setting structure. 
모applying sa net on w  leads to further aggregation  see figure 1c . the eigenvalues of the 1st and 1nd sa net are shown in the insert in figure 1c. as iteration proceeds  a clear gap is developed  indicating that clusters becoming more separated. 

figure 1: gene expression profiles of cancerous and normal lymphoma tissues samples from alizadeh et al. in original euclidean space  a   in scaled fca space  b   and in scaled pca space after one iteration of eq.1  c . in all 1 panels  objects in original space are shown in 1d-view spanned by the first two pca components. cluster structures become clearer due to self-aggregation. the insert in  c  shows the eigenvalues of the 1st and 1nd sa network. 
noise reduction 
모sa net has noises. for example  wsfii has sometimes negative weights  wsa ij whereas we expect them to be nonnegative. however  by corollaries 1 and 1  wsa has a diagonal block structure and every elements in the block are identical  eq.1  even when overlaps exit. this property allows us to interpret  as the probability that two objects i  j belong to the same cluster: 

to reduce noise in the dynamic aggregation  we set 
 1  
where and we c h o s e = 1. noise reduction is an integral part of sa net. in our experiments  final results are insensitive to  the above dynamic aggregation repeats self-aggregation process and forces data objects move towards the attractors  which are the desired clusters and their principal eigenvalues approach 1  see insert in fig.1c . usually  after one or two iterations the cluster structure becomes evident. 

learning 	1 

1 bipartite graphs 
spca applies to bipartite graph problems as well. the nonlinear scaling factors are  cf. eq.1 . let b = where is given in eq. 1 . applying svd on b  we obtain 
 1  
for row 
they 
have the same self-aggregation and related properties. we note that spca on bipartite graph leads to correspondence analysislgreenacre  1  from a new perspective. 
모the structure of self-aggregation network  can be meaningfully further decomposed: where provides the cluster structure for row objects  while provides the cluster structure for column objects. the offdiagonal block matrix provides the sharpened association between row and column objects ding  1 . 
1 	discussions 
in this paper  we present a data clustering framework based on properly motivated hopfield network  and show the clustering indicators are principal components. further motivated by cluster balance  we extend the framework to minmaxcut that utilized the laplacian matrix of a graph. the framework is generalized to multi-way clustering  by using scaled principal components  and self-aggregation networks are constructed. we prove the cluster member self-aggregation property of the network. this framework extends naturally to bipartite graphs which leads to row-row  column-column and row-column sa nets that simultaneously cluster the row and column objects. 
   in self-aggregation  data objects move towards each other guided by connectivity. this is similar to the self-organizing map  kohonen  1   where feature vectors self-organize into a 1d feature map while data objects remain fixed. all these have a connection to recurrent networks  hopfield  1; haykin  1nd ed . in hopfield network  features are stored as associative memories. in more complicated networks  connection weights are dynamically adjusted to learn or discover the patterns. the self-aggregation network provides a new mechanism to realize this unsupervised learning. 
