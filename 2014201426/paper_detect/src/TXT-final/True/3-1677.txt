
in this paper  we provide an algebraic approach to markov decision processes  mdps   which allows a unified treatment of mdps and includes many existing models  quantitative or qualitative  as particular cases. in algebraic mdps  rewards are expressed in a semiring structure  uncertainty is represented by a decomposable plausibility measure valued on a second semiring structure  and preferences over policies are represented by generalized expected utility. we recast the problem of finding an optimal policy at a finite horizon as an algebraic path problem in a decision rule graph where arcs are valued by functions  which justifies the use of the jacobi algorithm to solve algebraic bellman equations. in order to show the potential of this general approach  we exhibit new variations of mdps  admitting complete or partial preference structures  as well as probabilistic or possibilistic representation of uncertainty.
1	introduction
in the field of planning under uncertainty  the theory of markov decision processes  mdps  has received much attention as a natural framework both for modeling and solving complex structured decision problems  see e.g.  dean et al.  1; kaebling et al.  1 . in the standard mdp approach  the utilities of actions are given by scalar rewards supposed to be additive  uncertainty in the states of the world and in the consequences of the actions are represented with probabilities  and policies are evaluated using the expected utility  eu  model. although these choices are natural in various practical situations  many other options are worth investigating for several reasons :
  rewards of actions are not necessarily scalar nor additive. in multi-agent planning or in multicriteria mdps  the utility of any action is given by a vector of rewards  one per agent or criterion  and actions are compared according to pareto dominance  wakuta  1 . in qualitative frameworks  rewards are valued on an ordinal scale and therefore are not additive. the sum is then replaced by the min  max or any refinement of them  depending on the context.
  non-probabilistic representation of uncertainty might be of interest. in practice  it is sometimes difficult to quantify precisely the plausibility of states and consequences of actions. assessing probabilities in such situations seems difficult. for this reason  alternative approaches based on qualitative representations of uncertainty have been proposed  see e.g.  darwiche and ginsberg  1; dubois and prade  1; wilson  1   and might be used in the context of mdps.
  non-eu theories offer also interesting descriptive possibilities. despite the appeal of the expected utility model and its theoretical foundations  von neumann and morgenstern  1; savage  1   recent developments of decision theory have shown the descriptive potential of alternative representations of preferences under uncertainty. for example  the rank dependent expected utility theory  rdeu  is a sophistication of eu theory using probability transforms to better account for actual decision making behaviors under risk  quiggin  1 . besides this extension  various alternatives to eu have been proposed for decision making in a non-probabilistic setting. among them  let us mention the qualitative expected utility  qeu  theories proposed in  dubois and prade  1  and  giang and shenoy  1   and very recently  the generalized expected utility theory  geu  proposed in  chu and halpern  1a; 1b  which generalizes the notion of expectation for general plausibility measures.
¡¡despite the diversity of models proposed for decision making under uncertainty  very few of them are used in the context of dynamic decision making. this gap can be explained by the inconsistencies entailed by the use of non-eu criteria  typically rdeu  in the dynamic context  machina  1; sarin and wakker  1 . indeed  when using a given nonlinear utility criterion at each decision stage  the bellman principle is generally violated  so that backward induction is likely to generate a dominated policy; further  there is in general no operational way to determine an optimal policy. this simple statement largely explains the predominance of the eu model in dynamic decision making under risk.
¡¡in the last decade however  some alternative models to eu have been proved to be dynamically consistent  thus providing new possibilities for sequential decision making. this is the case of qualitative expected utility theory  dubois and prade  1  that has led to a possibilistic counterpart of mdps  sabbadin  1  with efficient algorithms adapted from backward induction and value iteration  substituting operations  +  ¡Á  by  max min  in computations. in the same vein   littman and szepesv¨¢ri  1  propose a generalized
version of mdps where max and + are substituted by abstract operators in bellman equations  and  bonet and pearl  1  propose a qualitative version of mdps. besides these positive results  very few alternatives to standard mdps have been investigated.
¡¡among the diversity of possible choices for defining a reward system  a plausibility measure over events and a preference over lotteries on rewards  we need to know which combination of them can soundly be used in the context of mdps and which algorithm should be implemented to determine an optimal policy. for this reason  we propose in this paper an algebraic generalization of the standard setting  relying on the definition of a semiring structure on rewards  a semiring structure on plausibilities of events  and a generalized expectation model as decision criterion  chu and halpern  1a . the generalization power of semirings have been already demonstrated in ai by  bistarelli et al.  1  in the context of constraint satisfaction problems. within this general setting  our aim is to present a unified treatment of mdps and to provide algorithmic solutions based on the general jacobi algorithm  initially introduced to solve systems of linear equations .
¡¡the paper is organized as follows: in section 1  we show by example how to recast an mdp as an optimal path problem in a decision graph. then we introduce an algebraic framework for mdps  relying on the definition of algebraic structures on rewards  plausibilities and expectations  section 1 . in section 1 we justify the use of the jacobi algorithm as a general procedure to determine an optimal policy in an algebraic mdp  amdp . finally  we consider in section 1 some particular instances of amdps  including new probabilistic and possibilistic mdps using partial preferences over rewards.
1	decision rule graph in mdps
we briefly recall the main characteristics of a markov decision process  mdp   puterman  1 . it can be described as a tuple  s a t r  where:
  s = {s1 ... sn} is a finite set of states 
  a = {a1 ... am} is a finite set of actions 
  t : s¡Áa ¡ú pr s  is a transition function  giving for each state and action  a probability distribution over states  in the sequel  we write t s a s¡ä  for t s a  s¡ä   
  r: s ¡Á a ¡ú r is a reward function giving the immediate reward for taking a given action in a given state.
a decision rule is a function from the set of states s to the set of actions a. there are n = mn available decision rules at each step. we write   = as = {¦Ä1 ... ¦Än} the set of decision rules. a policy at step t  i.e.  the tth-to-last step  is a sequence of t decision rules. for a policy ¦Ð and a decision rule ¦Ä  we note  ¦Ä ¦Ð  the policy which consists in applying first decision rule ¦Ä and then policy ¦Ð.
¡¡a history is a realizable sequence of successive states and actions. the accumulated reward corresponding to a history ¦Ãt =  st at st 1 ... a1 s1   with initial state st  is


table 1: decision rules.
. we denote ¦£t s  the set of t-step
histories starting from s. for an initial state s  a t-step policy ¦Ð =  ¦Ät ... ¦Ä1  induces a probability distribution pr¦Ðt  s ¡¤  over histories. the t-step value of being in state s and executing policy ¦Ð is given by  expected accumulated reward :
vt¦Ð s  = p¦Ã¡Ê¦£t s  pr¦Ðt  s ¦Ã r ¦Ã 
denoting vt¦Ð the vector whose ith component is vt¦Ð si   any two t-step policies ¦Ð  ¦Ð¡ä can be compared using the componentwise dominance relation ¡Ýrn defined by:

the t-step value of a policy of the form  is defined recursively by and  where fi : rn ¡ú rn is the update function which associates
to any vector x =  x1 ... xn  the vector where.
example 1 consider an mdp with s = {s1 s1}  a = {a1 a1}  t si aj sk  = 1 if i = j = k and 1 otherwise  r s1 a1  = 1  r s1 a1  = 1  r s1 a1  = 1 and r s1 a1  = 1. thus  there are n = 1 available decision rules at each step  see table 1 . for instance  decision rule ¦Ä1 consists in applying action a1 in states s1 and action a1 in state s1. in this example  the functions fi are given by: f1 x1 x1  =  1 + x1 + 1x1 + 1x1  f1 x1 x1  =  1 + x1 + x1  f1 x1 x1  =  1 + 1x1 + 1x1 + 1x1 + 1x1  f1 x1 x1  =  1 + 1x1 + 1x1 + x1  for all  x1 x1  ¡Ê r1.
¡¡given a finite horizon h  the optimal policy ¦Ð  can be found thanks to the following bellman equations:
		 1 
¡¡the solution of these equations can be reduced to a vectorweighted optimal path problem in a particular graph  with update functions  the fi's  on the arcs allowing the propagation of policy values over nodes. indeed  consider a graph where each node ¦Äti corresponds to decision rule ¦Äi at step t  and each arc of the form  ¦Äti ¦Ätj 1  corresponds to a transition between decision rules. moreover  nodes ¦Ä1j  j = 1...n are connected to a sink denoted 1  and a source denoted h is connected to nodes ¦Ähj   j = 1...n. hence  any path from node ¦Äti to node 1 corresponds to a t-step policy where decision ¦Äi is applied first. we name that graph the decision rule graph. note that the bellman update via fi's is nicely separable  fji's can be computed independently  and therefore the vector value of a path can be obtained componentwise  state by state  as usual in classic mdp algorithms. this property will be exploited later on for algebraic mdps. coming back to example 1 and assuming that h = 1  there are 1 available

figure 1: a decision rule graph.
policies  all possible combinations of two successive decision rules . the corresponding graph is pictured on figure 1.
¡¡for all i = 1...n  function fi is associated to every arc issued from ¦Äti  t = 1...h. moreover  the identity function is assigned to every arc issued from h. thus  the optimal policy can be computed thanks to backward induction on the decision rule graph  by propagating the value  1 ... 1  ¡Ê rn from the sink 1 corresponding to the empty policy. in example 1  backward induction leads to the labels indicated in table 1. the optimal policy value can be recovered on node h. the optimal vector value is  1  and the optimal policy  recovered from bolded values in table 1  is  ¦Ä1 ¦Ä1 .

table 1: labels obtained during backward induction.
¡¡in the next section  we show how to generalize this approach to a wide range of mdps. in this concern  we introduce the notion of algebraic markov decision process.
1	algebraic markov decision process
we now define a more general setting to model rewards and uncertainty in mdps. our approach relies on previous works aiming at generalizing uncertainty measurement and expectation calculus. our rewards take values in a set v and we use plausibility measures1  friedman and halpern  1  to model uncertainty. a plausibility measure pl is here a function from 1w  the set of events  to p  where w is the set of worlds  p is a set endowed with two internal operators ¨p and  p  the analogs of + and ¡Á in probability theory   a  possibly partial  order relation p  and two special elements 1p and 1p such that  for all p ¡Ê p. furthermore  pl verifies pl    = 1p  pl w  = 1p and pl  pl y   for all x y such that y   x   w. we assume here that pl is decomposable  i.e. pl x ¡Èy   = pl x ¨p pl y   for any pair of disjoint events x and y . to combine plausibilities and rewards  we use the generalized expectation proposed by  chu and halpern  1a . this generalized expectation is defined on an expectation domain  v p      1where   : v ¡Á v ¡ú v and   : p ¡Á v ¡ú v are the counterparts of + and ¡Á in probabilistic expectation  and the three following requirements are satisfied:  x y  z = x  y z   x   y = y   x  1p   x = x. for any plausibility distribution pl on v having its support in x  the generalized expectation writes: px ¡Êx pl x    x.
¡¡an algebraic mdp  amdp  is described as a tuple  s a t r   where t and r are redefined as follows:
  t : s ¡Á a ¡ú pl s  is a transition function  where pl s  is the set of plausibility measures over s valued in p    r: s ¡Á a ¡ú v is a reward function giving the immediate reward in v .
consistently with the standard markov hypothesis  the next state and the expected reward depend only on the current state and the action taken. in particular  plausibility distributions of type t s a  are  plausibilistically  independent of the past states and actions. this plausibilistic independence refers to the notion introduced by  friedman and halpern  1  and leads to the following algebraic counterpart of the probabilistic independence property: pl x ¡É y   = pl x   p pl y   for any pair x  y of independent events.
¡¡in this setting  rewards and plausibilities take values in two semirings. roughly speaking  a semiring is a set endowed with two operators allowing the combination of elements  rewards or plausibilities  together. we now recall some definitions about semirings.
definition 1 a semiring  x ¨   1  is a set x with two binary operations ¨ and    such that:
 a1   x ¨ 1  is a commutative semigroup with 1 as neutral element  i.e. a¨b = b¨a  a¨b ¨c = a¨ b¨c  a¨1 = a .
 a1   x   1  is a semigroup with 1 as neutral element  and for which 1 is an absorbing element  i.e.  a   b    c = a  
 b   c  a   1 = 1   a = a a   1 = 1   a = 1 .
 a1    is distributive with respect to ¨  i.e.  a ¨ b    c =  a   c  ¨  b   c  a    b ¨ c  =  a   b  ¨  a   c  .
¡¡a semiring is said to be idempotent when  x ¨  is an idempotent commutative semigroup  i.e.  a commutative semigroup such that a ¨ a = a . the idempotence of ¨ enables to define the following canonical order relation x:

¡¡from now on  we will assume that the rewards are elements of an idempotent semiring  v ¨v   v  1v  1v  . operator ¨v is used to select the optimal values in v   whereas operator  v is used to combine rewards. in classic mdps with the total reward criterion  ¨v = max and  v = +.
¡¡moreover the structure  p ¨p   p  1p 1p  is also supposed to be a semiring. operator ¨p allows to combine the plausibilities of disjoint events and operator  p allows to combine the plausibilities of independent events. note that the assumption that  p ¨p   p  1p 1p  is a semiring is not very restrictive since  darwiche and ginsberg  1   who
use similar properties to define symbolic probability  have shown that it subsumes many representations of uncertainty  such as probability theory  possibility theory and other important calculi used in ai.
¡¡now that the general framework has been defined  we can follow the usual approach in mdps and define a value function for policies. the accumulated reward for a history . for
an initial state s  a t-step policy ¦Ð =  ¦Ät ... ¦Ä1  induces a plausibility measure pl¦Ðt  s ¡¤  over histories. such a policy will be evaluated with respect to the generalized expectation of accumulated reward  which writes:

the policies can be compared with respect to the componentwise dominance relation between vectors in v n:
for all
¡¡most of the mdps introduced previously in the literature are instances of our algebraic mdp:
- in standard mdps  the underlying algebraic structure on p is  p ¨p   p  1p 1p  =   1  + ¡Á 1   and operators   = + and   = ¡Á are used to define the classic expectation operation. when rewards are de-

fined on  v ¨v   v  1v  1v   =  r max +  ¡Þ 1  where

r = r ¡È { ¡Þ}  we recognize the total reward crite-

rion. with  r max +¦Ã  ¡Þ 1   where x +¦Ã y = x + ¦Ãy   we recognize the weighted total reward criterion. with

 r max +h  ¡Þ 1  where a+h b = h1 a+b  we recognize the average reward criterion assuming that there is an initial dummy step with zero reward.
- qualitative mdps  introduced in  bonet and pearl  1   are amdps where the rewards and the plausibility measures are defined on the semiring of two-sided infinite formal series  which is a subset of the extended reals  wilson  1 .
¡¡in order to assign functions to the arcs in the decision rule graph  we first define fji : v n ¡ú v  the update function after applying decision rule ¦Äi x ¡Ê v n
 .
then  for any decision rule ¦Äi  we define the update function fi : v n ¡ú v n which associates to any vector x ¡Ê v n the vector  f1i x  ... fni x  .
¡¡dynamic consistency in amdps is guaranteed by specific properties on functions fi. the fulfillment of these properties strongly relies on the following conditions:
 c1  p    x ¨v y  =  p   x  ¨v  p   y   c1  x    y ¨v z  =  x   y  ¨v  x   z 
 c1  p    q   x  =  p  p q    x
 c1  pi  pi    x  v yi  = x  v  p i pi   yi 
 c1  p    x   y  =  p   x     p   y  for all p q pi ¡Ê p x y z yi ¡Ê v .
¡¡conditions  c1  and  c1  are two distributivity properties entailing a kind of additivity of  and    i.e.   for   ¡Ê {   } . condition  c1  enables the reduction of lotteries. condition  c1  enables to isolate a sure reward in a lottery and is similar to the distributivity axiom used in  luce  1 . condition  c1  is a distributivity condition as in classic expectation. we now establish a monotonocity result that will be used later  prop. 1  to justify a dynamic programming approach.
proposition 1 if  c1  and  c1  hold  then fi is nondecreasing for all ¦Äi ¡Ê    i.e

proof.	. for sj ¡Ê s  we have

by  c1  and  c1 . thanks to distributivity of  v over ¨v   we have. therefore . 
¡¡moreover  the value of a policy ¦Ð can be computed recursively thanks to the following result:
proposition 1 let ¦Ð =  ¦Äi ¦Ð¡ä  a  t + 1 -step policy  and assume that v1¦Ð =  1v  ... 1v  . if  c1    c1  and  c1  hold  then for all t ¡Ý 1.
proof. let s be a state and a denote ¦Äi s . we note ¦Ãt =  st at ... a1 s1  and ¦Ãt+1 =  s a st at ... a1 s1 . we have:
 
vt¦Ð  s  =	pl¦Ðt	 s ¦Ãt+1    r ¦Ãt+1 
by  c1 
	 	 
= r s a   v	t s a s¡ä   	pl¦Ðt ¡ä s¡ä ¦Ãt    r ¦Ãt  s¡ä¡Ês	¦Ãt¡Ê¦£t s¡ä 
by  c1  and  c1 
 
= r s a   v	t s a s¡ä    vt¦Ð¡ä s¡ä 
s¡ä¡Ês
therefore  for all.	
¡¡in order to establish the algebraic version of bellman equations  we define the subset of maximal elements of a set with respect to an order relation  as:

furthermore  we denote the set {y   x : y =
. when there is no ambiguity  these sets will be denoted respectively m y   and p  x . besides  for any
  for all x ¡Ê p  v n   f x  denote
we define the semiring  p  v n  ¨   1  where 1 =
{ 1v  ... 1v  }  1 = { 1v  ... 1v  }  and for all x y ¡Ê
p	v n	x	y	m x	y
with x  v y =  x1  v y1 ... xn  v yn . hence  the algebraic generalization of bellman equations  1  writes:

where fmi : p  v n  ¡ú p  v n  is defined by fmi  x  = m fi x   for all x ¡Ê p  v n .
1	generalized path algebra for amdps
following the construction proposed in section 1  a decision rule graph can be associated to an amdp. clearly  solving algebraic bellman equations amounts to searching for optimal paths with respect to the canonical order associated to ¨. we now show how to solve this problem.
¡¡consider the set f of functions from p  v n  to p  v n  satisfying for all f ¡Ê f  x ¡Ê p  v n   y ¡Ê p  v n : f x ¨ y   = f x  ¨ f y   f 1  = 1
the ¨ operation on p  v n  induces a ¨ operation on f defined  for all h g ¡Ê f and x ¡Ê p  v n   by:  h ¨ g  x  = h x  ¨ g x 
let   denote the usual composition operation between functions  id the identity function  and  for simplicity  1 the constant function everywhere 1. it has been shown by  minoux  1  that the algebraic structure  f ¨   1 id  is a semiring. we now prove that update functions's belong to f.
proposition 1  c1  and  c1  imply .
proof.  for any ¦Äi  we have since pi  pi 1v = i pi    1v  v 1v   = 1v  v  pi  pi   1v   = 1v by  c1  and absorption. now  we show that fmi  x ¨ y   = fmi  x  ¨ fmi  y    x y ¡Ê p  v n . consider x y in p  v n . first  we have fi m x ¡È y      fi x ¡È y      . second  we prove that m fi x ¡È y      fi m x ¡È y        . let z ¡Ê m fi x ¡È y   . then it exists w ¡Ê x ¡È y such that fi w  = z. if w ¡Ê m x ¡È y   then z ¡Ê fi m x ¡È y   . if w 1¡Ê m x ¡È y   then it exists w  ¡Ê m x ¡È y   such that w   v w. as fi is nondecreasing  we have . by assumption  fi w  ¡Ê m fi x¡Èy     therefore fi w   = fi w . finally  z ¡Ê fi m x ¡È y    since z = fi w  . by     and       we have m fi x ¡È y      fi m x ¡È y      fi x ¡È y  . therefore m fi x ¡È y    = m fi m x ¡È y      which means  by definition  that fmi  x¨y   = fmi  x ¨fmi  y  . 
¡¡propositions 1 and 1 prove that the algebraic generalization of jacobi algorithm solves algebraic bellman equations  1  and  1   minoux  1 . thanks to the particular structure1 of the decision rule graph  the jacobi algorithm takes the following simple form:
algebraic jacobi algorithm
1.
1.
	1.	repeat
1. t ¡û t + 1
1. for i = 1 to n do qit ¡û fmi  vt 1 
1. vt ¡û lni=1 qit
	1.	until t = h
we recognize a standard optimal path algorithm on the decision rule graph valued with functions. when the iteration has finished  vt is the set of generalized expected accumulated rewards at step t associated to optimal paths. the above algorithm is not efficient since lines 1 and 1
require to consider n = mn decision rules in the computation of the bellman update. actually  the complexity of the algorithm can be significantly improved. indeed  remark that  where gjk x  =
  and that  v ¡Ê v n 
. since the
maxima over a cartesian product equals the cartesian product of maxima over components  we have
. hence  lines 1
and 1 can be replaced by:
	1.	vt ¡û  
	1.	for v ¡Ê vt 1 do
	1.	for j = 1 to n do
	1.	for k = 1 to m do qtjk ¡û gjk v 
	1.	vtj ¡û lmk=1 qtjk
	1.	endfor
	1.	vt ¡û vt ¨  vt1 ¡Á .. ¡Á vtn 
	1.	endfor
this algebraic counterpart of backward induction runs in polynomial time when the reward scale is completely ordered and algebraic operators can be computed in o 1 .
1	examples
to illustrate the potential of the algebraic approach for mdps  we now consider decision models that have not yet been investigated in a dynamic setting.
  qualitative mdps. in decision under possibilistic uncertainty   giang and shenoy  1  have recently studied a new qualitative decision model  binary possibilistic utility   allowing to handle weak information about uncertainty while improving the discrimination power of qualitative utility models introduced by  dubois and prade  1 . the latter have been investigated in sequential decision problems by  sabbadin  1 . we show here that the former can be exploited in sequential decision problems as well.
¡¡possibilistic uncertainty is measured on a finite qualitative totally ordered set p  endowed with two operators ¡Å and ¡Ä  max and min respectively . we denote 1p  resp. 1p  the least  resp. greatest  element in p. the structure  p ¡Å ¡Ä 1p 1p  is a semiring.
¡¡in giang and shenoy's model  rewards are valued in an ordered scale 
¦Ë ¡Å ¦Ì = 1p} and
the relevant semiring is here  v ¨v   v  1v  1v   where:
v = {h¦Ë ¦Ìi : ¦Ë ¡Ê p ¦Ì ¡Ê p} h¦Á ¦Âi ¨v h¦Ë ¦Ìi = h¦Á ¡Å ¦Ë ¦Â ¡Ä ¦Ìi h¦Á ¦Âi  v h¦Ë ¦Ìi = h¦Á ¡Ä ¦Ë ¦Â ¡Å ¦Ìi
1v = h1p 1pi  1v = h1p 1pi
note that ¨v and  v are operators max and min on up.
¡¡the binary possibilistic utility model is a generalized expectation with operators   and   taken as componentwise ¡Å and ¡Ä respectively. note that this criterion takes values in up. thanks to distributivity of ¡Å  resp. ¡Ä  over ¡Ä  resp. ¡Å   conditions  c1  to  c1  hold  which proves that algebraic backward induction yields the value of optimal policies.
  multicriteria mdps. in planning problems  different aspects  time  energy  distance...  enter into the assessment of the utility of an action  and often these aspects cannot be reduced to a single scalar reward. this shows the practical interest of investigating mdps with multicriteria additive rewards and multicriteria comparison models. as an example  consider the following multicriteria decision model. let q denote the set of criteria and q a  possibly partial  order relation over q  reflecting the importance of criteria . following  grosof  1  and  junker  1   we denote  g the following strict order relation between vectors:

the interest of such a definition is to unify in a single model both the lexicographic order  when q is a linear order  and the pareto dominance order  when
¡¡in this case  the relevant semiring structure on rewards is defined by  neutral elements are omitted here :

hence  such a reward system can easily be inserted in the classical probabilistic setting. when the generalized expectation is chosen as the componentwise  usual  expectation  conditions  c1  to  c1  hold  which proves that algebraic backward induction yields the value of optimal policies.
1	conclusion
we have introduced a general approach for defining solvable mdps in various contexts. the interest of this approach is to factorize many different positive results concerning various rewards systems  uncertainty and decision models. once the structure on rewards  the representation of uncertainty and the decision criteria have been chosen  it is sufficient to check that we have two semirings on v and p and that conditions  c1  through  c1  are fulfilled to justify the use of an algorithm  ¨¤ la jacobi  to solve the problem. it is likely that this result generalizes to the infinite horizon case  provided a suitable topology is defined on the policy valuation space.
¡¡remark that  despite its generality  our framework does not include all interesting decision theories under uncertainty. for instance  in a probabilistic setting  the rdeu model  as well as other choquet integrals  cannot be expressed under the form of the generalized expectation used in the paper. actually  rdeu is known as a dynamically inconsistent model  machina  1; sarin and wakker  1  and it is unlikely that constructive algorithms like backward induction are appropriate. a further specific study for this class of decision models might be of interest.
