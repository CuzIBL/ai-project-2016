
in this article we introduce a new approach  and several implementations  to the task of question classification. the approach extracts structural information using machine learning techniques and the patterns foundare used to classify the questions. the approach fits in between the machine learning and handcrafting of regular expressions  as it was done in the past  and combines the best of both: classifiers can be generated automatically and the output can be investigated and manually optimised if needed.
1 introduction
before a question answering  qa  system can answer a question  it needs to have an idea what the question is about. one of the principal tasks of the question analysis stage of a qa system is the determination of the expected answer type  eat . finding the eat of a question is called question classification  or eat classification   hermjakob  1 .
　in this work we introduce a new approach to the problem of eat classification  based on structural information that can be extracted from the questions. re-occurring structures found in questions  such as  how far ...  may help finding the correct eat  in this case  distance   for a particular question. the approach described here automatically finds these structures during training and uses this information when classifying new questions.
　our approach combines the two main methods to question classification: machine learning and pattern matching. using machine learning  patterns are extracted from the training data. these patterns serve as regular expressions during the classification task.
　in the next two sections  we will describe two systems that fit into this approach. the first one uses a grammatical inference system and the second one is based on tries.
1 alignment-based learning classifier
the structure extraction phase of the first system is done by
alignment-based learning  abl   van zaanen  1 . abl

　　this work is supported by the australian research council  arc discovery grant no. dp1.	 desc 	 what  is  caffeine  
 desc   what  is  teflon    loc   where  is  milan 
	 loc 	what  are the twin cities 
table 1: example sentences with abl structure
is a generic unsupervised grammatical inference framework that learns structure from plain text sentences. the underlying idea of abl is that constituents can be interchanged. to give an example  if we exchange the noun phrase the man in the sentence he sees the man with another noun phrase a woman  we get another valid sentence: he sees a woman. this process can be reversed  by aligning sentences  and possible constituents  called hypotheses can be found. this is called the alignment learning phase  one of the three phases of abl  and the one that we use in this article. table 1 shows an example of the structure learned from a toy corpus of 1 sentences.
　in the training phase  regular expressions associated with the structures found are stored together with the eat of the corresponding questions. several eats with their frequencies may be stored if a regular expression matches several questions.
　we have experimentedwith two implementations. the first implementation  which we call hypo  uses the words in the hypotheses  i.e. the words between the brackets  to form regular expressions which are stored together with the eats of the associated questions. the second implementation  called unhypo  uses the words that are left after removing each hypothesis. for example  the hypo version uses the first question of table 1 to produce the patterns /what/  /is caffeine/  and /caffeine/  whereas unhypo produces the patterns /is caffeine/  /what/  and /what is/.
　during the classification phase we have experimented with two further approaches that differ on the use of the frequency counts of the matched regular expressions. the first method  called default  increments the counts of the eats of the question by the frequency that is stored with the eats of the regular expression. the second method  called prior  will simply increment the counts by 1. when all regular expres-
sions are tried  both methods select the eat with the highest count. if there several expressions with the same count  the default method makes a random choice  whereas the prior method selects the eat with the highest overall frequency.
　finally  we have experimented with the impact of the parts of speech  pos  as a simple approach to disambiguate words. we used brill's tagger  brill  1  and the resulting pos information is simply combined with the plain words. adjacent words that have the same pos are combined into one token. for example the question who is federico fellini  is  after pos tagging  divided into three tokens:  who  wp    is  vbz  and  federico fellini  nnp .
1 trie classifier
the second system uses a trie structure. a trie is a data structure defined by a recursive rule
	.	is a set of sequences
whose elements are taken from the alphabet . is the set of strings that contains all strings of that start with but stripped of that initial element  cle＞ment et al.  1 .
　during the learning phase  all questions are inserted into a trie structure that contains  in addition to the token  the eat and frequency information  the number of questions that use that path in the trie .
　during the classification phase  the trie is traversed in the usual way. if the new question is a prefix of a training question the traversal is trivial  and the node at the end of the traversal path indicates the eat of the question.
　to find a path for unseen questions  non-matching nodes are skipped in a methodical way in what we call the lookahead process. let us say that question tokens of question match up to :   and does not exist. the lookahead process then builds a set of sub-tries of the form
. the sub-trie with the
highest frequency associated is selected  and the process continues with until all tokens are consumed.
　there are two variations of the above process. in the strict method  and must have the same pos tag. if the resulting set of sub-tries is empty  the search process stops and the eat is retrieved from the node in the trie. in the flex method  if an empty set of sub-tries is retrieved  we consider as a sequence of question words until we find a question word equal to .
　we also experimented with a set of variations that have no pos information. in this case  and must be exactly the same token. again we allow for a strict and a flex version.
1 results
to compare our systems with current machine-learningmethods  we have used the same data as  zhang and sun lee  1 . this is a collection of 1 training questions and 1 test questions. the data have 1 coarse-grained classes and 1 fine-grained classes.
　table 1 indicates the precision of the questions classified with the coarse-grained classes for all our systems and for a baseline that always selects the most frequent class according to the training data. this baseline is  of course  the same for the plain words and pos tagged data. all our implementations performed well above the baseline.
wordsposbaseline11ablhypodefault11prior11unhypodefault11prior11triestrict11flex11table 1: results on the coarse-grained data
　the results show that the pos informationhelps the performance of the abl method but not of the trie-based method. overall  the trie-based approach outperforms the abl implementations. we obtained similar results with the fine-grained data  not shown for reasons of space .
　our best results fall close to the best-performing system in the bag-of-words and bag-of- -gramsversions of  zhang and sun lee  1 . their results ranged from 1%  nearest neighbours on words  to 1%  svm on -grams . given the simplicity of our methods and their potential for further improvement  this is encouraging.
1 conclusion
the results on the annotated questions of the trec1 data show that the approach is feasible and both systems generate acceptable results. we expect that future systems that fall in the structure induced question classification approach  for example  based on other grammatical inference systems  will result in even better performances.
　the automatically learned regular expressions can be inspected and extended by humans. the systems therefore combine the advantages of machine-learning and patternmatching methods.
　additionally  we think that the structure found by the systems can be used to find the focus of the question.
