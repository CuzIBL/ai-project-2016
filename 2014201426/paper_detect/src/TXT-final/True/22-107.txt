 
this paper describes dido  a system we have developed to carry out exploratory learning of unfamiliar domains without assistance from an external teacher. the program incorporates novel approaches to experience generation and representation generation. the experience generator uses a heuristic based on shannon's uncertainty function to find informative examples. the representation generator makes conjectures on the basis of small amounts of evidence and retracts them if they prove to be wrong or useless. a number of experiments arc described which demonstrate that the system can distribute its learning resources to steadily acquire a good representation of the whole of a domain  and that the system can readily acquire both disjunctive and conjunctive concepts even in the presence of noise. 
1. introduction 
this paper gives an account of dido  a learning system we have developed to carry out exploratory learning of unfamiliar domains*. we define the exploratory learning problem as follows: 
situation: an intelligent agent is placed in a novel domain comprised of a large number of entities. the agent has two types of interaction with the domain. it is equipped with a finite set of motor operations which can be applied to entities in the domain. it is also equipped with a finite set of perceptual operations which enable it to perceive the current state of any entities present in its locality. the agent has no prior knowledge of the effect of any of its motor operations on any entity. 
task: the agent knows that it may eventually have to solve problems in the domain but does not have any prior knowledge of what those problems might be. the agent must try to discover what effect its possible actions will have in any circumstances. more specifically  its task is to build a representation of the domain which will enable it to predict the outcomes of each of its motor operations in any 
     the work reported in this paper was supported by nsf grant# mcs-1. 
situation. this is the basic knowledge the agent would need to engage in problem solving. 
constraints: the domain is very large and the behavior of entities may be non-deterministic and exhibit change over time. no other agent is available to provide assistance in this knowledge acquisition task. the agent has only limited resources of time and memory available. since it is not known when problem solving will be necessary  it is desirable that the quality of the representation should rise steadily as learning proceeds. 
   this problem is significantly different from those addressed by other machine learning programs. the absence of prior knowledge means that deductive learning methods cannot be applied. any solution must be based on inductive inference. the most widely studied type of inductive inference problem is that of learning concepts from sets of classified examples  eg.winston  1; michalski & dietterich  1; quinlan  1  1 . the problem stated above differs from this type of problem in two respects: there is no external agent to supply classifications of examples  and there is no predefined classification scheme that the agent is trying to discover. a number of learning programs have been written which do not require such a predefined classification scheme  eg. michalski & stepp  1; fisher & langley  1 . the task performed by such systems  which is commonly termed 'conceptual clustering'  involves developing a classification scheme for a set of examples. such programs attempt to construct a parsimonious taxonomy to cover the example set on the basis of similarity between examples. the learning problem considered here resembles conceptual clustering in that the learning system must develop its own classification scheme. on the other hand it differs significantly in that the basis for forming classes is not similarity between examples but rather similarity of their behavior when subject to the agent's motor operations. thus the classification scheme developed is determined both by characteristics of the domain and by characteristics of the agent. in this sense the agent's task is to build an egocentric representation of its world. 
   learning involves two distinct but interrelated search processes  shalin et aly 1 : a search for a good representation in the space of possible representations  simon & lea  1; mitchell  1  1   and a search for informative examples in the space of possible 
	scott and markovitch 	1 

experiences  scott & vogt 1 . the information obtained from the experiences is used to guide the search for a better representation  while the current state of the representation may be used to guide the search for informative experiences. 
   the component of a learning system responsible for searching experience space is the experience generator. section 1 describes dido's experience generator which is based on a formalization of the concept of curiosity. the search of representation space is carried out by the representation generator. dido's representation generator is described in section 1. section 1 describes results obtained running an implementation of dido. finally  section 1 contains a discussion of the implications of this work. 
1. curiosity 	driven 	search 	of experience space 
the task of an inductive learning system's experience generator is to select informative examples from the experience space and supply them to the representation generator. an example is informative only if it leads to some change in the representation. the informativeness of an example is not an inherent property of the example since it depends on both the domain and the current representation. unfortunately examples are not equally informative and some may carry no information at all. 
   a large number of learning systems avoid the need for a built in experience generator by requiring an external agent 
 usually human  to serve this role  see michalski & dietterich  1 for review . winston  1  demonstrated that such an external teacher can greatly speed the learning process by selecting examples which are highly informative. 
dido has no such external assistance. furthermore 
dido has no preassigned class of problems to solve and so cannot make use of successes and failures at problem solving to guide the search for informative experiences  eg lex  mitchell et al  1 ; prodigy  carbonell & gill  
1  . what dido needs is a method of exploring domains which is analogous to human curiosity. one program which attempts to generate its own experiences under similar constraints is am  lenat 1 . lenat's formalization of curiosity was a set of 1 heuristic rules for assessing the 'interestingness' of objects. the objection to using such an elaborate implementation of curiosity is that it is not clear either how much each rule contributes  or how domain specific they were. we have provided dido with a simple domain independent method for selecting new experiences. 
   given the analogy with human curiosity it might seem reasonable to build an experience generator which favored highly novel objects. however  further consideration shows that such a system would not be very effective. it would provide the system with a large number of highly dissimilar experiences. these would form a poor foundation for a coherent and integrated representation of the domain. such a 
   system would learn very little about a lot. 
   the weakness of the novelty heuristic suggests that its opposite might be effective. that is the generator would 
1 	machine learning 
supply experiences which were as similar as possible to those already encountered. the argument in favor of this approach is that it would seem to have the potential to generate 'near misses'  winston 1  quite frequently. however further consideration shows that this familiarity heuristic is likely to be as ineffective as novelty. the experience generator would have no tendency to move outside one small region of the space of possible experiences. hence such a system would learn a lot about very little. 
   clearly what is needed is something that falls somewhere between novelty and familiarity. such a heuristic would supply experiences which could be related to previous experiences without becoming trapped in a small region of the experience space. this behavior is produced by using a heuristic based on uncertainty. the fundamental idea is that subsequent experiences should be generated in order to resolve uncertainties created through earlier experiences. 
   dido is attempting to build a representation which predicts the outcomes of applying operations to entities with a maximum likelihood of being correct. this representation takes the form of an inheritance network of classes. each class contains a number of practical conditionals. each practical conditional represents the knowledge that dido has acquired about the consequences of applying a particular operation to entities which are members of that class. the form of a practical conditional is as follows: 
¡¡ op   outcome1 p1  outcome1 p1 . outcomen pn    where each pj is an estimate  based on experiences  of the probability that the corresponding outcome will occur. probabilities are estimated using an exponential lag and outcomes with very small probability estimates arc discarded. if there is more than one outcome in the list dido is uncertain of the outcome. this uncertainty can be expressed precisely using the shannon uncertainty function  shannon & weaver  1  which is defined as: 

where the sum is taken over all alternative outcomes.  note that id1  quinlan  1  1  also uses this uncertainty function but for a very different purpose. id1 uses it as part of the representation generator to determine which subclasses to add to the current representation. dido uses it as part of the experience generator to determine what new experiences to select.  
   there is thus an uncertainty associated with every practical conditional in every class in dido's current representation. dido's experience generator finds the highest of these uncertainties  and selects a small number of entities that are members of the class concerned. the operation appearing in the most uncertain practical conditional is then applied to each of these entities and the outcomes noted. this procedure is termed a round of experiments. normally the outcomes will change the probabilities and hence lead to a change in uncertainty. dido attempts to build a class network in which the uncertainties are minimized. the way this happens is described in the next section. 
   the use of uncertainty to guide the search of experience space has a number of advantages. first it is simple and domain independent. it can be applied in any system in which it is possible to estimate probabilities of assertions being correct. second  it generates highly informative experiences. all the experiences produced are examples of classes of experience about which the learning system is doubtful. third  it makes efficient and effective use of resources. because the system is continually directing its attention to the area of highest uncertainty in the current representation  the level of uncertainty remains approximately uniform across the whole representation. this means that the system is steadily reducing its uncertainty about the whole of the experience space. 
   it is also worth noting that this was achieved because the representation included sets of contradictory assertions  alternative outcomes in this case  together with estimates of the likelihood that they were correct. this enabled dido to both identify and quantify the degree of doubt associated with what appears to be known. while the need to include measures of uncertainty in representations of uncertain domains has been recognized and explored for many years  the primary use of uncertainty in dido is to represent the system's uncertainty about the domain. such uncertainty can arise either because the domain is inherently uncertain or  as is more commonly the case  because the current representation is inadequate. 
1. conjecture 	based 	search 	of representation space 
dido builds an object based representation of a domain. 
the major feature of this is an inheritance network of classes. the members of each class are the entities which occur in the domain. every entity is a member of at least one class because dido begins with a representation comprising a single class   things  which includes all entities. 
   classes have two important components  the intension and the practical conditional set. the intension is a conjunctive predicate whose terms are built up from dido's set of sensory operations. any entity which satisfies the intension is a member of the class. practical conditionals were described in the preceding section. the practical conditional set represents dido's current knowledge of how members of the class behave when an operation is applied to them. it is unnecessary for there to be a practical conditional corresponding to every action in every class because practical conditionals can be inherited from superclasses. a practical conditional for a particular action only appears in a class when it is needed to shadow the practical conditional for the same action in a superclass. classes may have more than one immediate superclass. 
multiple inheritance conflicts do not occur because of the way in which the network is built. 
   two different approaches to representation generation are possible. the first is to be very cautious about making changes to the representation and only make changes when enough evidence has been accumulated to establish that they arc highly likely to be correct. the advantage of this approach is that it is unnecessary to retract any changes. the disadvantage is that the system is precluded from making and using conjectures for which there is only limited evidence. dido uses the opposite approach. changes are made tentatively on the basis of small amounts of evidence. if the conjectures prove to be wrong or of no value  they are rapidly discarded. we call such readily retracted changes conjectures.  see markovitch & scott  1  for discussion of knowledge which is not beneficial.  
   the representation generation process is basically one of learning by specialization  sometimes termed 'discrimination learning' . at the conclusion of each round of experiments  dido will have two samples of entities: successes  defined as those that gave the most probable outcome; and failures  that gave some other outcome. typically these samples are small. dido finds the sensory attribute value which correlates most highly with the most probable outcome and forms a new subclass having this as its intension. this subclass may already exist  since it might have been created previously as part of an attempt to reduce the uncertainty of a different practical conditional. if it does not exist dido creates it. dido then adds the practical conditional to this specialization class. if the specialization class has no other ancestors but the class in which the round of experiments was performed  this is straightforward. dido simply copies the practical conditional from the parent class. however  it may be that the specialization class has other ancestors. if there is more than one ancestor containing the practical conditional  the practical conditional with the highest probability for its expected outcome is copied into the specialization class. 
   this specialization process serves to extend the network to ever more specialized classes. it is balanced by two processes: retraction and generalization. retraction is the process by which incorrect or useless conjectures are discarded. every time the probabilities of a practical conditional are updated it is checked for usefulness. a practical conditional is deemed useless if removing it would not lead either to different predictions or to predictions with lower confidence. useless practical conditionals are removed. classes which have neither subclasses nor practical conditionals are also deemed useless and removed. 
practical conditionals containing only one outcome 
 which must have a probability of 1 and zero uncertainty  are said to be fixated. no experiments are performed regarding fixated practical conditionals. hence dido's learning process is complete when all practical conditionals in the representation have fixated. 
   generalization is a process which guards against overspecialization. whenever a class fixates  dido attempts to form a generalization by searching for other similar classes which contain non-fixated practical 
	scott and markovitch 	1 

conditionals having the same expected outcomes as the fixated practical conditional. if such a class is found a generalization class is formed which is the most specific superclass of the two classes being generalized. the nonfixated practical conditional is copied into this new class. if the fixated practical conditional was an overspecialization it will become useless and be discarded when fixation occurs in the more general class. 
   dido also makes use of a number of other methods for changing the representation. these handle contingencies which only arise occasionally  eg formation of redundant partitions of a class .  see scott & markovitch  1  for details . 
1. experimental runs 
   we have implemented dido and conducted a large number of experiments  using various domains. here we describe a selection of these. 
a typical run 
   this example  which illustrates dido's ability to discover the consequences of each member of the set of motor operations  involves a domain which might be part of an adventure type game. the entities dido may encounter have 1 attributes whose possible values are as follows: 
property: 	possible values 
type: 	dwarf  magician  pirate 
size: 	small  medium  large 
vitality: 	alive  dead 
coat color: red  blue  green  yellow  black  white  gold shoes: leather  boots  slippers  sandals  none 
the actions available are hit-with-axe  wave-wand  and rub-lamp. their consequences are: 
1 	machine learning 
hit-with-axe: kills any living thing wearing slippers. 
wave-wand: kills any living thing wearing boots. 
             turns any magician into a dwarf. rub-lamp: turns any dwarf into a pirate. 
turns any pirate into a magician. 
turns any magician into a dwarf. 
figure 1 shows the final representation dido achieved after 1 trials  at which point all the classes shown had fixated. as can be seen  this is a complete  consistent  and correct representation of the domain. furthermore  it is minimal in the sense that no representation with fewer classes is possible. 
figure 1 shows the steady progress of the system during learning. figure 1a shows the 'exam score'. during an exam  the learning is turned off and a random batch of 1 entities is shown to the system. the score is the percentage of outcomes predicted correctly. exams arc administered at regular intervals to monitor the progress of learning. note that 1% score is reached long before learning is complete. figure 1b shows the steady decline in average uncertainty. 
this figure also shows why learning is not complete when 
dido starts scoring 1% in exams. although the representation is correct  dido is not yet certain it is correct. during the last 1 trials dido is discovering that the representation achieved by trial 1 is a correct one. 
classification - conjunction and disjunction 
as we noted in the introduction  the task dido performs is different from that performed by other concept learning programs. in particular it differs from those that learn concepts from classified examples since there is no teacher to provide classified examples. this makes it hard to compare dido's performance with that of other systems. 

   since dido's representation generator involves a modified form of learning by specialization  a comparison with other programs which also learn by specializing general concepts  see langley 1 for review  would seem pertinent. in order to achieve this we ran dido with a 
   single motor operation ask-for-category. the outcome of applying this operation to an entity is a string which is the name of the category to which the entity belongs. in this way it is possible to get dido to learn a predefined classification scheme and hence compare the performance of its representation generator with those incorporated in other learning programs. 

   figure 1 shows the results obtained when dido learned conjunctive and disjunctive concepts. every entity had 1 attributes  and each of these took one of 1 possible values. relevant attributes were those that formed part of the definition of the category to be discovered. the graphs show the number of trials needed to reach 1% exam scores. as can be seen  both conjunctive and disjunctive concepts are readily discovered and  at least over the range considered  the number of examples needed seems to increase linearly with the number of relevant attributes. furthermore dido takes slightly longer to discover disjunctive concepts. this result is in marked contrast to the results reported by langley 
 1  on prism  a program which learns by specialization. he found that conjunctive concepts took much longer to discover than disjunctions  and that the number of examples per relevant attribute increased nonlinearly for conjunctions. 
noise and change 
   in order to test dido's ability to operate successfully in non-deterministic domains  we performed a series of classification tasks using varying levels of noise. various authors have defined noise level in different ways  eg langley  1; quinlan  1 . in our experiments noise level is simply a measure of the probability that the classification of an example will be inverted. this is equivalent to combining positive and negative noise following langley's  1  definition. the concept to be learned was a conjunction with 1 relevant attributes. various noise levels between zero and 1% were tried. to test the system  exams were run with zero noise. 
   for noise levels up to 1% the effect was simply to slow down the speed at which dido converged on the best approximate representation. for noise levels of 1% and above dido did not achieve the best approximation in 1 trials. she did however achieve a non-fixated representation in which the error level in exams  which were noise free  was considerably less than the noise level in the training examples. thus in the worst case  1% noise level  she achieved a representation which produced only 1% exam errors. at these higher levels of noise the uncertainty in dido's final representation was the same as the inherent uncertainty of the domain. 
1. discussion 
   how successful is dido at solving exploratory problems as defined in the introductory section of this paper  our original purpose in developing dido was to determine whether the concept of uncertainty could form a simple basis 
	scott and markovitch 	1 

for implementing curiosity. the results of our experiments show that using this heuristic to explore experience space does lead to a sensible choice of experiences and a consequent convergence on a good representation. the heuristic leads to efficient use of resources as is clearly shown in figure 1. dido achieves 1% exam scores long before all the practical conditionals have fixated. this happens because learning resources are switched between the operations available so that the amount of remaining uncertainty regarding each is about the same. the success is achieved quickly. there are 1 rules to discover and dido achieves 1% success after performing only 1% of the experiments possible in the domain. complete certainty is achieved after performing 1%. dido also uses space parsimoniously. individual experiences are not retained beyond the round of experiments in which they are generated. the number of classes in the network seldom exceeds double the number in the final solution. 
   the results obtained in classification experiments and in noisy domains show that the conjecture based method of learning  in which changes in the representation can be made rather freely because mistakes are readily discovered and retracted  may be of use in other types of concept learning. of particular interest is the absence of any marked nonlinearity in the time taken to learn progressively more complex conjunctions and disjunctions. however  there arc limitations to this method of representation generation. dido performs poorly at finding exclusive disjunctions and cannot solve multiplexor problems  wilson 1 . interestingly  people also find such concepts difficult to discover. in dido  these limitations arise as a direct consequence of the features which give rise to the speed at which correct answers are found for conjunction and disjunction. exclusive disjunction and the multiplexor problems are hard because partial solutions do not yield better than chance results. dido's readiness to retract apparently useless conjectures  which is the necessary price of making conjectures freely  leads to the abandonment of partially elaborated correct solutions. 
   dido's learning procedures could be applied to practical problems. in particular dido could be used to analyze empirical data such as very large databases composed of case histories paired with their eventual outcomes. the set of case histories would constitute dido's problem domain. using uncertainty guided search  case histories would be sampled in an attempt to learn to predict associated outcomes. the distinctive feature of dido's uncertainty guided search is that the sampling of the data set would concentrate on those areas about which remaining uncertainty was highest thus  once dido has learned the commonly occurring cases  resources will not be wasted considering further examples of those types  but will concentrate on seeking out and learning to predict the less common types. 
