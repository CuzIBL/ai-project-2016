 
     in recent years knowledge-based techniques like explanation-based learning  qualitative reasoning and case-based reasoning have been gaining considerable popularity in ai. such knowledge-based methods face two difficult problems: 1  the performance of the system is fundamentally limited by the knowledge initially encoded into its domain theory 1  the encoding of just the right knowledge to enable the system to function properly over a wide range of tasks and situations is virtually impossible for a complex domain. this paper describes research directed towards the construction of a system that will detect and correct problems with domain theories. this will enable knowledge-based systems to operate with imperfect domain theories and automatically correct the imperfections whenever they pose problems. this paper discusses the classification of imperfect theory problems  strategies for their detection and an approach based on experiment design to handle different types of imperfect theory problems. 
	i 	introduction 
     this paper addresses the problem of imperfect theories in al systems. it is increasingly apparent that knowledge is essential for intelligent behavior. this has led to a new trend in al towards knowledge-intensive methods like explanation-based learning  l. 1 . qualitative reasoning . and case-based reasoning  1. 1 . 
     the primary shortcoming of these approaches is not in the representation of the knowledge - a task that is relatively well understood - but in the subtleties of selecting the appropriate knowledge. the expert who is handcoding the knowledge has to anticipate the rich variety of tasks and the wide range of situations for which the knowledge may be used in order to insure that the system will function properly. also  all ai systems that rely on a programmer-specified domain theory are fundamentally limited by their initial knowledge. for example.  shows how the knowledge built into a learning system drastically influences its learning capability. 
     what is needed is a system that will automatically detect and correct problems with its domain theory. this will free the expert from the tedious and often impossible task of handcoding all the relevant knowledge. it will enable the use of  quick and dirty  methods to facilitate the construction of operational but imperfect domain theories. these domain theories can then be automatically debugged and corrected by the system. 
     mitchell et al.  l  have briefly classified problems with imperfect domain theories into three categories: 
 1  the incomplete theory problem: the deductions required cannot be computed because relevant information is missing. 
this research was supported in part by a university of illinois cognitive 
science/artificial intelligence fellowship and in pan by the office of naval research under grant n 1-k-1.  1  the inconsistent theory problem: the system can derive inconsistent statements from its theory. 
 1  the intractable theory problem: the deductions are computationally prohibitive and hence cannot be completed. 
     however  the underlying issues are too murky and subtle for the above categories to be cleanly separable. for example  inconsistencies and incompleteness in domain theories may be due to abstractions and approximations which make the theory tractable . inconsistent theory problems can be due to an incomplete theory if information necessary to nullify one of the inconsistent statements is missing. inconsistent statements can also result from the incomplete theory problem if the system is operating under the closed world assumption and does not consider the possibility of new information influencing its computations . apart from the above problems of interacting categories  the classification of mitchell el al. also ignores certain kinds of incompleteness and inconsistencies. 
     a complete taxonomy of imperfect theory problems includes two types of incompleteness and inconsistencies. the first type of incompleteness is the one discussed above in which a deduction cannot be completed because some relevant knowledge is missing. the second type of incompleteness is due to the lack of sufficient detail in the relevant knowledge. unlike the first case  deductions can be constructed leading to a conclusion. however  the lack of detail results in the system having to make assumptions and leads to the problem of multiple mutually inconsistent proofs for a conclusion. this type of incompleteness also results in large search spaces because the system does not have the required control knowledge to select the correct path at each choice point. the first type of inconsistency involves wrong knowledge that has to be identified and retracted. the second type of inconsistency involves missing knowledge that would have defeated the deduction leading to 
one of the inconsistent statements. 
     there are two aspects to the imperfect theory problems detection of the imperfections and the revision of the domain theory - and both of these present difficulties. this paper describes various strategies for delecting problems with the domain theory and a uniform approach based on experiment design to handle each type of problem. the system is assumed to start with an initially imperfect but operational theory. this is a psychologically motivated assumption since people also use simplified domain theories to make conclusions computationally tractable and they are still able to operate satisfactorily. during the course of the system's operation  problems with its domain theory are identified and corrected. no changes are made until a problem is detected. 
ii detection of the imperfect theory problems 
     this section describes four strategies for detecting problems with domain theories. though the detection strategies- are discussed in the context of explanation construction for 
	rajamoney and dejong 	1 

explanation-based learning  l. 1  they are also applicable for other problem solving tasks like qualitative reasoning and planning. explanation construction involves using facts and rules from the domain theory to show why a training instance is an example of the goal concept  figure la . the problems due to imperfect domain theories that are encountered during explanation construction are: 
broken explanation: there are gaps in the explanation leading to a broken explanation  figure lb . the rules or facts that are required to complete the explanation are missing from the domain theory  incompleteness - type i . 
contradiction: the system constructs explanations for conclusions which are contradictory  figure lc . this problem may be due to wrong rules or facts in the domain theory  inconsistency - type i  or due to missing rules or facts  inconsistency - type ii  that would resolve the contradiction by defeating one of the explanations  el or e1  thereby leading to the withdrawal of the corresponding previously justified conclusion  p or  not p  . multiple explanations: the system constructs multiple explanations for a conclusion when only one explanation is expected to be true in the real world  figure id . this problem is due to lack of knowledge which would help distinguish between the alternate explanations  incompleteness - type ii . this problem is especially important for explanation-based learning as is has implications for the new concept definition. 
resources exceeded: the system exceeds the resources  time  memory  etc.  allotted to it while constructing an explanation. this type of problem can be further classified as: 
large search space problem: the system has to search a large space during the construction of an explanation  figure le . though the explanation may exist and its size may be comparable to previous successful explanations the system cannot construct it since there are too many paths to explore. the system does not have the knowledge to decide between the alternate 
domain facts 
training example 

	contradiction 	p 

figure 1:  a  a typical explanation  b  a broken explanation  c  a contradiction  d  multiple explanations  e  large search space problem  f  small links problem. 
1 	knowledge acquisition 
paths  incompleteness - type ii  and is forced to search all paths. small links problem: the links connecting the explanation are too small and too many for the system to construct the complete explanation within the allotted resources  figure if   intractable theory problem . this problem is independent of the large search space problem and may occur even when no search is involved. 
ill dealing with the imperfect theory problems 
　　dealing with the above problems requires the acquisition of new knowledge. this section describes ongoing research on an extension to an approach discussed in  1. 1  that can be used to deal with the above problems. 
a. a brief review of the experiment design approach 
　　an approach that deals with the contradiction problem due to an inconsistent domain theory  type ii  is described in  1. 1 . the approach involves: 1  monitoring the execution of the system's plans. 1  detection of contradictions if the systems predictions are not compatible with the observations. 1  hypothesizing reasons which could resolve the contradiction. 1  designing experiments to test each hypothesis. 1  incorporating the information obtained by the experiments into the domain theory. five classes of experiments are described in . these experiments are used to discriminate among hypotheses  perform measurements  find  dependencies among parameters  classify objects based on their behavior with respect to a properly and define new properties of objects based on their behavior in a situation. these experiments are used to obtain new knowledge that is relevant to the determination of the correct hypothesis. 
b. extending the experiment design approach 
     the experiment design approach can be applied to each of the problems described in section 1: 
broken explanation: the system must be able to hypothesize different ways of filling the gaps in the explanations. in  1  1  the hypotheses were suggested by the system after an analysis of the situation that led to the failure. alternatively such hypotheses may be formed by analogy to previous experiences . once alternate hypotheses that can complete the explanation have been formulated experiments are designed to determine the best hypothesis. 
contradiction: experiments are designed to test each link in each explanation to isolate the faulty rule or fact that leads to the contradiction. once the fault has been isolated then hypotheses are formulated to correct the fault. if the contradiction is due to wrong rules or facts  inconsistent - type i  then the hypotheses can involve retraction of rules. if the contradiction is due to missing knowledge  inconsistent - type ii  then the hypotheses can involve positing rules that defeat the explanation. fxperiments are designed to identify the best hypothesis. multiple explanations: multiple explanations arise due to the lack of knowledge required to distinguish between the alternative explanations  incompleteness - type ii . fxperiments are designed to gather the information that the system needs to decide which explanations cannot hold for the given situation. 
this will enable it to determine the correct explanation. resources exceeded: the large search space problem can be handled by designing experiments to gather the information needed to make the right choice whenever alternatives develop. a number of approaches have been suggested for the small links problem  1  1 .  shows how approximations can be used to make explanations tractable.  describes an incremental failure-driven technique to refine abstract theories when the current theory fails to provide a satisfactory explanation. the approach suggested by  1. ll  involves describing the domain theory at different levels of abstraction. this allows the explanation to be constructed using fewer higher-level links. how-


figure 1: an example illustrating the mult pie explanations problem due to incomplete knowledge. 

ever  due to the abstractions and approximations a number of alternate low-level explanations may be possible for one higherlevel explanation and this failure cannot be handled by examining the more detailed levels. this is the  hierarchical  multiple explanation problem and the experiment design approach can be applied to find the correct explanation. 
c. an example 
     the system is given the distillation scenario shown in figure 1. a mixture of alcohol and water is heated and it is observed that an unknown liquid is formed in the second container and that its amount is increasing. the domain theory does not have rules or facts that allow the system to determine which liquid will boil first  incomplete - type 1  and therefore it has to take into account all possibilities. the system constructs three different explanations for the increase in the amount of the liquid in the second container  the multiple explanation problem . for example  if the boiling point of alcohol is less than that of water then when the temperature of the mixture reaches the boiling point of alcohol the heat flow to the mixture will cause alcohol to boil. boiling will produce alcohol vapor which will cause the pressure in the container to increase. the pressure will become greater than the pressure in the second container and there will be a flow of alcohol vapor to the second container. this vapor will cool and condense since the second container is at a very low temperature. the condensing alcohol forms the explanation for the observed formation and increase in the amount of the unknown liquid. similarly  if the boiling point of alcohol is less than or equal to that of water then water or a mixture of alcohol and water will condense in the second container. it is important to determine which explanation is correct since the explanation is worth generalizing and learning only if a useful goal is being achieved - for example  if alcohol is condensing then we have separated alcohol from water or obtained a purer version of alcohol  distillation . the system identifies the correct explanation by designing experiments to determine whether the liquid formed in the second container is water  alcohol or a mixture of both. this example also illustrates the large search space problem if the above task is part of a much larger task - like understanding a distillation factory - that builds in separate directions on each explanation. then the above experiments help in pruning the search space by immediately eliminating two of the three choices for the unknown liquid. the system can also design experiments to select the correct path during explanation 'construction by determining independently whether the boiling point of water is greater than  equal to or less than that of 
alcohol and applying that information to the given situation. 
iv conclusions 
　　in this paper we have discussed problems with and extensions of mitchell et al.'s classification of imperfect theory problems. four strategies for detecting imperfections in domain theories were described. a uniform approach for handling these problems based on experiment design was also described and illustrated by an example. these methods were discussed in the context of explanation construction for explanation-based learning. however the detection strategies and the experiment design 
approach are general and can be applied to other knowledgeintensive al areas like case-based reasoning  expert systems and 
qualitative reasoning. 
acknowledgments 
　　we would like to thank ray mooney and steve chien for their helpful comments on drafts of this paper. 
