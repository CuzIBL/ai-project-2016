 
this paper describes a natural language processing system developed for the semantic network array processor  snap . the goal of our work is to develop a scalable and high-performance natural language processing system which utilizes the high degree of parallelism provided by the snap machine. we have implemented an experimental machine translation system as a central part of a realtime speech-to-speech dialogue translation system. it is a snap version of the d m d i a i o g speech-tospeech translation system. memory-based natural language processing and syntactic constraint network model has been incorporated using parallel marker-passing which is directly supported from hardware level. experimental results demonstrate that the parsing of a sentence is done in the order of milliseconds. 
1 	i n t r o d u c t i o n 
in this paper  we will demonstrate that natural language processing speeds in the order of milliseconds is attainable by using a marker-propagation algorithm and a specialized parallel hardware. 
¡¡the significance of the high-performance  or real-time  natural language processing is well known. parsing sentences at the milliseconds speeds enables the realization of a speech recognition module capable of real-time speech understanding which eventually leads to the real-time simultaneous interpretation system. also  the millisecond order performance enables the system to parse hundreds of sentences in a second  or over 1 million sentences per hour. this in turn makes possible bulk processing of text such as full-text retrieval  summarization  classification  translation  indexing and tagging. 
¡¡in order to accomplish the high-performance natural language processing  we have designed a highly parallel machine called semantic network array processor  snap   moldovan and lee  1   lee and moldovan  1  and implemented an experimental machine translation system called dmsn ap using a parallel marker-passing scheme. dmsnap is a snap implementation of the dmdialog speech-
   *this research has been funded by the national science foundation grant no. mip-1 and mip-1. 
to-speech dialogue translation system  kitano  1a   kitano  1a   but with some modifications to meet hardware constraints. despite its high performance  our system carries out sound syntactic and se  antic analysis including lexical ambiguity  structural ambiguity  pronoun reference  control  unbounded dependency  and others. 
¡¡in the next section  we describe briefly the snap architecture  then  describe design philosophy behind the d m s n a p followed by descriptions on implementation and linguistic processing. finally  performance are presented. 
1 	s n a p architecture 
the semantic network array processor  snap  is a highly parallel array processor fully optimized for semantic network processing with marker-passing mechanism. in order to facilitate efficient propagation of markers and to case development of applications  a set of marker propagation instructions has been microcoded. snap supports propagation of markers containing  1  bit-vectors   1  address  and  1  numeric value. by limiting content of markers  significant reduction in cost and resource has been attained without undermining performance requirements for knowledge processing. several al applications such as natural language processing system  classification system  kim and moldovan  1   and rule-based system has been developed on snap. 
the architecture 
snap consists of a processor array and an array controller 
 figure 1 . the processor array has processing cells which contain the nodes and links of a semantic network. the snap array consists of 1 processing elements each of which consists of tms1 dsp chip  local sram  etc. each processing elements stores 1 nodes which act as virtual processors. they are interconnected via a modified hypercubc network. the snap controller interfaces the snap array with a sun 1 host and broadcasts instructions to control the operation of the array. the instructions for the array are distributed through a global bus by the controller. propagation of markers and the execution of other instructions can be processed simultaneously. 
instruction sets 
a set of 1 high-level instructions specific to semantic network processing are implemented directly in hardware. these include associative search  marker setting and propagation  logical/arithmetic operations involving markers  create and 
	kitano  moldovan  and cha 	1 

figure 1: snap architecture 
delete nodes and relations  and collect a list of nodes with a certain marker set. currently  the instruction set can be called from c language so that users can develop applications witli an extended version of c language. from the program ming level  snap provides data-parallel programming environment similar to c* of the connection machine  thinking machine corp.  1   but specialized for semantic network processing with marker passing. 
propagation rules 
several marker propagation rules arc provided to govern the movement of markers. marker propagation rules enables us to implement guided  or constraint  marker passing as well as unguided marker passing. this is done by specifying type of links that markers can propagate. all markers in d m s n a p are guided markers  thus they are controlled by propagation rules. the following are some of the propagation rules of snap: 
  seq rl  r1  : the seq  sequence  propagation rule allows the marker to propagate through rl once then to r1. 
  spread rl  r1  : the spread propagation rule allows the marker to travel through a chain of rl links and then r1 links. 
  comb  rl  r1  : the comb  combine  propagation rule allows the marker to propagate to all rl and r1 links without limitation. 
knowledge representation on snap 
snap provides four knowledge representation elements: node  link  node color and link value. these elements offers wide range of knowledge representation schemes to be mapped on snap. on snap  a concept is represented by a node. a relation can be represented by either a node called relation node or a link between two nodes. the node color indicates the type of node. for example  when representing usc is in los angeles and cmu is in p i t t s b u r g h   we may assign a relation node for i n . the 
in node is shared by the two facts. in order to prevent the wrong interpretations such as usc in p i t t s b u r g h and cmu in los angeles  we assign in#1 and in#1 to two distinct in relations  and group the two relation nodes by a node color in. each link has assigned to it a link value which indicates the strength of interconcepts relations. this link value supports probabilistic reasoning and connectionist-like processing. these four basic elements allow snap to support virtually any kinds of graph-based knowledge representation formalisms such as kl-one  brachman and schmolze  1   conceptual graphs  sowa  1  kod1ak  wilcnsky  1  etc. 
1 	philosophy behind d m s n a p 
d m s n a p is a snap implementation of the dmdialog 
speech-to-speech dialogue translation system. naturally  it inherits basic ideas and mechanisms of the dmdialog system such as memory-based approach to natural language processing and parallel marker-passing. syntactic constraint network is introduced in d m s n a p whereas dmdialog has been assuming unification operation to handle linguistic processing. 
memory-based natural language processing memory-based nlp is an idea of viewing nlp as a memory activity. for example  parsing is considered as a memorysearch process which identifies similar cases in the past from the memory  and to provide interpretation based on the identified case. it can be considered as an application of memorybased reasoning  mbr   stanfill and waltz  1 and casebased reasoning  cbr   riesbeckand schank  1  to nlp. this view  however  counters to traditional idea to view nlp as an extensive rule application process to build up meaning representation. some models has been proposed in this direction  such as direct memory access parsing  dmap   ricsbeck and martin  1  and dmdialog  kitano  1aj. we consider that memory-based approach is superior than traditional approach. for arguments concerning superiority of the memory-based approach over the traditional approach  see  nagao  1    riesbeck and martin  1    kitano  1al  isumita and lida  1   kitano and higuchi  1a   and  kitano and higuchi  1b . 
parallel marker-passing 
one other feature inherited from the dmdialog is use of parallel marker-passing. in d m s n a p   however  a different approach has been taken with regard to the content of markers propagate through the network. since dmdialog has been designed and implemented on idealized simulation of massively parallel machines  markers carry feature structure  or graph  along with other information such as probabilistic measures  and unification or a similar heavy symbolic operations has been assumed at each processor element  pe . in the dmsnap  content of the marker is restricted to  1  bit markers   1  address markers  and  1  values 1. propagation of feature structures and heavy symbolic operations at each pe  as seen in the original version of the dmdialog  are not practical assumptions to make  at least  on current massively parallel machines due to processor power  memory capacity on each pe  and the communication bottleneck. propagation of feature structures would impose serious hardware design problems since size of the message is unbounded 
     1  we call a type of marker-passing which propagates feature structures  or graphs  an unbounded message passing. a type of markerpassing which passes fix-length packets as seem in dmsnap is a finite message passing. this classification is derived from  blcl loch  1 . with the classification in  blelloch  1   our model is close to the activity flow network. 
 unbounded message passing . also  pes capable of performing unification would be large in physical size which causes assembly problems when thousands of processors are to be assembled into one machine. even with machines which overcome these problems  applications with a restricted message passing model would run much faster than applications with an unbounded message passing model. thus  in dmsnap  information propagated is restricted to bit markers  address markers  and values. these are readily supported by snap from hardware level. 
syntactic constraint network 
syntactic constraint network  scn  is a new feature which has not been used in the previous works in memory-based nlp. scn is used to handle syntactic phenomena without undermining benefits of memory-based approach. although  unification has been the central operation in the recent syntactic theories such as lfg  kaplan and bresnan  1  and hpsg  pollard and sag  1   we prefer scn over unificationbased approach because unification is computationally expensive and it is not suitable for massively parallel implementation. although there is a report on an unification algorithm on massively parallel machines  kitano  1b   still it is computationally expensive  and takes up major part of computing time even on snap. in addition  there is a report that unification is not necessary the correct mechanism of enforcing agreement  ingria  1. also  the classification-based approach  kasper  1   which pre-compiles a hierarchy of feature structures in the form of a semantic network  can carry out similar task with less computational cost  kim and moldovan  1 . finally  current unification hard-rejects failure which is not desirable from our point. we want the system to be robust enough that while recognizing minor syntactic violation  it keep processing to get meaning of the sentence. 
in the syntactic constraint network model  all syntactic con-
straints are represented in the finite-state network consists of  !  nodes representing specific syntactic constraints  such as 1sgpres    1  nodes representing grammatical functions  such as subj  obj  and obj1 for functional controller   and  1  syntactic constraint links which control state-transitions and the passing of information among them. although  unification has been used to  1  enforce formal agreement   1  percolate features  and  1  building up feature structure  we argue that these functions are attained by independent mechanism in our model. formal agreement is enforced by activation and inhibition of nodes through active syntactic constraints. per-
colation of feature is attained by passing of address through memory and syntactic constraint networks. it should be noted that not all features now being carried by unification grammar need to be carried around in order to make an interpretation of sentences. our model only propagates necessary information to relevant nodes. finally  instead of building up features  we distributively represent meaning of the sentence. when parsing is complete  we have a set of new nodes where each represents an instance of concept and links defines relation among them. 
¡¡we are currently investigating whether our model is consistent with human language processing which has limited memory capacity  gibson  1 . 

figure 1: concept sequence on snap 
1 	i m p l e m e n t a t i o n o f d m s n a p 
d m s n a p consists of the memory network  syntactic constraint network  and markers to carry out inference. the memory network and the syntactic constraint network are compiled from a set of grammar rules written for d m s n a p . this section describes these components and a basic parsing algorithm to provide brief implementation aspects of the 
dmsnap. 
1 	memory network on snap 
the major types of knowledge required for language uanslation in d m s n a p are: a lexicon  a concept type hierarchy  concept sequences  and syntactic constraints. among them  the syntactic constraints are represented in the syntactic constraint network  and the rest of the knowledge is represented in the memory network. the memory network consists of various types of nodes such as concept sequence class  csc   lexical item nodes  lex   concept nodes  cc  and others. nodes are connected by a number of different links such as concept abstraction links  isa   expression links for both source language and target language  eng and jpn   role links  role   constraint links  constraint   contextual links  context  and others. 
   a csc captures ordering constraints of natural language  and it roughly corresponds to phrase structure rules. cscs can be used to represent syntax and semantics of sentences at different levels of abstraction from instances of surface sequence to linguistically motivated grammar such as lexicalfunctional grammar  lfg   kaplan and bresnan  1 . as shown in figure 1  a csc consists of a root node  csr   clement nodes  cse   a first link  a last link  next link s  and role links. a csr is a representative node for the meaning of the entire csc structure. csrs are connected to their designated interlingual concepts by eng or jpn. each csc has one or more cses linked to a csr by role links. the ordering constraints between two concept sequence element nodes arc represented by next link. first and last links in each csc points to the first and last elements  respectively. also  each cse represents the relevant case role  and the case role has a selectional restriction. since we want to avoid heavy symbolic operations during parsing  role links and associated constraint links are used instead of performing type and value consistency check by unification. therefore each cse is used for both enforcing the ordering constraint and capturing semantic information. 
¡¡besides  concept instance nodes  ci  and concept sequence instance structures  csi  are dynamically created during parsing. each ci or csi is connected to the associated cc or 
	kitano  moldovan  and cha 	1 

csc by inst link. cis correspond to discourse entities proposed in  webber  1 . three additional links are used to facilitate pragmatic inferences. they arc context links  constraint links and eqrole links. a context link is a path of contextual priming which is crucial in word sense disambiguation. when a word is activated during processing  the activation spreads through context links and impose contextual priming to relevant concepts. a constraint link denotes an antecedent/consequence relationship between two events or states  which is created between two csrs. an eqrole link denotes the necessary argument matching condition for testing an antecedent/consequence relationship  which is created between two cses in different cscs. 
1 	syntactic constraint network 
d m s n a p has a syntactic constraint network  scn  which captures various syntactic constraints such as agreement  control  etc. syntactic constraint network consists of syntactic constraint nodes  sc nodes   syntactic function nodes  sf nodes   and syntactic constraint links  sc-links . sc nodes represents syntactic constraints such as 1rd singular present  1sgprcs  and reflexive pronoun  ref . these nodes simply get bit markers to indicate whether these syntactic constraints arc active or not  and send bit markers to show which lexical items are legal candidate for the next word. sf nodes represents grammatical functions such as functional controllers. sf nodes generally get an address marker and a bit marker. the address marker carries point to the ci nodes which should be bound to a case-frame in dislocated place in the network  and a bit marker shows whether the specific grammatical function node should be activated. when both a bit marker and an address marker exist at a certain sf node  the address marker is further propagated through sc-links to send information which is necessary to carry out interpretation of sentences involving control and unbounded dependency. 
1 	markers 
the processing of natural language on a marker-propagation architecture requires the creation and movement of markers on the memory network. the following types of markers are used: 
a-markers indicate activation of nodes. they propagate through isa links upward  carry a pointer to the source of activation and a cost measure. 
p-markers indicate the next possible nodes to be activated. 
they are initially placed on the first element nodes of the 
cscs  and move through next link where they collide with a-markers at the element nodes. 
g-markers indicate activation of nodes in the target language. they carry pointers to the lexical node to be lexicalized  and propagate through isa links upward. 
v-markers indicate current state of the verbalization. when a v-marker collides with the g-marker  the surface string  which is specified by the pointer in the gmarker  is verbalized. 
c-markers indicate contextual priming. nodes with cmarkers are contextually primed. a c-marker moves from the designated contextual root node to other contextually relevant nodes through contextual links. 
sc-markers indicate active syntax constraints  and primed and/or inhibited nodes by currently active syntactic constraints. it also carries pointer to specific nodes. 
¡¡there are some other markers used for control process and timing  they are not described here. these five markers are sufficient to understand the central part of the algorithm in this paper. 
1 	dmsnap parsing algorithm 
overall flow of the algorithm implemented on snap consists of the following steps: 
1. activate a lexical node 
1. puss an a-marker through isa link; pass sc-markers through sc-link. 
1. when the a-marker collide with a p-marker on cse  the p 
marker is passed through next link once. however  if the cse was the last element of the csc  then the csc is accepted and an a-markcr is passed up through isa link from csr of the accepted csc. 
1. when the p-marker passed through the next link in step 1  then a copy of the p marker is passed down through inverse isa link to make top-down prediction. 
1. pass sc-markers from active nodes to activate and/or inhibit syntactic constraints  and to percolate pointers to the specific cl 
1. repeat 1 through 1 until all words are read. 
1. compute total cost for each hypothesis. 
1. select lowest cost hypothesis. 
1. remove other hypotheses. 
¡¡this parsing algorithm is similar to the shift-reduce parser except that our algorithms handles ambiguities  parallel processing of each hypothesis  and top-down predictions of possible next input symbol. the generation algorithm implemented on snap is a version of the lexically guided bottom-up algorithm which is described in  kitano  1b . 
1 	linguistic processing in d m s n a p 
we will explain how dmsnap carries out linguistic analysis using two sets of examples: 
	| 	example i  
  s i john wanted to attend uc ai-1. 
s1 he is at the conference. s1 he said that the quality of the paper is superb. 	| 1 example ii  s1 dan planned to develop a parallel processing computer. s1 eric built a snap simulator. s1 juntae found bugs in the simulator. s1 dan tried to persuade eric to help juntae modify the simulator. s1 juntae solved a problem with the simulator. i s1 it was the bug that juntae mentioned. 	| ¡¡the examples contain various linguistic phenomena such as: lexical ambiguity  structural ambiguity  referencing  pronoun reference  definite noun reference  etc   control  and unbounded dependencies. it should be noted that each example consists of a set of sentences  not a single sentence isolated 


figure 1: part of memory network 
from the context  in order to demonstrate contextual processing capability of the d m s n a p . these sentences are not all the sentences which d m s n a p can handle. currently  dmsnap handles substantial portion of the atr's conference registration domain  vocabulary 1 words  1 sentences  and sentences from other corpora. 
1 	basic parsing and generation - translation 
the essence of d m s n a p parsing and generation algorithm is described using sentence s i . a part of memory network involved in this explanation is shown in figure 1. c- denotes concepts; and  ...  denotes surface string in the lexical node. notice that only a part of the memory network is shown and no part of the syntactic constraint network is shown here. also  the following explanation does not describe activity of the syntactic constraint network part. this will be described in a relevant part later. 
¡¡initially  the first cse in every esc on the memory network gets a p-marker. this p-marker is passed down isa links. 
the ccs receiving a p-marker are c-person and c-arrend. also the closed class lexical items  cci  in the target language propagate g-marker upward isa links. 
upon processing the first word 'john' in the sentence s i   
c-joiin is activated so that c-john gets an a-marker and a ci john#l is created under c-john. at this point  the corresponding japanese lexical item is searched for  and jon is found. a g-marker is created on jon. the a-marker and g-marker propagate up through isa links  activating c-male-person and c-person in sequence  and  then  role links. when an a-marker collides with a p-marker at a cse  the associated case role is bound with the source of the a-marker and the prediction is updated by passing pmarker to the next cse. this p-marker is passed down isa links. in this memory network  the actor roles of concept sequences want-circum-e is bound to john# 1 pointed by the a-marker. this is made possible in the snap architecture which allows markers to carry address as well as bit-vectors and values  where many other marker-passing machines such as netl  fahlman  1  and ixm1  higuchi et. al.  1  only allow bit-vectors to be passed around. also  g-markers 
are placed on the actor role cse of want-circum-j. the gmarker points to the japanese lexical item 'jon'. 
¡¡after processing ' wanted' and 'to'  a p-marker is passed to circum and  then  to attend-conf. at this point  a source language  english  expression for the concept attend-conf is searched for and attend-conf-e is found. the first cse of attend-conf-e gets a p-marker. after processing attend' and 'ijcal-1'  attend-conf-e becomes fully recognized1 so that a csi having cis is created under attend-conf-e. then the associated concept attend-conf is activated. an amarker is passed up from attend-conf to the last element of the csr want-circum-e. as the result  the csr wantcircum-e and its cc want-circum are activated in sequence. 
therefore the parsing result is represented by the activated cc 
want-circum and the associated csi. also  upon processing 
'ijcai-1  the concept c-conference is activated and then c-markers are passed to nodes connected to c-conference by context links. this is an operation for contextual priming. 
¡¡when the parsing is done  a v-marker is passed to the target language  japanese  expression want-circum-j from 
want-circum  and  then  to the first cse of want-circum-j. since the first cse has a g-marker pointing to jon  'jon' becomes the first word in the translated japanese sentence and then the v-marker is passed to the next cse. see  kitano  1b  for the details of generation process. this operation is repeated for all cses in the csc. finally  the japanese sentence t1 is constructed for the english sentence s1. 
¡¡with this algorithm  the first set of sentences  s1  s1 and s 1  is translated into japanese: t1 jon ha ichikai-1 ni sanka shilakatta. t1 kare ha kaigi ni iru. t1 kare ha ronbun no shitsu ga subarashii to itla. 
1 	anaphora 
anaphoric reference is resolved by searching for discourse entity as represented by cis under a specific type of concept node. sentence s1 contains anaphora problems due to 'he' and 'the confercnce  when processing 'he  dmsnap searches for any cis under the concept c-male-person and its subclass concepts such as c-john. in the current discourse  john#1 is found under c-joiin. john#1 and ucai-1 are created when the s1 is parsed. an a-marker pointing to john#1 propagates up through isa links. likewise  ijcai1 #1 is found for c-conference. in this sentence  there is only one discourse entity  ci in our model  as a candidate for each anaphoric reference  thus a simple instance search over the typed hierarchy network suffices. however  when there are multiple candidates  we use the centering theory by introducing forward-looking center  cf   backward-looking center  cb   etc  brennan et. al.  1 . also  incorporating the notion of the focus is straightforward  sidner  1 . 
1 	lexical ambiguity 
d m s n a p is capable of resolving this lexical ambiguity through use of contextual priming using the contextual marker  c-marker   tomabechi  1  and the cost-based disambiguation  kitano et. al.  1 . sentence s1 contains a 
1
fully recognized means that the csc can be reduced  in the 
shift-reduce parser's expression. 
	kitano  moldovan  and cha 	1 
word sense ambiguity in the interpretation of the word 'paper ' as either a technical document or a sheet of paper. upon reading ' paper'  c-thesis and c-paper are activated. at this time  c-thesis has a c-marker. the c-marker comes from activation of c-ucal-1 and c-conference  in previous 
sentences  which has contextual links connecting concepts relevant to academic conference such as c-thesis. the meaning hypothesis containing c-thesis costs less than the one with c-paper so that it is selected as the best hypothesis. 
1 	control 
control is handled using the syntactic constraint network. sentence s 1 is an example of sentence involving functional control  bresnan  1 . in s1  both subject control and object control exist - the subject of 'persuade* should be the subject of 'tried'  subject control   and the subject of 'help' should be the object of 'persuade*  object control . in this case  cscs for infinitival complement has cse without next link. such an cse represents missing subject. there are subj  obj  and obj1 nodes  these are functional controller  in the syntactic constraints network each of which store pointer lo the ci node for possible controllee. syntactic constraint links from each lexical items of the verb determine which functional controller is active. activated functional controller propagate a pointer to the ci node to unbound subject nodes of cscs for infinitival complements. basically  one set of nodes for functional controller handles deeply nested cases due to 
functional 	locality. 
¡¡take an example from s1  when processing 'dan*  a pointer to an instance of 'dan' which is c-dan#1 is passed to sub j node of functional controller. then  when processing 'tried'  a sc-marker propagates from the lexical node of 'tried* to subj through sc-link  and the subj node is being activated. 
then  the pointer to c-dan#1 in the subj node propagate to 
subj role node  or actor node  of the csc for infinitival complement. after processing 'to' the csc for infinitival complement is predicted. temporal bindings take place in each predicted csc. when processing 'persuade'  however  obj gets activated since 'persuade* enforces object control  not subject control. thus  after 'eric' is processed  a pointer to an instance of 'eric* propagate in to the already active obj node  and then propagate to subj role node  or actor role node  of each csc for infinitival complement. this way  d m s n a p performs control. 
1 	structural ambiguity 
structural ambiguity is resolved by the cost-based ambiguity resolution method fkitano et. al.  1 . the cost-based ambiguity resolution takes into account various psycholinguistic studies such as  crain and steedman  1  and  ford et. al.  
1 . sentence s1 contains a structural ambiguity in the pp-attachment. it can be interpreted cither: 

in this case  two hypotheses are activated at the end of the parse. then  d m s n a p computes the cost of each hypothesis. factors involved are contextual priming  lexical preference  existence of discourse entity  and consistency with world knowledge. in this example  the consistency with the world knowledge plays central role. the world knowledge is a set of knowledge of common sense and knowledge obtained from understanding previous sentences. to resolve ambiguity in this example  the d m s n a p checks if there is a problem in the simulator. constraint checks are performed by bit-marker propagation through constraint links and eqrole links. since there is a ci which packages instances of error and snap-simulator then the constraint is satisfied and the second interpretation incurs no cost from constrain check. however  there is no ci which packages instances of juntae and snap-simuilator. therefore the first interpretation incurs a cost of constraint violation  1 in our current implementation . thus dmsnap is able to interpret the structural ambiguity in favor of the second interpretation. 
1 	unbounded dependency 
there are two ways to handle sentences with unbounded dependency. the first approach is straightforward memorybased approach which simply store a set of cscs involves unbounded dependency. a large set of cscs would have to be prepared for this  but its simplicity minimized computational requirements. alternatively  we can employ somewhat linguistic treatment of this phenomena within our framework. the syntactic constraint network has a node representing 
topic and focus which usually bound to the displaced phrase. an address of ci for the displaced phrase  such as 
'the bug' in the example s1  is propagated to the topic or focus nodes in the syntactic constraint network. further propagation of the address of the ci is controlled by activation of nodes along the syntactic constraint network. the network virtually encodes a finite-state transition equivalent to {compixcomp}*gf-comp  kaplan and zaenen  1  where gf-comp denotes grammatical functions other than comp. the address of the ci bound to topic or focus can propagate through the path based on the activation patterns of the syntactic constraint network  and the activation patterns are essentially controlled by markers flow from the memory network. when the csc is accepted and there is a case-role not bound to any ci  object in the example   the cse for the case-role bound with the ci propagated from the syntactic constraint network. 
1 	performance 
d m s n a p complete parsing in the order of milliseconds. 
while actual snap hardware is now being assembled and to be fully operational by may 1  this section provides performance estimation with precise simulation of the snap machine. simulations of the d m s n a p algorithm have been performed on a sun 1 using the snap simulator which has been developed at usc  lin and moldovan  1 . the simulator is implemented in both sun common lisp and c  and simulates the snap machine at the processor level. the lisp version of the simulators also provides information about the number of snap clock cycles required to perform the simulation. 
¡¡there are two versions of d m s n a p   one written in lisp and one in c. the high-level languages only take care of the process flow control  and the actual processing is done with snap instructions. the performance data summarized in 
table 1 was obtained with the first version of d m s n a p written in lisp. furthermore  with a clock speed of 1 mhz  these execution times are in the order of 1 millisecond. these and 


table 1: execution times for dmsnap 
other simulation results verify the operation of the algorithm and indicate that typical runtimeis on the order of milliseconds per sentence. 
¡¡the size of the memory network for example ii is far larger than that of example 1  yet we see no notable increase in the processing time. this is due to the use of a guided markerpassing which constraints propagation paths of markers. our analysis of the algorithm shows that parsing time grow only to sublinear to the size of the network. 
1 	conclusion 
in this paper  we have demonstrated that high-performance natural language processing with parsing speeds in the order of milliseconds is achievable without making substantial compromise in linguistic analysis. to the contrary  our model is superior to other traditional natural language processing models in several aspects  particularly  in contextual processing. 
¡¡the d m s n a p is based on the idea of memory-based model of natural language processing. the d m s n a p is a variation of the odmdialog speech-to-spcech dialog translation system. we use the parallel marker-passing scheme to perform parsing  generation  and inferencing. the syntactic constraint network was introduced to handle linguistically complex phenomena without undermining benefits of the memory-based approach. 
¡¡not only the d m s n a p exhibits high-performance natural language processing  but also demonstrates capabilities to carry out linguistically sound parsing particularly on contextual processing. the use of the memory network to distributively represent knowledge and modify it to reflect new states of the mental model is an effective way to handle such phenomena as pronoun reference and control. 
¡¡in summary  we demonstrated that the model presented in this paper is a promising approach to high-performance natural language processing with highly contextual and linguistics sound processing. we hope to extend this work to the real-world domains in the near-future. we are convinced that millisecond performance opens new possibilities for natural language processing. 
