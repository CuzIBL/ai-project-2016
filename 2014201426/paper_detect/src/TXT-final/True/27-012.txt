 
markov decision processes  mdps  have recently been appbed to the problem of modeling decisiontheoretic planning while traditional methods for solving mdps are often practical for small states spaces  their effectiveness for large ai planning problems is questionable we present an algorithm  called structured policy iteration  spi   that constructs optimal policies without explicit enumeration of the state space the algorithm retains the fundamental computational steps of the commonly used modified policy iteration algorithm  but exploits the variable and prepositional independencies reflected in a temporal bayesian network represen tation of mdps the principles behind spi can be applied to any structured representation of stochastic actions  pobcies and value functions  and the algorithm itself can be used in conjunction with recent approximation methods 
1 	introduction 
increasingly research in planning has been directed towards problems in which the initial conditions and the effects of actions are not known with certainty  and in which multiple potentially conflicting objectives must be traded against one another to determine optimal courses of action for this reason  there has been much interest in decision theoretic planning  dean and wellman 1  in particular  the theory of markov decision processes  mdps  has found considerable popularity recently both as a conceptual and computational model for dtp  dean et al 1  boutilier and dearden 1  tash and russell 1  
　while mdps provide firm semantic foundations for much of dtp  the question of their computational utility for ai remains many robust methods for optimal policy construction have been developed in the operations research  or  community  but most of these methods require expbcit enumeration of the underlying state space of the planning problem  which grows exponentially with the number of variables relevant to the problem at hand this severely affects the performance of these methods  the storage required to represent the problem  and the amount of effort required by the user to specify the problem. much emphasis in dtp research has been placed on the issue of speeding up computation  and several solutions proposed  including local search methods  dean et al 
1 
1  dearden and bouuher 1  barto  bradtke and singh 1  tash and russell 1  or reducing the state space via abstraction  boutilier and dearden 1  both approaches reduce the state space in a way that allows mdp solution techniques to be used  and generate approximately optimal solutions  whose accuracy can sometimes be bounded a prion  boutilier and dearden 1   while approximation is no doubt crucial  two questions remain a  what if optimal solutions are required'1 b  what if the state space reduction afforded by these methods is not great enough to admit feasible solution  
　the approach we propose is orthogonal to the approximation techniques mentioned above it is based on a structured representation of the domain that allows the exploitation of regularities and independencies in the domain to reduce the  effective  state space this reduction has an immediate effect on the computation of the solution  the storage required and on the effort requrred to specify the problem the approach has the following benefits 
  it computes an optimal  rather than an approximate so-lution thus  it can be applied in instances where optimal  ty is strictly required 
  it employs representations of actions and uncertainty that are well known in the ai literature 
  it is orthogonal to  and can be used in conjunction with  many ofthe approximation techniques mentioned above 
this third point is especially significant because approximation methods such as abstraction often require that one optimally solve a smaller problem. 
　in this paper  we describe our investigations of a commonly used algorithm from or called modified policy iteration  mpi   puterman and shin 1  we present a new algorithm called structured policy iteration  spi  which uses the same computational mechanism as mpi as in  boutilier and dearden 1   we assume a compact representation of an mdp  in this case using a  two-slice  temporal bayesian network  dean and kanazawa 1  darwiche and goldszmidt 1  to represent the dependence between variables before and after the occurrence of an action in addition  we use a structured decision tree representation of the conditional probability matrices quantifying the network to exploit  propositional  independence  that is  independence given a particular variable assignment  see also  smith  holtzman and matheson 1   propositional independence is reflected in the specific quantification of the network  in contrast to the van-

able independence captured by the network structure such representations allow problems to be specified in a natural and concise fashion  and they have the added advantage of allowing problem structure to be easily identified 
　using this representation  we can exploit the structure and regularities of a domain in order to obviate explicit slate space enumeration roughly  at any point in our computation  states are partitioned in two distinct ways those states assigned the same action by the 'current  policy are grouped together  forming one partition of the state space  and those state whose 'current  estimated value is the same are grouped  forming a second partition mpi-style computations can be performed  but need only be considered once for each partition  rather than for each state the motivation for our method is similar to that underlying bayes nets and influence diagrams namely that many problems seem to exhibit tremendous structure just as network algorithms have proven practical for reasoning under uncertainty  we expect spi to be quite useful in practice 
　in section 1 we briefly describe mdps and the mpi algorithm  we refer to  puterman 1  for a more detailed description of mdps and solution techniques in section 1 we discuss our representation of mdps using decision trees  and in section 1 we describe the structured policy iteration algorithm. the two phases of the algorithm  structured successive approximation and structured policy improvement are described individually we illustrate the algorithm on a detailed example  and describe the results of our implementation we refer to the full paper  bouulier  dearden and goldszmidt 
1  for a much more detailed descnphon of the algorithm and implementation  and discussion of additional issues 
1 	modified policy iteration 
we assume a dtp problem can be modeled as a completely observable mdp we assume a finite set of states s and actions a  and a reward function r while an action takes an agent from one state to another the effects of actions cannot be predicted with certainty  hence we wnte pr s   a  1  to denote the probability that s1 is reached given that action a is performed in state si these transition probabilities can be encoded in an  matrix for each action complete observability entails that the agent always knows what state it is in we assume a bounded  real-valued reward function r with r{s  denoting the  immediate  utility of being in state s for our purposes an mdp consists of 1  a r and the set of transition distributions 
	a plan or policy is a mapping 	where 
denotes the action an agent will perform whenever it is in stale  given an mdp  an agent ought to adopt an optimal policy that maximizes the expected rewards accumulated as it performs the specified actions we concentrate here on discounted infinite horizon problems the current value of future rewards is discounted by some factor  and 
   'the analogy in fact is quite strong tatman and shachter  1  have shown that influence diagram methods perform dynamic programming steps on mdp problems in a way that  compacts  the state space somewhat. however their method is restricted to ftnicehonzon problems  and adopts value iteration  which converges much 1 slowly on the infinite  or indefinite  bonzon problems frequently encountered in planning  puterman 1  
   1thus we restnct attention to stationary policies. for the problems we consider optimal stationary policies always exisl 
we want to maximize the expected accumulated discounted rewards over an infinite time period the expected value of a fixed policy at any given stale is given by 
the value of n at any initial state 	can be computed by 
solving this system of linear equations a policy 	is optimal 
if 	for all a 	s and policies 
　howard's  1  policy uerabon algorithm constructs an optimal policy by improving the  current   initially random  policy by finding for each state some action better than the current action for that state each iteration of the algorithm involves two steps  policy evaluation and policy imprvvemenr 
1 for each s 	1  compute 
1 for each e e s find the action a that maximizes 

the algorithm iterates on each new policy until no improvement is found the algorithm will terminate with an optimal policy and in practice tends to converge in a reasonable number of iterations 
　policy evaluation requires the solution of a set of  s  linear equations in  s  unknowns this can be computationally prohibihve for very large stale spaces however  one can estimate through several steps of successive approximation is approximated by a sequence of vectors   each a successively better estimate the initial estimate is any random  s  vector the estimate  a  is given by 
modified policy iteration  puterman and stun 1  uses some number of successive approximation steps to produce an estimate of  at step 1 we refer to  puterman 1  for theoretical and practical advice for choice of good stopping cntena mpi is used frequently in practice for large state space problems with good results  puterman 1  
1 representation of m d p s 
it is unreasonable to expect that dtp problems while formulable as mdps  will be specified in the manner described above since stale spaces grow exponentially wiih the number of propositions relevant to a problem  one should not expect a user to provide an explicit probability matrix for each action  or a |1|-vector of immediate rewards regularities in action effects and reward structure will usually permit more natural and concise representations 
　we will illustrate our representational methodology  and algorithm  on a simple example a robot is charged with the task of going to a cafe to buy coffee and delivering it to a user in their office it may rain on the way and the robot will get wet unless it has an umbrella. we have six propositions  hence1stateb -xgocationofrobot l at office  at cafe   
w  robot is wet   u  robot has umbrella   r  raining   hcr 
 robot has coffee   hcu  user has coffee  - and four 
boutilier dearden  and goldszmidt 


figure 1 action network for delc 
go  to opposite location  buyc  buy coffee   delc  deliver coffee to user   getu  get umbrella  each of these actions has the obvious effect on a state  but may fail with some probability  see  boutilier dearden and goldszmidt 1  for a full problem specification  
　we discuss one possible representation for actions and uhli ties  bayesuan networks  and in the next secoon show how this 
information can be exploited in mpi while our algorithm depends on the particular representation given  the nature of our method does not and could be used with  say the probabilis-
tic strips representation of  boutilier and dearden 1  kushmenck  hanks and weld 1  
　we assume a set v of atomic propositions characterizing the relevant features of our domain because of the markov assumption  the effect of a given action a is completely determined by the current state of the world  and can be represented by a  two-slice  temporal bayes net  dean and kanazawa 1 darwiche and goldszmidt 1  we have one set of nodes representing the state of the world prior to the action  one node for each .  another set representing the world after the action has been performed and directed arcs representing causal influences between these sets 1 figure 1 illustrates this network representation for the action delc  deliver coffee   we will have one such network for each action 
　the post-action nodes have the usual matrices describing the probability of their values given the values of their parents  under action a we assume that these conditional probability matrices are represented using a decision tree  or if-then rules   smith holtzman and matheson 1  this allows independence among variable assignments to be represented  not just variable independence  as captured by the network structure   and is exploited to great effect below a tree representation of the matrices for variables hcu and w in the delc network is shown in figure 1 along with the induced matrix  our convention is to use left arrows for  true  and right-arrows for  false   each branch thus determines a partial assignment to the parents of that variable  in the network  with some parents unmenboned the leaf at each branch denotes the probability of the variable being true after the action is executed given any conditions consistent with that branch 
in this case when and hcr hold pnor to the action the tree associated with proposition p m the 
　　to simplify the exposiuon we only consider binary variables and assume that there are no arcs between post action variables relaxing these assumptions  as long as the network: is acyclic  docs not complicate our algorithm in any essentia  ways 

figure 1 reward function network 
network for an action a is denoted since we are interested only in the transition probabilities. for a known state s1  we do not require pnor probabilities  or matrices  for pre-action variables  the roots of the network  
　we note that many actions affect only a small number of variables  to ease the burden on the user  we allow unaffected variables to be left out of the network specification for that action persistence is assumed and the arcs  indicated by broken arrows  and trees can be constructed automatically for instance  all of l  r  w and u are unaffected by the action delc it is easy to see how such an action representation induces a transition matrix over the state space 
　we assume that the immediate reward r is solely a function of the state of the world as such  we can use a simple  atemporai influence diagram  to capture the regularities in such a function since  immediate  reward is independent of stage and the action performed  we need only one network to capture reward figure 1 illustrates such a network only variables that influence reward need be specified- one may also use a tree-structured representation for r as shown  where leaves now indicate the reward associated with any state consistent with the branch  it is easy to see how such a tree determines the reward funcuoni r s  in this example the robot gets a reward of o 1 if the user has coffee and 1 if it stays dry which are added to determine r s  
1 	structured policy iteration 
given a network formulanon of an mdp one might compute an optimal policy by constructing the appropriate transition matrices and reward vector  and solving with standard techniques but as the number of propositions increase  stale spaces grow exponentially  and these methods quickly become infeasible in addition  although these algorithms may converge in relatively few iterations  memory requirements are quite intensive 1 if a problem can be represented compactly  the representation must exploit certain regularities and structure in the problem domain therefore one can often expect that optimal policies themselves have certain structure  as do value functions vw the optimal policy for our example problem can be expressed quite concisely for example  delc is the best action whenever hcr and l are both true  regardless of the truth values of the other four variables thus by associating the action delc with the proposition   we capture the policy for 1 states with one assertion 
　we propose a method for optimal policy construction that eliminates the need to construct explicit transition matrices  reward and value vectors  and policy vectors our method is based on mpi  but exploits the fact that at any stage in the 
   1 putennan  1  describes this as a potential bottleneck  our previous experiences also suggests this is often problematic computation  a  the current policy may be structured  and  b  the value function for a policy or some estimate thereof  may be structured rather than having a policy vector of size |s| individually associating an action with each state  one can use a structured representation 
definition a structured policy is any set of formula-action pairs such that the set of propositions  formulae partitions the state space this induces ihe  explicit  policy 
structured policies can be represented in many ways  e g   with decision lists  rivest 1   we adopt a decision tree representation similar to the representation of probability ma tnces above leaves are labeled with the action to be performed given the partial assignment corresponding to the branch thus  if there are k leaf nodes  the state space is partitioned into k subsets or clusters figure 1  the second tree  illustrates a structured policy we have 1 clusters and 1 action assignments rather than one action assignment for each of 1 states1 
　a structured value vector can be represented similarly as a set of formula-value pairs where states satisfying  have value again  we will use a tree-structured representation for such a para don in this case  each leaf is annotated with the value associated with that partition  see 
figure 1  
the insights crucial to our algorithm are the following 
 a  if we have structured policy and a structured value estimate for an improved estimate  can 
often preserve much of this structure 
 b  if we have a structured value estimate we can construct a structured improving policy 
the first observation suggests a structured form of successive approximation  while the second suggests that one can improve a policy in a way that exploits structure this gives nse to the spi algorithm for structured policy iteration 
1 choose a random structured policy 	ihen loop through 1 
1 approximate value function 	using structured successive apprvjamnhon 
1 produce an improved structured policy   if no improvement u possible terminate  
we describe these components of the algorithm in turn below bubal structured policy selection is fairly unconstrained 
in the example below  we adopt the greedy  one-step  policy 
  deliver coffee no matter what the state  simpler policies should be preferred 
1 	structured successive approximation 
phase 1 of each iteration of spi invokes structured successive approximation  ssa  we assume we have been given a structured policy and an initial structured estimate of that policy's value during the first iteration of spi  ssa may use the immediate reward tree as its initial structured estimate in subsequent iterations the initial estimate is the computed value tree for the previous policy 
   1we note thai tree representations of policies are someumes used in reinforcement learning as well  chapman and kaelbling 1  however  the monvanon there is somewhat different- in addiuon  the ordering of variables in ihe tree can have a dramaiic impact on the me of the representation  sec section 1  
　given a policy we wish to determine its value the basic step of successive approximation involves producing a more accurate estimate of a policy's value given some previous estimate using equation 1 successively better estimates are produced until the difference between and  is  componentwise  below some threshold.1 
　ssa embodies the intuition that  given a structured value vector the conditions under which two states can have different values can be readily determined from the action representation in particular  although an action may have different effects at two states if this difference is only in variables or variable assignments not relevant to the structured value vector   then these stales must have identical values in since is tree-structured  the ssa algorithm can easily determine what assignments are relevant to value at stage i the crucial feature of the algorithm is its use of the acdon representation to cluster the states    form a new tree under which the policy must have the same value at stage by doing so  we can c a l c u l a t e o n c e for each 
leaf of the tree rather than for each state in the stale space this may have a signifj cant impact on both tune and memory requirements in many cases 
　we first describe the mam loop of the ssa assuming a single action  uniform policy  to be executed  see algorithm explain tree m figure 1   and give a detailed example we then describe how general policies are dealt wim 
　we accept a structured value vector tree   the current estimated value for the current policy and an acnod a to be performed at stage t  as if we were computing in eq 1  given an action a to be performed  the states that can have different values are diose that lead  under action a  to different partitions of tree with different probability roughly tree describes not only what is relevant to the value of the policy  executed for i stages   but also how its value depends on  or is independent of  the particular variable assignments in the tree to generate the tree we want to explain the partitions in that is  we want to generate the conditions that  if known pnor to the action  would cause a state transition with some fixed probability to fixed partitions  or leaves  of tree 
　since the probability of reaching a given partition in is a function of the probabilities of the individual variables on its branch  we can build explanation componentwise 
   1betier stopping cntem are possible  puterman 1  but have no bearing on our algorithm 
boutilier  dearden  and goldszmidt 
we consider the variables in tree{v'  individually more precisely explanations are generated by a process we call abducnve repaninoning  quite similar in spirit to probabilistic horn abduction  poole 1  a given traversal of tree vl  induces an ordering of relevant  post-action  variables  we  explain  variables in tree v'  according to this order  step 1 of figure 1  
   for each variable x in tne vl   the conditions under which the probability of x vanes when action a is executed 1 given by which is simply read from the network for a  see step 1b 1  hence explanation generation for the individual variables is trivial - an explanation consists of the tree whose branches are partial truth assignments and whose leaves reflect the probability that the variable becomes true this explanation must be added to the current  partial  tree for   step 1b 1  however  it need not be added to the end of every partial branch  asserts the conditions under which x is relevant to value  the explanation for x need only be added to the leaves where those conditions are possible  steps 1a and 1b  since the tree is generated in the order dictated by  the probabilities of the relevant variables are already on the leaves of the partial tree once is added to the required leaves the new leaves of 
the tree now have pr x  attached in addition to the probabilities of the previous variables  step 1b 1   and these can be used to determine where the explanation for the next vanable must be placed should the variable labeling a node of tree x a  occur earlier in the partial tree for v   + '  that node in tree x a  can be deleted  since the assignments to that node in  must be either redundant or inconsistent-
step 1b 1  thus  much shrinkage is possible  see below 1 
　figure 1 illustrates the value trees that result for two successive approximation steps  and as well as the fiftieth step in our example  using the initial policy delc the generation of  is straightforward the first 
relevant variable hcu will have different outcome probabilities as dictated by tree{hcv delc  in figure 1 this requires the addition of variables  the other relevant variable w has its tree added to each of the leaves of tree hcu  delc   1ince it is relevant no matter how hcu turns out. this results in the tree structure shown in figure 1 labeled 
　more interesting is the generation of '.  using  illustrated in figure 1  which we now describe in some detail the four variables in are ordered hcu  l  hcr  w we start by inserting  slagel  which explains hcu the probability of hcu given the relevant assignment labels the leaf nodes next variable l is  explained  by from  we notice that l is only relevant when is false therefore  we only add l to those leaves where 
 we notice that such leaves only exist below the node l in our partial tree we also notice that  contains only the variable l  by persistence   
thus  no additional branches need to be added to the tree  any 
　　in general  one has to consider also the impact of the immediate reward function  whose tree can be incorporated into the tree  see step 1   however this is often unnecessary when the reward function is used as an initial value estimate 
   1wc don't use    is relevant to value whenever is less than certain 
further partitioning on lis either redundant or inconsistent  
the net result is the simple addition of a probability label for l on these leaves  see stage1  in general  for more complicated trees  we will add a tree of the form to a leaf node  and eliminate some  but perhaps not all  of its nodes 
　the next variable is hcr  which is only relevant when and l bold  and tree hcr delc  is added only at leaves where 	however as with l  the addinon of i  containing only the variable 
hcr  is redundant since leaves satisfying this condition he below node hcr is die partial tree  see stage1  finally  the variable w must be explained w is relevant at all points in the partial tree  so its is added to each leaf node  stage1  
　finally  with the probability labels the value tree . the reward tree  and the discounting factor  here the leaf nodes can be labeled with the values for v1  figure 1  in this example  abductive repartitioning gives rise to six distinct values in the estimates and our mechanism generates a tire with eight leaves  but two pairs of these can be identified as necessarily having the same value  indicated by the broken ovals   see section 1 thus  in principle  only six value computations need be performed rather than 1 
　to deal with general policies instead of single actions  ss a proceeds as follows we assume the policy is represented structurally as  for each action a that occurs in the explanation algorithm is run  as above the nee generated for an action a is then appended to the leaves of  at which a occurs since may make certain distincuons that occur in the appended 'acuon trees   we delete any redundant nodes to simplify the tree  either during or after its construction  
　the ssa algorithm requires some number of rounds of successive approximation before a reasonable estimate of the policy's value can be determined and die policy improvement phase can be invoked while the number of backups per step can potentially be reduced exponentially  there may be considerable overhead involved m determining the appropriate partitions  or me reduced  state space   we first note that the reduction will often be worth the additional computation  especially as state spaces become  realistically  large - even as domains increase  the effects of actions may be quite localized in many problem settings we can expect this reduction for a particular policy to be quite valuable in addition  and more importantly  this reparti honing need not be performed at each successive approximation step as the following theorem indicates  once the partition stabilizes for two successive approximations  it cannot change subsequently 
　thus  the backups for successive estimates can proceed without any repartitioning essentially  the very same computations are performed for each partition as are performed for each state in  unstructured  sa  with no additional overhead in our example  we reach such convergence quickly when repartitioning to to compute we discover that the partition is unchanged the tree-representation of  is identical to that for  thus  after one repartitoning the partition of our value vector has stabilized and backups can proceed apace 


figure 1 fifty iterations of ssa 

figure 1 generation of abductively repamnoned vector 

the value vector v1 contains the values that are within 1% of the true value vector   as shown in figure 1 
　it is important to note that while the value vector approximates the true value function for the specified policy  the approximation is an inherent part of the policy evaluation algorithm no loss of information results from our partitioning the structured value estimates v are the same as those generated in the classic successive approximation  or mpi  algorithm they are simply expressed more compactly 
1 	structured policy improvement 
when we enter phase 1 of the spi algorithm  we have a current structured policy  and a structured value vector  the policy improvement phase of mpi requires that we determine possible  local  improvements in the policy - for each state we determine the action thai maximizes the one-step expected reward  using our value estimate as a terminal reward function  that is  the a that maximizes should a be different from we replace by a in our new policy 
　once again  we want to exploit the structure in the network to avoid explicit calculation of all |s||a| values while there are several ways one might approach this problem  one rather simple method is based on the observation made above for any fixed structured value vector and action 
a we can determine the value vector 	using the algorithm 
explain described above abducuve repartiboning is used to identify the relevant pre-action conditions that influence this outcome  and provides us with a new partition of the state space for a  dividing the space into clusters of states whose value is identical we determine one such partitioning of the state space for each action a and compute the value of a for each partition 
　figure 1 illustrates the value trees generated by the abduc tive repatationg scheme for the two actions delc  the tree v1 from figure 1  and go the values labeling the leaves indicate the values in the policy improvement phase of the algorithm - these are determined by using the probability labels generated by abduebve reparunoning and the tree for   we ignore actions buyc and geiu which generate similar trees to delc but which are dominated by delc at this stage of the algorithm   we note that these values are undiscounted and do not reflect immediate reward with out action costs these factors cannot cause a change in the relative magnitude of total reward for an action thus  actions need only be compared for expected .fature rewards  although action costs are easily incorporated 1 
　with these values we must now determine a new locally improved policy choosing either go or delc for each state given mat the expected value of the action only vanes between partitions  we can use the trees to quickly determine which action is best in each partition in the worst case  where the partitions are orthogonal  we must consider their cros1-product here however  the trees share much structure our current algorithm performs a reasonably straightforward merging of the trees in question  keeping the coarsest partitions possible to produce a tree with the dominating values and action choices the result of this process is illustrated in figure 1 which shows the maximally improved policy while there are a number of methods for merging the trees for each action  our current implementation reorders the trees so that the variable ordenngs are consistent  with the current value 
　　p we should point out that while tree{go  has 1 value partitions  in fact  there are only 1 distinct values among these twenty moreover these coincident values are due to structural properties of the problem and can be identified beforehand as necessarily having the same value for legibility we ornii the ovals that join these leaves with the same value 
boutilier  dearden and goldszmidt 

	action co 	+ 	action imc ＊ m u k u l y improved policy 
figure 1 value trees for candidate actions and the improved policy 

ordering if feasible   this allows a straightforward comparison of leaves to determine dominance and the structure of the new policy tree 
　we note that if both go and delc provide equal value at a given state  the action of the earlier policy is chosen for that slate  as is usual in policy iteration thus some of the states where delc is chosen may have had equal value if go had been chosen  but the persistence condition dictates our choice of delc the net effect is often a simpler policy tree it also allows the easier detection of the termination condition 
1 	analysis 
the final policy produced by the spi algorithm  1 e  some number of ssa and improvement steps  is shown in figure 1 along with the value function it produces this policy is produced on the fourth iteration a fifth policy improvement step is attempted but no improvement is possible  note that for this fifth iteration no value approximation of the policy's value is performed  thus  the state space is partitioned into 1 clusters allowing a very compact specification of the policy the value tree corresponds to 1 backups of successive approximation since all value trees for prior iterations are  strictly  smaller than this  we see thai we had to consider at most 1 distinct  states  at any given time  the first three policies peaked at 1  1 and 1 partitions   rather than the 1 slates of the original state space in fact  the 1 sets of duplicate values are easily show to have exactly the same value because of structural properties of the problem though we have not yet implemented the algorithm to take advantage of this property  in principle we could use at most 1 clue ten and compute at most 1 value estimates instead of 1 
　the intent of this paper is to suggest methods by which structure implicit in problem representations can be imposed on policy and value vectors in policy construction clearly we cannot expect spi or related methods to work well on all mdps  for not all problems have compact representations that can be exploited  or even if a compact representation is possible  the optimal policy may not be compact. the most we might hope for is that a problem witii a compact description  input  and optimal policy  output  can be computed efficiently using spi  e g   in polyume in the size of the input/output  unfortunately  even this cannot be guaranteed consider the following example  whose network is diagramed in figure 1 we assume n propositions p  and actions  action a  will cause p  to become true as 
learning 

figure 1 description of action a  
long as are all true but it has the side effect of setting each of false each of the n actions can be represented in 1 n  space our reward function assigns 1 to the state making all p  true and 1 to all other states thus the problem is representable in space with a discounting rate the optimal policy has die form do a   else if .   which in tree form requires 
space but the value is different at each state e since spi makes all distinctions relevant to value  it must produce a complete tree of size   requiring tune exponential m 
the problem description and solution 1 
　our initial experiments have provided some suggestions about the types of problems on which spi should work well and where further optimizations can be made not surprisingly spi spends relatively more time on policy improvement vs evaluation compared to mpi this is due to the overhead involved in merging action trees good tree manipulation algorithms will play a role here the evaluation phase of spi compares favorably to mpi even when overhead is accounted for when many iterations of successive approximation are needed  the overhead is amortized  due to theorem 1  and 
spi outperforms mpi considerably 
　naturally  as trees become larger the overhead of spi becomes a crucial factor our experience with small  1 state  problems suggests that spi requires problems whose structure allows  the equivalent of  deletion of roughly 1 variables  i e  overhead appears to be proportional to tree-
　l1essenqally the optimal policy winds through the stale space like a binary counter vt a  could be compactly encoded with an appropriate of  parameterized  metric representation 

figure 1 the optimal policy and value function size   further experimentation with large problems is necessary to venfy this conjecture the performance of spi with relatively small trees suggests that a hybrid approach  using spi and switching to mpi if trees become too large  may work well in practice in other words  we can use spi until the effort required to discover irrelevant distinctions ceases to pay off as a general remark goal based problems with competing objectives seem to be especially well-suited to spi  as opposed to process-onented problems  since choosing a particular goal to pursue tends to render features related to other objectives irrelevant 
1 	concluding remarks 
we have presented a natural representational methodology for mdps that lays bare much of a problem's structure  and have presented an algonthm that exploits this structure in the construction of optima  policies the key component of our algonthm uses an abductive mechanism that generates partitions of the state space that  at any point in the computation  group together states with the same estimated value or best action this allows the computation of value estimates and best actions to be performed for partitions as a whole rather than for individual states this work contnbutes both to al  a spe 
cific dtp algonthm  and or  a representational methodology and clustenng technique for mdps  
　currently we are investigating a number of extensions to this model our experimental results point out two important bottlenecks the first involves multivalued variables which if relevant  cause branching on all values in many domains features are relevant if a vanable has one value  but not oth ers using decision lists  or hybnds  to quantify bayes nets will help in this regard  they can be used also to represent policies and value vectors the second involves the ordering of variables in trees good heunsti.es may help keep the size of policy and value trees close to optimal related is the use of acyclic graph representations instead of trees 
　one of the most promising aspects of this work is the fact that it provides structures that should help in approximating optimal policies the conditional relevance of variables can be quantified and trees can be pruned by deleting nodes having the least impact on value  even at intermediate stages in this way  abstraction methods such as those of  boutlier and dearden 1  can be made. far more  adaptive   
acknowledgements thanks to tom dean  david poole and marty puterman and an anonymous reviewer for discussions and comments on this paper this research was supported by nserc research grant ogp1 nce iris-d project ic-1 and rockwell international the first two authors also acknowledge the kind support of rockwell international 
