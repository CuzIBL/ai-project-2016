 
in this paper we propose the framework of monte carlo algorithms as a useful one to analyze ensemble learning. in particular  this framework allows one to guess when bagging will be useful  explains why increasing the margin improves performances  and suggests a new way of performing ensemble learning and error estimation. 
1 introduction 
ensemble learning aims at combining learned hypotheses for classification tasks instead of choosing one. it has proved to be a valuable learning approach  and it is actively investigated in the machine learning community  freund 1; freund & schapire 1; breiman  1; wolpert  i1|. 
in addition to the advantages pointed out by dietterich   what really attracts the great interest to ensemble learning is its amazing effectiveness and robustness to overfitting. in fact  ensemble classifiers  and specifically adaboost  often show a drop in generalization error far beyond the point in which the training error reaches zero. 
in this paper wc concentrate on two approaches  namely bagging  breiman  1   and boosting  as implemented in the adaboost algorithm  freund & schapire  1   and we propose an analysis of these two approaches by exploiting links with monte carlo theory. this parallel allows a deeper understanding of the basic process underlying hypothesis combination. in particular: 
  it suggests the conditions under which bagging is likely to be better than single hypothesis learning  
  it explains why reducing the margin  schapire et ai  1  increases the performances  
  it allows some reasons for robustness to overfitting to be conjectured. 
the article is organized as follows: in section 1 we briefly review bagging and adaboost algorithms  and in section 1 we introduce basic terminology and results 
learning 
lorenza saitta universita del piemonte orientale 
 spalto marengo 1  alessandria  italy saitta mfn.unipmn.it from monte carlo theory. section 1 is the core of the paper: it introduces the link between monte carlo theory and ensemble learning algorithms  and analyzes the two algorithms in the ideal situation of having access to the whole example and hypothesis spaces. section 1 relaxes the hypotheses of section 1. in particular  it focuses on adaboost's case offering an intuitive accounting of the generalization performances of the algorithm. finally  section 1 concludes the paper and proposes future work. 
1 bagging and adaboost 
bagging  breiman  1  is one among the most studied ensemble learning algorithms. it takes in input a learning set lo  an integer number t  and a  possibly weak  induction algorithm a. at each iteration step t the algorithm creates a bootstrap replicate l  efron & tibshirani  1  of the original learning set  and uses it in conjunction with a to form a new hypothesis ht. when t hypotheses have been learned  they are combined using a simple majority voting rule. 
breiman  found a significant increase of classification accuracy  when the hypotheses are bagged  on different datascts from the uci repository  for both regression and classification tasks. moreover  in the same paper  a theoretical analysis is carried on. the notion of order correct classifier is introduced  and it is shown that when a is order correct  then bagging is expected to behave almost optimally  i.e.  it behaves like the bayes classifier . throughout the paper it is claimed that  bag-
ging stable classifiers is not a good idea . a  stable  classifier is qualitatively defined as one that does not show large variations in the learned hypotheses when the training set undergoes small variations. experimental analysis of bagging's performances is provided by bauer and kohavi   quinlan   and dietterich  1|. 
the simplest version of adaboost  freund & schapire  
1  deals with binary classification problems  and this is the one we refer to. adaboost maintains a weight distribution over the training examples  and adjusts it at each iteration. the adjusted weights are chosen so that 
1 
higher values are associated to previously misclassified examples. as a main effect  the weighting scheme forces a to focus on  more difficult  examples  ensuring thus that all the examples will be  sooner or later  taken into account by the weak learner. the hypothesis combination rule is a weighted majority vote that puts more weight on successful hypotheses  i.e.  hypotheses that incur a low weighted training error . 
despite its apparent simplicity adaboost solves a difficult problem: choosing example and hypothesis weights in such a way to ensure an exponential decrease in the training error committed by the boosted committee. moreover  the choice is made without any knowledge or assumption on the weak learner itself  except that the weak learner is always able to outperform random guessing . in contrast  bagging only works when  benign  weak learners are used. on the other hand  adaboost seems to be more sensitive to noise than bagging  quinlan  1 . 
as mentioned earlier  adaboost shows the characteristic of being robust with respect to ovcrfitti.ig  schapire et al.  1; drucker & cortes  1; quinlan  1; breiman  1; freund & schapire  1 . a rather convincing account of this behavior imputes to adaboost the ability to be very successful in increasing the final classifier's margin over the training examples  schapire e t a l   1 . 
in the effort of understanding and/or explaining adaboosfs behavior  boosting has been related to other existing theories  such as game theory  schapire  1   logistic regression  friedman et al.  1; collins et al.  1   optimization  ratsch et al.  1   and brownian motion  freund  1 . 
1 monte carlo algorithms 
in this section we recall some basic notions from monte 
carlo algorithms theory. in the literature  the name monte carlo often denotes a generic stochastic algorithm; in this paper we will refer to the more restrictive definition given by brassard and bratley . let be a class of problems  y a finite set of answers for the problems in  be a function that maps each problem into the set of its correct answers. 
definition / - 	a stochastic algorithm a  mapping  into 
y  is said to be a monte carlo algorithm  if  when applied to an instance of it always terminates  providing an answer to   which may be occasionally incorrect. 
two properties of monte carlo algorithms turn out to be relevant: 
definition 1 - a monte carlo algorithm is consistent if it never outputs two distinct correct answers when run two times on the same problem instance 
definition 1 - a monte carlo algorithm is p-correct if the probability that it gives a correct answer to  is at least p  independently of the specific problem instance n considered. 
the great interest in monte carlo algorithms resides in their amplification ability: given a problem instance and a monte carlo algorithm a/c  if mc is run multiple times on  and the majority answer is taken as the result  then  under mild assumptions  the probability of a correct answer exponentially increases. the conditions arc that mc must be consistent for and that p must be greater than random guess. we note that  whenever reduces to a mapping from to y  instead of 1    the algorithm is consistent. let us denote by the algorithm obtained by combining with a majority vote the answers given by multiple runs of a monte carlo mc. 
1 complete information case 
in this section we provide a theoretical account of the links between monte carlo theory and bagging or boosting  in the case we have complete information on the learning/classification problem. more precisely  let x be a finite set of examples1  a target concept  binary classification   a a  weak  learner. and w be a probability distribution over x. let moreover be the power set of x. any intensional concept description  belonging to a given language will be mapped to an element i   of .   i.e.  is the set of examples classified by that description as belonging to co. in this way  we can reduce ourselves to a finite set of  extensional  hypotheses. let  be any subset of .a probability distribution q is associated to the elements of . the set 'can be thought of as the collection of the hypotheses generated by a learning algorithm a applied to a number of training sets  or a set of hypotheses provided by an oracle. learning can then be simulated by extracting with replacement elements from  according to q  each extraction corresponds to a run of 
a on some training set . 
let us represent the described situation by means of table 1. here  let be the probability that hypothesis correctly classifies e x e the average of such probabilities for curacy of hypothesis  over the whole x. by using the given probability distributions  we can compute the average accuracy for the whole table: 
		 1  
 the finiteness of x is not essential; it is only assumed for the sake of simplicity. 
1  this assumption implies that the bayes error is zero. 
learning 

table 1 - theoretical setting 

given a subset z of x   the measure of z w.r.t. w  
i.e.: 
analogously  if 
clearly the measures 	reduce to  and 
  respectively  if q and w are uniform distributions. 
1 monte carlo and bagging 
we will now establish a link between monte carlo algorithms and bagging. let l1 be a training set  and t an integer; let i be a generic bootstrap replicate of l1. in this case   is the set of all the different hypotheses that can be obtained by applying a to .  replicates. the frequencies with which each is generated determine q. 
if we look at table 1  bagging proceeds  so to say  by columns  by extracting t columns from and using them to obtain a majority vote on each considering the global average accuracy r  or even the exact distribution of the rj it is not easy to say how many of the  will be correctly classified by the majority voting procedure  and  then  whether to perform bagging will be rewarding. it is generally said  breiman  1  that bagging is useful when a is  unstable . we will show later that this notion of  instability  can be made more precise. 
let us consider now a monte carlo interpretation of table 1. to the contrary of bagging  a monte carlo algorithm    proceeds by rows: for e a c h   p r o b l e m to be solved   it extracts t hypotheses from and classifies xk by majority voting. this process is repeated for each row . this procedure is necessary  because the components of the monte carlo voting must be independent  but the entries in each column are not. monte carlo theory tells us that the correctness of  can be amplified if: 
1. the t trials are independent 
1. mc is consistent 
	1. 	
condition  1  is true by construction of the process  extraction with replacement of t hypotheses for each row . 
1  let us set apart  for the moment  the computational issues. 
condition  1  is true as long as the bayes error is zero. 
condition  1  may or may not be true  depending on the set 
let  be the set of examples for which  according to the monte carlo theory  all these examples will be  amplified   i.e.  they will be correctly classified in the limit . the application of  to all the rows yields an asymptotic accuracy: 
		 1  
by denoting by pt the expected monte carlo accuracy when only a finite number t of extractions is performed  we obtain: 
 1  
let now greater than 
 1  
is the probability that a hypothesis  better  than bagging is obtained in a single extraction. then  in an infinite number of trials  such a hypothesis will occur with a frequency  it is clear that  in the limit  bagging is surely convenient only when when  because  in this case  there is no single hypothesis better than bagging. when  a better hypothesis will surely appear  soon or later. the interesting case  however  occurs when only a finite number t of extractions is performed. before approaching this case  let us make some considerations on what has been described so far. 
 let us first notice that ensemble learning claims that amplification occurs when the rj  accuracy of each of the combined hypotheses  are greater than 1  condition on the columns . actually it may not be the case. what is really required is that  for each example   more than 1 of the learned hypotheses are correct on it  condition on the rows . then  monte carlo theory suggests that bagging would be successful when it is possible to find a subset of  such that . before looking with more details into the relationship between the rjs and the  let us consider some extreme cases. 
let a be a weak learner that always returns the same hypothesis  regardless of the training set. the lucky situation in which that single hypothesis  is quite accurate over the training set docs not imply anything about the occurrence of monte carlo amplification. in fact  all the  are equal to 1 on those examples which arc correctly classified by and 1 otherwise. in other words  whatever the accuracy of no amplification at all occurs. this is an example of a perfectly stable learner. 
learning 	1 let us consider a situation in which the global average r of the accuracy is a little more than 1. this value is at the same time the average of the last column and on the last row in table 1  according to  1 . for the sake of illustration  let q and w be uniform distributions. let us consider the following three extreme situations  a de-
tailed analysis of several other interesting cases is provided by esposito   : 
 a  the  in the last column are distributed in such a way that one half of the examples have  values close to one  and one half of the examples have  values close to zero. the rjs in the last row  on the contrary  have all almost the same value around 1.  all hypotheses show the same performances and they cover almost the same exam-
ples  
 b  the  in the last column have all almost the same value around 1  whereas the rjs in the last row have  for one half  values close to one  and for the other half close to zero.  some hypotheses are very good and other are very bad  but their coverage is uniformly distributed on the examples . 
 c  both distributions are uniformly valued around 1  all the hypotheses are uncertain on all examples . 
in case  a  half of the examples arc amplifiable  but bagging is not useful  because any single hypothesis classifies correctly half of the examples  so that nothing can be gained by the bagging process. 
in case  b   the number of amplifiable examples depends on how many of them have actually values a little over 
1. but  even in the best case   bagging is again not convenient. in fact  half of the single hypotheses have r  = 1; then  in one over two extractions  on average  a perfect hypothesis is extracted. in case  c   if the values in the last columns arc all a little greater than 1  then this is the case in which bagging is convenient  because the single hypothesis have all accuracy close to 1. actually  case  c  is the ideal case for bagging  because amplification can be obtained with the minimum effort  with  bad  hypotheses we may obtain a large amplification . 
the following theorem summarizes the above considerations and establishes a link between bagging and 
monte carlo amplification  proof is provided by esposito  : 
theorem 1 - the amplified accuracy p cannot be greater than 1r  where r is the average accuracy of the bagged hypotheses. 	
then  even by bagging single hypotheses with accuracy less then 1 it is possible to obtain higher accuracies by monte carlo amplification. however  in order to reach p  = 1  the value of r must be at least 1. even in this case  some of the combined hypotheses are allowed to have a accuracy less than 1  without hindering amplification  provided that  and that r does not go below 1. 
1 monte carlo and adaboost 
let us notice that  if the set  of hypotheses coincides with the power set 1x  all possible hypotheses are considered   we are in the situation in which all r = 1  and   so that no amplification is possible. this means that the learning procedure was not able to make any useful selection inside the hypothesis space. on the contrary  we would like that the weak learner has the nice property of focusing on the best hypotheses. in the previous section we showed that the basic monte carlo approach  implemented by bagging  may fail due to a number of reasons; this happens  for instance  in the cases  a  and  b  of section 1. even if not explicitly stated  the underling problem is that the weak learner may not be suited enough for this kind of procedure. the question we arc now trying to investigate is what to do when such a situation occurs  in particular when we do not want or can modify the weak learner. then  is it possible to convert case  a  or  b  into case  c   or  said in other words  is it possible to convert a learning problem in which the monte carlo assumptions for amplification 
do not hold into one in which they do  
the solution to this problem can be found by thinking again of the role of the weak learner in the monte carlo process. as mentioned earlier  the learner has the duty to modify the distribution from which the hypotheses are drawn  i.e.  the qj of the columns  in such a way that 
fct
bctter  hypotheses are more likely to be drawn. moreo-
ver  we can actually change the behavior of the weak learner through its input  i.e.  by modifying the training set. in particular  we have to force the weak learner to extract hypotheses that make the minimum value of the p xij 's grow larger than 1  in order to satisfy the monte carlo conditions of amplifiability for all the examples . 
since the weak learner is influenced by the training set  the solution is to modify the training set in such a way that examples with low  values arc more represented  in the attempt to make the weak learner try harder to be successful on them. what we obtain with this reasoning is an adaboost-like algorithm. in other words  when the weak learner is unable to satisfy monte carlo conditions for amplification  adaboost forces it to satisfy them by weighting the examples. more precisely  adabost tries to increase the minimum  with the unconscious goal of extending . in fact  when the weak learner satisfies relatively mild conditions  it has been shown by schapire et al.  that adaboost is actually effective in increasing the minimal margin: 
definition 1 - for any particular example  the classification margin  is the difference between the weight assigned to the correct label and the maximal weight assigned to any single incorrect label. 
in the case of binary classification  the margin is the difference between the weight assigned to the correct label 
and the weight assigned to the incorrect one. 
theorem 1 - in the case of binary classification  the following relation holds between the margin and the monte 
carlo 
		 1  
learning 

the proof of theorem 1 derives immediately from the definition of margin and . formula  1  explains why increasing the margin improves the performances of boosting: in fact  increasing the minimum margin lets the cardinality of the set  increase  augmenting the number of examples for which the monte carlo conditions for amplifiability hold. 
1 non-asymptotic case 
let us consider a finite integer t. let denote the probability that a hypothesis  better  than bagging is obtained in r extractions. in order to compare bagging with hypothesis selection  let us consider a classification accuracy given table 1  and given a number  close to 1  let be the minimum number of components of the bagged hypothesis such that: 
		 1  
if  then probability  1  is zero. 
analogously  let be the set of hypotheses with rj   a and r a  be the minimum number of hypothesis extractions such that: 
		 1  
again  if  probability  1  is zero. 
we can draw a graph of t  ordinate  vs. r  abscissa   parameterized by  the graph is problem dependent. what we can say in general is that points in the graph above the diagonal correspond to situation for which selection is better than bagging  for the same accuracy ex  bagging requires more trials   points below the diagonal correspond to situation for which selection is worse than bagging  for the same accuracy  bagging requires less trials   and points on the diagonal correspond to situations of indifference. 
1 problems with partial information 
the setting discussed in section 1 is useful for understanding the functioning of ensemble learning  but we need to extend it to the case of real learning problems. in concrete situation  neither the set  nor the example space x is available a priori. what we have is a training set  and a test set to  both subsets of x  and a learning algorithm a for generating hypotheses. we learn hypotheses  possibly consistent with l    and estimate the generalization error on 
if we consider the problem from the monte carlo perspective and we go back to table 1  we can think to proceed as follows. at the beginning we have  on the rows  the training and the test examples  but the columns are empty. let us consider the first example let a run on t replicates of l1 and let us compute which 
1 
 in this paper we do not consider to use cross-validation or leavc-onc-out. 
is an unbiased estimate of the bernoulli probability p x/ . even though the t hypotheses generated are only used to classify x1  and could then be discarded  nothing hinders from recording them in the first t  or less  if there is duplication  columns of table 1. when we consider jr   we repeat the whole procedure  running again a on t new replicates of l1 and classifying x1 again we record the new generated hypotheses  or update the probability distribution q in case of repetitions . when we finish examining l1  we have also filled the part of the table corresponding to the test set  which allows estimates for the examples in the test set to be computed. each is based on t m experimental values  which is the number of runs necessary to estimate the generalization accuracy using the leave-one-out method on a number m of training examples. 
what is interesting is that  during learning  we also learn whether it is convenient or not to exploit bagging for classifying future unseen examples. it also turns out  experimentally  that a much lower number of runs than t-m is very often sufficient to obtain a stable estimate of the sought probabilities. 
1 adaboost's generalization behavior 
in this section we discuss adaboost's generalization capabilities and their interpretation in the monte carlo framework. having related the margin of an example with the average probability that it is correctly classified by the hypotheses learned via the weak learner allows us to transfer results provided by schapire et al.  to the monte carlo framework. interestingly enough  the link may be used to give a new  more intuitive  interpretation of the margin explanation of adaboost's generalization capabilities. moreover a nice explanation of why the test error is often observed to decrease even after the training error reaches zero can be suggested. 
to understand how generalization error can be interpreted in the monte carlo framework it is necessary to deepen a little the discussion about the link between ensemble learning and monte carlo algorithms. let us choose the randomization procedure  whatever it is . once this is done  the choice of the weak learner determines the distribution of the . in other words  when the two major parameters of the ensemble algorithm are fixed  we can think of  as a theoretical property of the examples. 
once the ensemble process starts  we compute the estimations  the accuracy of the estimates increases with the number of iterations performed. whether the final rule will classify correctly or not any example  is a matter of two things: the value of the underlying exact 
1  the datasets used  as well as part of the experimental results  can be found at the url: 
learning 	1 http://www.di. unito.it/~csposito/mcandboost 
probability  and how lucky we were in the choice of the rules used to perform the actual estimation. the above argument might explain the finding of the test error decreasing  for adaboost  beyond the point in which training error reaches zero. in fact  as an effect of the learning bias introduced by the weak algorithm  it is likely that the  corresponding to examples in the training set would be greater than the ones corresponding to the other examples. it is then likely that those examples would be correctly classified with high probability after a moderate number of iterations. on the contrary  other examples would require a greater number of iterations  and thus they may start to be correctly labelled well beyond the point where the training error reaches zero. in fact  even though any example with will eventually be correctly classified  the number of iterations necessary to see this effect to emerge strongly depends on t. the observed effect of the test error decreasing is hence due to those examples for which is only slightly greater than 1. initially  their is possibly underestimated by the observed . for small values of t   can be quite different from and 
greater values of t are necessary to let the choice of hypotheses converge toward a set for which and  hence  to obtain a stable correct classification of in the test set. 
1 conclusions and further work 
in this paper we have proposed to analyze ensemble learning  in particular bagging and  partly  adaboost  from the perspective of monte carlo algorithm theory. we have shown that this theory can shed light on some basic aspects of bagging and boosting  and  also offers a new way to actually perform ensemble learning and error estimation. more research is nevertheless needed to deepen our understanding of the presented framework. in particular  it would be very interesting to investigate the relationships between the monte carlo explanation of ensemble learning and other well known ones  for instance  the bias-variance decomposition . moreover  it would be very interesting to investigate how a particularly aggressive class of monte carlo algorithms  namely the one of biased monte carlos  relates to ensemble learning. 
