
in recent years  there has been a growing interest in using rich representations such as relational languages for reinforcement learning. however  while expressive languages have many advantages in terms of generalization and reasoning  extending existing approaches to such a relational setting is a non-trivial problem. in this paper  we present a first step towards the online learning and exploitation of relational models. we propose a representation for the transition and reward function that can be learned online and present a method that exploits these models by augmentingrelational reinforcement learning algorithms with planning techniques. the benefits and robustness of our approach are evaluated experimentally.
1 introduction
in reinforcement learning  an agent can observe its world and perform actions in it. its goal is to maximize the obtained reward. for small domains with a limited number of states  exact solution methods such as dynamic programming exist. however  these methods are unable to handle real-world domains with large state spaces. for such problems  structuring the world and generalization becomes essential. recently  there is a strong interest in relational reinforcement learning  tadepalli et al.  1   a framework not only providing an expressive language for describing the world and for generalization  but also able to handle  relational  state spaces which can not be easily represented using vector spaces.
¡¡for example  consider a blocks world where there is a set of blocks {b1 b1 ...bn}. every block bi stands either on the floor  denoted on bi fl   or on some other block bj  denoted on bi bj   and on every block bi there is either exactly one other block or it is clear  denoted clear bi  . here  we describe a state by the set of facts that are true in that state  e.g. {clear b1  on b1 b1  on b1 fl  clear b1  on b1 fl }.
the agent can take a clear block bi and put it on another clear block bj or on the floor  denoted move bi bj  . move actions with other arguments  e.g.  move fl b1   are possible but have no effect. as the number of blocks is not limited in this world  it is clear that such states can not be easily represented by vectors. in contrast  in a relational setting concepts such as  on  and  clear  are intuitive to work with.
¡¡several studies have shown that learning a model of the world  a transition function and the reward function  is often beneficial for the agent. once a model has been learned  it can be used in several different ways. first  it can help to speed up the learning process by generating more training examples through simulation of actions in states  as happens in the dyna architecture  sutton  1 . second  it allows the agent to reason about actions in a way similar to planning  tesauro  1   which may allow it to achieve better rewards in exploitation mode and to make better estimates of q-values in exploration mode by using lookahead  the td-leaf method  baxter et al.  1  is an example of the latter . note that both options are complementary and can be combined.
¡¡though in the relational setting most research so far has focused on a model-free setting  recently there is growing interest in extending methods for learning and exploiting models of the world to a relational setting  see  van otterlo  1  for a recent overview . this is a non-trivial task as the use of a more expressive relational language inevitably implies that models of the world are more complex and harder to learn and apply. for instance  with the dyna strategy  it is fairly easy to learn a model by keeping a probability distribution on states. in the relational case however a probability distribution on the large space of relational states is necessary  which is a lot harder  as shown in the field of statistical relational learning.
¡¡our work investigates the feasibility and benefit of using relational learned models for lookahead. moreover  we study whether such a strategy is still beneficial in complex worlds where it is not possible to learn a perfect model. we present marlie  model-assisted reinforcement learning in expressive languages   the first system to learn a relational transition and reward function on-line. our contribution is threefold.  1  we propose a representation for the transition function that facilitates its efficient and incremental learning.  1  we propose a learning and exploitation method. in contrast to earlier approaches learning relational models off-line  e.g.  zettlemoyer et al.  1    the partial model is exploited immediately  avoiding as much as possible an initial period where the agent gains poor rewards trying to learn a good model  and in contrast to work such as  kersting et al.  1  this model does not need to be complete. also note that we are consideringthe full rl problemin that our techniquedoes not require resets or generative models as e.g. in  fern et al.  1 .  1  we experimentally evaluate the efficiency and benefits of our approach and examine the influence of the quality of the learned model on these results.
organization. section 1 presents some background and section 1 shows how the transition function of a relational mdp can be represented and learned online. section 1 describes how using the model to look some steps ahead improves over standard q-learning. the experimental evaluation is shown in section 1. section 1 discusses related work and we conclude in section 1.
1 reinforcement learning and mdps
reinforcement learning  rl   sutton and barto  1  is often formulated in the formalism of markov decision processes  mdps . the need to model relational domains has led to different formalizations of relational mdps  rmdps   e.g.  fern et al.  1; kersting and de raedt  1; kersting et al.  1. we use the following simple form:
definition 1 a relational mdp  rmdp  is defined as the
  where ps is a set of state
predicates  pa is a set of action predicates and c is a set of constants. a ground state  action  atom is of the form p c1 ... cn  with p/n ¡Ê ps  p/n ¡Ê pa  and  i : ci ¡Ê c}. a state in the state space s is a set of ground state atoms; an action in the action state a is a ground action atom.
¡¡the transition function t : s ¡Á a ¡Á s ¡ú  1  defines a probability distribution over the possible next states:
 denotes the probability of landing in state s when executing action a in state s. the reward function r : s ¡Á a ¡ú r defines the reward for executing a certain action in a certain state.
the task of reinforcement learning consists of finding an optimal policy for a certain mdp  which is  initially  unknown to the rl-agent. as usual  we define it as a function of the discounted  cumulative reward  i.e. find a policy ¦Ð : s ¡ú a that maximizes the value function: v ¦Ð s  =
  where 1 ¡Ü
¦Ã   1 is the discount factor  which indicates the relative importance of future rewards with respect to immediate rewards.
¡¡the rrl-system  driessens  1  applies q-learning in relational domains  by using a relational regression algorithm to approximate the q-function defined as

knowing these q-values  an optimal policy ¦Ð  of the mdp can be constructed as ¦Ð  s  = argmaxa q s a .
1 online learning of relational models
in this section we propose a representation for the transition function and reward function that can be easily and efficiently learned in an on-line setting. this module learns these functions in the form of probability distributions and   given ps  pa and c of the rmdp.
1 representation of the world model
as said above  a state is a set of ground state atoms; hence  using a binary random variable for every possible ground state atom at each time point t  a dynamic bayesian network  dbn   dean and kanazawa  1  can represent the transition function. the action taken at time point t is represented by a random variable at  one for every t  that ranges over all atoms from a that represent valid actions. the reward at time t is represented by a real-valued random variable rt.
¡¡the reward in the currentstate depends on the randomvariables representing the state and the action taken. the action taken depends on the current knowledge of the agent and the current state. its conditional probability distribution  cpd  is not explicitly modeled as the chosen action is the result of the agent's reasoning process. the current state in turn depends on the previous state and the action taken in that state. this specifies a layered network structure which is a partial order over the random variables. there can still be dependencies between variables of the same state. we assume an expert provides an order on the random variables describing states such that a random variable only depends on those preceding it in this order. hence we avoid the problem of learning the structure of the network  a problem that would be especially hard in the case of online learning because a revision of the structure would interfere with the learning of the conditional probability tables in uninvestigated ways.
¡¡the cpds of state random variables can be compactly represented by relational probability trees  neville et al.  1; fierens et al.  1 . in our setting  the main idea is to have a single relational probability tree for every predicate symbol p ¡Ê ps. this tree is used to model the conditional probability distribution that gives for every ground atom x with predicate symbol p the probability that it will be true in the next state given the current state s and action a. this allows for maximal generalizations by using the same tree for all atoms of the same predicate symbol  but avoids the problems arising when generalizing over predicates with a different number of arguments with different types and different semantics. as an example  figure 1 shows a probability tree cpd for clear x  in the blocks world. another relational probability tree is used to represent the cpd of the reward random variable. note that the learned relational probability tree does not necessarily use all preceding random variables of the network as splits in internal nodes.
1 learning the model
all the uninstantiated parts of the model are cpd's represented by relational probability trees. therefore  learning the model reduces to the online learning of a set of decision trees. in our implementation we use an improved1 version of the incremental relational tree learning algorithm tg  driessens et al.  1 .

figure 1: this probability tree shows the probability that block a will be clear when the action move x y   is executed in state state. the first node in the tree checks if block a was already clear  essentially the frame assumption . if this is the case  it can only become not clear when some other block is moved on top of it. if the block was not clear in the original state  it can only become clear in the afterstate if the block directly on top of it got moved to another block.
¡¡learning examples can easily be created from the agents experience. note that the number of possible facts in the next state may be very large  and therefore we do not generate all possible examples but apply a suitable sampling strategy.
1 online exploitation of relational models
the  partially correct  transition function that is being learned enables the agent to predict future states. in this paper  we investigate the use of q-learning with lookahead trees to give the agent more informed q-values by looking some steps into the future when selecting an action. these lookahead trees are similar to the sparse lookahead trees used in  kearns et al.  1  to obtain near-optimal policies for large mdps.
¡¡since the transition function can be stochastic or there may be uncertainty on the effect of a particular action  due to incomplete learning   an action needs to be sampled several times to obtain an accurate value. this sampling width sw is a parameter of the algorithm. starting from the current state in the root node  we generate for every possible action  sw  directed  edges using the action as a label for that edge. the node at the tail of this edge represents the state obtained from executing that action in the head node. this can be continued until the tree reaches a certain depth. the q-values of the deepest level can be estimated by the learned q-function. by back-propagating these values to the top level  a policy can be constructed using these top level q-values as in regular q-learning. when back-propagating the q-values for the different samples of a certain action in a certain state are averaged and the q-value of a higher level is determined using the bellman equation  eq. 1 .
¡¡several optimizations to this scheme are possible. our implementation uses preconditions. this is especially useful in planning domains as  typically  the world remains unchanged if the agent tries an illegal action. while the transition function only indirectly states the preconditions of an action  the introduction and the learning of an extra binary random variable lt  intended to be true if the state of the world changes  allows one to prune away actions predicted to be illegal.
¡¡besides the sampling width  also other parameters can influence a lookahead tree. currently  we are investigating the use of beam-like and randomized searches.
1 empirical evaluation
in this section we will present an empirical evaluation of our approach. first  we want to evaluate whether our incremental relational decision tree learner is able to build a model of the world. second  we want to investigate how much the performance and speed of the agent improves by adding lookahead. third  we want to evaluate the robustness of the approach  i.e.  evaluate whether the lookahead is still beneficial when the learner is not able to build a complete model.
1 domains experimental setup
in all the following experiments  the rrl-tg driessens et al.  1  system is used to estimate the q-values. since the transition function for these domains are still learned rather easily  a sampling width of two is used. the agent also learns the function modeling the preconditions to prune the lookahead tree. the exploration policy consists of performing a single step lookahead. to eliminate the influence of a specific ordering on the random variables  independence is assumed between random variables in the same state. the figures show the average over a 1-fold run where each test run consists of 1 episodes and the average reward over this 1 episodes following a greedy policy  i.e.  the percentage of episodes in which a reward is received  is used as a convergencemeasure.
blocks world in the following experiments  we use the blocks world as described in the introduction with the stackgoal and the on a b  goal  defined in the same way as in  driessens  1 . in the on a b -goal the agent only receives a reward iff block a is directly on top of block b. the objective of the stack-goal is to put all blocks in one and the same stack  i.e.  if there is only one block on the floor. the results for the unstack-goal where the agent is rewarded iff all blocks are on the floor are not included as the behavior was comparable to the stack-goal.
¡¡during exploration  episodes have a maximum length of 1 steps above the ones needed by the optimal policy  during testing only optimal episodes are allowed. in the stackproblem the blocks world has seven blocks  for the on a b goal the number of blocks was varied for each episode between four and seven. the same language bias is used as in previous experiments with the rrl-tg algorithm in the blocks world  driessens  1 .
logistics the second domain is a logistics domain containing boxes  trucks and cities. the goal is to transport certain boxes to certain cities1. the possible actions in this domain are load box on truck/1  which loads the specified box on the specified truck if they are both in the same city  the unload box on truck/1 which takes the box of the truck and movesit in the depot of the city where the truck is located. the third possible action is the move/1-action  which moves the truck to the specified city. the state space ps consists of the following predicates: box on truck/1  box in city/1 and truckin city/1. these predicates also make up the language bias used in these experiments  i.e.  the tree learning algorithm can test if a certain box is on a certain truck etc.
¡¡in the first setting there are two boxes  two cities and three trucks and the goal is to bring the two boxes to two specific cities. during exploration  1 steps are allowed  but during testing the maximum length of an episode is 1 steps. for the second setting the domain is extended to four boxes  two cities and two trucks and the goal is to bring three specific boxes to certain locations within 1 time steps.

figure 1: errors made by the learned transition function
¡¡first  we test the quality of the transition function. to achieve this  we apply the learned model as a classifier. we randomly generate facts and predict if they will be true in the resulting next state  given some state and action. figure 1 shows the percentage of errors made per step  false positives  fp  and false negatives  fn  indicate the number of atoms that were incorrectly predicted as true and false respectively. for the blocks world with seven blocks the transition function is learned rather rapidly and is optimal after about 1 episodes. for the logistics domain with five boxes  trucks and cities  it appears that a reasonable transition function is learned rapidly  but it never reaches optimal performance.
¡¡next  we evaluate the improvement obtained by lookahead planning. therefore  we ran both experiments with standard rrl-tg and experiments with different amounts of lookahead using the learned model. figure 1 and figure 1 show the results for the on a b  and stack goals of the blocks world respectively and for the logistics domain the results are plotted in figure 1.

	figure 1: blocks world with on a b  goal

figure 1: blocks world with stack goal
¡¡these experiments show that the number of episodes needed to obtain a good policy is much smaller when looking ahead. however  learning and lookahead takes additional computation time. therefore  for worlds where performing actions is not expensive  it is also useful to evaluate how the total time needed to obtain a good policy compares with and without learning. figure 1 shows the average reward in function of the time needed to learn the policy for the on a b  goal  the other settings produce similar results  omitted due to lack of space . due to the computational costs of learning the model  using no lookahead performs better than single step lookahead. indeed  in our current implementation  learning the model of the world is the computational bottleneck for small levels of lookahead. this is mainly due to the sampling techniques to create the learning examples  something we will improve in future work. however  performing two steps of lookahead still outperforms the standard version without lookahead.

	figure 1: logistics

figure 1: blocks world with the on a b  goal
¡¡finally  we evaluate the robustness of our approach. we set up this experiment by generating less training examples for the transition function per step  extending the time to learn a good transition function. every 1 episodes  we test both the quality of the transition function  the number of classification errors  and the average reward obtained by the learned policy using single step lookahead. in figure 1 we plotted these points for two different learning rates  showing the obtained reward in function of the quality of the model. the horizontal line shows the performance of a learning agent after 1 episodes using no lookahead. the figure shows that even when the learned model is less accurate  it is still beneficial to use lookahead. only when the model performs very inaccurately  performance can drop.
1 related work
an important part of related work are the indirect or modelbased approaches in the propositional setting. most of these fit the dyna architecture  sutton  1  as mentioned in the introduction. there the agent learns a model of the world

figure 1: reward versus the quality of the model
and uses these approximations of the transition and reward function to perform hypothetical actions to generate extra updates for the q- or value function. algorithms such as prioritized sweeping  moore and atkeson  1   and extensions of these  focus the hypothetical actions on interesting parts of the state space. as mentioned earlier  these approaches are orthogonal to our approach and will be explored in our future work.
¡¡learning a model of the environmentbecomes a non-trivial task in the relational setting. one method that focuses specifically on learning transition functions with relational structure is  zettlemoyeret al.  1 . they present a method for learning probabilistic relational rules when given a dataset about state transitions that are applicable in large noisy stochastic worlds. the main difference with  the learning part of  our approach is that this method is not directly applicable to reinforcement learning  since it does not work incrementally and it is limited to domains where actions only have a small number of effects.
¡¡the emerging field of statistical relational learning has seen a lot of work on relational upgrades of bayesian networks. more specifically   sanghai et al.  1  defines relational dynamic bayesian networks  rdbns  as a way to model relational stochastic processes that vary over time.
¡¡combining search and rl has shown to be successful in the past  for instance in the context of game playing baxter et al.  1 . in  davies et al.  1   online search is used with a  very  approximate value function to improve performance in continuous-state domains. our approach can be seen as an instance of the learning local search  lls  algorithm described there.
¡¡many new rrl algorithms have been proposed lately  but to our knowledge this is the first indirect rrl approach. the most related is  sanner  1   where a ground relational naive bayes net is learned as an estimation of the value function. the major difference however is that this work does not consider the aspects of time since they consider game playing and hence restrict themselves to undiscounted  finite-horizon domains that only have a single terminal reward for failure or success.
1 conclusions and future work
in this paper we presented marlie  the first reinforcement learning system learning and exploiting a relational model of the world on-line. we argued that this system has several advantages when compared to earlier work. comparedto earlier on-line learning approaches  our approach is fully relational  allowing for a wider range of problems. compared to earlier relational approaches  we argue that learning a complete model may be difficult. marlie builds probabilistic models allowing for partial models and uncertainty about the dynamics of the world  while at the same time exploiting knowledge as soon as it is available.
¡¡there are several directions for future work. first  one could improve the sampling strategies for looking ahead; e.g. by using iterative deepening strategies or sampling some random variables only lazily. second  self-evaluation may be beneficial. in this way once it is shown that  parts of  the learned transition function is correct or good enough  no more resources need to be invested in it  but it may stay useful to check it at regular times . on the other hand  if it is known that the learned model or certain parts of it are not satisfactory yet  this can be taken into account in the decision making process. finally  it would also be interesting to investigate how to use poor or partial models in combination with traditional planning reasoning strategies.
acknowledgments
tom croonenborghs is supported by the flemish institute for the promotion of science and technological research in industry  iwt . jan ramon and hendrik blockeel are postdoctoral fellows of the fund for scientific research of flanders  fwo-vlaanderen .
