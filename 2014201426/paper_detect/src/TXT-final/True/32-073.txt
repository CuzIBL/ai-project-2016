 
we present a new algorithm to reduce the space complexity of heuristic search. it is most effective for problem spaces that grow polynomially with problem size  but contain large numbers of short cycles. for example  the problem of finding a lowest-cost corner-to-corner path in a d-dimensional grid has application to gene sequence alignment in computational biology. the main idea is to perform a bidirectional search  but saving only the open lists and not the closed lists. once the search completes  we have one node on an optimal path  but don't have the solution path itself. the path is then reconstructed by recursively applying the same algorithm between the initial node and the intermediate node  and also between the intermediate node and the goal node. if n is the length of the grid in each dimension  and d is the number of dimensions  this algorithm reduces the memory requirement from  to 
 the time complexity only increases by a constant factor of  in two dimensions  and 1 in three dimensions. 
1 	introduction 
1 	search consider an  grid  where each edge has a potentially different cost associated with it. we want a lowestcost path from the upper-lefthand corner to the opposite lower-righthand corner  where the cost of a path is the sum of the costs of the edges in it. one important application of this problem is finding the best alignment of two genes of lengths n and m  represented as sequences of amino acids  carrillo and lipman  1 . we omit the details of the mapping between these two problems  and present it only to suggest that the problem is of practical importance. if we want to align d different sequences simultaneously  the problem generalizes to finding a lowest-cost path in a d-dimensional grid. 
　another example of this problem is that of finding a shortest path in a maze. in this case  an edge between two nodes either has a finite cost  indicating that there is a direct path between the two nodes  or an infinite cost  indicating there is a wall between them. 
　in this paper  we consider only the orthogonal moves up  down  left and right  and want a lowest-cost path from one corner to another. the extension to include diagonal moves is straightforward. while our experiments use a two-dimensional grid  the algorithm trivially generalizes to multiple dimensions. in general  a lowest-cost path may not be a shortest path  in terms of the number of edges. for example  in going from upper-left to lower-right  a lowest-cost path may include some up and left moves  as our experiments will show. 
1 previous work 
1 problems that fit in memory 
if the grid is small enough to fit into memory  the problem is easily solved by existing methods  such as di-
jkstra's single-source shortest path algorithm  dijkstra  1 . it requires 1 nm  time  since each node has a constant number of neighbors  and 1 nm  space  in the worst case. since we are only interested in the cost to the goal  we can terminate the algorithm when this value is computed  but in practice almost the entire grid will usually be searched  as our experiments will show. 
　if we restrict the problem to moves that go toward the goal  it can be solved by a much simpler dynamic programming algorithm. we simply scan the grid from left to right and from top to bottom  storing at each point the cost of a lowest-cost path from the start node to that grid point. this is done by adding the cost of the edge from the left to the cost of the node immediately to the left  adding the cost of the edge from above to the cost of the node immediately above  and storing the smaller of these two sums in the current node. this also requires  time and  space. 
　the difference between these two problems is that in the general case  any of the neighbors of a node can be its predecessor along an optimal path  and hence dijkstra's algorithm must maintain an open list of nodes generated but not yet expanded  and process nodes in increasing order of their cost from the root. 
　since both algorithms may have to store the whole graph in memory  the amount of memory is the main 

constraint. we implemented dijkstra's algorithm on a two-dimensional grid using randomly generated edge costs  on a sun ultra-sparc model 1 workstation with 1 megabytes of memory. the largest problem that we can solve without exhausting memory is a 1 x 1 grid. our implementation takes about five and a half minutes on problems of this size. 
1 	problems that don't fit in memory 
the problem is much more difficult when the entire grid cannot fit in memory. this can happen if all the edge costs are not explicitly listed  but are implicitly generated by some rule. for example  in the gene sequence alignment problem  the edge cost is based on the particular pair of amino acids being aligned at a particular point in the two genes  and hence the number of different edge costs is only the square of the number of amino acids. in an even simpler example  finding the longest common subsequence in a pair of character strings  hirschberg  1   the edge costs are just one or zero  depending on whether a pair of characters at a given position are the same or different  respectively. 
　in our experiments  each edge of the grid is assigned a unique number. this number is then used as an index into the sequence of values returned by a pseudo-random number generator  and the corresponding random value is the cost of the edge. in particular  we use the pseudorandom number generator on page 1 of  kernighan and ritchie  1 . this requires the ability to efficiently jump around in the pseudo-random sequence  an algorithm for which is given in  korf and chickering  1 . 
　one approach to the memory problem is to use a heuristic search  such as a*  hart  nilsson  and raphael  1  to reduce the amount of the problem space that must be searched. this assumes that we can efficiently compute a lower bound on the cost from a given node to the goal  and has been applied to the gene sequencing problem  ikeda and imai  1 . if we establish a nonzero minimum edge cost in our random grids  we can use the manhattan distance to the goal times this minimum edge cost as a lower-bound heuristic. unfortunately  a* must still store every node it generates  and ultimately is limited by the amount of available memory. 
　the memory limitation of algorithms like dijkstra's and a* has been addressed by ai researchers over the last 1 years  korf  1 . many such algorithms  such as iterative-deepening-a*  ida*   korf  1   rely on a depth-first search to avoid the memory problems of bestfirst search. the key idea is that a depth-first search only has to keep in memory the path of nodes from the start to the current node  and as a result only requires memory that is linear in the maximum search depth. 
　while depth-first search is highly effective on problem spaces that are trees  or only contain a small number of cycles  it is hopeless on a problem space with a large number of short cycles  such as a grid. the reason is that a depth-first search must generate every distinct path to a given node. in an  grid  the number of different paths of minimum length from one corner to the opposite corner is for example  a 
1 x 1 grid  which contains only 1 nodes  has 1 different minimum-length paths from one corner to another  and a 1 x 1 grid  with only 1 nodes  has over 1 such paths. a minimum-length path only includes moves that go toward the goal  such as down and right moves in a path from upper-left to lower-right. 
	another technique  	based 	on finite-state-machines 
 taylor and korf  1   has been used to avoid this problem in regular problem spaces such as grids. unfortunately  this method assumes that all minimum-length paths to a given node are equivalent  and does not apply when different edges have different costs. 
　other techniques  such as caching some nodes that are generated  have been applied to these problems  miura and ishida  1 . the problem with these techniques is that they can only cache a small fraction of the total nodes that must be generated on a large problem. 
　we implemented ida* on our random grid problems  with pitiful results. the largest problems that we could ran were of size  in addition to the problem of duplicate node generations  since most paths had different costs  each iteration on average only expanded about four new nodes that weren't expanded in the previous iteration. as a result  five problems of size  expanded an average of 1 billion nodes each  and took an average of about an hour each to run. 
1 divide & conquer bidirectional search  dcbds  
we now present our new algorithm. while we discovered it independently  a subsequent search of the literature revealed a special case of the main idea  hirschberg  1 . for pedagogical purposes  we first describe our general algorithm  and then hirschberg's special case. 
a best-first search  such as dijkstra's algorithm or 
a*  stores both a closed list of nodes that have been expanded  and an open list of nodes that have been generated  but not yet expanded. the open list corresponds to the frontier of the search  while the closed list corresponds to the interior region. only nodes on the open list are expanded  assuming the cost function is consistent  and thus we could execute a best-first search without storing the closed list at all. 
　in an exponential problem space with a branching factor of two or more  the open list is larger than the closed list  and not storing the closed list doesn't save much. in a polynomial space  however  the dimensionality of the frontier is one less than that of the interior  resulting in significant memory savings. for example  in a twodimensional problem space  the size of the closed list is quadratic  while the size of the open list is only linear. 
　there are two problems with this approach that must be addressed. the first is that duplicate node expansions are normally eliminated by checking new nodes against the open and closed lists. without the closed list  to prevent the search from  leaking  back into the closed region  we store with each open node a list of forbid-
	k1rf 	1 


figure 1: divide and conquer bidirectional search 

den operators that would take us into the closed region. for each node  this is initially just the the operator that generates its parent. as each node is generated  it is compared against the nodes on the open list  and if it already appears on open  only the copy arrived at via the lowest-cost path is saved. when this happens  the new list of forbidden operators for the given state becomes the union of the forbidden operators of each copy. 
　in fact  this technique can be used to speed up the standard dijkstra's and a* algorithms with a closed list as well. it is faster to not generate a node at all  than to generate it and search for it in open and closed lists. in our grid experiments  this technique alone sped up our implementation of dijkstra's algorithm by over 1%. 
　the main value of this technique  however  is that it executes a best-first search without a closed list  and never expands the same state more than once. when the algorithm completes  we have the cost of an optimal path to a goal node  but unfortunately not the path itself. if we store the path to each node with the node itself  each node will require space linear in its path length  eliminating all of our space savings. in fact  this approach requires more space than the standard method of storing the paths via pointers through the closed list  since it doesn't allow us to share common subpaths. 
　one way to construct the path is the following. we perform a bidirectional search from both the initial state and the goal state simultaneously  until the two search frontiers meet  at which point a node on a solution path has been found. its cost is the sum of the path costs from each direction. we continue the search  keeping the intermediate node on the best solution found so fax  until the total solution cost is less than or equal to the sum of the lowest-cost nodes on each search frontier. at this point we are guaranteed to have a node on a lowestcost solution path. we save this intermediate node in a solution vector. then  we recursively apply the same algorithm to find a path from the initial state to the intermediate node  and from the intermediate node to the goal state. each of these searches will add another 
1 	search 
node to the final solution path  and generate two more recursive subproblems  etc  until we have built up the entire solution. we call this algorithm divide-and-conquer bidirectional search  or dcbds. 
　figure 1 shows an idealized view of dcbds. the left panel shows the final search horizons of the first bidirectional search. their intersection  node a  is the first node found on the optimal solution. the center panel shows the next two searches  from node a toward both the initial and goal states  adding the intersections at nodes b and c  respectively  to the solution. finally  the right panel shows the next level of searches  adding nodes d  e  /  and g to the solution path. the reason the search frontiers look like circles and arcs of circles is that they represent an uniformed dijkstra's algorithm  which doesn't know the direction to the goal. 
1 	hirschberg's algorithm 
 hirschberg  1  gives an algorithm for computing a maximal common subsequence of two character strings in linear space. it generates a two-dimensional matrix  with each of the original strings placed along one axis. an element of the matrix corresponds to a pair of initial substrings of the original stings  and contains the length of the maximal common subsequence of the substrings. 
　if n and m are the lengths of the original strings  the standard dynamic programming algorithm for this problem computes this matrix by scanning from left to right and top to bottom. this requires 1 nm  time and 1 nrn  space. however  to compute any element of this matrix  we only need the value immediately to its left and immediately above it. thus  we can compute the entire matrix by only storing two rows at a time  deleting each row as soon as the next row is completed. in fact  only one row needs to be stored  since we can replace elements of the row as soon as they are used. unfortunately  this only yields the length of the maximal common subsequence  and not the subsequence itself. 
　hirschberg's algorithm computes the first half of the matrix from the top down  storing only one row at at 

time  and the second-half from the bottom up  again only storing one row. then  given the two different versions of the middle row  one from each direction  he finds the column for which the sum of the two corresponding elements from each direction is a maximum. this point splits both original strings in two parts  and the algorithm is then called recursively on the initial substrings  and on the final substrings. 
the most important difference between dcbds and 
hirschberg's dynamic programming algorithm is that the latter scans the matrix in a predetermined systematic order  while dcbds expands nodes in order of cost. the dynamic programming algorithm can only be used when we can distinguish the ancestors of a node from its descendents a priori. for example  it could be modified to find a lowest-cost path in a grid only if we restrict ourselves to minimum-length paths. dcbds generalizes hirschberg's dynamic programming algorithm to bestfirst search of arbitrary graphs. 
1 	complexity of dcbds 
in a problem of size dcbds reduces the space complexity from a very significant improvement. for example  if we can store ten million nodes in memory  this increases the size of twodimensional problems we can solve from about 1 x 1 to about 1 1 x 1 1 before memory is exhausted  since the maximum size of a search frontier is roughly the sum of the lengths of the axes. in practice  time is the limiting factor on large grids  and not space. the asymptotic time complexity of hirschberg's algorithm  which only considers moves directly toward the goal  is the same as for the standard dynamic programming algorithm  or  on a d-dimensional grid. 
　to analyze the time complexity of dcbds  we model the search frontiers as circles and arcs of circles. a search frontier represents an open list  and consists of a set of nodes whose costs from the start node are approximately equal  since the lowest-cost node is always expanded next. in our experiments  we only consider the moves up  down  left  and right. thus  a set of nodes whose distance from the start are equal  in terms of number of edges  would be diamond shaped  with points at the four compass points. in this diamond  however  the nodes at the points only have a single path of minimal distance to them  but the nodes closest to the diagonals through the center have a great many different paths to them  all of minimal distance. thus  the lowest-cost path to a node near the diagonal is likely to be much smaller than the lowest-cost path to a node near a point of the diamond. since the frontier represents a set of nodes of nearly equal lowest-path cost  the frontier near the diagonals bows out relative to the points of the diamond  approximating the circular shape. in fact  our graphic simulation of best-first search on a grid shows that the search frontiers are roughly circular in shape. 
　　the time complexity can be approximated by the number of nodes expanded  which is proportional to the area contained within the search frontier. assume that we have a square grid of size whose lowest-cost path is along the diagonal  which is of length  the first bidirectional search  to determine point a in figure 1  will cover two quarter circles  each of which is of radius for a total area  c h 
equals at the next level  we need two bidirectional searches  one to determine point 1  and one for point c. this generates two quarter circles from the initial and goal corners  plus the full circle centered at node a and reaching nodes b and c. this full circle will be generated twice  once to find node 1  and once for node c. thus  we generate circles  each of which are of radius for a total area of  or 
at the third level  which generates nodes d  e  
/  and g  we generate three full circles twice each  plus two quarter circles  all of radius  in general  the set of searches at the nth level of recursion sweep out a total area of circles  each of radius 
the total area of all the searches is the sum of these terms from n = 1 to the number of levels. as an upper bound  we can write it as the infinite sum 

it is easy to show that this sum converges to one  so the total area  and hence time complexity of dcbds  is 
　to find a lowest-cost corner-to-corner path  the search frontier of dijkstra's algorithm will spread in a circular arc from the initial corner to the goal corner  at which point the entire grid will usually be covered. since the area of the grid is r1  the overhead of dcbds compared to dijkstra's algorithm is a constant factor of 
　we can perform the same analysis in three dimensions  the differences being that the searches sweep out volumes of spheres instead of areas of circles  the main diagonal of a cube is  instead of  and the searches from the initial and goal states only generate eighths of a sphere  instead of quarters of a circle. in three dimensions  dcbds generates a constant factor of  1 more nodes than dijkstra's algorithm. 
1 	experiments 
we tested dcbds on the problem of finding a lowestcost corner-to-corner path on a two-dimensional grid. each edge of the grid is assigned a random cost  and the cost of a path is the sum of the edge costs along it. we considered the general lowest-cost problem  which allows moves away from the goal as well as toward the goal. using a technique that allows us to efficiently jump around in a pseudo-random number sequence without 
	k1rf 	1 

size shortest path solution length total nodes dijkstra nodes dcbds nodes ratio 1 
1 
1 
1 
1 
1 1 1 
1 
1 1 
1 1 1 
1 
1 1 
1 
1 1 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 1 1 
1 1 
1 1 
1 1 
1 1 
1 1 1 
1 1 
1 1 1 1 1 
1 1 1 1 1 
1 1 
1 1 
1 1 
1 1 
1 1 
1 1 
1 1 
1 1 
1 1 1 1 
1 
1 
1 1 
1 
1 
1 
1 table 1: experimental results for corner-to-corner paths on square grids 

generating all the intermediate values  korf and chickering  1   we can search much larger random grids than we can store in memory. 
　table 1 shows our experimental results. for each grid size  we tested dcbds on three different random problem instances  generated from different initial random seeds  and averaged the results for the problem instances. the results from one instance to the next are very similar  allowing such a small sample size. the first column gives the length of the grid in each dimension  and the second gives the number of edges in a shortest corner-tocorner path  which is twice the grid size minus two. the third column gives the average number of edges in the lowest-cost corner-to-corner path. the reason some of these values are odd is because they are averages of three trials each. this data shows that in general a lowest-cost path is usually not a path of minimum length. 
　the fourth column gives the total number of nodes in the grid  which is the square of the grid size. the fifth column shows the average number of nodes expanded by dijkstra's algorithm  for problems small enough to fit in memory. this data shows that dijkstra's algorithm generates almost all the nodes in the grid. since grids of size 1 and greater are too large to fit in 1 megabytes of memory  we were unable to run dijkstra's algorithm on these problems  and hence those entries are empty. 
　the sixth column shows the average number of nodes expanded by dcbds  and the sixth column shows the ratio of the number of nodes expanded by dcbds  divided by the number of nodes that would be expanded by dijkstra's algorithm  given sufficient memory. even though we can't run dijkstra's algorithm on problems greater than 1 nodes on a side  we compute the ratio on the assumption that dijkstra's algorithm would generate the entire grid  if there were sufficient memory. as predicted by our analysis  the number of nodes expanded by dcbds is approximately times the total number of grid points. this factor seems to increase slightly with increasing problem size  however. 
　　the actual asymptotic running time of both algorithms is where n is the size of the grid in one dimension. the term comes from the total number of nodes in the grid that must be examined. the logn term comes from the fact that both algorithms store the 
1 	search 
open list as a heap  and the size of the open list is 1 n   resulting in time per node to access the heap. 
　even though dcbds expands over three times as many nodes as dijkstra's algorithm  it takes less than twice as long to run. the main reason is that by saving the operators that have already been applied to a node  and not reapplying them  expanding a node takes less time than applying all operators and checking for duplicates in the closed list. the grids of size 1 take dcbds about two hours to run  and those of size 1 take about 1 days. the grids of size 1 require the storage of about 1 nodes  and with only 1 megabytes of memory  we can store over twelve million nodes. thus  memory is no longer a constraint. 
1 	further work 
an obvious source of waste in dcbds is that most of the individual searches are performed twice. for example  in the center panel of figure 1  the full circle centered at node a is searched twice  once to locate node 1  and then again to locate node e. by performing the search for nodes b and c simultaneously  we would only have to generate the circle once. the same optimization can be applied to all the full-circle searches. since most of the searches are full circles  this would reduce the time complexity by up to a factor of two  making dcbds run almost as fast as dijkstra's algorithm. 
　the drawback of this optimization is that it complicates the algorithm. in particular  all the searches at the same level of recursion must be performed simultaneously. for example  in the right panel of figure 1  the searches corresponding to the full circles must be interleaved. this destroys the simple recursive structure of the algorithm  replacing it with an iterative outer loop with increasing numbers of interleaved searches in each iteration. in addition  some of these searches will terminate before others  and some may not even be necessary. 
　our current implementation continues executing recursive bidirectional searches until the initial and goal nodes are the same. another obvious optimization would be to terminate the recursion when the problem size is small enough that there is sufficient memory to hold the entire subgrid in memory  and then execute dijkstra's algorithm at that point. since our analysis suggests that 

the lower-level searches in the recursion hierarchy contribute diminishingly less to the overall time  this optimization may not result in significant savings. 
　the idea of storing only the open list in memory suggests yet another algorithm for this problem. what we could do is to execute a single pass of dijkstra's algorithm  saving only the open list  but writing out each node as it is expanded or closed to a secondary storage device  such as disk or magnetic tape  along with its parent node on an optimal path from the root. then  once the search is completed  we reconstruct the solution path by scanning the file of closed nodes backwards  looking for the parent of the goal node  then the parent of that node  etc. since nodes are expanded in nondecreasing order of their cost  we are guaranteed that the complete solution path can be reconstructed in a single backward pass through the file of closed nodes. 
　the advantage of this approach is that the capacity of most secondary storage devices is considerably greater than that of main memory  and we can access the device sequentially rather than randomly. unfortunately  most such devices can't be read backwards very efficiently. the best we could do would be to simulate this by reading a disk or tape file in blocks large enough to fit in main memory  and then access these blocks in reverse order. given the efficiency of dcbds  and the slow speed of secondary storage devices  however  it's unlikely that this will lead to a faster algorithm. 
1 	conclusions 
we generalized hirschberg's dynamic programming algorithm to reduce the memory requirement of best-first search in arbitrary graphs with cycles. the most important difference between dcbds and the dynamic programming algorithm is that the latter only works when we know a priori which neighbors of a node can be its ancestors and descendents  respectively  while dcbds requires no such knowledge. for example  hirschberg's algorithm can find a lowest-cost path in a grid if we only allow edges in the direction of the goal  whereas dcbds allows arbitrary solution paths. our experiments show that in general the lowest-cost path in a two-dimensional grid is not of minimal length. 
　dcbds is most effective on polynomial-sized problems that are too big to fit in memory. in such problems  it reduces the memory requirement from 
 our analysis suggests that the time cost of this reduction is only a constant factor of in two dimensions  which is supported by our experimental results. in three dimensions  our analysis predicts a constant overhead of  further optimizations could 
reduce these constants by up to a factor of two  and  additional constant savings may reduce the actual running time to no more that traditional best-first search. 
　while we used dijkstra's algorithm in our experiments  the generalization of dcbds to a* is straightforward. a* may prune more of the search space  allowing larger problems to be solved  but it is also space-bound in practice  and hence will benefit from this technique. 
　the traditional drawback of bidirectional search has been its memory requirements. ironically  dcbds shows that bidirectional search can be used to save memory  and has the potential to revive study of this area. 
1 	acknowledgements 
thanks to toru ishida and teruhisa miura for introducing me to the gene sequence alignment problem. thanks to hania gajewska for developing the graphics code. this research was supported by nsf grant iri-1. 
