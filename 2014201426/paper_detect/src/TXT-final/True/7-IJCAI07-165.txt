
deictic representation is a representational paradigm  based on selective attention and pointers  that allows an agent to learn and reason about rich complex environments. in this article we present a hierarchical reinforcement learning framework that employs aspects of deictic representation. we also present a bayesian algorithm for learning the correct representation for a given sub-problem and empirically validate it on a complex game environment.
1 introduction
agre  1; 1  introduced deictic representations to the ai community as a way of reasoning and learning in large complex domains. it is a representational paradigm based on using a small set of pointers that lets the agent focus on parts of the state space relevant to solving the problem at hand.
¡¡deictic pointers might be simple physical locators or they might encode complex semantics. agre and chapman  employ pointers that let the agent precisely locate important components of the system  but their method needs substantial pre-processing and domain knowledge. while solving the arcade game pengo  their agent pengi employs complex pointers such as bee-attacking-me  ice-cube-next-to-me  etc. the actions of the agent are then defined with respect to these pointers  for example  push ice-cube-next-to-me toward beeattacking-me. in general  deictic representations can be used in rich environments with incredible amounts of detail. they are also useful in systems where there are physical limitations on the sensory capabilities so that some form of attentional mechanism has to be employed  minut and mahadevan  1 .
¡¡there has been considerable interest in employing deictic representations in a reinforcement learning  rl  framework  whitehead and ballard  1; mccallum  1; finney et al.  1 . the attractions of such a synthesis are obvious and can lead to a trial-and-error approach that can work in large environments. in this article we develop a hierarchical deictic rl framework based on relativized options  ravindran and barto  1a . specifically we extend our earlier approach to factored mdps and show that certain aspects of deixis can be modeled as finding structured homomorphicreductions of the problem. we present a bayesian algorithm for learning the correct configuration of pointers and validate our approach on a simulated game domain inspired by agre and chapman
.
¡¡we employ markov decision processes  mdps  as our basic modeling paradigm. first we present some notation regarding factored mdps  section 1   a brief summary of relativized options  section 1  and introduce deictic option schemas  section 1 . then we present our bayesian algorithm to choose the right pointer configuration to apply to a sub-task  section 1 . we relate our approach to an existing family of deictic algorithms in section 1. in section 1 we describe some related work  and we conclude in section 1 by discussing some future directions for research.
1 notation
a	structured  finite 	mdp is	described by	the tuple
  where s is a finite set of states  a is a finite set of actions  ¦·   s¡Áa is the set of admissible state-action pairs  p : ¦·¡Ás ¡ú  1  is the transition probability function with being the probability of transition from state s to state s under action a  and r : ¦· ¡ú ir is the expected reward function  with r s a  being the expected reward for performing action a in state s. the state set s is given by m features or variables    where si is the set of permissible values for feature i. thus any s ¡Ê s is of the form  where si ¡Ê si for all i.
¡¡the transition probability function p is usually described by a family of two-slice temporal bayesian networks  1tbns   with one tbn for each action. a 1-tbn is a two layer directed acyclic graph whose nodes are {s1 ... sm} and. here si denotes the randomvariable representing feature i at the present state and si denotes the random variable representing feature i in the resulting state. many classes of structured problems may be modeled by a 1-tbn in which each arc is restricted to go from a node in the first set to a node in the second. the state-transition probabilities can be factored as:
probparents 
where parents   denotes the parents of node si in the 1-tbn corresponding to action a and each probparents is given by a conditional probability table  cpt  associated with node si. in computing the conditional probabilities it is implicitly assumed that the nodes in parents   are assigned values according to s.
an option  or a temporally extended action   sutton et al. 
1  in an mdp is defined by the
  where the initiation set i   s is the set of states in which the option can be invoked  ¦Ð is the option policy  and the termination function ¦Â : s ¡ú  1  gives the probability of the option terminating in any given state. the option policy can in general be a mapping from arbitrary sequences of state-action pairs  or histories  to action probabilities. we restrict attention to markov sub-goal options in which the policies are functions of the current state alone  and an option terminates on reaching a set of pre-defined  sub goal states. the states over which the option policy is defined is known as the domain of the option. in such cases the option policy can be defined in a sub-mdp  known as the option mdp  consisting of the states in the domain of the option.
1 relativized options
a relativized option combinesmdp homomorphisms ravindran and barto  1b  with the options framework to compactly represent a family of related options. an mdp homomorphism is a transformation from an mdp m to a reduced model m such that a solution to m yields a solution to m. notationally  an mdp homomorphism  h  is defined as a surjection from   given by a tuple of surjections  with h  s a   =  f s  gs a    where is called
the homomorphic image of m under h. an optimal policy in m when lifted to m yields an optimal policy in m.
¡¡in a relativized option  the policy for achieving the option's sub-goal is defined in the image of a partial homomorphism defined only over a subset of s. this image is called the option mdp. when the option is invoked  the current state is projected onto the option mdp  mo =
  and the policy action is lifted to the original mdp based on the states in which the option is invoked. a relativized option is defined as follows:
definition: a relativized option of an mdp m =  is the tuple  where i  
s is the initiation set  ¦Â : so ¡ú  1  is the termination function and is a partial homomorphism from the mdp to the option mdp mo with
r chosen based on the sub-task.
in other words  the option mdp mo is a partial homomorphic image of an mdp with the same states  actions and transition dynamics as m but with a reward function chosen based on the option's sub-task. the homomorphism conditions hold only for states in the domain of the option o. the option policy ¦Ð : ¦·o ¡ú  1  is obtained by solving mo by treating it as an episodic task. when lifted to m  ¦Ð is suitably transformed into policy fragments over ¦·.
¡¡a relativized option can be viewed as an option schema where a template of an option policy is specified using a parameterized representation of a family of sub-problems.
when the option is invoked  a particular instantiation of the schema is chosen by binding to the appropriate resources. given a set of possible bindings   ravindran and barto  1a  presented a bayesian approach to choosing the right binding to apply in a given context based on experience gathered in solving the task. it assumed that the set of bindings were given by a family of transformations  h  applied to ¦· and maintained a heuristic weight vector  wn h ¦× s    which is a measure of the likelihood of transformation h ¡Ê h being the right transformation in the context represented by ¦× s .1the weight vectors are updated using:

 where   and k =  is a nor-
malizing factor. since the projected transition probability is lower bounded by ¦Í  this approach works even when the homomorphic image is not exact. in this article we extend our earlier work to cases where the bindings are specified by deictic pointers.
1 deictic option schemas
the set of candidate transformations  h  can be specified by a set of deictic pointers together with their possible configurations. the agent learns to place the pointers in specific configurations to effect the correct bindings to the schema. we call such option schema together with the set of pointers a deictic option schema. formally a deictic option schema is defined as follows:
 definition: a deictic option schema of a factored mdp  is the tuple   where o =
  is a relativized option in m  and d =
{d1 d1 ¡¤¡¤¡¤ dk} is the set of admissible configurations of the deictic pointers  with k being the numberof deictic pointers available. for all i  di   1 ¡¤¡¤¡¤ m} is the collection of all possible subsets of indices of the features that pointer i can project onto the schema  where m is the number of features used to describe s.
¡¡the set di indicates the set of objects that pointer i can point to in the environment. for example  in a blocks world domain this is the set of all blocks. in our example in the next section  it is the set of all possible adversaries.
¡¡once a deictic option schema is invoked the decision making proceeds in two alternating phases. in the first phase the agent picks a suitable pointer configuration  using a heuristic weight function similar to the one given in section 1. in the second phase  the agent picks an action  based on the perceptual information obtained from the pointers  using a qfunction. this calling semantics is similar to that followed by whitehead and ballard .
each member of h has a state transformation of the form
  where ji ¡Ê di for all i and ¦Ñj  is the projection of s onto the subset of features indexed by j. if h is known a priori then the pointer configurations can be chosen appropriately while learning. in the absence of prior knowledge the bayesian algorithm developed in  ravindran and barto  1a can be used to determinethe correct bindingsto the schema from among the possible pointer configurations. but  the algorithm is not entirely suitable for deictic option schemas for the following reason.
¡¡the algorithm assumes that the candidate transformations are not structured and maintains a monolithic weight vector  wn ¡¤ ¡¤ . in the case of deictic option schemas the transformations are structured and it is advantageous to maintain a
 factored  weight vector .
ideally each component of the weight vector should be the likelihood of the correspondingpointer being in the right configuration. but usually there is a certain degreeof dependence among the pointers and the correct configuration of some pointers might depend on the configuration of other pointers.
¡¡therefore  three cases need to be considered. assume that there are only two pointers  i and j  for the following discussion  but the concepts generalize to arbitrary number of pointers.
1. independent pointers: for every j ¡Ê di  ¦Ñj satisfies the homomorphismcondition on transition probabilities. then  the right assignment for pointer i is independent of the other pointers and there is one component of the weight vector corresponding to pointer i and the updates for this components depends only on the features indexed by some j ¡Ê di.
1. mutually dependent pointers: for each j ¡Ê di and  satisfies the homomorphism conditions. but do not satisfy the homomorphism conditions for some j ¡Ê di and. thus  they cannot be treated separately and the composite projections given by their cross-products has to be considered. there is one component of the weight vector that corresponds to this cross-product projection. the update for this component will depend on the features indexed by some j ¡Ê di and.
1. dependent pointer: for each  satisfies the homomorphism conditions  as does ¦Ñj. but does not satisfy the homomorphism conditions for at least some value of. this means pointer i is an independent pointer  while j is a dependent one. there is a separate component of the weight vector that corresponds to pointer j  but whose update depends on the features indexed by some.
¡¡the weight vector is chosen such that there is one component for each independent pointer  one for each dependent pointer and one for each set of mutually dependent pointers. let the resulting number of components be l. a modified version of the update rule in  ravindran and barto  1a  is used to update each component l of the weight independently of the updates for the other components:

 1 

figure 1: a game domain with interacting adversaries and stochastic actions. the task is to collect the black diamond. the adversaries are of three types-benign  shaded   retriever  white  and delayers  black . see text for more explanation.
where  is again a function of s that captures the features of the states necessary to distinguish the particular sub-problem under consideration  and
 is the normalizing factor.  is a  projection  of  computed as follows. let j be the set of features that is required in the computation of wnl  hi ¦× s  . this is determined as described above for the various cases. then 
probparents.
1 experimental illustration in a game environment
we now apply a deictic option schema to learning in a modified version of the game environment introduced in  ravindran and barto  1a . the layout of the game is shown in figure 1. the environment has the usual stochastic gridworld dynamics. there is just one room in the world and the goal of the agent is to collect the diamond in the room and exit it. the agent collects a diamond by occupying the same square as the diamond. a boolean variable have indicates possession of the diamond.
¡¡the room also has 1 autonomous adversaries. the adversaries may be of three types-benign  delayer or retriever. if the agent happens to occupy the same square as the delayer it is captured and is prevented from moving for a random number of time steps determined by a geometric distribution with parameter hold. when not occupying the same square  the delayer pursues the agent with probability chase. the benign robots execute random walks in the room and act as mobile obstacles. the retriever behaves like the benign adversary till the agent picks up the diamond. once the agent picks up the diamond  the retriever's behavior switches to that of the delayer. the main difference is that once the retriever occupies the same square as the agent  the diamond is returned to the original position and the retriever reverts to benign behavior. the retriever returns to benign behavior if the agent is also  captured  by the delayer. none of the adversaries leave the room  and thus the agent can  escape  from the room by exiting to the corridor. the agent is not aware of the types of the

figure 1: the option mdp corresponding to the sub-task getobject-and-leave-room for the domain in figure 1. there is just one delayer and one retriever in this image mdp.
individual adversaries.
¡¡the option mdp  figure1  is a symmetrical roomwith just two adversaries-a delayer and a retriever with fixed chase and hold parameters. the features describing the state space of the option mdp consists of the x and y coordinates relative to the room of the agent and of the adversaries and a boolean variable indicating possession of the diamond. the room in the world does not match the option mdp exactly and no adversary in the world has the same chase and hold parameters as the adversaries here.
¡¡the deictic agent has access to 1 pointers: a delayer pointer that projects one of the adversaries onto the delayer in the image mdp and a retriever pointer that projects one of the adversaries onto the retriever in the image mdp. the delayer pointer is an independent pointer and the retriever pointer is dependent on the delayer pointer. the sets ddelayer and dretriever are given by the 1 pairs of features describing the adversary coordinates.
¡¡in addition to the pointers the agent also has access to some background information  such as its own location  which can be formalized as a self pointer  and whether it has the diamond or not. note that since the option mdp is an approximate homomorphicimage  the homomorphismconditions are not strictly met by any of the projections. therefore  in computing the weight updates  the influence of the features not used in the construction of the image mdp are ignored by marginalizing over them.
1 experimental results
the performance of the deictic agent is compared with a relativized agent  monolithic agent  that employs the same option mdp but chooses from a set h of 1 monolithic transformations  formed by the cross product of the 1 configurations of the deictic pointers. both agents employ hierarchical smdp q-learning with the learning rates for the option and the main task set to 1. the agents are both trained initially in the option mdp to acquire an approximate initial option policy that achieves the goal some percentage of the trials  but is not optimal. both agents use  greedy exploration. the results reported are averaged over 1 independent runs.
¡¡on learning trials both agents perform similarly  figure 1   but the monolithic agent has a marginally better initial performance. to understand this we look at the rates at which the transformation weights converge  figures 1  1  and 1 are for

figure 1: average number of steps per episode taken by both agents for solving the task shown in figure 1.

figure 1: typical evolution of a subset of weights of the monolithic agent on the task shown in figure 1.
a single typical run . figure 1 shows that typically the deictic agent identifies the delayer quite rapidly. in fact it takes only an average of 1 update steps to identify the delayer  over the 1 independent runs. the monolithic agent takes much longer to identify the right transformation  number 1 in our encoding   as seen from figure 1. on an average it takes around 1 update steps. as figure 1 shows  identifying the retriever is harder and the deictic agent takes about 1 update steps on an average.
¡¡this result is not surprising  since the correct position for the retriever depends on position of the delayer pointer. therefore  while the delayer is being learned  the weights for the retriever receive inconsistent updates and it takes a while for the weights to get back on track. for much of the time we are interested in only identifying the delayer correctly and hence the deictic agent seems to have lower variability in its performance. overall a single run with the composite agent takes around 1 hours of cpu time on a pentium iv  1 ghz machine  while a single run of the deictic agent takes around 1 hours. further  the monolithic agent considers all possible combinations of pointer configurations simultaneously. therefore  while it takes fewer update steps to converge to the right weights  the deictic agent makes far fewer number of weight updates: 1 vs. 1 on an average.

figure 1: typical evolution of a subset of the delayer weights of the deictic agent on the task shown in figure 1.

figure 1: typical evolution of a subset of the retriever weights on the task shown in figure 1.
comment
the algorithm used above updates the weights for all the transformations after each transition in the world. this is possible since the transformations are assumed to be mathematical operations and the agent could use different transformations to project the same transition onto to the option mdp. but deictic pointers are often implemented as physical sensors. in such cases  this is equivalent to sensing every adversary in the world before making a move and then sensing them after making the move  to gather the data required for the updates. since the weights converge fairly rapidly  compared to the convergence of the policy  the time the agent spends  looking around  would be a fraction of the total learning time.
1 perceptual aliasing and consistent representations
the power of deixis arises from its ability to treat many perceptually distinct states in the same fashion  but it is also the chief difficulty in employing deictic representations. this phenomenonis known as perceptual aliasing  whitehead and ballard  1 . one approach to overcome perceptual aliasing is a class of methods known as consistent representation
methods. these methods split the decision making into two phases: in the perceptual phase the agent looks around the environment to find a consistent representation of the underlying state. a consistent representation  whitehead and ballard  1  of a state is one such that all states that map to the representation have the same optimal action-value function. in the overt phase the agent picks an action to apply to the environment based on the current sensory input. learning takes place in both phases. the lion algorithm  whitehead and ballard  1  is an example of a consistent representation algorithm. here q-learning is used in the overtphase and a simple learning rule based on one step error information is used to train the sensory phase. if the one step error in the q update rule for a particular configuration is negative then that representation is considered perceptually aliased and is ignored in the future. this simple rule limits the applicability of this algorithm to deterministic settings alone.
¡¡if the representation used is a homomorphic image then it is a consistent representation  as mentioned in  ravindran and barto  1b . by restricting the definition of deictic option schema to employ partial homomorphicimages as option mdps  it is guaranteed that a consistent representation is always employed. in the absence of knowledge of the option homomorphism  finding the right transformation to employ constitutes the search for a consistent representation and we employ bayesian learning in this phase. as with the lion algorithm  a form of q-learning is used in the overt phase.
1 related work
deixis originates from the greek word deiknynai which means to show or to point out. it is employed by linguists to denote the pointing function of certain words  like here and that  whose meaning could change depending on the context. deixis was introduced to the ai community by agre. as mentioned earlier   agre and chapman  1  used deictic representations to design an agent  pengi  that plays the arcade game pengo. pengi was designed to play the game from the view point of a human player and hence used visuals from a computer screen as input.
¡¡ whitehead and ballard  1  were the first to use deictic representations in a rl system  with their lion algorithm. unfortunately  the method the lion algorithm employs to determine consistency works only in deterministic environments.  mccallum  1  takes a more direct approach to overcoming perceptual aliasing. he employs deixis to solve a car driving task and models the problem as a partially observable mdp. he uses a tree structure  known as u-trees  for representing  states  and identifies the necessary distinctions so that the resulting representation is consistent. but his approach is not divided into explicit perceptual and overt phases. there has not been much work on using hierarchical rl and deixis. the only work we are aware of is by  minut and mahadevan  1 . they develop a selective attention system that searches for a particular object in a room. it operates by identifyingthe most salient object in the agent's visual field and shifting its visual field to center and focus on that object. they employ an option to identify the most salient object in the current visual field. though they do not state it thus  this is a  deictic  option  whose effect depends on the current visual field.
¡¡a systematic study on using deictic representations with rl was reported by  finney et al.  1 . they employ a straightforward deictic representation with two pointers on a blocks world task. they use the g-algorithm to represent past information as a tree. they report that their approach does not work well for a variety of reasons. first the tree grows very large rapidly. the deictic commands are defined with respect to the two focus pointers. when long sequences of actions are required with a small number of pointers  it is easy to lose focus. while they try to address this by redesigning the pointers  they do not have much success. one way to alleviate this problem is by adopting a hierarchical approach as we do in this work. if the number of pointers required by each deictic level to maintain focus is not large  we can avoid some of the problems encountered by  finney et al.  1 .
1 discussion and future work
while deixis is a powerful paradigm ideally suited for situations that are mainly reactive  it is difficult to employ a purely deictic system to solve complex tasks that require long-range planning. our hierarchical deictic framework allows us to employ deictic representations in lower levels of the problem to leverage their power and generalization capacities  while at the higher levels we retain global context information in a non-deictic fashion. mixing such representations allows us to exploit the best of both worlds and to solve tasks that require maintaining long term focus. it is our belief that there is no pure deictic system in nature. while it has been established that humans employ deixis in a variety of settings  we certainly maintain some higher level context information. while gathering ingredients for making tea  we might be using deictic pointers for accessing various containers  land et al.  1   but we also are continuously aware of the fact that we are making tea.
¡¡the bayesian approach we outlined requires that we have a complete model of the option mdp  which we assumed was available apriori. to learn this would require a lot more experience with the option mdp than we would need to learn a reasonable policy. currently we are experimenting with learning a partial model of the option mdp  such that it is sufficient to identify the correct configuration of the deictic pointers.
¡¡the various approaches to learning with deictic pointers  whitehead and ballard  1; finney et al.  1  usually employ simple physical locators. agre  1  uses complex pointers  but hand codes the policy for maintaining the focus of these pointers. for example  a set of rules are used to determine which is the bee-attacking-me and the pointer is moved suitably. our approach falls somewhere in between. we start by defining a set of simple pointers. as learning progresses the agent learns to assign these pointers consistently such that some of them take on complex roles. we can then assign semantic labels to these pointers such as robot-chasing-me.
¡¡it should be noted that a homomorphic image implies a consistent representation but the reverse is not true. the notion of a consistent representation is a more general concept than a homomorphic image and corresponds to optimalaction value equivalence discussed in  givan et al.  1 . by restricting ourselves to homomorphic images we are limiting the class of deictic pointers that we can model. further work is needed to extend our framework to include richer classes of deictic pointers and to employ memory based methods. nevertheless in this article we have taken the first steps in accommodating deictic representations in a hierarchical decision theoretic framework.
