
term weighting systems are of crucial importance in information extraction and information retrieval applications. common approaches to term weighting are based either on statistical or on natural language analysis. in this paper  we present a new algorithm that capitalizes from the advantages of both the strategies by adopting a machine learning approach. in the proposed method  the weights are computed by a parametric function  called context function  that models the semantic influence exercised amongst the terms of the same context. the context function is learned from examples  allowing the use of statistical and linguistic information at the same time. the novel algorithm was successfully tested on crossword clues  which represent a case of single-word question answering.
1 introduction
term weighting is an important task in many areas of information retrieval  ir   including question answering  qa   information extraction  ie  and text categorization. the goal of term weighting is to assign to each term w found in a collection of text documents a specific score s w  that measures the importance  with respect to a certain goal  of the information represented by the word. for instance  passage retrieval systems weigh the words of a document in order to discover important portions of text and to discard irrelevant ones. this is the case of applications as qa  snippet extraction  keyword extraction  automatic summarization.
　common approaches to term weighting can be roughly divided into two groups: statistical and linguistic techniques. the methods in the former group are based on a statistical analysis that extracts  from a document collection  features as word frequencies or information-theoretical measures. tfidf  salton and mcgill  1  is a popular statisticallyinspired method currently used in many ir systems to measure the importance of words. a major assumption in this approach is that the words of a document can be considered as unordered and independent elements. such assumption is shared by many other measures as information gain  gain ratio  best-match  e.g. bm1  or the chi-square function  manning and schtze  1 . even if the literature presents several interesting adaptations of tfidf for specific problems  e.g.  debole and sebastiani  1     to the best of our knowledge there has been no attempt to compute tfidflike term weights that make use of relationships among terms. latent semantic indexing  lsi   deerwester et al.  1  is a statistical approach that does not suffer from the wordindependence assumption. lsi tries to measure the principal associative patterns between sets of words and concepts using the singular value decomposition technique  without taking into account word order. unfortunately lsi cannot be employed in many practical problems. this is the case of qa  because lsi projects document words into a non-linguistic space  whereas qa requires answers in natural language.
　on the other side  techniques that are more deeply inspired by natural language theories and applications  voorhees  1   as morphological and syntax analysis  naturally exploit the information provided by word contexts. but their design is very difficult and require a large human effort.
　in this paper  we present a novel algorithm for term weighting that aims to combine the advantages of both statistical and linguistic strategies by adopting a machine learning approach. our method exploits the relationships  i.e. order  distance  etc.  among the words of a document in order to evaluate their semantic relevance to a given question. the intuition behind the proposed model is that the relevance of a term can be computed recursively as the combination of its intrinsic relevance and the relevance of the terms that appear within the same context. the influence exercised by a word on another one is computed using a parametric function  called context function. this function is trained by examples  thus it is well suited for using together statistical  e.g. term frequency  and linguistic information  morphological  syntactical  lexical   requiring no pre-designed rules.
　the main idea underlining the proposed approach is tightly related to the textrank algorithm mihalcea and tarau  1  that has been succesfully applied to keyword extraction and text summarization. more precisely  the context-based algorithm extends textrank  by allowing a more general modelling of the word relationships and by providing a learning algorithm to define the strength of those relationships.
　the context-based algorithm has been evaluated on a specific problem  i.e. single-word question answering  where the goal is to find the single correct word that answers a given question. single-word qa is particularly suited to evaluate a term weighting algorithm  since it directly exploits the rank produced by the weights. the proposed method has been applied to improve the performances of a particular answering system  webcrow  ernandes et al.  1    designed to address crossword clues. the results show that the approach is viable for the single-word qa problem and suggest that it could be extended to other fields of text processing.
　section 1 describes the context-based term weighting algorithm. section 1 presents the experimental environment used to evaluate the proposed method. in section 1 we report the experimental results obtained by the system in the singleword qa problem. section 1 provides our conclusions.
1 context-based term weighting
the proposed method exploits the contexts of words. a word context primarily consists of the text surrounding a given word  but could also include other features  as document titles  hyper-linked documents  and so on. the idea is that  in order to measure the relevance of a word with respect to a certain goal  e.g. a query  a document category   the features of the context in which the term appears are important as well as the features of the word itself. roughly speaking  a word is important not just when this is statistically strong  but also when the words that belong to its context are important too.
　this recursive definition recalls that of social networks  seeley  1   where the status of a person depends on the status of the persons that are somehow related to it. we assume that a text document can be represented by a social network  where the importance of the words can be computed on the basis of its neighbors. more precisely  the weight s w  of the word w is computed as
	  	 1 
where dw is the default score of w  r w  is the set of words that belong to the context of w  cw u is a real number measuring the influence exercised by u over w  and λ （  1  is a damping factor. thus  the score of a word will result from the combination of the default score dw and its context score
.
　the real numbers dw and cw u can be calculated using information about the occurrences and the context of the words  the frequency and the linguistic role of a word  the distance between two words  and so on . the functions used to compute these values can be either predefined or learned by examples in a machine learning fashion. in our work  we assume that dw is given by an initial scoring system  e.g. tfidf . we then learn a parametric function for calculating the cw u whose goal is to improve the initial values.
　since the dw can be provided by external informative sources  the algorithm is well suited to exploit the weights produced by other algorithms in order to improve their performances. it is worth to notice that both dw and cw u can be computed exploiting statistical and linguistic features. from this point of view our method represents a mechanism to integrate both kinds of information.
　in order to implement the proposed approach  the following problems have to be addressed: how to define a context function that computes the cw u values  how to compute the term weights s w  using eq. 1   how to automatically learn the context function from a set of examples and how to evaluate the performances of the system. these issues will be considered in the following sub-sections.
1 context functions
a documentset containsmultiple instances of words and each instance  a word-occurrence  is affected by a different context. therefore  we distinguish between words  that we shall represent with w  and word occurrences  represented with a hat  w  . here we assume that cw u can be computed as the sum of the contributions of all the occurrences of w and u
	.	 1 
here  occ w  is the set of instances of word w  and ict w u    is the set of the occurrences of u that belong to the context of w   i.e.   where ctxt w   is the context of w . moreover  given a set of parameters p  cp w   u   is the parametric context function that establishes the strength of the influence between the instances w   the word under evaluation  and u   the context word . in our work this function is learned by examples1.
　the context of a word occurrence w  can be defined as the set of words that are contained in the same document and within the surround of w  with dimensions kl +kr  left margin and right margin . thus  if  ...u  kl 1 u  kl ... u  1 w  u 1  is a text passage of any document  then ctxt w   = {u  kl ...  u  1 u 1 ... u +kr} holds. in this formulation we introduce the strong assumption that direct semantic relations between words can be registered only within their surrounds. this assumption is plausible and widely adopted  e.g. snippets in search engines . on the other hand  the approach is easily extended to the cases when the context includes other features as the document title  the section title  the text of the anchors pointing to the document  and so on.
　the most delicate point of the system is the definition of the function cp w   u  . this establishes how word couples can influence one another on the basis of features extracted from the words and from the relationships between words. theoretically  the influence function can exploit any sort of information related to w  and u : information-theoretical  e.g. in text categorization: information gain  gain ratio   morphological  i.e. part-of-speech classes of w and u   syntactical  i.e. syntax role of w and u  or lexical  e.g. wordnet distance between w and u . the features that have been used in our preliminary experiments are exclusively statistical  see tab. 1 . interestingly  even with this reduced list of basic features  the system displays notable performances.
　moreover  how do we merge all this information into one single output  the most general approach consists of implementing cp w   u   by a modeling tool that has the universal approximation property  i.e. it allows to realize any function with any desired degreeof precision. these tools includeneural networks  polynomials  and rationales. while we plan to use neural networks in the future  for the introductory scope of this paper we preferred to exploit a simpler approach both for the implementation of cp w   u   and for the learning algorithm. such a simplification allows to evaluate the proposed method on the term weighting problem  avoiding that the results are altered by the complexity of the system. we defined the influence function as
	 	 1 
i=1
where xi is the value associated with the i-th feature  the αi βi are the model parameters  p =
 α1 ... αn β1 ... βn   and σ is the logistic sigmoid function σ x  = 1/ 1 + e x . notice that the logistic sigmoid is a monotone increasing function that approaches 1  when x ★  ±  and approaches 1  when x ★ ±. thus  each term σ αixi + βi  of eq.  1  is a sort of soft switch related to the i-th feature and controlled by the parameters αi and βi. when αi   1  the switch is  on   i.e. close to 1   if xi is large and positive and it is  off   i.e. close to 1   if xi is large and negative. more precisely  the parameter αi controls the steepness and the direction of the soft switch  while βi controls the point where the switch assumes a medium value 1. finally  the whole function cp w   u   can be described as a simple boolean and operation which is  on  only if all the switches are  on .
1 computing and learning context weights
for a correct definition of the weights  system  1  must have a unique solution. stacking all the variables  eq.  1  can be written as
               s =  1   λ d + λcs    1  where {w1 ... wn} is the set the words of the dictionary 
s  is the vector of the weights  d =
is a square matrix contain-
ing the word influence factors cw u.
　it can be easily proved that  provided holds for any matrix norm ，  eq.  1  has a unique solution which can be computed iterating the system1 st+1 =  1 λ d+λcst  i.e. s = limt★± st for any initial s1. in practice  the computation can be stopped when the difference between the scores at iteration t + 1 and iteration t are below a specified small value . the method is called jacobi algorithm  golub and loan  1  for the solution of linear systems. it is worth to mention that this technique is extremely efficient and can be applied even on sparse matrices containing billions of variables  page et al.  1 .
　the learning procedure can be implemented by any optimization algorithm that minimizes an error function ep  where p =  p1 ... pn  denotes the set of parameters of the context function cp w   u  . the function ep measures the performance of the weighting system on a given dataset and it will be discussed in details in the following section. in our method the training algorithm repeats the following two steps for a predefined number of times:
1. compute the gradient δeδpp of the error function with respect to the parameters;
1. adapt the parameters p by resilient method  riedmiller and braun  1 .
the resilient parameter adaptation is a technique that allows to update the parameters using the signs of the partial derivatives of the error function. it has been experimentally proved that resilient is often more efficient than common gradient descent strategies  riedmiller and braun  1 .
　the gradient is approximated by a simple brute force algorithm. each component of the gradient δpi is given by the corresponding incremental ratio  which requires to calculate the error function for n + 1 times at each training epoch. it is worth to mention that the proposed learning algorithm is a simplified version of the one described in  gori et al.  1 . the latter procedure can be used efficiently even for models having a large number of parameters. such a fact is important  since it guarantees that our implementation of the context function cp w   u   can be efficiently replaced by more complex and general models as  e.g. neural networks.
　in our implementation we decided to learn  along with the set of parametersp  the dampingfactorλ  see eq.  1    which  intuitively  establishes the level of confidence attributed by the system to the information extracted from the context. with this incorporation  a couple of improvements are expected. firstly  the experiments that would be required to establish the best λ are avoided. secondly  the new weights s are guaranteed not to be outperformed by the default scoring value d. in fact  if the context function turns out to be deleterious for term ranking  λ can be set to 1 and hence the weighting system replicates the ranking performances of d. during the experiments this case has not been observed.
1 evaluating term weighting performances
the criterion to evaluate a term weighting system depends on the specific task that is addressed by the application. in qa-like problems  as the one proposed in this paper  the performance of the system depends on the position that the correct answer assumes in the candidate answer list. two positional and cumulative measures can be used for qa evaluation: the mean reciprocal rank of correct answers  mrr   which is a standard evaluation measure in the qa community  and the success rate  sr  at n  in which we annotate the probability of finding a correct answer within the first n candidates. formally  given a set of questions q = {quest 1  ... quest q  ... quest n } and denoted by pos aq  the position of the correct answer aq for question quest q   we have
mrr 1 
	sr 	 1 
where Θ is heaviside function defined by Θ x  = 1  if x − 1  and Θ x  = 1  otherwise.
　the above performance measures can be used both for evaluating the weighting system and for implementing the learning procedure. a drawback of using positional measures is that they are discrete functions and thus non differentiable. in fact the values of mrr and sr change abruptly when the positions of two words are exchanged. a possible approach could be to adopt a learning scheme robust to discrete environments  e.g. simulated annealing or genetic algorithms. however  a major problem of these algorithms is their stochastic nature that makes the convergence very slow.
　in order to adopt a differentiable error function  we defined a differentiable function soft pos that replaces the concept of position pos

where w is the set of words considered by the weighting algorithm  a is the word whose position value has to be calculated  and γ is a predefined parameter. moreover  σ is a sigmoid function  i.e. a monotoneincreasing differentiablefunction such that limx★± σ x  = 1 and limx★ ± σ x  = 1.
　each term contained in the sum of eq.  1  compares the score of word a with the score of another word w. if γs w  is very larger than γs a   then the term assumes a value close to 1  and  in the converse case when γs w  is very smaller than γs a   the term approaches 1. thus  for large γ  each terms approaches a heaviside function and softpos a  approximates pos a .
　based on eq.  1  and  1  we defined a differentiable evaluation function which approximates mrr:
softmrr
　　　　　　　　　　　　　　　　　　　　　　　 1  once the system is trained using this soft function  the evaluations can be done using the standard mrr  or sr .
1 experimental setup
1 the single-word qa problem
the problem on which the proposed approach was applied is a special case of question answering. in single-word qa each question has to be answered with a single correct word  given a number of documents related to the question. since the target answer is a unique correct word the performance evaluations  unlike standard qa  do not require any human refereeing. to the best of our knowledge no standard dataset is available for the task of single-word qa  but a very popular and challenging example is provided by crosswords. since crossword clues are intrinsically ambiguous and opendomain  they represent an very challenging reference point. it is worth mentioning that single-word qa was used as a testbed since it is particularly suited to evaluate term weighting. actually  the answers of the qa system are directly dependent on the ranking produced by term weights.
　the aim of our experiments is to show that the contextbased algorithmimprovesthe ranking quality of the candidate answers provided by webcrow  ernandes et al.  1 . webcrow is a web-based qa system designed to tackle crossword clues for crossword cracking. at the core of this system there is a web search module  which  given a clue  retrieves
namefeature setfs-aidf w   idf u   dist w   u  fs-bidf w   idf u   dist w   u    dist u q 	 fs-b*idf w   idf u   dist w   u    dist u q 	   sw-listtable 1: the set of features inserted in the context functions used for the experiments. dist w   u   is the number of separating words between occurrence w  and u . dist u q    is the number of separating words between u  and the closest occurrences of the words of query q. sw-list indicates that a stopword list is used in order to avoid the presence of stopwords within the context window.
useful documents using a search engine  currently google  and extracts the candidate answers. these words are then ranked using statistical and morphological information.
　in the experiments we extended webcrow with the addition of the the context-based algorithm. the results show that the ranking of the correct answers is improved.
　our dataset consisted of 1 italian crossword clues1 randomly extracted from the archive in  ernandes et al.  1 . the dataset has been divided into two subsets. on one side  ne-questions  1 examples   whose answers are exclusively factoid named entities  e.g.  real-life comic played in film by dustin hoffman: lennybruce. on the other side  nonne-questions  1 examples   whose answers are nonnamed entities  common nouns  adjectives  verbs  and so on  e.g.  twenty four hours ago: yesterday .
1 weighting techniques under comparison
in order to assess the effectiveness of the context-based algorithm we compared its ranking performances with three different weighting techniques. the first is tfidf  which gives a statistical evaluation of the importance of a word to a document. the other two techniques  webcrow-s and webcrow-sm  represent two ranking methods adopted by webcrow. webcrow-s exploits only statistical information of the words  the documents and the queries. this weighting scheme is a modified version of the one presented in  kwok et al.  1 . webcrow-sm additionally uses a morphological analysis of the documents joined with a morphological answer type classification of the clues. for all the details about webcrow  see  ernandes et al.  1 .
1 experimental results
train set and test set. the two subsets  ne and nonnequestions  were randomly divided into a train set and a test set  containing 1% and 1% of the examples respectively. the experiments were repeated 1 times for each configuration. each example of the data set contained: i  a question/answer couple; ii  a set of h documents retrieved by google  the documents are selected in google's order ; iii  the vector of ranked terms w  the candidate answer list  extracted from the h documents. in our experiments w contains words of any length.
features. several contextfunctions were trained  each one with a different set of features. table 1 presents the list of feature combinations adopted in the experiments. to fairly
learning curves  mrr 

figure 1: learning curve of the context function on the nequestion problem with softmrr and the fs-b feature configuration. webcrow-s is outperformed after 1 epochs. after 1 epochs no sensible improvement can be observed.
success rate at n

figure 1: success rate at n for the ne-questions test set. the curves depict the probability  x-axis  of finding the correct answer to a question within the first n candidate answers  y-axis . for clarity  the curve of webcrow-sm is not reported.

default twfeature setnenonnetfidf11tfidf + contextfs-a1-webcrow-s1-webcrow-s + contextfs-b1-webcrow-s + contextfs-b*1-webcrow-sm11webcrow-sm + contextfs-b*11table 1: mrr performance.the results of the three baseline algorithms are given in bold. column default tw denotes the algorithm used for the default term weights dw  feature set specifies the feature set of the context function. the last two columns contain the mrr values obtained by the system on the two subsets of the testset.  context  indicates that the context weighting algorithm is used.
compare the context-based weighting with the other techniques the features were restricted to those effectively used by each algorithm under comparison. for example  to reweight tfidf scores we adopted a feature set  called fs-a  tab. 1  first row   that exploits no information about the question. in this work no morphological information was integrated in the context functions. nevertheless  these were able to improve also webcrow-sm's performances  which adopts morphological analysis. for simplicity we chose 1 as a fix dimension for the context window  with kl = 1 and kr = 1.
initialization and stop policy. the parameters of the context functions were initialized with small random weights  with the only exception of λ  that was initialized to 1. each training session was stopped after 1 epochs. this stop policy was suggested by a prior observation of the variation curves over both the learning set and the test set  see fig. 1 . the resilient learning scheme guaranteed a fast convergence  requiring less than 1 epochs to reach good local minima.
exp 1: answering ne-questions. for the ne-questions problem the context functions were trained and tested using h = 1 as the number of documents used for extracting the word candidates. the results are presented in table 1  which shows the performances in terms of mrr. the ranking performances of the compared methods are given in bold at row 1  tfidf   1  webcrow-s  and 1  webcrow-sm .
　the context-based algorithm outperformed the compared weighting methods. in particular  the reweighting of the default scores provided by webcrow-s  produced a 1% increment of the mrr  from 1 to 1  row 1 .
　an insightful resume of the performance improvements is provided by the success rate curve  fig. 1   that shows the probability of finding the correct answer within the first n words of w. each weighting scheme under comparison appears consistently below the curve of its contextbased reweighting. in particular  with n = 1  the observed success rate was: tfidf  1%; tfidf+context  1%; webcrow-s  1%; webcrow-s+context  1%; webcrow-sm  1%; webcrow-sm+context  1%.
exp 1: answering nonne-questions. since the nonnequestions representa moredifficult task  all the contextfunctions were trained and tested with a higher number of documents  h = 1. the improvement  tab. 1  of the ranking quality provided by the context-based algorithm was less evident than on the ne subset  but still clear. this phenomenon depends on the different nature of these questions  which make improbablea strong increment of the mrr using exclusively statistical features  as revealed by the poor performance of the tfidf measure  first row . the reweighting of the term scores provided by webcrow-sm lead to a 1% increment of the mrr  from 1 to 1  row 1 and 1 .
exp 1: varying the number of the web documents  h .
an interesting feature to evaluate is the ability of the context function to work with a number of documents that differs from that used in the learning phase. for this reason we tested the ranking quality of the context functions trained over the ne subset  with dw given by webcrow-sm  last two rows of table 1   varying h. the context-based algorithm proved to be robust to the changes of h  constantly outperforming webcrow-sm  fig. 1 . with h = 1  the addition of the context increased the mrr by 1%  from 1 to 1 .
an example. an example can help to better illustrate these promising results. webcrow-sm answered the question:  la paura nella folla: panicofear among the crowd: panic   with the correct answer in 1-th position. with the addition of the context-based weighting  panic jumped to 1nd position  and other related words appeared among the first 1 candidates  as: collective  multitude  emergency and irrational. a passage extracted from one of the documents may explain why the context information can result ef-
mrr varying the number of documents

figure 1: the mrr performance of the context-based algorithm over ne-questions  in comparison with webcrow-sm  varying the number of docs used in the test set.
fective:  ...the panic is an irrational behavior of the crowd ... . it is clear that the words panic and irrational can be strongly reinforced by the contextual presence of other relevant words  like behavior and crowd. 1 computational cost
the main drawback of the context-based weighting method is due to the additional time cost introduced by the learning step  that is linear in the number of examples and in the number of iterations required for learning convergence. nevertheless the training of the context functions can be successfully obtained with a small set of examples  as proved in the previous paragraphs  noticing that the dimension of the training set is independent with respect to the test set. the latter could be several order of magnitudes larger without requiring an increment of the learning set.
　leaving aside the learning phase and the initialization of the data used by the context function  i.e. word distances  idf  etc.   the theoretical computational requirement of the context-basedweighting algorithmis o |w  |〜 kl+kr 〜t   where |w  | is the total number of word occurrences in the document set   kl+kr  provides the dimension of the context window and t is the number of iterations that are required by the system to converge. in theory  the convergence of the jacobian algorithm  sec. 1  is exponential. we experimentally observed  fig. 1  that the system converges in a small number of iterations  often requiring less than four steps to reach a maximum weight modification rior to 1. thus  t does not introduce a great overhead.
1 conclusions and further work
in this paper we have proposed a novel term weighting method that exploits the information provided by the contexts in which the terms are found. we have defined a parametric function  called context function  that models the influence exercised by a word on another one that participates in the same context. furthermore  we have presented a learning environment that allows us to adapt the parameters of the context function to the training data. the approach has been proved to be effective on the problem of single-word question answering  improving the answer-ranking performances of three different term weighting schemes. future matter of research includes the application of the context-based term weighting to other fields  as document classification and information extraction and the use of more complex implementation of the context functions.
convergence of the algorithm

number of iterations
figure 1: the convergence of the algorithm  a straight line on a log scale  is exponential. we report both 1-norm and ±-norm of the score difference  between two subsequent iterations.
