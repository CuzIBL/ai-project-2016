 
     this paper describes a natural language system which improves its own performance through learning. the system processes short english narratives and is able to acquire  from a single narrative  a new schema for a stereotypical set of actions. during the understanding process  the system attempts to construct explanations for characters' actions in terms of the goals their actions were meant to achieve. when the system observes that a character has achieved an interesting goal in a novel way  it generalizes the set of actions they used to achieve this goal into a new schema. the generalization process is a knowledge-based analysis of the causal structure of the narrative which removes unnecessary details while maintaining the validity of the causal explanation. the resulting generalized set of actions is then stored as a new schema and used by the system to correctly process narratives 
which were previously beyond its capabilities. 
	i 	introduction 
     a natural language system requires extensive knowledge about the world clearly  if a computer system is to summarize  translate  or answer questions about a text  it must have knowledge about the concepts expressed in the text. imagine trying to process a narrative describing a bank robbery without knowledge of money and why people want it. this is a conceptual rather than linguistic requirement  and it means that at the heart of a natural language processor there must be a problem solver to infer missing but important concepts  to insure that the narrative phrases are causally related in an appropriate way  and perhaps to guide the linguistic processing  dejong1l 
     schema-based problem solvers  charniak1  minsky1  schank1  have proven themselves more workable for natural language processing applications than their heuristic search counterparts. in order to process a wide range of text  a schema-based natural language processor must possess many schemata  perhaps hundreds of thousands. this presents both practical and theoretical problems. somehow these schemata must find their way into the system. they cannot all be built in by hand; there are simply too many. furthermore  hand coding does not allow for dynamic augmentation of world knowledge. this is an important facet of language processing. for example  the word iiskyjacking   is now an accepted newspaper term but was unheard of twenty years ago. readers have learned it as a by-product of their normal newspaper reading and natural language processing systems must be able to do the same. 
     we have taken the first steps in this direction at the university of illinois. a natural language processing system called genesis  for generalizing explanations of stories into schemata  has been designed and implemented which acquires new schemata 
     this work wa* supported in pin by the air force office of scientific research under gram f1o-1-k-1 ind in pan by the national science foundation under graninsf-ist-1. 
in the normal course of processing narratives.1 after acquiring new schemata  the system is able to correctly process narratives that were previously beyond its capabilities. 
     we call the learning process used by genesis explanatory schema acquisition  dejong1 . it is a form of explanation-based learning  dejong1l which can be briefly defined as learning a new problem solving method by analyzing the causal structure of a problem solution. the system is fully implemented and an example sequence demonstrating the system's learning is given later in the paper. a longer version of this paper appears as  mooney1l 
	d 	general system organization 
     the general organization of the genesis narrative processing system is shown in figure 1. first  english input is processed by a 
     parser into a conceptual representation  crep   a case-frame representation which uses some conceptual dependency primitives  schank1  as well as predicates for complex schemata. currently  we are using an adaptation of dyer's mcdypar  dyer1  for this purpose; however  since the focus of our research is learning  we make no claims about parsing and alternative approaches could be used for this task  eg.  marcus1  waltz1  . 
     the basic task of the understander is to construct a causally complete representation called the model a model for a narrative has explicit representations for all the inputs as well as the many inferences that must be made to causally connect them together. there are four types of causal links for connecting assertions in the model of a narrative. these are: precondition: a link between a state and an action it enables. effect. a link between an action and a resulting state. 

lthe use of the term  itory  in natural language preening ha* been controver 
tial  brewer1  therefore  in this paper we have adopted the term  narrative  to refer to connected text which may tack a plot or other defining aspect of a  story.  

1 	r. mooney and g. dejong 
motivation: a link between a volitional action and the beliefs and goals of the actor which motivated him to perform the action. inference: a link between a state and another state which it implies. to avoid confusion  such  causal  links between assertions in the model will be called support links  since a precondition of an action supports the performance of that action but does not cause it the closely related term data dependency link  doyle1  is not used since it is normally reserved for the support of inferences  not for the support of both inferences and actions. inferring causal connections necessarily employs a large amount of background knowledge which is stored in the schema library. the techniques and representations used in this process are similar to those used in past work in narrative understanding  charniak1  cullingford1  dejong1  dyer1  wilensky1  and are discussed in sections iv and v. 
　　　in order to demonstrate the abilities of the understander  a simple question answering system is used to inspect the model. since our interests lie in guiding the generalization process through the use of causal relationships  this subsystem is primarily used for accessing the reasons why an actor performed a certain action or why a particular state exists. this information is easily retrieved by inspecting the support links between the various states and actions in the model. of course  there are many issues involved in retrieving the most appropriate answers to questions  see llehnert1   which we do not fully confront in this subsystem. a simple natural language generator for translating replies into english is also included as part or the system. 
　　　if an actor in a narrative achieves an important goal through a novel combination of actions  the explanation for how the goal was achieved is generalized into a new schema. the combination of actions which supports the achieved goal state is generalized as far as possible without breaking any of the connecting support links or violating the well-formedness of individual actions and states. this generalized structure is then stored as a new schema in the library where it is used to facilitate the processing of future narratives. this generalization process is discussed in section vi and is the key to the learning technique of explanatory schema acquisition. 
	ill 	an example 
　　　currently  genesis has acquired two new schemata. in one example  the system learns a schema for someone kidnapping an individual and holding them for ransom. in the other  it learns a schema for someone burning his own building to collect the insurance. here we will show the performance of the system on the kidnapping example. before processing the following narratives  genesis contains information in its schema library about bargaining  capturing and confining individuals  threatening  and many other concepts; however  it does not have a schema for kidnapping for ransom. first it receives the following  test narrative: 
input: ted is the husband of alice  he won $ 1 in the lottery. bob imprisoned alice in his basement bob got 
$1 and released alice  
processing story. 
finished processing. 
no thematic goals achieved by a novel combination of actions: no generalization. 
ready for questions: 
   who gave bob the money  answer unknown. 
   why did bob lock alice in his basement  cannot find sufficient reason for his action. 
   why did bob release alice  
cannot find sufficient reason for his action. 
notice that in this narrative it is not mentioned how bob got the money or why bob imprisoned alice and then released her. since the system does not have a schema for kidnapping  it cannot infer the missing information and construct a causally complete explanation of the narrative. next  it is given the following narrative. 
input: fred is the father of mary and is a millionaire. john approached mary. she was wearing blue jeans. john pointed a gun at her and told her he wanted her to get into his car. he drove her to his hotel and locked her in his room. john called fred and told him john was holding mary captive. john told fred if fred gave him $ 1 at trenos then john would release mary. fred gave him the money and john released mary. 
processing storyfinished processing. 
john achieved the thematic goal: john has $ 1. 
generalizing- assembling new schema: capture-bargain. 
ready for questions: 
1
  why did john imprison mary in his room  
so john and fred could make a bargain in which john released mary and fred gave john 1 dollars at trenos restaurant 
   why did john make the bargain with fred  
because john wanted to have 1 dollars more than he wanted to hold mary captive. 
1
  why did fred make the bargain with john  
because fred wanted mary to be free more than he wanted to have 1 dollars. 
   why did fred want mary free  because fred was mary's father. 
unlike the first narrative  this one is detailed enough to allow genesis to causally connect the individual actions. the resulting causal structure is then generalized into a new schema of kidnapping for ransom  which genesis calls capture bargain . next  the system is given the first narrative again  and using the schema it has just acquired  it is able to infer the missing information and causally connect the actions. consequently  it is able to answer the questions which previously it could not answer. 
input: ted is the husband of alice. he won $ 1 in the lottery. bob imprisoned alice in his basement bob got $1 and released alice. 
processing storyfinished processing. 
no thematic goals achieved by a novel combination of actions no generalization. 
ready for questions: 
   who gave bob the money  ted gave bob 1 dollars. 
   why did bob lock alice in his basement  
so bob and ted could make a bargain in which bob released alice and ted gave bob 1 dollars. 
   why did bob release alice  
because bob and ted made a bargain in which bob released alice and ted gave bob 1 dollars. 
	iv 	knowledge representation 
     genesis* knowledge is represented in a library of schemata: packets of general information about stereotypical objects  situations  and actions. all schemau in the library are arranged in a hierarchical inheritance net under the three major classes of action  state  and object  the highest level class is simply called 
schema . each schema has a set of roles associated with it which can be filled by other schemata to create an insunce of the schema. the type of information associated with a schema depends on whether it is an action. state  or object so each of these will be discussed in turn. 
　　　action schemau represent dynamic events which change the sute of the world. the following pieces of information are atuched to action schemau. in addition to the information atuched directly to a particular schema  each action inherits the information atuched to actions above it in the abstraction hierarchy. 
role constraints: each role is marked with the type of schema which can legally fill it. defaults default fillers can be specified for each role. preconditions: states which must be true in order for the action to uke place. motivations: sutes  beliefs and goals  which explain why an actor would perform this action. effects: sutes which are true after the action is performed. terminations: sutes which are no longer true after the action is performed.  these are similar to the delete-lists in strips but sutes are temporally marked as no longer holding instead of being deleted from the model.  expansion schemau: a set of lower-level sutes and actions which actually make up this action along with the support relationships between them  similar to the body of a script . suggested schemata: larger composite actions which this ac-tion may be a part of. 
determining conditions: a set of lower-level actions and sutes which if all present indicate the occurrence of this action. 
     states  on the other hand  represent relatively sutic situations in the world  such as an individual being someone's father or being in possession of some object the following pieces of information are atuched to state schemata. in addition to the information atuched directly to a particular schema  each state inherits the information atuched to states above it in the abstraction hierarchy. 
role constraints: 	each role is marked with the type of schema which can legally fill it 
defaults: 	default fillers can be specified for each role. 
inferences: 	other sutes which are reasonable inferences 
to make from this sute. 
achieving actions: 	actions which can be used to achieve this sute. 
     objects represent types of things in the world the information atuched to object schemau varies from class to class. cornmon examples for physical objects would be defaults for size  shape  and other physical attributes. 
	v 	the understanding process 
     since applying explanatory schema acquisition depends on having a causal chain of actions to generalize  the  undemanding  ability of genesis is concentrated on constructing this chain by inferring missing information and causally connecting inputs together. we do not attempt to deal with other important issues which have recently occupied researchers in narrative undemanding such as plot units  lehnert1l thematic abstractions units  dyer1  story points  wilensky1l affect  dyers1l and integrated parsing  dejong1  dyer1 . 
　　　in accomplishing the usk of constructing causal connections  genesis  like faustus  norvig1  wilenskys1l uses a combination of top-down and bottom-up processing techniques. if a set of inputs in a narrative matches a schema which the system already has  then it uses top-down processing to fill in the expansion of this schema with the particular inputs of this narrative  much like a script driven program such as sam  cullingford1  or frump  dejong1 . however  if an action in the narrative is not explained by a known schema  it attempts to conned it 1 other actions and sutes in the narrative by searching for existing sutes which fulfill the preconditions for this action  or by hypothesizing intermediate actions which causally connect it to existing sutes or actions. in this way  it also operates in a more bottom-up fashion like planbased programs such as pam  wilensky1 . 
a. schema activation and determination 
　　　if a schema-based system is to be able to process a range of possible inputs  it must have access to a large number of schemata. therefore  in order to avoid repeated searching through the entire daubase of schemata  it must also have an efficient method for selecting the particular schemau which are applicable to the current input several researchers have addressed this difficult problem  charniak1  dejong1  norvig1  and below is a brief description of the approach genesis uses. 
　　　when genesis processes an input  it adds it to the model and activates all the schemau in the list of suggested schemata atuched to the schema class of the input. active schemau then monitor subsequent inputs and check if they match parts of its expansion and can therefore be considered part of this active schema. when all the determining conditions of an active schema are met  it is determined or considered to have occurred in the narrative and is added to model along with the schemau and support relationships given in its expansion.1 if a determining condition is an action  then it is also considered to have occurred if all of its effects are in the model. 
b. bottom-up construction of support relationships 
　　　when a new schema insunce is added to the model  either as the result of an input or an inference on the part of the system   the system first tries to explain it as part of a known schema. however  if the new insunce does not suggest any higher-level schemau nor match part of any already active schemata  then genesis tries to causally connect it to other actions and sutes in the model using planning information. 
     the first step in integrating a new schema insunce into the model is to add any primary inferences or effects. the effects and inferences atuched to a schema are divided into primary and secondary categories. primary ones are used in a forward inferencing fashion while secondary ones are used in a backwards inferencmg fashion and only added to the model if they are required by the explanation. 
　　　if the new insunce is an action  then its preconditions must be reconciled with the model. this means that it first searches the model for each precondition and if it finds it  it adds an appropriately labeled support link from it to the new action. if it does not find a precondition  it next attempts to infer it by searching for secondary effects or inferences which match this precondition. if this also fails  it hypothesizes the existence of an action which can be used to achieve this precondition  using the achieving actions atuched to this sute  and attempts to reconcile its preconditions with the model. 
genesis also attempts to find motivations for volitional 
1 	r. mooney and g. dejong 
able to construct the support network shown in figure 1. it should be noted that the support networks shown in this paper  called highest-level support networks  contain only the highest level schemata which were determined to be in the narrative. most of ihe representation at the level of the inputs and their connecting inferences is contained in the expansions of the capture and bargain schemata which were activated bottom-up from the inputs. 
     as indicated earlier  this structure is then generalized into a new schema. when the first narrative is processed again  bob's action of imprisoning alice in his basement determines a capture schema  and this in turn suggests the new  kidnap  schema. the new schema is then used in a top-down fashion to fill in missing information. it is finally determined when both of the effects of the bargain: alice becoming free again and ted receiving money  are added to the model. the final support network  the expansion of the new schema for this narrative  is shown in figure 1. this causal structure allows the system to answer the questions it could 
not answer before learning the schema. 
	vi 	the generalization process 
once a causally complete explanation has been constructed figure 1: support network for narrative #1 after learning 
to learn a useful new schema. if so  it generalizes the causal structure in the model into a new schema and stores it in the schema 
library where it can be used in the processing of future narratives. 
a. 	when to generalize 
     if every combination of actions the system encountered was generalized into a new schema  the system would soon become overloaded with rarely used schemata. most actions would activate a large number of schemata and selecting among these would require 
an excessive amount of processing time. in order to avoid this problem  certain conditions must be met before a combination is generalized. 
     tim  the combination of actions should achieve a goal for one of the characters in the narrative. in the process of motivating actions  the understander checks if an action achieves a goal for a 
     character  so finding achieved eoals is a simple matter of inspecting 

the model. second  this goal should be a common one which is likely t   be encountered again. a goal is considered to be common enough if it is a thematic goal  for example satisfying hunger or acquiring money. since in the sample narrative john achieves the thematic goal of possessing money  it satisfies both of these conditions. 
     the final condition for generalization is the obvious one of not already possessing a schema for the combination of actions which achieves the thematic goal. this simply involves checking the highest-level support for the achievement of the goal and making sure it contains a combination of actions. if the system already had a schema for this case  it would have used it in processing the narrative and the goal would be supported by an instance of this schema instead of a combination of actions. additional conditions for generalization were discussed in  dejong1i however  these are currently not implemented. 
b. 	generalizing the support network 
　　　after deciding to generalize  the system extracts the explanation for the goal state  isolating the actions and states which actu-
ally contribute to its achievement. this simply involves extracting the highest-level support for the achieved state. in the example  the achieved state is john possessing $1 and the support for this state is shown in figure 1. this step eliminates extraneous information in the narrative which does not contribute to the achievement of the goal  such as the fact in the example that mary was wearing blue jeans. 
　　　once the support network is extracted  there are several steps involved in constructing a generalized version of this causal structure. the overall approach is to initially generalize as far as possible and then re-introduce only the constraints necessary to maintain the causal connections between schemata and the wellformedness of individual schemata. initially  the class of each schema instance is generalized to schema  the highest level in the hierarchy  and each role filler is replaced by a unique new parameter. constraints are then imposed on this over-generalized structure to make it a well-formed causal network. these constraints progressively refine the class of each instance and constrain certain role fillers to be equal. 
　　　first  the goal which the support network achieves is constrained to be a thematic goal. this is accomplished by constraining it to match the pattern for the thematic goal which was achieved in the original narrative. in the kidnapping example  this constrains the goal state to be the kidnapper acquiring money. 
　　　next the inierschema constraints are imposed. these involve maintaining the validity of each connecting support link in the network. we will use the kidnapping narrative to illustrate this process by showing how the father relationship in its support network is only constrained by the explanation to be a positive-ipt  for positive-interpersonal-theme  a superclass ofprarent  spouse  etc. . every time a support link is added during understanding  it is annotated with the pattern from the schema library used to construct it and the class in the schema hierarchy where it was inherited from. in the example  when genesis infers that fred wants mary free more than he wants to have $1 as a secondary inference from the fact that he is her father  it annotates with the corresponding inference pattern from the schema library and the fact that this inference was inherited from the schema positive-ipt. when the interschema constraints for this link are imposed  the ransom payer's goal-priority is constrained to match the system's inference pattern  and the instance which was a father state in the original narrative is constrained to be a positive-ipt. thus  the system only imposes the required relationship between the individuals filling the roles of kidnap victim and ransom payer. the fact that there was a specific father-daughter relationship in this particular narrative is recognized as incidental and not crucial in maintaining the validity of the explanation. 
     next  the intraschema constraints are imposed. these concern maintaining the well-formedness of each individual schema instance. this is accomplished by imposing on the filler of each role the appropriate role constrain! from the schema library. for example  since the role constraints specify that the subject of a possess schema be a person  the subject role filler of each possess schema in the support network is constrained to be a person. 
　　　the final step in constructing a generalized support network is to merge parameterized instances which have been constrained to be equal. the resulting instances form a set of conceptual roles for the overall schema. in the example  this collects together all the individual occurrences of the kidnapper  the victim  the ransom payer  and the ransom money and creates a unique object for each one. 
　　　the result of this generalization process is a general causal structure which achieves a common goal. the generalized support network generated for the kidnapping example is shown in figure 
1. 

c 	packaging into a new schema 
　　　the final step in acquiring a new schema is separating the generalized support network into preconditions  effects  expansion schemata  etc.  which can be added to the schema library. following is an outline of how this information is extracted 
roles  with constraints : the subject of the achieved thematic goal becomes the actor of the new schema. new roles are created for each remaining person and object in the generalized support network. in the example  roles are created for the kidnapper  victim  ransom payer  and ransom money. 
preconditions: states which are leaves of the generalized support network but not motivations of actions by the main actor. in the example  the ransom payer possessing the ransom money is a precondition. motivations: states which are leaves of the generalized support network and motivations of 
actions by the main actor. in the example  the kidnapper wanting to have money is a motivation. 
1 r. mooney and g. dejong 
effects: effects of all actions within the generalized support network which are not terminated by other internal actions. in the example  the kidnapper possessing the ransom money is an effect terminations: states which are terminated by an action within the generalized support network but not produced by another internal action. in the example  the ransom payer no longer possessing the ransom money is a termination. 
determining conditions: the set of all actions within the generalized support network. in the example  the bargain between the ransom payer and the kidnapper is a determining condition. 
expansion schemata:  all remaining schemata in the generalized support network along with their connecting support relationships. suggested schemata: the schema for each action within the generalized support network is marked as suggesting the new composite action. in the example  the schema capture now suggests the new schema. 	vii 	relation to other work 
　　　the process just described uses a database of background knowledge to generalize the causal structure or explanation of a single example. this approach differs dramatically from most approaches to learning  e.g.  michalski1  mitchell1  winston1   in which generalization is accomplished by extracting features which are shared by a number of examples. 
　　　genesis* generalization process is most similar to the method used by strips to generalize planning sequences into new macrops  fikes1  however  unlike strips  genesis generalizes actions and states as well as objects and locations  and generalizes the order of independent actions  since it uses a dependency network instead of a linear ordering of steps . 
　　　the general technique used by genesis  explanatory schema acquisition  is also being applied to learning theorem proving strategies  1'rorke1l robot assembly tasks lsegre1l and concepts in physics problem solving  shavlik1 . explanatory schema acquisition is closely related to a growing body of recent work in explanation-based or analytic learning  minton 1  mitchell1  silver1  winston1  which is characterized by learning from a single example through the analysis of its causal structure. 
　　　however  there are also important differences between explanatory schema acquisition and some of the other work referenced above. while genesis learns from the problem solving behavior of other agents  strips and mitchell's lex learn from their own problem solving. although learning from external behavior makes a system less autonomous  it allows a system to learn plans which are beyond its own ability to generate. in addition  lex only learns heuristics for applying operators it already possesses and not new 
combinations 	of 	operators 	which 	achieve 	important 	goals 
 macrops or schemata . although winston's system  like genesis  learns from short narratives  it learns if-then rules and not schemata. in addition  winston's system does not need to infer causal connections during  understanding  since all causal connections are given explicitly in the input text 
viii conclusion 
　　　unlike most learning systems  explanatory schema acquisition does not depend on correlational evidence. thus  it is capable of one trial learning. also  it avoids the problem of searching through a large space of features for ones which are relevant to a new concept. only features which contribute to the explanation of an achieved goal are considered for inclusion in the description of a concept. the approach is heavily knowledge-based; a great deal of background knowledge must be present for learning to take place. finally  the system does not increase its representation power with this kind of learning. the learning results in greatly improved efficiency of processing by avoiding combinatonally explosive searches. 
　　　in the future we plan to address the issue of schema refinement. clearly  the system ought to have the capability of refining existing schemata if the svstem is presented with an example which violates its expectations. we also hope to explore language learning. it should be possible to acquire the english names for these new problem solving schemata from context. this direction of parallel and interacting language and concept development should complement existing work on inducing grammars 
 berwick1  and learning to attach new names to known concepts iselfndgem . 
acknowledgements 
　　　we would like to thank paul ororke and alberto segre for helpful comments on various versions of this paper. 
