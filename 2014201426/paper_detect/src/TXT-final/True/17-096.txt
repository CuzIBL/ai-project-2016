 
introspection is a general term covering the ability of an agent to reflect upon the workings of his own cognitive functions. in this paper we will be concerned with developing an explanatory theory of a particular type of introspection: a robot agent's knowledge of his own beliefs. the development is both descriptive  in the sense of being able to capture introspective behavior as it exist; and prescriptive  in yielding an effective means of adding introspective reasoning abilities to robot agents. 
1 	introduction 
introspection is a general term covering the ability of an agent to reflect upon the workings of his own cognitive functions. in this paper we will be concerned with developing a theory of a particular type of introspection: an agent's knowledge of his own beliefs. there are at least two reasons why it is important to develop such a theory  one descriptive and the other prescriptive. as collins and his coworkers have shown  in    an agent often reasons about his own beliefs and nonbeliefs in deciding the answer to a posed query; hence a descriptively adequate account of agents' beliefs must deal with introspection. the second reason is that researchers attempting to build artificial agents must imbue these agents with introspective knowledge if they are to act in an intelligent manner. moore  gives the example of an agent who must introspect about his beliefs in order to form a correct plan to achieve a goal. 
　in this paper we offer an explanatory theory of belief introspection based on the concept of a belief subsystem as developed in konolige   . put simply  a belief subsytem is the computational structure within an artificial agent responsible for representing his beliefs about the world. because the belief subsystem is  at hand  and available to the agent  it is possible for the agent to gain knowledge of his beliefs by simply making recursive calls to this belief subsystem  perhaps with ever-decreasing resource allocations. this  in a nutshell  is the model of introspection we adopt. its advantages are that it is an 
this research was made possible in part by a gift from the system development foundation. it was also supported by grant n1-c-1 from the office of naval research. 
adequate explanatory theory of belief introspection  and that it is immediately prescriptive: the theory shows how artificial agents that exhibit introspective reasoning of the requisite sort can be built. 
　given the importance of introspective reasoning  it is perhaps surprising that the problem of finding a good explanatory basis for belief introspection in artificial agents has scarcely been addressed. in section 1 we review two approaches that differ from ours in being nonconstructive: an ideal agent's introspective reasoning is defined by putting constraints on her belief set. the disadvantage of such nonconstructive theories is that  in general  they do not extend to the case where an agent's reasoning powers are bounded by resource limitations. 


	k. konolige 	1 

                                                                 space requirements preclude more than sketches of most proofs in proposition 1 suppose m is an introspective belief sub- this paper. 

1 	k. konolige 

proposition 1 if the underlying language is propositional  and its base $et i$ nondoxastie  an ideal introspective belief subsystem is decidable. 
　the proof here is straightforward: any query will have a finite maximum embedding n of self-belief operators. one need only look at the  decidable  theorems produced by the first n levels of the introspective machine. as long as queries do not include any quantification into the context of the self-belief operator  we can extend this result to any underlying language which can be decided by reduction to the propositional calculus. for example  monadic predicate calculus  pc  and the class of sentences have this property. 
　these two propositions to some extent delimit the nature of decidability for introspective subsystems. a natural question to ask is if proposition 1 can be extended to the case of any decidable underlying language. the answer to this has important consequences for adding introspective ability to artificial agents  because these agents are  nonintrospectively  decidable: they must answer a belief query in a finite amount of time. 
proposition 1 // the underlying language is monadic pc  and its base set is nondoxastic  an ideal introspective belief subsystem is decidable. 
the proof of this proposition relies on kripke's result in 
 that monadic modal pc is not decidable. the difference between monadic modal pc and propositional modal languages is that the former allows quantifying into the modal context. as we mentioned  queries without quantifyingin are decidable for monadic pc. thus the presence of quantifying-in seems to pose an inherently difficult computational problem for introspective systems. yet the expressivity of quantifying-in is desirable in many applications; levesque  gives the example of a question-answering system in which sentences of the form express the fact that there are individuals with property p whose identity is unknown to the database. 
　proposition 1 is discouraging  since it means that in constructing introspective agents  we must either use a very weak underlying language  or give up some of the three conditions of ideality. we discuss the latter method in the next section. note that even without proposition 1  there are reasons for developing the theory of non-ideal agents. first  even with a very weak underlying language and a decidable subsystem  an agent may have limited resources for derivation of beliefs  and can only compute an approximation to the conditions of definition 1. second  we mentioned that human agents are not always ideal agents  and we would like to model their cognitive behavior. 
1 	real agents 
in figure 1  a belief subsystem had to respond either yes or no to every query. in a computational setting with finite resource bounds  it may not be possible to do this in a consistent way. for example  if the underlying language is pc  there are some  nondoxastic  queries that do not have a derivation  and hence the belief subsystem should respond no; but there is no algorithm for determining this in a finite amount of time. to accommodate this situation  we allow a subsystem to return und  undecided  as one of its answers. 
　let r be a resource bound. if m derives a query within this bound  we write yes; if it decides that is not derivable  we write :no; and if it cannot 

	k. konollge 	1 


1 	k. konolige 

　these two relations are the only interdependencies of the constraints. there are thus nine distinct combinations that can be arranged in a lattice as in figure 1. the arrows indicate domination relations; the constraint pfu+nfu is thus the strongest of the possible conditions on introspective belief  in the sense that every introspective belief subsystem that obeys it also obeys every other possible combination of the faithfulness and fulfillment constraints. note that positive fulfilled systems dominate negative faithful ones  and negative fulfilled systems dominate positive faithful ones. 
example 1 the use of introspective belief subsystems as a descriptive model of human belief will be illustrated with one example  drawn from hintikka . he argues that if someone believes she also believes that she believes it  at least in the absence of strict resource limitations on reasoning . this is our condition of positive fulfillment  where the resource r is always taken to be arbitrarily large  and we consider only the first level of introspection  m and im . hintikka goes on to argue that people will often have false ideas about their own beliefs  e.g.  an utterance of the form 
s believes that eke believes that  although she  1  does not believe it 
can be a true statement about the state of s's beliefs.1 in terms of the introspective model  we would say that human belief subsystems are not positive faithful  and hence not negative fulfilled . 
1 this is sentence 1 on page 1 of hintikka |s . 
　there is an additional curiousity to hintikka's theory. although the first level of introspection is characterized as being positive fulfilled but not necessarily positive faithful  it appears that subsequent levels are considered to be totally faithful. for example  the utterance 
1 believes the following: that she believes that  1  . she believes  although she does not believe it 
which is the statement of  1  as applied to s's idea of herself  is taken to be always false. in our introspective model  this is a statement about self-belief sentences of the introspective machine i m . to capture this behavior  we simply let im's concept of self-belief be positive faithful. 
1 	computational issues 
we now present some of our computational results on introspective machines. generally  we are interested in the problem of converting a nonintrospective belief subsystem into an introspective one; one can imagine retrofitting an existing knowledge base with a mechanism for reasoning about its own beliefs. the questions we pose will have the following form: given a particular introspective constraint  a point in the lattice of figure 1   and perhaps other conditions on nonintrospective behavior  can we implement a belief subsystem obeying these constraints  thai is  we would like to find an algorithm that will return a definite answer  yes or no  to every query  given the constraints  so that the introspective belief subsystem is decidable. we first make this notion of decidability precise for resource-limited agents. 

same as that of an ideal agent. note that a real agent is ideal only if she has an algorithm that will decide any query in the finite resource bound  real agents are always computational. 
　now let us assume the first two conditions of definition 1 hold  and explore the computational nature of belief systems obeying various introspection conditions. by  nondoxastic m  we mean that the base set of every belief subsystem of m is nondoxastic. 
proposition 1 let the introspection constraint be pfu+nfu 
// the underlying language is 
1. semidecidable  m is undecidable; 
1. propositional  nondoxastic m is decidable; 
s. monadic pc  nondoxastic m is undecidable. 
　this proposition just collects the results of the last section  propositions 1 with respect to real agents. note that  except in the case of a propositional language  m must return und for some queries  no matter what resources are available. in these cases  real agents are not even approximations of ideal agents  since there is no limit in which their behavior becomes the same. 
　now suppose we are given a nonintrospective belief subsystem m whose base set is nondoxastic   and we are asked to construct an introspective subsystem m' whose first component is m. we are free to choose the introspective components  as long as they satisfy conditions  1  and  1  of proposition 1. the following proposition tells us the best we can do in terms of satisfying various introspective constraints. 
proposition 1 suppose the underlying language of m is decidable. then if the introspection constraint is 
j. pfu+nfu  m' is undecidable; 
1. pfu+pfa  m' can be semidecidable; s. nfa+ pfa  m' can be decidable. 
　the first result is simply  1  of proposition 1. the second says that if we only want to enforce positive fulfillment and positive faithfulness  the best we can do is to construct an introspective subsystem that is semidecidable. and finally  if the introspection constraint is simple faithfulness  we can construct a decidable m'. of course  we can do better than this for particular underlying language*  e.f.  propositional   but there exists a decidable language for which these bounds are strict  namely  monadic pc . 
　let us put these results into perspective. if we are given a nonintrospective agent whose inference rules are complete and whose beliefs are decidable  the best we can do in retrofitting introspective reasoning is to make the agent's self-beliefs faithful. however  if we start with an agent whose rules are incomplete  or we are willing to give 
	k. konolige 	1 
up completeness  we can enforce stricter introspective constraints. but now these constraints are relative to a much weaker notion of belief derivation. for example  suppose an agent has no inference rules at all  so that her only nonintrospective beliefs are the base sentences. certainly we can form a decidable introspective belief subsystem in which pfu+nfu holds; is a belief if is a member of the base sentences  and is a belief if not  and membership in the finite base set is decidable. 
1 	comparison to related work 
our definition of an ideal introspective agent has many points of similarity with work by halpern and moses  and moore . in both these latter cases an underlying propositional language is used  and beliefs sets are defined nonconstructively as stable sets  stalnaker   although his original definition did not include consistency . 
definition 1 a stable set s obeys the following constraints: 

　now we would like an ideal rational agent's beliefs to be a stable set. to build an agent with ideally rational beliefs  we require favorable answers to the following questions. 
 a  given a sentence  that represents the initial beliefs of an agent  what is the appropriate stable set containing a that should be the belief set of the agent  
 b  is there an algorithm for computing it  
　the answer to  a  is not as simple as might be supposed  because it involves finding a stable set that includes and makes the fewest assumptions about what the agent believes in addition to  the presence of doxastic sentences in complicates matters  and indeed halpern and moses differ from moore in identifying an appropriate belief set. however  if is consistent and nondoxastic  both approaches converge on a single stable set. further  this stable set is identical to the belief set of an ideal introspective agent with base set  so that by proposition 1 there exists an algorithm for deciding membership in the stable set  the algorithm  of halpern and moses  decides the stable set in this case . 

1 	k. konolige 
relatively simple case of monadic pc and nondoxastic a  the question of membership in the stable set is undecidable. thus for these systems we must answer question  b  in the negative. 
1 	conclusion 
we have developed a theory of introspection based on the idea that an agent can use a model of her own belief subsystem to reason about self-belief. the theory can serve as a descriptive tool  since we can describe agents with varying degrees of self-knowledge; hence it may be useful to researchers interested in modelling the cognitive state of users  e.g.  in domains such as natural-language systems  tutoring systems  intelligent front ends to databases  and so on . the theory also is a guide to building agents with introspective capabilities  or retrofitting these capabilities onto existing artificial agents. 

　although this example is suggestive  we do not yet have any definitive results on the relationship between moore's autoepistemic theories and ideal introspective subsystems. 
