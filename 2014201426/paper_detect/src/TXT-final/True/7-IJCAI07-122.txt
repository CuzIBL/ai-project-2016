
we present a bayesian formulation of locally weighted learning  lwl  using the novel concept of a randomly varying coefficient model. based on this  we propose a mechanism for multivariate non-linear regression using spatially localised linear models that learns completely independent of each other  uses only local information and adapts the local model complexity in a data driven fashion. we derive online updates for the model parameters based on variational bayesian em. the evaluation of the proposed algorithm against other state-ofthe-art methods reveal the excellent  robust generalization performance beside surprisingly efficient time and space complexity properties. this paper  for the first time  brings together the computational efficiency and the adaptability of 'non-competitive' locally weighted learning schemes and the modelling guarantees of the bayesian formulation.
1 introduction
locally weighted projection regression  lwpr   vijayakumar et al.  1  is a prime example of recent developments in the area of localised learning schemes that have resulted in powerful non-linear regression algorithms capable of operating in real-time  high dimensional  online learning scenarios. they have been proven to work on many real world applications including  for e.g.  supervised learning of sensorimotor dynamics in multiple degree of freedom anthropomorphic robotic systems  vijayakumar et al.  1 .
¡¡all locally weighted schemes  including lwpr  have to determine a region of validity of the local models  i.e.  an adaptive local distance metric  in a data driven fashion. this is usually achieved by minimising some sort of cross validation cost on the fit using gradient descent methods. however  the initialization of the local complexity parameter or distance metric  the forgetting factor and the learning rates involved in the gradient method necessitate careful hand tuning of multiple open parameters in existing methods. this may not be trivially achieved in many real world problems with limited prior domain knowledge. also  there exists no proper probabilistic formulation of the local weighted learning framework - a necessary development in order to exploit the model selection guarantees that bayesian methods provide while retaining the flexibility provided by nonparametric localised learning.
¡¡one of the most attractive characteristics of lwpr-like localised learning schemes is its independent learning rules for each individual local model  which combines or blends the outputs only at the stage of prediction. in addition to avoiding negative interference  schaal and atkeson  1   this property also allows asynchronous learning of local models leading to improved efficiency. we preserve this property in our model by building a generative probabilistic model for each individual local model and derive corresponding learning rules. we show that our novel formulation performs robustly in estimating local model complexity  competes with the state-of-the-art methods in generalization capability  can be extended to learn in truly incremental fashion  i.e.  without storing data and is surprisinglyefficient in both computational complexity and space.
1 randomly varying coefficient model
modelling spatially localized linear models using a probabilistic framework involves deriving a formulation that allows to model the fit  in our case a linear fit  and the bandwidth at a particular location in the input space. each of these local models can then be combined to provide a prediction for a novel data. additionally  in order for the local models to be independent  each of them should be capable of modelling the entire data by learning the correct bandwidth that partitions the data into two parts - one which corresponds to the linear region of interest and the other which does not. in this paper  we accomplish this by formulating a probabilistic model called randomly varying coefficient rvc  model which builds upon the idea of a random coefficient model  longford  1 .
¡¡for a locally linear region centered around xc a generative model for the data points can be written as:
		 1 
where x represents the center subtracted  bias augmented input vector  represents the corresponding regression coefficient and  ¡« n 1 ¦Ò1  is the gaussian mean zero noise with a standard deviation ¦Ò. the data is assumed to have been generated in an
region of locality

figure 1: variation of prior with the location of the input

figure 1: the 'local' generative model
iid fashion. crucially  we allow the regression coefficient to be a random variable with a prior distribution given by:
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¦Âi ¡« n ¦Â  ci 	 1  where we have assumed that each ¦Âi is generated from a gaussian centered around ¦Â  with the confidence being represented by the covariance ci. the covariance itself is defined to be proportional to the distance of xi from the center. this has the effect that for points that lie close to the center  the distribution of ¦Âi is peaked around ¦Â  resulting in a linear region around the center. this has been illustrated schematically in fig. 1 where point c is the center of the local model: for a point a that lies close to c we assign a prior that is fairly tight around the mean whereas for a point b that lies away from c the prior is much broader. one can consider various distance functions to index the variation of the covariance matrix c. here  we restrict ourselves to a diagonal version  each diagonal element varying quadratically with x as:
c
where hj is the bandwidth parameter of the kernel defining the extent of the locality along the j-th dimension. this choice of the kernel parametrization allows us to use a conjugate gamma prior over hj. the higher values of hj imply lesser variation amongst the coefficients ¦Âi and hence  larger regions of linearity. although the bandwidth modulates the bias-variance tradeoff  an unconstrained likelihood maximization will  in general favor large hj since it implies a higher confidence over larger regions of the data. therefore  we use a gamma regularizer prior over the bandwidth parameters such that it favors relatively small values of hj leading to more localised models:
	hj1 ¡« gamma aj bj 	 1 
we	shall	further	assign	noninformative	normal	prior
n ¦Ì s  for the parameter ¦Â  and a noninformative inverse gamma prior with hyperparameters c and d for ¦Ò. we assume a uniform prior for the regularizer hyperparameters aj and bj. fig. 1 summarizes the resultant probabilistic model for a single local model. in this model  one can marginalize out the hidden variables ¦Âi to obtain
p yi|¦Â  ¦Ò h1 ...hd+1  = z p yi|¦Âti xi ¦Ò1 p ¦Âi|¦Â  ci d¦Âi
  yi ¡« n ¦Â txi xti cixi + ¦Ò1 
 1 
it is interesting to note that the form of likelihood in eq.  1  corresponds to a heteroscedastic regression and will be used in later sections for prediction. in the next section we deal with computing the parameter updates and the resultant ensemble posteriors in an efficient manner.
1 learning
our objective is to learn the posterior over the parameters ¦Â   hj  ¦Ò and to obtain point estimates for the hyperparameters - aj  bj. the joint posterior is given by:
  ¦Ò|y a b c d ¦Ì s  = p y ¦Â  h ¦Ò a b c d ¦Ì s 
p h ¦Â
p y a b c d ¦Ì s 
 1 
where we have used h to denote the vector  h1 ...h1d+1 t and y denotes the training data  y1 ...yn t  a ¡Ô  a1 ...ad+1 t and b ¡Ô  b1 ...bd+1 t . however  the posterior over the parameters is rendered intractable due to the difficulty in evaluating the denominator of eq.  1 . this necessitates the use of variational bayesian em to evaluate the posterior
p h ¦Â  ¦Ò|y a b c d ¦Ì s  and learn the regulariser hyperparameters a and b.
1 variational approximation
to learn the parameters of the model we can maximize the marginal log likelihood with respect to the parameters treating ¦Âi as the hidden variables. the marginal log likelihood is given by:
l=lnp y|a b c d ¦Ì s =lnz p y ¦Â1 ...¦Ân h ¦Â  ¦Ò|a b ¦Ì s c d d¦Â1 ...d¦Ân dhd¦Â d¦Ò=lnz  	p yi|¦Âi ¦Ò p ¦Âi|¦Â  h1 ...hd+1 yi
y
p hj1|aj bj p ¦Â |¦Ì s p ¦Ò1|c d #d¦Â1 ...d¦Ân
j
	dh1 ...dhd+1d¦Â d¦Ò	 1 
using jensen's inequality  the objective function that lower bounds l is given by:
f = z hq ¦Â1 ...¦Ân h ¦Â  ¦Ò1 
p y ¦Â ...¦Â  h ¦Â  ¦Ò |a b ¦Ì s c d 
	ln	1n	  ¦Ò1 	#d¦Â1 ...d¦Ân
q ¦Â ...¦Ân h ¦Â
dhd¦Â d¦Ò1
 1 
the optimal value for q ¦Â1 ...¦Ân h ¦Â  ¦Ò  that makes the bound tight is given by the joint posterior p ¦Â1 ...¦Ân h ¦Â  ¦Ò|y  but since this posterior is intractable  we make an approximation by assuming that the posterior over the variables is independent and can be expressed as q ¦Â1 ...¦Ân h ¦Â  ¦Ò  =
.	this form of
approximation is often called an ensemble variational approximation  details of which can found in  beal  1 . substituting the factorised approximation in eq.  1  we get:

where denotes the expectation with respect to the distribution q. the optimal values of the posterior probabilities can be computed iteratively by maximizing the functional fapprox with respect to each individual posterior distribution keeping the other distributions fixed akin to an em procedure. such a procedure can be shown to improve our factorised approximation of the actual posterior in each iteration. skipping the derivation  such a procedureyields the following posterior distributions:
q ¦Âi|y  ¡« n ¦Íi gi  1 q ¦Â |y  ¡« n ¦Ì  s   1 q h1j|y  ¡« gamma a j  bj  1 q ¦Ò1|y  ¡« inv-gamma c   d   1 where
g
		 1 
where the second part has been derived by making use of the sherman-morrison woodbury theorem. here is the expectation with re-
spect to q¦Ò1. furthermore  using results from eq.  1  
		 1 
	s   	 1 

here  ¦Íi j and ¦Ì i j denote the j-th element of the respective vectors and gi jj and s jj denotes the j-th diagonal element.
	c = c + n/1	 1 
	d = d + xh yi   ¦Íti xi 1 + xti gixii/1	 1 
i
we also need to learn the point estimates for the regulariser hyperparameters aj and bj. maximum likelihood value for the hyperparameters aj and bj can be found by maximizing the bound fapprox given by eq.  1  with respect to these hyperparameters keeping the posterior distributions q fixed. considering only the terms involving the hyperparameters:
e = z q hj1|a j  bj lnp hj1|aj bj dhj1
maximising e with respect to the hyperparameters is equivalent to minimising the kl divergence between the distributions q and p. since the posterior q and prior p share the same parametric form  kl divergence is minimised when the parametersof these distributionsmatch. this leads to the simple update rule for the hyperparameters given by:
	aj = a j 	bj =  bj	 1 
the hyperparameters¦Ì  s  c and d are initialised such that the correspondingpriors are non-informative. an initialisation of ¦Ì = 1  s = 1¡Ái  c = 1 and d = 1 ensures such a condition. on the other hand the regulariser hyperparameters a and b are initialised such that it encourages small h. a value of a = 1 and a sufficiently large value for b ensures such a bias. these are the settings used by rvc for all the evaluations carried out in sec. 1.
1 prediction using the committee of local models
we have dealt so far with building a coherent probabilistic model for each local expert and have derived inference procedures to estimate the parameters of individual model. given the ensemble of trained local experts  in order to predict the response yq for a new query point xq  we take the normalised product of the predictive distribution of each local expert. this is close in spirit to the paradigm of product of experts  hinton  1  and the bayesian committee machines  tresp  1 . the predictivedistribution of each local expert is given by:
p yq|y  = z p yq|¦Â  ¦Ò h q ¦Â |y q ¦Ò1|y q h|y dhd¦Â d¦Ò1
 1 
where p yq|¦Â  ¦Ò h  has the form given by eq.  1 . we can further integrate out ¦Â  from eq.  1   but cannot do the same for ¦Ò1 and h. hence  we approximate q ¦Ò1|y  and q h|y  by a delta function at the mode which implies q ¦Ò1|y  ¡Ö ¦Ä¦Òmode1	and q h|y  ¡Ö ¦Äh1mode. the final predic-
tive distribution for the k-th local model is: yq k ¡« n ¦Ì txq k xq kt s k + ckhmode xq k + ¦Òmode1	 
where xq k refers to the query point with the k-th center subtracted and augmented with bias. blending the prediction of different experts by taking their product and normalising it results in a normal distribution given by:
yq ¡« n ¦Ì ¦Æ1 	where .
here  ¦Ì is a sum of the means of each individual expert weighted by the confidence expressed by each expert in its own prediction ¦Ák  ¦Æ1 is the variance and ¦Ák is the precision of each expert: ¦Ák = 1/ xtq k s k +ck xq k +¦Òk1   ck = diag{xtq kxq k/hj k1 }
1 online updates
the iterative learning rules to estimate the posteriors over parameters given the appropriate prior and the data  represented by eqs.  1 - 1   can be rewritten in the form of online updates by exploiting the bayesian formalism. in a batch mode of posterior evaluation  we have
n
posteriorn = y likelihoodi  ¡Á prior1
i
the same can be expressed as a set of online updates: posteriori = likelihoodi ¡Á priori; priori+1 = posteriori therefore we can transform the batch updates that we had derived earlier into online updates given by :

	a	a	 1 
	c i = ci + 1	 1 
 1 
we repeat the above updates for a single data point {xi yi} till the posteriors converge - here  ¦¨  represents the posterior of ¦¨. for the  i + 1 -th point  we then use posterior of i-th step as the prior as illustrated in algorithm 1.
addition/deletion of local models
the complexity of the learner is adapted by the addition and deletion of local models. when the predictive likelihood for a new data point is sufficiently low then one can conclude that the complexity of the learner needs to be increased by adding a new local model. this leads to the simple heuristic for the addition of a local model wherein a local model is added at a data point when the predictive probability for the particular training data is less than a fixed threshold. the data point serves as the center for the added local model.
¡¡when two local models have sufficient overlap in the region they model  then one of them is redundant and can be pruned. the overlap between two local models can be determined by the difference in the confidence expressed in their prediction for a common test point. the addition and deletion heuristics that have been used here is similar to the ones used in  schaal and atkeson  1 .

algorithm 1 training a local model

1: initialise hyperparameters: ¦¨1 ¡Ô {¦Ì1 s1 c1 d1 a1 b1}.
1: for i = 1 to n do
1:input xi  yi1:repeat1:estimate posteriorhyperparameters¦¨ i using ¦¨i and eq.  1    1  and eqs.  1  -  1 .1:estimate values of the hyperparameters a and b of the regulariser prior using eq.  1 .1:until convergence of posteriors1:¦¨i+1 = ¦¨ i1: end for

1 complexity analysis
the time complexity of the algorithm is dominated by the computation of gi in eq.  1 . the equations that use gi are eq.  1  and eq.  1  and these can be rewritten to avoid explicit computation of gi. eq.  1  requires only the diagonal elements of gi which can be computed in o d  since
gi j j  = ci j j   ci j j xi j  1/ ¦Ò1+¦Ãi 	using eq.  1 
where ¦Ãi = xti cixi which can also be computed in o d  due to the fact that ci is diagonal. on the other hand  eq.  1  requires the evaluation of xti gixi which in turn can be written down as:
x
and can also be computed in o d . furthermore  the matrix inverses in eq.  1  and eq.  1  can also be computed in o d  due to the fact that si and ci are diagonal matrices. therefore the overall time complexity per online update is o dm  where d is the number of dimensions and m the number of local models. the algorithm doesn't require any data points to be stored and hence  has a o m  space complexity for the sufficient statistics stored in the local models. the independence of the local models also means that the effective time complexity can be brought down to o d  using m parallel processors. the time complexity for prediction is o dm  including the evaluation of mean and the confidence bounds. we can see from this analysis that the algorithm is very efficient with respect to time and space  in fact it matches lwpr's efficiency  and hence  is a strong candidate for situations which require real time and online learning.
1 evaluation
in this section  we demonstrate the salient aspects of the rvc model by looking at some empirical test results  compare the accuracy and robustness against state of the art methods and evaluate its performance on some benchmark datasets.

	 a 	 b 	 c 
figure 1:  a local fits and bandwidth adaptation. fit and confidence bounds learned by  b  rvc model and by  c  gp model.¡¡fig. 1 a  shows the local linear fits  at selected test points  learned by rvc from noisy training data on a function with varying spatial complexity. such functions are extremely hard to learn since models with high bias tends to oversmooth the nonlinear regions while more complex models tend to fit the noise. one can see that the linear fit roughly corresponds to the tangential line at the center of each local model as expected. a more significant result is the adaptation of the local bandwidth. the bottom section of fig. 1 a  plots the converged locality measure computed as product of the bandwidth parameters along each input dimension - illustrating the ability to adapt the local complexity parameter in a data driven manner. for this illustration  the local centers are placed in a dense  uniformgrid in inputspace. using the same target function  we compare the fits and confidence bounds learned by rvc and gaussian processes  gp   williams  1  in fig. 1 b  and  c . it is important to note that we have deliberately avoided using training data in  1 1  and the confidence bounds of rvc nicely reflect this.

figure 1: fit and confidence bounds for the motorcycle dataset learned by the rvc model  local models were centered at 1 uniformly distributed points along the input 
¡¡our next experiment aims to illustrate the ability of rvc to model heteroscedastic data  i.e.  data with varying noise levels . fig. 1 illustrates the fit and the confidence interval learnt on the motorcycle impact data discussed in  rasmussen and gharamani  1 . notice that the confidence interval correctly adapts to the varying amount of noise in the data as compared to the confidence interval learnt by a gp with squared exponential kernel shown in fig. 1. this ability to model non-stationary functions is another advantage of rvc's localised learning. in the evaluations presented so far  we have used rvc in the batch mode using the updates that we derived in sec. 1 c.f. eqs. 1 - 1 . the subsequent evaluations in this section make use of the online updates derived in sec. 1.

figure 1: fit and confidence bounds for the motorcycle dataset learned by the gaussian processes model
¡¡to compare the online learning characteristics  we trained the three candidate algorithms on 1 data points from the sinc function corrupted with output noise:.
1
11
# data points  a 1
1sincboston
 b ozonefigure 1:  a  comparison of online learning dynamics for sinc function  b  comparison of generalization errorafter each training data was presented to the learner  the error in learning was measured using a set of 1 uniformly distributed test points. the rvc model was allowed only a single em iteration for each data point to ensure a fair comparison with lwpr. the resulting error dynamics is shown in fig. 1 a . in this comparison  gp exhibits a sharply decreasing error curve which is not surprising considering that it is essentially a batch method and stores away all of the training data for prediction. when we compare rvc with lwpr  we find that rvc converges faster while using roughly similar number of local models. this can be attributed to the bayesian learning rules of rvc that estimates the posterior over parameters rather than point estimates. since the posterior is a product of likelihood and prior  in the event of sparse data  as in the initial stages of online learning   the prior ensures that the posterior distributions assigned to the parameters and in turn the predictions of the learner are reasonable. also the optimization of the regularizer hyperparameters for every data point implies a faster adaptation and hence  a faster convergence.
¡¡finally  we compare the generalization performance of the algorithms on artificial as well as real world datasets. the sinc function  air dataset described in  bruntz et al.  1  and the boston housing dataset from the uci repository were used as benchmarkdatasets. the air ozone dataset which is a three dimensional dataset with 1 data points was split into 1 training and 1 test points. the 1 dimensional boston dataset was split into 1 training and 1 test points. the online learners namely rvc and lwpr were trained in epochs of repeated presentation of the training data  till convergence - lwpr required careful tuning of the distance metric initialization and learning rates to achieve the performancereported here as opposed to the uninformative priors used for rvc. the performance of gp  rvc and lwpr shown in fig. 1 b  are statistics accumulated over 1 different train-test splits. asymptotically  all three methods perform very well on the sinc data set  achieving nmse of less than 1. for the ozone dataset which is highly nonlinear rvc and lwpr performs better than gp. for the boston dataset  we find that the performance of rvc is close to that of lwpr while slightly inferior to the gp results - although this difference is statistically insignificant.
1 discussion
the major contribution of the paper is the development of a bayesian formulation for independent spatially localised learners for multivariate nonlinear regression. we have used a novel formulation of data dependent priors in order to carve out locally linear regionswhile avoiding competition amongst local models. the 'non-competitive' behaviour of each local model allows independent  efficient learning while the bayesian regularizer hyperpriors guard against the danger of overfitting or over-smoothing through automatic local bandwidth adaptation. we evaluated the rvc model against the state of the art in non-linearregression techniques on artificial as well as real world data sets. rvc matched the generalization performance of lwpr while avoiding cumbersome parameter tuning for initialization. it achieves competitive performancecompared to gp - essentially a batch method  while being much more computationally efficient  linear in number of training data and input dimensionality as opposed to cubic in training data for gp . the space and computational efficiency of rvc coupled with the ability to grow model complexity in a data driven fashion makes it a strong candidate for practical online and real time learning scenarios.
