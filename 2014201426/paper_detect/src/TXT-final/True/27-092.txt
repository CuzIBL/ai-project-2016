 
this paper proposes a new algorithm which when provided the relative costs of computation vs. probing minimizes the total cost of diagnosis. during the diagnosis process the decision of whether to probe or to compute is dependent on the expected costs and benefits of each alternative. it is unlikely that we will be able to find general analytic and simpleto-compute models for the costs and benefits. therefore  we base our algorithm on simple empirically derived models of costs and benefits. with these models  our algorithm operates by continuously choosing the optimum action to make next. this algorithm will not blow up on the rare pathological cases and will always  on average  find diagnoses at equal to or better cost than a conventional gde/sherlock. when the cost of probing is high  then our algorithm behaves exactly the same as gde/sherlock. when the cost of computation is high  the algorithm performs the diagnosis at far lower cost than gde/sherlock. 
1 	introduction 
a key goal of the diagnostic task is the identification of which component malfunctions are causing the undesirable symptoms. recently there has been a considerable amount of exploration of the task of restoring the device to correct functioning at minimal cost   l l ; 1; 1 . much of this research is aimed at minimizing the total cost of probing  repairing  and downtime. for example  the decision of whether to repair or to probe is dependent on the relative costs of probing vs. repairing. there has also been a considerable amount of effort into designing better diagnostic algorithms . although these algorithms have made model-based diagnosis far more practical  their design goals have been to produce faster algorithms without concern about the tradeoff of inference time vs. other costs. in this paper we explore the tradeoff between inference and probing costs 
1 	qualitative reasoning and diagnosis 
and propose a new algorithm which when provided the relative costs of computation vs. probing minimizes the total cost of diagnosis. 
   there are three basic reasons why it is important to take into account computation time. first  no matter how good the algorithm  there are devices and inputs that require an inordinate amount of computation. when faced with these rare  but nevertheless very real  cases many algorithms fail to provide useful diagnoses within reasonable time. second  the relative costs of computation and probing vary tremendously in applications. in one application  almost every signal can be measured immediately. in another application  measuring a signal may require sending a technician a mile down a mine shaft. on the other hand  some applications have enormous computational resources available  while in others the amount of computational resources available is minimal. finally  even if computation could be regarded as  free   downtime to the customer often is not. the cost of the computation is often not the actual cpu charges  but the cost during which the customer's system is nonoperational. 
1 	the gde/sherlock framework 
we adopt the model-based diagnosis framework of gde/sherlock  1; 1 . a typical gde  implementation consists of a constraint propagator  constraints are typically used to model the components  and an assumption-based truth maintenance system  the atms is used to record assumptions . gde operates as follows: 
1. using the constraint propagator and the atms  identify all  preferably minimal  conflicts  a conflict is a set of components  at least one of which is faulted . the conflicts are represented as atms nogoods. 
1. from the conflicts  construct all diagnoses. 
1. given all the diagnoses  	determine the optimal probe. 
the sequence of computation and probing can be illustrated by fig. 1. only by determining all the diagnoses first can gde produce the optimal sequence of probes. 
　sherlock  is a far more efficient diagnostic algorithm  upon which we base our new algorithm. the first two steps of the g d e algorithm are inherently exponential. we can remove much of those exponentials by observing that most diagnoses  the low probability ones  have an insignificant effect on probe selection and most conflicts are irrelevant for determining those diagnoses. by focusing the atms  an htms   we avoid atms label explosion. sherlock operates roughly as follows  for more details see  : 
1. perform a best-first search  using prior probabili-ties  to find a candidate diagnosis which does not subsume any of the currently known conflicts. 
1. use the constraint propagator and the htms to test whether the candidate diagnosis is consistent with the observations. if it is not  then find one conflict which subsumes it and add it to the current known conflict set  and return to step 1. 
1. save the new diagnosis  compute its relative poste-rior probability via bayes rule. 
1. if the cutoff criteria is not me  then go to step 1. 
1. if we have one diagnosis  we are done. 
1. perform the best probe  based on the diagnoses so far. go to step 1. 
in the situation  which we explore in this paper  where most diagnoses have similar probability  the probecompute profile of sherlock is no different than that  of gde  fig. 1 . we propose that computation and probing be intermixed  illustrated in fig. 1 . intermixing computation and probing will clearly increase the total amount of probing-probes will be selected with less available information about the space of possible diagnoses. what may not be as obvious is that this intermixing will decrease overall computation time. when the probabilities of diagnoses vary significantly  sherlock will also intermix computation and probing because if all current diagnoses are eliminated  it will return to step 1. but this is ad hoc - our new algorithm will make the decision of whether to compute or probe on a principled basis. 
   in sherlock  every candidate diagnosis has to be checked and eliminated  in step 1 or step 1 . the fact that computation and probing are intermixed does not reduce the total number of candidates that have to be checked. computation time is improved because candidates can be eliminated far more quickly if more information is known about the faulty device. a probe may yield a new conflict which can easily eliminate a large 

number of the candidates in step 1. if a particular candidate is not subsumed by a conflict  it still needs to be checked to see whether it gives rise to new conflicts in step 1. every new probe made of the device  in essence  splits the device into two or more pieces. thus the test of a new candidate is much less expensive. so the basic advantage of intermixing computation and probing is that the same computation task will become cheaper once more is known about the device. 
　steps 1 through 1 above form an anytime algorithm- it can be stopped at any time and provide a current list of valid diagnoses in decreasing probability order and the accuracy of this diagnosis set increases monotonically. steps 1 and 1 consume all the cost  so we will modify the algorithm to do a cost-benefit analysis at those points to decide whether to probe or to compute. the resulting algorithm will be described later. 
　in order to simplify the analysis we make a number of assumptions about the nature of the diagnosis task: 
  costs of all probes are constant and equal. this can be relaxed somewhat by introducing multi-step lookahead; but we do not explore this issue in this paper. if probe costs vary  then the decisions made by the algorithm may not be reasonable. 
  the cardinality of the diagnoses is presumed to be constant. in other words  we do not expect the car-
de kleer and raiman 
dinality of the diagnoses being considered to change during the diagnosis session.  for example  we cannot handle the case where all singlefaults are eliminated  and then doublefaults are considered. our proposed algorithm will suggest suboptimal probes in this case.  
1 	benefit of knowing another candidate 
gde/sherlock's algorithm searches the diagnostic space in a fixed order. it yields the diagnoses in decreasing probability. at any point the search can be suspended and restarted. the basic question we need to answer is the relative value of finding a next candidate. that value will be the number of fewer measurements the algorithm will have to make given that we know that candidate. this is not easy to estimate. gde/sherlock cannot easily provide the number of diagnoses that explain the symptoms before the cutoff criteria will be met1  nor what their probabilities. hence  we have no information about the number of diagnoses that might exist. to truly make the optimum next probe requires knowing all the diagnoses. perhaps there is some analytic formulation of the number of candidates  but we have not been able to construct one. instead we employ an heuristic formulation based on extensive experimentation with the gde/sherlock algorithm. 
　we have extensively analyzed a set of logic devices described in . those analyses provide considerable indirect information about computational cost. there is essentially very little benefit obtained from multiple step lookahead to determine the next probe . in  we showed that it isn't even necessary to use gde/sherlock to obtain a reasonable algorithm. we presented a crude diagnostic algorithm based on random generate-and-test which works reasonably well using a small number of diagnoses to decide the next probe. that paper includes a number of graphs of how the number of measurements needed to diagnose drops with the number of diagnoses used to make the decision of which measurement to make next. gde/sherlock is  of course  a much computationally efficient algorithm  but much more complex . as the graphs only show the number of diagnoses  they are independent of the candidate generation algorithm and therefore apply to gde1 as well. the horizontal axis shows the number of diagnoses used to make the probe decision. the vertical axis shows the diagnostic cost  the average number of probes required to find every single-
　　for all the examples discussed here failure mode probabilities are similar and small. the cutoff criteria used is met when the upper bound of posterior probability of the next candidate is significantly less than that of the best candidate. see summary for more discussion. 
1
　　they were  in fact  generated by gde so if there is anything suspect about this  its not that they do not describe gde correctly. 
1 	qualitative reasoning and diagnosis 

fault in the device-each with a number of sensitive vectors  . 
   fig. 1 shows the graphs for device c1. this device has 1 gates and 1 test points. as there are a large number of diagnoses  and only some of them are considered  different subsets yield different results. samples were gathered by randomly choosing different subsets of the diagnoses. the middle graph on the figure shows the mean  the upper and lower ones show one standard deviation away.  these upper and lower curves are not used in this paper.  we have generated these curves for all of the devices in the test suite. these curves asymptotically approach log1n where n is the number of components in the device. this is roughly what one would expect in the situation where components have two fault modes and we only consider single faults. if there are n components  there are 1n initial diagnoses. on average  half of those are eliminated by the initial symptom leaving n possible diagnoses. using binary tests to discriminate among n alternatives takes log1n probes on average. more generally  if components have / fault modes and the cardinality of the diagnoses are /  we expect the diagnostic 
cost in these graphs to approach /og1   - the shape of the curve is harder to estimate. the case where k   1  k is the number of diagnoses considered  is difficult to analyze. for k   1  the result can be modeled fairly consistently with  c is diagnostic cost  a decreasing exponential: 


after subsequent probes. more likely it will decrease it because the value of higher k's drops as the number of diagnoses that remain drop. on the principle that the best prediction of a future decision is what we decide in the present  we use this formula. 
1 cost of g e n e r a t i n g a n o t h e r 
c a n d i d a t e 
the cost of generating another candidate is dependent on the kind of device  the size of the device  the inputs  etc. again we are not able to come up w i t h an analytic model of the computational cost of finding the next diagnosis. however  by observing the gde/sherlock algorithm in operation one sees some valuable properties. let's first look at the case where we are not making probes. gde/sherlock generates diagnoses at a roughly constant rate. intuitively  one can see how this arises from the best-first search. during the best-first search most diagnoses are eliminated by existing conflicts. the best-first search avoids these diagnoses at nearly negligible cost. the significant cost is incurred when a candidate is discovered which is not subsumed by any known conflict  but we cannot be sure is free of conflicts. in this case we have to re-invoke the a t m s . if no inconsistency is found  then we have a new diagnosis. if an inconsistency is found  then a conflict s  is found and used to accelerate the best-first search. rarely does a device have a large number of conflicts worth recording before finding another diagnosis. 
   therefore  our algorithm predicts the computational time to produce the next candidate as equal to the average time to find the candidates it has so far. in order to account for the rare case where the cost of generating candidates varies greatly  it includes the time spent since the last diagnosis was found in the calculation. therefore  if it spends a lot of computational effort to find the next diagnosis  then its prediction of when the next diagnosis will be found gets further and further out and if the time to find this diagnosis becomes too large the search is abandoned. 
   t h e introduction of probing lias little effect on the rate of candidate diagnosis production. this can be qualitatively understood as follows. the addition of a probe makes the test of any candidate about twice as fast. this is partially due to the fact that a probe may have introduced new general conflicts therefore making it easy for the best-first search to skip these candidates directly. in addition  when a candidate diagnosis must be tested against the observations  at step 1   the t m s operations are faster because the probe fixed a device quantity thereby splitting the device and thereby shortening a t m s labels. as the probes tend to be quite good  even when k is small  the probe reduces the number of diagnoses by approximately 1. it reduces the number of diagnoses the algorithm has discovered by 1  as well as the yet undiscovered ones. hence  discovering new 

table 1: 	typical example of circuit c1 showing time to produce candidate does not vary significantly during diagnosis. 
diagnoses has become twice as difficult. the two effects tend to cancel yielding an unchanged rate of generation of new diagnoses. table 1 illustrates the kind of costs we see in diagnosing devices. 
   although the rate of diagnosis production after a probe does not change  the number of diagnoses that remain to be found after a reasonable probe is halved. the probe has  in effect  eliminated these diagnoses for free. 
1 a d i a g n o s t i c a l g o r i t h m w h i c h t a k e s c o s t i n t o a c c o u n t 
in order to utilize our model our algorithm will always find at least  k = 1  candidates. this is both because our model of the benefit of a next diagnosis only applies if k   1 and it allows for the computation a meaningful average cost of diagnosis discovery. the algorithm is provided two exchange rates: ep which is the cost incurred per probe; e1 which is the cost incurred per second of inference. the key step in the algorithm is to determine whether it is worthwhile to make another measurement. the implementation maintains a variable t  initially t = 1  which represents the time taken to find the current diagnoses. m will be the estimate of the number of probes necessary to isolate the diagnosis   l o g 1 -  fn /1initially . 
1. if they can be found before sherlock's stopping criteria is met  find 1 diagnoses which explain all the current symptoms. update t with the time to find these diagnoses. 
1. if there is only one diagnosis  stop. 
1. if the stopping criteria has been met  go to step 1. 
1. if m   log1k  set m to log1k. compute the benefit  b  of finding another candidate: 

1. if tei/k b  i.e.  the cost of finding the next candidate outweighs the benefit   then go to step 1. 
de kleer and raiman 


1. allow the computation to run for bk/ et seconds or until another diagnosis is found. update t. if a diagnosis is found  go to step 1. 
1. perform the probe  using one-step lookahead and then entropy  which best discriminates among the current k diagnoses. divide m by 1. 
1. remove the diagnoses eliminated by the probe result. update k and t accordingly. go to step 1. 
1 	experimental results 
we have run our algorithm on all the devices in our test suite. table 1 lists the costs to diagnose a significant sample of c1's faults. the first line in the table makes computation nearly free and probing expensive. this is the presumption of gde's architecture and the resulting costs for gde and the new algorithm are the same. in the second entry  cost of computation has become extremely expensive. gde which does not take cost of computation into account performs much worse  while the new algorithm performs significantly better. the last column of the table  p  is the average number of probes the new algorithm makes. here we see the number of probes changes significantly depending on the cost tradeoff-exactly what one would expect. table 1 shows the results for the smaller device c1  1 test points  1 components . the cost benefit of the new algorithm is clear for this device also. 
1 	perspective from decision theory 
our algorithm can be viewed as a very simple  myopic  decision theoretic strategy. for clarity  the following analysis presumes binary valued variables. let / p  s  be our best estimate the cost to diagnose the device having made p probes and s candidates remaining. our algorithm repeatedly makes the decision of whether to probe or to compute. the decision of whether to compute or to probe is determined with one-step lookahead. the decision to probe has cost ep + f p + 1   s/1 . the decision to compute is more complex to compute. we need to 
1 	qualitative reasoning and diagnosis obtain the best estimate of cost if we continue to compute. by tracking the costs to compute past candidates we maintain the mean and standard deviation of times to compute candidates. let p t  be the area under a normal curve from present to t. suppose we compute for time t  the expected cost is: 
i. 
we choose t such this is minimized. 
　for simplicity let us only compute changes of / p  s . therefore  the cost of computation is the maximum of: 

our algorithm estimates f p  s  - f p  s + 1  directly  b in our algorithm . to accurately determine the cost of probing we need to compute / p s  - / p + ! ′   we 
estimate this to be negligible as two effects tend to cancel. every good probe reduces the number of possible candidates by 1. likewise every good probe reduces the number of known candidates by 1  i.e.  the s/1 . our preliminary experiments demonstrate it to be of little utility to compute the standard deviations and means of candidate computation costs. we will continue to use our simpler approximation for decision making. it is likely that if we encounter devices which have wide variance in candidate computation times that our approximation will become significantly suboptimal. 
1 	summary 
we have presented a simple algorithm which explicitly trades off computational cost against probing cost. by intermixing computation with probing  the new algorithm lowers the total cost of finding a diagnosis over the less flexible approach that is usually employed. 
　the overall result is only a small beginning of the research needed to understand the tradeofts within diagnosis in order to develop minimal cost algorithms. there are a number of immediate simplifications made in this paper that need to be more closely analyzed. some of them are: 
1. our algorithm always needs at least 1 diagnoses to function. however  if probing is completely free there is no necessity to ever construct any diagnoses at all as the faulty component can be determined directly. our algorithm's architecture does not accommodate this possibility. 
1. sometimes all faults of the initial cardinality are eliminated. our algorithm needs to be extended to take this possibility into account. 
1. when the prior probabilities of component failure modes vary significantly  the estimate of the size of the diagnosis space  m  in the algorithm is too high  producing too high a benefit of future probes. 

1. the algorithm should be extended to accommodate varying probe costs. for example  it would expend a significant amount of computation  if the next probe to perform was very expensive. 
1. the model of the benefit of finding a new candidate was derived from the case where failure probabilities are nearly equal and with the standard ultimate gde/sherlock stopping criteria  probability cliff  for finding new candidates. the stopping criteria used clearly influences the benefit of finding a next candidate. 
1. we observe that the algorithm usually lowers k during a diagnostic session. which is of little surprise- as the number of diagnoses goes down  the advantage of finding more candidates becomes less. however  our benefit model was based on a constant k. this probably introduces a small overestimation in the algorithms calculation of the benefit of a next candidate. 
1. we assume / p  s  - f p + 1  s/1  is negligible. this needs more careful analysis and experimentation. 
acknowledgments 
we thank daniel bobrow  david heckerrnan and eric 
horvitz for their helpful comments and insights on this paper. 
