pointing  language and the visual world: 
towards multimodal input and output for natural language dialog systems 
introduction to a panel chaired by 
wolfgang wahlstcr 
computer science department 
university of saarbriicken 
1 saarbriicken 1  frg 
panelists: jtirgen allgayer  erhard w. hinrichs  jaap ph. hoepelman  willem levelt  norman sondheimer 
　
　in face-to-face conversation humans frequently use deictic gestures  e.g. the index finger points at something  
in parallel to verbal descriptions for referent identification. such a multimodal mode of communication can improve human interaction with machines  as it simplifies and speeds up reference to objects in a visual world. 
　the basic technical prerequisites for the integration of pointing and natural language  nl  are fulfilled  highresolution bit-mapped displays and window systems for the presentation of visual information  various pointing devices such as light-pen  mouse  and touch-sensitive screens for deictic input . but the remaining ai problem is that explicit meanings must be given to natural pointing behavior in terms of a formal semantics of the visual world. 
　unlike the usual semantics of mouse clicks in direct manipulation environments  in human conversation the region at which the user points  the demonstratum  is not necessarily identical with the region which he intends to refer to  the referent . in conventional systems there exists a simple one-to-one mapping of a demonstratum onto a referent  and the reference resolution process does not depend on the situational context. moreover  the user is not able to control the granularity of a pointing gesture  since the size of the predefined mouse-sensitive region specifies the granularity. 
　compared to that  natural pointing behavior is much more flexible  but also possibly ambiguous or vague. without a careful analysis of the discourse context of a gesture there would be a high risk of reference failure  as a deictic operation does not cause visual feedback from the referent  e.g. inverse video or blinking as in direct manipulation systems . 
　although the * common visual world' of the user and the system could be any graphics or image  current projects combining pointing and natural language focus on forms or geographic maps. 
　for example  the tactilus subcomponent of our xtra system handles a variety of tactile gestures  including different granularities  inexact pointing gestures  and pars-pro-toto deixis. in the latter case  the user points at an embedded region when actually intending to refer to a superordinated region. xtra provides nl access to an expert system  which assists the user in filling out a tax form. during the dialog  the relevant page of the tax form is displayed on one window of the screen  so that the user can refer to regions of the form by tactile gestures. the syntax and semantics of the tax form is represented as a directed acyclic graph  including relations such as 'geometrically embedded' or 'conceptual part of   which contains links to concepts in a kl-one knowledge base. 
　the deixis analyzer of xtra is realized as a constraint propagation process over these networks. in addition  tactilus uses various other knowledge sources of xtra  e.g. the semantics of the accompanying verbal description  case frame information  the dialog memory  for the interpretation of the pointing gesture. 
　while the simultaneous exploitation of both verbal and non-verbal channels provides maximal efficiency  most of the current prototypes don't use truly parallel input techniques  since they combine typed nl and pointing. in these systems the user's hands move frequently back-and-forth from the keyboard to the pointing device. note however  that multimodal input makes even nl interfaces without speech input more acceptable  less keystrokes  and that the research on typed nl forms the basis for the ultimate speech understanding system. 
　another restriction of current prototypes is that the presented visual material is fixed and finite  so that the system builder can encode its semantics into the knowledge base. while some of the recent nl interfaces respond to queries by generating graphics  they are not able to analyze and answer follow-up questions about the form and content of this graphics  since they do not have an appropriate representation of its syntax and semantics. here one of the challenging problems is the automatic formalization of synthetic visual information as a basis for the interpretation of gestural input. 
some of the open questions addressed by the panel are: 
- how can non-verbally communicated information be included in a formal semantic representation of discourse  
- what is an adequate architecture of parsers and generators for multimodal communication  
- what effects have gestures on the attentional state and intentional structure of a dialog  
- how could a generator decide whether to use a pointing gesture  a verbal description or a combination of both for referent identification  knowledge-based media choice   
- what are the temporal interdependences of verbal and non-verbal output in deictic expressions  synchronization of speech and gesture   
- how can we cope with complex pointing actions  e.g. a continuous movement of the index finger  drawing a circle around a group of objects  underlining something  specifying a direction or a path  or a quick repetition of discrete pointing acts  emphatic pointing  multiple reference   
	wahlster 	1 
