
learning word sense classes has been shown to be useful in fine-grained word sense disambiguation  kohomban and lee  1 . however  the common choice for sense classes  wordnet lexicographer files  are not designed for machine learning based word sense disambiguation. in this work  we explore the use of clustering techniques in an effort to construct sense classes that are more suitable for word sense disambiguation end-task. our results show that these classes can significantly improve classifier performance over the state of the art results of unrestricted word sense disambiguation.
1 introduction
perhaps the most serious problem faced by research in word sense disambiguation  wsd  is acquiring labeled training data for supervised learning. this is a crucial problem  since no system with unsupervised learning has shown comparable results to those of supervised systems  and labeling data for wsd is labor-intensive.
　one way of overcomingthis problemis to reduce the specificity of senses and focusing on a few dominant senses  mohammad and hirst  1 . in addition to this  one can maximize the use of knowledge one gathers from available labeled data  by identifying common 'classes' of word senses depending on the similarities in their usage.
　wordnet lexicographer files  lfs  are sense groups  that are created manually during the construction of wordnet  fellbaum  1 . they provide a rough classification of senses. for instance  first senses of nouns cat and dog fall into the lf animal  and first sense of verb dance falls into motion. wordnet has 1 lfs defined for nouns  and 1 for verbs. lfs have been an intuitive choice for semantic classes or 'supersenses' for words due to many reasons  including the popularity of wordnet as a lexical resource  and availability of data labeled with respect to wordnet senses. two works discuss how to use lfs in fine grained wsd: crestan et al.  classified word instances into wordnet lfs in senseval-1 evaluation exercise. kohomban and lee  proposed how training examples from different words can be utilized to learn wordnet lfs  which could then be used for fine-grained wsd of nouns and verbs. ciaramita and johnson  used contextual features to classify unknown nouns into wordnet lfs.
　this use of wordnet lfs begs the question: can we do better if we design the sense classes specifically for finegrained wsd  we answer this question in the affirmative  by using clustering techniques to automatically derive the sense classes. we show that sense classes constructed in this way significantly outperform the wordnet lfs when used with kohomban and lee's  method for all word fine-grained wsd. one interesting result is that the amount of inevitable losses  caused by multiple fine-grained senses falling into the same sense class  can be made dramatically smaller  even when the numberof sense classes is kept the same as the number of wordnet lfs. additionally  our method can be applied to parts of speech other than nouns and verbs  where wordnet lfs cannot be effectively used. the resulting wsd system yields state of the art results on the senseval-1 and 1 english all-words task evaluation datasets; our result on senseval-1 data is the best that we are aware of.
1 generic word sense classes: motivation
this work borrows from  kohomban and lee  1; crestan et al.  1  one major idea: if we can classify word instances into a coarse-grained set of word sense classes  and if we know which fine-grained senses fall into these classes  then we can always use the same system as a fine-grained wsd system by replacing the resulting coarse-grained classes with the most frequent fine-grained sense within each class. this way we lose some senses  hence some accuracy. kohomban and lee  argued that this loss can be affordable  given that the classifier can gain from coarser granularity  as coarser classes reduce the data sparsity. our results from this paper show that with properly designed classes  this loss can be made far smaller than the loss of the wordnet lfs previously used.
　a system working on this principle uses the fine-to-coarse sense mapping to convert any available data  labeled with fine-grained senses  into training examples for coarse-grained classes. a classifier uses training examples from different words  to label any word instance into a class containing one or more of its senses; fine grained sense is then assigned in the manner described above. all one needs for this scheme to work is a mapping of fine-grained senses to a coarse set  generic for all words; most previous work used wordnet lfs.
　however  wordnet lfs are not designed to work as a generic set of classes for wsd. thinking on the wsd application setting  one can identify two issues that can hinder the wsd performance when lfs are used as sense classes:
feature coherence
commonly used features in wsd are those available within text  such as collocations and part of speech. to the best of our knowledge  there is no provenevidence that wordnet lfs form cohesiveclasses in term of these features; counter examples can be found.
　wierzbicka   for instance  provides examples that show that even closely related word/hypernym pairs do not share same usage patterns: word pairs such as apple/fruit  fish/animal  insect/animal  are not readily interchangeable in practical language usage although they are cohesive parts of taxonomy;  there is an animal on your collar  sounds very odd although insect is an animal in wordnet terms. where the linguistic usage is much different  assuming all examples to be in the same class would merely introduce noise. on the other hand  contextually similar usages could have been put into further-away wordnet taxonomies for semantic reasons  making it impractical to differentiate those senses using contextual features alone. also some semantically close word senses are assigned totally different lfs; for instance ionosphere/1:location and stratosphere/1:object.
　another problem with wordnet lfs is that some lfs are subsumed by others: food  for instance  is a subset of substance. this can create confusion in features when learning.
also some lfs with arguably close meanings  such as cog-
nition  feeling  and motive  may be hard to differentiate using contextual features alone. it might possibly be better to group them into a single class.
　lfs for adjectives do not relate to either the underlying concept or contextual features  and are not applicable as generic semantic classes.
loss of senses
the coarser the classes  the greater the chance that a given class includes more than one sense of a given word. as finesense to class mapping  described at the beginning of this section  is a many-to-one mapping  we can lose a few senses for each word in the reverse mapping  resulting in errors in finegrained wsd. granularity of senses that a fine-grained wsd system can attain with wordnet lfs is poor  having only 1 classes for nouns and 1 for verbs.
1 clustering as a solution
in order to address these two issues  we suggest a more direct  task-oriented approach.
　using the features within text to find common groups of senses based on their context has been shown to be useful previously  lin and pantel  1; magnini and cavaglia`  1 . we use this idea for generic sense classes: using clustering techniques  we try to generate automatically a set of 'classes' of word senses that are based on lexical and syntactic features alone. since these classes are directly based upon features  unlike wordnet lfs  we expect them to be easier to learn using the same features. there is no new linguistic assumption made here; the only assumption made on classes is that the senses that fall into the same class  by showing similar usage patterns in a labeled corpus  will show consistent behavior elsewhere. this is the basic reasoning behind inductive learning.
　we address the issue of sense-loss due to coarse grain nature of wordnet lfs by having a larger number of classes than wordnet does as lfs.
　it could be argued that the wordnet hierarchy encodes much of human hand-crafted knowledge  and should be retained as much as possible in the construction of coarsegrained classes. we tested this idea by partitioning the wordnet hierarchy into segments that are finer than the wordnet lfs  while retaining the wordnet hierarchical relationships within each partition. our result shows that this method does not work well  both for reducing sense loss as well as in the final classifier performance.
　in the next section  we will describe these two clustering schemes  i.e. purely feature based and wordnet-hierarchy constrained. section 1 will analyze how well we managed to reach our design goals of feature coherence and sense granularity  using feature based sense clustering. in section 1  we present the framework we set up for evaluating the performance of the classes in the real end-task: fine grained wsd. section 1 discusses the results  and we show that improvement over state of the art is possible with our system  comparing with previously published results.
1 clustering schemes
this section describes the implementation of our proposal  automatic generation of classes based on features  and the control experiment  where we used similar techniques to obtain classes that are constrained within wordnet hierarchies. these will be referred to as fb and wn respectively.
　syntactic and lexical features in text do not necessarily correlate. for this reason  we decided to test two different clustering arrangements  which are respectively based on local context and part of speech features from labeled data  see sections 1 and 1 for details on data and features . features are represented as a binary vector. local context feature vectors were of large dimension  but were very sparse; we used singular value decomposition to reduce feature space dimension  and discard elements with singular values smaller than 1% of the largest. data thus obtained is used in fb and wn schemes. each scheme has two class arrangements  based on local context and pos features.
1 purely feature-based classes  fb 
in this section  we discuss clustering senses independently of the original wordnet hierarchy; our target here is better feature-class coherence. the idea is that as long as the corpus behavior of two senses remain the same  it is possible to assume them as being in some hypothetical generic 'class'  regardless of our being able to understand  or label  the exact semantics of that class. if we can find such classes using labeled data  then it must be possible to use them in wsd  in place of wordnet lfs  as described in section 1.
　a sense is represented by the average of vectors of labeled instances in the corpus for that sense. we omitted the senses that are absent in the labeled corpus  and in the wsd task  section 1   considered them to have their own classes.
　our clustering algorithm is inspired by the k-means+ algorithm  guan et al.  1 . instead of initializing the clusters randomly  we chose to base them on original wordnet lfs. instead of iterating with a fixed number of clusters  we used a method of growing new clusters from outliers of existing clusters. after each iteration of k-means algorithm  we calculate the variance of clusters formed. then we check the squared distance of each point in the cluster to its centroid; if the ratio of this distance to variance is larger than a given constant 1 the point is isolated as a new cluster on its own. upon reaching convergence  another refinement is made: if a cluster has a smaller number of members than desirable  we merge it with the nearest cluster  chosen by simple-linkage condition: that is  cluster cj is the 'nearest' to cluster has the node within the shortest possible distance to any node in ci. this allows for non-spherical clusters  while preserving the size of clusters above a certain limit.
　once the clusters are formed  it is straightforward to create the sense mapping  which can be used in our classifier as discussed above  also in section 1 . we applied this method for nouns  verbs and adjectives.1
1 classes constrained within wordnet  wn 
first  we build trees from wordnet hierarchy  with senses as nodes  and their hypernyms as respective parent nodes. trees that belong to same lf are connected together with a root node. then  the feature coordinates  as earlier  of each sense are added to the tree at its respective node. for a given tree segment  the centroid of coordinates can be calculated by averaging all sense coordinates within that segment; average square distance to centroid is a measure of cohesiveness of a tree. we consider each node in the tree as a candidate breaking point  and decide where to break by checking which split gives the largest reduction in total variance of the system. the partitioning proceeds in a greedy manner  by selecting at each run the node that gives the best overall improvement.
　as earlier  smaller clusters were removed by merging them back; however we cannot pick the geometricallynearest cluster to merge as this would distort the wordnet hierarchy consistency requirement. so a cluster was merged back to the point from which it was originally detached.
　adjectives and adverbs cannot be organized into proper tree forms as they do not have hypernyms. so this method was limited to nouns and verbs only.
1 effects of clustering
in this section  we will analyze empirically the basic effects of our clustering schemes  discussing how effectively we managed to obtain the properties we desired as design goals.

figure 1: proportional 'loss' of senses vs number of classes for nouns  for fb and wn: s1  s1  sc are senseval-1  1 all-words task data and semcor; optimal clustering is highlighted. left-most points of wn correspond to wordnet lfs. feature-based classes consistently yield better senseseparation  even at smaller numbers of classes.

figure 1: proportional 'loss' of senses vs number of classes for verbs. details as per figure 1.
1 sense resolution
we compared the 'sense loss' of the fb classes with that of the wn classes.
　recall from section 1 how the sense loss occurs; losses will be counted as errors in the fine-grained wsd system  so minimizing losses is a desirable quality of classes. given a class mapping and a labeled corpus  we can assess the loss by counting the proportion of labeled instances that belong to 'lost' senses. figure 1 and 1 shows this loss in labeled data sets due to fb and wn  at different numbers of classes. wn starting points in the graph are original wordnet lfs.
　although both schemes seem to benefit from larger numbers of classes  there is an additional gain in fb that we did not anticipate: it achieves good resolution  even at smaller numbers of classes. at the same number of classes as the wordnet lfs  it is possible to obtain more than a 1% reduction in sense loss. recall that fb can either split clusters or reorganize points in order to reduce variance  while wn can only split  as reorganizing would violate the hierarchy. this means that fb can  in theory  achieve better optimization for the same number of clusters  and this seems to work in practice as well. this is an added advantage  as smaller clusters 
local contextposnounsverbsnounsverbswordnet lfs1111wn1111fb1111table 1: average information gain values
although reducing the sense loss  have the undesirable property of including a fewer number of senses. this limits the number of training examples per class.
　in the actual wsd task  we used cross validation to guess the best number of classes for each clustering; these are shown highlighted in the graphs.
1 feature coherence
it was observed that a given fb class can include groupings of different semantics. for instance  a small noun class was dominated by three distinct 'themes' - some of them were {kneecap/1 forearm/1  palm/1}  {coattail/1  overcoat/1  shirtsleeve/1}  and {homeland/1  motherland/1}. but this mix does not pose a problem as similarity weighting of instances  see section 1  can lessen the influence from unrelated words as training instances. a similarity measure based on wordnet hierarchical proximity introduces some of the hierarchy information back to the classification  ensuring both contextual and taxonomical coherence.
　to empirically evaluate how well these classes can be separated by features in the end-task classifier  we calculated feature information gain values for 1-token pos/local context window on complete semcor data set. information gain of a feature i with set of values vi is given by
 
　where is the entropy of the class distribution c. this provides a measure of how well a given set of classes can be separated by a given feature. since the class distribution c and feature values v for each feature are available for semcor  it is straightforwardto apply the formula to obtain wi for a given feature. table 1 shows information gain measures in nouns and verbs for semcor data  for local context and pos features  averaged over six positions in context windows . it can be seen that both wn and fb clusters improve the gain with smaller class sizes  but fb clusters yield the best gain.
1 wsd end-task evaluation framework
in order to empirically validate the effect of the two properties of classes  which we thought of as critical for wsd end-task  we used the system originally described in  kohomban and lee  1  for fine-grained wsd.1 we measure the
'quality' of our classes by using them instead of wordnet lfs that were used in the original work  and evaluating the performance of the resulting wsd system.
1 data
we use labeled data from semcor corpus  fellbaum  1  chapter 1  as training data. to determine global classifier settings  a randomly selected part of this  1 instances for each part of speech  is held-out as validation data set. where word-level validation is employed  randomly picked word instances  up to 1  from the training data were kept aside. evaluation was done on senseval-1 and senseval-1  edmonds and cotton  1; snyder and palmer  1  english all-words task data sets. our tests use wordnet 1.1 senses.
1 features
features used for learning are implemented in the same way as described in  kohomban and lee  1 .
local context: this is a   n +n  symmetric window of words to the both sides of word under consideration. the size of the window n （ {1 1} was chosen by cross validation. all words were converted to lower case and punctuation marks were excluded.
part of speech: this is similar to the local context window  using the same rules of not considering parts of speech of punctuations and not exceeding sentence boundaries. grammaticalrelations: this included the basic syntactic relations such as subject-verb and verb-object  as well as prepositional phrases  such as 'sound of bells' for word sound.
1 classifier
all we obtain from the clustering process is the mapping from fine grained senses into their respective class number. each fine-grained sense labeled instance in the training set is used in the classifier as an example for that particular class  using the class mapping. training data for each word is limited to those instances belonging to classes that include one or more senses of that word. the classifier used is timbl memory based learner  daelemans et al.  1  which is essentially a k-nn classifier.
　kohomban and lee  showed that the noise due to using examples from different words could be reduced if the examples were weighted according to the relatedness of example-instance word to the word being labeled. we employ the same method for nouns and verbs in our experiment  using the same relatedness measure for weighting  proposed by jiang and conrath  jcn  . this measure takes in to account the proximity of the two senses within the wordnet hierarchy  as well as the information content of the nodes of the path which links them together.1
　instance weighting is implemented by modifying the distance Δ x y   between a training instance x and testing instance y   in the following way:

sx y is the jcn similarity between x and the most frequent sense of y that falls within the class ofis a small constant to avoid division by zero.
　a separate classifier was used for each of the three feature type described above. these three classifiers  and a classifier which always predicts wordnet first sense as its output  participated in simple majority voting. in case of a tie  the first sense was chosen.
weighted majority voting as kohomban and lee  reported  weighted majority algorithm  littlestone and warmuth  1  can increase classifier performance over simple majority algorithm. for the final combination  we used the validation data to pick the best clustering scheme  pos or local context  as well. nouns and adjectives did well with local context based clusters  while verbs did well on pos based clusters.
final results
once an instance is classified as a particular class  we have to decide which fine-grained sense it belongs to. there can be more than one sense that falls into a given class. as mentioned in section 1  we use the heuristic of picking the sense with the smallest wordnet sense number within the class. this is motivated by the fact that wordnet senses supposedly come in descending order of their frequency.
　for each clustering  the words with senses that fall into multiple classes were classified as described in this section. all the other words are assigned wordnet sense 1. also  we check for multi-word phrases that have separate wordnet entries. these phrases were assigned sense 1 as they usually have only one sense. adjective clustering was not applicable for wn  as mentioned earlier. in fb  method used for adjectives was the same as above; in wn  and for adverbs in both cases  we resorted to wordnet first sense.
adjective similarity
jcn similarity measure depends on hypernym hierarchy  and is not applicable for adjectives  which do not have a hierarchical organization. as kohomban and lee  reported  only jcn similarity measure could help the classifiers outperform the baseline. in addition  the adjective lfs  only three in number  are not suitable for the wsd framework. however  fb clustering scheme could be applied on adjectives as well  as we can group adjective senses into smaller classes. in addition  the context vectors we get from svd provide a way for calculating inter-sense similarity. the coordinate vectors resulting from svd gives a smoothed out measure of average behavior of a sense. this idea has been successfully used in wsd previously  strapparava et al.  1 . we could use it as a measure of similarity as well  by using the dot product of coordinate vectors as the similarity between senses.
　in general  when we used this measure in the classifier process  it yielded results that outperformed the baseline. however the measure could not outperform jcn. so we limited its use to adjectives only.
1 results
we evaluated the two clustering schemes fb and wn in the framework described in section 1  in clustering schemes that are based on both pos and local context features.
senseval-1senseval-1baseline11senseval best11crestan et al.1-table 1: baseline and previous results
nounverbadj.combinedbaseline1111wordnet lfs1111wn  pos1111wn  lc1111fb  pos1111fb  lc1111baseline1111wordnet lfs1111wn  pos1111wn  lc1111fb  pos1111fb  lc1111table 1: results for different original wordnet lfs  wn and fb clustering schemes  simple majority voting  for senseval1  above  and senseval-1  below  data. pos and lc are clusterings based on pos and local context features  section 1 .
　table 1 shows the baseline  wordnet first sense  and the performance1 of two best systems reported in senseval   mihalcea  1  and  decadt et al.  1   as well as that of  crestan et al.  1   which used wordnet lfs.
　table 1 shows the results of the clustering schemes we discussed  using simple majority voting  see section 1   as well as the re-run of the system with original wordnet lfs. results are given for three parts of speech and the combined system  as described in 'final results' . wn clusters' performance is not significantly different from that of original wordnet lfs; this may be because the constraining on the hierarchy had the same type of localization effect that original system achieved with jcn weighting  thus not yielding any additional information and even preventing some informative examples from being used. this supports our idea that one cannot obtain good performance merely by splitting lf into a finer set of classes.
　on the other hand  the feature-based  fb  classes provide contextually-based information that are not given by the hierarchy  and could have complemented the information from the jcn similarity measure  which is based on the hierarchy. in other words  clustering independently of the hierarchy is a better way to utilize the two different sources of information: semantic  from taxonomical hierarchy   and lexical/syntactic  from linguistic usage patterns .
　it can also be seen that for fb  pos based clustering performed well for verbs  while local context based clustering did well with nouns and adjectives. a rough explanation may be that verbs generally benefit from syntactic features  which are available through pos. the effect is not consistent for
senseval-1senseval-1wordnet lfs11wn11fb11table 1: results after weighted majority voting.
wn; again  this may be due to the fact that hierarchical constraining impedes the clustering effectiveness as well as the utility of jcn.
　table 1 show the results after using weighted majority algorithm for classifier combination. the results of the feature based classes here is better than all other systems. except for  mihalcea  1   it outperforms all previously reported state-of-the art results in respective senseval tasks.
　compared with the results using wordnet lfs  improvements given by our feature based system  on complete data set  is statistically significant on mcnemar test  p   1 . although the improvements over previous systems are numerically small  the figures are considerable when compared with the similar order improvements of the state-of-the-art systems over baseline performance.
1 conclusion
we explored the idea of using generic word sense classes for fine-grained wsd  and discussed some issues one faces when using wordnet lexicographer files as sense classes. we proposed an alternative task-oriented classification scheme  a set of generic sense classes that are based on lexical and syntactic features of text. we gain better classifier accuracy by optimizing the set of target classes to suit the system  instead of the common practice of optimizing the classifier or features. in addition  our system can be used on wsd of parts of speech such as adjectives  where wordnet lexicographer files are inapplicable.
　we evaluated the classes we generated by implementing a system that previously reported good results on wsd by learning wordnet lfs  and using the classes we generated in place of the lfs. our results show that the new classes can improve over wordnet lfs  and yield results that outperform most of the best results on senseval-1  and the best published results on senseval-1.
