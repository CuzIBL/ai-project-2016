
this paper investigates how to represent and solve multiagent task scheduling as a distributed constraint optimization problem  dcop . recently multiagent researchers have adopted the c t ms language as a standard for multiagent task scheduling. we contribute an automated mapping that transforms c t ms into a dcop. further  we propose a set of representational compromises for ct ms that allow existing distributed algorithms for dcop to be immediately brought to bear on ct ms problems. next  we demonstrate a key advantage of a constraint based representation is the ability to leverage the representation to do efficient solving. we contribute a set of pre-processing algorithms that leverage existing constraint propagation techniques to do variable domain pruning on the dcop. we show that these algorithms can result in 1% reduction in state space size for a given set of c t ms problems. finally  we demonstrate up to a 1% increase in the ability to optimally solve c t ms problems in a reasonable amount of time and in a distributed manner as a result of applying our mapping and domain pruning algorithms.
1 introduction
the coordinated management of inter-dependent plans or schedules belonging to different agents is an important  complex  real-world problem. diverse application domains such as disaster rescue  small-team reconnaissance  security patrolling and others require human teams to schedule their tasks and activities in coordination with one another. it is envisioned that automated planning and scheduling processes in the form of agents operating on portable computing hardware can assist human teams in such domains. the problem is inherently a distributed one; an agent that manages the schedule of a human user must effectively manage the interdependencies between its schedule and the schedules of other users by exchanging information and coordinating with other agents. no single agent has a global view of the problem; instead  each must make local planning and scheduling decisions through collaboration with other agents to ensure a high quality global schedule.
¡¡a key step towards addressing the distributed scheduling problem is to devise appropriate representations. in this paper  we have three goals in mind. we desire a representation that  a  captures the problem's distributed multiagent aspects   b  gives formal guarantees on solution quality  and  c  is amenable to distributed solving by existing methods.
¡¡in regard to goal  a   the coordinators task analysis environmental modeling and simulation  c t ms  representation  boddy et al.  1  is a general language  based on the original t ms language  decker  1   that was jointly designed by several multiagent systems researchers explicitly for multiagent task scheduling problems. c t ms is a hierarchical task network  htn  style representation where the task nodes in the network have probabilistic utility and duration. c t ms is an extremely challenging class of scheduling problem and has been adopted as a common challenge problem domain representation for distributed multiagent task scheduling research.
¡¡we propose a distributed constraint reasoning approach for solving problems expressed in the c t ms language. our methodology is one of problem transformation in which we create a mapping that converts a c t ms instance into an instance of the distributed constraint optimization problem  dcop   modi et al.  1 . dcop was devised to model constraint reasoning problems where several agents must communicate to ensure that solutions are globally optimal. several distributed algorithms for dcop currently exist in the literature including distributed dynamic programming  dpop  petcu and faltings  1    cooperative mediation  optapo  mailler and lesser  1   and asynchronous backtracking  adopt  modi et al.  1  .
¡¡the expressivity of c t ms comes at a cost; it is arguable that the primary concern in the design of the c t ms language was to comprehensively express the complexities of the distributed scheduling problem  while less concern was placed on goals  b  and  c  enumerated above. indeed  current distributed solution techniques cannot be applied straightforwardly or are unable to provide solution quality guarantees. thus  to make progress on this difficult problem  we contribute a set of representational compromises in the form of a subset of c t ms  called dsc t ms  that is soluble by current dcop algorithms.
¡¡finally  we demonstrate three key advantages of our constraint based approach. first  we provide a mapping that automatically transforms a c t ms problem into an equivalent dcop. that is  the optimal solution to a given c t ms problem is also the optimal solution to the dcop obtained by applying our problem transformation mapping  i.e.  our mapping preserves the problem exactly. second  arguably one of the primary advantages of adopting a constraint based approach is the ability to apply existing constraint propagation techniques from the constraint programming literature. thus  we leverage our problem transformation by applying constraint propagation pre-processing techniques to perform domain pruning to significantly reduce the search space. we demonstrate a 1% reduction in state space size for a set of benchmark problems. finally  another advantage of our approach is that by mapping the c t ms problem into a dcop  we are able to immediately apply existing solution techniques for dcop to the c t ms problem domain.
1 related work
numerous techniques have been employed to address the problem of multiagent coordination and scheduling. musliner  et al.  map c t ms to a markov decision process which is then used to generate a policy dictating when and how agents execute methods  musliner et al.  1 . musliner  et al.  also propose distributed constraint satisfaction as a method for negotiating a feasible schedule by maximizing previously-calculated local expected qualities  musliner et al.  1 . smith  et al.  address the problem of dynamic schedule revision  due to changes in the model  using simple temporal networks  smith et al.  1 . phelps and rye address this same problem through a domainindependent implementation of generalized partial global planning  in which proxy methods allow for distributed approximation of the performance characteristics of potentially complex tasks  phelps and rye  1 .
¡¡constraint propagation is a general family of techniques that check a constraint based representation for consistency to reduce problem search space  often in a pre-processing phase. bartak provides a survey of the general topic in¡ä	 bartak ¡ä 1 . one of the primary advantages of adopting a constraint based approach is the ability to apply constraint propagation techniques such as node  arc and path consistency. as such  constraint propagation is well-studied with several algorithms available that vary in complexity and pre-processing resource requirements. the first arc consistency algorithms were formalized in  mackworth  1  including the widely used ac-1 algorithm. bessiere ` et al.  improve on averagecase efficiency with the ac-1 and ac-1 algorithms  bessiere` et al.  1 . more recently  researchers have introduced distributed methods for arc consistency processing including the disac-1 algorithm  hamadi  1  and the dmac protocol  silaghi et al.  1 .
1 formalization
this section formalizes the notion of a dcop and specifies the subset of c t ms on which we focus.
1 distributed constraint optimization
a dcop is a subclass of distributed decision processes in which a set of agents are responsible for assigning the values of their respective variables subject to a set of constraints. a
dcop can be defined as a tuple  where
a	is a set of agents;
v is a set of variables  {v1 v1 ... v|v |}; d is a set of domains  {d1 d1 ... d|v |}  where each d ¡Ê d is a finite set containing the feasible values for its associated variable;
f	is a set of |v |1 cost functions  one for each pair of variables  such that fij : di ¡Á dj ¡ú n1 ¡È {¡Þ}.
each cost function maps every possible variable assignment to its associated cost  for all pairs of variables and for all possible assignments. these functions can also be thought of as constraints;
¦Á is function ¦Á : v ¡ú a mapping variables to their associated agent.  means that it is aj's responsibility to assign the value of vi; and
¦Ò is a function ¦Ò : f ¡ú n1 that aggregates the individual costs1.
the objective of a dcop is to have each agent assign values to its associated variables in order to minimize ¦Ò f  for a given assignment of the variables. this definition was adapted from  davin and modi  1  and has been modified for the sake of brevity  clarity  and applicability.
1 c t ms
c t ms is a derivative of the t ms modeling language  decker  1  and can be used for representing instances of task domains of the distributed multiagent scheduling problem. unlike other htn representations  c t ms emphasizes modeling of the interrelationships between tasks more so than those between agents and their environment  musliner et al.  1 . for the sake of exposition and formalism  we represent c t ms instances using set theory  whereas the actual specification is a grammar. we represent a c t ms instance as a tuple  where
n	is a set of agents;
m is a set of methods;
t is a set of tasks;
e is a set of  non-local effects ;
g is a special task known as the  task group;  ¦Ì is a function ¦Ì : m ¡ú a; ¦Õ is a function ¦Õ : m ¡È t ¡ú t ¡È { } that maps methods and tasks to their parent task; and
¦Ø	is a quality accumulation function ¦Ø : m ¡È t ¡ú
r+ ¡È{1} that returns the quality of a method or task. for t ¡Ê t   ¦Ø t  is usually defined as a function of the associated qualities of all x ¡Ê ¦Õ 1 t .
each m ¡Ê m is itself a tuple   where l is the earliest start time of the method; u is a deadline for the method; and d is the expected duration of the method. note that l ¡Ü u  but l + d might not necessarily be less than or equal to u. each t ¡Ê t is defined as a similar tuple  except tasks  as a type of virtual method aggregator  do not have explicit durations. also  these bounds on execution time are inherited from parents. in other words  l¦Õ x  ¡Ü lx and u¦Õ x  ¡Ý ux. it is assumed that ¦Õ creates parent-child relationships such that the resulting hierarchy is a tree rooted at g. the range of ¦Õ ensures that all methods are leaves.
¡¡c t ms prescribes four primary types of quality accumulation function  qaf : sum  min  max  and syncsum  the qualities of the children are summed if and only if all of the children are scheduled to start execution at the same time .
¡¡e is a tuple containing sets of functions  called  non-local effects   nles  in c t ms  that define temporal precedence constraints between methods and tasks: . each function in each of the sets maps pairs of methods and tasks to a boolean; if the function maps to true  then the associated nle holds for that pair of methods/tasks. e contains a set of hard  enables  nles  if x enables y   then y cannot execute until x has accumulated positive quality . d is a set of  disables  nles  which is the opposite of an enables nle. finally  f and h are respectively  facilitates  and  hinders   both being soft nles that either increase or decrease the quality of targeted methods/tasks depending on the quality of the source method/task.
¡¡a  schedule  is a grammar within the c t ms specification for defining the chosen start times of methods; it can be formalized as a function s : m ¡ú n1 ¡È { }. a start time of   means that the method will not be executed. a feasible schedule obeys both mutex and precedence constraints  i.e. an agent may not execute more than one method at a time and all nles are obeyed . the objective is to create a feasible schedule that maximizes ¦Ø g .
1 mapping c t ms to a dcop
the basis of our approach is to map a given c t ms representation of a distributed scheduling problem to an equivalent dcop whose solution leads to an optimal schedule. the technical challenge lies in ensuring that the resulting dcop's solution leads to an optimal schedule. the following sections formalize the approach.
1 challenges
dcops are a promising solution technique  but there exist several challenges that must first be addressed:
representation. there are different semantics in which the problem can be represented: one could have a variable for each method in the c t ms instance that is assigned the value of its start time  or one could have a variable for each instance in time that will be assigned the method that will be executed at that time.
determinism. c t ms prescribes probability distributions over certain parameters  such as method duration  which is a problem since dcops are deterministic.
aggregation functions. dcop solution algorithms do not allow for arbitrary aggregation functions  ¦Ò . for example  adopt requires associativity  commutativity  and monotonicity of the aggregation function in order to ensure optimality  modi et al.  1 .
n-ary constraints. although dcops allow for constraints of arbitrary arity  discovery of efficient algorithms for solving problems containing greater-than-binary constraints is still an open problem  modi et al.  1 .
finite domains. regardless of how the problem is represented  time must be discretized since both the number of variables and the cardinality of variables' domains must be finite.
1 method
the representational challenge is met by creating one dcop variable for each method; the variables will be assigned the start times of their associated methods. the variables' domains will therefore contain the feasible start times of the associated method. time is discretized into quanta for the domains. likewise  the probability distributions in c t ms are discretized using expected values.
¡¡for each agent in the c t ms instance  n ¡Ê n  create an associated dcop agent  a ¡Ê a. for each method  m ¡Ê m  create an associated variable vm ¡Ê v . therefore  the ¦Á function of the dcop can be defined as an analog of the ¦Ì function of c t ms. the domains of the variables will contain all possible start times of the method  including an option for the method to forgo execution . in order to encode the mutex constraints 
  n ¡Ê n :    mi mj  ¡Ê ¦Ì 1 n  ¡Á ¦Ì 1 n  :
when .
in other words  for all agents n ¡Ê n find all pairs of methods mi and mj that share agent n and create a hard dcop constraint  i.e. of infinite cost  for all pairs of equal domain values for the associated variables. this will ensure that an agent may not execute multiple methods at once. nles  i.e. precedence constraints  are encoded similarly. for example  the enables nles are encoded as follows. each e ¡Ê e is a function e :  m ¡Èt  ¡Á m ¡Èt   ¡ú b mapping pairs of methods and tasks to a boolean. let   : t ¡Á  m ¡È t   ¡ú b be a function such that  and let  + be the transitive closure of  .  + x y   implies that x is in the subtree rooted at y . therefore 
is a method
 is a method :
x must have finished executing before y  can start .
in other words  e x y   means that all methods in the subtree rooted at x must have finished executing before any of the methods in the subtree rooted at y may be executed1. for each e ¡Ê e  if the transitive closure e+ x y   maps to true  x is said to precede y in an  nle chain.  then 
|
 + mj y   ¡Ä mj ¡Ê m
:	fij x ¡Ê when finally  create one soft |m|-ary constraint between all of the variables that aggregates the qafs in the htn.
1 dcop-solvable c t ms
the fundamental shortcoming of the mapping proposed in the previous section is the use of n-ary constraints  which are extremely inefficient  and often not supported  by current solution techniques. dcop-solvable c t ms  dsc t ms  is our proposed a subset of c t ms that allows for immediate application of current dcop algorithms.
¡¡dsc t ms consists of only enables and disables nles  and may only contain either:  1  all sum and syncsum qafs;  1  all max qafs; or  1  all min qafs. in each case the mapping is exactly the same as that previously introduced  however  the |m|-ary soft constraint is decomposed into |m| unary constraints. add one unary soft constraint for all methods' variables as follows: .
¡¡if a method is not scheduled to execute  its unary constraint will have a cost equal to the quality that the method would have contributed to g had it been executed. this mapping will produce a dcop with |m| variables and worst-case o |m|1  constraints. in case 1  when all of the qafs are summation  use summation for dcop aggregation function ¦Ò. likewise  in case 1  use maximization for ¦Ò. finally  in the case of minimization  one can create the dual of the minimization problem and apply the max aggregation function.
¡¡it is theoretically possible to extend dsc t ms to allow for cases in addition to the three listed above  assuming n-ary constraints are feasible for small n. for example  one could encode a dcop containing all four types of qaf by adding an n-ary soft constraint for each maximal subtree of the htn that is rooted by either a max or min qaf  adding a unary soft constraint  as usual for dsc t ms  for each method not a member of one such maximal subtree.
  ti ¡Ê t |¦Ø ti  is max or min ¡Ä
  tj ¡Ê  + ti tj  : ¦Ø tj  is sum or syncsum  :
create an n-ary soft constraint encoding the qafs in the subtree rooted at ti .
¡¡a detailed analysis of the correctness and complexity of our mapping is available in  sultanik  1 .
1 efficiency optimizations
nothing thus far in the mapping precludes domains from being infinite  which is one of the challenges  and limitations  of using dcop. from a practical standpoint  this is also a problem because most dcop solution techniques have exponential computational complexity with respect to both the domain size and number of variables. since the number of variables is generally inflexible  not only do we need to make the domains finite but we ideally need to make them as small as possible while ensuring that the solution space of the dcop still contains the optimal solution.
1 na¡§ ve domain bounding
it is possible to create a finite  although not necessarily tight  upper bound on the start times of the methods. let us consider a c t ms instance in which all methods have an earliest start time of zero. in this case  assuming all of the methods will be executed  the optimal start time of a method cannot be greater than the sum of the expected durations of all of the other methods. in the general case of heterogeneous earliest start times  we can define an upper bound on the start time of a method m as the maximum finite earliest start time in m plus the duration of all other methods.
1 bound propagation
although the nature of the distributed scheduling problem implies that a child's bounds are inherited from  and therefore cannot be looser than  its parent's  c t ms neither requires nor enforces this. bounds can be propagated down the tree from the root to improve upon the na¡§ ve bounding. a distributed method for implementing this procedure  requiring only local knowledge  is given in algorithm 1. to calculate the bounds for the methods of agent a  the algorithm would be invoked as recurse-exec-bounds a g c 1 ¡Þ .
algorithm 1 recurse-exec-bounds
require: a is the agent from whose perspective we will create the bounds  t is the task rooting the tree whose bounds we will create  c is a c t ms problem instance   is a lower bound on the bounds of t  and ¦Ô is an upper bound on the bounds of t.
ensure: ¦Âa : t ¡ú  nt1 to lower and upper bounds on their start times. the subscript¡È{¡Þ} ¡Á n1 ¡È{¡Þ}  is a function mapping all tasks in the subtree rooted at
   a exists to emphasize the point that each agent has its own ¦Â function; the mapping of each ¦Â function is contingent upon the extent of knowledge the agent has been given within the problem instance. 1:
1: if visible-to  c t a  then 1:	earliest-start-time c t 
1:
1:	end if
then
1:	end if
1: end if
1: ¦Â	t	l u
1: for allif is-sm¡Êethodsubtasks  c s c t  then  do
1: this means s is a method  i.e. a special type of task  1: if visible-to  c s a  then
1:1:	lumm¡û¡ûearliestlu	-start-time c s 
1:	ed ¡û¡ûdexpectedeadline -c sduration 	 c s 
1:	a ¡û 1:	if then
1:	end ifm ¡û
1:	if then
1:	end ifm ¡û
1:	end if¦Âa s  ¡ú	 lm um  1:	else
1:	this means s is a regular task. 1:	recurse-exec-bounds a s c l u 
1:	end if
1: end for

1 constraint propagation
a	binary	constraint 	fij 	is	arc	consistent	if
.	a	dcop is said to be arc consistent if all f ¡Ê f are arc consistent  bartak  1¡ä  . we use forward constraint propagation down the nle chains to prune the domains  ensuring arc consistency of the dcop. a distributed method for constraint propagation is given in algorithm 1. note that this algorithm makes use of a function broadcast-bounds c ¦Âa a   which has the following postcondition: agent a's bounds will be broadcast to all other agents that share an nle with the method/task associated with the given bound. algorithm 1 works by having agents continually broadcast their current start time bounds for their methods; if they receive a bound that violates arc consistency  they increase the lower bound on their method's start time until the constraint is arc consistent and re-broadcast the new bounds. since the lower bounds monotonically increase and are bounded above  the algorithm must terminate. an analysis of the messaging overhead of this algorithm is presented in ¡ì1.
algorithm 1 dist-constraint-prop c ¦Âa a q 
require: c is a c t ms problem instance and ¦Âa is the associated bound function for the agent  a  that is running this instance of the algorithm. q is a queue that is continuously updated with incoming broadcasts.
 1: broadcast-bounds c ¦Â a  1:
1:	end if
1:	¦Ä ¡û ls+expected-durationthen	 c s    lt
1:	end if 1: end while

1 results
using the c t ms scenario generator created for darpa's coordinators project  we randomly-generated a set of 1 c t ms instances  each with four agents  three-to-four windows1  one-to-three agents per window  and one-to-three nle chains. the scenario generator does not ensure the existence of a feasible schedule for its resulting c t ms instances. even with the small simulation parameters and using all of our domain reduction techniques  the average state space size of these problems was astronomical: on the order of 1. therefore  some of the problems inevitably require inordinate amounts of computation time. there seems to be a phase transition in the problems  such that some are soluble within the first couple thousand cycles of the dcop algorithm  while the rest keep searching for an optimal solution into the millions of cycles. in terms of computation time  this equates to several orders of magnitude difference: seconds versus days. this necessitated a threshold-that we've set to 1 dcop cycles-above which a c t ms instance is simply declared  insoluble.  we have not found a single c t ms instance that has been soluble in greater than 1 cycles  given a reasonable amount of computation time .
¡¡we used the dcop algorithm adopt  modi et al.  1  to solve the resulting dcops. of the 1 random problems  none were soluble by the na¡§ ve domain bounding approach. applying bound propagation  algorithm 1  to the na¡§ ve bounding resulted in 1% of the problems becoming soluble. applying all of our methods resulted in 1% solubility. using an upper one-sided paired t-test  we can say with 1% certainty that algorithm 1 made an average domain size reduction of 1% over the domains produced from algorithm 1. if we look at this reduction in terms of state space size  however  it becomes much more significant: an average 1% decrease in state space size. table 1 presents the state space reduction efficiency of our constraint propagation technique in terms of problem solubility. since constraint propagation was fairly consistent in the percentage of state space reduced between those problems that were soluble and those that were insoluble  this suggests that the 1% of the problems that remained insoluble were due to the large state space size inherent in their structure. for example  the insoluble problems' state spaces were  on average  one million times as large as those that were soluble.
¡¡we also conducted tests over c t ms problems of differing complexity  by varying the number of windows and nle chains . the number of windows is correlated to the number of variables in the resulting dcop  while the number of nles is correlated to the number of constraints. these data are presented in table 1. notice that problems bounded na¡§ vely were never soluble. over the most complex problems with 1 windows and 1 nles chains  algorithm 1 required an average of 1 messages  with a standard deviation of 1 . this was negligible in comparison to the number of messages required to arrive at an optimal solution.
1 discussion
we have presented a mapping from the c t ms modeling language to an equivalent dcop. we have shown that the resulting dcop of a subset of this language  dsc t ms  is soluble using existing techniques  and whose solution is guaranteed to lead to an optimal schedule. we have empirically validated our approach  using various existing techniques from the constraint processing literature  indicating that these problems are in fact soluble using our method.
¡¡we are optimistic in extending our mapping to subsume a larger subset of c t ms. there are also various heuristic techniques in the literature  such as variable ordering  chechetka and sycara  1   that can be applied to the mapping while retaining formal guarantees on solution quality. if the resulting schedule need not be optimal  i.e. feasibility is sufficient   approximation techniques for dcops exist.
avg. domainavg. finalstate spacefinal statesolubilitysize reductiondomain sizereductionspace sizesolved
unsolved1%
1%1
11%
1%1..1 ¡Á 1¡Á
table 1: efficiency of algorithm 1 at reducing average domain size and state space size  in terms of solubility.
% solubleavg. # cyclesavg. # messages# windows# nle chainsna¡§ vecpna¡§ vecpna¡§ vecp11.1-1-111.1-1-111.1-1-111.1-1-111.1-1-111-1-111-1-111.1-1-111-1-1*	11.1-1-1table 1: solubility statistics for different complexities of c t ms instances. all simulations were conducted with four agents. none of the problems bounded using the na¡§ ve method were soluble. * this is the default configuration for the c t ms¡¡with the groundwork laid in solving distributed multiagent coordination problems with distributed constraint optimization  we have many extensions in which to investigate. scenario generator.
