
we present a bounded policy iteration algorithm for infinite-horizon decentralized pomdps. policies are represented as joint stochastic finite-state controllers  which consist of a local controller for each agent. we also let a joint controller include a correlation device that allows the agents to correlate their behavior without exchanging information during execution  and show that this leads to improved performance. the algorithm uses a fixed amount of memory  and each iteration is guaranteed to produce a controller with value at least as high as the previous one for all possible initial state distributions. for the case of a single agent  the algorithm reduces to poupart and boutilier's bounded policy iteration for pomdps.
1 introduction
the markov decision process  mdp  framework has proven to be useful for solving problems of sequential decision making under uncertainty. for some problems  an agent must base its decision on partial information about the system state. in this case  it is often better to use the more general partially observable markov decision process  pomdp  framework. though pomdps are difficult to solve in the worst case  much progress has been made in the development of practical dynamic programming algorithms  smallwood and sondik  1; cassandra et al.  1; hansen  1; poupart and boutilier  1; feng and zilberstein  1 .
¡¡even more general are problems in which a team of decision makers  each with its own local observations  must act together. domains in which these types of problems arise include networking  multi-robot coordination  e-commerce  and space exploration systems. to model such problems  we can use the decentralized partially observable markov decision process  dec-pomdp  framework. though this model has been recognized for decades  see  e.g.   witsenhausen  1    there has been little work on efficient algorithms for it.
¡¡recently  an exact dynamic programming algorithm was proposed for dec-pomdps  hansen et al.  1 . though the algorithm was presented in the context of finite-horizon problems  there are various ways to extend it to the infinitehorizon case. however  in both cases  it suffers from the fact that the memory requirements grow quickly with each iteration  and in practice it has only been used to solve very small problems. it is likely that any optimal algorithm would suffer this problem  as finite-horizon dec-pomdps have been shown to be nexp-complete  even for just two agents  bernstein et al.  1 .
¡¡in this paper  we present a memory-bounded dynamic programming algorithm for infinite-horizon dec-pomdps. the algorithm uses a stochastic finite-state controller to represent the joint policy for the agents. a straightforward approach is to use a set of independent local controllers  one for each agent. we provide an example to illustrate that higher value can be obtained through the use of shared randomness. as such  we define a joint controller to be a set of local controllers along with a correlation device. the correlation device is a finite-state machine that sends a signal to all of the agents on each time step. its behavior can be determined prior to execution time  and thus it does not require that the agents exchange information after receiving local observations.
¡¡our algorithm generalizes bounded policy iteration for pomdps  poupart and boutilier  1  to the multi-agent case. on each iteration  a node is chosen from one of the local controllers or the correlation device  and its parameters are updated through the solution of a linear program. the generalization has the same theoretical guarantees as in the pomdp case. namely  an iteration is guaranteed to produce a new controller with value at least as high for every possible initial state distribution.
¡¡in our experiments  we applied our algorithm to idealized networking and robot navigation problems. both problems are too large for exact dynamic programming  but could be handled by our approximation algorithm. we found that the addition of a correlation device gives rise to better solutions. in addition  larger controllers most often lead to better solutions.
¡¡a number of approximation algorithms have been developed previously for dec-pomdps  peshkin et al.  1; nair et al.  1; emery-montemerlo et al.  1 . however  the previous algorithms do not guarantee both bounded memory usage and monotonic value improvement for all initial state distributions. furthermore  the use of correlated stochastic policies in the dec-pomdp context is novel. the importance of correlation has been recognized in the game theory community  aumann  1   but there has been little work on algorithms for finding correlated policies.
1 background
in this section  we present our formal framework for multi-agent decision making. a decentralized partiallyobservable markov decision process  dec-pomdp  is a tuple hi s {ai} {oi} p ri  where
  i is a finite set of agents indexed 1 ... n
  s is a finite set of states
  ai is a finite set of actions available to agent i and
a~ = ¡Ái¡Êiai is the set of joint actions  where ~a = ha1 ... ani denotes a joint action
  oi is a finite set of observations for agent i and o~ = ¡Ái¡Êioi is the set of joint observations  where ~o = ho1 ... oni denotes a joint observation
  p is a set of markovian state transition and observation probabilities  where p s1 ~o|s ~a  denotes the probability that taking joint action ~a in state s results in a transition to state s1 and joint observation ~o
  r : s ¡Á a~ ¡ú   is a reward function
¡¡in this paper  we consider the case in which the process unfolds over an infinite sequence of stages. at each stage  all agents simultaneously select an action  and each receives the global reward and a local observation. the objective of the agents is to maximize the expected discounted sum of rewards received. we denote the discount factor ¦Ã and require that
1 ¡Ü ¦Ã   1.
1 finite-state controllers
our algorithm uses stochastic finite-state controllers to represent policies. in this section  we first define a type of controller in which the agents act independently. we then provide an example demonstrating the utility of correlation  and show how to extend the definition of a joint controller to allow for correlation among agents.
1 local finite-state controllers
in a dec-pomdp  each agent must select an action based on its history of local observations. finite-state controllers provide a way to represent local policies using a finite amount of memory. the state of the controller is based on the observation sequence  and the agent's actions are based on the state of its controller. we allow for stochastic transitions and stochastic action selection  as this can help to make up for limited memory. this type of controller has been used previously in the single-agent context  platzman  1; meuleau et al.  1; poupart and boutilier  1 .
¡¡formally  we define a local finite-state controller for agent i to be a tuple hqi ¦×i ¦Çii  where qi is a finite set of controller nodes  ¦×i : qi ¡ú  ai is an action selection function  and ¦Çi : qi ¡Á ai ¡Á oi ¡ú  qi is a transition function. the functions ¦×i and ¦Çi parameterize the conditional distribution p ai qi1|qi oi .

figure 1: this figure shows a dec-pomdp for which the optimal memoryless joint policy requires correlation.
¡¡taken together  the agents' controllers determine the conditional distribution p ~a ~q1|~q ~o . this is denoted an independent joint controller. in the following subsection  we show that independence can be limiting.
1 the utility of correlation
the joint controllers described above do not allow the agents to correlate their behavior via a shared source of randomness. we will use a simple example to illustrate the utility of correlation in partially observable domains where agents have limited memory. this example generalizes the one given in  singh et al.  1  to illustrate the utility of stochastic policies in single-agent partially observable settings.
¡¡consider the dec-pomdp shown in figure 1. this problem has two states  two agents  and two actions per agent  a and b . the agents each have only one observation  and thus cannot distinguish between the two states. for this example  we will consider only memoryless policies.
¡¡suppose that the agents can independently randomize their behavior using distributions p a1  and p a1   and consider the policy in which each agent chooses either a or b according to a uniform distribution. this yields an expected reward of  per time step  which results in an expected long-term reward of . it is straightforward to show that no independent policy yields higher reward than this one for all states.
¡¡next  let us consider the larger class of policies in which the agents may act in a correlated fashion. in other words  we consider all joint distributions p a1 a1 . consider the policy that assigns probability  to the pair aa and probability  to the pair bb. this yields an average reward of 1 at each time step and thus an expected long-term reward of 1. the difference between the rewards obtained by the independent and correlated policies can be made arbitrarily large by increasing r.
1 correlated joint controllers

variables:
objective: maximize 
improvement constraints:
probability constraints:
	 c xx c ai  = 1 	 c ai oi xx c ai oi qi1  = x c ai 
	ai	qi1
	 c ai x c ai  ¡Ý 1 	 c ai oi qi1 x c ai oi qi1  ¡Ý 1

table 1: the linear program used to find new parameters for agent i's node qi. the variable x c ai  represents p ai|qi c   andin the previous subsection  we established that correlation can be useful in the face of limited memory. in this subsection  we extend our definition of a joint controller to allow for correlation among the agents. to do this  we introduce an additional finite-state machine  called a correlation device  that provides the variable x c ai oi qi1  represents p ai qi1|c qi oi .
extra signals to the agents at each time step. the device operates independently of the dec-pomdp process  and thus does not provide the agents with information about the other agents' observations. in fact  the random numbers necessary for its operation could be determined prior to execution time.
¡¡formally  a correlation device is a tuple hc ¦×i  where c is a set of states and ¦× : c ¡ú  c is a state transition function. at each step  the device undergoes a transition  and each agent observes its state.
¡¡we must modify the definition of a local controller to take the state of the correlation device as input. now  a local controller for agent i is a conditional distribution of the form p ai qi1|c qi oi . the correlation device together with the local controllers form a joint conditional distribution p c1 ~a ~q1|c ~q ~o . we will refer to this as a correlated joint controller. note that a correlated joint controller with |c| = 1 is effectively an independent joint controller.
¡¡the value function for a correlated joint controller can be computed by solving the following system of linear equations  one for each s ¡Ê s  ~q ¡Ê q~  and c ¡Ê c:
	v  s ~q c 	= xp ~a|c ~q  r s ~a  +
~a
¦Ã x p s1 ~o|s ~a p ~q1|c ~q ~a ~o 
s1 ~o q~1 c1
¡¤ p c1|c v  s1 ~q1 c1  .
we sometimes refer to the value of the controller for an initial state distribution. for a distribution ¦Ä  this is defined as
.
it is assumed that  given an initial state distribution  the controller is started in the joint node which maximizes value from that distribution.
1 bounded policy iteration
we now describe our bounded policy iteration algorithm for improving correlated joint controllers. to improve a correlated joint controller  we can either change the correlation device or one of the local controllers. both improvements can be done via a bounded backup  which involves solving a linear program. following an improvement  the controller can be reevaluated through the solution of a set of linear equations. below  we describe how a bounded backup works  and prove that it always produces a new controller with value at least as high for all initial state distributions.
1 improving a local controller
we first describe how to improve a local controller. to do this  we choose an agent i  along with a node qi. then  we search for new parameters for the conditional distribution
p ai qi1|c qi oi .
¡¡the search for new parameters works as follows. we assume that the original controller will be used from the second step on  and try to replace the parameters for qi with better ones for just the first step. in other words  we look for the best parameters satisfying the following inequality:
	v  s ~q c 	¡Ü xp ~a|c ~q  r s a  +
~a
¦Ã x p ~q1|c ~q ~a ~o p s1 ~o|s ~a 
s1 ~o q~1 c1
¡¤ p c1|c v  s1 ~q1 c  
for all s ¡Ê s  q i ¡Ê q i  and c ¡Ê c. note that the inequality is always satisfied by the original parameters. however  it is often possible to get an improvement.
¡¡finding new parameters can be done using linear programming  as shown in table 1. we note that this linear program is the same as that of poupart and boutilier  for pomdps  with the nodes of the other local controllers and correlation device considered part of the hidden state. its size is polynomial in the sizes of the dec-pomdp and the joint controller  but exponential in the number of agents.
1 improving the correlation device

table 1: the linear program used to find new parameters for the correlation device node c. the variable x c1  representsthe procedure for improving the correlation device is very similar to the procedure for improving a local controller. we p c1|c .
first choose a device node c  and consider changing its parameters for just the first step. we look for the best parameters satisfying the following inequality:
	v  s ~q c 	¡Ü xp ~a|c ~q  r s a  +
~a
¦Ã x p ~q1|c ~q ~a ~o p s1 ~o|s ~a 
s1 ~o q~1 c
¡¤ p c1|c v  s1 ~q1 c1  
for all s ¡Ê s and ~q ¡Ê q~.
¡¡as in the previous case  the search for parameters can be formulated as a linear program. this is shown in table 1. this linear program is also polynomial in the sizes of the dec-pomdp and joint controller  but exponential in the number of agents.
1 monotonic improvement
we have the following theorem  which says that performing either of the two updates cannot lead to a decrease in value for any initial state distribution.
theorem 1 performing a bounded backup on a local controller or the correlation device produces a correlated joint controller with value at least as high for every initial state distribution.
proof. consider the case in which some node qi of agent i's local controller is changed. let vo be the value function for the original controller  and let vn be the value function for the new controller. recall that the new parameters for p ai qi1|c qi oi  must satisfy the following inequality for all s ¡Ê s  q i ¡Ê q i  and c ¡Ê c:
	vo s ~q c 	¡Ü xp ~a|c ~q  r s a  +
~a
¦Ã x p ~q1|c ~q ~a ~o p s1 ~o|s ~a 
s1 ~o q~1 c1
¡¤ p c1|c vo s1 ~q1 c  
notice that the formula on the right is the bellman operator for the new controller  applied to the old value function. denoting this operator tn  the system of inequalities implies that tnvo ¡Ý vo. by monotonicity  we have that for all k ¡Ý 1  . since  we have that vn ¡Ý vo. thus  the value of the new controller is higher than that of the original controller for all possible initial state distributions.
¡¡the argument for changing nodes of the correlation device is almost identical to the one given above. 1
1 local optima
although bounded backups give nondecreasing values for all initial state distributions  convergence to optimality is not guaranteed. there are a couple of factors contributing to this. first is the fact that only one local controller  or the correlation device  is improved at once. thus  it is possible for the algorithm to get stuck in a suboptimal nash equilibrium in which each of the controllers and the correlation device is optimal with the others held fixed. it is an open problem whether there is a linear program for updating more than one controller at a time.
¡¡of course  a bounded backup does not find the optimal parameters for one controller with the others held fixed. thus  a sequence of such updates may converge to a local optimum without even reaching a nash equilibrium. for pomdps  poupart and boutilier  provide a characterization of these local optima  and a heuristic for escaping from them. this could be applied in our case  but it would not address the suboptimal nash equilibrium problem.
1 experiments
we implemented bounded policy iteration and tested it on two different problems  an idealized networking scenario and a problem of navigating on a grid. below  we describe our experimental methodology  the specifics of the problems  and our results.
1 experimental setup
although our algorithm guarantees nondecreasing value for all initial state distributions  we chose a specific distribution to focus on for each problem. experiments with different distributions yielded qualitatively similar results.

	 a 	 b 
figure 1: average value per trial run plotted against the size of the local controllers  for  a  the multi-access broadcast channel problem  and  b  the robot navigation problem. the solid line represents independent controllers  a correlation device with one node   and the dotted line represents a joint controller including a two-node correlation device.¡¡we define a trial run of the algorithm as follows. at the start of a trial run  a size is chosen for each of the local controllers and the correlation device. the action selection and transition functions are initialized to be deterministic  with the outcomes drawn according to a uniform distribution. a step consists of choosing a node uniformly at random from the correlation device or one of the local controllers  and performing a bounded backup on that node. after 1 steps  the run is considered over. in practice  we found that values usually stabilized within 1 steps.
¡¡we varied the sizes of the local controllers from 1 to 1  the agents' controllers were always the same sizes as each other   and we varied the size of the correlation device from 1 to 1. thus  the number of joint nodes ranged from 1 to 1. memory limitations prevented us from using larger controllers. for each combination of sizes  we performed 1 trial runs. we recorded the highest value obtained across all runs  as well as the average value over all runs.
1 multi-access broadcast channel
our first domain is an idealized model of control of a multiaccess broadcast channel  ooi and wornell  1 . in this problem  nodes need to broadcast messages to each other over a channel. only one node may broadcast at a time  otherwise a collision occurs. the nodes share the common goal of maximizing the throughput of the channel.
¡¡at the start of each time step  each node decides whether or not to send a message. the nodes receive a reward of 1 when a message is successfully broadcast and a reward of 1 otherwise. at the end of the time step  each node observes its own buffer  and whether the previous step contained a collision  a successful broadcast  or nothing attempted.
¡¡the message buffer for each agent has space for only one message. if a node is unable to broadcast a message  the message remains in the buffer for the next time step. if a node i is able to send its message  the probability that its buffer will fill up on the next step is pi. our problem has two nodes  with p1 = 1 and p1 = 1. there are 1 states  1 actions per agent  and 1 observations per agent. the discount factor is 1. the start state distribution is deterministic  with the buffer for agent 1 containing a message and the buffer for agent 1 being empty.
1 meeting on a grid
in this problem  we have two robots navigating on a twoby-two grid with no obstacles. each robot can only sense whether there are walls to its left or right  and the goal is for the robots to spend as much time as possible on the same square. the actions are to move up  down  left  or right  or to stay on the same square. when a robot attempts to move to an open square  it only goes in the intended direction with probability 1  otherwise it either goes in another direction or stays in the same square. any move into a wall results in staying in the same square. the robots do not interfere with each other and cannot sense each other.
¡¡this problem has 1 states  since each robot can be in any of 1 squares at any time. each robot has 1 observations  since it has a bit for sensing a wall to its left or right. the total number of actions for each agent is 1. the reward is 1 when the agents share a square  and 1 otherwise  and the discount factor is 1. the initial state distribution is deterministic  placing both robots in the upper left corner of the grid.
1 results
for each combination of controller sizes  we looked at the best solutions found across all trial runs. the values for these solutions were the same for all controller sizes except for the few smallest.
¡¡it was more instructive to compare average values over all trial runs. figure 1 shows graphs of average values plotted against controller size. we found that  for the most part  the average value increases when we increase the size of the correlation device from one node to two nodes  essentially moving from independent to correlated .
¡¡for small controllers  the average value tends to increase with controller size. however  as the controllers get larger  there is no clear trend. this behavior is somewhat intuitive  given the way the algorithm works. for new node parameters to be acceptable  they must not decrease the value for any combination of states  nodes for the other controllers  and nodes for the correlation device. this becomes more difficult as controllers get larger  and thus it is easier to get stuck in a local optimum.
¡¡improving multiple controllers at once would help to alleviate the aforementioned problem. as mentioned earlier  we do not currently have a way to do this using linear programming  and it thus remains an interesting topic for future work.
1 conclusion and future work
we have presented a bounded policy iteration algorithm for dec-pomdps. besides the fact that it uses finite memory  the algorithm has a number of other appealing theoretical guarantees. first  by using correlated joint controllers  we can achieve higher value than with independent joint controllers of the same size. second  assuming a constant number of agents  each iteration of the algorithm completes in polynomial time. finally  monotonic value improvement is guaranteed for all states on each iteration.
¡¡our empirical results are encouraging. by bounding the size of the controller  we are able to achieve a tradeoff between computational complexity and the quality of the approximation. up to a point  increasing the sizes of the local controllers leads to higher values on average. after this point  average values tend to level off or decrease. increasing the size of the correlation device leads to higher value  which is consistent with our theoretical results.
¡¡for future work  there are many more experiments that can be done with bounded policy iteration. for instance  in moving to a larger controller  we could use the previous controller as a starting point  rather than starting over with a random controller. poupart and boutilier's  escape technique could be useful here. also  rather than choosing nodes uniformly at random for updating  we could develop a principled way to order the nodes.
¡¡we are also looking into ways of extending the algorithm to handle problems with large numbers of agents. in many problems  each agent interacts with only a small subset of the other agents. this additional structure can be exploited to reduce the size of the problem representation  and it should be possible to extend our algorithm to take advantage of these local interactions.
¡¡finally  it would be interesting to extend bounded policy iteration to the noncooperative setting  where each agent has a separate reward function. one approach is to require that a change in parameters does not lead to a decrease in value for any agent. another approach is to consider just the value function for the agent whose node is being updated. this should move the joint controller towards a nash equilibrium.
1 acknowledgments
we thank martin allen and ozg¡§ ur s ims ek for helpful discus-¡§ sions of this work. this work was supported in part by the national science foundation under grants iis-1 and
iis-1  by nasa under cooperative agreement ncc 1  and by the air force office of scientific research under grant f1-1.
