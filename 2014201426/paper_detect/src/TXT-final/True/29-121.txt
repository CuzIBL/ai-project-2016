 
identifying user-dependent information that can be automatically collected helps build a user model by which to predict what the user wants to do next and to do relevant preprocessing. such information is often relational and is best represented by a set of directed graphs. a machine learning technique called graph-based induction {gbi  efficiently extracts regularities from such data  based on which a user-adaptive interface is built that can predict next command  generate scripts and prefetch files in a multi task environment. the heart of gbi is pairwise chunking. the paper shows how this simple mechanism applies to the top down induction of decision trees for nested attribute representation as well as finding frequently occurring patterns in a graph. the results clearly shows that the dependency analysis of computational processes activated by the user commands which is made possible by gbi is indeed useful to build a behavior model and increase prediction accuracy. 
1 	introduction 
computers are still not easy to use. the main reason is their ignorance about the user. the user information that is available to an interactive computer system is limited  and thus  the user model acquisition is a difficult problem. classical acquisition methods like user interviews  application-specific heuristics  and stereotypical inferences are often not appropriate  and a better automated method is being sought. 
　finding regularities in data is a basis of knowledge acquisition  and extracting behavioral patterns from the user information is one such problem. each user has a different way of doing the same thing and identifying the information that can characterize the user and be automatically collected is crucial. once such information is found and if an appropriate machine learning technique can induce regularities in each user's behavior to carry out his/her intended task  we can use them to guide the 
1 	invited speakers 
daily work and to do some preprocessing  which may facilitate easiness of usage and increase efficiency. 
　we discuss three learning tasks  command prediction  script generation and file prefetching in multi task environments. the scope of user behavior is limited to a sequence of task execution  e.g.  editing  formatting  viewing  etc.  using plural application programs. 
　most studies that attempted to develop a useradaptive interface system only analyzed the sequence of user behaviors  from which to automate the repetitions  see 1 . in this setting  the data can easily be represented by at tribute-value pairs  each attribute denoting the sequence order and its value  the command. since the command sequence does not necessarily typify the user's behavior  the user model constructed from only the sequence information may not adequately capture the user's behavior. we focused on the process i/o information that is also automatically collected along with the command sequence. since this is dependency information and its relationship cannot be fixed in advance  it is not straightforward to represent this by attributevalue pairs. 
we show that graph-based induction  yoshida and 
motoda  1  can nicely be applied to the three learning tasks. in this paper  we revisit gbi  show how it can extract typical patterns from a set of directed graphs and how it can induce classification rules using a similar technique in the top down decision tree induction  tddt  algorithm. the first and the second learning tasks are implemented as clipboard which is a window like unix shell  yoshida and motoda  1   and the third task is implemented as prefetch daemon that is hidden from the user. the results clearly show that the dependency analysis of computational processes activated by the user's commands  which is made possible by gbi  is indeed useful. clipboard is in daily use and its prediction accuracy and response time are satisfactory. prefetch daemon works as expected only for i/o intensive task due to an implementation problem  and thus needs further improvement. 
　the following section introduces the three learning tasks. subsequent sections describe the learning method gbi and summarize the results of learning experiments performed to date. the last two sections consider lessons 

learned from this study and directions for future research. 
1 	learning tasks 
command prediction is a real time task that takes a user's operational history and predicts the next command. figure 1 shows  in a simplified form  an example of operational history when a user is making a document using a latex document formatter. the bold arrows show the command sequence. the history includes  in addition to this  i/o relationships between commands  and thus  takes the form of a directed graph. each link has a label that corresponds to a file extension. for example  the link connecting latex to emacs has a label tex. however  one link is reserved for sequence information. 
clipboard keeps recording and updating the history  and at any point of operation  predicts the next command. the learning task is to induce classification rules from the past history. for each command in the past  a directed graph of a certain depth  number of sequentially connected links  and width  number of sibling links  is taken out1. each directed graph forms a training example. its root is a class and the rests are considered to be nested attributes. 

figure 1: i/o relationships between commands  applications  
　script generation is a batch task that extracts frequently occurring patterns from a large graph representing a history of order of days  generalizes the arguments and generates shell scripts to execute a sequence of operations by a single command. figure 1 shows an example of the generated scripts when a user repeatedly calls up emacs  latex and xdvi 
　file prefetching is a real time task that predicts files to be used in the immediate future and prefetches them into the cache. unlike the command prediction  prefetching must predict a few steps ahead and thus more than one file. the learning task is done in a batch mode using a large directed graph. the task is to extract frequently occurring patterns first like script generation  from each 
1
　　in the experiments described in 1  the depth was set 1 and the width 1  this is maximum and automatically adjusted . 

figure 1: example of a generated script 
of which a prefetch rule is generated and then to merge them into a single trie structure  example shown in fig. 1 . the prefetching is made in real time based on this trie. since prefetching is automatic  this task is invisible. 
1 	graph-based induction 
1 	finding regularities in a directed graph 
gbi was originally intended to find interesting concepts from inference patterns by extracting frequently appearing patterns in the inference trace. in  yoshida and mo toda  1   it is shown that gbi was able to discover the notion of not and nor from the simulation traces of an electric circuit. in this application  the original inputs are causal relations of voltage and current between various nodes of the circuit; there is no notion of logical operation. however  by finding regularities in the input traces  it was able to lift up the abstraction level and find more abstract concepts. later  we showed that the same idea can be applied to other types of learning  speed up learning and classification rule learning   yoshida et a/.  1 .   
　the original gbi was so formulated to minimize the graph size by replacing each found pattern with one node that it repeatedly contracted the graph. the graph size definition reflected the sizes of extracted patterns as well as the size of contracted graph. this prevented the algorithm from continually contracting  which meant the graph never became a single node. because finding a subgraph is known to be np-hard  the ordering of links is constrained to be identical if the two subgraphs are to match  and an opportunistic beam search similar to genetic algorithm was used to arrive at suboptimal solutions. in this algorithm  the primitive operation at each step in the search was to find a good set of linked pair nodes to chunk  pairwise chunking . 
   because the search is local and stepwise  we can adopt an indirect index rather than a direct estimate of the graph size to find the promising pairs. on the basis of this notion  we generalize the original gbi  and further extend it to cope with the classification problem. the idea of pairwise chunking is given in fig. 1  and the general algorithm in fig. 1. 
　the selection criterion of the pair nodes should be such that its use can find interesting patterns  e.g.  patterns occurring more frequently than others or patterns more 
motoda & yoshida 

figure 1: 	the idea of graph contraction by pairwise chunking 
easily identifiable than others . proper termination condition must be used in accordance with the selection criterion  e.g.  iteration number  chunk size  change rate of selection indexes  etc. . examples of such indexes are information gain  quinlan  1   information gain ratio  quinlan  1  and gini index  breiman et a/.  1 . 
　we use information gain as an index here  but the other indexes can be used in the same way. unlike decision tree building where the index is used for selecting an attribute  here we have to select linked pair nodes. each node has a value  color  and each link has a label. we can interpret the triplet as saying that the value of the i-th attribute of the parent ak is  or when the i-th attribute takes the value bj  its immediate result is ak. the problem is which  i  j  k  to select to chunk. a natural way is to focus on one of the three elements  and select the best remaining two to identify the chosen element. three alternatives exist: a  focus on k  b  focus on i and c  focus on j. case a  tries to find the attribute and its value pair that best characterizes the chosen immediate result. likewise  case b  tries to find the result and the attribute value pair that best characterizes the chosen attribute  and case c  tries to find the attribute and its result pair that best characterizes the chosen attribute value. which one to adopt depends on what the directed graph represents in terms of the original problem description. the default is to choose a . 
　in what follows  only case a  is described. the other two are obtained by permutating the subscripts. let the underline in the subscript mean its complement  e.g.  i means the attributes other than the i-th.   the overline 
1 	invited speakers 
　this is recursively repeated until each subgroup  after testing  contains a single class value or some stopping condition is satisfied. 
1 	clipboard interface 
figure 1 shows the system configuration for clipboard 
interface and prefetch daemon. the process i/o recorder is a part of the operating system and records all the i/o operations of each command issued. this information is represented together with the command sequence by a directed graph as operation history. gbi program runs on this graph and generates prediction  classification  rules and typical patterns. the mousebased command controller uses these to 1  select the next command  and to 1  create unix shell scripts. the prefetch daemon uses the typical patterns to generate prefetch rules and merges them into a trie structure to 1  prefetch files. 
 c  clipboard suggests emacs for paper.tex 
figure 1: screen image of clipboard 
figure 1: clipboard and prefetch system configuration 	1 	c o m m a n d p r e d i c t i o n 

　figure 1 displays the screen images of clipboard during a simple document processing task. when clipboard starts without any information  the screen lists only file names  fig. 1  a  . at this stage  after selecting a file to be processed  the dialogue box appears so that the user can specify the command  fig. 1  b  . if the user specifies emacs  it treats emacs as the default for the file with the extension tex. clipboard tries to learn the appropriate command for each file extension  and recommends the command by icons  fig. 1  c  . clipboard never asks the user for information. the user can always override clip boards recommendation  which triggers the learning task. icons for the same files change over time reflecting context changes. 
　currently  clipboard interface is written by the gbi program has both c and lisp versions. the prefetch daemon is written by java. 
i/o information analysis 
　consider an operation history in table 1. as shown in steps  a    b   and  c   the file paper.dvi is processed by three different commands: xtex  xdvi and dvi1ps. figure 1 shows the corresponding directed graphs that are inputs to gbi. the algorithm described in 1 first chooses the dvi attribute  and its value latex for testing  and chunks the triplets  xdvi  dvi  latex  in 
 b  and  in  c . the no branch contains only one instance   a   and the yes branch contains two instances   b  and  c . next  the algorithm chooses the sequential attribute and its value xdvi for testing and chunks the triplet  this separates  c  from  b  and the induction stops1. the 
     1 in reality  there are many occasions in history where dvi files are used by the same command that has different de-
motoda & yoshida 


shaded parts in fig. 1 are the typical patterns. figure 1 is the interpretation of the extracted patterns. 

figure 1: interpretation as prediction rules 
　gbi assumes the existence of a strong correlation between the linked attributes. as described in 1  the algorithm follows the standard tddt induction  but the attributes to be selected are dynamically modified in the process. note that it is impractical to represent the graph structure by a single table of attribute-value pairs. 
evaluation 
the above algorithm for the classification problem was implemented and tested for the command prediction problem using both artificially generated and real operation data. 
　artificial data were generated approximating user's behavior by a markov model that comprises five different tasks. the model used is shown in fig. 1. about 
pendency  in which case the chunking process becomes more complicated. 
1 	invited speakers 
1 different sequences were generated  in which commands that were not in the model  e.g.  is   df  etc.  were added as noise. three fold cross validation was used to evaluate the prediction accuracy. the results are shown in table 1. this table includes the results obtained by other methods for comparison. 

figure 1: markov model used to generate artificial data 
table 1: prediction accuracy for artificial data 

def.: default value for most frequently used command 
ld: linear discrimination method 
gbi : without dependency info  for the root node  command to predict  
gbi1. with dependency info  for the root node 
　there are two cases for gbi. gbi1 is the case where dependency information is used only for the commands  nodes  preceding the root node. in other words  no dependency information is used for the root node. this reflects the fact that the argument is not known in advance to predict the next command. gbi1 is the case where the dependency information for the root node  command to predict  is also used. this corresponds to a case where the file to process is specified  and this is exactly what the current clipboard interface does1. in  yoshida and motoda  1  the former is called command prediction and the latter  application selection. 
　default is the simplest way of prediction that always assumes the most frequently used command to be the next command. ld is a linear discrimination method  james  1   which gave the same answer as the default and did not improve the accuracy. the best result by the conventional method was achieved by the decision-tree learning method cart  breiman et al.  1 . ld and cart use only sequential information because these methods cannot deal with information 
     1  this is not a strong restriction because files associated with a given task are generally known and the prediction of the command for each of these files can be made with this method. 

having a graph structure. from these results  it is clear that the i/o dependency information  in particular  the one immediately before the command to predict  plays an important role in increasing the accuracy of prediction. 
　the same algorithm was tested against the real data that had been taken from the log of daily usage over three months of a single user. the data include about 1 kinds of commands. two-thirds of them was used as a training data set and the rest as a test data set. the result is shown in table 1. it is clear that gdi outperforms the other methods. interestingly gbi 1 is much better than cart in real data. this is probably because the number of commands actually used is much larger than the artificial data case and the noise level is also higher. unfortunately the value for gbi1 is notavailable for the same data set. it is instead estimated by the daily usage when the performance approached the steady state. once again  the role of i/o dependency is clear. 
table 1: prediction accuracy for real data 

　the non-essential commands such as is and df can be naturally ignored by a mouse-based interface system. if we ignore these effects and focus on the important commands  we obtain the results shown in table 1  which is by far better. while evaluation of clipboard is still ongoing  most of the important commands predicted by clipboard is quite adequate  and the user does not feel any burden in using it. 
table 1: 	prediction accuracy of selected commands 
 gbi1  

1 	s c r i p t g e n e r a t i o n 
i / o information analysis 
to be precise  the i/o recorder keeps track of 1  all process creations in the operating system  and 1  all i/o operations  open system calls . thus  even in a multiwindow and/or a multi-task environments  it is possible to extract relationships between commands that may have been issued across the different shells. we use the whole graph to extract patterns. the extracted patterns are frequently appearing ones in the history  and we convert them to shell scripts. the input file name is changed to the argument of the script with extensions retained  see fig. 1 . 
evaluation 
table 1 lists the scripts generated from the sample history  which involves about 1 process creations and about 1 i/o operations. the number of processes includes system programs that were not invoked by the user  e.g.  telnet daemon  line printer spooler daemon  etc.   some user commands  e.g.  shell scripts   and created child processes. the number of the actual commands invoked by the user was approximately 1. 
table 1: generated scripts with more than three commands 

　since the algorithm only considers the frequency or its equivalent as measured by the index  evaluation of the usefulness or importance of the generated scripts must be rendered to the user. unlike the case for command prediction  there is no direct feedback from the user. the scripts in table 1 have clear meanings except script 1. without having knowledge about the c compiler  clipboard could generate scripts 1 and 1. clipboard did not use any pre-specified knowledge about latex and related commands in generating script 1. script 1 is a unique script for this particular user. without clipboard the user has to write this by him or herself. 
1 	prefetch daemon 
i/o information analysis 
in a multi-task environment different users work on the same machine for different tasks  e.g.  editing and pro gramming . even though the i/o operation sequence of each task has regularity  the overall i/o sequence is affected by the subtle timing of each task progress. the graph structure can encode the correct information even in a multi-task environment. just like in the case of script generation  gbi analyzes the process data and represents them by a set of directed graphs  from which it extracts typical patterns. each of the patterns represents an aspect of the user  we call it user model for convenience . figure 1 shows how these patterns are used to prefetch files. first  each of the patterns is converted into a prefetch rule. unlike the command predictions  the point here is not to predict the root node from the rest  but to predict from the bottom  first  node in the sequence how certain files are going to be used along the subsequent command execution. each rule consists of a sequence of events  i.e.  command executions and i/o operations  with a list of files to be prefetched. next  each rule is merged into a single trie structure. the system checks the common event sequences in the rules and merges the same parts into a single structure. for example  in fig. 1  the first nodes of the two user models  
motoda & yoshida 

a and b  are the same and are thus merged. in order to improve the prefetch accuracy  the frequency information in the log is used to prune files1. the generation of trie structure is performed as a batch process. 

figure 1: user models and a merged trie structure for prefetching 
evaluation 
after the batch process constructs the trie structure  the prefetch daemon uses this trie structure to prefetch files. the daemon maintains the status information for each process. if a new process is activated  the prefetch daemon creates a new pointer which points the root node of the trie structure. if the process executes command emacs  i.e.  the program memorized in the succeeding trie node   the daemon prefetches program files make and bibtex and updates the pointer. in fig. 1 process  a  shows the position of the pointer after it executed emacs and then bibtex. each time it updates the pointer  it also looks for the same command from the root  i.e.  the command just below the root node  as if a new process with this command was initiated. when it finds the command  it also prefetches the associated files. this is recursive. if the actual events of the process exhibit a different sequence from the trie  all the pointers for this process are removed and the prefetch daemon ignores the process until a new process is initiated. 
　the above prefetch mechanism was tested for the daily usage data  the length of the log was about 1 . the prefetch cache size was automatically adjusted by os. the trie had approximately 1 nodes. although further experiments are necessary  the preliminary experiments show that the trie structure has high prediction accuracy. for the experiment we conducted  the hit rate was almost 1%. 
　unfortunately  even with the high hit rate  the current implementation slows down the cpu intensive tasks due 
1
　　there are many patterns that partially overlap and/or are subpatterns of the others. a threshold can be set to the number of occurences of the files for them to be prefetched. 
1 	invited speakers 
to the cpu resources used by the prefetch daemon. we could only speed up i/o intensive tasks. it could indeed speed up the invocation of a large program such as xwindows and mule to the extent that we did not feel we had waited. the process switching overhead and the java byte code interpretation are the sources of the problem. a kernel embedded file prefetcher that is coded by c and assembler would solve the problem. 
1 	discussion 
1 	learning semantics from syntax 
although what obi does is simply extracting the syntactic/statistical nature of what a user has done in the past  it is still possible to extract useful semantics of the user's behavior. the user never tells the start of his/her task to clipboard  but the scripts generated by gdi does capture a piece of meaningful tasks. most crucial is the information source. the surface form of the user's input  i.e.  command sequence  was not enough. other information that is hidden and invisible  i.e.  process i/o  contributed much. standard techniques  e.g.  index based on information theory  cross validation  etc.  that statisticians have developed are also important factors. 
1 	i n f o r m a t i o n to c a p t u r e user b e h a v i o r 
 piernot  1  addresses the importance of the context in an interface system. file extensions we used in our analysis to capture the i/o information helped provide rich context. other information that may help capture the user's behavior is command exit status and time of execution. for example  if the user fails to compile a program because of a simple syntactic error  the next step tends to be an editing task. if s/he succeeds  it tends to be a test run. thus  the exit status seems to be informative. since most users tend to check e-mail in the morning  the time of day also seems to be informative. experiments using clipboard utilizing such information are currently under investigation. 
the method of encoding information is also important. 
we encoded the i/o information from how a file was made by application program. the experimental results suggest the adequacy of this encoding  but this is not the only way to use the i/o information. for example  how a file was used by application program is another way of encoding. figure 1 shows a graph format that was designed to emphasize this aspect. we confirmed the usefulness of this encoding with a version of clipboard that uses this as an alternative to the sequence information. note that this encoding has a noise-tolerant nature. user errors  such as mistyping and wrong command selection  and unexpected interrupts  such as new mail arrival  sometimes cause noise in sequence information. the replaced i/o information is less affected by such noise. 
1 	m e t h o d o f a n a l y z i n g user b e h a v i o r 
if the user is always logical and consistent  the analytical methods  such as explanation-based learning  are ad-

 c  is the directed graph of the history information at step  c  in figure 1. the shaded part indicates that paper  dvi was previously used by xdvi.  c ' is the reconstructed graph. the dvi node is removed for brevity and the  used by  xdvi node replaces the sequence information. 
figure 1: graph encoding the knowledge of how a file was used. 
equate in making the user behavior model. unfortunately  the user is sometimes illogical and inconsistent  and capriciousness makes it difficult to apply analytical methods to the interface problem. the statistical methods  such as linear discrimination and k-nearestneighbor  jlames  1   and empirical learning methods  such as  quinlan  1   seem to be more adequate. the errors  i.e.  mistyping and wrong command selection  are naturally ignored as noises in these methods. however  these methods are not suited to handle structural data as was the case for this study. 
　if we set the maximum width  number of input files  per command and the maximum depth  number of chains of i/o relationship   it is possible to design a table of attributes and values that can record all the necessary information. if we take the maximum width as 1 and the maximum depth as 1  a table with  attributes is created1. this is only for one instance. if the analysis requires 1 cases  the table size becomes huge. 
inductive logic programming  ilp   quinlan  1; 
muggleton and feng  1; pazzani and kibler  1   on the other hand  is more expressive and captures the relations most naturally in first-order logic. it can also handle noise  quinlan  1; pazzani and kibler  1 . to explore the potential of this approach  we tried to use focl  one of the most efficient ilp systems  to analyze the real data used in section 1. however  focl took more than 1 hours to find the first test condition of the first rule; therefore we had to give up this approach1. 
　gbi's expressiveness lies in between the attributevalue pairs and the first-order logic. it is a limited form of propositional calculus. its learning potential is much weaker than that of ilp  but stronger than that of the attribute-value representations and yet as efficient. we demonstrated that command prediction we addressed in this paper is a class of the problem that gbi's framework fits well. 
1 	other applications 
the idea of clipboard seems to be useful in designing interface systems of other kinds such as automatic chart 
   1 note that a typical  not maximum  single run of the latex command receives 1 input files  e.g.  .tex  .aug  . sty. .bbl  .eps  .tfm  .fmt  etc . 
   1 we have not taken advantage of the search strategy used in gbi. 
format selection in spread sheet and data base  naiveuser guidance and installation guidance-and-diagnosis systems. the last two are meant to apply the knowledge learned from expert behavior to non-expert users. during the development of clipboard  we were able to use the i/o information itself  i.e.  the raw history data  for debugging purposes. a good display system of this information seems to be beneficial even for an expert user. 
　we are aware of some minor things that could improve clipboard's ease of use. for example  we could improve 
clipboard's selection function by highlighting the second suggestion shown in the dialog box  see fig. g  b   when the user wants to override clipboard''s first suggestion  which is displayed by icon . 
　one promising application that goes beyond those within a single machine is dynamic world wide web caching. the rapid growth of information gathering through w w w causes a heavy network overload  and the resulting slow response is causing a problem. distributed caching is a promising approach. our preliminary study  yoshida  1  by gbi shows that it is possible to reduce the overload by extracting frequent occurring data transmission patterns from the wide area network flow and using this to allocate distribute cache storage. the simulation assumed the situation where 1 w w w servers are accessed simultaneously by 1 clients. each client and router had a 1 mb cache capacity. the data were taken from the access log of our proxy server that included 1 million data transfers  1 gb in size . figure 1 shows how the traffic changes with the time of day with and without cache  from which we observe 1% reduction of traffic between 1 am and 1 pm. the traffic reduction at the peak time amounts to 1 mb. 

figure 1: network traffic distribution over the time of day with and without cache 
1 	related work 
intellectual assistance by computers has attracted many people  and various attempts have been undertaken with 
	motoda 1c yoshida 	1 
different approaches and for different tasks. there are many terms that characterize these approaches such as learning apprentice  software agent  learning agent  interface agent  programming by example or demonstration  personal knowledge based system  etc. what is common to many of them is that they observe repetition or regularity in the user's behavior and use them for automation  prediction and customization in one way or another. 
　the amount of knowledge that has to be provided in advance varies among the approaches. general remarks are that making the user program everything requires too much insight  understanding and effort from the user  and having to encode a lot of domain-specific background knowledge about the task and the user also requires a huge amount of work from the knowledge engineer. both have fixed competence  and are hard to customize to individual user differences or changes of habits. some sort of automatic knowledge acquisition that can capture each user's habits is needed. 
　eager  cypher  1  is an example of program by demonstration  pbd   which is a hyper' text system that keeps watching a user's actions  detects an iteration and offers to run the iterative procedure to completion by generalizing the repetitions and making macros. myers 's demonstrational formatter  myers  1  is also an example of pbd. it does not focus on the repetition  but generalizes a single example to create a template for later use  which enables the formatting of headers  itemized lists  tables  references  etc. another example is gold  myers et a/.  1  which is a business chart editor. it is given the knowledge of properties of the data and the typical graphics in business charts to generalize a single  or a very few examples  by interpreting them as a combination of primitives. 
    greenberg and witten  1  analyzes repetitive patterns in the unix command histories and observes some regularities.  masui and nakayama  1  also uses the repetitive nature for a predictive user interface. when a user types a repeat key after doing repetitive operations  an editing sequence corresponding to one iteration is detected  defined as a macro  and executed at the same time. although being simple  it covers a wide range which had to formerly be covered by keyboard macro. 
　all of the above approaches do not use machine learning techniques although they do guess and generalize. the interface agent of  maes and kozierok  1  takes a machine learning approach. they address the problem of self-customizing software at a much more task independent level. the core is to learn by observing the user  i.e.  by find reguralities in the user's behavior and using them for prediction. they also adapt two other learning modes: learning from user feedback and learning by being told. they used memory-based learning  k-nearest neighbor  which is good for explanation. situations in the user are described in terms of a set of attributes which are hand-coded. the tasks that they applied are a calendar management agent and an electronic mail clerk. the personal learning apprentice cap  dent et a/.  
1 	invited speakers 
1  is similar to the above. it is an interactive assistance that learns continually from the user to predict default values. their application is a calendar management apprentice which learns preferences as a knowledgeable secretary might do. two competing leaning methods are used: decision tree learning and backpropagation neural net. the attribute value representation suffices for this purpose. another related system addresses the task of form-filling  hermens and schlimmer  1 . they use decision tree learning to predict default values for each field on the form by referring to values observed on other fields and the previous form copy. 
　 schlimmer and hermens  1 's pen-based interactive note taking system is a self-customizing software to eliminate the need for user customization. it starts with partially-specified software and applies a machine learning technique to complete any remaining customization. the system learns a finite state machine to characterize the syntax of user's notes and learns decision tree to generate predictions. letizia  lieberman  1  is an interface agent that assists a user browsing the www. it tracks user behavior and attempts to anticipate items of interest by doing concurrent  autonomous exploration of links from the user's current positions. intelligent agent for information browsing is a hot area and many systems are being pursued  e.g.   etzioni1 and weld  1; perkowits and etzioni  1 . 
　the research on prefetching is carried out by a separate community. the standard least recently used  lru  based caching offers some assistance  but ignoring any relationships that exist between file system events fails to make full use of available information. the closest work that uses the relationship would be  kroeger and long  1 . they use trie structure to memorize previous i/o sequence but no explicit learning is performed. their results indicate that the predictive caching gains on the average 1% more cache hits than the lru based caching. however  since they are using only sequential information  their method does not work well in a multi-task environment. 
　all of the applications that use machine learning techniques do not require relational representations. the data are represented by a set of features. analysis of sequential information is enough for the selected applications. some require additional task specific knowledge. we showed in this paper that there are other applications that this success cannot be easily generalized  and proposed the gbi as a general induction mechanism for this type of applications. 
1 	conclusion 
we have modeled a user adaptive interface that can predict next command  generate scripts and prefetch files in a multi-task environment. the analysis of behavioral data indicated that the directly observable sequential records are not enough to capture the behavior  and that simultaneous use of process i/o information that is hidden from the user is beneficial. an efficient induction algorithm that can handle relational data was needed and a technique called graph-based induction was applied. it can find frequently occurring patterns from a graph representation. it also induces classification rules from structured data that have intra-relationship. pairwise chunking  which is the heart of the algorithm  does not guarantee an optimal solution by any means  but empirical study shows that use of statistical measure results in a good solution. it is efficient and can run in real time. the command prediction module is in daily use. shell script generation works as expected but is less used. prefetching daemon still needs a better implementation to enjoy the real benefit . 
acknowledgments 
　much of the work was conducted while the first author was at arl  hitachi  ltd. authors are grateful to shojiro asai and katsumi miyauchi of arl for their generous support of this work. they extend their thanks to takashi washio  tadashi horiuchi and toshihiro kayama of osaka university for the discussions. 
