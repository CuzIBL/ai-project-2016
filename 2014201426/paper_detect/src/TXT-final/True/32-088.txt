 
context-specific independence  csi  refers to conditional independencies that are true only in specific contexts. it has been found useful in various inference algorithms for bayesian networks. this paper studies the role of csi in general. we provide a characterization of the computational leverages offered by csi without referring to particular inference algorithms. we identify the issues that need to be addressed in order to exploit the leverages and show how those issues can be addressed. we also provide empirical evidence that demonstrates the usefulness of csi. 
1 	introduction 
the theory of probabilistic inference begins with a joint probability over all variables of interest. the amount of numbers it takes to specify a joint probability is exponential in the number of variables. for this reason  probabilistic inference was thought to be infeasible until the introduction of bayesian networks  bns   pearl  1; howard and matheson  1 . making use of conditional independence  a bn factorizes a joint a probability into a list of conditional probabilities. the factorization renders inference computationally feasible in many applications because each of the conditional probabilities involves only a fraction of the variables. 
　in practice  there are often conditional independence relationships that are true only in specific contexts. the concept of context-specific independence  csi  was introduced specifically for such relationships. csi has its roots in the influence diagram literature  olmsted  1; fung and shachter  1; smith et al.  1  and was first formalized by  boutilier et al.  1 . researchers have shown that csi can be exploited to speed up various bayesian network inference algorithms such as symbolic probabilistic inference  d'ambrosio  1   search santos jr. and shimony  1   cutset conditioning 
 boutilier et al.  1; geiger and heckerman  1  clique tree propagation  ctp   boutilier et al.  1   arc reversal  cheuk and boutilier   1   and variable elimination  ve   poole  1 . 
　this paper results from efforts to identify the common principle underlying those works. we attempt to answer the following questions: why in general csi leads to faster inference  in other words  how do we characterize the computational leverages offered by csi without referring to particular inference algorithms  what issues do we need to address in order to exploit the leverages  how do we address those issues  finally  how much can we gain  
　it is well known that the computational leverages afforded by conditional independence can be characterized in terms of factorization: conditional independence allows one to factorizes a joint probability into a list of conditional probabilities. as it turns out  the computational leverages offered by csi can also be characterized in terms of factorization. more specifically  csi allows one to further decompose some of the conditional probabilities  giving rise to a finer-grain factorization of the joint probability. this is precisely why csi can speed up inference. 
　in order to take advantage of the finer-grain factorization  the main technical issue that one needs to address is that some of the factors in the factorization are partial functions. fortunately  this issue can easily be addressed using an operation called union-product. 
　in addition to providing a clear picture about the role of csi in probabilistic inference  this paper also gives a general method for exploiting csi. the method can be easily grounded with popular inference algorithms such as ctp  lauritzen and spiegelhalter  1; jensen et al.  1; shafer and shenoy  1  and ve 
 zhang and poole  1; dechter  1 . all one has to do is change one basic operation  namely replacing product of full functions with union-product of partial functions. 
　experiments have been performed to empirically demonstrate the effectiveness of csi. the results confirmed that csi can significantly speed up inference. 
1 bayesian networks and probabilistic inference 
to start with  this section briefly reviews the concepts of bayesian networks and factorization. we also explain 

1 	uncertainty and probabilistic reasoning 


figure 1: a bayesian network. 
why factorization is the key to efficient inference. 
1 bayesian networks 
a bayesian network 1  bn  is an annotated directed acyclic graph  where each node represents a random variable and is associated with the conditional probability of the node given its parents. in addition to the explicitly represented conditional probabilities  a bn also implicitly represents conditional independence assertions. 
let  be an enumeration of all the nodes 
in a bn such that each node appears before its children and let  be the set of parents of a node xi. the following assertions are implicitly represented: each variable xi is conditionally independent of variables in  given variables in 
　the conditional independence assertions and the conditional probabilities  attached to the nodes to-
gether entail a joint probability over all variables. as a matter of fact  we have 
		 1  
where the first equality follows from chain rule and the second follows from the conditional independence assertions. 
1 	probabilistic inference 
inference refers to the process of computing the posterior probability  of a list x of query variables after obtaining some observations  here y is a list of observed variables and y1 is the corresponding list of observed values. 
　　the posterior probability can be obtained from the marginal probability which in turn can be computed from the joint probability by marginalizing out variables outside 
one by one. since a bn implicitly represents a 
joint probability  one can in theory perform arbitrary inference. in practice  this is not viable because marginalizing out a variable from a joint probability requires an exponential number of additions. 
　the key to efficient inference lies in the concept of factorization. a factorization of a joint probability is a 
　list of factors  functions  from which one can reconstruct the joint probability. 
　　because of  1   we say that a bn factorizes a joint into conditional probabilities 	and  and that the conditional probabilities constitute a multiplicative factorization of the joint probability. the bn in fig-
ure 1  for instance  gives us the following multiplicative factorization of  
 1  
we will use this network as a running example through out the paper. 
　to see why factorization is of fundamental importance to inference  consider a joint probability  over n binary variables. to marginalize out a variable  means to compute 
 this computation is global since 
all variables are involved. it takes  numerical additions and hence is infeasible except when n is very small. now suppose we have a multiplicative factorization of 
consequently  we can marginalize out the factorization as follows: 
1. remove from the factorization all functions that involve  
1. compute the product of the functions; 
1. marginalize out from the product; and 
1. put the resulting function back to the factorization. 
this is the principle underlying inference algorithms such as ctp and ve. here one needs to compute 
this computation is local in the sense that it involves only some of the variables. it is usually much cheaper than the global computation mentioned above. 
　in our running example  marginalizing out  means to compute without factorization. 
this takes 1 additions. with factorization  on the other 
hand  one needs to compute which takes only 1 additions. 
1 csi and decomposition of conditional probabilities 
we next review the concept of csi and shows how it leads to decomposition of conditional probabilities. 
1 	context-specific independence 
let c be a set of variables. a context on c is an assignment of one value to each variable in c. we denote a context by where  is a set of values of variables 
	zhang and poole 	1 


in c. two contexts are incompatible if there exists a variable that is assigned different values in the contexts. they are compatible otherwise. 
　this following definition of csi is due to  boutilier et al.  1 . let x  y  z  and c be four disjoint sets of variables. x and y are independent given z in context 
 if 

whenever  when z is empty  one sim-
ply says that x and y are independent in context  
　as an example  consider four variables: income  profession  weather  and qualification. a farmer's income depends on weather and typically does not depend on his qualification. on the other hand  a office clerk's income depends on his qualification and typically does not depend on weather. in other words  income is independent of qualification in the context  professionsfarmer  and it is independent of weather in the context  profession=office-clerk . 
1 	decomposition of conditional probabilities 
to illustrate how csi leads to decomposition of conditional probabilities  we use as an example. consider and sepa-
rately. assume 	is independent of 	in context 
then 	consequently  we can decompose 	which requires 1 numbers to specify  into two smaller components and  which require only 1 numbers to specify. 
　to make the example more concrete  suppose  is given by the tree shown in figure 1 
 1 . the tree states that 	  for instance  is 1. because 	is independent of x1 in context the tree can be decomposed into the two smaller trees shown in figure 1  1  and  1   which represent and 	respectively. 
　next assume is independent of given in context and is independent of given in con-
text 	then 	 
and 	consequently  we can decompose which requires 
1 numbers to specify  into two smaller components  and which take only 1 numbers to specify. for concreteness  assume the two smaller components are given by the trees in figure 1. 

figure 1: decomposition of  
　after the decompositions  the decomposition given in  1  becomes 
		 1  
this decomposition is of finer-grain because the conditional probabilities of and have be broken up into smaller pieces. 
1 	making inference with refined factorizations 
this section shows how to make inference with factorizations such as the one given by  1 . a technical issue that we need to address is that some of the factors are partial functions. for example   is a partial function of  and in the sense that it is not defined for the case when  
　in general  a partial function of a set x of variables is a mapping from a proper subset of possible values of x to the real line. in other words  it is defined only for some but not all possible values of x. the set of possible values of x for which a partial function is defined is called the domain of the partial function. a full function of x is a mapping from the set of all possible values of x to the real line. in other words  it is defined for all possible values of x. in the rest of a paper  we will use the term  function  when we are not sure whether a function is a partial function or a full function. 
1 	union-product 
to manipulate partial functions  we need the operation of union-product. suppose x  y  and z are three disjoint sets of variables and suppose and are two functions. the union-product of  is the function of variables in given by 
	undef i n e d i f 	b	o	t	h	&  
       undefined if 	defined  undefined 
	if 	undefined  
defined if both 	& 
defined 
the operation is illustrated in figure 1. we sometimes 
write as to make explicit the arguments of / and g. when the domains of and  are 
1
	the notation 	is produced in 	using macro 

1 	uncertainty and probabilistic reasoning 


figure 1: union-product: the two circles in the left figure represent the domains of two functions g and h. the domain of the union-product  is the union of those of g and h. the union-product equals the product of g and h in the area where both g and h are defined; it equals g in the area where only g is defined; and it equals h in the area where only h is defined. 
disjoint  we call  the union of g and h and write it as  
　　here are some of the properties of the union-product operation. first  the union-product of two full functions is simply their product. together with the concept of union  this explains the term  union-product . second  the union-product of a full function with another function  full or partial  is a full function. third  the unionproduct operation is associative and commutative. we can hence talk about the union-product of a list of functions. the union-production of a list of functions will be denoted as  
1 	decompositions 
the concept of union allows us to rigorously defined decomposition. a list of functions with disjoint domains is decomposition of a function  a decompo-
sition is proper if no two functions in the decomposition share the same set of arguments. a decomposition of a function  is nontrivial if at least one function in the decomposition has fewer arguments than  itself. a function is decomposable if it has a nontrivial decomposition. 
　the function shown in figure 1  1  is decomposable. it can be nontrivially decomposed into the the two partial functions shown in figure 1  1  and  1 . 
1 	union-product factorizations 
with union-product  we can now make explicit the sense in which the list of functions given in  1  is a factorization of the joint probability  
　　a list of functions is a union-product factorization  or simply a up-factorization  of a function 
note that functions in a decomposition must have disjoint domains whereas domains of functions in a upfactorization might intersect. a decomposition is a upfactorization but not vice versa. 
　　for any variable  let be the set of functions in that contain as an argument. a up-factorization is normal if is a full function whenever  
the list of function given in  1  is a factorization of because 

where the first equality is true because the unionproduct operation is associative  the second equality follows from the assumptions made in section 1  and the third equality follows from the first property of unionproduct. the factorization is also normal. for example  
function  so must be union-product. 
　in general  let be the set that consists of  for each variable in a bn  the conditional probability of the variable or  when the conditional probability is decomposed  its components. then  is a normal up-factorization of the joint probability of all variables. it is of finer-grain than the multiplicative factorization given by the bn if at least one conditional probability is decomposed. 
1 	inference w i t h up-factorizations 
the following theorem  which we state without proof  lays the foundation for making inference with normal up-factorizations. 
theorem 1 suppose t is a normal up-factorization of a full function f and is an argument of  then is a normal up-factorization of 
　according to the theorem  a variable  can be marginalized out from a normal up-factorization as follows: 
1. remove from the factorization all functions that involve  
1. compute the union-product of the functions; 
1. marginalize out from the union-product; and 
1. put the resulting function back to the factorization. 
comparing this procedure with the one outlined in section 1  we see that existing inference algorithms such as ctp and ve can be adapted to work with normal upfactorizations by simply replacing product of full functions with union-product of partial or full functions. 
　the above descriptions are rather abstract. for example  implementations of union-product and marginalization are not given. those and other details can be found in a longer version of the paper  zhang  1 . in that paper  the method is also compared to previous methods. 
1 	csi and inference efficiency 
using the running example  this section illustrates why inference with finer-grain up-factorizations is more effi-
	zhang and poole 	1 


figure 1: elimination of variable 
dent than with the corresponding multiplicative factorizations. 
　consider marginalizing out  without csi  we need to compute 
		 1  
with csi  we need to compute 
	 1  
the second computation is cheaper than the first one for a number of reasons. the conditional probability  is decomposed into two components that 
takes fewer numbers to specify. the conditional probability  is also decomposed into two com-
ponents that take fewer numbers to specify. moreover  only one of those two components is involved in the second computation. as a consequence  the second computation has fewer variables  not involved  and fewer numbers to deal with. 
　in general  marginalizing out a variables from a finergrain up-factorization involves fewer variables and fewer numbers than with the corresponding multiplicative factorization. it is therefore cheaper. 
there is another reason why finer-grain up-
factorizations leads to faster inference. the result of expression  1  is a full function of  and  the full function happens to be decomposable and can be decomposed into the two partial functions shown in figure 1. in such a case  we can compute the decomposition directly. this is less expensive than computing the full function itself because the decomposition requires fewer numbers to specify. 
　when we compute decompositions of functions instead of functions themselves  we are preserving structures. preserving structures not only benefits the current step of inference but also simplifies future steps. it is therefore an important issue. in  zhang  1   this issue is addressed in detail. 
1 	empirical results 
experiments have been conducted to demonstrate the computational benefits of csi. a bn named water was used in the experiments 1. water is a model for the biological processes of a water purification plant. it consists 

figure 1: representation complesdties of conditional probabilities with and without csi. 

figure 1: performances of ve with and without csi. 
of 1 variables. strictly speaking  conditional probabilities of the variables are not decomposable. to make them decomposable  some of the probability values were modified. the induced errors are upper bounded by 1. using a decision-tree-like algorithm  quintan  1   we were be able to decompose some of the modified conditional probabilities and thereby reduce their representation complexities drastically 1. see figure 1. 
　the experiments were based on the ve algorithm and were performed on a sun ultra 1 machine. the task was to eliminate all variables according to a predetermined elimination ordering. in the first experiment  an ordering by kjaerullf was used 1. the the amounts of times in cpu seconds that ve took  with and without csi  to eliminate the first n variables for n running from 
1 to 1 are shown in left chart of figure 1. we see that ve ran much faster with csi. in particular  the entire elimination process took about 1 seconds with csi. without csi  however  it took about 1 seconds. 
　in the second experiment  we generated 1 new elimination orderings by randomly permuting pairs of variables in kjaerullf's ordering. a trial was conducted with each ordering. the performances of ve  with and without csi  across all the trials are summarized in the right chart of figure 1. we see that ve was significantly more efficient with csi than without csi. the speedup was more than one magnitude on average. moreover  there are four trials where ve was not able to complete without csi due to large memory requirements. with csi  on 

1 	uncertainty and probabilistic reasoning 

the other hand  ve completed each of those four trials in less than 1 seconds. 
1 	conclusions 
this paper studies the role of csi in bayesian network inference. it differs from earlier work in that we do not attempt to demonstrate the usefulness of csi in particular inference algorithms. rather  we provide a general characterization of the computational leverages offered by csi. this characterization fits well with the characterization of the computational leverages afforded by conditional independence. they both are in terms of factorization. while conditional independence provides one with a factorization of a joint probability  csi allows one to refine the factorization. 
　we clearly identify the issues that one needs to address in order to takes advantage of the computational leverages offered by csi. there are two issues: partial functions and preservation of structures. the first issue in addressed in detail and the second issue is addressed in  zhang  1 . we also give a general method for exploiting csi. the method can be easily grounded with popular inference algorithms such as ctp and ve. all one has to do is to replace product of full functions with union-product of partial and full functions. finally  we provide empirical evidence that demonstrates the usefulness of csi. 
acknowledgements 
the authors thank the anonymous reviewers for useful comments and suggestions. research is supported by hong kong research grants council grant hkust1e and natural sciences and engineering research council of canada research grant ogp1. 
