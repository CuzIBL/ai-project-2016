 
one of the weak points of the present critics of ai is their lack of an alternative cognitive theory. in the paper the outline of such a theory is sketched  based on  one reading  of piaget's genetic epistemology. this is used to clarify the ambiguity of the terms 'symbol' and 'representation'  thereby making it possible to suggest a reconciliation of the positions of the critics and defenders of the representational theory of the mind. 
1. introduction 
it is a strange situation. ai is flourishing  scientifically and in an ever growing number of applications. at the same time the field receives critique  not only from without but also from within  the recent book by winograd & flores  being one prime example. and the critique is aimed at the very foundations of ai  the knowledge representation hypothesis  smith  1  and the physical symbol system hypothesis  newell  1 . 
for those cognitive scientists sympathetic to the arguments of the al-critics  this situation creates an interesting cognitive dissonance  from which at least two interrelated questions emerge: what would a cognitive theory that could accomodate these seemingly contradictory positions look like  and what could such a theory say - or at least indicate - about what computers can do  
my aim with the present paper is to show how the two positions of 'mainstream ai' and its critics can be reconciliated by suggesting the outlines of one possible such theory and then suggest what an answer to the second question emerging from this position would look like. i don't really believe that the issue is of utmost importance for the active al-researcher  who i assume will continue with his work regardless of what philosophically interested scholars will say and do. but it is of some importance for those of us concerned with the impact of ai on society  since this impact comes not only from what existing systems can do  but perhaps to an even larger extent from what the layman and the politicians believe will be 
1 	foundations 
possible to do in the future. and this belief is shaped by claims of ai practitioners and critics. 
the essence of the argument is simple. i will claim that the two central terms in the debate  'symbol' and 'representation' are semantic ally ambiguous  and furthermore that this is overlooked by both sides in the debate  thus making way for an overestimation of the explanatory scope of their theoretical positions. if this claim is accepted  it is then possible to see why critics such as dreyfus and winograd & flores in essence are right in their arguments  but wrong in their predictions. 
nothing of what i will say is really new. what i have tried to do is to compile work from a number of different scholars into a hopefully coherent position  and tried to point out some conclusions which follow from it. since i am treading in the footsteps of a number of researchers  a large portion of the paper is devoted to a review of these positions. the limited space forces me to keep the reviews short  but i hope i will manage to give a flavor of their respective positions to those not familiar with them  and i am sure i will make readers familiar with the work of newell  maturana  piaget etc shake their heads in despair at my oversimplifications . 
1. the knowledge representation hypothesis 
knowledge and knowledge representation are important concepts for many areas of cognitive science. for ai  they are central. one could perhaps even say that the question of how knowledge should be represented in intelligent systems  and problems related to this  is the common denominator between the many diverse research areas of ai. for most researchers  knowledge is a symbolic representation  and inference or thinking is done through the manipulation of symbols. this is not something new or unique to ai. in fact  the basic idea was formulated very clearly by kenneth craik in the early forties  craik  1   and the information processing paradigm in cognitive psychology shares these assumptions. in ai  brian smith  has formulated explicitly the so-called knowledge representation hypothesis  which he claims in one form or another lies behind most work in artificial intelligence  though never explicitly formulated. 
he summarizes the hypothesis as follows: 	forming that process. any mechanically embodied intelligent process will be comprised of structural ingredients that a  we as external observers naturally take to represent a propositional account of the knowledge that the overall process exhibits  and b  independent of such external semantical contribution  play a formal but causal and essential role in engendering the behavior that mani-
fests that knowledge. 
the two important points are thus that there exists something that we as observers take as representing 'the knowledge'  and that this not only exists but actually is a part of producing the intelligent behavior. the latter part seems uncontroversial  but how is the knowledge represented  the standard answer in ai would seem to be something to the effect that it is symbolically represented. but what then is a symbol  this question is not exactly a new one  it's been with us since the ancient greeks. but what's interesting in the present context is that allen newell  has claimed that a new notion of symbols has evolved within ai. this notion of symbol is intrinsic to the concept of a physical symbol system  i.e. a class of systems capable of having and manipulating symbols  and at the same time realizable as a physical system. newell states explicitly that 'these symbols are in fact the same that we as humans have and use everyday of our lives'  and that 'humans are instances of physical symbol systems'  ibid  p 1 . intelligent programs are physical symbol systems too  this is where we have a connection between human intelligence and the broader class of intelligent systems that encompasses other intelligent machines too. however  he also claims that this notion of symbol 'is distinct from the notion of symbol that has arisen in describing directly human linguistic  artistic and social activities'  ibid  p 1 . 
i will not here give a detailed account of newell's rich argument  but concentrate on those aspects that are relevant for the present task  hopefully without doing to much injustice to the original paper. as newell observes  'the most important concept for a symbol system is that which gives symbols their symbolic character  i.e. what lets them stand for some entity'  ibid  p 1 . he calls this concept designation  claiming that it is more or less synonymous with terms such as reference  denotation  meaning  standing for  etc. he defines designation in the following way: 
designation: an entity x designates an entity y relative to a process p  if  when p takes x as input  its behavior depends on y. 
this process is closely connected to the process of interpretation  which is defined by newell as follows: 
interpretation: the act of accepting as input an expression that designates that process  and then peri assume that this picture seems familiar to most readers  even for those that have not read newell's paper  since what newell in a sense has done  is to have made explicit the meaning and usage of the term symbol  as it is used in artificial intelligence when for instance talking of lisp programming as symbolic programming. for most ai practitioners  
this position is obvious and self evident. 
however  while this paradigm has dominated  it has not been uncontested. hubert dreyfus' what computers can't do  dreyfus  1  is a well known  early example. dreyfus' critique  based on the philosophy of heidegger has hardly been accepted by the majority of the research community. more recently  terry winograd and fernando flores have criticized the same tradition from a similar perspective  winograd  1  winograd & flores  1 . this critique is based not only on heidegger's philosophy  but also on the biologist maturana's theories of the organization of living systems and of language and cognition  maturana  1  1 . in the next section i shall give a short review of the arguments of winograd and flores. 
1. knowledge is not represented 
winograd and flores challenge the assumption that thinking and other cognitive activities are based on the manipulation of mental representational structures. as previously mentioned  they base their analysis on the work of maturana  who is a biologist who did his first work on the visual system of the frog  maturana  et al. 1   where he was led to reformulate his views of the function of the visual system. this work has then led to work on language and cognition  where he has tried to use the results from the work on the visual system to explain these higher cognitive functions. 
starting from the traditional view  in which there exists  an objective  absolute  reality  external to the animal and independent of it  not determined by it  which it could perceive  cognize   and the animal could use the information obtained in this way to compute a behavior adequate to the perceived situation   maturana came to realize that  the central purpose in the study of color vision could not be the study of a mapping of a colorful world on the nervous system  but rather that it had to be the understanding of the participation of the retina  or nervous system  in the generation of the color space of the observer .  maturana  1  
the view that we don't have direct access to the outside world  but that our experience is partly determined by the activities in the nervous system  is not unique these days. but maturana goes one step further when he claims that the cognitive system does 
	dahlback 	1 
not make any use of any representational structures. the light striking the retina triggers chemical changes in the neurons  which causes the structure of the nervous system to change  cf the neurological argument in sec 1 . the arguments by winograd & flores are obviously more intricate than this. but i hope the foregoing sections make it possible to get a glimpse of a central part of their argument  namely that it is not only possible to explain intelligent behavior without using symbolic representations  but that any attempt to do so is based on a faulty epistemology  separating organism from the environment.  the tradition of doing so is called by them the rationalistic tradition. there seems to be a close resemblance between this and lakoffs  concept of the objectivist paradigm . 
1. what happened to cognition  
some cognitive scientists might dismiss the arguments by winograd & flores  on the grounds that they only consider biological  maturana  and philosophical  heidegger  aspects  and that therefore their arguments apply to other domains of explanation than cognition. maturana talks about cognition  but in a sense quite different from the way the term is usually used. one could argue that before the critics of the representational theory of the mind have formulated an alternative cognitive theory  their position has little to offer in way of an alternative for the cognitive scientist  be he psychologist or ai researcher.  there exists of course an alternative to this  and that is to claim that we don't need any cognitive theory to explain the mind. this seems to be the stance taken by searle  . 
however  in my opinion there already exists such a cognitive theory  or at least the skeleton of such a 
theory  and that is the genetic epistemology of jean piaget. i furthermore believe that this theory can be used  not only to fill the gap in the aforementioned critique  but also to clarify some of the issues raised in the debate between critics and defenders of the representational theory of the mind. 
1. a biological epistemology 
here i will not try to give any general overview of piaget's work  but instead confine myself to an introduction to his theory of knowledge  and the view of symbols and representation which follow from that theory. it is important to realize that what follows is different on central points from most english and american expositions of piaget. i am of course not claiming that my reading is the only   correct  reading of piaget's work. anyone who has read him in the original will realize how outrageous such a claim would be. piaget is not exactly an easy writer! and i am not interested in some exegetic analysis of the numerous books and articles published. my aim is simply to summarise one plausible reading  which i would 
1 	foundations 
claim is interesting in and of itself  and which also has interesting consequences for a discussion on the nature of knowledge and representation. 
jean piaget is best known as a child psychologist. but what makes him important is not perhaps primarily what he has taught us about the mental development of the child  but his use of experimental psychological methods  used within a biological framework  for studying epistemological questions. in his own words:  my most central concern has always been to determine the contributions of the person's activities and the limiting aspects of the object in the process of acquiring knowledge   flavell  1  p vii . 
for piaget  a biological organism implies a structure which is responsive to its environment. the stimulus is never something which is  out there   but rather that aspect of the environment to which the organism is responding. the central factor in this is the underlying structure of the organism. the stimulus is assimilated to the underlying structure  and if it could not be assimilated  it would simply not exist for that organism. 
this structuring exists on all levels of behavior. 
knowledge is used as a general term  much wider than conscious knowledge  and is in fact synonymous with this structuring aspect of action. knowing is therefore an activity of the subject  and knowledge is something constructed. that this view of knowledge as action-knowledge is relevant for lower organisms is not perhaps unique. it is has also been a central part of von bertalanffy's  general system theory  which in turn was inspired by von uexkull's umweltlehre  von uexkull  1  which was summarized by von bertalanffy as follows:  from the great cake of reality  every living organism cuts a slice  which it can perceive and to which it can react  owing to its psycho-physical organization  i.e. the structure of receptor and effector organs.   von bertalanffy  1  p 1  this position seems also to be similar to that of maturana. but what perhaps makes piaget unique is his claim that all intelligence and knowledge can be and should be seen in this perspective: in his insistence that knowledge is not caused by nor upheld through symbolic representations  while at the same time giving place for the symbolic or representational activities of the mind within an integrated theory. 
piaget distinguishes between two forms or types of knowledge  operational or operative  which is based on a logico-mathematical type of activity  of ordering  counting  classifying  i.e. activities for which the objects as such are no more than a support  and figurative  which is aimed at extracting information from the objects themselves  such as their colors  form  weight etc  inhelder  1 .  there is an obvious similarity between piaget's distinction operative-figurative knowledge and tulving's  distinction between semantic and episodic knowledge . piaget has primarily studied the operational knowledge - in a way you can say that he has studied the development of kant's categories. 
the operative knowledge  the logico-mathematical knowledge  is for piaget the primary knowledge: and this in two senses; it is considered the central aspect of thinking  with the figurative in some sense secondary or dependent on it  and it is the most studied aspect of the intellectual development. consistent with his view of knowledge as action-knowledge  an operation is an action in the same literal sense in which making a detour to find a desirable object is an action.  furth  1  but it is not the action as such  but the generalizable aspect of the act which is central  and which also is called a scheme   a scheme is  in fact  that which is generalizable in a given action   piaget  1  . these structures or categories are not inborn and ready-made  but the result of a developmental process  and the knowledge is not an internal copy or representation of the environment  but the action-structures which guides the organism-environment interaction. and these action-structures are not symbols. as furth puts it:  for piaget representation and symbol are consequences of knowing  not explanatory antecedents or intermediaries   furth  1  p 1 . more on this later. 
piaget uses the term figurative knowledge to refer to the static  figural and particular aspect of knowledge. he contends that this aspect is dependent on the operative knowledge  in the sense that perception  imagery etc is dependent on the current level of operative development.  to give one example of this dependency: before the development of eye-hand coordination  which is crucial for the development of the scheme of the permanent object  the child is unable to choose the bigger of two objects  when the bigger is further away and therefore has a smaller apparent size on the retina.  i will not go deeper into this part of piaget's theory here for two reasons. it is not needed for the rest of the argument  and it is a part of the theory that clearly is in need of further development. 
instead i want to point out that there is an obvious similarity between piaget's and maturana's positions. what seems to be a common denominator is the realization that the organism never reacts to its environment as a 'tabula rasa'  but only through its perceptual and cognitive structures  and that it is these as much as the 'objective world' which determines what is and is not the 'umwelt' of the organism. 
a central question in this context is obviously what makes it possible for an organism to react adaptively to the world. piaget uses a 'pragmatic' concept to explain this. he claims that there is a tendency for equilibration between organism and environment  and within the operative structures. in effect  this is only two sides of the same coin. as with other aspects of piaget's theory  the concept of equilibrium is not easy to understand. in fact  most of his readers would probably claim exactly the opposite!  for a discussion of this and other aspects of piaget's theory  linking it also to ai  see boden  1  but for our present purposes it is sufficient to note that piaget has started to develop a theory which makes it possible to explain the development of adaptive reactions to the environment without having to postulate some sort of 'copyknowledge' of the world as it is. another important point is the emphasize on viewing the organism in relation to its environment  which points to the close connection between knowledge and adaption  in a sense  newell's  the knowledge level expresses a similar point from a different perspective . 
1 representation 
i claimed earlier that piaget explains knowledge without the use of symbolic representations  while at the same time giving place for symbolic activities within the theory. to understand this  we need to clarify the meaning  or rather meanings  of the term 'representation   the following discussion leans heavily on the work by furth . 
the term 'representation' can be used in both an active and a passive sense. furth claims the active sense to be the primary one  to make something present by means of .. . here the person is the subject of the activity  and the symbol is a mediating instrument. in the passive sense the person has  disappeared   and the symbol becomes the subject of the sentence  as in  this map represents the city of timbuktu  . in this case there is some figural resemblance between the symbol and that which it represents  but there are also cases where the relation is more abstract or conventional  words in natural language being an obvious example. 
in modern cognitive science  with its roots in 1th century empiricist philosophy  the latter meaning of representation is the most common one. in this tradition  the internal representation of external reality is the chief explanatory factor for intelligent behavior  and knowledge is more or less synonymous with symbolic representations. 
in contradistinction to this  piaget retains the active use of the term representation  and explains knowledge without representations  as was pointed out in the previous sections. or  perhaps it would be more correct to say that he is aware of the two uses of the term  and in discussing symbolic behavior proper  language  images  dreams etc.  keeps this distinction in mind 
in fact the word 'representation' is used in two different senses. in the wide sense  representation is identical with thought  that is  with all intelligence which is not simply based on perceptions or movements  sensory-motor intelligence   but on a system of con-
	dahlback 	1 
cepts or mental schemes. in the narrow sense  representation can be limited to the mental image or to the memory-image  that is to the symbolic evocation of absent realities. moreover  it is clear that these two kinds of representation  wide and narrow  are related to each other insofar as the concept is an abstract scheme and the image a concrete symbol; even though one no longer reduces thought to a system of images  it is conceivable that all thought is accompanied by images. for if thinking consists in relating significations  the image would be a 'significr' and the concept a 'significatc'.  piaget  1  p. 1  
i will later try to show that this distinction between two meanings of 'representation' is paralleled by a necessary distinction between two meanings of the word 'symbol'. 
1 	symbols 
thus  for piaget the use of symbols is always seen in relation to the active aspect of representation mentioned before. they are seen as re-presentations  i.e. the evocation of absent realities. central to all symbols is the differentiation between the sign and its signification  which is something more than its referent . it is important to notice that piaget's denial of the symbolic nature of knowledge is closely tied to his notion of symbols  where the user of the symbol has the capacity to differentiate between the symbol and the symbolised  and that his comments on this issue should bee seen in the light of his arguments against an epistemological position that following hume makes no fundamental distinction between symbols such as images and objects of knowing. 
a critical point is of course what an operative structure  or a scheme  is. wason & johnson-laird  have shown that piaget's view of the so-called formal abstraction is not correct. there is no place in the present context to even hint at a way to resolve the problem. for those readers familiar with the work of wason it johnson-laird i just want to point out that i think the critique can be handled without damaging those aspects of the theory of concern for us now  sorry for the hand-waving! . 
1. 'a symbol is not a symbol' 
so  after this long  but still very condensed and difficult to digest exposition of some central aspects of piaget's theory  i will try to use it for a hopefully clarifying discussion of the twin concepts 'symbol' and 'representation'. and this can be done without having to accept piaget's theory in all its details. 
as we saw in the beginning  critics of ai and cognitive science in general  such as dreyfus and winograd & flores  have as one of their favorite arguments that the mind is not symbolic in nature. and on the other side we have the knowledge representation hypothesis and the physical symbol system hypothesis  ex-
1 	foundations 
plicitly formulated by smith and newell  but more or less implicitly accepted by the majority of workers in these fields  and whose major point is that the mind is symbolic in nature  and that the development of artificial symbolic manipulation devices  i.e. computers  makes the development of artificial intelligences possible. and my simple point is  that in some sense both are correct! correct in their premises that is  but due to a lack of understanding of the limitations of their theory's domains of explanation  wrong in their conclusions. 
if by a symbol we mean something which makes it possible to make an absent reality present in the same way that a child uses a shoe box as a symbol of a bed  i.e. with a knowledge on part of the symbol user that the symbol is not that which it represents  then it is obvious that an ai system or newell's physical symbol system is not a symbol-using system in this sense  and the 'knowledge representation' is not a representation in the narrow sense of the word. the 'symbols' are simply something for the program to react to. but they stand for nothing  they mean nothing to the program. an analogy from neurology can perhaps clarify this position for the doubting reader. 
penfield & roberts  have shown that electrical stimulation of neurons in the brain of patients undergoing brain surgery with local anaesthesia caused visual experiences in these patients. my claim is that the action of the neurons reacting to this simulation are essentially similar to a running computer program. the input is something to react to automatically  and it makes no sense to ask what it means to the system. it has no meaning  it is not a symbol in the strict sense of the word  and  as penfield and roberts have shown  it makes no difference for a local point in the system where the 'stimulation' comes from. it's there  and the reaction is determined by this and only this  if the structure of the neuron or lisp function or whatever is taken as given . this is related to searle's often quoted claim that the  'symbolic'  programs 'have only syntax but no semantics'  searle  1 . 
it is in this sense that the mind is not symbolic in nature  and knowledge is not represented  and it is this that i take as the core aspect of and common denominator of the theories of piaget and maturana. and if we take knowledge to be the generalizable aspect of the organism-environment interactions  there are no computers that have 'knowledge'. and it is here that i think that the positions of the al/cognitive science critics are essentially correct. 
but it seems to me that they have missed an important point. notwithstanding their criticism of current theories in the field  humans can use symbols! and use them  also in thinking. we use an inner language for solving problems  we use imagery for problem solving and daydreaming; symbols function as a support for a number of cognitive activities  and sometimes the way we have represented  in the strict or active sense  the problem can actually prevent us from finding the solution. 
1. a synthesis 
let us then try to formulate a synthesis which can accommodate both uses of the term 'symbol*. it would look something like this: there exists two different levels or domains. the first  and in some sense primary one  is the organism/environment interaction as described by maturana  or the sensory-motor and operational knowledge of piaget  and for those of you familiar with his theory  it also resembles the cartesian automata of johnson-laird . behavior can on this level be described and explained without postulating any mental symbolic representations in the strict sense. 
the second level is the symbolic level proper. this exists as a superstructure  as it were  in some organisms - at least in humans. they can use symbols  especially language and images to reflect  plan etc; i.e. to reason about future actions in 'mental models' in their working memory. this has all the disadvantages pointed out by winograd & flores when the gap between the symbolization and the actual is too wide. but it is still of immense value in well understood but complex domains. especially if the agent is aware of the fact that the model is only a model  and therefore is alert to the possibility that it can be incorrect. after all  it is only a map  and maps should not be confused with the reality  as we all know. 
1. and the consequences for ai  
the situation for ai is somewhat paradoxical. as pointed out previously  the computer system cannot be seen as a symbol-using system in the strict sense. some readers might then draw the conclusion that this fact holds a promise that sooner or later we will be able to create intelligent machines  with an intelligence similar to the human intelligence  since i also claimed that this is what characterizes it too. but i would claim that the only possible conclusion to draw is in fact exactly the opposite! and here is the reason: 
since the computer is not a biological organism  interacting with the environment  its 'knowledge' has to be spoon fed  as it were  into the system. and the person doing the spoon-feeding is the programmer. and the only knowledge that he can use for this process  is knowledge that for him is symbolized. symbolization is a prerequisite for communication of complex ideas or concepts. in a sense  the computer and it s program can be regarded as an externalization of a mental symbolic model. 
this explains why the most successful intelligent programs  regardless of their use of ai techniques or not  has been developed in areas where there exists a usable symbolization of the necessary knowledge. chess programs are a good example of this. the necessary knowledge had been symbolized and developed through centuries to make it possible for chess players to communicate their experiences. and the 'only' thing the the program developers had to do was to adapt this to the strengths and weaknesses of the computer. and of course it works  as do a number of 'number crunching' programs  for which the situation is essentially the same. and they work even better than humans  since the computer is not hampered by the processing limitations of man's conscious mental processes such as slow speed and memory limitations. 
the critique of dreyfus & dreyfus that this is not the way a human functions is then almost correct  but not in one essential aspect; when a person needs to consciously manipulate his knowledge  or communicate it  then he uses symbolic representations. and these representations can be manipulated in a computer. therefore  we can say that there are two plausible answers to the question if an ai program is symbolic. seen from within the program  it is not symbolic in the strict sense. but for us  the users of the system  it manipulates symbols. it is an extension of our working memory. 
so the key issue when discussing what expert systems will be able to do in the future is not if the mind uses symbolic representations or not. in most cases it doesn't. but that is not the point. consequently  the success of expert systems in this perspective will not be dependent on the knowledge engineers success in uncovering the rules and representations that the expert uses. even if dreyfus & dreyfus  have almost exclusively studied a domain  chess  whose rules can be explicitly formulated and where clearcut criteria for success can be established - something which makes the generaliz ability of their findings an open question - we can accept their claim that experts don't use rules. 
the key issue is instead whether we can find ways of symbolizing and representing knowledge that before the advent of ai systems we had no need to handle in such ways. 
a corollary of the position put forward here is that even if the critiques of the representational theory of the mind are essentially correct  in that the mind is not a symbolic system  there is no way to mark in advance the demarcation line between what can and cannot be handled in an al-system. this will all depend on the possibility of developing formal symbolic representations for knowledge where such does not presently exist  simply because previously there was no need for them. non-monotonic logics seem to be a prime example of this. 
	dahlback 	1 
1. much ado about nothing  
so what was all this about  the consequences for ai don't seem all that startling. just continue to develop formalisms for representing and manipulating areas of knowledge not previously manipulated. it is an open question how successful we will be  and this question is in a sense empirical rather than theoretical. claiming otherwise would be as sensible as claiming that zenon was right when he said that achilles would never catch up with the turtle  just because at that time there did not exist an efficient formal way of representing the problem. 
but  as i said in the beginning  the importance of arguments such as these lies not so much in their consequences for the ai reserarcher in his work  but rather - if they are accepted - in modifying some claims about what computers can do and not do in the future. so philosophical arguments are more important for their social implications than for their scientific consequences. 
on the other hand  there are some important consequences for another branch of cognitive science  namely psychology. but that  as hans christian andersen said  is another story. 
acknowledgements 
many friends at the department of computer science  linkoping university  and department of psychology  stockholm university have participated in discussions related to the issues in this paper. none of them had a chance to influence the content of the paper  so not to embarrass anyone i just thank you all for enjoyable and sometimes clarifying discussions. special thanks to ivan rankin for his effort to transform my swenglish into english. 
