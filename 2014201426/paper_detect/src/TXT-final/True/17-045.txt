 
　　　this paper describes a new theory of how spreading activation may occur in associative memory models formulated as parallel activation networks. the theory postulates that competition for activation by nodes/concepts in a network is a fundamental principle of memory r e t r i e v a l . using only excitatory connections between concepts  a specific implementation of this model is able to demonstrate  virtual lateral inhibition  between competitors and other interesting behaviors that have required use of explicit inhibitory connections in the past. 
	i 	introduction 
　　　during the last several years there has been a great deal of interest in ai in determining what kinds of parallel architectures best meet the needs of various ai tasks. this paper is concerned with  value-passing systems   networks in which the processing elements communicate by passing around continuous quantities  numbers  and by performing simple arithmetic operations on these values   1   . such architectures are often intended as models of associative memory  and frequently they are characterized by an analogy with neurobiological networks and processing paradigms. recent examples include  connectionist models    1      interactive activation models    the boltzman machine  and act ml 
　　　this paper presents a new  competitionbased  theory about how spreading activation may occur in value-passing associative memory models. first some terminology and the need for a model of spreading activation that can support  virtual lateral inhibition  are discussed. then a theory is introduced which postulates that competition between cognitive activities for limited resources is a fundamental organizing principle of memory retrieval. a specific instantiation of the theory is used to illustrate the concepts involved. 
 supported by the office of naval research  the national science foundation  dcr-1   and software architecture and engineering  inc. 
	ii 	parallel activation models of memory 
　　　in memory models implemented as valuepassing systems  each processing node typically represents a  concept  or  hypothesis   and the level of activation associated with a node represents the relevance of or confidence in the concept/hypothesis represented by that node  1 1. for this reason  and because of the distributed nature of the computations involved  the term parallel activation network is used for such models. nodes communicate with each other using links which have one or more weights associated with them. if the link from node a to node b has a positive weight  indicating that activation of node a tends to increase activation of node b  then such a link is excitatory. conversely  a negative weight indicates an inhibitory connection. there are both excitatory and inhibitory interconnections at the level of neuronal circuits in the nervous system. it is therefore not surprising that 
both types of interconnections have been adopted by way of analogy in higher-level cognitive networks. 
　　　it has often proven convenient to view parallel activation networks as partitioned into layers of conceptually similar nodes  and to conceive of information as i n i t i a l l y entering one  lowest  layer  e.g.  features  and propagating  upwards  to others  e.g.  letters   1   when nodes in a lower layer directly inhibit nodes in a higher layer such inhibition may be called forward inhibition since it is in the direction of i n i t i a l flow of activation. conversely  inhibition in the opposite direction may be called backward inhibition  and inhibition between nodes in the same layer can be referred to as lateral inhibition. lateral inhibition has long been recognized to play an important role in contrast enhancement in neural networks and has been argued to be an important aspect of selective behavior   1   . explicit lateral inhibitory links have also often been used in parallel activation models of associative memory in cognitive science to produce selective behavior  1 1   representing one way in which these recent cognltlvely-oriented models have borrowed processing paradigms from earlier neural modeling studies. for example  figure 1 shows two mutually inhibitory nodes n1 and n 1 . with a 

typical model for spread of activation even a 
transiently higher input to one node  say n   followed by equal inputs to both nodes can lead to stabilization of activation with one node maximally activated  n1 and the other having zero activation  n1 . this  winner-take-all  phenomenon  comes about because of the lateral inhibitory connections. 

figure 1: lateral inhibition using explicit inhibitory links between two  competing  nodes  modified from  . weights are indicated adjacent to connections. 
　　　while lateral inhibition is a useful concept in parallel activation models  the use of explicit lateral inhibitory links to achieve lateral inhibition poses a number of d i f f i c u l t i e s for implementing real world models of associative memory. at the cognitive level  in contrast to the neurobiological level  there is at best limited empirical evidence for such explicit  negative associations  between the concepts involved. published tabulations of the associations between related concepts usually include only positive weights  frequencies . since these empirical studies do not provide negative weights between competing concepts  one is faced with the problem of determining how such weights are to be assigned in building parallel activation networks where weights are based on probabilitites   furthermore  as the number of  competing  nodes grows  the number of mutual lateral inhibitory connections needed can grow quite rapidly. for example  in the print-to-word mapping  a single character node may connect to thousands of word nodes  each of which would apparently require a mutually inhibitory connection with all of its competing word nodes   
　　　in the following an approach to parallel spreading activation is proposed that can resolve these problems. specifically  this approach is capable of producing virtual lateral inhibition: apparent lateral inhibition between competing nodes in the absence of explicit lateral inhibitory connections. this phenomenon is produced without giving up the notion that computations in parallel activation networks should be local: each node can only  see  the activation and link weights of its immediately adjacent neighbor nodes. 
	i l l 	competition-based spread of activation 
　　　in previous parallel activation models  the activation flowing into a node is not determined by the level of activity possessed by the receiving node. typically  the incoming 
j. reggia 1 
activation  seen  by a node niis a weighted sum or some other function of the activities of its neighbors  1 1 1   this incoming activity is used by node ni to update its own activation level  which is then distributed by node ni during the next increment of time as output to appropriate neighbors. 
　　　in the competition-based parallel activation theory described here  the spread of activation is determined in a different way. when a node ni assumes an activation level above its normal resting level  its neighbors actively compete for the  energy  possessed by node ni. further  the ability of a neighboring node to compete for n i 's activity or resources is proportional to that neighbors existing level of activation. resources acquired by a neighbor node in this fashion occur at the expense of resources that are available to its competitors  leading one to predict that virtual lateral inhibition w i l l occur. the metaphor used here is that the  stronger  a node is   i . e .   the higher its level of activation   the more effectively it can compete with other nodes for a source of energy/activation. 
　　　to examine this idea  one possible instantiation of competition-based spreading activation is presented below  other formulations are possible and are currently being explored . in the formulation given here  the presence of an underlying associative network is assumed where the nodes in the network are partitioned into disjoint layers as illustrated in figure 1. for convenience  we also assume that each node in one layer is directly connected to at least one node in each adjacent layer.  these assumptions are not an essential part of the theory.  
　　　each connection between a node in one layer and that in another has two weights reflecting the directionally-oriented frequencies of association between the  concepts  represented by the nodes. for example  if ni is a node in one layer associated with a node nj in a different layer  then a bidirectional link appears between ni and nj in the network. one weight  attached  to this link is w i j representing the frequency  conditional probability estimate  with which the concept represented by ni occurs given that the concept represented by nj is known to be present. similarly  the attached weight wji represents the frequency with which nj occurs given that ni is known to be present. we restrict such weights to 1   w s t   1; the non-negative nature of these weights distinguishes them from the possibly negative  synaptic weights  in neural models and in psychologically-oriented parallel activation models of associative memory  1. nodes in this example network do not explicitly inhibit other nodes in either the same layer or in adjacent layers. furthermore  permitting weights to be highly asymmetric  wij wji in general  also distinguishes this approach from others  1 . 

1 	j. reggia 

figure 1: 	a word-sense disambiguation network. 
　　　as a specific  toy  example  consider a grossly simplified version of the problem of mapping a word into the appropriate word sense  figure 1 . layer w represents word/morpheme units  assume there are just four possible words   layer s represents possible word senses or  meanings   again  just four   and layer c  which we ignore for the time being  represents a number of possible semantic contexts in which word sense disambiguation can occur. note that each word node wi relates to multiple senses  e.g.  node w1 is connected to s1  s1 and s1  reflecting the ambiguity of words in natural language. thus {s1 s1 s1} represents the nodes in layer s which are competitors for the t i t l e 
　　　of  sense indicated by w1.  similarly  each word sense is connected to multiple possible words  e.g.  synonyms . 
　　　in figure 1  the weight wji corresponding to spread of activation from node ni to node nj is written adjacent to nj. the weights on  outputs  from a node in one layer to nodes in an adjacent layer sum to 1  consistent with our earlier definition of weights as conditional probability estimates. each node ni in memory has an activation level a i   t   at time t  constrained so that ~ and we let a i   t   represent the belief in the entity represented by node ni. each node also has a natural resting activation level ri and a decay rate di reflecting how quickly a i   t   returns to the resting level in the absence of external influences. we make two assumptions. first  we assume that at time t the maximum rate at which node n1 is capable of distributing activation to its competing neighbors in an adjacent layer is proportional to a i   t   . second  we assume that ni parsimoniously  desires  to support at most a total of one unit of activation in associated competing nodes in an adjacent layer. thus  the total amount of activation ni distributes to an adjacent layer at any point in time decreases as the collective activity of its neighbors in that layer rises. for example  in going from words to word senses  this second assumption states that each word maps onto exactly one of its possible senses. these two assumptions can be approximated by stating that the total output of node n  to its neighbors at time t is given by 

as long as this quantity is non-negative  and zero otherwise. 
　　　the idea of competition is introduced into this model by permitting the neighbors of ni in an adjacent layer to actively compete for the total output activity of ni the a b i l i t y of neighbor node nj to compete for ni's output activity o u t i   t   is proportional to its  strength  a j   t     and to the weight of association w j i . thus  the activation out ji t  transferred from ni to nj at time t is given by 

 the symbol  *  is read  is proportional t o     .  stronger  neighbors of ni therefore extract a greater portion of n j 's f i n i t e activation energy  leaving a smaller portion for  weaker  competitors. it is the fact that a j   t     the activation of the receiving neighbor node  appears in the formula for o u t j i   t   above that makes this a competition-based model. finally we define the total flow of activity into a node nj at time t to be 

the sum of a l l its inputs from neighbors. 
　　　in summary  a specific form of spreading activation has been described where the portion of a node n i 's output activation going to neighbor node nj is proportional to the a b i l i t y of nj to compete for that activity  reflected by the formula for o u t j i   t   above . given this competition-based approach to distribution of activation  we can adopt an approach similar to that used by others to update a node's activation level  e.g.   . let the symbol i |   t   indicate the net flow of activation into nn at time t  i . e .   

then the rate a i   t   at which ni's activation changes is given by the net flow of activation into the node minus the decay: 
the term assures that a i   t   approaches its maximum value asymptotically. note that a l l of the above computations are local in the sense that ni only needs to  see  the activations and weights associated with its immediate neighbors. 
　　　two numerical examples are now presented to demonstrate the behavior of the above competition-based parallel activation model. these examples are based on the network in 
figure 1  and were implemented using pan  a parallel activation network simulator. pan is a 
lisp program that permits one to specify a network and method for spread of activation  and to describe external inputs to the network that are to occur during simulation  pan then performs the indicated simulation while periodically displaying the activity level of nodes  pan is similar in s p i r i t to iscon 
 . in the examples  pan is used with the specific competition-based model of spreading activation described above. we have run larger simulations  over 1 nodes  with different networks but space limitations prevent their discussion. 

figure 1: a simulation specification given to pan simulator based on network in figure 1. 
　　　figure 1 illustrates a  simulation specification  as it is given to pan. the f i r s t eight lines specify the four word and four word sense nodes of figure 1  indicating that each node has both a resting activity and a decay rate of 1. the next four lines specify the parent connections into the word nodes along with appropriate weights  and the subsequent four lines specify the corresponding son connections into the word senses  compare with figure 1 . the next line  indicating external 
input to the network  has the format  input node start-time stop-time amount . 
figure 1 thus indicates that an external input of 1 units of activation enters word w1 during the f i r s t 1 units of time  simulating the occurrence of word w1. next a constant delta of 1 units of time is specified  indicating the fineness of time quantization  and pan is told to run the simulation for 1 units of time. 
upon giving the simulation specification in 
figure 1 to pan  the following output was produced by the above competition-based model: 
time 	hi 	s1 	s1 	s1/s1 
1 	1 	1 	1 
	1 	1 	1 	1 	1 
1.1.1.1.1 1 1 1 1 1.1.1.1.1 
 1.1.1.1.1 1 1 1 1 
1 	1 	1 	1 	1 1 	1 	1 	1 	1 
	j. reggia 	1 
competition-based spread of activation. f i r s t   the spread of activation is circumscribed in that it radiates only to nodes s1 and s1  the senses evoked by w1. activation levels in a l l other nodes remain at zero. second  although the input stimulus terminates after six units of time  activity in the activated nodes subsequently remains stable. this is reminiscent of  stable coalitions 1 . third  this example demonstrates virtual lateral inhibition between s1 and s1  with the dominant s1 rapidly suppressing activity in s1 indirectly by absorbing the majority of activition available from w1. past models of spreading activation have only produced such lateral inhibition by having explicit inhibitory connections between nodes like s1 and s1. finally  note that there is a brief i n i t i a l period of time when the ratio of activity s1/s1 is relatively high. this i n i t i a l  window of 
opportunity  makes possible some interesting context effects  as illustrated below. 
　　　for the second example  we illustrate how a prexisting  context  can result in the appearance of a switch being thrown to redirect the flow of activation. to do this  we use the same simulation specification as that given in figure 1  except the previous single external 
input statement is replaced with the following two statements: 
	 input 	w1 	1 	1 	1  
	 input 	s1 	1 	1 	1  
the f i r s t input stimulus is exactly the same as in the preceding example. the second weaker input  1  is directed to s1  recall that s1 was indirectly inhibited by s1 in the previous example . this latter  input  is used for illustrative purposes to simulate positive feedback from the  context  c1 in which disambiguation of word w1 is occurring  see layer c in figure 1 . in this situation  pan produces the following l i s t i n g of activations: 
	time 	w1 	s1 	s1 

1 	1 	1 
	1 	1 	1 	1 
1.1 	1 	1 
	1 	1 	1 	1 
1 	1 	1 	1 1 	1 	1 	1 
	1 	1 	1 	1 
in this example  the absolute contextual effect produced by retrograde excitation to node s1  switches  or  gates  the flow of activity from node w1 completely to s1  contrast with the f i r s t example above . the absolute switching here is contingent upon the resting level of s1 and s1 being zero i n i t i a l l y ; in networks where ri might be a very small positive number  perhaps reflecting the relative frequency with which the concept represented by node ni occurs  switching/gating of the sort demonstrated here 

this 	simple 	 lossless  	network 	illustrates 	a number 	of 	important 	properties 	possible 	with 

directs a small amount of activation to s1. 

1 	j. reggia 
	iv 	discussion 
　　　this paper has presented an approach to spreading activation in parallel associative networks that is distinguished from previous approaches by its competition-oriented nature. a theory of competition-based parallel activation as a model of associative memory has not been significantly studied in ai in the past  although there is some relevant related work. for example  the idea of nodes as active agents bears some resemblance to  actors   1  and the  contract net framework  involves at least an implicit notion of competition  1. however  both of these and similar models are 
 message passing systems  at the ai symbol processing level  1. others have postulated competition and/or parsimonious allocation of activation/energy as important influences in cognition  but this work has been at the  hardware level  of neural modeling and has been formulated quite differently  1 1 1. 
　　　as illustrated above  even in the absence of explicit inhibitory connections between nodes  a competition-based approach to spreading activation can exhibit a number of important properties: virtual lateral inhibition between appropriate nodes  circumscribed activation  stability of activation   stable coalitions    and context effects  e.g.  switching . our research group is currently investigating the feasibility of developing a full-scale model of the cognitive activities involved in the real world print-to-sound mapping . this task should provide an excellent test of the theory of competition-based spread of activation proposed in this paper. 
