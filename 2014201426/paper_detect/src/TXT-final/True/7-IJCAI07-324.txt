
decentralized decision making under uncertainty has been shown to be intractable when each agent has different partial information about the domain. thus  improving the applicability and scalability of planning algorithms is an important challenge. we present the first memory-bounded dynamic programming algorithm for finite-horizon decentralized pomdps. a set of heuristics is used to identify relevant points of the infinitely large belief space. using these belief points  the algorithm successively selects the best joint policies for each horizon. the algorithm is extremely efficient  having linear time and space complexity with respect to the horizon length. experimental results show that it can handle horizons that are multiple orders of magnitude larger than what was previously possible  while achieving the same or better solution quality. these results significantly increase the applicability of decentralized decision-making techniques.
1 introduction
for over 1 years  researchers in artificial intelligence and operations research have been working on decision making under uncertainty. the markov decision process  mdp  has been proved to be a useful framework for centralized decision making in fully observable stochastic environments. in the 1's  partially observable mdps  pomdps  were introduced to account for imperfect state information. an even more general problem results when two or more agents have to cooperate to optimize a joint reward function  while having different local observations. this problem arises in many application domains such as multi-robot coordination  manufacturing  information gathering and load balancing. the decentralized partially observable mdp  decpomdp  framework is one way to model these problems. it has been shown that finite-horizon dec-pomdps are nexp-complete  bernstein et al.  1 . thus  decentralized control of multiple agents is significantly harder than single-agent control and provably intractable. even ¦Åapproximations are hard  rabinovich et al.  1 . due to these complexity results  optimal algorithms have mostly theoretical significance. consequently  researchers have started to focus on approximate algorithms to solve significantly larger problems. but scalability remains a major research challenge and the main question - how to use the available memory and time most efficiently - remains unanswered. a detailed survey of existing formal models  complexity results and planning algorithms is available in  seuken and zilberstein  1 .
¡¡this paper presents a fundamentally new approach to overcome the time and space complexity of existing optimal and approximate algorithms. most of the previous algorithms cannot solve problems with a horizon larger than 1. our goal has been to increase this horizon barrier by several orders of magnitude. the approach is based on the first exact dynamic programming  dp  algorithm for finite-horizon decpomdps  hansen et al.  1 . the original dp algorithm  presented in section 1  builds the final solution bottom-up  starting with a 1-step policy for the last time step and successively working towards the first time step. in every iteration  it eliminates unnecessary strategies to reduce the total number of policies kept in memory. unfortunately  the algorithm runs out of memory very quickly. the key observation is that this algorithm keeps too many policies in memory  even though only very few of them are actually necessary for optimal or near-optimal behavior. thus  the main idea is to identify a small set of policies that are actually useful for the construction of good joint policies. this is achieved by using a set of top-down heuristics to identify tentative policies that are not necessarily optimal  but can be used to explore the belief space. section 1 describes how a set of relevant belief states can be identified using these heuristics. the dp algorithm can then select the optimal joint policy trees for those belief states  leading to a bounded number of good policy trees. section 1 presents the details of our memory-bounded dynamic programming algorithm and proves its linear time and space complexity with respect to the horizon length. we have applied our algorithm to a set of standard test problems from
the literature. section 1 details the results and shows that the algorithm can solve problems with horizons that are multiple orders of magnitude larger than what was previously possible.
1 related work
over the last five years  researchers have proposed a wide range of optimal and approximate algorithms for decentralized multi-agent planning. in this paper we focus on finitehorizon problems; infinite-horizon problems often require different solution techniques. one important class of finitehorizon algorithms is called  joint equilibrium-based search for policies   jesp   where the algorithms do not search for the globally optimal solution  but instead aim for local optimality  nair et al.  1 . the authors introduce the idea of focusing on the reachable belief states  but the approach still leads to exponential complexity.
¡¡the approach that is closest to our work is called pointbased dynamic programming for dec-pomdps  szer and charpillet  1 . that algorithm computes the policies based on a subset of the reachable belief states. but in contrast to our work  this approach does not employ top-down heuristics to identify the most useful belief states and it is not memory-bounded. it still leads to double-exponential worstcase complexity.
¡¡a dec-pomdp can also be seen as a partially observable stochastic game  posg  with common payoffs  emerymontemerlo et al.  1 . in this approach  the posg is approximated as a series of smaller bayesian games. interleaving planning and execution  this algorithm finds good solutions for short horizons  but it still runs out of memory after horizon 1.
¡¡another way of addressing the high complexity of decpomdps is to develop algorithms for problem classes that exhibit certain structure  becker et al.  1; goldman and zilberstein  1 . however  such techniques still have exponential complexity unless the interaction between the agents is very limited. a model that is particularly useful if the agents do not share the same payoff function is the i-pomdp framework  gmytrasiewicz and doshi  1 . i-pomdps have similar complexity barriers  leading the authors to introduce an approximate algorithm based on particle filtering  doshi and gmytrasiewicz  1 . this approach addresses the belief space complexity  but the policy space complexity remains too high and limits scalability.
1 solution techniques for dec-pomdps
we formalize the problem using the dec-pomdp framework  bernstein et al.  1 . our results  however  apply to equivalent models such as the mtdp or the com-mtdp  pynadath and tambe  1 .
definition 1  dec-pomdp  a finite-horizon decentralized partially observable markov decision process is a tuple  where   i is a finite set of agents indexed 1 ... n.
  s is a finite set of states.
  b1 ¡Ê ¦¤s is the initial belief state  state distribution .
is a finite set of actions available to agent i and a =  i¡Êiai is the set of joint actions  where  denotes a joint action.
  p is a markovian transition probability table.
denotes the probability that taking joint action a in state s results in a transition to state s.
 is a finite set of observations available to agent i and ¦¸ =  i¡Êi¦¸i is the set of joint observations  where  denotes a joint observation.   o is a table of observation probabilities.
denotes the probability of observing joint observation o given that joint action a was taken and led to state s.
is a reward function. denotes
the reward obtained from transitioning to state s after taking joint action a.
  t denotes the total number of time steps.
¡¡because the underlying system state of a dec-pomdp is not available to the agents during execution time  they must base their actions on beliefs about the current situation. in a single-agent setting  the belief state  a distribution over states  is sufficient for optimal action selection. in a distributed setting  each agent must base its actions on a multi-agent belief state - a distribution over states and over the other agents' policies. unfortunately  there is no compact representation of this multi-agent belief state. it only exists implicitly for each agent if the joint policy is known and the entire history of local observations is taken into consideration.
definition 1  local and joint policies for a dec-pomdp  a local policy for agent i  ¦Äi  is a mapping from local histories of observations oi = oi1 ¡¤¡¤¡¤oit over ¦¸i  to actions in ai.
a joint policy   is a tuple of local policies  one for each agent.
¡¡solving a dec-pomdp means finding a joint policy that maximizes the expected total reward. a policy for a single agent i can be represented as a decision tree qi  where nodes are labeled with actions and arcs are labeled with observations  a so called policy tree . let qti denote the set of horizon-t policy trees for agent i. a solution to a dec-pomdp with horizon t can then be seen as a vector of horizon-t policy trees  a so called joint policy tree
  one policy tree for each agent  where qit ¡Ê qti. these policy trees can be constructed in two different ways: top-down or bottom-up. if the goal is the optimal policy  both techniques lead to the same final policy. but if the goal is an approximate solution  the different characteristics of the construction processes can be exploited.
top-down approach the first algorithm that used a topdown approach  maa   makes use of heuristic search techniques  szer et al.  1 . it is an extension of the standard a  algorithm where each search node contains a joint policy tree. for example  if ¦Ä1 is a horizon-1 joint policy tree  an expansion of the corresponding search node generates all possible joint policy trees of horizon 1 that use the joint policy tree ¦Ä1 for the first two time steps. using a set of heuristics that are suitable for dec-pomdps  some parts of the search tree can be pruned. but the algorithm runs out of time very quickly  because the search space grows double exponentially.
bottom-up approach: dynamic programming the first non-trivial algorithm for solving dec-pomdps used a bottom-up approach  hansen et al.  1 . policy trees were constructed incrementally  but instead of successively coming closer to the frontiers of the trees  this algorithm starts at the frontiers and works its way up to the roots using dynamic programming  dp . the policy trees for each agent are kept separately throughout the construction process. in the end  the best policy trees are combined to produce the optimal joint policy. the dp algorithm has one iteration per step in the horizon. first  all policies with horizon 1 are constructed. in each consecutive iteration  the dp algorithm is given a set of horizon-t policy trees qti for each agent i. the set qti+1 is then created by an exhaustive backup. this operation generates every possible depth-t+1 policy tree that makes a transition  after an action and observation  to the root node of some depth-t policy tree. after the exhaustive backup  the size of the set of policy trees is |qti+1| = |a||qti||o|. if all policy trees are generated for every step in the horizon  the total number of complete policy trees for each agent is of the order o |a| |o|t  . this double exponential blow-up is the reason why the naive algorithm would quickly run out of memory.
¡¡to alleviate this problem  the algorithm uses iterated elimination of dominated policies. for two agents i and j  a policy tree qi ¡Ê qi is dominated  if for every possible multi-agent belief state mb ¡Ê ¦¤ s¡Áqj  there is at least one other policy tree qk ¡Ê qi   qi that is as good as or better than qi. this test for dominance is performed using a linear program. removing a dominated policy tree does not reduce the value of the optimal joint policy. if used in every iteration  this technique significantly reduces the number of policy trees kept in memory  because for every eliminated policy tree q  a whole set of policy trees containing q as a subtree is implicitly eliminated as well. but  even with this pruning technique  the number of policy trees still grows quickly and the algorithm runs out of memory even for very small problems.
¡¡a more detailed analysis of the construction process shows that most of the policy trees kept in memory are useless. these policy trees could be eliminated early on  but they are not. one reason is that a policy tree can only be eliminated if it is dominated for every belief state. but for many decpomdps  only a small subset of the belief space is actually reachable. another reason is that a policy tree can only be eliminated if it is dominated for every possible belief over the other agents' policies. but obviously  during the construction process  the other agents also maintain a large set of policy trees that will eventually prove to be useless. inevitably  these policy trees are also considered in the test for dominance which inhibits the elimination of a large set of policies.
¡¡unfortunately  these drawbacks of the pruning process cannot be avoided. before the algorithm reaches the root of the policy trees  it cannot predict which beliefs about the state and about the other agents' policies will eventually be useful. this observation leads to the idea of combining the bottomup and top-down approaches: using top-down heuristics to identify relevant belief states for which the dynamic programming algorithm can then evaluate the bottom-up policy trees and select the best joint policy. figure 1 illustrates this idea.

figure 1: the construction process of a single-agent policy tree combining the top-down and the bottom-up approach.
1 top-down heuristics for dec-podmps
even though the agents do not have access to the belief state during execution time  it can be used to evaluate the bottomup policy trees computed by the dp algorithm. policy trees that are good for a centralized belief state are often also good candidates for the decentralized policy. obviously  a belief state that corresponds to the optimal joint policy is not available during the construction process. but fortunately  a set of belief states can be computed using multiple top-down heuristics - efficient algorithms that find useful top-down policies. once a top-down heuristic policy is generated  the most likely belief state can be computed. obviously  this does only lead to exactly one belief state. but depending on the specific problem  the bottom-up dp algorithm can handle up to a few dozen belief state candidates  at most 1 in our experiments . a whole set of reachable belief states can be identified using standard sampling techniques. the rest of this section describes several useful heuristics to identify these belief states.
the mdp heuristic a dec-pomdp can be turned into a fully-observable mdp by revealing the underlying state after each step. a joint policy for the resulting mdp assigns joint actions to states  thus this policy cannot be used for the real dec-pomdp. but during the construction process  it has proven very useful to identify reachable belief states that are very likely to occur.
the infinite-horizon heuristic infinite-horizon decpomdps are significantly different from finite-horizon decpomdps. instead of maximizing expected reward over t time steps  the agents maximize the reward over an infinite time period. obviously  optimal policies for finite-horizon problems with a very short horizon may be very different from those for infinite-horizon problems. but the longer the horizon  the more similar the solutions become in general. thus  infinite-horizon policies can be used as top-down heuristics to identify useful belief states for problems with sufficiently long horizons. infinite-horizon dec-pomdps are undecidable and thus it is generally impossible to determine when optimal policies for finite and infinite horizon problems match. but because the rewards are discounted over time  a bound on the difference in value can be guaranteed. recently  approximate algorithms have been introduced that can solve those problems very efficiently  bernstein et al.  1 . the resulting policies can be used to simulate multiple runs where the execution is simply stopped when the desired horizon and a corresponding belief state are reached.
the random policy heuristic the mdp heuristic and the infinite-horizon heuristic can be modified by adding some ¦Åexploration - choosing random actions with probability ¦Å in every step. this helps cover a larger area of the belief space  in particular if the other heuristics are not well suited for the problem. for the same reason  a completely random heuristic can also be used - choosing actions according to a uniform distribution. this provides a set of reachable belief states and is not dependent on the specific problem. obviously  those random policies achieve low values. but this is not a problem  because only the belief states and not the policies are used.
heuristic portfolio the usefulness of the heuristics and  more importantly  the computed belief states are highly dependent on the specific problem. for some problems  the solution computed with the mdp heuristic might be very similar to the optimal policy and accordingly the computed belief states are very useful. but for other problems  this heuristic might focus the algorithm on the wrong parts of the belief space and consequently it may select poor policy-tree candidates for the decentralized case. this problem can be alleviated by using what we call a heuristic portfolio. instead of just using one top-down heuristic  a whole set of heuristics can be used to compute a set of belief states. thus  each heuristic is used to select a subset of the policy trees. learning algorithms could optimize the composition of the heuristic portfolio  but this aspect is beyond the scope of this paper.
1 memory-bounded dynamic programming
the mbdp algorithm combines the bottom-up and top-down approaches. the algorithm can be applied to dec-pomdps with an arbitrary number of agents  but to simplify notation  the description in algorithm 1 is for two agents  i and j. the policy trees for each agent are constructed incrementally using the bottom-up dp algorithm. but to avoid the double exponential blow-up  the parameter maxtrees is chosen such that a full backup with this number of policy trees of length t   1 does not exceed the available memory. every iteration of the algorithm consists of the following steps. first  a full backup of the policies from the last iteration is performed. this creates policy tree sets of size |a||maxtrees||o|. next  top-down heuristics are chosen from the portfolio and used to compute a set of belief states. then  the best policy tree pairs for these belief states are added to the new sets of policy trees. finally  after the tth backup  the best joint policy tree for the start distribution is returned.
1 theoretical properties
overcoming the complexity barriers of dec-pomdps is challenging  because there is a double exponential number of possible policy trees  each containing an exponential number of nodes in the time horizon. accordingly  an important feature of the mbdp algorithm is that it can compute and represent the policy trees using just linear space.
algorithm 1: the mbdp algorithm
1 begin
1 maxtrees ¡û max number of trees before backup
1 t ¡û horizon of the dec-pomdp 1	h ¡û pre-compute heuristic policies for each h ¡Ê h
1 q1i q1j ¡û initialize all 1-step policy trees
1 for t=1 to t do
1 qti+1 qtj+1 ¡û fullbackup qti   fullbackup qtj  selit+1 seljt+1 ¡û empty for k=1 to maxtrees do
choose h ¡Ê h and generate belief state b
1 foreach qi ¡Ê qti+1 qj ¡Ê qtj+1 do 1 evaluate each pair  qi qj  with respect to b
1 add best policy trees to selit+1 and seljt+1
1 delete these policy trees from qti+1 and qtj+1
1 qti+1 qtj+1 ¡û selit+1 seljt+1
1 select best joint policy tree ¦Ät from {qti  qtj }
1 return ¦Ät
1 end
theorem 1. the mbdp algorithm has a linear space complexity with respect to the horizon length.
proof: once the variable maxtrees is fixed  the number of policy trees per agent is always between k and k. the lower limit k is equal to maxtrees  which is the number of trees after the heuristics have been used to select the k-best policy trees. the upper limit k is equal to |a||maxtrees||o|  which is the number of policy trees after a full backup. by choosing maxtrees appropriately  the desired upper limit can be pre-set. in every iteration of the algorithm  no more than k policy trees are constructed. for the construction of the new policy trees  the algorithm uses pointers to the k policy trees of the previous level and is thus able to potentially attach each subtree multiple times instead of just once as illustrated in figure 1. due to this efficient pointer mechanism  a final horizon-t policy tree can be represented with k¡¤t decision nodes. thus  the amount of space grows linearly with the horizon length. for n agents  this is o n k¡¤t +k  . 

figure 1: an exponential size policy can be represented using linear space by re-using policy trees. here maxtrees = 1.
¡¡in general  the idea of the pointer mechanism could be applied to other algorithms and improve them as well. it reduces the number of nodes used for one final policy tree from o |o|t  to o k ¡¤ t . however  if k grows double exponentially - as in the optimal dp algorithm - using the pointer mechanism only has marginal effects. for an improvement in space complexity it is also necessary that k - the number of possible policy trees kept in memory - be sufficiently small.
theorem 1. the mbdp algorithm has a linear time complexity with respect to the horizon length. proof: the main loop of the algorithm  lines 1  depends linearly on the horizon length t. inside this loop  all operations are independent of t once k and k are fixed. the remaining critical operation is in line 1  where the heuristic policies are pre-computed. currently  the only heuristic that guarantees linear time is the random policy heuristic. the other heuristics have higher  but still polynomial-time complexity. in practice  the time used by the heuristics is negligible. if a top-down heuristic is used that can be computed in linear time  the time complexity of the whole algorithm is linear in the horizon length.	
1 recursive mbdp
as pointed out earlier  the effectiveness of any top-down heuristic may depend on the specific problem. to alleviate this drawback  we use a heuristic portfolio. but once the algorithm has computed a complete solution  we have a joint policy that definitely leads to relevant belief states when used as a heuristic. this is exactly the idea of recursive mbdp. the algorithm can be applied recursively with an arbitrary recursion-depth. after the first recursion has finished  the final joint policy can be used as a top-down heuristic for the next run and so on.
1 pruning dominated policy trees
as described in section 1  the exact dp algorithm uses iterated elimination of dominated strategies to reduce the size of the policy tree sets. interestingly  this pruning technique can be combined with the mbdp algorithm in two different ways. first  after a full backup is performed  the pruning algorithm can be used to eliminate dominated policy trees  which reduces the set of remaining policy trees. consequently  no dominated policy tree will be selected using the top-down heuristics. second  the pruning technique can be applied after the heuristics have been used to select the policy trees for the next iteration. pruning dominated policy trees at this step leads to a more efficient use of memory in the next iteration.
1 experiments
we have implemented the mbdp algorithm and performed intensive experimental tests. the algorithm has three key parameters that affect performance. one parameter is the maximum number of trees  maxtrees  which defines the memory requirements; runtime is quadratically dependent on maxtrees. its effect on the value of the solution is analyzed in section 1. the second parameter is the depth of the recursion  which affects runtime linearly. we define a trial run of
mabc problemtiger problemhorizonvaluetime s value¦Òtime s 1.1.1.111.1.1.111.1.1.1.1.1111111.1.1.1.1.1 1.1.1.1.1.1 1 1111 1 1111table 1: performance of the mbdp algorithm.
the algorithm as the entire computational process including all recursive calls. the best solution found during one trial run is returned as the final solution. because the algorithm is partly randomized  it is obvious that on average  the higher the recursion depth  the better the solution value. the third parameter controls the selection of heuristics from the portfolio. in the following experiments we used a uniform selection method  giving the same weight to all heuristics  because it performed well without fine-tuning. we also experimented with the pruning technique described in section 1  but it had little impact on the runtime or solution quality. hence  we decided not to include it in the following experiments.
1 benchmark problems
we have applied the mbdp algorithm to two benchmark problems from the literature: the multi-access broadcast channel  mabc  problem involving two agents who send messages over a shared channel and try to avoid collisions  hansen et al.  1   and the multi-agent tiger problem  where two agents have to open one of two doors  one leading to a dangerous tiger and the other to a valuable treasure  nair et al.  1 . table 1 presents performance results of the mbdp algorithm for the two test problems. shown are solution value and computation time  which are both averaged over 1 trial runs. for the tiger problem  we also show the standard deviation ¦Ò  because in this case the algorithm achieved different solution values over the 1 trials. for the mabc problem  we used maxtrees = 1 and a recursion depth of 1. for the tiger problem we used maxtrees = 1 and a recursion depth of 1.
¡¡to evaluate the mbdp algorithm we compared it to the optimal dp algorithm  hansen et al.  1   the jesp algorithm  nair et al.  1   the point-based dynamic programming algorithm  pbdp   szer and charpillet  1  and a randompolicy generating algorithm. these are all offline planning algorithms. the computation is performed in a centralized way and the final solution is a complete joint policy tree for the desired horizon  which can then be executed by multiple agents in a decentralized way. in contrast  the bayesian game approximation algorithm  emery-montemerlo et al.  1  interleaves planning with execution. thus  it allows for some amount of decentralized re-planning after each step. to perform a consistent comparison  we also created an online version of mbdp and in fact it outperformed the other algorithm. but a discussion of the online algorithms is beyond the scope of this paper and thus the following performance comparison does not include results for the online algorithms.
mabc problemtiger problemhorizonoptimalpbdprandommbdpoptimaljesppbdprandommbdp1.1.11-1-1-1-1-11.1.1.1.1.1-11-111.1.1.1.1.1 1-111-111- --111-111- --111-111- --111-111----111--11----111--11----111--11----1.1.1 1--11----1.1.1 1--1.1 1----1.1.1 1--1.1 1----1 11table 1: performance comparison of the different algorithms on the mabc problem and the tiger problem.¡¡table 1 presents a performance comparison of the different algorithms for both test problems. for this comparison  the same experimental setup as described above was used. shown are the solution values achieved for each horizon. a     indicates that the algorithm could compute a solution for this horizon  but the achieved value was not reported in the corresponding paper. a  -  indicates that the algorithm ran out of time or memory for this horizon. even though the experiments have been performed on different systems with different processor speed and available memory  the two key findings of this comparison are still applicable: first  for the horizons where comparison is feasible  the mbdp algorithm achieves the same or higher value than all previous approaches. second  the results match nicely our theoretical findings: they show that the linear complexity of the mbdp algorithm makes it possible to solve problems with much larger horizons  improving scalability by multiple orders of magnitude with respect to all existing algorithms. none of the previous approaches could compute a solution for the problems with a horizon of 1 whereas the mbdp algorithm can easily compute solutions up to horizon 1. for large horizons  the comparison with the random policy algorithm shows that the two test problems are non-trivial  as the difference in achieved value is significant. for example  for the mabc problem the mbdp algorithm achieves approximately 1% of the value the random policy algorithm achieves. on the tiger problem random policies lead to extremely poor performance compared to the mbdp algorithm.
1 trade-off: recursion depth vs. maxtrees
the parameters maxtrees and the recursion depth present an important trade-off: their increase generally increases both solution value and runtime. we examine this trade-off below and identify the best parameters for the test problems.
¡¡one interesting experimental result for the mabc problem is that maxtrees = 1 is sufficient to produce the best solution for all horizons  even with recursion depth 1. apparently  this problem exhibits some structure that makes only a few decision nodes necessary even for long horizons. in this regard  the tiger problem is more complex: for longer horizons  the best solution is not found in every trial  even with maxtrees   1 and a recursion depth of 1. nevertheless  just a few policy trees are necessary to find near-optimal solutions. figure 1 shows that when maxtrees is increased  the solution value levels off very quickly. the values shown are for the tiger problem with horizon 1  averaged over 1 trials with a recursion depth of 1. experiments with other horizons yielded qualitatively similar results.

figure 1: a small maximum number of policy trees is sufficient for finding near-optimal solutions.
¡¡the best parameter settings for maxtrees and the recursion depth vary with the available runtime. table 1 shows experimental results for the tiger problem with horizon 1  where the best parameter settings for given time limits up to 1 seconds have been identified. the solution values shown are averaged over 1 trial runs. the results illustrate the tradeoff between the recursion depth and the maximum number of trees. in particular  they show that different parameter settings optimize this trade-off for different time limits.
time limit  s maxtreesrecursion depthvalue11.11111.11111.11111.11111.111table 1: the best parameter settings and solution values for the tiger problem with horizon 1 for given time limits.
1 conclusion and future work
we have presented the first memory-bounded dynamic programming algorithm for finite-horizon dec-pomdps. it is the first to effectively combine top-down heuristics with bottom-up dynamic programming. unlike previous approximate algorithms  the mbdp algorithm guarantees linear space and time complexity with respect to the horizon length  thereby overcoming the high worst-case complexity barrier of dec-pomdps. one particularly attractive feature of the algorithm is that the policy trees it generates are represented using linear space due to an efficient pointer mechanism. in contrast  a single complete policy tree normally contains an exponential number of decision nodes. our experimental results show that the algorithm performs very well. it is truly scalable in terms of the horizon. it solves problems with horizons that are multiple orders of magnitude larger than what was previously possible  while achieving the same or better solution quality in a fraction of the runtime. we have also analyzed the important trade-off between the maximum number of policy trees kept in memory and the recursion depth of the algorithm. interestingly  very few policy trees are sufficient to produce near-optimal results for the benchmark problems. we are currently working on a deeper analysis of the theoretical guarantees of the algorithm  i.e. deriving bounds on the approximation error and analyzing the influence of different heuristics on performance.
¡¡in future work  we plan to apply the mbdp algorithm to problems with larger state spaces as well as larger action and observation sets. a finite-horizon dec-pomdp has an exponential complexity with regard to the number of observations. increasing scalability in that regard is thus an important challenge. for some problem sizes  additional techniques will have to be employed to further improve scalability. one option is to explore the full potential of the pruning technique. currently  the mbdp algorithm and the pruning algorithm can only be used as two separate entities  which has not proven to be very useful. we are investigating ways to use the top-down heuristics to identify relevant multi-agent belief states for which the pruning algorithm checks dominance and then selects the best bottom-up policies. the algorithm and representations used in this work open up multiple research avenues for developing effective approximation algorithms for decentralized decision making under uncertainty.
1 acknowledgments
we thank marek petrik and daniel bernstein for helpful discussions on this paper. this work was supported in part by the air force office of scientific research under award no. fa1-1 and by the national science foundation under grants no. iis-1 and iis-1.
