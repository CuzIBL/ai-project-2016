 
   an efficient backtracking method for lisp  used in the mlisp1 language  is described. the method is optimal in the following senses: 
 1  only necessary state information is saved. the backtracking system routines are sufficiently efficient to require less than ten percent of the execution time of typical 
jobs. 
 1  most common operations - fetching/storing the value of a variable or the property of an atom  function entry/exit - take no longer with backtracking than without it. this is achieved by not changing the way values are stored. 
 1  if backtracking is not used  an insignificant overhead is involved in maintaining the backtracking capability. 
the mlisp1 algorithm and philosophy are briefly contrasted with those of several existing backtracking systems  with historical comments on the development of the theory of backtracking. 

to the desired goal. 	at each branching point in the tree  a decision must be made as to which alternative to try next. 
backtracking is designed to s i m p l i f y t h e p r o g r a m m i n g of this type of problem. 

a branching point is called a  decision point  in this paper. 
frequently  insufficient information is available at decision points to determine which branches will lead to the goal. if a wrong branch is tried  it  fails ; that is  the program returns to the decision point  pretends that the incorrect branch had never been attempted  and selects another alternative  this process is called  backtracking.  it is algorithmic because it covers the entire goal tree; every branch at every branching point will eventually be tried in the worst case. 
　several languages have incorporated backtracking  but none of them have incorporated exactly the same features and none of them have implemented backtracking in exactly the same way. the differences are in terms of objectives and in terms of methods. we shell first discuss the nature and then the origin of these differences  beginning with the different implementation methods used 
backtracking 1 1 1 1 1 has begun to be used in the past backtracking has been criticized in some quarters as being an 
five years as a control structure for programming languages dealing with problems in artificial intelligence. this paper is an attempt to contribute to a better understanding of what backtracking is  what it is useful for  and how to implement it efficiently. briefly  backtracking is an algorithmic device for solving problems expressible as a set of possible alternatives  called a goal tree  where not all of the alternatives will lead inherently bad control structure.' criticisms of any concept 
come in two flavors:  1   theoretical criticisms  of the concept  and  1   pragmatic criticisms  of machine implementations of 
the concept. the criticisms of backtracking have mostly fallen into this latter category; they criticize the early  pioneering systems like planner. more recent systems like ecl and mlisp1 incorporate enough flexibility to answer many of the 1
published objections to backtracking. finally  drawing on our personal experience  we wish to point out that mlisp1 has been used for two years by the artificial intelligence community at stanford. it has shown itself to be versatile  easy to use  and efficient enough to be eminently practical. 
1 	before 	continuing  	we 	should 	mention 	briefly 	that 
two views of computations 
   all existing backtracking methods have been implemented from one of two viewpoints  which we shall call the  sequential  view and the  state  view of backtracking. these correspond to the way computations are ordinarily viewed the  sequential  view of computations holds that a computation is a sequence of discrete steps  the last of which yields the desired result. all algorithms consist of such sequences. for example  the algol statements 

may be viewed as a sequence of steps leading to the computation of a value for w. the third and fourth statements themselves consist of a sequence of more primitive steps. 
   another way to view computations is in terms of state transformations  the  state  approach. the  state of a computation  is defined to be the set of current values of all variables. the program counter  stacks  etc. are considered to be system variables. a computation is a sequence of state transformations  the result of which is a state representing the 
desired computation. continuing the previous example but representing the state of the machine by a partially-specified tuple; 

the computation becomes 

though we have not mentioned the intermediate states  e.g. 
1*y   a complete description must include them. 
sequential backtracking 
　theoretical systems 1 these two views of computations have affected the theory and development of backtracking systems. golomb and baumert were among the first to explore the computational applications of backtracking. they took the state view of computations and demonstrated that backtracking could be used to reduce the search of the space of solution states. floyd's early investigations took the sequential approach. he proposed having an  inverse  for each statement in the computation  including assignments  
conditionals  subroutine entries and exits  and i/o. backtracking then consists of stopping the forward execution of statements end executing inverse statements until the 
decision point is again reached. at this point  golomb and floyd agree  everything has been reset to its original condition  and processing may again proceed in a forward direction. 
each command  computation step  is expanded into one or more commands  some of which carry out the effect of the original command in the nondeterministic algorithm  and which also stack information required to reverse the effect of the command when backtracking is needed  while others carry out the backtracking by undoing all the effects of the first set.  1  p.1  
   this has several advantages. expanding each computation step is a mechanical process that can be added to existing 
compilers and interpreters. the same code is generated as before to carry out the forward steps  plus some additional code to save the backtracking information. furthermore  the modular design facilitates adding backtracking to a system. the inverse of each statement type can be added and debugged separately. 
   p l a n n e r ' but the sequential   inverse statement   m e t h o d of implementing backtracking soon changed its o b j e c t i v e s . it was observed that not ell statements need to be undone. sometimes variables will be set on one branch in the goal tree that cannot affect the execution of other branches. therefore  the argument goes  why bother to backtrack these variables  this is the approach taken by planner  a lisp-based pat tern-matching system incorporating several powerful non-procedural features such as goaldirected computation. planner uses backtracking to implement these features.  actually  planner has not been fully implemented yet; this discussion really deals with a subset called micro-planner implemented by sussman  winc-grad and charniak. nevertheless  we shall continue to use the name planner.  not all statement-types are undoable. functions that may be backtracked are designated by the letters  th  on the front of their names. for example  

cannot be backtracked  but 

can be. similarly  thputprop  thremprop  thassert end therase are functions for changing the data base that are backtrackable whereas putprop and remprop are not. but function entries and exits are always undoable; i.e. planner will always restore the control environment when a failure happens  but it will not restore the values of variables or properties of atoms unless explicitly instructed to. this gives the programmer more control over backtracking  and enables him to eliminate the saving of superfluous information on the other hand it requires the programmer to k n o w what information mutt be saved  shifting the burden of backtracking 
to the programmer's shoulders  and in the end making such systems harder to use. a further disadvantage is that the programmer may overspeolfy the amount of information to be saved; it is frequently difficult and non-intuitive to decide what is the optimum amount of information to save. 
　this implement ion of backtracking has had a profound and confusing effect on its objectives. a distinction has come to be made between automatic control and data backtracking. 
1  in the authors' view  this distinction is a negative aspect of the theory of backtracking that has obscured rather than clarified the issues involved. the main purpose of b a c k t r a c k i n g is to enable a program to t r y l a t e r alternatives in a goal tree as if e a r l i e r unsuccessful ones had never been 
attempted if the state of the computation is not reset at the beginning of each alternative  then the alternatives will behave differently depending on the order in which they are tried. this is a red herring that merely obscures the understanding of nondeterministic programs. if the programmer wants some information preserved when an alternative fails  he should explicitly say so  and any good nondeterministic programming language should provide him with facilities for doing so. 
state backtracking 
　corresponding to the  state  view of computations  there is a  state  view of backtracking. this approach may be summarized as follows: when a decision point is encountered 
during a computation  the complete state of the machine  the current values of all variables including system variables  is  saved.  when a failure occurs  the state of the machine is  restored  to the saved state in one operation  and computation proceeds along another alternative at the decision point. if there are no more alternatives  the failure is propagated to the preceding decision point. the state method has led to a more faithful adherence to floyd's and golomb's view of backtracking  namely that everything is restored to the way it was before the incorrect alternative was attempted. in addition to the control environment being always restored  as in planner  the data environment is also always restored. 
　the state-saving and -restoring approach  state method  has one main advantage over the inverse-statement approach  sequential method : the process of failing and trying another alternative is usually more efficient. in the inverse-statement approach  statements may be backtracked unnecessarily. for example  suppose a decision point occurs  and then the value of a variable is changed 1 times before a failure happens. each of these 1 stores to the variable will have to be undone by an  inverse store   a restoration of the previous value of the variable. but each inverse store will just undo the effect of the previous one; the last inverse store executed effectively undoes them all. 1 of the 1 inverse stores will be wasted. in the state-saving approach  failure transfers control directly back to the decision point. once there  the entire state is restored in one operation. in mlisp1  this operation is a very rapid one. 
   e c l 1 ecl is a blend of the sequential and state methods. it has an nassign operator corresponding to planner's thsetq  but it also uses a  backup stack  much like mlisp1's state stack  see below . in an interesting variation on mlisp1's stack saving algorithm  the working stack is saved on the backup stack when functions are about to exit  rather than when decision points are executed. in some cases this will result in less information being saved than in mlisp1. however  it has the peculiar property that when a failure occurs  variables not explicitly saved will be restored to the last values they had in the fuction  not to the values theny had when the decision point was set. as in planner only the control environment is automataically restored. the programmer has the responsibility for explicitly saving  via nassign  variable values he wants restored correctly. in most other respects the ecl algorithm is very similar to mlisp1's. 
　s a i l 1 sail has taken a different approach. rather than add a context mechanism to the language  the sail implementers added a coroutine structure. coroutining is logically equivalent to backtracking. the set of alternatives at a decision point is represented by a set of coroutines. failure is achieved by deactivating the coroutine representing the current path and activating another one. in sail  the programmer is required to do all state saving himself. two new statements have been added to do this: 
remember  list of variables  in  context  
restore  list of variables  from  context  
the special word all may be used instead of the list of variables and means  do the operation on all the variables previously remembered in that context   thus sail is similar to planner and ecl in that it automatically restores the control environment but not the data environment; the data must be explicitly restored. if a programmer really wants sail to behave like a state-saving system  he must at every decision point do a remember of every variable  including every array element  in the system  and at failure do a restore all. it then becomes equivalent to p1p-1  see below . one seldom uses sail in this manner  however; the remember/restore primitives are intended to give the programmer a convenient way to keep certain data from being destroyed during processing in hypothetical situations  not to implement full backtracking. 
　while coroutining is logically equivalent to backtracking  it is neither physically nor conceptually the same. the internal structures are quite different; for example  there is no  backup stack  in coroutine systems. conceptually the main difference seems to center on the issue of whether information should be saved automatically or explicitly. the emphasis on simplifying programming has led most backtracking systems to do a large number of things automatically - saving and restoring data  and control management - whereas coroutines have been regarded as a language tool  like iteration  that should be invoked explicitly. 
　m l i s p 1 l1 in mlisp1  the system automatically saves the necessary information. the advantages of automatic state saving are that ft is simple to use  and it eliminates human 
error. the programmer never has to figure out what values to save; indeed  he may often be unaware that state saving is going on. in addition to eliminating a possible source of bugs  not saving enough information   possible inefficiencies  saving too much information  are also prevented. programming in this type of a system is not much more difficult than programming in a system without backtracking. 
   p o p 1 the efficiency of the state approach depends on the efficiency of saving and restoring the state. the straightforward but inefficient method is to copy the value of every currently-active variable into some area of memory or secondary storage every time a decision point is encountered. thereafter  no further attention is paid to changes in variable values. when a failure occurs  the saved values are reloaded from storage  and every variable is restored to its old value regardless of whether or not its value had changed. this is the method used by pop-1 when backtracking was introduced into that language. 
   q a 1 qa1 is much more efficient in its state saving than pop-1. at decision points not much happens except that a context number is incremented. thereafter  when a value is stored in a variable  the context number is associated with the new value. in fact nothing much happens at failures either  except that the context number is decremented and a jump is executed! this is faster than any other system's failure. furthermore  in qa1 it is possible to modify or restore a backtracking context anywhere in the goal tree and then resume from there. but in qa1 the penalty is paid in referencing a variable's value  or any atom's property . all properties including the value property are stored in a structured al1st under each atom. this alist must be searched every time a variable or property is referenced the net effect is that fetches and stores slow down by one to two orders of magnitude. mlisp1 stores variables and properties in such a way that fetches take virtually no longer with backtracking than without  and stores are slower only in some cases. 
the mlisp1 algorithm 
   mlisp1 is an extensible language using backtracking and based on the stanford lisp 1 system   on the pdp-1 computer. the language is intended to be a practical tool for implementing product/on compilers and translators  as well as being a research tool. ml1sp1 has been operational for two years  which has given us a good deal of experience with the practical problems of production backtracking systems. a logic compiler  lcf 1   a deduction system  fol   an english parser  an english to french translator  an algol compiler  and the mlisp1 translator itself are major systems that have been written in mlisp1. this experience has led us to the conclusion that at this point not just one more feature  but efficiency and simplicity are the fundamental needs to make backtracking practical. 
   the philosophy of mlisp1 is that backtracking should s i m p l i f y programming. the programmer should never be concerned with explicitly saving information just so that statements can be backtracked  and he should seldom have to rewrite routines so that they can be included in a backtracking program.  in planner  existing routines may have to be rewritten by changing setqs to thsetqs  putprops to thputprops  etc  before they can be included in a nondeterministic program. the inverse changes have to be made when a backtracking routine is included in a deterministic system.  such considerations have nothing to do with problem solving. 
   mlisp1 implements backtracking by modifying the lisp interpreter and the lap assembly program. the principle change is to the bind code  by which lambdas and setqs bind their variables in interpreted functions. in addition  a  state stack  is maintained on which information from the normal pushdown stack  p  and the special pushdown stack  sp  is saved. in the stanford lisp 1 system  all control information and the values of local variables in compiled functions are put on the p stack. therefore  saving the contents of the p stack will save this information  the values of all variables in interpreted functions and of variables used free in compiled functions are put on the property lists of the variables. these will be called  property list variables.  saving the property lists of atoms will save the current values of property list variables  as well as any other property list changes. recursive calls on functions using property list variables stack the old variable values on the sp stack before rebinding  so that saving the sp stack will save their old values. these three structures - the p stack  the sp stack  and the property lists of atoms  - together with a control point to transfer to when a failure occurs  completely specify the state of any lisp l.c imputation. 
　this is a lot of information to handle at once  causing many implementers to shy away from the state-saving approach. but the volume may be reduced by saving just the incremental state  just the changes to the state that have occurred since the last decision point. the work load at decision points may be further reduced by distributing the state saving throughout the computation. 
　when an mlisp1 decision point is encountered  there are three major effects on the system: 
   1. a unique positive integer is associated with each dynamic decision point. this number is called the  context number   and the context number in effect during any given part of the computation is called the  current context number  when a decision point is encountered  the current context 
number is incremented by one. it monotonically increases unless a decision point is deleted.  normally a decision point is deleted only when all the alternatives at it have been tried and have been unsuccessful  although it may also be deleted explicitly.  the context number provides a means of referencing specific backtracking contexts and of communicating between contexts. in mlisp1  any context which knows the number of an earlier context may pass information back to that context  called  setting variables in context.  thus when a branch fails but gains valuable information in the process of trying  it can pass the information up the tree to be used in selecting another branch or by the next branch selected. 
   1. the second thing that happens at decision points is the  incremental  p and sp stacks are saved on the state stack. the definition of the incremental stacks is somewhat complicated. consider the situation represented in the figure below. 
the top of the p stack since the function's local variables and temporary results  which are stored above its return address  may be modified before the function exits.  if this happened  the state stack would no longer contain a complete copy of everything below lwm.  also the return address to which lwm points is changed to jump to a system routine; this routine moves lwm down to the previous one whenever the etack is about to become smaller than the current lwm. this is similar to ecl's method which moves lwm down to the next return address  rather than to the previous lwm  
   1. no property list variables are explicitly saved at decision points  but incrementing the context number affects property list variables later. associated with every property list variable is the context number in effect when the variable was last set. whenever a property list variable is about to be set  its context is compared with the current context number. if they are the same  then the value is simply changed and processing continues. no information is saved if 


two decision points have already been set and the incremental p stack at each has been copied into the state stack: a -* a'  b -* b'.  since basically the same operations are performed on the sp stack  we will only discuss the p stack here.  now a third decision point is about to be set. the question is; how much should be copied this time  a system variable called the l o w w a t e r m a r k  lwm  always points to the level of the p stack below which the state stack contains a complete copy of everything. therefore  just the information from lwm to the top of the p stack must be copied to the state stack  this is accomplished by a memory-to-memory block transfer  one multi-cycle instruction on the pdp-1 after some initial set-up . finally the new lwm is set to the stack location of the return address of the function containing the decision point  which is usually the first return address from the top of the p stack. this requires searching the p stack. we cannot simply use they are different  then 
 a  the variable's old value and context  together with the current context number  are saved on a context list  see below ; 
 b  the variable's context is changed to the current context number  to reflect the fact that the variable has now been set in this context; 
 c  finally  the variable's value is changed to the new value. 
the same process occurs whenever any property under an atom is changed  not just the value property. the context associated with an atom is actually in the form 

where in the case of variables one of the indicators is value. this list must be searched whenever a property is about to be changed. the old values are saved on a  context list   in the form 

when a feilure occurs  the system runs down this list restoring all the properties that had been changed in the current context. the context list is generally short since only one atom/indicator pair will occur in each context  namely the f i r s t change to that pair. this is a large improvement over the inverse statement method ot restoring values. 
   saving the values when variables are about to be set distributes the state saving throughout the program. 	most 
existing backtracking methods have this property. the advantage is that the amount of information saved becomes roughly proportional to the amount of work done on any one branch of the goal tree. if the branch fails early  then little state-saving work will have been done; if processing continues longer on the branch  then more state information comes to be saved. 
mlisp1 improvements in state saving 
　 a  not every change to a nondeterministic variable is saved  as in planner and ecl   just the first change in each backtracking context. 
　 b  mlisp1 uses the standard lisp 1 value cell for variable values  so that fetching a variable's value is just as fast with as without backtracking  in fact  mlisp1 uses the standard lisp representation for all properties on property lists  so that all gets are the same speed nondeterministicaily as deter ministically. this is the main source of the efficiency 
that mlisp1 has over qa1. 
　 c  when lisp 1 functions are compiled  many things are done more efficiently. local variables are stored on the p stack  and their values are fetched or stored in one machine instruction. fetches on free variables require only one instruction. function entry and exit require one instruction. all of these efficiencies are preserved by mlisp1. the only inefficiency introduced is in stores to free variables  which are represented as property list variables in lisp j.1 and thus must go through the process explained in  1  above. since in lisp  functions are usually debugged in interpreted mode and only compiled to increase efficiency  it is crucial in a production backtracking system that the efficiency of compilation be preserved. in research systems like planner and qa1  efficiency comparable to mlisp1 has not been attained. 
language features for backtraeking 

   mlisp1 uses backtracking to implement a context sensitive pattern matcher. pattern matching routines are written using a new expression  the let expression. as in planner  these routines may be invoked when their syntax pattern matches an input stream  though unlike planner the mlisp1 input stream must be unstructured  e.g. tokens in a file or in a linear list . the mlisp1 pattern matcher is designed to assist and simplify writing translators for other languages. in addition to pattern matching  a second new expression has been added to mlisp  the select expression  as the way to incorporate backtracking in ordinary programs. we will not explain these expressions in great detail here; they are explained fully in 
the mlisp1 report.1 however we will discuss their use of backtracking. 
let expression 
   the pattern language includes three  meta syntax  constructions which directly create decision points; rep  repeat   opt  optional  and alt  alternative . their primary purpose is to simplify  clarify  and reduce the number of rules necessary to specify a complex syntax. examples: 

meaning: let the production  identifier list  be zero or more identifiers separated by commas  and have as its value a list of the identifiers scanned. this corresponds to the bnf rules: 

1. { opt  pattern  } 

meaning: let the production  if  be the word if followed by an expression  followed by the word then and another expression  optionally followed by the word else and a third expression  and have as its value the corresponding lisp  cond  expression. this corresponds to the bnf rules: 


meaning: let the production  expression  be either the  begin end   the  if   or the  for  production  and have the value of the respective production as its value. this corresponds to the bnf rules: 

　we will give a brief example of how nondeterminism is used in these expressions. the execution of an opt expression proceeds as follows: 
 a  set a decision point 
 b  match the opt pattern against the current input stream. 
 c  if the match succeeds  the opt returns with a list of the values of the pattern elements matched. if the match fails  failure is called. 
termination function to the domain. exit with this value as the value of the select.  the termination function may call failure . if the value of the termination condition is false  proceed to the next step. 
 d  apply the value function to the domain  and exit with this value as the value of the select. 
 e  if a failure returns to the select  apply the successor function to the domain to yield a new domain. 
 f  go to step  c . 
　we will give a few examples of how the select may be used  

 d  if a failure happens  either in the matching of the opt pattern or later  the state of the computation is restored to its state at the beginning of the opt  the decision point is deleted  and computation proceeds with the empty list nil as the value of the opt. 
select expression 

　the other way to incorporate backtracking into a program is by the select expression. the select expression is like a nondetemninistic for-loop. it sets a decision point and allows each of a set of alternatives to be tried. it has a very general form  and is a generalization of floyd's choice function and fikes* choice-condition combination.1 the various expressions are converted to functions by the following expansion: 
 lambda   formal variable    expression   
these lambda functions are defined as: 

the execution of a select expression proceeds as follows: 
 a  evaluate the domain  which may be any expression  to get an initial domain. 
 b  set a decision point. 
 c  apply the termination condition to the domain. if the value is true  delete the decision point and apply the 
 1  floyd's choice function may be written: 

calling choiceuo  will give ten choices. the initial domain is just the integer 1. the value function is the identity function  lambda  1  1 . 
the successor function is addition by one  lambda  i   plus i d   . 
the termination condition is a check if the maximum has been 
exceeded 
 lambda  i   greaterp i n  . 
the termination function 
　　　　　 lambda  i   failure   propagates the failure if the termination condition becomes true. this illustrates the use of the intrinsic function failure  1 function of no arguments that fails to the last decision point set and restores the state of the computation at that point. 
　 1  the most common use of select is to select items one at a time from a list  and try out each item selected. for this reason  several of the clauses in the select expression syntax are optional. the following two forms are equivalent. 

the finally expression need not propagate failure; it may pass information back to earlier contexts  or simply compute a final value. 
　 1  one of the most interesting uses of the successor expression  which computes a new domain  is to reorder the old domain based on the information gained by trying the last alternative. qa1 and other systems have this ability. 


here the functions all operate on a goal list  represented by the variable goals. the successor function modifies this variable  in a way that may be influenced by the last alternative tried  for example  by passing some information in context   thus affecting subsequent alternatives. 
other features 
　two final backtracking features available in mlisp1 are a function called flush  which prunes a part of the goal tree by flushing elements off of the state stack  and a notation for setting variables in other backtracking contexts. normally assignments such as  x  y  take effect in the current 
context  but  x{1}  y  sets x to the value of y in the context 1. x will now not be backtracked until a failure to context 1 occurs or until it is again set in the current context. this enables the programmer to specify that information is to be saved until a certain context is destroyed. any expression to compute a context number may occur inside the braces. 
   rep  opt  alt  select  failure  flush and setting in context constitute the complete set of backtracking facilities available in mlisp1. note that unlike floyd's theoretical system  there is no success function; success is the absence of a failure  just as true is anything but nil in lisp. these primitives are slightly less powerful than those available in some other systems  particularly planner and ecl which allow failing to a label. for example  in ecl the programmer may declare tag points  which are like nondeterministic labels  and then fail explicitly to these tag points. however  although they are very powerful  any control structure can be be buiit up out of go to's   the arguments against the use of go to's in deterministic languages apply as well to the use of these nondeterministic go to's and labels. just as many languages are trying to replace go to's with while- and for-loops  we have tried to replace nondeterministic go to's with a nondeterministic for-loop; the select expression. 
summary 
   an efficient implementation of backtracking used in the mlisp1 language has been described. the efficiencies are due to a smooth integration of backtracking into an existing lisp system; in particular  the way variables are stored has not been changed  so that fetches and most stores are not degraded. the theory of existing backtracking systems has been related to two ways of viewing the structure of computations. mlisp1 has a  state-saving  structure  as opposed to a  sequential  or  inverse statement  structure. 
other backtracking systems have been discussed and compared with the mlisp1 implementation. finally  the language features available in mlisp1 for using backtracking have been presented. our main conclusion is that it is possible to incorporate backtracking into a production system in such a way that the most frequent operations are not 
degraded in performance. backtracking is a more useful and more used control structure when this ie done. 
　with some simplifications and omissions  the existing implementations of backtracking are summarized in the following table. 

bibliography 
1. bobrow  d.g. and wegbreit  b. a model and stack 
implementation 	of 	multiple 	environments 
report no.1  bolt  beranek and newman  1. 
1. bur stall  r.m.  collins  j.s. and popple stone  r.j  p r o g r a m m i n g in pop1  university press  edinburg  scotland  1  1. 
1. feldman  j.a.  low  j.r.  swinehart  d.c. and taylor  r.h. 
keoent developments in s a i l   an algol based language for a r t i f i c i a l intelligence 
artificial intelligence project memo aim-1  stanford university  1. 
1. fikes  r.e. a heuristic program for solving 
problems 	stated 	as 	nondeterministic 
procedures ph.d. thesis  carnegie-mellon university  1. 
1. floyd  r.w.  nondeterministic algorithms  j.acm 1  a  oct. 1   1. 
1. golomb  s.w  and baumert  l.d.  backtrack programming  
j.acm 1  1 {oct. 1   1. 
1. hewitt  c.  procedural embedding of knowledge in planner  proc. ijca1  1  1. 

br1 
1. hewitt  c.  planner: a language for manipulating models and proving theorems in a robot  al memo 1  rev   mit  1. 
1. milner  	r. 	logic 	for computable functions 
description artificial intelligence project memo aim-1  stanford university  1. 
1. prenner  c.j.  spitzen  j.m.  and wegbreit  b.  an implementation of backtracking for programming languages  sigplan notices 1  1  nov. 1   1. 
1. quam  l.h. and diffie  w. stanford lisp 1 
m a n u a l at operating note 1  stanford university  1. 
1. rulifson  j.f. qa1 programming conoepts al 
technical note 1  stanford research institute  1. 
1. smith  d.c. and enea  h.j. mli1 artificial intelligence project memo aim-1  stanford university  1. 
1. sussman  g.j. and mcdermott d.v.  why conniving is 
better than planning  proc. fjcc 1  dec. 1   1. 
1 
1 

1 









