
we develop two related themes  learning procedures and knowledge transfer. this paper introduces two methods for learning procedures and one for transferring previously-learned knowledge to a slightly different task. we demonstrate by experiment that transfer accelerates learning.
1 introduction
procedures are interesting for many reasons: infants exercise procedures  called  circular reactions  by piaget  almost from birth. these gradually develop in complexity  adding new elements  and eventually incorporate objects in the infant's environment. piaget believed infants learn much about the world by executing procedures. older students do  as well; indeed  the literature on expert/novice differences can be summarized in a single phrase: novices know  how   experts also know  why.  novices can run statistical tests but not understand why they are appropriate  they can follow recipes but not understand the underlying chemistry and gastronomy.
　there are advantages to learning fact-like knowledge alongside procedures. one can do useful work with procedures even if one doesn't completely understand them. this means learning is grounded in activity and is gradual over the life of the agent. by exercising procedures the learner produces occasional failures as well as the context one needs to gradually learn both new procedures and non-procedural knowledge. the latter includes the facts and reasons we call  understanding   or  less colloquially  the conditioning variables that affect the probabilities that procedures will succeed. in social environments  there often is a human to help the learner correct missteps before a procedure goes completely wrong. this minimizes the credit-assignment problem.
　our intention is to have a machine learn a sequence of increasingly-difficult games  starting with extremely easy ones. all the games are two-person card games. one player makes mistakes  the other is a competent player who offers corrections as necessary. for convenience we call these players the learner or child  c  and the adult  a . correct play for the games can be modeled by finite state machines  so the learner learns finite state machines and uses its current machine to generate its next move.
　the experiments in this paper involve a sequence of three games. in the first game  the child and the adult each have two stacks of cards  one in which all of the cards are face-up and another in which all of the cards are face-down. let's denote these stacks by the owner  a for adult or c for child  and whether they are face-up  u  or face-down  d . therefore  the four stacks are au  ad  cu  cd. each player must flip over the card on top of their face-down stack and move it to the top of their face-up stack. each player does this as fast as they can  and can ignore the other player's actions for the purposes of this game.

figure 1: a state machine for game 1 that yields perfect play.
　in the second game  the players take turns flipping and moving cards  as shown in figure 1. the two nodes on the left specify constraints on what the child observes  namely  the action that is currently being taken by the adult. to be in the lower-left state in the machine  the adult must currently be flipping over the top card on her facedown stack  flip top ad   . when this happens  the action executed by the child is wait. that is  when the adult does flip top ad    the child does wait at the same time. this leads to a state in which the adult does move top ad   top au    which also results in the child waiting. then  when the adult executes a wait action  the constraint in the upper-right state   the child flips the card on top of her face-down stack  and the adult continues to wait  the lower-right state  as the child does move top cd   top cu  . some time later  the adult flips the card on top of her face-down stack  and the cycle repeats.
　the third game is just like the second game  except when either player turns over a card and the color of that card matches that of the card atop the other player's face-up stack  both players say  squawk .
1 learning the state machines
our goal is to learn procedures  and  specifically  perfect-play state machines such as those we described in the previous section. a useful distinction can be drawn between learning a state machine and learning a state machine given a related state machine. we are most interested in the second case  which we present as a kind of transfer learning. even so  we begin our discussion of learning with two methods for learning state machines de novo. these are bayesian model merging  bmm   stolcke  1   a well-known method for learning hmms; and state splitting  ss   which also learns hmms  though by splitting rather than merging states.
　the training data in both approaches was a sequence of observations such as move top cd   top cu    the symbol squawk  and the special symbol no. the bmm and ss algorithms rely on no to indicate that the child has done something that is not allowed in perfect play. incorrect play is generated by the learner's incorrect machine in the active learning regime. passive learning requires training data that includes mistakes and adult admonitions  i.e.  no  and corrections. for this purpose we build simulators of child and adult play.
　bmm is a top-down approach in which an initial hmm is constructed with one state for every observation  and observations are greedily merged to maximize the posterior likelihood of the data using a description length prior on hmms. it is incrementalin the sense that new observationscan be added during the merging process. ss is a bottom-up approach that starts with one state that matches all observations  and repeatedly splits states on observable features of the world in an effort to more accurately predict negative feedback from the teacher. features are chosen that minimize entropy in the distribution of feedback  positive or negative  over all states.
　regardless of whether the hmm was learned via bmm or ss  it can be used for action selection  game play  and to parse new observations. we are interested in whether learning a series of related  yet increasingly difficult  games requires less effort in total than learning the most difficult game de novo. for example  is it easier to learn our three games in sequence  using the machine learned for game n to bias the learning of game n+1  than to learn to play squawk with no prior machine to serve as bias 
　when using bmm to learn a new game biased by an existing hmm  new states and transitions are added for the new observations  but old and obsolete state transitions are not removed. instead  as more and more data are collected for the new game  the probabilities of obsolete state transitions that are no longer traversed will tend to zero. this is not acceptable  as we want to quickly refine the model to work with the new game while harnessing as much information as possible from previous experience. to achieve this we preserve model structure but eliminate the bias of old probability parameters when moving from one game to the next. in bmm terminology  this means that we reset viterbi counts to be uniform and small when starting to learn a new game.
1 experiments
figure 1 shows two machines that were intermediate steps on the path to learning the turn taking game. in the top machine  there is a single state from which the child can wait  move cd cu   or flip cd . edges are labeled with two counts - the number of times the transition was taken and the number of times that transition resulted in a no. this machine does not yield perfect play  so the state will be split. the observable feature that most reduces entropy in the distribution of negative feedback is the feedback received for the move just made.

figure 1: machines produced by ss when learning the turn taking game.
　the bottom machine in figure 1 shows the result of splitting on this feature and pruning away any transitions that always cause a no. this machinecorrectly captures the fact that if the adult corrects the child  the only legal action is to wait while the adult repairs the incorrect action  which will result in a yes. residual feedback non-determinism exists in the top state due to errors in action ordering  e.g.  flipping twice in a row . therefore  that state will be split next.
　in a second set of experiments  the child chose actions randomly from a set of known actions until a model was developed  using bmm  that could guide action selection. initially  therefore  the child would frequently choose an incorrect action. learning early on with no model is tedious and time-consuming as it requires heavy exploration. the second game  the simple turn-taking game  was still easy to learn. only 1 simulation time steps were required to learn the optimal model for that game when starting from scratch. without using this model to bias the model learned for the third game  1 time steps were required to achieve the optimal model for the more complex game. however  when the game 1 model was used to bias the learning of game 1  only an additional 1 time steps were needed to learn game 1  making a total of 1 time steps. that is  game 1 took in total half as long to learn when game 1 was learned as an intermediate step in the learning process.
acknowledgments
this work was supported by darpa  contract number: 1; project name: learning by doing .
