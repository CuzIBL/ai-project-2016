 
we introduce a new kernel for support vector machine learning in a natural language setting. as a case study to incorporate domain knowledge into a kernel  we consider the problem of resolving prepositional phrase attachment ambiguities. the new kernel is derived from a distance function that proved to be succesful in memory-based learning. we start with the simple overlap metric from which we derive a simple overlap kernel and extend it with information gain weighting. finally  we combine it with a polynomial kernel to increase the dimensionality of the feature space. the closure properties of kernels guarantee that the result is again a kernel. this kernel achieves high classification accuracy and is efficient in both time and space usage. we compare our results with those obtained by memory-based and other learning methods. they make clear that the proposed kernel achieves a higher classification accuracy. 
1 introduction 
an important issue in natural language analysis is the resolution of structural ambiguity. a sentence is said to be structurally ambiguous when it can be assigned to more than one syntactic structure  zavrel et al.   1 . in prepositional phrase  pp  attachment one wants to disambiguate between cases where it is uncertain whether the pp attaches to the verb or to the noun. 
example 1 consider the following two sentences: 
1. i bought the shirt with pockets. 
1. 1 washed the shirt with soap. 
and nagao  1 . a computer program usually cannot rely on that kind of knowledge. 
　this problem has already been tackled using memorybased learning like for example k-nearest neighbours. here  the training examples are first stored in memory and classification of a new example is done based on the closest example stored in memory. therefore  one needs a function that expresses the distance or similarity between examples. there already exist several dedicated distance functions to solve all kind of natural language problems using memorybased learning  veenstra et al  1; zavrel et al  1; daelemans et al  1 . 
　we will use a support vector machine  svm  to tackle the problem of pp attachment disambiguation. central to svm learning is the kernel function k : .x x x -  r where x contains the examples and the kernel k calculates an inner product in a second space  the feature space f. this product expresses how similar examples are. 
　our goal is to combine the power of svms with the distance functions that arc well-suited for the probem for which they were designed. deriving a distance from a kernel is straightforward  see section 1. however  deriving a kernel from a distance is not trivial since kernels must satisfy some extra conditions  i.e. being a kernel is a much stronger condition than being a distance. in this paper we will describe a method that shows how such dedicated distance functions can be used as a basis for designing kernels that sequentially can be used in svm learning. 
　we use the pp attachment problem as a case study to illustrate our approach. as a starting point we take the overlap metric that has been succesfully used in memory-based learning for the same problem  zavrel et al  1 . 
　section 1 will give a short overview of the theory of svms together with some theorems and definitions that are needed in section 1. based on  zavrel et al  1   section 1 gives an overview of metrics developed for memory-based learning 

how the shirt is washed  ratnaparkhi  1 . 　this type of attachment ambiguity is easy for people to resolve because they can use their world knowledge  stetina 
　* author funded by a doctoral grant of the institute for advancement of scientific technological research in flanders  1wt . 1 support vector machines 
for simplicity  in our explanation we will consider the case of binary classification only  i.e. we consider an input space 
x with input vectors x and a target space d = {1  -1}. the case-based reasoning 1 　in sentence 1  with modifies the noun shirt because with applied to the pp attachment problem. in section 1 the new pockets  pp  describes the shirt. in sentence 1 however  with kernels will be introduced. finally sections 1 and 1 give some modifies the verb washed because with soap  pp  describes experimental results and a conclusion of this work. 

goal of the svm is to assign every x to one of two classes d = {1 -1}. the decision boundary that separates the input vectors belonging to different classes is usually an arbitrary n - 1-dimensional manifold if the input space x is ndimensional. 
　svms sidestep both difficulties  vapnik  1 . first  overfitting is avoided by choosing the unique maximum margin hyperplane among all possible hyperplanes that can separate the data in f. this hyperplane maximizes the distance to the closest data points. 
　1b be more precise  once we have chosen a kernel k we can represent the maximal margin hyperplane  or decision boundary  by a linear equation in x convex quadratic objective function with linear constraints. moreover  most of the cti prove to be zero. by definition the vectors xi corresponding with non-zero a  are called the sup-
port vectors sv and this set consists of those data points that lie closest to the hyperplane and thus are the most difficult to classify. 
　in order to classify a new point xnew  one determines the sign of 
if this sign is positive xnew  belongs to class 1  if negative to class - 1   if zero xnew lies on the decision boundary. note that we have now restricted the summation to the set sv of support vectors because the other a  are zero anyway. 
1 	some properties of kernels 
even if we don't know the exact form of the features that are used in f. moreover  the kernel expresses prior knowledge about the patterns being modelled  encoded as a similarity measure between two vectors  brown et al  1 . 
　but not all maps over x x x are kernels. since a kernel k is related to an inner product  cfr. the definition above  it has to satisfy some conditions that arise naturally from the definition of an inner product and are given by mercer's theorem: the map must be continuous and positive definite i vapnik  1 . 
　in this paper we will use following methods to construct kernels icristianini and shawe-taylor  1 : 
m1 making kernels from kernels: based on the fact that kernels satisfy a number of closure properties. in this case  the mercer conditions follow naturally from the closure properties of kernels. 
m1 making kernels from features: start from the features of the input vectors and obtain a kernel by working out their inner product. a feature is a component of the input vector. in this case  the mercer conditions follow naturally from the definition of an inner product. 
1 	metrics for memory-based learning 
in this section  we will focus on the distance functions  zavrel et al  1; cost and salzberg  1  used for memory-based learning with symbolic values. first  we will have a look at the simple overlap metric  som  and next we will discuss information gain weighting  1gw . memory-based learning is a class of machine learning techniques where training instances are stored in memory first and classification of new instances later on is based on the distance  or similarity  between the new instance and the closest training instances that have already been stored in memory. a well-known example of memory-based learning is k-nearest neighbours classification. we will not go into further detail  the literature offers 

1 	case-based reasoning 


case-based reasoning 	1 


1 	case-based reasoning 


case-based reasoning 	1 

value for c forces the svm to avoid classification errors. however  choosing a value for c that is too high will result in overfitting. increasing the dimensionality of f makes linear separation easier  cover's theorem . this can be seen by comparing the classification accuracies between the kernels kig and kpig- the results also show that our kernel kpig outperforms all other methods in the comparison. 
1 	conclusion 
in this paper  we have proposed a method for designing kernels for svm learning in natural language settings. as a starting point we took distance functions that have been used in memory-based learning. the resulting kernel kpig achieves high classification accuracy compared to other methods that have been applied on the same data set. in our approach we started from the vectors of the input space  defined a mapping on those vectors and worked out their inner product in the feature space. all necesary kernel conditions follow naturally from the definition of the inner product. we used the distance formula for kernels to show that the kernels are actually based on the distance functions from section 1. the experimental results of the kernel kpig show that increasing the dimensionality in the feature space yields better results. we increased the dimensionality of the feature space by making combinations of the features    through a polynomial kernel. in this way  we do not only take into account the similarity between corresponding features of vectors  but we also take into account the similarity between non-corresponding features of the vectors. in fact this comes down to taking into account  to a greater extend  the context in wich a word occurs. 
　we used the resolvement of pp attachment ambiguities as a case-study to validate our findings  but it is our belief that the kernel kpig can be used for a wide range of natural language problems. in the future we will apply our kernels in more complex natural language settings and compare the results to other methods that have been applied on the same problems. at the moment we are already running experiments on language independent named-entity recognition  sang  1 . the first results look promising  but it is too early to draw any significant conclusions. we will also try to further extend the kernels proposed in this paper to achieve even higher accuracies. moreover  there are still many distance functions reported in the literature that have not yet been investigated to see whether they are applicable in svm learning  we intend to investigate such distance functions and if possible derive a kernel from them. 
