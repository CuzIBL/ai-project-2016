 
   co-occurrence constraints play an important role in rule-based systems such as natural language processing. the constraints appear in many forms including agreement restrictions  e.g.. number agreement between subjects and predicates   selectional restrictions on complement types  and filler-gap movement dependencies. a general account of such constraints is given in a parallel execution model for rule-based systems - active production networks  apns . the apn model is similar to connectionist  or spreading activation  models  but explicitly provides a functional interpretation of rule-based phenomena such as variable binding  multiple instantiations  including recursion   and contextual expectations. co-occurrence constraints are represented by a theory of coindexing and trace capture  based on a feedback mechanism. several examples of constraint processing are presented which  surprisingly  also include phenomena such as phonological nulls and contraction. 
1. introduction 
   there has been an increasing interest by researchers in cognitive science and artificial intelligence in parallel processing models. examples include anderson's act* system  anderson 1   the work on connectionism at rochester  feldman and ballard 
1   the boltzmann machine model  fahlman 1; fahlman  hinton and sejnowski 1   and the models of hopfield  1  and kirkpatrick  kirkpatrick  gelatt and vecchi 1 . the common theme in this research is that intelligent activity can be modeled by large networks of simple processing elements that use some restricted form of message passing on a massively parallel basis. the models are somewhat similar to earlier neural network or spreading activation theories of human cognitive processes  selfridge 1; minsky and papert 1; quillian 1; collins and loftus 1   but benefit from advances in knowledge representation  production systems and learning algorithms. for example  several learning algorithms for multi-stage networks have recently been discovered  including the back-propagation learning algorithm of rumelhart  rumelhart and mcclelland 1 . the lack of such learning algorithms was one obstacle to 
1 	natural language 
previous development of neural network models  minsky and papert 1 . 
   current connectionist approaches are particularly attractive for computations which can be cast as function optimization. by using distributed energy minimization principles analogous to those of physical systems  e.g. annealing   minima that represent the problem solutions can be computed. a standard technique is to encode problem constraints in such a way that the energy minimum represents a  best-fit  solution; the energy minimization process is thus an iterative relaxation toward a solution. hopfield and tank  1  have worked on schemes to model associative memories and the traveling salesman problem. simulated annealing has been applied to problems in vlsi design  kirkpatrick. gelatt and vecchi 1; otten and van ginneken 1 . in ai much of the work has concentrated on best-fit problems in machine vision  sabbah 1; hinton 1  and natural language problems such as word sense disambiguation  cottrell 1; waltz and pollack 1 . although relaxation techniques have promise as best-fit categorizers. it is not clear how the current models will deal with the rule-governed behavior of parsers or problem solvers. selman  1  has looked at the problem of parsing bounded length input strings. touretzky and hinton  1  have proposed a hybrid production system architecture that attempts to capture some aspects of rule-based behavior. the overall operation is reminiscent of an ops-1 interpreter in which conflict resolution is accomplished by a relaxation process. 
   the active production network  apn  model is a parallel  rule-based network framework which accounts for properties such as priming  variable binding  and coindexing that are important for tasks such as natural language processing. the apn model describes processing behavior in terms of a distributed activation algorithm as in connectionist models  but it is not centrally concerned with questions of detailed neural modeling  distributed vs. local representations  or fixed vs. dynamic connections. the emphasis is on the functional characteristics of context-driven  parallel  rule-based processing. the translation of the apn model into low-level connectionist architectures remains an interesting  open problem. 

1. the apn model 
   each connector  node  in an active production network is defined by an expression that describes the message patterns to which it responds and the resulting messages that are propagated. for convenience  each non-leaf connector is viewed as being primarily an input connector  multiple inputs  one output  or an output connector  multiple outputs  one input .* figure i shows a simple apn and the corresponding rule-based description. the connector names in an apn are uninterpreted but are usually chosen to reflect the concepts which they represent. apns bear some resemblance to rule connection graphs for theorem provers and graphical representations for grammars such as augmented transition networks  atns   woods 1 . as in connectionist models  however  the apn network simultaneously represents the data  grammar  facts  rules  of the process and the process itself. 
   a comparison of the rules in figure 1 b  with the network in figure 1 a  reveals that the set of connectors has been explicitly designed to correspond to familiar concepts in rule-based systems  while retaining a clear interpretation in the activation-based model. the input connectors  for example  include disjunction and conjunction. the conjunctive connectors  furthermore  divide into synchronous and asynchronous types  and the asynchronous conjunctions may be ordered  sequence  or unordered. terminal symbols correspond to network leaves having no inputs. other common rule notations can be built out of these basic building blocks. optionality. as in the vp rule  is expressed by a combination of disjunction and sequence. repetition can be represented as recursive optionality. the unordered  asynchronous input conjunction  not shown in figure 1  is useful in grammars for non-configurational  free word order  languages  for specifying order-free  conjunctive  semantic relationships and for representing unbounded movement. 
   the association of lexical and non-lexical features is often described in first-order rule languages by elevating lexical categories to predicates having associated terms to represent non-lexical features. the synchronous input conjunction performs this feature association or concept specialization. specialization primarily defines feature sets and enforces selectional restrictions by intersecting features from orthogonal feature dimensions. for example  the concepts 
* in general  we will relax this convention to allow multiple disjunctive outputs  for input connectors  or disjunctive inputs  for output connectors . prior definitions of the apn framework  jones 1; jones and driscoll 1  were equivalently stated in node-based rather than connector-based terms. nodes were defined with arbitrary amounts of nested input logic. the more atomic  connector-based scheme has proved more flexible and straightforward to implement. 


 *of course  v -trans  can also be considered a specialization of -trans. 
	jon- 	1 

feature dimensions. it is often convenient  as in figure 1 b . to organize a grammar into rules which convey phrase structure  generally input connector logic  and rules which express lexical properties  output connector logic . ordered and unordered output feedback connectors  not shown in figure i  are dis-
cussed in a later section on coindexing. 
　　in the basic cycle of execution in an apn parser  each word  or morpheme  in a sentence initiates message passing activity in the network through the corresponding  usually leaf  connector of the network; thus  the words supply exogenous inputs to the network. a uniform activation algorithm specifies the message passing behavior of each type of connector in response to the messages that it receives. in general  activation messages sweep  upward  in the network to instantiate or further instantiate the rules represented by the connectors in the network. partially instantiated asynchronous rules initiate expectation messages  downward  through the network to contextually prime it. in parsing terms  the expectation messages dynamically compute a left-branching reachability set. 
　　messages are simple  having only an associated time and value. they do not encode complex structures such as entire binding lists  parse trees  feature lists or meaning representations. similarly  the local processing of messages by connectors consists of simple operations such as maximizing input values. consequently  the local computation time is bounded and the  result  of a computation consists entirely of the activation trace and the new state of the network. the information present in the activation trace can be interpreted to create a more traditional parse tree representation. for similar connectionist views  see feidman and ballard  1  and rumelhart and 
mcclelland  1 . a comparison of marker passing  value passing and unrestricted message passing systems is given bv fahlman  hinton and sejnowski 
 1 . 
   message values are subject to the following decay function: 
where /  /  is the message value at time /  and /  t{   is the value of the most recent message  from time r1for the sake of efficiency in the current sequential simulation  the implementation numbers the input events and associates these integral  times  with the values sent in messages. the value is dynamically recomputed as necessary according to the decay function. 
1. an example 
   before we enter a more formal discussion  a simple example should help to convey a feel for the exe-
1 	natural language 
cution behavior of the apn model. figure 1 illustrate the operation of the apn in figure 1 on the input string likes me. the subscripted connectors and links which appear to the right of the defined network represent the activation trace. the trace may be thought of as either an embedded state in the defined network or as a newly constructed copy or instance of a subgraph of the defined network. for expository purposes  the following discussion adopts the latter view. the widths of the links in figure 1 crudely approximate activation levels which are actually represented as real numbers in the range  -max  + m a x   . the width of the output links from a connector indicates whether its pattern is fully  level = + m a x   or partially  1   level   + m a x   satisfied. expectations are shown by dashed lines in widths which analogously illustrate full  level = -max  or partial  -max   level   1  activation levels. unactivated links are shown by a thin solid line. 
　　figure 1 a  shows the network after the message passing activity spawned by the exogenous input likes. the activation messages passed upward in the network cause new connectors  or states  v1. +trans1. v +trans 1* etc. to be instantiated. disjunctive input connectors  e.g.. vpo  maximize their inputs. input sequences  e.g.  v-npo  respond weakly to the first input  using it primarilv to gate the second input value. expectation messages are propagated downward from v-npo  opposite in sign  but equal in magnitude to the value of the first input  to condition the network for an np. at the end of each cycle of message passing  the most highly-expected leaves represent the new  primed context. 
　　in figure 1 b . me is introduced and activation proceeds upward in the presence of the expectations. expectation messages do not instantiate rules  but they can dynamically alter the behavior of the network. at np  the expectations from v-npo cause it  rather than pp. for example  to receive the activation message. thus the network exhibits nonmonotonic behavior dependent upon the processing context. in different expectation environments  the same input sequence can lead to different activation patterns. 
　　it may help to compare the apn execution model with that of prolog. prolog uses backward-chaining  static rule ordering and backtracking for nondeterminism. roughly  apns use forward-chaining  dynamic rule ordering  based on expectations which are passed by backward-chaining  and parallelism. the message passing activity in the network has a formal inference analog. from an activation-based perspective  the rules in figure 1 b  may be understood as universally quantified by an input event. for example  the rule v +trans :-likes is formally represented as  va  likes  .* -vco a +trans  x . an input event at time *o will activate likes  xq  and effectively instantiate the rule by universal specializa-

tion and modus ponens. once an input has been bound  its activation value can modulate in response to further input events which it dominates. jones  1  lists inference schemas that describe the semantics of activation messages for apn connectors. 
   the value of vpo and hence the expectations from any input sequences being driven above vpo are continuously modulated up and down as the constituents below it are started and progress to completion. the final value of vpo  in figure 1 b . is +max. indicating that the phrase was successfully parsed. if a pp attachment were to commence at this point  the value of vpo would be lowered until the pp  popped  by raising its value. it is this raising and lowering of activation levels  a kind of distributed pushing and popping of context  that promotes generally contextfree descriptions  augmented by coindexing constraints  of the syntactic base of natural languages. a similar phenomenon may extend to other levels of description  e.g.. see litman  1  for a discussion of nested discourse contexts . 
   another interesting property of the apn execution model is that an ungrammatical input may be recognized  but the overall score computed for the phrase will be penalized. unlike rigid rule-based systems where rules discretely succeed or fail  apn rules are  leaky  and may partially succeed. this situation arises  for example  when obligatory sequences or conjunctions are only partially satisfied. also  a small percentage of an expectation value received by an input sequence connector is distributed to the second input; consequently  the nearest attachment can be found even when constituents are missing. 
1. coindexing 
   the grammar writing style exemplified in figure 1 introduced a number of important representational paradigms including the widespread use of features and specialization in an x style grammar. in x theory  rules generally have the form 
x c 1  ... c  -x n - r - c ; +i         q   where each c  is either a full projection of a lexical category or a grammatical formative  jackendoff 1 . the remainder of this paper addresses the problem of how to represent the numerous co-occurrence constraints that must be added to the context-free skeleton. we will focus primarily on agreement restrictions and selectional restrictions on complements  although a related application of the techniques may apply to 

filler-gap movement dependencies as well  jones 
1 . 
   a common type of constraint  technically distinguished here as feature agreement  occurs when the values of features in two constituents may vary  but must match. in logic programming  feature agreement is implicitly specified by using the same variable name and requiring that the variable assignments unify. for example  determiner-noun number agreement is written as: 
np :- det  number  . n  number  . 
   another type of co-occurrence constraint  feature selection  occurs in situations in which the presense of a particular feature constrains the value of another feature in a complement. for example  the rule for transitive vps in figure 1 fails to capture the case constraint on the object np. a more accurate description is: 
	vp 	:- v +trans . np +obj . 
one account is that the +trans feature controls the case of the np. a similar  if not the same  feature of prepositions controls case in pps. the same method can be used to define complement and adjunct conditions in general  e.g.  voice  mood  tense . unfortunately  the usual logic programming style does not make the dependence of +obj upon +trans explicit as in the feature agreement case. the rule language being developed in an apn compiler project syntactically distinguishes several types of feature dependency. 
   feature agreement and feature selection both specify coindexing relationships. the general method for coindexing in the apn framework requires an activation trace along a feedback path between the coindexed feature  represented by an output feedback connector  and an asynchronous input conjunction which dominates it. as expectations feed back to the coindexed point  the coindexed feature is released into the new expectation environment creating a new activation trace. the trace effectively pre-instantiates the next input in the asynchronous input conjunction  constraining it to possess the selected features. note that this trace capture occurs during the time frame of the current input without advancing in the input string. the feedback response threshold can be tuned to disallow feedback through distant  weak  nonhead  paths; this effectively implements a version of the head-feature convention prevalent in linguistic theories. 
   the captured trace produces different effects depending on the form of the grammar that contains it. in the feature selection case  shown in figure 1 and discussed below  the expectations are transferred to the selected feature s . for feature agreement  as in figure 1  the expectations feed back directly to the coindexed feature. feature control is thus a lexical 
1 	natural language 
property; for example  the +obj feature must conjoin to the np head at the level of the lexicon  below any phrase structure specifier sequences. the expectation levels for the selected feature s  will be modulated together with those of the head  n  by any intervening context. 
   figures 1 b - d  illustrate the apn feature selection technique applied to object case control for transitive verbs with the phrase likes me. figure 1 b  shows the network state after activation from the verb. note that an object np instance has already been created and an input sequence is used to transfer the expectations to +obj feature. in figure 1 c . after me. the +obj feature was correctly bound. note that a lexical entry not possessing a feature of type +obj  such as /  would fail to satisfy the expectations of caseo and would be penalized accordingly. 
   figures 1 b - d  shows an example of feature agreement with the phrase a deer. in figures 1 b - c   the activation from numbero is integrated with the signal from deto and fed back to to the n side of the np. where it is captured. the expectations uhop off  the trace at the disjunctive input connector numbero. which posts expectations for the connector  -plur  that spawned its currently active input. the expectation feedback prevents the full binding of the numbero connector to an instance  but results in a pattern specialization. effectively  this causes the rule  vx  -plur  x -+number  x{   to become active. in figure 1 d   numbero is rebound to the new feature value 
-plurj. 
   the unordered output feedback connector  xand . captures more general feature agreement relationships than seq   multiple  matching input sequences can specify different input orders. in english subject-verb agreement  for example  the tensed verb element precedes the subject in questions and follows the subject in declaratives. as in the case of the input connectors seq  and xand . seq  is really just a special case of xand  in which the ordering is strict. 
   the reader should note that the feature dimensions of number and lexical category in the np grammar of figure 1 a . for example  are orthogonal. the syntactic number agreement can be layered onto an initial  over-generalized grammar that permitted any determiner and noun combination on the basis of lexical category alone. the number feature can also be coindexed to verbs for subject-verb agreement. this kind of error correction is non-destructive  i.e.. previous network fragments do not have to be eliminated or completely rewritten. this property may be especially relevant for learning algorithms or incremental compiling approaches. since the computations in each feature dimension proceed in parallel  the accuracy is improved without a penalty in  parallel  performance. the intersection of relatively simple feature systems thus gives rise to complex behavior. woods  1  offers a similar example of cascaded 




represent this theory. the same general principle used for feature selection is applied to preinstantiate the lexical category itself  instead of one of its features . upon feedback to +trans  it will insert itself as the null preposition. in turn  as a preposition  it will then pre-select the +obj feature for the object np. this has the advantage of localizing np case control to the preposition only. 

figure 1. an example of phonological nulls 
　　interestingly  a similar principle can be used in the representation of phonological contractions or compounds. figure 1 illustrates the idea for auxiliarynegation contractions. the feedback onto the contraction completes the respelling of can't  for example  into can not. 
1. conclusions 
   although more work is needed to understand how to model syntactic and semantic processing in activation-based schemes  several key representational techniques have been developed. the techniques include the intersection and synchronization of modular rule systems  specialization  feature selection  feature agreement and phonological nulls. in addition  a general grammar writing methodology based on the features and phrase structure style of x theory  modularity  and feature coindexing has been proposed. this choice is consistent with trends in linguistics away from transformational theories toward modular theories such as government-binding theory or perhaps some form of phrase structure grammar such as gpsg. it is our hope that adjustments in linguistic theory and in the apn model will produce a parsimonious account of the expressive power of natural languages. the current apn model has evolved over the last three years and should probably be taken as representative of a class of possible approaches. 
　　the parallel execution model of apns has several advantages over serial models. besides the ability to maintain multiple hypotheses in parallel  the bandwidth among collections of modules  knowledge sources  can be quite high. large numbers of features can be activated simultaneously in multiple modules; conversely  the collective context from these knowledge sources is also simultaneously projected in the form of expectations to reduce the nondeterminism engendered from any single source alone. furthermore  the model can be used for both recognition and generation processes. 
　　unlike most connectionist models  we have tried to model explicitly the functional characteristics of rule based systems such as variable binding  feature selection and agreement  multiple instantiations  including recursion   and contextual expectations. although we are supportive of connectionist efforts to develop neural models and learning algorithms  we feel that there is also a need for research efforts such as the apn model to reach toward them from what is known about rule-based systems. efforts from both directions can help prune the space of possible models  offer additional levels of description  and bridge the  symbol  gap. 

	jones 	1 

acknowledgements 
   i owe a particular debt to guy story who. besides providing programming support for the apn system  has also contributed valuably to the insights presented in this paper. 
