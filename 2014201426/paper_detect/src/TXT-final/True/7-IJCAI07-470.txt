
in this paper  we argue that the agglomerative clustering with vector cosine similarity measure performs poorly due to two reasons. first  the nearest neighbors of a document belong to different classes in many cases since any pair of documents shares lots of  general  words. second  the sparsity of class-specific  core  words leads to grouping documents with the same class labels into different clusters. both problems can be resolved by suitable smoothing of document model and using kullbackleibler divergence of two smoothed models as pairwise document distances. inspired by the recent work in information retrieval  we propose a novel context-sensitive semantic smoothing method that can automatically identifies multiword phrases in a document and then statistically map phrases to individual document terms. we evaluate the new model-based similarity measure on three datasets using complete linkage criterion for agglomerative clustering and find out it significantly improves the clustering quality over the traditional vector cosine measure.
1 introduction
document clustering algorithms can be categorized into agglomerative and partitional approaches according to the underlying clustering strategy  kaufman and rousseeuw  1 . the agglomerative approaches initially assign each document into its own cluster and repeatedly merge pairs of most similar clusters until only one cluster is left. the partitional approaches iteratively re-estimate the cluster model  or the cluster centroid  and reassign each document into the closest cluster until no document is moved any longer. in comparison with partitional approaches  the agglomerative approach does not need initialization and gives very intuitive explanation of why a set of documents are grouped together. however  it suffers from the o n1  clustering time and performs poorly in general in terms of cluster quality  steinbach et al.  1 . in this paper  we will analyze the underlying reasons of its poor performance and propose a solution.
　steinbach et al.  1  argue that the agglomerative hierarchical clustering perform poorly because the nearest neighbors of a document belong to different classes in many cases. according to their examination on the data  each class has a  core  vocabulary of words and remaining  general  words may have similar distributions on different classes. thus  two documents from different classes may share many general words  e.g. stop words  and will be viewed similar in terms of vector cosine similarity. to solve this problem  we should  discount  general words and  emphasize  more importance on core words in a vector. besides  we think the poor performance of the agglomerative clustering can also be attributed to the sparsity of core words in a document. a document is often short and contains very few number of core words. thus  two documents from the same class may share few core words and be falsely grouped into different clusters when using vector cosine similarity metric. to solve this problem  we should assign reasonable positive counts to  unseen  core words if its related topical terms occur in the document.
　discounting seen words and assigning reasonable counts to unseen words are two exact goals of the probabilistic language model smoothing. in this paper  we view the calculation of pairwise document similarity as a process of document model smoothing and comparison. as usual  we use the kullback-leibler divergence distance function to measure the difference of two models  i.e. word probability distributions . so the problem is reduced to obtaining a good smoothed language model for each document in the corpus. the language modeling approach to information retrieval  ir  has been received much attention in recent years due to its mathematical foundation and empirical effectiveness. in a nugget  the language modeling approach to ir is to smooth document models  lafferty and zhai  1 . to the best of our knowledge  the document model smoothing has not been studied in the context of agglomerative clustering. in this paper  we adapt the existing smoothing methods used in language modeling ir to the context of agglomerative clustering and hypothesize that document model smoothing can significantly improve the quality of the agglomerative hierarchical clustering.
　in ir  a simple but effective smoothing strategy is to interpolate document models with a background collection model. for example  jelinek-mercer  dirichlet  absolute discount  zhai and lafferty  1  and two-stage smoothing  zhai and lafferty  1  are all based on this strategy. in document clustering  tf-idf score is often used as the dimension values of document vectors. the effect of tf-idf scheme is roughly equivalent to the background model smoothing. however  a potentially more significant and effective smoothing method is what may be referred to as semantic smoothing where context and sense information are incorporated into the model  lafferty and zhai  1 . the first trial of semantic smoothing may be dated back to latent semantic indexing  lsi  deerwester et al.  1  which projects documents in corpus into a reduced space where document semantics becomes clear. lsi explores the structure of term co-occurrence and can solve synonymy. however  it brings noise while reducing the dimensionality because it is unable to recognize the polysemy of a same term in different contexts. in practice  it is also criticized for the lack of scalability and interpretability.
　berger and lafferty  1  proposed a kind of semantic smoothing approach referred to as the statistical translation language model which statistically mapped document terms onto query terms.
	p q | d  =t q | w  p w | d 	 1 
w
where t q | w  is the probability of translating the document term w to the query term q and p w | d  is the maximum likelihood estimator of the document model. with term translations  a document containing  star  may be returned for the query  movie . likewise  a document with the dimension of  star  but not  movie  may be merged into a cluster of  entertainment  together with a document containing  movie  but not  star . however  like the lsi  this approach also suffers from the context-insensitivity problem  i.e.  unable to incorporate contextual information into the model. thus  the resulting translation may be fairly general and contain mixed topics. for example   star  can be either from the class of  entertainment   movie star  or from the class of  military   star war .
　unlike berger and lafferty  1  who estimated word translation probabilities purely based on word distributions in a corpus  cao et al.  1  constrained word relationships with human knowledge  i.e. relationships defined in wordnet  in order to reduce noise. they further combined linearly such a semantic-constrained translation model with a smoothed unigram document model. however  their model still did not solve the context-insensitivity problem in essence.
　compound terms often play an important role for a machine to understand the meaning of texts because they usually have constant and unambiguous meanings. bai et al.  1  adopted compound terms for text classification. however  compound terms are not used for smoothing purpose in their work. instead  compound terms are directly working as features in conjunction with single-word features. in our previous work  zhou et al.  1   we proposed a context-sensitive semantic smoothing method for language modeling ir. the method decomposes a document into a set of weighted context-sensitive topic signatures and then statistically maps topic signatures into individual terms; a topic signature is defined as a pair of two concepts which are semantically and syntactically related to each other. the topic signature is similar to a compound term in the sense that both have constant and unambiguous meanings in most cases. for instance  if  star  and  movie  forms a topic signature  its context may be highly related to  entertainment   but rarely to  war . the extraction of concepts and concept pairs  however  relies on domain ontology  which is impractical for many public domains.
　to overcome this limitation  we propose the use of multiword phrases  e.g.  star war    movie star   as topic signatures in this paper. same as a concept pair  a multiword phrase is often unambiguous. furthermore  multiword phrases can be extracted from a corpus by existing statistical approaches without human knowledge. last  documents are often full of multiword phrases; thus  it is robust to smooth a document model through statistical translation of multiword phrases in a document to individual terms.
　we evaluate the new model-based document similarity metric on three datasets using agglomerative clustering with complete linkage criterion  kaufman and rousseeuw  1 . the experiment results show that the kl-divergence similarity metric performs consistently better than the vector cosine metric. moreover  the kl-divergence metric with semantic smoothing significantly outperforms with simple background smoothing. the result of the agglomerative clustering with semantic smoothing is comparable to that of the k-means partitional clustering on three testing datasets.

figure 1. illustration of document indexing. vbtb  vbdb and vbwb are phrase set  document set and word set  respectively.
1 document model smoothing
1 semantic smoothing of document model
suppose we have indexed all documents in a given collection c with terms  individual words  and topic signatures  multiword phrases  as illustrated in figure 1.
the translation probabilities from a topic signature tk to any individual term w  denoted as p w|tk   are also given. then we can easily obtain a document model below:
	pt  w | d  = p w | tk   pml  tk | d 	 1 
k
the likelihood of a given document generating the topic signature tk can be estimated with
c tk  d 
	pml  tk | d  =		 1 
c ti  d 
i
where c ti d  is the frequency of the topic signature tbib in a given document d.
　we refer to the above model as translation model after berger and lafferty's work  1 . as we discussed in the introduction  the translation from multiword phrase to individual term would be very specific. thus  the translation model not only weakens the effect of  general  words  but also relieves the sparsity of class-specific  core  words. however  not all topics in a document can be expressed by topic signatures  i.e.  multiword phrases . if only translation model is used  there will be serious information loss. a natural extension is to interpolate the translation model with a unigram language model below:
	pb  w | d  =  1  pml  w | d  +p w | c 	 1 
here   is a coefficient accounting for the background collection model p w | c  and pml  w | d  is a maximum likelihood estimator. in the experiment    is set to 1. we refer to this unigram model as simple language model or baseline language model. we use jelinek-mercer smoothing on the purpose of further discounting  general  words.
　the final document model for clustering use is described in equation  1 . it is a mixture model with two components: a simple language model and a translation model.
	pbt  w | d  =  1-  pb  w | d  +pt  w | d 	 1 
the translation coefficient     is to control the influence of two components in the mixture model. with training data  the translation coefficient can be trained by optimizing the clustering quality.
1 topic signature extraction and translation
zhou et al  1  implemented topic signatures as concept pairs and developed an ontology-based approach to extract concepts and concept pairs from documents. however  for many domains  ontology is not available. for this reason  we propose the use of multiword phrases as topic signatures and employ xtract  smadja  1  to identify phrases in documents. xtract is a kind of statistical extraction tool with some syntactic constraints. it is able to extract noun phrases frequently occurring in the corpus without any external knowledge. xtract uses four parameters  strength  kb1b   peak z-score  kb1b   spread  ub1b   and percentage frequency  t   to control the quantity and quality of the extracted phrases. in the experiment  the four parameters are set to 1  1  1  and
1  respectively.
table 1. examples of phrase-word translations. the three phrases are automatically extracted from the collection of 1-newsgroup by xtract. we list the top 1 topical words for each phrase.
arab countrynuclear powergay peopletermprob.termprob.termprob.arab1nuclear1gay1country1power1homosexual1israel1plant1sexual1jew1technology1church1israeli1air1persecute1jewish1fuel1friend1palestine1fossil1abolitionist11.1reactor1parent1syria1steam1society1expel1contaminate1lesbian1terror1water1themselves1iraq1cold1lover1davidsson1cool1lifestyle1war1tower1emotion1homeland1industry1thier1egypt1radioactive1repress1zionist1boil1affirm1legitimism1site1ministry1kaufman1built1straight1rejoinder1temperature1preach1　for each phrase tbkb  we have a set of documents  d bkb  containing that phrase. intuitively  we can use this document set d bkb to estimate the translation probabilities for tbkb  i.e.  determining the probability of translating the given phrase tbkb to terms in the vocabulary. if all terms appearing in the document set center on the sub-topic represented by tbkb  we can simply use the maximum likelihood estimator and the problem is as simple as term frequency counting. however  some terms address the issue of other sub-topics while some are background terms of the whole collection. we then use a mixture language model to remove noise. assuming the set of documents containing tbkb is generated by a mixture language model  i.e.  all terms in the document set are either translated by the given topic signature model p w |tk   or generated by the background collection model p w |c     we have:
	p w |tk  c  =  1  p w |tk   +p w | c 	 1 
where   is a coefficient accounting for the background noise and tk denotes parameter set of translation probabilities for tbkb. under this mixture language model  the log likelihood of generating the document set dbkb is:
	log p dk |tk  c  =c w  dk  log p w |tk  c 	 1 
w
where c w dk   is the document frequency of term w in dbk.b  i.e.  the cooccurrence count of w and t bkb in the whole collection. the translation model can be estimated using the em algorithm  dempster et al.  1 . the em update formulas are:
	 n 	  w |tk  
	p 	 w  =	 1 
  w |tk    w | c 
 n 
	 n+1 	c w dk   p 	 w 
	p	 w |tk   =	 n 	 1 
	c wi  dk   p 	 wi  
i
in the experiment  we set the background coefficient  =1. we also truncate terms with extremely small translation probabilities for two purposes. first  with smaller number of translation space  the document smoothing will be much more efficient. second  we assume terms with extremely small probability are noise  i.e. not semantically related to the given topic signature . in detail  we disregard all terms with translation probability less than 1 and renormalize the translation probabilities of the remaining terms.
1 the kl-divergence distance metric
after estimating a language model for each document in the corpus with context-sensitive semantic smoothing  we use the kullback-leibler divergence of two language models as the distance measure of the corresponding two documents. given two probabilistic document models p w|d1  and p w|d1   the kl-divergence distance of p w|d1  to p w|d1  is defined as:
p w | d1 
 d1 d1  = p w | d1 log   1  wv	p w | d1 
where v is the vocabulary of the corpus. kl-divergence distance will be a non-negative score. it gets the zero value if and only if two document models are exactly same. however  kl-divergence is not a symmetric metric. thus  we define the distance of two documents as the minimum of two kl-divergence distances. that is 
	dist d1 d1  = min{ d1 d1    d1 d1 }	 1 
　the calculation of kl-divergence involves scanning the vocabulary  which makes the solution computationally inefficient. to solve this problem  we truncate terms with its distribution probability less than 1 while estimating document model using the equation  1  and renormalize the probabilities of remaining terms. because we keep terms with high probability values in document models  it makes almost no difference in clustering results.
1 experiment settings and result analysis
1 evaluation methodology
cluster quality is evaluated by three extrinsic measures  purity  zhao and karypis  1   entropy  steinbach et al.  1  and normalized mutual information  nmi  banerjee and ghosh  1 . due to the space limit  we only list the result of nmi  an increasingly popular measure of cluster quality. the other two measures are consistent with nmi on all runs. nmi is defined as the mutual information between the cluster assignments and a pre-existing labeling of the dataset normalized by the arithmetic mean of the maximum possible entropies of the empirical marginals  i.e. 
i x;y 
	nmi x y  =		 1 
 logk + logc /1
where x is a random variable for cluster assignments  y is a random variable for the pre-existing labels on the same data  k is the number of clusters  and c is the number of preexisting classes. regarding the details of computing i x; y   please refer to  banerjee and ghosh  1 . nmi ranges from 1 to 1. the bigger the nmi is the higher quality the clustering is. nmi is better than other common extrinsic measures such as purity and entropy in the sense that it does not necessarily increase when the number of clusters increases.
　we take complete linkage criterion for agglomerative hierarchical clustering. the two document similarity metrics are the traditional vector cosine and the kullback-leibler divergence proposed in this paper. for cosine similarity  we try three different vector representations: term frequency  tf   normalized term frequency  i.e.  tf divided by the vector length   and tf-idf. for kl-divergence metric  we use document models with semantic smoothing as described in equation  1  and test 1 translation coefficients     ranging from 1 to 1. when  =1  it actually uses simple background smoothing.
　in order to compare with the partitional approach  we also implement a basic k-means using cosine similarity metric on three vector representations  tf  ntf  and tf-idf . the calculation of the cluster centroid uses the following formula:
1
	centroid = c dcd	 1 
where c is the corpus. since the result of k-means clustering varies with the initialization. we run ten times with random initialization and average the results. for various vector representations  each run has the same initialization.
1 datasets
we conduct clustering experiments on three datasets: tdt1  la times  from trec   and 1-newsgroups  1ng . the tdt1 corpus has 1 document classes  each of which reports a major news event. la times news are labeled with 1 unique section names  e.g.  financial  entertainment  sports  etc. 1-newsgroups dataset is collected from 1 different usenet newsgroups  1 articles from each.
　we index 1 documents in tdt1 that have a unique class label  1 documents from top ten sections of la times  and all 1 documents in 1-newsgroups. for each document  we index its title and body content with both multiword phrases and individual words  and ignore other sections including meta data. a list of 1 stop words is used. in the testing stage  1 documents are randomly picked from each class of a given dataset and merged into a big pool for clustering. for each dataset  we create five such random pools and average the experimental results. the ten classes selected from tdt1 are 1  1  1  1  1  1  1  1  1  and 1. the ten sections selected from la times are entertainment  financial  foreign  late final  letters  metro  national  sports  calendar  and view. all 1 classes of 1ng are selected for testing.
table 1. statistics of three datasets
dataset nametdt1la times1ng# of indexed docs111# of words111# of phrases111avg. doc length  word 11avg. doc length  phrase 11# of classes11.1 experiment results and analysis
the dragon toolkit  zhou et al.  1  is used to conduct clustering experiments. the translation coefficient     in equation  1  is trained over tdt1 dataset by maximizing the nmi of clustering. the optimal value  =1 is then applied to two other datasets. the nmi result of the agglomerative hierarchical clustering with complete linkage criterion is listed in table 1. when the vector cosine measure is used as pairwise document similarity  the tf-idf scheme performs slightly better than the tf scheme. as we discussed before  the heuristic tf-idf weighting scheme can discount  general  words and strengthen  specific  words in a document vector. thus  it can improve the agglomerative clustering quality. the kl-divergence similarity measure with background smoothing of document models  i.e.   =1  consistently outperforms the cosine measure on both tf and tf-idf schemes. as expected  the kl-divergence measure with context-sensitive semantic smoothing significantly improves the quality of the agglomerative clustering on all three datasets. after semantic smoothing  the classindependent general words will be dramatically weakened and the class-specific  core  words will be strengthened even if it does not appear in the document at all. thus  the distance of intra-class documents will be decreased while the distance of inter-documents will be increased  and hence improve the clustering quality.
table 1. nmi results of the agglomerative hierarchical clustering with complete linkage criterion
datasetcosinekl-divergencetf/ntftf-idfbackgroundsemantictdt1.1.1.1.1la times11111ng1111　to see the robustness of the semantic smoothing method  we show the performance curve in figure 1. except for the point of  =1  the semantic smoothing always improve the cluster quality over the simple background smoothing. in general  nmi will increase with the increase of translation coefficient till the peak point  around 1 in our case  and then go downward. in our experiment  we only consider phrases appearing in more than 1 documents as topic signatures in order to obtain a good estimate of translation probabilities. moreover  not all topics in a document can be expressed by multiword phrases. thus  the phrase-based semantic smoothing will cause information loss. we interpolate the translation model with a unigram language model to make up the loss. now it is easy to understand why the nmi goes downward when the influence of the semantic smoothing is too high. actually  lsi also causes information loss when the dimensionality reduction is too aggressive; but there is no mechanism to recover the loss. in this sense  the semantic smoothing approach is more flexible than lsi.

figure 1. the variance of the cluster quality with the translation coefficient     which controls the influence of semantic smoothing
　steinbach et al.  1  reported that k-means performed as good as or better than agglomerative approaches. our experiment also repeated this finding. using vector cosine similarity measure  the complete linkage algorithm performs significantly worse than the k-means  see table 1 and 1 . however  with semantic smoothing of document models  the result of complete linkage clustering is comparable to that of k-means on three representation schemes  tf  norm tf  and tf-idf . this is also a kind of indication that semantic smoothing of document models is very effective in improving agglomerative clustering approaches.
table1. nmi results of the regular k-means clustering. k is the number of true classes listed in table 1.
datasettfntftf-idftdt1.1.1.1la times1111ng1111 conclusions and future work
the quality of agglomerative hierarchical clustering often highly depends on pairwise document similarity measures. the density of class-independent  general  words and the sparsity of class-specific  core  words in documents make the traditional vector cosine a poor similarity measure for agglomerative clustering. to solve this problem  we develop a context-sensitive semantic smoothing method to  smooth  document models  i.e. discounting seen  general  words and assigning reasonable positive counts to unseen  core  words in a document  and further use kullback-leibler divergence of smoothed probabilistic models as the document similarity measure for clustering. the clustering experiments on three different datasets show that the combination of semantic smoothing and the kl-divergence similarity measure can significantly improve agglomerative hierarchical clustering.
　our semantic smoothing approach uses unambiguous multiword phrases as topic signature and statistically maps phrases onto individual terms. conceptually  a phrase here corresponds to a latent dimension in latent semantic indexing  lsi . however  the semantics of phrase is explicit and clear. because multiword phrases are unambiguous in most cases  its translation to individual terms is very specific whereas lsi brings noise when exploring latent semantic structures due to term polysemy. lsi also causes information loss during dimensionality reduction. our approach can recover the information loss by interpolating the phrase translation model with a smoothed unigram language model. but how to obtain optimal weights for each component in the mixture model will be an open problem. in this paper  we empirically tuned a fixed translation coefficient to optimize the clustering results. ideally  this coefficient should be optimized over each document. in addition  we use natural language processing techniques to identify sub-topics  phrases  from texts. this is somehow ad hoc nature and could be improved in future.
　recent advances in document clustering have shown that model-based partitional approaches are more efficient and effective than similarity-based approaches in general  zhong and ghosh  1 . however  most generative models simply use laplacian smoothing to smooth the cluster models on the purpose of avoiding zero probability. for future work  we will apply context-sensitive semantic smoothing to model-based partitional approaches  which may further improve their clustering quality.
acknowledgment
this work is supported in part by nsf career grant  nsf iis 1   nsf ccf 1  pa dept of health tobacco settlement formula grant  no. 1 and no. 1   and pa dept of health grant  no. 1 . we also thank three anonymous reviewers for their instructive comments on the paper.
