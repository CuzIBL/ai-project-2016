
we describe our approachfor generatingexpressive music performances of monophonic jazz melodies. it consists of three components:  a  a melodic transcription component which extracts a set of acoustic features from monophonic recordings   b  a machine learning component which induces an expressive transformationmodel from the set of extracted acoustic features  and  c  a melody synthesis component which generates expressive monophonic output  midi or audio  from inexpressive melody descriptions using the induced expressive transformation model. in this paper we concentrate on the machine learning component  in particular  on the learning scheme we use for generating expressive audio from a score.
1 introduction
expressive performance is an important issue in music which has been studied from different perspectives  gabrielsson  1 . the main approaches to empirically study expressive performance have been based on statistical analysis  e.g.  repp  1    mathematical modelling  e.g.  todd  1    and analysis-by-synthesis  e.g.  friberg  1  . in all these approaches  it is a person who is responsible for devising a theory or mathematical model which captures different aspects of musical expressive performance. recently  there has been work on applying machine learning techniques to the study of expressive performance. widmer  widmer  1  has focused on the task of discovering general rules of expressive classical piano and recognizingfamous pianists from their playing style. lopez de mantaras et al.  lopez de mantaras  1  reported on saxex  a case-based reasoning system capable of inferring a set of expressive transformations and applying them to a solo performance in jazz. in this paper we describe an approachto investigate musical expressive performancebased on inductive machine learning. in particular  we are interested in monophonicjazz melodies performed by a saxophonist. our work differentiates from that of widmer in that  being focused on saxophone jazz performances  we are interested in intra-note variations  e.g. vibrato  absent in piano  as well as melody alterations  e.g. onset deviations  ornamentations  which are normally considered performance errors in classical music. the work of lopez de mantaras et al. is similar to ours but they are unable to explain their predictions. the deviations and changes we consider are on note duration  note onset  note energy  and intra-note features  e.g. attack  vibrato . the study of these variations is the basis of an inductive content-based transformation tool for generating expressive performances of musical pieces. the tool can be divided into three components: a melodic transcription component  a machine learning component  and a melody synthesis component. in the following  we briefly describe each of these components.
1 melodic description
sound analysis and synthesis techniques based on spectral models are used for extracting high-level symbolic features from the recordings. the sound spectral model analysis techniques are based on decomposing the original signal into sinusoids plus a spectral residual. from the sinusoids of a monophonic signal it is possible to extract information on note pitch  onset  duration  attack and energy  among other high-level information. this informationcan be modified and the result added back to the spectral representation without loss of quality. we use the software smstools which is an ideal tool for preprocessing the signal and providing a highlevel description of the audio recordings  as well as for generating an expressive audio according to the transformations obtained by machine learning methods.
¡¡the low-level descriptors used to characterize the melodic features of our recordings are instantaneous energy and fundamental frequency. the procedure for computing the descriptors is first to divide the audio signal into analysis frames and compute a set of low-level descriptors for each analysis frame. then  a note segmentation is performed using lowlevel descriptor values. once the note boundaries are known  the note descriptors are computed from the low-level and the fundamental frequency values  see  gomez et al.  1  for details about the algorithm .
1 expressive performance knowledge induction
data set. the training data used in our experimental investigations are monophonic recordings of three jazz standards  body and soul  once i loved and like someone in love  performed by a professional musician  a saxophone player  at 1 different tempos around the nominal tempo. the resulting data set is composed of 1 performed notes.
descriptors. in this paper  we are concerned with note-level  in particular note duration  note onset and note energy  and intra-note-level  in particular intra-note pitch and amplitude shape  expressive transformations. each note in the training data is annotated with its corresponding deviation and a number of attributes representing both properties of the note itself and some aspects of the local context in which the note appears. information about intrinsic properties of the note include note duration  note metrical position  and note envelope information  while information about its context include the note narmour group s   narmour  1   duration of previous and following notes  and extension and direction of the intervals between the note and the previous and following notes.
machine learning techniques. in order to induce predictive models for duration ratio  onset deviation and energy  variation  we have applied machine learning techniques such as regression trees  model trees and support vector machines  for a complete comparison of the accuracy of these techniques  see  ramirez et al.  1  . among these techniques  model trees is the most accurate method  and thus  we have based the machine learning component of our tool on this method. we have also induced rule-based models  ramirez et al.  1  to explain the predictions made by our tool. in order to induce a predictive model for intra-note features we have devised a learning scheme roughly described as follows:
1. apply k-means clustering to all the notes in the data set.we decided to set the number of clusters to five. this decision was taken after analyzing a large number of notes in our data set and considering that there were basically five qualitativelydifferenttypes of noteshapes. we characterize each note in the data set by its attack  sustain and release.
1. apply a classification algorithm  i.e. classification trees  to predict the cluster to which the note belongs. in order to train our classifier we used the descriptors described above.
1. given a note and its cluster  apply a nearest neighbor algorithm to determine the most similar note in the cluster. we use the pitch and duration of a note as the distance measure  i.e. given a note  we look in the predicted cluster for the closest note in duration and in pitch. we are particularly interested in duration and pitch because we want to minimize the loss in sound quality when transforming the selected note to a note with the required pitch and the computed duration.
¡¡once we obtain all the notes in a score by applying the learning scheme described above  we proceed to 'glue' the obtained notes together. finally  we apply an algorithm to obtain smooth note transitions. a sample of a melody produced by our learning scheme can be found at www.iua.upf.es/¡«rramirez/promusic/demo.wav.
1 melody synthesis
the melody synthesis component transforms an inexpressive melody input into an expressive melody following the induced models. given a melody score  i.e. an inexpressive description of a melody   our tool can either generate an expressive midi performance  or generate an expressive audio performance. in the second case  in addition to using the duration  onset and energy models for computing expressive deviations of these parameters  we apply the intra-note model to obtain the set of notes to be used to construct the audio expressive performance. in order to build the final audio performance we transform each of the obtained notes according to the computed duration  onset and energy deviations  and concatenate the transformed notes using an algorithm that optimizes the transitions between notes.
acknowledgments
this work is supported by the spanish tic project promusic  tic 1-c1 . we would like to thank emilia gomez  esteban maestre and maarten grachten for processing the data.
