 
stochastic local search  sls  algorithms for prepositional satisfiability testing  sat  have become popular and powerful tools for solving suitably encoded hard combinatorial from different domains like  e.g.  planning. consequently  there is a considerable interest in finding sat-encodings which facilitate the efficient application of sls algorithms. in this work  we study how two encodings schemes for combinatorial problems  like the well-known constraint satisfaction or hamilton circuit problem  affect sls performance on the sat-encoded instances. to explain the observed performance differences  we identify features of the induces search spaces which affect sls performance. we furthermore present initial results of a comparitive analysis of the performance of the sat-encoding and -solving approach versus that of native sls algorithms directly applied to the unencoded problem instances. 
1 	introduction 
in the past few years  the development of extremely fast stochastic algorithms for the propositional satisfiability problem  sat  have stirred considerable interest in the ai community. modern stochastic local search algorithms  like walksat and its more recent variants  mcallester et al  1   can solve hard instances with several thousand variables and ten thousands of clauses within minutes of computing time. while only a very few  real-world  problems come as instances of sat  many of these are combinatorial problems with quite natural csp-like formulations  which can be easily encoded into sat. in practice  however  for solving problems by encoding them into sat and applying state-ofthe-art sat solvers  one has to find encodings which cannot only be efficiently generated  but which are also efficiently solvable by the respective sat-solvers. while recent successes in solving sat-encoded planning problems suggest that such efficient encodings can be found  our understanding of how encodings affect the performance of modern sat algoridims and how good encodings can be found is very limited.  characterizing the computational properties of different encodings of a real-world problem domain  and/or giving general principles that hold over a range of domains  has 
1 	challenge papers 
therefore been proposed as a challenge for propositional reasoning at ijcai1  selman et al  1 . 
　we approach this problem by studying the impact of encoding strategies on search space structure and the performance of stochastic local search  sls  algorithms. in particular  we investigate the following questions: 
1. does it pay off to minimise the number of propositional variables  i.e.  are compact encodings which achieve small search spaces preferable to the bigger  but conceptually simpler sparse encodings  
1. is the generic problem solving approach using satencodings and modern sls algorithms for sat competitive with applying sls algorithms to the un-encoded problem instances  
　our investigation is based on two case-studies  covering sets of hard instances of the np-hard constraint satisfaction problem  csp  and hamilton circuit problem  hcp . building on earlier work on search space structure and sls performance  clark et al  1; yokoo  1; frank et al  1  and sat encodings  ernst et al  1   we empirically analyse the impact of different encoding strategies on sls performance and provide explanations for our observations by identifying search space features correlated with the observed sls performance. furthermore  we compare the performance of sls algorithms on the best sat-encoding of hard random binary csp encodings with the performance of the best known native csp algorithm based on stochastic local search. 
　as a preview of the results presented in the next sections  we briefly summarise our answers to the questions from above - however  given the general nature of these questions and the limited scope of the underlying empirical evidence these answers are tentative and should be understood as testable hypotheses. 
1. according to our results for csp and hcp  it seems to be much more advisable to use sparse rather than compact encodings  as the search spaces produced by the compact encodings are smaller but have characteristic features which impede local search. 
1. when comparing the performance of sls algorithms for csp and sls-based sat algorithms applied to satencoded csps  we observe a surprisingly small advantage for the direct csp solving approach  which is outweighed by other advantages of the generic sat-
　
encoding and solving approach such as the availability of very efficient implementations. 
　the remainder of this paper is structured in the following way. section 1 gives some background on sls algorithms for sat and introduces the problems classes and test-sets used for our empirical analysis. section 1 reviews previous work on search space structure and its impact on sls performance and presents our approach to search space structure analysis. sections 1 present the empirical investigations of the three questions from above  and section 1 contains some conclusions and points out directions for future research. 
1 	background 
stochastic local search approaches for sat became prominent in 1  when independently selman  levesque  and mitchell  selman et al  1  as well as gu  gu  1  introduced algorithms based on stochastic local hill-climbing which could be shown to outperform state-of-the-art systematic sat algorithms on a variety of hard subclasses of sat. since then  numerous other sls schemes for sat have been proposed. to date  state-of-the-art sls algorithms can solve hard sat problems up to several thousand variables  including sat-encoded problems from other domains. in the recent past  especially the successful applications of sls-based sat algorithms for solving sat-encoded planning problems  have stirred considerable interest in the ai community  kautz and selman  1; ernst et a/.  1; kautz and selman  1 . 
　the algorithms considered here are model finding algorithms for cnf formulae. the underlying state space is always defined as the set of all assignments for the variables appearing in the given formula. local search steps modify at most the value assigned to one of the propositional variables appearing in the formula; such a move is called a variable flip. the objective function is generally defined as the number of clauses which are unsatisfied under a given variable assignment; thus  the models of the given formula correspond to the global minima of this function. the general idea for finding these is to perform stochastic hill-climbing on the objective function  starting from a randomly generated initial assignment. 
　the main difference between the individual algorithms lies in the strategy used to select the variable to be flipped next. in this paper  we focus on the well-known walksat family of algorithms  mcallester et a/.  1   which provided a substantial driving force for the development of sls algorithms for sat and have been extremely successful when applied to a broad range of problems from different domains. walksat algorithms start from a randomly chosen variable assignment and repeatedly select one of the clauses which are violated by the current assignment. then  according to some heuristic a variable occurring in this clause is flipped using a greedy bias to increase the total number of satisfied clauses. for the original walksat algorithm  in the following referred to simply as walksat  the following heuristic is applied. if in the selected clause variables can be flipped without violating other clauses  one of these is randomly chosen. otherwise  with a fixed probability p a variable is randomly chosen from the clause and with probability 1-pa variable is picked which minimises the number of clauses which are currently satisfied but would become violated by the variable's flip  number of breaks . the walk probability p  also called the noise parameter  has an important influence on the algorithms' overall performance. in this paper  we always use approximately optimal noise parameter settings for evaluating sls performance; these settings are experimentally determined such that they minimise the expected number of flips for solving the given problem instance. 
　for the empirical study presented here  we use two wellknown classes of combinatorial problems: random binary constrained satisfaction problems  csps  and random hamilton circuit problems  hcps . both binary csps and 
hcps form np-complete problem classes. random binary csps have been studied extensively by various researchers  especially in the context of phase transition phenomena and their impact on the performance of csp algorithms  smith and dyer  1; prosser  1 . to obtain a test-set of hard problem instances  csp instances with 1 variables and domain size 1 were sampled from the phase transition region characterised by a constraint graph density of  = 1 and a constraint tightness of  = 1. filtering out the insoluble instances with a complete csp algorithm  test-set csp1  containing 1 soluble instances from this problem distribution  was generated. 
　for empirically investigating sls performance for different sat-encodings of the hcp  we focussed on the hamilton circuit problem in directed random graphs. for a given graph  the hcp is to find a cyclic tour  hamilton circuit  using the edges of the graph which visits every vertex exactly once. in earlier work  when investigating the dependence of the existence of hamilton circuits on the average connectivity  edges per vertex   a phase transition phenomenon was observed  cheeseman et al.  1 . the associated peak in hardness for backtracking algorithms was located between k: = e/ nlogn   1 and 1  where n is the number of vertices and e the number of edges  frank and martel  1 . based on these results  we created a test-set by randomly sampling soluble hcp instances from distributions of directed random graphs with n = 1 and k = 1. the test-set was generated using a hcp generator and solver developed and provided by joe culberson; insoluble instances were filtered out using the integrated systematic hcp solver  such that the final test-set hcplo contains 1 soluble instances. 
1 search space structure and sls performance 
obviously  the behaviour of sls algorithms is determined by the topology of the search space  i.e.  the objective function induced by a specific problem instance. recently  a growing number of researchers have been investigating the nature of this dependency for sls-based sat algorithms  clark et al.  1; yokoo  1; frank et al.y 1 . however  all of the studies we are aware of are restricted to random-1-sat  a widely used class of randomly generated sat problems  while sat-encoded problems from other domains have not been addressed. in  clark et al.  1   it has been shown that for a given problem instance  its number of solutions is strongly negatively correlated with the performance of some sls algorithms for sat and csp based on hill-climbing. 
this confirms the intuition that instances with a high solution density  i.e.  a large number of solutions  tend to be much easier to solve for sls algorithms than instances with very few 
	h1s 	1 
　
solutions.  yokoo  1  explains the peak in local search cost observed at the phase-transition region of random-1sat in terms of the number and size of local minima regions in the search space. his analysis  however  relies on an exhaustive analyses of the search space and is therefore restricted to very small problem instances.  frank et al  1  analyse the topology of search spaces induced by random1-sat formulae  but do not investigate the concrete impact of these features  such as the size of local minima  on sls performance. 
　the approach taken here is to compare both the impact of encoding strategies on sls performance as well as to provide explanations of the observed correlation in terms of search space features induced by the different encodings. to be not restricted to very small problem instances which possibly do not reflect the typical situation as encountered for realistic problems  i.e.  instances which can be solved with state-ofthe art sls algorithms   we restrict our analysis to features with can be determined without exhaustive search of large parts of the search space. the features we measure are: 
  the solution density  number of solutions/search space size ; 
  the standard deviation of the objective function 
 sdnclu ; 
  the local minima branching along sls trajectories 
 blmin . 
as the solution density is known to be an important factor for 
sls performance  it should be taken into account although it is generally time-consuming to measure. for counting the number of solutions of a given problem instance  we applied a modified version of the asat algorithm  dubois et al  1  to the sat-encoded instances. to our best knowledge  both sdnclu and blmin have not been studied before in the context of search space structure. 
　intuitively  large sdnlcu values should indicate a rugged search space structure for which sls approaches based on hill-climbing are more effective than for featureless  flat search spaces with many plateaus. to measure sdnclu values  we determine the objective function value  number of unsatisfied clauses  for a sample of 1 randomly chosen assignments. to improve comparability of the results for different numbers of clauses  we scale objective function values to the interval  1   by division by the number of clauses . the sdnclu value is then computed as the empirical standard deviation over this sample. although compared to the search space sizes of our test instances the sample size is very small  we get meaningful results - presumably because of the global effects of the encoding strategies on search space topology. 
　since it is known that the number and size of local minima plays an important role for sls performance on random1-sat  yokoo  1   we extended these results by studying the structure of local minima regions. we do this by measuring the average branching of local minima states  i.e.  for a given variable assignment  the number of neighbouring assignments1 with the same objective function value. the 
     1  the neighbourhood relation is the same as used by all gsattype algorithms  i.e.  two assignments are neighbours if and only if they differ in the truth value of exactly one variable. 
1 	challenge papers 
underlying intuition is that highly branched local minima regions are more difficult to escape from  as there are fewer escape routes for random walk and similar plateau escape techniques  but more possibilities for non-deterministic loops to occur in the search trajectory. local minima states are typically quite rare for the problem instances studied here; thus  blmin cannot be measured by randomly sampling the search space. instead we sampled the local minima states along sls trajectories  using a sample size of 1. again  to improve comparability of the results between different problem instances  we scale the values thus obtained to the interval  1   by division by the number of variables . 
　for a given problem instance  walksats performance  denoted as isc - local search cost  is measured as the expected number of flips per solution  determined from 1 runs per instance using an approximately optimal noise setting  see above  and a cutoff parameter high enough to guarantee that in every run a solution was found. 1 
1 	compact versus sparse encodings 
many combinatorial problems  such as number partitioning  bin packing  or hamilton circuit  can be quite naturally formulated as discrete constraint satisfaction problems. when encoding such csp formulations into sat  perhaps the most intuitive way is to encode each assignment of a value to a csp variable by a different propositional variable  de kleer  1 . we call this the sparse encoding  since it results in relatively sparse constraint graphs1 for the resulting cnf formulae. this encoding strategy requires  d    n variables  where 
 d  is the domain size and n the number of csp variables.1 
　given the intuition that high solution densities should facilitate local search  see above   it seems to be worthwhile to consider encodings which minimise the number of propositional variables  and therefore the potential search space size without affecting the number of solutions. one encoding strategy which achieves that is the compact encoding obtained by representing each value assignment to a csp variable binarily using a set of  propositional variables.compared to the sparse encoding  this strategy significantly reduces the search space by a factor of 1 n     d  - 
  while the number of clauses is usually similar  as it is usually dominated by clauses encoding the constraint relations  the number of which is identical for both encodings . the compact encoding is well-known from the literature; it has been proposed in the context of sat variable complexity by  iwama and miyazaki  1  and is also used in the satbased medic planning system  where it is called  factored representation   ernst et ai  1 . 
　to investigate the impact of these two encoding schemes on sls performance we measured walksats performance 
   1 our actual experimental methodology is based on measuring run-time distributions  rtds  as outlined in  hoos and stiitzle  1 ; the rtd data is not reported here  but can be obtained from the author. a more detailed description of the empirical study and its results can be found in  hoos  1  and will be presented in more detail in an extended version of this paper. 
   1 the constraint graph consists of one node for each propositional variable and edges between nodes corresponding to variables which occur together in some clause of the given cnf formula. 
   1 for simplicity's sake we assume that the domain sizes for all csp variables are identical. 
　
on the test-sets of random binary csp and hcp instances described above. the hcp instances are encoded as csps by focussing on vertex permutations of the given graph as solution candidates  this idea has been proposed in  iwama and miyazaki  1  ; in particular  for each vertex v we introduce a csp variable the value of which represents v's position in the permutation. the constraint relations ensure that each vertex appears exactly once  type 1 constraints   i.e.  the candidate solution corresponds to a valid permutation of the vertices  and that each pair of neighbouring vertices in this  cyclic  permutation is connected by an edge in the given graph  type 1 constraints . 
　figure 1 shows the correlation of the average local search cost  isc  between the different encodings across the testset csp1; each data point corresponds to the isc for the sparse vs the compact encoding for one problem instance from the original test-set. as can be seen from the scatter plot  there is a strong linear correlation between the logarithm's average local search cost for both encodings  correlation coefficient r = 1   i.e.; instances which are relatively hard for walksat when sparsely encoded also tend to be hard when using the compact encoding. but this analysis also shows that the compact encoding results in instances for which the isc is generally ca. 1 times higher than for the corresponding sparsely encoded instances  although the compact encoding requires less then half the number of variables and significantly fewer clauses than the sparse encoding  cf. table 1 . this confirms earlier observations  ernst et al  1  that the compact encoding generates problem instances which are typically extremely hard for stochastic local search. 
　but what exactly makes this encoding so ineffective  table 1 shows the solution density  sdnclu  and blrnin values for the easiest  median  and hardest  w.r.t. their isc values for walksat  problem instance from the test-set c sp1 -1. the data confirms that within the test-set  the solution density is the predominant factor affecting local search cost  as earlier observed for random-1-s at test-sets  clark et al  1. however  between the two encodings  this does not hold. 
instead  the sdcnlu and the blmin values indicate that the compact encoding induces a flatter  featureless search space topology characterised by considerably higher branched local minima states. as argued above  intuitively this makes local search more difficult  which is consistent with the considerably higher local search cost observed for walksat on the correspondingly encoded csp instances. interestingly  this explanation is also quite consistent with the  relatively big  differences in local search cost within the corresponding testsets  although the relatively small differences in sdcnlu and blrnin seem to confirm the predominant role of the solution density. 
　applying the same analysis to sparsely and compactly encoded versions of the hcp test-set hcplo gives exactly analogous results  cf. table 1 ; the correlation coefficient for the correlation between log /sc  for -s and -c is 1  and thus confirms our observations and interpretation given above. apparently  the compact encoding induces rather flat  featureless search spaces which impede local search and local minima escape to such an extent that walksats performance is significantly reduced despite the much higher solution density achieved by reducing the number of variables. 
there is  however  another factor to be considered. the 

figure 1: correlation of average local search cost per instance between sparse  horizontal  and compact sat-encoding  vertical  for csp instances from test-set csp1-1 when using walksat with approx. optimal noise. 
cpu-time required for each single flip performed by walksat is roughly constant for a given problem instance; it depends  however  considerably on the syntactical features of the given formula  especially number and length of clauses . comparing the cpu-time per variable flip between the formulae generated by the sparse and compact encoding schemes reveals that for the compact encoding also the cputime per flip is significantly higher  for both  csp1-1-c and hcplo-c instances approximately by a factor of 1  than for the sparse encoding. this can be explained by the fact that the compact encoding produces longer and more tightly connected clauses  in terms of number of clauses which share at least one variable with any given clause   such that each variable flip affects potentially more clauses  by breaking or fixing them . so the compact encoding induces both  a syntactic structure  the longer and more tightly connected clauses  and a semantic structure  the search space features discussed above  which reduce walksats performance by increasing the cpu-time per search step and decreasing the efficiency of these search steps. 
1 	to encode or not to encode  
the results presented in the preceeding sections suggest that when solving combinatorial problems by encoding them into sat and applying powerful sls algorithms  there might be a tradeoff between the size of the representation and sls performance. apparently compact encodings can be used to generate formulae with small search spaces; however  these have characteristic features which impede the performance of walksat and similar sat algorithms. one obvious question therefore is the following. given a formulation of a combinatorial problem as a csp  should we encode into sat at all  or rather apply sls algorithms for csp to directly solve the unencoded problem  
　here  we present some initial results on this question. specifically  we analyse the correlation between the performance of walksat and an wmch  an analogous algorithm for csp which is based on the min conflicts heuristic  minton et al  1  and has been introduced in  steinmann et a/.  1 . like walksat  wmch is a sls algorithm 
	h1s 	1 
　

table 1: easy  median  and hard problem instances from compactly and sparsely encoded csp and hcp test-sets; the table shows the size of the instances  the expected local search cost for walksat  in steps/solution  using approx. optimal noise   as well as the number of solutions  solution density  normalised sdnclu and average blmin value  for details  see text . 
　

figure 1: correlation of average local search cost per instance for walksat vs wmch applied to test-set csp1 -1; both algorithms use approx. optimal noise  walksat is used with the sparse sat-encoding. 
which iteratively repairs violated constraints following the steepest gradient  but also allows to escape from local minima of the objective function by allowing occasional  randomised uphill moves  for details  see  steinmann et al  1  . like for walksat  the local search cost  tec  for wmch was measured as the expected number of local search steps to find a solution  based on 1 runs of the algorithm for each given problem instance. generally  we always used approximately optimal noise parameter settings and a cutoff parameter high enough to guarantee that in every run a solution was found. 
　analysing the correlation between the average local search cost for walksat  using the sparse encoding  and wmch across the test-set csp1 reveals an extremely strong linear correlation between the logarithms of the average local search cost for both algorithms  c/ figure 1; correlation coefficient r = 1   le..   instances which are relatively hard for walksat also tend to be hard for wmch and vice versa. a linear regression analysis of this data shows that walksat tends to require ca. 1 times more steps than wmch for solving the problem instances from our test-set  the coefficients of the regression analysis are a = 1 and b = -1 . 
1 	challenge papers 
however  neither walksat nor wmch are the best known 
sls algorithms for sat and csp  respectively. we therefore performed the same analysis for the best-performing slsbased sat algorithm and the best sls-based csp algorithm we are aware of - novelty  mcallester et al.  1   one of the most recent variants of walksat  and the tabu search algorithm by galinier and hao  galinier and hao  1 . 1 using optimal noise parameters for both algorithms  we find that when measuring the average number of local search steps per solution  galinier's and hao's csp algorithm has generally only an advantage of a factor between 1 and 1 over novelty. the correlation between both algorithm's performance is very strong  cf. figure 1; correlation coefficient = 1 ; however  novelty occasionally gets stuck in local optima which it cannot escape  causing the outliers in the scatter plot 1 instances   while the csp algorithm shows no such behaviour. but this happens only for 1 of our 1 instances and for these only in a small number of the multiple tries we performed. we conjecture that by exending novelty with a stronger stochastic escape mechanism  this phenomenon can be eliminated. but except for these outliers  the regression analysis indicates that both algorithms show approximately the same scaling behaviour w.r.t. instance hardness. this is somewhat surprising  since galinier's and hao's algorithm and novelty are conceptually significantly less closely related than wmch and walksat. when performing an exactly analogous analysis for instances with 1 constraint variables and 1 values  we find a similar situation; only now  the performance advantage of the native csp algorithm over novelty is only a factor of ca. 1 on average  comparing the number of local search steps . also  we observe no increased occurence of the outliers mentioned above. this suggests that when increasing the problem size  the sat-based approach might have a slight scaling advantage over the native csp algorithm. 
　generally  our results from comparing the performance of sls-based algorithms for csp and sat indicate that when measuring the average number of steps per solution  the dif-
1
   the considerably more complicated r-novelty algorithm  mcallester et al  1  for sat  which shows an even better performance than novelty on random-1-sat  is inferior to novelty for the random binary csps used here. 
　

figure 1: correlation of average local search cost per instance for novelty vs galinier's and hao's tabu search algorithm for csp applied to test-set csp1; all algorithms use approx. optimal noise  novelty is used with the sparse satencoding. 
ferences are rather surprisingly small. we deliberately refrained from comparing cpu-times for these algorithms  because the implementations for the sat algorithms are significantly more optimised. the sat algorithms generally have the advantage that they are conceptually easier  which facilitates their efficient implementation  evaluation  and further development. of course  our analysis is too limited to give a conclusive answer  but the results reported here suggest that for solving hard csp instances  encoding them into sat and using a state-of-the-art sls algorithm for sat to solve the sat-encoded instances might be very competitive compared to using more specialised sls-based csp solvers. 
1 	conclusions and future work 
in this paper we presented an initial investigation of how satencoding strategies affect the topological structure of the induced search spaces and sls performance. our empirical results show that for two well-known classes of np-hard combinatorial problems  random binary csps and hcps in random directed graphs near the solubility phase transition  compact sat-encodings  which minimise the number of propositional variables for the encoded problem instances  exhibit structural features which impede local search algorithms like walksat. this observation is consistent with earlier observations for planning problems and hcps  that compact encodings are extremely hard to solve for sls algorithms. at the same time  it shows that the influence of solution density on sls performance which has been observed for csp and sat  is of minor significance compared to features like the overall ruggedness of the search space  as measured by the standard deviation of the objective function  or the local minima branching. however  consistently with earlier results  it plays an important role in explaining the differences in sls performance observed across test-sets of randomly generated instances  when using a fixed sat-encoding. as a consequence of these results  compact encodings should be avoided in the context of using sls algorithms for solving sat-encoded combinatorial problems. 
　furthermore  when comparing the performance of sls algorithms directly applied to test-sets of random binary csp instances to with the performance of sls-based algorithms for sat using the sparse encoding  we found that there is a strong correlation of the expected number of search steps required for finding a solution across the test-set. although when comparing the best known sls algorithms for csp and sat in this way  the direct csp algorithm requires fewer local search steps per solution  we feel that this might very well be outweighed by other advantages of the sat-encoding and solving approach  like the availability of extremely fast implementations or its conceptual simplicity. also  due to the restrictions of the implementatins of the sat-based csp algorithms available for this preliminary study only binary csp instances could be analysed. for more structured  nonebinary csp instance  one might encounter a different situation; although it is not clear that  a  existing csp algorithms can exploit this structure  and  b  even if they could  the same would not also be possible without too much effort for the sat-encoded problem instances. 
　it should also be noted that  according to the results reported here and elsewhere  ernst et al  1; kautz and selman  1   it seems that finding efficient sat-encodings could very well be significantly easier than developping specialised algorithms for the respective domains. we do not believe that the generic problem solving approach based on sat-encodings and extremely optimised sat-algorithms will generally be competitive with specialised algorithms which make use of domain knowledge. however  it is quite possible that it can be established as another generic method for attacking problems for which domain-specific knowledge is not  yet  available or which are too specific and limited in their application to make the development of specialised algorithms worthwhile. in this sense  the sat-encoding and -solving approach could be compared to other generic problem solving methods  like  e.g.  mixed integer programming  mip   which has become a standard generic solving technique for combinatorial problems in operations research. 
　the novel tools and techniques presented here can be easily applied to other hard combinatorial problems and satencoding schemes. out investigation can be extended in various directions; in particular  it provides a basis for studying the following research questions: 
  based on the knowledge of the search space structure induced by specific encoding schemes  can we design sat algorithms which exploit these features  
  given the knowledge about which features affect sls performance  can we devise efficient encoding strategies which improve sls performance  
  for which problem classes is the sat-encoding and -solving approach  using sls algorithms for sat  competitive with analogous  generic sls algorithms directly applied to the unencoded problem instances  
　thus  while this study does certainly not provide the final answers to the challenge or the more specific questions stated in section 1  it shows a way for systematically investigating the relation between encodings  search space structure  and sls performance which will hopefully lead to improved encodings and sat algorithms  as well as  in the long run  to a realistic assessment of the generic sat-encoding and -solving approach for solving hard combinatorial problems. 
	hoos 	1 
　
acknowledgements 
the csp instances used for the experimental part of this paper were generated using software provided by thomas stlitzle whom i also wish to thank for many interesting and stimulating discussions. furthermore  i thank bart selman for his encouraging comments on an earlier version of this paper as well as wolfgang bibel and the intellectics group at tu darmstadt for their support during a significant part of this work. finally  i gratefully acknowledge valuable input by david poole and the members of the laboratory of computational intelligence at the university of british columbia. this research was partially supported iris phase-iii project bou   preference elicitation and interactive optimization.  
