
while traditional information extraction systems have been built to answer questions about facts  subjective information extraction systems will answer questions about feelings and opinions. a crucial step towards this goal is identifying the words and phrases that express opinions in text. indeed  although much previous work has relied on the identification of opinion expressions for a variety of sentiment-based nlp tasks  none has focused directly on this important supporting task. moreover  none of the proposed methods for identification of opinion expressions has been evaluated at the task that they were designed to perform. we present an approach for identifying opinion expressions that uses conditional random fields and we evaluate the approach at the expression-level using a standard sentiment corpus. our approach achieves expression-level performance within 1% of the human interannotator agreement.
1 introduction
traditional information extraction tasks can be viewed as beginning with a set of questions about facts  such as who   where   or how many . researchers then build systems to extract the answers to these questions from text  muc1  1; nis  1; duc1  1 . more recently  researchers have investigated the construction of language-processing systems that extract answers to subjective questions  such as how does x feel about y   see section 1 . intelligence analysts  for example  could use such opinion-oriented systems to monitor trouble spots around the world while marketing researchers could use them to understand public opinion about their product.
　as with factual information extraction and question answering  subjective information extraction and question answering will require techniques to analyze text below the sentence level. for example  stoyanov et al.  show that identifying opinion expressionsis helpfulin localizing the answers to opinion questions.
　consider the following sentences  in which we denote two kinds of opinion expression in boldface and italic  described below .
1: minister vedrine criticized the white house reaction.
1: 1 persons were killed by sharpshooters faithful to the president.
1: tsvangirai said the election result was  illegitimate  and a clear case of  highway robbery .
1: criminals have been preying on korean travellers in china.
to understand  extract  or answer questions about the opinions in these sentences a system must  minimally  determine the basic attributes of the opinion: is its polarity positive  negative  or neutral  with what strength or intensity is the opinion expressed: mild  medium  strong or extreme  who or what is the source  or holder  of the opinion  what is its target  i.e. what is the opinion about  the opinion expressions marked in the sentences above are the key to answering these questions. in particular  the marked phrases denote the polarity of the opinion: for example   criticized  and  faithful to   examples 1 and 1  denote negative and positive attitudes  respectively. the opinion expressions also often provide linguistic anchors for the automatic extraction of the source and target of the opinion. the predicate  criticized   for example  organizes the semantic roles that denote the source of the opinion  the agent role =  minister vedrine   and the target of the opinion  the object/theme role =  white house reaction  . similarly  the opinion expression  faithful to  organizes the semantic roles associated with the source  the agent role =  sharpshooters   and the target  the object/theme role =  the president   of the opinion in example 1.
　wiebe et al.  distinguish two types of opinion expressions  and we follow their definitions here. direct subjective expressions  dses   shown in boldface  are spans of text that explicitly express an attitude or opinion.  criticized  and  faithful to   examples 1 and 1   for example  directly denote negative and positive attitudes towards the  white house reaction  and  the president   respectively. speech events like  said  in example 1 can be dses if they express subjectivity. in contrast  expressive subjective elements  eses   shown in
italics  are spans of text that indicate  merely by the specific choice of words  a degree of subjectivity on the part of the speaker. the phrases  illegitimate  and  highway robbery   iob	... faithful/b to/i the/o president/o ./o io	... faithful/i to/i the/o president/o ./o
figure 1: how to encode the class variable: the iob method and the io method. io is used in this paper.
for example  indirectly relay tsvangirai's negative opinion of  the election result   example 1   and the use of  preying on   instead of  say   mugging   indicates the writer's sympathy for the korean travellers in example 1.
　while some previous work identifies opinion expressions in support of sentence- or clause-level subjectivity classification  riloff and wiebe  1; wiebe and wilson  1   none has directly tackled the problem of opinion expression identification  developed methods for the task  and evaluated performance at the expression level. instead  previous work in this area focuses its evaluation on the sentence-level subjectivity classification task.
　in this work  we treat the identification of opinion expressions as a tagging task  and use conditional random fields to address this problem. for comparison  we chose two lists of clues to subjectivity from previous work  wilson et al.  1b; wiebe and riloff  1 . these clues have previously been evaluated only for their utility in clause- or sentencelevel classification. here we interpret the clues as expressions of opinion and compare them to our results. we achieve fmeasures of 1% for eses and 1% for dses  within 1% of the human interannotator agreement for dses and within 1% for eses.
　the rest of this paper proceeds as follows. in section 1  we discuss our approach to identifying direct subjective expressions and expressive subjective elements. in section 1  we present experimental results. in section 1  we present related work and in section 1  we conclude and discuss future work.
1 the approach
as the example sentences in section 1 suggest  identifying subjective expressions is a difficult task. the expressions can vary in length from one word to over twenty words. they may be verb phrases  noun phrases  or strings of words that do not correspond to any linguistic constituent. subjectivity is a realm of expression where writers get quite creative  so no short fixed list can capture all expressions of interest. also  an expression which is subjective in one context is not always subjective in another context wiebe and wilson  1 . therefore we present in this section our machine learning approach for the identification of direct subjective expressions and expressive-subjectiveelements. we treat the task as a tagging problem  and use conditional random fields  lafferty et al.  1. below we discuss the encoding of the class variable  section 1   the features  section 1   and the learning method  section 1 .
1 the class variable
a common encoding for extent-identification tasks such as named entity recognition is the so-called iob encoding. using iob  each word is tagged as either beginning an entity  being in an entity  i.e. an opinion expression   or being outside of an entity  see figure 1 . while we initially used this encoding  preliminary investigation on separate development data revealed that a simpler binary encoding produces better results. we suspect this is because it is rarely the case in our data that two entities are adjacent  and so the simpler model is easier to fit. thus  we tag each token as being either in an entity or outside of an entity. when predicting  a sequence of consecutive tokens tagged as in constitutes a single predicted entity.
1 features
in this section  we describe the features used in our model. we include features to allow the model to learn at various levels of generality. we include lexical features to capture specific phrases  local syntactic features to learn syntactic context  and dictionary-based features to capture both more general patterns and expressions already known to be opinionrelated. the same feature set is used for identifying both types of subjective expression. for pedagogical reasons  we present the features as categorically valued  but in our model we encode all features in binary  i.e. a feature  f v  is 1 for a token t if f t  = v  and 1 otherwise .
lexicalfeatures we include featureslexi  defined to be the word at position i relative to the current token. we include lex 1 lex 1 ...lex1. these are encoded into about 1 binary features per position  i.e. the vocabulary size .
syntacticfeatures we includea feature part-of-speech defined to be the part of speech of the current token according to the gate part-of-speechtagger  cunningham et al.  1   encoded into 1 binary features . we also include three features prev  cur  and next  defined to be the previous  current  or following constituent type  respectively  according to the cass partial parser  abney  1. these are encoded into about 1 binary features each.
dictionary-based features we include features from four sources. we include a feature wordnet  defined to be all synsets which are hypernyms of the current token in the wordnet hierarchy  miller  1 . this is encoded into 1 binary features  many of which may be 1 for a given token. we include a feature levin  defined to be the section of levin's  categorization of english verbs in which a given verb appears  and a feature framenet  defined to be the category of a word in the categorization of nouns and verbs in framenet1. finally  we include a feature that specifically targets subjective expressions. wilson et al.  1b  identify

1
cass is available for download at http://www.vinartus.
net/spa/. 1
http://www.icsi.berkeley.edu/ framenet/
number of sentences1number of dses1number of eses1average length of dses1 wordsaverage length of eses1 wordstable 1: statistics for test data
a set of clues as being either strong or weak cues to the subjectivity of a clause or sentence. we identify any sequence of tokens included on this list  and then define a feature wilson that returns the value '-' if the current token is not in any recognized clue  or strong or weak if the current token is in a recognized clue of that strength. this clue is encoded into two binary features  the '-' case is not encoded .
1 the learning method
we chose to use a linear-chain conditional random field model for all of our experiments  using the mallet toolkit  mccallum  1 . this discriminatively-trained sequence model has been found to perform extremely well on tagging tasks such as ours  lafferty et al.  1 . based on pilot experiments on development data  we chose a gaussian prior of 1.
1 experiments
in this section  we describe the data and evaluations used in our experiments  describe the baselines we compare to  and present our results.
1 data
the mpqa corpus  wiebe et al.  1 consists of 1 newswire documents annotated with a variety of annotations of interest for subjectivity research. in particular  all dses and eses in the documents have been manually identified. in this work  we used 1 documents for development of our features and determination of parameters  and kept the remaining 1 blind for evaluation. all of our evaluation results use 1-fold cross-validation on the 1 documents. table 1 presents some statistics on these 1 documents.
1 evaluation
as with other information extraction tasks  we use precision  recall and f-measure to evaluate our method's performance. precision is |c|p”p| | and recall is |c|c”p| |  where c and p are the sets of correct and predicted expression spans  respectively. f is the harmonic mean of precision and recall  p1pr+r. our method often identifies expressions that are close to  but not precisely the same as  the manually identified expressions. for example  for the expression  roundly criticized   our method might only identify  criticized.  we therefore introduce softened variants of precision and recall as follows. we define soft precision as and soft recall as
  where a c p  is a
predicate true just when expression c  aligns  to expression p in a sense defined by a. we report results according to two predicates: exact and overlap. exact c p  is true just when c and p are the same spans - this yields the usual notions of precision and recall. a softer notion is produced by the predicate overlap c p   which is true when the spans of c and p overlap1.
1 baselines
for baselines  we compare to two dictionaries of subjectivity clues identified by previous work  wilson et al.  1b; wiebe and riloff  1 . these clues were collected to help recognize subjectivity at the sentence or clause level  not at the expression level  but the clues often correspond to subjective expressions. each clue is one to six consecutive tokens  possibly allowing for a gap  and matching either stemmed or unstemmed tokens  possibly of a fixed part of speech. in the following experiments  we report results of the wiebe baseline  which predicts any sequence of tokens matching a clue from wiebe and riloff  to be a subjective expression  and the wilson baseline  using similar predictions based on clues from wilson et al.  1b . when predicting dses  we remove all clues from the list which never match a dse in the test data  to make the baseline's precision as high as possible  although since many potentially subjective expressions are often not subjective  the precision is still quite low . we similarly trim the lists when predicting the other targets below. apart from this trimming  the lists were not derived from the mpqa corpus. note that the higher-performing of these two baselines  from wilson et al.  1b   was incorporated into the feature set used in our crf models1.
1 results
tables 1 and 1 present experimental results on identifying direct subjective expressions and expressive subjective elements in several settings  as well as presenting the two baselines for comparison purposes. we experiment with two variants of conditional random fields  one with potentials  features  for markov order 1  similar to the features in a hidden markov model  labeled crf-1 in the tables   and one with features only for order 1  equivalent to a maximum entropy

overlapexactmethodrecallprecisionfrecallprecisionfwiebe baseline1.1.11.1.11.1.1wilson baseline1.1.11.1.11.1.1crf-1-dse1.1.11.1.11.1.1crf-1-dse&ese1.1.11.1.11.1.1crf-1-dse1.1.11.1.11.1.1crf-1-dse&ese1.1.11.1.11.1.1table 1: results for identifying direct subjective expressions. superscripts designate one standard deviation.
overlapexactmethodrecallprecisionfrecallprecisionfwiebe baseline1.1.11.1.11.1.1wilson baseline1.1.11.1.11.1.1crf-1-ese1.1.11.1.11.1.1crf-1-dse&ese1.1.11.1.11.1.1crf-1-ese1.1.11.1.11.1.1crf-1-dse&ese1.1.11.1.11.1.1table 1: results for identifying expressive subjective elements. superscripts designate one standard deviation.
overlapexactmethodrecallprecisionfrecallprecisionfwiebe baseline1.1.11.1.11.1.1wilson baseline1.1.11.1.11.1.1crf-1-dse+ese1.1.11.1.11.1.1crf-1-dse&ese1.1.11.1.11.1.1crf-1-dse+ese1.1.11.1.11.1.1crf-1-dse&ese1.1.11.1.11.1.1table 1: results for identifying expressions which are either dses or eses. superscripts designate one standard deviation. dse&ese indicates a model trained to make a three-way distinction among dses  eses  and other tokens  while dse+ese indicates a model trained to make a two-way distinction between dses or eses and all other tokens.
overlapexactfeature setrecallprecisionfrecallprecisionfbase1.1.11.1.11.1.1base + levin/fn1.1.11.1.11.1.1base + wilson1.1.11.1.11.1.1base + wilson + levin/fn1.1.11.1.11.1.1base + wordnet1.1.11.1.11.1.1base + wilson + wordnet1.1.11.1.11.1.1base + levin/fn + wordnet1.1.11.1.11.1.1base + levin/fn + wordnet + wilson1.1.11.1.11.1.1table 1: results for feature ablation for identifying dses. fn is the framenet dictionary features.  base  indicates the lexical features and the syntactic features. the bottom line represents the same model as crf-1-dse&ese in table 1.

model  labeled crf-1 in the tables . orthogonally  we compare models trained separately on each task  classifying each token as in a dse versus not or in an ese versus not  labeled just dse or ese in the tables  to models trained to do both tasks at once  classifying each token into one of three classes: in a dse  in an ese  or neither1  labeled dse&ese in the tables .
　because the baselines were not designed to distinguish between dses and eses  we run another set of experiments where the two categories are lumped together. the rows labeled dse&ese use the models trained previously to distinguish three categories  but are here evaluated only on the binary decision of opinion expression or not. the rows labeled dse+ese are trained to classify a token as i if it is in either a dse or ese  or o otherwise. the results of these experiments are reported in table 1.
　finally  to determine the effect of the various dictionaries  we examine all combinations of the various dictionaries wordnet  framenet  levin  and the clues from wilson et al.  1b   to save space  we combine the two smallest dictionaries  framenet and levin  into one . these results  on the dse task  are reported in table 1.
1 discussion
we note that the order-1 models outperform the order-1 models according to overlap f-measure and recall  but by exact f-measure and either precision metric  the order-1 models are superior. the creators of the dataset state  we did not attempt to define rules for boundary agreement in the annotation instructions  nor was boundary agreement stressed during training.   wiebe et al.  1  page 1 . for example  whether a dse ought to be annotated as  firmly said  or just  said  is left up to the annotator. therefore  we hypothesize that the model with greater capacity  the order 1  may overfit to the somewhat inconsistent training data.
　the ablation results  in table 1  indicate that the wordnet features are by far the most helpful. the other two dictionary sets are individually useful  with the wilson features being more useful than the levin/framenet ones   but above the wordnet features the others make only a small difference. this is interesting  especially since the wordnet dictionary is entirely general  and the wilson dictionary was built specifically for the task of recognizing subjectivity. ablation tables for the other two targets  eses and dse&ese  look similar and are omitted.
　in looking at errors on the development data  we found several causes which we could potentially fix to yield higher performance. the category of dses includes speech events like  said  or  a statement   but not all occurrences of speech events are dses  since some are simply statements of objective fact. adding features to help the model make this distinction should help performance. also  as others have observed  expressions of subjectivity tend to cluster  so incorporating features based on the density of expressions might help as well  wiebe and wilson  1 .
　finally  we note that the interannotator agreement results for these tasks are relatively low; 1 for dses and 1 for eses  according to a metric very close to overlap f-measure1. our results are thus quite close to the human performance level for this task.
1 related work
subjectivity research has been quite popular in recent years. while ultimately research in lexicon building  hatzivassiloglou and mckeown  1  e.g.   and classification  dave et al.  1  e.g.  may be relevant  we focus here on work in extracting sub-sentential structure relevant to subjectivity.
　bethard et al.  address the task of extracting propositional opinions and their holders. they define an opinion as  a sentence  or part of a sentence that would answer the question 'how does x feel about y '   a propositional opinion is an opinion  localized in the propositional argument  of certain verbs  such as  believe  or  realize . their task then corresponds to identifying a dse  its associated direct source  and the content of the private state. however  in each sentence  they seek only a single verb with a propositional argument  whereas we may identify multiple dses per sentence  which may be multi-word expressions of a variety of parts of speech.
　another group of related work looks at identifying a class of expressions similar to the dses we identify  wiebe et al.  1; munson et al.  1; wilson et al.  1a  1. we cannot compare our results to theirs because this previous work does not distinguish between dses and objective speech expressions  and because the prior results only address finding single word expressions.
　another area of related work is reputation or sentiment analysis  morinaga et al.  1; nasukawa and yi  1 . this work is in the context of marketing research  and involves identifying polarity and sentiment terminology with respect to particular products or product categories. their notions of sentiment terms are related  but not identical  to ours. however  they do provide evidence that working at the expression level is of interest to consumers of opinion-oriented information extraction.
1 conclusions and future work
extracting information about subjectivity is an area of great interest to a variety of public and private interests. we have argued that successfully pursuing this research will require the same expression-level identification as in factual information extraction. our method is the first to directly approach the task of extracting these expressions  achievingperformance within 1% of human interannotator agreement. in the future  we hope to improve performance even further by the methods discussed earlier  and build on our expressionlevel identification towards systems that present the user with a comprehensive view of the opinions expressed in text.
acknowledgements
the authors would like to thank oren kurland  alexandru niculescu-mizil  filip radlinski  and theresa wilson for helpful comments on earlier versions of this paper.
