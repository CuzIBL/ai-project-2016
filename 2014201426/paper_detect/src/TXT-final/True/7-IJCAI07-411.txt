
current approaches to solving markov decision processes  mdps  are sensitive to the size of the mdp. when applied to real world problems though  mdps exhibit considerable implicit redundancy  especially in the form of symmetries. existing model minimization methods do not exploit this redundancy due to symmetries well. in this work  given such symmetries  we present a time-efficient algorithm to construct a functionally equivalent reduced model of the mdp. further  we present a real time dynamic programming  rtdp  algorithm which obviates an explicit construction of the reduced model by integrating the given symmetries into it. the rtdp algorithm solves the reduced model  while working with parameters of the original model and the given symmetries. as rtdp uses its experience to determine which states to backup  it focuses on parts of the reduced state set that are most relevant. this results in significantly faster learning and a reduced overall execution time. the algorithms proposedare particularly effective in the case of structured automorphisms even when the reduced model does not have fewer features. we demonstrate the results empirically on several domains.
1 introduction
markov decision processes  mdps  are a popular way to model stochastic sequential decision problems. but most modeling and solution approaches to mdps scale poorly with the size of the problem. real world problems often tend to be very large and hence do not yield readily to the current solution techniques. however  models of real world problems exhibit redundancy that can be eliminated  reducing the size of the problem.
one way of handling redundancy is to form abstractions  as we humans do  by ignoring details not needed for performing the immediate task at hand. researchers in artificial intelligence and machine learning have long recognized the importance of abstracting away redundancy for operating in complex and real-world domains  amarel  1 . given a model  finding a functionally equivalent smaller model using this approach forms the crux of the model minimization paradigm. identifying symmetrically equivalent situations frequently results in useful abstraction. informally  a symmetric system is one which is invariant under certain transformations onto itself. an obvious class of symmetries is based on geometric transformations such as  rotations  reflections and translations. existing work on model minimization of mdps such as  givan et al.  1  and  zinkevich and balch  1  do not handle symmetries well. they either fail to consider stateaction equivalence or do not provide specific algorithms to minimize an mdp  considering state-action equivalence.
¡¡in this article we consider a notion of symmetries  in the form of symmetry groups  as formalized in  ravindran and barto  1 . our objective here is to present algorithms that provide a way of using the symmetry information to solve mdps  thereby achieving enormous gains over normal solution approaches. first  we present a time-efficient algorithm  g-reduced image algorithm  to construct a reduced model given the symmetry group. the reduced model obtained is functionally equivalent to the original model in that  it preserves the dynamics of the original model. hence a solution in the reduced model will lead to a solution in the original model. however  the reduced model can be significantly smaller when compared to the original model depending on the amount of symmetry information supplied. thus  solving the reduced model can be a lot easier and faster. further  we identify that an explicit construction of a reduced model is not essential for the use of symmetry information in solving the mdp. we use the g-reduced image algorithm as a basis to present the reduced real time dynamic programming rtdp  algorithm that integrates the symmetry information into the rtdp algorithm  barto et al.  1  used for solving mdps. though the algorithm works directly with the original model it considers only a portion of the original model that does not exhibit redundancy and also is relevant in achieving its goals. this focus on the relevance of states results in significantly faster learning leading to huge savings in overall execution time. to make the algorithms more effective  especially in terms of space  we advocate the use of certain structural assumptions about mdps. we use several domains to demonstrate the improvement obtained by using the reduced rtdp algorithm.
after introducing some notation and background information in sec. 1  we present the g-reduced image algorithm in sec. 1. we then present the reduced rtdp algorithm in sec. 1. the experiments done and results achieved are presented in sec. 1. finally we conclude the article by giving some directions for future work in sec. 1.
1 notation and background
1 markov decision processes
a markov decision process is a tuple s a ¦· p r  where s = {1 ... n} is a set of states  a is a finite set of actions  ¦·   s ¡Á a is the set of admissible state-action pairs  p : ¦· ¡Á s ¡ú  1  is the transition probability function with p s a s  being the probability of transition from state s to state s under action a  and r : ¦· ¡ú r is the expected reward function  with r s a  being the expected reward for performing action a in state s. let as = {a| s a  ¡Ê ¦·}   a denote the set actions admissible in state s. we assume that  s ¡Ê s as is non-empty.
¡¡a stochastic policy ¦Ð is a mapping ¦· ¡ú  1   s.t.  a¡Êas ¦Ð s a  = 1   s ¡Ê s. the value of a state s under policy ¦Ð is the expected value of the discounted sum of future rewards starting from state s and following policy ¦Ð thereafter. the value function v¦Ð corresponding to a policy ¦Ð is the mapping from states to their values under ¦Ð. it can be shown that v¦Ð satisfies the bellman equation:
	v¦Ð s  =  ¦Ð s a r s a  + ¦Ã	p s a s v¦Ð s 	 1 
¡¡¡¡¡¡¡¡¡¡¡¡¡¡a¡Êas	s ¡Ês where 1 ¡Ü ¦Ã   1 is a discount factor.
¡¡the solution of an mdp is an optimal policy ¦Ð  that uniformly dominates all other policies for that mdp. in other words v¦Ð   s  ¡Ý v¦Ð s  for all s ¡Ê s and for all ¦Ð.
1 factored markov decision processes
factored mdps are a popular way to model structure in mdps. a factored mdp is defined as a tuple s . the state set  given by m features or variable  s  si  where si is the set of permissible values for the feature i  a is a finite set of actions  ¦·   s ¡Á a is the set of admissible state-action pairs. the transition probabilities p are often described by a two-slice temporal bayesian network  1-tbn . the state transition probabilities can be factored as:
m
	p s a s  = prob si|pre si a  	 1 
i=1
where pre si a  denotes the parents of node si in the 1tbn corresponding to action a and each of the probabilities associated with nodeprob si|pre si a   is given by a conditional probabilitiy tablesi. the reward function may be similarly represented.
1 homomorphisms and symmetry groups
this section has been adapted from  ravindran and barto  1 .
¡¡let b be a partition of a set x. for any x ¡Ê x  x b denotes the block of b to which x belongs. any function f from a set x to a set y induces a partition  or equivalencerelation  on x  with f if and only if f x  = f x  and x x are fequivalent written x ¡Ôf x. let b be a partition of z   x ¡Á y  where x and y are arbitrary sets. the projection of b onto x is the partition b|x of x such that for any x x ¡Ê x  x b|x =  x b|x if and only if every block containing a pair in which x is a componentalso contains a pair in which x is a component or every block containing a pair in which x is a component also contains a pair in which x is a component.
definition 1. an mdp homomorphism h from an mdp m = s a ¦· p r to an mdp m = s a ¦· p r is a surjection from ¦· to ¦·  defined by a tuple of surjections  f  {gs|s ¡Ê s}  with h  s a   =  f s   gs a    where f : s ¡ú s and gs : as ¡ú af s  for s ¡Ê s  such that:  s s ¡Ê s a ¡Ê as
	p f s   gs a   f s   =  p s a s 	 1 
sf
	r f s   gs a   = r s a 	 1 
we use the shorthand h s a  for h  s a  .
definition 1. an mdp homomorphism h =  f  {gs|s ¡Ê s} from mdp m = s a ¦· p r to mdp m =
s a ¦· p r is an mdp isomorphism from m to m if and only if f and gs  are bijective. m is said to be isomorphic to m and vice versa. an mdp isomorphism from mdp m to itself is call an automorphism of m.
definition 1. the set of all automorphisms of an mdp m  denoted by autm  forms a group under composition of homomorphisms. this group is the symmetry group of m.
¡¡let g be a subgroup of autm. the subgroup g induces a partition bg of ¦·:   s1 a1  bg =   s1 a1  bg if and only if there exists h ¡Ê g such that h s1 a1  =  s1 a1  and  s1 a1   s1 a1  are said to be g equivalent written  s1 a1  ¡Ôg  s1 a1 . further if s1 ¡Ôbg|s s1 then we write as shorthand s1 ¡Ôg|s s1. it can be proved that there exists a homomorphism hg from m to some m  such that the partition induced by hg  bhg   is the same bg. the image of m under hg is called the g-reduced image of m.
¡¡adding structure to the state space representation allows us to consider morphisms that are structured  e.g.  projection homomorphisms  see sec. 1 of  ravindran and barto  1  . it can be shown that symmetry groups do not result in projection homomorphisms  except in a few degenerate cases. another simple class of structured morphisms that do lead to useful symmetry groups are those generated by permutations of feature values. let m be the set of all possible permutations of {1 ... m}. given a structured set x  xi and a permutationm  we can define a permutation on x by ¦Ò x1 ... xm  = x¦Ò 1  ... x¦Ò m   and it is a valid permutation on x if x¦Ò i  ¡Ê xi for all i and for all x1 ... xm.
definition 1. a permutation automorphism h on a structured mdp m = s a ¦· p r is a bijection on ¦·   defined by a tuple of bijections  f s   gs a   with h  s a   =  f s   gs a    where f ¡Ê m : s ¡ú s is a valid permutation on s and gs : as ¡ú af s  for s ¡Ê s  such that:	s s	s a	as
p f s   gs a   f s   = p s a s 
m
= prob sf i | f pref s  sf i  a     1 
i=1
	r f s   gs a   = r s a 	 1 
¡¡here f pref with sf j  assigned according to f s .
1 g-reduced image algorithm
1 motivation
in a large family of tasks  the symmetry groups are known beforehand or can be specified by the designer through a superficial examination of the problem. a straight forward approach to minimization using symmetry groupswould require us to enumerate all the state-action pairs of the mdp. even when the symmetry group  g  is given  constructing the reduced mdp by explicit enumeration takes time proportional to |¦·|.|g|.
¡¡we present in fig. 1  an efficient incremental algorithm for building the reduced mdp given a symmetry group or subgroup. this is an adaptation of an algorithm proposed by  emerson and sistla  1  for constructing reduced models for concurrent systems.
1 given m = s a ¦· p r and g ¡Ü autm  1 construct m/bg = s a ¦· p r.
1 set q to some initial state {s1} s ¡û {s1}
1 while q is non-empty
1 s = dequeue{q}
1 for all a ¡Ê as
1 if  for some  s a  ¡Ê ¦·  then
1 ¦· ¡û 
1 a ¡û a ¡È a
1 r s a  = r s a 
1 for all t ¡Ê s such that p s a t    1
1 if t ¡Ôg|s s  for some s ¡Ê s 
1 p s a s  ¡û p s a s  + p s a t 
1 else
1 s ¡û s ¡È t
1 p s a t  = p s a t 
1 add t to q.
figure 1: incremental algorithm for constructing the greduced image given mdp m and some g ¡Üautm. q is the queue of states to be examined. terminates when at least one representative from each equivalence class of g has been examined.
1 comments
the algorithm does a breadth-first enumeration of states skipping states and state-action pairs that are equivalent to those already visited. on encounteringa state-action pair not equivalent to one already visited  it examines the states reachable from it to compute the image mdp parameters. the algorithm terminates when at least one representative from each equivalence class of g has been examined. for a proof that the transition probabilities actually represent those for the reduced image  see app. a. the algorithm as presented assumes that all states are reachable from the initial state. it is easy  however  to modify the algorithm suitably. assuming an explicit representation for the symmetry group and that table look-up takes constant time  the algorithm will run in time proportional to |¦·|.|g|. however an explicit representation of g demands exorbitant memory of the order of |g|.|¦·|.
¡¡as discussed in sec. 1  structured morphisms can be used advantageously to reduce the state space. the advantage here is that the morphisms forming the symmetry group need not be stored explicitly as they are defined on the features instead of states.
¡¡for example  let us consider the case of permutation automorphisms. to check whether  s a  ¡Ôg  s a   we need to generate |g| states that are equivalent to  s a  by applying each h ¡Ê g. each application of h incurs a time linear in the number of features. thus in this case the time complexity of the algorithm presented is of the order of |¦·|.|g|.m  where m is the number of features whereas no space is needed for storing the g explicitly.
¡¡thus by restricting the class of automorphisms to functions that are defined on features instead of states  we only incur additional time which is a function of the number of features  significantly less than the number of states  along with a drastic decrease in the space complexity. the use of factored representations leads to further reduction in space needed for storing the transition probabilities and the reward function  thereby making the algorithm presented more effective than its use in the generic case. also as g is just a subgroup  the algorithm can work with whatever little symmetry information the designer might have.
1 reduced rtdp algorithm
1 motivation
given a real world problem modeled as an mdp  the state space invariably consists of vast regions which are not relevant in achieving the goals. the minimization approach leads to a certain degree of abstraction which reduces the extent of such regions. nonetheless the reduced image still contains regions which are not relevant in achieving the goals even in the reduced model. since our goal here is only to find a policy for acting in the original model  we can forgo the explicit construction of the reduced model by integrating the information in the symmetry group into the algorithm which solves the original model. though there are a variety of ways to solve an mdp  we choose to take up rtdp as it uses the experience of the agent to focus on the relevant sections of the state space. this saves the time spent on explicit construction of the reduced model.
¡¡also the g-reduced image algorithm as presented doesn't preserve any structure in the transition probabilities or the reward function that might have existed because of the use of factored representations. consequently the reduced image might take considerably more space than the original model. 1 given m = s a ¦· p r and g ¡Ü autm 
1 hashtable q ¡û nil is the action value function.
1 repeat  for each episode 
1 initialize s and s ¡û {s}
1 choose a from s using policy derived from q  e.g.
-greedy policy 
1 repeat  for each step in the episode 
1 if  s a  ¡Ôg  s a  for some  s a  ¡Ê q where 
1 s ¡û s;a ¡û a 1	continue.
1 take action a and observe reward r and next state s
1 choose a from s using policy derived from q
 e.g.-greedy policy 
1 for all t such that p s a t    1 if t ¡Ôg|s s  for some s 
1 p s a s  ¡û p s a s  + p s a t 
1 else
1 s ¡û s ¡È t
1 p s a t  = p s a t 
1 if  s a   q 1 add  s a  to q. 1 q s a  ¡û 1
1 q s a  ¡û r s a 
	+ 	¦Ã.p s a s . max	q s a 
a ¡Êas
s ¡Ês
1 s ¡û s;a ¡û a
figure 1: rtdp algorithm with integrated symmetries which computes the action value function for the reduced mdp without explicitly constructing it.
the algorithm we present in fig. 1 tries to achieve the best of both worlds as it not only works with the original model but also includes the state space reduction by integrating the symmetry group information into the rtdp algorithm.
1 convergence of reduced rtdp
the algorithm is a modification of the rtdp algorithm with steps from the previous algorithm integrated into lines 1 to 1. if we assume that we have the reduced mdp m  then leaving out lines 1 to 1 and lines 1 to 1 leaves us with the normal rtdp algorithm being run on the reduced image since as explained below  for all  s a  ¡Ê ¦· r s a  = r s a . due to the equivalence tests done at lines 1 and 1  the algorithm maintains a policy for and considers only the reduced state space. from app. a  lines 1 to 1 compute the transition probabilities for the reduced image. from eqn. 1  r s a  is the expected reward under the reduced image. so for all  s a  ¡Ê ¦· r s a  = r s a . thus the update equation in line 1 can be rewritten as 
q s a  = r s a  + 	¦Ã.p s a s . max	q s a 	 1 
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡a ¡Êas s ¡Ês which is nothing but the update equation for the reduced image. thus it is exactly similar to running normal rtdp on the reduced image. as normal rtdp converges to an optimal action value function  barto et al.  1   the reduced rtdp also converges  as long as it continues to back up all states in the reduced image.
¡¡the complete construction of the reduced image can take up a considerable amount of time mapping all the irrelevant states into the reduced model whereas with the use of this algorithm one can get near optimal policies even before the construction of a reduced image is complete. it is also faster than the normal rtdp algorithm as its state space is reduced by the use of the symmetry group information.
1 experiments and results
experiments were done on three domains that are explained below. to show the effect of the degree of symmetry considered in the domain we consider a 1-fold symmetry for which g is a strict subgroup of autm  that is  g   autm and full symmetry g = autm. we compare the reduced rtdp algorithm using these 1 degrees of symmetry with the normal rtdp algorithm. we present learning curves representing the decrease in the number of steps taken to finish each episode. to show the time efficiency of the reduced rtdp algorithm we present a bar chart of times taken by the reduced rtdp algorithm using 1 degrees of symmetry and the normal rtdp algorithm for completing 1 episodes of each domain. all the algorithms used a discount factor  ¦Ã = 1. an epsilon greedy policy with  = 1 was used to choose the actions at each step. due to lack of space we present one graph per domain though experiments were done with different sizes of each domain. the results are similar in other cases. we note exceptions if any as is relevant.
1 deterministic grid-world dgw 
two grid-worlds of sizes 1 and 1 with four deterministic actions of going up  down  right and left were implemented. the initial state was  1  and the goal states were { 1   1 }and { 1   1 }respectively. for the 1-fold symmetry  states about ne-sw diagonal  i.e.   x y  and  y x  were equivalent. if the grid is of size m ¡Á n then let maxx = m   1 and maxy = n   1. for the full symmetry case  states  x y    y x    maxx-x maxy-y  and  maxyy maxx-x  were equivalent. state action equivalence was defined accordingly.
1 probabilistic grid-world pgw 
two grid-worlds of sizes 1 and 1 with four actions of going up  down  right and left were implemented. unlike the deterministic domain  here actions led to the relevant grid only with a probability of 1 and left the state unchanged with a probability of 1. the initial state was  1  and the goal states were { 1   1 } and { 1   1 } respectively. for the 1-fold symmetry  states about ne-sw diagonal  i.e.   x y  and  y x  were equivalent. for the full symmetry case  states  x y    y x    maxx-x maxy-y  and  maxyy maxx-x  were equivalent. state action equivalence was defined accordingly.

figure 1: learning curves for the deterministic grid world 1 grid .

figure 1: learning curves for the probabilistic grid world 1 grid .
1 probabilistic towers of hanoi ptoh 
the towers of hanoi domain as implemented had 1 pegs. two domains  one with 1 and the other with 1 disks were implemented. actions that allowed the transfer of a smaller disk onto a larger disk or to an empty peg were permitted. the actions did the transfer of disk with a probability of 1 and left the state unchanged with a probability of 1. the initial state in the case of 1 disks was { 1    1     } and { 1    1    1 } in the 1 disk case. the goal states were designed to allow various degrees of symmetry. for a 1-fold symmetry  the goal states considered were states where  all disks were either on peg 1 or 1. equivalent states were those that have disk positions of pegs 1 and 1 interchanged. for the full symmetry case  the goal states considered were states where  all disks were on any one peg. equivalent states were those that have disk positions interchanged by any possible permutation of the pegs. state action equivalencewas defined accordingly.
1 time efficiency
the bar-graph in fig. 1 shows the running times  scaled to even the graph  of the normal rtdp  reduced rtdp with a 1-fold symmetry and reduced rtdp with full symmetry. the first cluster is on the deterministic grid-world domain with a 1 grid  the second cluster is on probabilistic grid-world with a 1 grid and the third on probabilistic towers of

figure 1: learning curves for the probabilistic towers of hanoi 1 disks .
hanoi with 1 disks.1

figure 1: comparison of running times scaled .
1 discussion
the results are as expected. the comparisons show that the reduced rtdp algorithm learns faster than the normal rtdp both in the full symmetry case as well as in the 1-fold symmetry case. further among the reduced rtdp algorithms  the one using more symmetry is better than the one with lesser symmetry. the same is reflected in the running times of algorithms. the full symmetry case is at least 1 times faster than the normal rtdp. the 1-fold symmetry is also faster than the normal rtdp.
¡¡one observation contrary to the graph shown in the bar graph of fig. 1 is that when reduced rtdp algorithms are used for very small domains like 1-disk towers of hanoi  the overhead involved in checking equivalence of states outweighs the benefit from the reduction due to symmetry. though we have not been able to quantify the exact extent of the trade-offs involved  we feel that when the expected length of a trajectory to the goal state from the initial state is small in comparison with the state space  the benefits obtained by using the symmetry group information are masked by the overhead involved in doing the equivalence comparisons. however  this is true only in case of very small domains. in any domain of reasonablesize the agent implementing normal rtdp has to explore vast spaces before arriving at the correct trajectory. but  for an agent implementing reduced rtdp  the symmetry restricts exploration to a smaller space. also  greater the symmetry used  lesser the space that has to be explored. this explains the better performance of the reduced rtdp algorithm.
1 conclusions and future work
the algorithms presented in this article provide an efficient way of exploiting varying amounts of symmetry present in a domain resulting in faster learning and reduced execution times. with the use of structured morphisms on factored representations the algorithms are even more effective  especially in terms of space.
¡¡the notion of equivalence used here is very strict. one direction for future work  that we perceive  is to include notions of approximate equivalence. another possibility will be to quantify the exact trade-offs involved due to overheads of checking equivalence and the performance gained by the use of symmetries. as we assume that the symmetry group information is input to the algorithm  another direction to proceed will be to attempt symmetry detection  which has been discussed in  puget  1a  and  puget  1b .
a transition probabilities computed for the reduced model
let m = s a ¦· p r be an mdp and g the given symmetry group. let bg be the partition induced by g. let m/bg = s a ¦· p r be the reduced image. let ¦Ñ s a  denote the set of states reachable from state s by doing

action a. let bg|s = { s bg|s |  s bg|s ¡É ¦Ñ s a  =  }. when  s a  is used with p  they represent blocks whereas when used with p   s a  ¡Ê s and are representatives for  s a bg
from def. 1 the transition probabilities  p  satisfy 
 bg|s
	p s a  s bg|s  =  p s a s 	 1 
sbg|s

by the definition of bg|s 
 bg|s
	p s a  s bg|s  = 1	 1 
as s	g	p s a s  = 1
 bg|s   bg|s 
	p s a  s bg|s  =		p s a s 	 1 
	s	bg|s
as bg|s is a partition of s   t ¡Ê ¦Ñ s a   there exists exactly

one bg|s ¡Ê bg|s   bg|s	such that t.
hence eqn. 1 can be rewritten as

	p s a  t bg|s  =		p s a s 	 1 
	s	g
it is evident that lines 1 to 1 of fig. 1 implement eqn. 1 and eqn. 1.
