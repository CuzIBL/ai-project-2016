 
when dealing with narrative texts  a system must possess a strong domain theory  and especially knowledge about situations occurring in the world. otherwise the system must envisage comprehension as a complex process including learning from the texts themselves to improve its capabilities. this requires managing past solutions and completing them when analoguous situations happen in other texts in order to create general situations. we propose a system  mlk  memorization for learning 
knowledge   that organizes specific situations in an episodic memory by aggregating the similar ones in a single unit. this aggregation process leads to a progressive enrichment and generalization of the overall situations and of their specific features. mlk is a 
system conceived to allow the emergence of structures  their accessing being realized by a propagation process. therefore  with mlk  we are able to address the problem of understanding and learning even when a domain theory is lacking. 
1 introduction 
modelling and using the experience of a system is central for text understanding as the machine does not possess all the necessary background knowledge. so if a system can take advantage of the information present in texts and reuse it  it will be able to improve itself. psychological studies  vygotsky  1; bartlett  1  show that  when experiences are memorized  they are grouped according to resemblance criteria to form a unit  and then abstract classes emerge progressively. the possibility of using known situations related to a new one for understanding it  allows a human to complete his own knowledge about this situation and to improve his capacities of understanding. a system applying such a method is permanently evolving and as a result  a same story may be analyzed in different ways at two different times. we are mostly interested in pragmatic knowledge about concrete situations occurring in the world  traditionally represented by schemata. this kind of knowledge is necessary to integrate sentences in a context and to infer chains of causal relations between events. 
　many studies have been carried out on memorizing  understanding and learning as components of a whole 
1 	cognitive modeling 
brigitte grau 
limsi-cnrs and iie-cnam 
bp 1 
1 orsay cedex france 
process. these systems learn new specialized situations  swale  schank and leake  1  and aqua  ram  1  by explanation based learning  ebl  and occam  pazzani  1  by integrating empirical learning and ebl. a common point in these systems is the presence of general knowledge to guide the explanation process. 
　in contrast  our goal is to build a system able to work even when lacking general pragmatic knowledge about concrete situations. in this context  text analysis relies on linguistic clues  such as causal and temporal information  and on other known similar situations if they exist. these weak methods do not lead to a causal explanation of all the events of a text  and thus their pertinence is not fully justified. the presence of an event in two particular cases is not sufficient to conclude if it belongs to a possible general situation. only the conjunction of its recurrence and the presence of causal clues in different texts will make its abstraction possible. furthermore  all the events describing a situation are not specified in a text  but those which are relevant to the story. that is why a learning process must recognize similar situations to complete them text after text and not just memorize these situations independently of each other. 
　to allow the emergence of pertinent features from all similar situations  our system  mlk  uses incremental learning. when memorizing  mlk groups similar situations together to form an aggregated structure  in which similar features are reinforced while rare events become weaker. this aggregation process leads to a progressive enrichment and generalization of the overall situations and of their specific features. this kind of memorization is conceived to build structures on which abstraction of general units will be based and to provide an adaptative retrieval process by propagating activation. the retrieval of situations analoguous to a new one is always dependent on the state of the memory. this approach has several advantages related to the management and the retrieval of similar cases and provides a more psychologically-plausible model of memorization and learning when general background knowledge is lacking. 
1 the mlk system 
mlk is initially given a semantic representation of sentences organized in situations. the segmentation is currently done by hand. we use linguistic criteria  causal clues  order of sentences and temporal markers  plus the type of the predicates in each clause. situations are named thematic 

units  tus  because they group the specific knowledge about a topic. representations of texts form episodes: a set of tus related by thematic links  such as topic shift  another subject  and topic deviation  a particular point . the episodes define more global contexts containing specific situations. a tu is a causal structuration of events. features contained in each slot are represented by conceptual graphs  sowa  1 . they are derived from the semantic knowledge of the system which is represented by a lattice of types of concepts associated to canonical and definition graphs. the canonical graphs contain semantic constraints on the casual roles fillers. in definition graphs  more precision about the roles is available  as well as about the predicate consequences. 
　the aggregation process is based on similarities between situations. two situations are similar if their causal structure and their features are analoguous. a similarity measure has been defined on the basis of the structure itself and of its features. the process entails grouping similar features and reinforcing them by augmenting their weight. other features are simply added. the reinforcement of some elements implies the weakening of others. by iterating this process  a situation will be progressively completed and the most recurrent features describing it will emerge. they will be the candidates for the description of the abstracted situation. 
　we consider short narrative texts  such as those found in newspapers  that may reference more than one situation. for example  a murder attempt  an aggression  a quarrel  political murders: they all describe analoguous situations that lead to progressively build a single aggregated tu about an attempted murder  described in figure 1  section 1 . 
　the aggregation process also accounts for the case retrieval problem. in most cbr systems  cases are indexed by predetermined features. in  kolodner and simpson  1   cases are classified according to general situations and the access mode is based on the traversal of a network of discriminating indices. in aqua  cases are indexed by several features  especially by the general situations  mops  involved. but when lacking general situations  the system does not know what it is learning about and therefore no indices can be used. another approach is to perform the retrieval of cases by a propagation process  as presented in remind  lange and wharton  1 . propagation requires weighing the features of the structure. in remind  this task is made by hand when the domain knowledge is given. in mlk  the aggregation leads to computing all the weights and to dynamically modifying them according to the experience. retrieval criteria evolve while the memory content changes without maintaining an explicit structure of indexation features. the problem of deciding what features are relevant for retrieving a structure does not occur. the spreading activation network encodes knowledge about concepts and situations. concepts can be viewed as indices for situations  because they are part of the conceptual graphs that represent the features of the tus. furthermore they create links between tus other than the thematic links. 
　when analyzing a text  sentences are decomposed in clauses and the mlk retrieval process is given each concept of a clause. units in the network representing these concepts are clamped to high level of activation. activation then spreads into the network. as the activation function encodes the prior context  situations that fit with the current clause and are coherent with the context are highly activated. 
　thus the aggregation mechanism provides an answer to the organization and structuration of cases in an episodic memory for the purpose of learning from texts  even when general situations do not guide the process. our model satisfies two constraints: it allows incremental learning independently of the domain and is a case basis for cbr. 
1 the aggregating episodic memory 
1 representation of tus and encoding of episodes 
　a text is represented by an episode which is a structured set of tus. one of them encodes the main topic of the episode and the others are linked to this main situation by thematic links. tus can be considered as instances of schemata. they encode causal and temporal knowledge between events of a specific situation issued from a text analysis. their representation formalism is derived from the schema representation chosen  which is close to mops  schank  1 . figure 1 shows the segmentation of a text  the distribution of the clauses in the tus and details one tu. 

figure 1: a text and its representation 
a tu is described by three slots: circumstances  description and outcomes. the circumstances slot is valued by a set of conceptual graphs that represent the states known when the situation occurs. conceptual graphs in the description slot 
	ferret & grau 	1 

represent the events which take place and are partially ordered by chronological links. the outcomes slot contains the resulting states and is also valued by a set of graphs. we can see that some conceptual graphs are issued from semantic inferences  as patient and wounded. the conceptual graphs are build around a predicate. the predicate is the central concept when describing an event; its type is a subtype of action  process or state. the other concepts fill in the casual roles linked to the predicate. their types must be specializations of the types coming from the canonical graphs  we call these latter types 'reference types' . 
1 similarity 
the aggregation process requires that mlk find known situations in memory similar to the new tus. situations in memory result from preceding aggregations and are named aggregated tus. the aggregation takes place at two levels. at the episode level  its purpose is to make the global contexts emerge by the means of recurrent sets of situations. at the tu level  its purpose is to learn about a situation by incrementally completing it while recurrent events emerge. at both levels  the aggregation process has to evaluate the similarity of tus with aggregated ones. 
for this purpose  the similarity measure we use  ferret and 
grau  1  is decomposed in two steps. global similarity is based on the number of similar events  according to the slots they belong to. the exact degrees of similarity of slots and events are examined when this global similarity gives a medium result. two events are similar if their predicates contain the same type and if at least one of their concepts filling casual roles is similar. this occurs if their types share a supertype inferior to the reference type of the casual role. the degree of similarity of events and tus is computed respectively on the basis of the weights of concepts and graphs. an example is given in section 1. 
　this similarity measure results in grouping features that belong to a same topic without consideration of a description level. situations in texts which refer to the same topic are often different specializations in which some events are the same but roles are different. an aggregated tu is conceived to be the basic structure on which abstraction will rest. this latter process will be in charge to eventually split a memorized situation into several units  general and specialized schemata  according to the weights of the events and concepts. if the similarity criteria were more strict  each aggregated tu would be too specific and the common points of a general unit would not emerge. 
1 episode aggregation 
　at the episode level  mlk reinforces configurations of similar situations. aggregation in these cases consists in the reinforcement of the common thematic links and the aggregation of the related tus. at the tu level  the graphs of events that have the same predicate are aggregated; the others are just added. same causal or temporal relations between graphs are also reinforced. aggregation of graphs leads to progressively generalizing their concepts. 
　as previously mentioned  aggregated tus result from successive aggregations in the episodic memory. so  they are 
1 	cognitive modeling 
structures like the tus  whose features are weighed aggregated graphs. this kind of graphs is defined as an extension of conceptual graphs in such a way as to maintain the capability of applying the basic derivational operations. we have added to them the similarity computation and the aggregation process. the aggregation process is defined as a maximal join where predicates must join: same casual roles are joined and their types of concepts are abstracted as in figure 1. new relations and concepts are then added. 

　types of aggregated concepts are computed at each aggregation. they are the most specific concept abstraction  msca   different from the reference type  of all the aggregated instances. if such a type does not exist in the lattice  we choose the msca that groups the maximum of instances if it is possible. to compute it  each aggregated concept keeps the instances it comes from. for each of these instances  we also keep the specific episode it is part of  as  in the figure . thus  we can rebuild all the specific situations which have contributed to the formation of an aggregated tu. this capability is preserved to enable the retrieving of specific links between graphs. it will be useful to justify a reasoning based on the aggregated situation  tu aggregation being done regardless of shared instances between graphs. for example  if an agent of an event is also the patient of another event in the same situation  this piece of knowledge is not maintained at the aggregation level since each aggregated concept type evolves inside its own graph regardless of the evolution in the other graphs. this kind of knowledge is also basic to schemata abstraction; it prepares the formation of roles. the last characteristic added on the aggregated concepts is a counter to compute their weight. weights on the predicates represent the importance of the event inside the tu and weights on the other concepts  the importance of the concept in the graph. these counters are equal to the number of aggregations. thus  the weight of an event is computed by dividing the counter of the predicate by the number of times the tu has been aggregated. similarly  the weight of a concept is the counter of the concept divided by the number of aggregations of the predicate. weights on relations between two graphs are the number of aggregations divided by the number of common episodes between the two graphs. 
　the following example  in which five tus have been aggregated  only shows the predicates of the graphs with their weights and the episodes they belong to. for presentation purpose  chronological relations between events in the 


inter-graphs relations: 	d.d -  o.a  1   1  
             d.c -  o.b  1   1   o.c  1   1 1  figure 1: the aggregated tu attempted murder 
description slot and the aggregated concepts have not been mentioned. only causal relations between actions and their results have been precised. states setting up a situation are extremely various in texts  and often without explicit causal relations; so they are all considered as possible circumstances. at this state of the memorization  none emerge completely  but we can note that some are semantically close: located vs live and disagree vs threatened. events in the description slot are more interesting with a characteristic event always reinforced  stab. others have to be confirmed before deciding their relevance  like arrest. notice that the reinforcement of a causal relation between two events is also an indication of their interest. weak weights on events can be interpreted as anecdotes  stumble  or events that do not really belong to the situation. outcomes are often linked to an action and are significant. a discussion about the formation of this tu is presented in section 1. 
1 the retrieval process 
a spreading activation mechanism makes possible the use of episodic memory as an associative memory in order to select the relevant knowledge for processing the clauses coming from the texts. the figure 1 shows activation of four aggregated graphs resulting from the network after it successively receives as input the clauses of the following little story: 
yesterday  i went out to do shopping. i bought a spare part to repair my car but finally  i let the mechanic repair it. 
　the two aggregated graphs  buyobject and shopping  are part of an aggregated tu called coingsupermarket. repaircar and repairengine are part of the repairingcar aggregated tu. in accordance with the thematic shift in the story  going from shopping to repairing a car   there is a change among the most activated elements of the memory. after clamping the concepts of the first sentence  the most activated elements are part of the goingsupermarket tu. on the contrary  the graphs of the repairingcar aggregated tu are not activated because nothing is mentioned about the situation of repairing. when the second sentence is introduced  after cycle 1   the graphs of the goingsupermarket tv 

figure 1: activation level of aggregated graphs 
are still highly activated. this is the result of two influence streams. first  the previous activation state of the memory created a context which constrains its evolution. second  concepts coming from the sentence  in this case  the concept buy  confirm the previous topic. but we can see also that others concepts  sparepart  car  repair  make another'topic appear. this explains why the graphs of the repairingcar tu  and the tu itself  begin to be activated. after the processing of the third sentence  after cycle 1   the context effect of the memory for the first topic is not significant any more  the previous confirmation was not very strong  and no element of this third sentence is specifically linked to it. on the contrary  it strengthened the second topic. this explains why we can observe that the activation of the elements of the goingsupermarket tu decreases rather strongly and that the opposite effect happens for the elements of the repairingcar tu. 
　the spreading activation mechanism which supports this process is divided in two steps. firstly  it defines a subset of the memory  in which the selection will be done. the episodic memory may be very large if the system has a great amount of experiences. hence it is not realistic from a computational point of view to involve the whole memory in the core selection mechanism. moreover  this could be a source of noise and disturb the selection process. it is of course necessary in this task to bypass some similarity failures: the above little story does not tell us about a supermarket  yet the goingsupermarket tu  which is the best thematically related tu  has been nevertheless retrieved. this is due to the spreading of the activation flow towards more general or more specific concepts than the ones initially clamped. but this flow has to stay around the initial concepts. thus it is not interesting to select a surtype of supermarket as publicplace because we do not want to activate all the situations which take place in a public place. 
　the definition of this subset of the memory is based on the propagation of a constant flow of activation in the network  a flow that starts from the concepts that make up the clauses of the texts. as the global amount of activation that can be used 
	ferret & grau 	1 

for the propagation is always the same  the activation level of the units is less and less high as these units are more and more distant. when the activation level of an unit is under a 
given threshold  the propagation towards its next units is stopped. in the lattice of concept types  a specific mechanism makes more activation going towards subtypes of a concept type than towards its surtypes. 
　the second step of the spreading activation mechanism aims at selecting the aggregated tus in the episodic memory that are the most relevant with regard to the clauses which are considered at a given stage of the analysis of a text. this process is akin to the evidential activation in remind. when this working mode is active  the network mentioned above  which is a recurrent one due to the symmetry of the connections  has a two-phases dynamics: at the beginning  it has great liberty to explore the space of possible states and this liberty is restrained progressively in order to make the network converge on one state. this is achieved locally by the activation function of the units through its divisor term: 

with aj t : activation level of unit i after t cycles wjj: weight of the connection between units i and j 
　the second term of this function is particularly significant since it is the support of the context effect mentioned above which makes the network take into account the result of the previous spreading activation sessions. 
1 discussion 
1 processing of texts 
we give here a complete example processed by mlk. the memory contains in particular the tu attempted aurder described in figure 1 resulting from the processing of five texts. the creation of the tu comes from the first episode  aiiport murder attempt because none similarity has been found with existing tus. the other texts  a quarrel   two politic murders  and  and an aggression   contain a situation matching with the evolutionary aggregated attempted murder tu. with the episodes  and   the similarity results from a deep comparison of the graphs while within the two last episodes  it results more from the global measure. thus  the more the tu is growing  the more the similarity becomes obvious. the new processed text is given in figure 1. we will focus on the processing of the tu mlklattempted murder. each graph causes a propagation and entails highly activating the attempted murder unit. the circumstances and outcomes slots are strongly similar while the description slot has been found similar after a deep evaluation. therefore aggregation is done; results are given in figure 1. 
　the aggregated tu has been enriched by new events. located has been reinforced while mad is added in the 
circumstances slot. in the description slot  stab is reinforced and take hospital is added. in the third slot  wounded is reinforced and patient added. a new causal relation appears 
1 	cognitive modeling 

figure 1: a resulting tu after the aggregation process 
between take hospital and patient. comparison of the weights before and after aggregation  shows that they increase with reinforcement while decreasing in the opposite case. even with only six examples  a structure begins to emerge and the overall frame becomes meaningful. our method groups events that are thematically linked and  even if misunderstandings occur when segmenting texts  the aggregation will fix them in the long term. aggregation provides a strong process where some noises are allowed. 
1 towards abstraction 
at this state of structuration  we can notice in the evolution of the aggregated tu some indications about further abstraction. strongly weighed events will belong to a general situation  as stab; events as arrest have to be confirmed. events such as dead and wounded are not simultaneously present in the different episodes but are causally linked to the same action. this will suggest creating a generalized event for them in the general structure  with two specializations including these specific events. thus  an aggregated structure will lead to create schemata hierarchically structured. 
　generalization of concepts also prepares the abstraction phase. information that will be used are weights  but also the distribution of the episodes and the aggregated relations. 
1 a reasoning medium 
the episodic memory is build for the purpose of learning and improving the comprehension. in this latter perspective  we have developped a thematic analysis that segments a text into tus by using the memory and its selection mechanism 
 ferret and grau  1 . the accumulated experience of mlk is then reused to process new texts. 
　enrichment of tus will allow to improve understanding and make inferences. for example  in the fifth episode  the fact that the aggressor is imprisoned is explicitly mentioned. because this kind of fact has been memorized as the outcome of the arrest event with an important weight  the arrest of the aggressor could be inferred. such an inference can be justified because the roles are identical in the specific situations. mlk makes this kind of reasoning possible due to the case basis dimension of its memory. 

1 comparison with other models 
　aqua is the closest system to mlk in terms of their goals. it learns explanation patterns  xps   even if they are incomplete. these xps are completed when the lacking explanations are found in further texts. but the two processes are quite different. aqua needs a strong domain theory to build its explanations. this knowledge is represented by mops and abstract xps. aqua learns new specializations of these xps. learning and generalization occur at each new case. generalization relies on the explicative structure even if some actions need further explanations. in mlk  to avoid the need of general descriptions in all domains  causal links come from linguistic clues  from the assumption that texts are coherent by themselves and from semantic knowledge. if these links are recurrent  they will be confirmed. on the other hand  if they are unusual or result from a misinterpretation  they will disappear by non-reinforcement. so generalization is done in two steps. the aggregation principle allows relevant events to appear with their inter-relations. it replaces the explanation mechanism needed to justify events and solves the incremental aspect of learning. the second step of abstraction  where events and roles are abstracted to produce general situations  must be realized by another process  when situations in the episodic memory are stabilized. 
　case retrieval is quite different in mlk and problems of misindexation solved by aqua do not occur. it is closer to remind which integrates episodic memory retrieval and comprehension in the same propagation process. comparison is only done on the episode retrieval results  since comprehension is quite different when dealing with incomplete knowledge. differences come from the integration and the structuration of episodes. first  updating of the weights in mlk is done automatically. second  frames in remind are intermediate between our semantic graphs and the tus. episodes in remind are analoguous to our tus but tus are much more structured units where all the frames explaining an episode are grouped. in mlk  weights on events code the importance of the frame in the episode. thus  this upper level affects the spreading of the activation and selects the tus not only superficially  as they can be in remind  but also thematically close. 
1 conclusion 
in this article  we have presented mlk  an episodic memory model which has been designed to support a comprehension process tightly tied to learning. in this model  text representations built by the comprehension process are stored in memory in order to be used later by this same process to analyze other texts. but unlike traditional cbr systems  the memorization in mlk does not consist in classifying a new element according to an existing frame. by aggregating the text representations  the similar situations to which they refer form aggregated units containing events having a weight which characterizes their recurrence degree. this allows a new sort of knowledge to emerge. weights are used in order to support the retrieval process in the memory: a spreading activation mechanism selects the aggregated units that are the most relevant according to both the current input and the context set by the previous inputs. so  even when a strong domain theory is absent  mlk is able to memorize and organize the result of a comprehension process in such a way that it is able to recall it later in a contextually relevant way. moreover  this knowledge evolves progressively as new text representations are memorized. 
at its present state  our memory model is implemented in 
smalltalk and has been tested with a set of 1 conceptual frames  concept types and the graphs associated to them  and 1 aggregated tus resulting from 1 texts  i.e. 1 tus. a linguistic based module whose goal is to build the internal representation of the tus and a generalization process are being studied. so  in the future  we intend to integrate all these aspects in order to build a complete system integrating learning and comprehension. 
