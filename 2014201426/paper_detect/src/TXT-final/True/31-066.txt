 
modeling of a connectionist rule-based systems  or neuro-aj hybrid system  discussed through the paper will be a fruitful step towards the practical modeling of human cognition. this paper investigates a plausible and useful integration method of symbolic al techniques and connectionist models and proposes a practical implementation  mainly how variables can be included in the structured information provided as facts and rules in the system. 
1 	introduction 
although connectionist models and symbolic ai techniques are often seen as rivals  the aim of this paper is to show the usefulness of a hybrid system of symbolic ai and connectionist models  namely a rule-based connectionist inference system. the system inherits aspects of two 
previously 	proposed 	hybrid 	systems 
  kozato&dewilde 1ab   and combines their advantages. the first system handles probabilistic knowledge represented only as propositions  whereas the second handles structured information or predicates in a very limited way. the system described here handles structured information with similarity between the individual components  involves lenient variable handling to allow flexible inferences  and possesses an efficient rule selection or search mechanism by evaluating several units of knowledge together. in addition to these functions  strict variable handling mechanism is also included. to yield these functions together  the system is designed as a combination of four different types of connectionist networks and a data buffer. the networks work in 
cooperation to derive new facts and the data buffer keeps the facts provided by the user or derived by the networks. 
1 representing information units and rules 
each unit of knowledge provided by the user and the facts derived by the system are both represented as information units. an information unit consists of three information fragments: a subject concept  an object concept  and the 
1 	connectionist models 
relation. strict  syntactic  variables or just variables can be included in information units in the same manner as a subject or object concept. for example  a variable x is described in two forms: s:x which can be substituted with a 
subject concept and o:x with an object concept. an information unit with a variable looks as follows: 
like s:x o: bananas  
a rule implemented in the system consists of a pair of premises followed by a conclusion. the premises and conclusions are expressed as information units. for example  a rule looks like the following: 
rulel: if like s:gorillas o:x  and belong s:x o:fruits  then eat s:gorillas o:x  
the premises of this rule can be satisfied with pairs of information units such as 
like s:gorillas o:bananas  and belong s:bananas o:fruit   or like s:gorillas o:apples  and belong s:apples o:fruit  
the former pair derives eat s:gorillas o:bananas  and the latter eat s:gorillas o:apples . 
　　as mentioned above  the system is equipped with a lenient variable handling mechanism. for example  the premise part of rulel may match like s:chimps o:bananas  when like s:gorilias o:bananas  is absent in the data buffer  providing that s.gorillas and s: chimps are represented as very similar information fragments in the system inference domain. in such a case s:gorillas of rulel can be called a 
　　lenient variable or semantic variable and the inference operation which involves such variables is called semantic inference. 
1 distributively represented symbols and loose pattern matching 
there are basically two ways to represent symbols in a connectionist network. one is the localised method by which only one symbol is allocated in a single unit  and the other is the distributed method by which a symbol is assigned to a collection of units and each unit may represent fractions of more than one symbol. the model proposed in this paper mainly take the latter method in a hopfield binary neural network because it seems the way to yield the maximum capability of the connectionist model and brings several advantages as follows: 
 1  even with the binary state units  on or off   the system is capable of expressing graded certainty of information. 
 1  the system may gain a high capacity to store knowledge with fewer units. 
 1  similarity information between units of knowledge is soundly implemented. 
loose pattern matching  which is one of the fundamental properties of many connectionist models  works very effectively if it participates in a rule-based inference operation. it has been utilised for a rule selection mechanism and the interpretation of vague knowledge. 
1 lenient variable handling 
loose pattern matching governs a certain kind of variable handling mechanism with no additional structure or facility. unlike strict variable handling employed by ordinary symbol processing systems  lenient variable handling is based on the semantic similarity of information. 
　　the model described in this paper represents information in a hopfield binary neural network to realise lenient variable handling in the following manner: 
 1  units of information handled in the systems are represented as structured information composed of subject/object concepts and relations. 
 1  subject/object concepts are provided with a similarity degree to the other concepts  e.g. sichimps  1%s:gorillas. 
 1  each subject/object concept is allocated to a collection of neurons  and the neuron patterns for the concepts overlap in proportion to the similarity degree between the concepts  e.g. for the above similarity information a half of the fractions of the information fragments s:gorillas and sxhimps are allocated to the same neurons. 
　　in this model  s:gorillas is called a lenient variable when a network training pattern representing 
like s:gorillas o:bananas  accepts a unit activation pattern like s:chimps o:bananas . 
　　lenient variables are quite suitable for the plain architecture of connectionist networks. this is because no additional structure or facility is required for the hopfield network for the introduction of variables to the inference operation. besides that  the generalisation function of connectionist models can be used to perform approximate inferences. owing to the generalisation effect  any network training pattern can match a similar but not exactly identical unit activation pattern. a subject or object concept distributively represented in the hopfield network may work as a lenient variable for a subject or object concept included in the initial input information  if they are very similar  or in other words if their representations in the hopfield network overlap a lot. 
　　similar models previously proposed have not taken the idea of lenient variables into account. one but a very clear reason is that most systems of this kind are based on the localist representation method. the typical examples are the models proposed by ballard  ballard  1   by shastri  shastri  1   by shastri and ajjanagadde  shastri & ajjanagadde 1   and by hendler  hendler 1 . it seems very troublesome that a concept is leniently represented by a localist method. there is a model which takes a distributed method to represent information  that is the 
model 	proposed 	by 	touretzky 	and 	hinton 
  touretzky&hinton 1 . however  it has not expanded the generalisation effect towards the new interpretation of variables. more discussion can be found in ikozato  1 . 
1 	overview 
the system consists of 1 parts as described in figure 1. 

figure 1 system overview 
　　the flow of information in the system is presented by the activation signals. the data buffer hands over certain information units to the winner predominant networks and receives additional information units derived by the feedforward network. during the update phase  the hopfield binary network receives as well as sends back some information units from/to the winner predominant networks in the form of information fragments  and also sends out and receives information fragments to/from the structural constraint network. after this phase  the hopfield network hands over the information fragments left in it to the feedforward network. the sequence of an inference operation proceeds as follows: 
 stepl  the data buffer sends out activation signals to the winner predominant networks in order to project certain information units called the initial input information onto them. 
 step1  each of the winner predominant networks passes the information units it holds to the hopfield binary network by activating the units to which the information fragments included in these information units are allocated.  step1  the hopfield network starts the unit update 
	kozato 	1 

process in order to select a rule by which a new information unit can be derived. 
 step1  after certain steps of the update process of hopfield network units which is called an update operation term  the hopfield network communicates with the winner predominant networks to exchange information so as to adjust the direction of the update operation toward a proper stable unit activation pattern which must express two information units as the premises of a rule. 
 step1  during the update operation phase which consists of a number of update operation terms  the hopfield network also sends out activation signals to the structural constraint network to solve certain structural 
constraints between two information units as the premises of a rule when they include the same variables. 
 step1  an update operation phase terminates when only one information unit is left in each winner predominant network and it cannot be changed by a further update operation term. this termination indicates that the hopfield network has selected a rule which can be used to derive a new information unit. then the information fragments and variables left active in the hopfield network are sent out to the input port of the feedforward network to derive a new information unit. 
 step1  through the feedforward network  an 
information unit is generated and sent out from the output port. if the premises and conclusion of the selected rule include the same variable  then variable substitution may also take place. 
 step1  the information unit derived from the 
feedforward network is then handed over to the data buffer to be stored and used for another inference operation. 
1 architecture 
1 the data buffer 
the data buffer is used to store all the information units of a certain domain provided by the system user and derived through the inference operation. when the system starts the inference operation  the buffer hands over a certain number of information units to the winner predominant networks as the initial input information  and also when the system ends the inference operation  it accepts a new information unit which has just been derived. 
1 the hopfield binary network 
the hopfield binary network is used as a loose pattern matching machine to select one of the rules implemented in the system. the rule selection is completed by choosing a rule whose premises are most likely to be satisfied by the initial input information projected on the winner predominant networks. an actual pattern matching process performed by the hopfield network is completed in a 
sufficient number of the update operation terms. one update operation term is made up of a certain number of asynchronous unit updates  e.g.  the total number of units of the hopfield network. 
1 	connectionist models 
1.1 	processing units 
hopfield network units have only two states  on or off  and are categorised into the following three units: 
 1  concept units: the indiscriminately divided fractions of an information fragment are allocated to a collection of concept units  and the information fragment is called the base of those units. because each concept unit represents a concept fractionally  the entire set of concept units with the same base being active would represent a concept with 1% validity. 
 1  relation units: each relation is allocated to a relation unit. 
 1  variable units: each variable is allocated to a variable unit. 
1.1 	structure 
the network is divided into two bodies: one is called the left bank and the other the right bank. each bank which comprises a set of concept units  relation units and variable units is used to implement one of the two premises of a rule. within a bank  all the concept units are fully connected to all the relation units  and all the relation units are fully connected to all the variable units. however  there are no direct connections between concept units and variable units since they are semantically independent of each other. between the two banks  all the relation units are fully connected. this enables two rule premises in each bank to be recalled as one pattern after the update operation phase. 

1 the winner predominant networks 
the winner predominant networks maintain the original combinations of information fragments as autonomous information units. because each information unit consists of a set of information fragments  it is essential to ensure correct combination so as to represent correct information. 
　　each network comprises two kinds of sub-networks called the help-each-other type sub-networks and the winner-take-all type sub-networks as illustrated in figure1. they consist of the nodes and bidirectional links between them. the necessary number of nodes is varied but always 

a multiple of the total number of the information fragments included in the system inference domain. each node has two states  alive or dead. a node becomes alive if it is activated at the projection of the initial input information from the data buffer  otherwise it is dead. such a node is called a live node. only live nodes can be operational and possess a certain level of activation signal between inactive and fully active. only the sub-networks which include live nodes take part in the inference operation. they are called live sub-networks. 
1.1. help-each-other type sub-networks 
a help-each-other type sub-network enhances the unity of an individual information unit. it chains nodes to which different information fragments are allocated. the connections are made with bidirectional links so that a node receives as well as sends out an activation signal from and to the adjacent nodes. 
　　a sub-network represents the existence of an information unit as the three live nodes to which either a subject concept  an object concept or a relation is allocated. such live nodes work cooperatively within a sub-network to keep themselves active by sending encouragement signals to each other. the magnitude of the encouragement signal is in proportion to the activation level of the live nodes. figure1 describes the activity of a help-each-other type sub-network. 
1.1 connection to/from the hopfield binary network the nodes of the winner predominant networks and the concept units of the hopfield network are connected by bidirectional links. activation signals are exchanged between the networks. 
the activation signals from a winner predominant network on the left-hand  or right-hand  side are sent to the left  or right  bank of the hopfield network just before every update operation term. this is to install an input activation pattern of hopfield network units representing the information units held in the winner predominant network in the hopfield network for the next update operation term. the update operation phase finally 
terminates after the signal flow in this direction so that the final information fragments left in the winner predominant networks will be handed over to the hopfield network and sent out to the feedforward network. 
　　on the other hand  the activation signals are sent out to the winner predominant network on the left-hand  or right-hand  side from the left  or right  bank of the hopfield network just after an update operation term. this is to bring back the activation pattern of the hopfield network obtained by the last update operation term to the winner predominant networks  and to check whether the 
information fragments represented by that pattern are still consistent with the information units held in the winner predominant network. the activation signal flow from the subject  or object  concept units to the winner predominant networks is only allowed  when no subject  or object  variable unit on the same bank is active. if a subject  or object  variable unit is active  then the activation signals from the concept units are ignored and the variable unit returns the activation signals to the winner predominant network. for this purpose  switching gates to control the activation signal flow are present on the links. 
1 the structural constraint network 
the structural constraint network is added to the system to provide the structural constraint imposed by the introduction of strict variables into the rules. the task imposed on the structural constraint network is to avoid inconsistent convergence of the hopfield network to a unit activation pattern which involves inconsistent variable substitutions. 
	kozato 	1 
　　the network is divided into two sections. one is to check the existence of the semantically identical 
information fragments  and the other is to set the structural constraints between the semantically identical variables. it consists of the following components. see figure1. 
if an identical concept detector unit detects that two sets of concept units are both almost active  it sends out a signal to the bidirectional links which connect the semantically corresponding pairs of variable units to adjust the weights of the links. 

 1  identical concept detector units each of which receives and compares the activation signals from two sets of concept units where a semantically  or both semantically and syntactically  identical concept is assigned. 
 1  unidirectional links between the concept units and the identical concept detector units. 
 1  weighted bidirectional links between pairs of variable units. in each pair  a semantically  or both semantically and syntactically  identical variable is represented. 
 1  unidirectional links between the identical concept detector units and the weighted bidirectional links. 
1 an example core section 
the hopfield network  two winner predominant networks  and the structural constraint network form the core section of this system. figure1 illustrates an example core section where the following collection of information fragments are implemented. 
 1  subject/object concepts:gorillas  chimps  fruits  bananas 
 1  subject/object variables: w  x  y  z 
 1  relations: like eat satisfy  belong  happy-with  dream 
this system can hold rules consisting of these information fragments in the hopfield network and feedforward network. 
 for example  rule 1 as well as others like those presented below can be implemented in them. 

rule1: if eat s:x o: y  and happy-with s:x o:y  then dream s:x o:y  
rule1: if dream s:x o:y  and eat s:x o:y  then like s:x o:y  
1 the feedforward network 
once the hopfield network has consistently converged to a rule premise pattern  the feedforward network takes charge of the derivation of a new information unit. it receives a collection of information fragments and variables as a pair of information units  and produces an information unit based on a rule implemented in the network. as the input activation patterns to the feedforward network may include probabilistic data owing to the similarity information implemented in the hopfield network  the rule applications through the network can be probabilistic. for example  the network may receive 
like with 1% certainty  o:x  
sxhimps with 1 % certainty  s:gorias with 1% certainty  
o:bananas with 1 % certainty  and oifruits with 1 % certainty 
from the left bank of the hopfield network as the first premise of a rule and 
belong with 1 % certainty  s:x  
     s:bananas with 1% certainty  and s:fruits with 1 % certainty from the right bank as the second premise  and produce 
eat s:gorias o:bananas  with 1 % or less  possibly 1 %  certainty 
by rulel. 
　　the network consists of three sections: input ports  output ports  and analogue units. each input or output port is dedicated to representing one of the information fragments or variables represented in the hopfield network. an input port receives a set of binary signals from the hopfield network units where the same information fragment is assigned and converts them to an analogue activation signal to the analogue units. an output port displays an output activation signal for an information fragment and sends it to the data buffer. 
　　analogue units carry out the actual inferences. they are placed in three layers: input  hidden  and output layers. each input or output unit is directly connected to an input or output port  whereas the hidden units are placed between them. 
　　in order to provide the variable substitution mechanism to the system  extra circuits can be added onto the feedforward network so as to accomplish this task mechanically. 
1 network training 
1 the hopfield binary network 
each of the two premises for a rule is stored in one of the two banks. for rulel  the first premise like s:gorillas o:x  is stored on the left bank of the hopfield network as the 
activation pattern of units to which either like  s:gorillas or 
o:x is allocated  whereas the second premise 
belong s:x o:fruits  is stored on the right bank as the activation pattern of units to which either belong  s:x or o:fruits is allocated. the storage operation is completed by 
an automatic training procedure with the sum-of-outer-products function through which the appropriate weights are set on the connection arcs between hopfield network units. 
1 the feedforward network 
the feedforward network is trained to implement the rule premise/conclusion relations by error back propagation. the premises of a rule are used as the input pattern for training  whereas the conclusion is used as the desired output pattern. for example  the input pattern to train rulel is the activation signals to the input ports  of the feedforward network  for like  s:gorillas  and o:x  as well as belong  s:x  and o:fruits  and the desired output pattern is the activation signals to compare the output activation signals from the network at the output ports  of the feedforward network  for eat  s:gorillas and o:x. more discussion can be found in  kozato  1 . 
1 update operation phase 
1 	the winner predominant networks and 
the hopfield network 
since the initial input information to the hopfield network includes several information units together and each information unit is merely a collection of information fragments  there is no way that the hopfield network alone can correctly recognises the valid combinations of information fragments as autonomous information units during the update operation phase. therefore  it is very probable that the network will converge to an activation pattern which represents no information unit stored in the data buffer but a strange combination of information fragments such as figure1. in order to avoid such a situation  the hopfield network needs to work in cooperation with the winner predominant networks. the cooperation task is performed after every update operation term of the hopfield network by opening the connections between the concept and relation units of the hopfield network and the nodes of the winner predominant networks. 
　　if the state of the hopfield network is changed after an update operation term so that some of the concept and relation units connecting to the live nodes of the winner predominant networks lower their activation level from the original level set by the live nodes  the activation level of those live nodes is also lowered. accordingly  each such live node becomes less able to encourage the other live nodes within the same help-each-other type sub-network. this means that a certain information unit expressed by that help-each-other type sub-network tends to be removed from 
	kozato 	1 
the initial input information to leave out the redundant information for the rule selection. 
　　on the other hand  every live node in a winner-take-all type sub-network tries to suppress the other live nodes according to the activation level. that is  the higher the activation level of a live node is  the more the node suppresses the others and gains its activation. therefore  even if a live node receives less activation signals from the connected hopfield network units  it can still regain its activation on condition that the other live nodes in the same help-each-other type sub-network have gained higher activation than the others belonging to the same winner-take-all type sub-network. 
　　after this process  some of the information units in the winner predominant networks are weakened or weeded out. the information rearranged in the winner predominant networks is then reloaded to the hopfield network for the next update operation term. 

1 the structural constraint network and 
the hopfield network 
the hopfield network also works in cooperation with the structural constraint network. this is to check whether 
there is any contradiction between the information units held in the winner predominant networks and the 
information represented by the unit activation pattern in the hopfield network. any premise pattern of a rule including variables should be expressed by the hopfield network on condition that consistent variable substitution is ensured. 
　　the units on both the banks of the hopfield network representing a semantically  or both semantically and syntactically  identical concept are connected to an identical concept detector unit. also  every pair of variable units representing a semantically  or both semantically and syntactically  identical variable on both the banks is connected by a bidirectional link with a certain weight. 
　　in the beginning of every update operation term  each identical concept detector unit checks whether or not the two sets of concept units connecting to the identical concept detector unit are both active. if this is confirmed  the unit forces the negative weight of the corresponding links between variable units to be neutralised  ＼zero . for example  refer to figure1   once the identical concept detector unit detects that the concept units representing 
s1 	connectionist models 
s:gorillas on both the banks of the hopfield network have been activated together  the identical concept detector unit sends out a signal to the weighted links between the variable units representing s:w  s:x  s:y  and s:z on both the banks so as to set the weights to zero. accordingly  the variable units connected by the links are cut off so that these variable units are freed from the structural constraint. 
1 	conclusion 
hopfield network  or even more generally  connectionist models  have not been designed to be capable of representing structured items in a simple manner nor processing each item discriminatory. to achieve a complicated task in such a system  some additional facilities for an extra mechanism or a large scale modification of the architecture to make it suitable for a particular use is necessary. in this model  the problem has been solved by the introduction of the structural constraint network for variable substitution and the winner predominant networks for pattern matching among several units of structured information. 
