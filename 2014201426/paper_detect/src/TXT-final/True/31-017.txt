 
sensor and motor systems are not separable for autonomous agents to accomplish tasks in a dynamic environment. this paper proposes a method to represent the interaction between a vision-based learning agent and its environment. the method is called  motion sketch  by which a one-eyed mobile robot can learn several behaviors such as obstacle avoidance and target pursuit. a motion sketch is a collection of visual motion cues detected by a group of visual tracking routines of which visual behaviors are determined by individual tasks  and is tightly coupled with motor behaviors which are obtained by q-learning  a most widely used reinforcement learning method  based on the visual motion cues. in order for the motion sketch to work  first the fundamental relationship between visual motions and motor commands is obtained  and then the q-learning is applied to obtain the set of motor commands tightly coupled with the motion cues. finally  the experimental results of real robot implementation with real-time motion tracker are shown. 
1 	introduction 
recent research in artificial intelligence has developed computational approaches of agent's involvements in their environments  agre  1 . an autonomous agent is regarded as a system that has a complex and ongoing interaction with a dynamic environment that is difficult to predict its changes. our final goal  in designing and building an autonomous agent with vision-based learning capabilities  is to have it perform a variety of tasks adequately in a complex environment. in order to build such an agent  we have to make clear the interaction between the agent and its environment. there have been a variety of approaches to analyze the relationship between the agent with visual capabilities and its environment. 
　in physiological psychology   held and hein  1  have shown that self-produced movement with its concurrent visual feedback is necessary for the development of visually-guided behaviors. their experimental results suggest that perception and behavior are tightly coupled 
1 	action and perception 
in autonomous agents that perform tasks. in biology   horridge  1  similarly have suggested that motion is essential for perception in living systems such as bees. 
　in computer vision area  so-called  purposive active vision paradigm   aloimonos  1; sandini and grosso  1; edelman  1  has been considered as a representative form of this coupling since  aloimonos et a/.  1  proposed it as a method that converts the ill-posed vision problems into the well-posed ones. however  many researchers have been using so-called active vision systems in order to reconstruct 1-d information such as depth and shape from a sequence of 1-d images given the motion information of the observer or capability of controlling the observer motion. furthermore  though purposive vision does not consider vision in isolation but as a part of complex system that interacts with world in specific ways  aloimonos  1   very few have tried to investigate the relationship between motor commands and visual information  sandini  1 . 
　in robot learning area  the researchers have tried to make agents learn a purposive behavior to achieve a given task through agent-environment interactions. however  almost of them have only shown computer simulations  and only a few real robot applications are reported which are simple and less dynamic  maes and brooks  1; connel and mahadevan  1 . the use of vision in the reinforcement learning is very rare due to its high costs of sensing and processing. 
　in order to realize tight coupling between visual sensor and motor systems  we should consider the relationship between the low level representation of motion  motor commands to actuators  and the visual information  and develop a learning capability to abstract the low level representation into a form suitable for task accomplishment. in this paper  we propose a method to represent the interaction between the agent and its environment which is called  motion sketch  for a real one-eyed mobile robot to learn several behaviors such as obstacle avoidance and target pursuit. a motion sketch is a collection of visual motion cues detected by a group of visual tracking routines of which visual behaviors are determined by individual tasks  and is tightly coupled with motor behaviors which are obtained by q-learning  a most widely used reinforcement learning method  based on the visual motion cues. 
in the next section  we describe the basic idea of the motion sketch for our example task. in section 1  we give a method for acquisition of the fundamental relationship between visual motion cues and robot motor commands. in section 1  we describe a reinforcement learning method to obtain target pursuit behavior and obstacle avoidance one. then  in order to demonstrate the validity of our method  we show the experimental results of the real robot implementation with a real-time visual motion tracker  inoue et a/.  1  in section 1. 
1 	motion sketch 

figure 1: motion sketch 
　the interaction between the agent and its environment can be seen as a cyclical process in which the environment generates an input  perception  to the agent and the agent generates an output  action  to the environment. if such an interaction can be formalized  the agent would be expected to carry out actions that are appropriate to individual situations.  motion sketch   we proposed here  is one of such formalizations of interactions by which a vision-based learning agent that has real-time visual tracking routines behaves adequately against its environment to accomplish a variety of tasks. 
　figure 1 shows a basic idea of the motion sketch. the basic components of the motion sketch are visual motion cues and the motor behaviors. 
　visual motion cues are detected by several visual tracking routines of which behaviors  called visual behavior  are determined by individual tasks. the visual tracking routines are scattered over the whole image and an optical flow due to an instantaneous robot motion is detected. in this case  the tracking routines are fixed to the image points. the image area to be covered by these tracking routines are specified or automatically detected depending on the current tasks  and the cooperative behaviors between tracking routines are performed for the task accomplishment. for the target pursuit task  the multiple templates are initialized and every template looks for the target to realize stable tracking. in the task of obstacle detection and avoidance  the candidates for obstacles are first detected by comparing the optical flow with that of non-obstacle  ground plane  region  and then the detected region is tracked by multiple templates each of which tracks the inside of the moving obstacle region. 
　the motor behaviors are sets of motor commands obtained by q-learning  based on the detected motion cues and given task. the sizes and positions of the target and the detected obstacle are used as components of a state vector in the learning process. 
　visual and motor behaviors work in parallel in the image and compose a layered architecture. the visual behavior for monitoring robot motion  detecting the optical flow on the ground plane on which the robot lies  is the lowest and might be subsumed in part due to occlusion by other visual and motor behaviors for obstacle detection/avoidance and target pursuits which might occlude each other. 
　thus  the  motion sketch  represents the tight coupling between the agent that can perform an appropriate action sequence so as to accomplish the given tasks and its environment which is represented by visual motion cues from the visual tracking routines. the motion sketch does not need any calibrations nor any 1-d reconstruction so as to accomplish the given task. the visual motion cues for representing the environment does not seem dependent on scene components nor limited to the specified situations and the task. furthermore  the interaction is quickly obtained owing to the use of real-time visual tracking routines. 
　the behavior acquisition scheme consists of the following four stages: 
stage 1 obtaining the fundamental relationship between visual and robot motions by correlating motion commands and flow patterns on the floor with very few obstacles. 
stage 1 learning target pursuit behavior by tracking a target. 
stage 1 detection of obstacles and learning an avoidance behavior. 
stage 1 coordination of the target pursuit and obstacle avoidance behaviors. 
at each stage  we obtain the interaction between the agent and its environment. 
1 	obtaining sensorimotor apparatus 
before introducing the method for obtaining sensorimotor apparatus  motion mechanism and visual tracking routines we use in the experiment are shown. 
 a  pws system: 
　the robot has a power wheeled steering  hereafter pws  system driven by two motors into each of which 


1 	action and perception 

&e k  indicates the decreasing degree of e k  by using the 1 ~ k principal components for obtaining the approximation of pi 
figure 1 shows the relationship between  and 
k. from this figure  it is sufficient to use the first two principal components for describing pi as a linear combination of principal components. that is  including more than the third pricipal components does not have influence on decreasing more than 1 pixel error per a point. 

figure 1: the change rate of error values per a point 
thus  vector pi may be approximated by 

the first two principal components obtained in the real experiment are shown in figure1. obviously  the first 
 a  corresponds to a pure rotation and the second  b  to 

figure 1: relation among the possible actions 
　thus  we can compress the visual motion patterns by the obtained fundamental relationship and then use it to include the ego-motion information in the internal state space of the agent in the learning process. 
1 behavior acquisition based on visual motion cues 
1 	basics of reinforcement learning 
reinforcement learning agents improve their performance on tasks using reward and punishment received from their environment. they are distiguished from supervised learning agents in that they have no  teacher  that tells the agent the correct response to a situation when an agent responds poorly. an agent's only feedback indicating its performance on the task at hand is a scalar reward value. one step q-learning  watkins  1  has attracted much attention as an implementation of reinforcement learning because it is derived from dynamic programing  bellman  1 . the following is a simple version of the 1-step q-learning algorithm we used here. 
initialization: q  - a set of initial values for the action-value function  e.g.  all zeros . repeat forever: 

a pure backward motion. 

visual functions of tracking routine our visual tracking routine has the following visual functions. 
 a  an initial image specified 

figure 1: visual functions of tracking routine 
1. a target image is specified by a human operator in advance as shown in figure 1 a . a target is tracked by an object tracker which consists of 1 visual tracking routines fixed together as shown in figure 1 b . even if the pattern of the target is deformed by occlusion or the vibration of the robot body  the object tracker can continue to the track target owing to the use of multiple visual tracking routines. 
1. we prepare three kinds of resolutions a normal  a half and a quarter resolutions  as shown in figure 1 c . even if the the pattern of the target becomes large or small  the object tracker can continue to track it by changing the image resolution and the search area for the block matching. 
1. when the target detection fails  a search-wholeimage routine is called in order to detect the target again outside the pre-defined search area. 
we define the state of the target in the image based on its position and size  three levels  obtained by the visual tracking routines. 
state and action spaces in q-learning in order to apply the q-learning scheme to a target pursuit task  we define a number of sets and parameters. the state of the target  s in the image is quantized into 1 sub-states  combinations of three positions  left  center  and right  and three sizes  large  near   medium  and small  far  . similarly  changes in position and size of the target in the image are quantized into 1 sub-states  combination of three states for position changes  move left  no move and move right  and three states for size changes 
 enlarge  no change  shrink . we add two lost situations  target is lost into the left side or the right side  in the state space. futhermore  we add the action index  totally 1 actions  just taken on observing the current situation into the state space in order that we deal with the so-called perceptual aliasing problem. that is  induing 
1 	action and perception 
the self-motion index into the agent's internal state enables the agent to discriminate both changes caused by the observer motion and an actual changes happened in the environment. 
totally  we have 1 states in the set s. we have 
1 actions in the action set a. we assign a reward value 1 when the robot touched the target or 1 otherwise. a discounting factor 1 is used to control to what degree rewards in the distant future affect the total value of a policy. in our case  we set the value a slightly less than 1  1 = 1 . 
1 	obstacle avoidance behavior acquisition 
 a  detection and tracking of obstacles by flow differences 
we know the flow pattern p{ corresponding to the action i in the environment without any obstacles. the noise included in p{ is not so much  because this flow pattern is described as a linear combination of the two principal motion vectors. therefore  it makes motion segmentation easy. motion segmentation is done by comparing the flow pattern p  with the flow pattern p＜b* which is obtained in the environment with obstacles. the area in the p＜bs is detected as the area for obstacle candidates if its components are different from that of pi. this information  position and size in the image  is used to obtain the obstacle tracking behavior. after obtacle detection  the visual tracking routines are set up at the positions where the obstacle candidates are detected and the regions are tracked until the region disappears from the image. 
 b  learning obstacle avoidance behavior 
learning to avoid obstacles consists of two stages. first  the obstacle tracking behavior is learned by the same manner as in learning the target pursuit behavior. next  the obstacle avoidance behavior is generated by using the relation between the possible actions and the obstacle tracking behavior as follows:  1  the relationship between the possible actions is divided into four categories by clustering the action space in terms of the coefficients  a1 al1   see figure1 b     1  the obstacle tracking behavior is mapped on the relationship  and the category ct which includes the obstacle tracking action is found   1  the obstacle avoidance action is selected among the categories except for ct. more correctly  the obstacle avoidance action is obtained by finding the action having the smallest action-value function with respect to the obstacle tracking behavior among the categories except for ct. 
1 experimental results for a real system 
1 a configuration of the system 
　figure 1 shows a configuration of the real mobile robot system. we have constructed the radio control system of the robot  asada et a/.  1 . the image processing and the vehicle control system are operated by vxworks os on mvme1 mc1 cpu  computer which are connected with host sun workstations via ether net. the image taken by a tv camera mounted on the robot is 


transmitted to a uhf receiver and subsampled by the scan-line converter  sony corp. . then  the video signal is sent to a fujitsu tracking module. the tracking module has a function of block correlation to track some pre-memorized patterns and can detect motion vectors in real time. in the figure 1  a picture of the real robot with a tv camera  sony camera module  and a video transmitter is shown. 
1 target tracking with no obstacles 
the experiment consists of two phases: first  learning the optimal policy / through the computer simulation  then apply the learned policy to a real situation. figure 1 shows a sequence of images where the robot succeeded in pursuing a target. the top of figure 1  a  shows the initial position of the target. the top figures in the figure 1  b    c  and  d  shows the processed images. the white rectangle in each image shows the target position which is tracked. the white lines in these images show the optical flows. in this way  based on the hierachical architecture of the visual tracking routine  we can perform the target tracking and the optical flow detection in parallel on the real system. 

	 c  	 d  
figure 1: the robot succeeded in pursuing a target. 
1 obstacle detection and avoidance 
figure 1 shows a sequence of images where the robot succeeded in avoiding a moving obstacle. the top figures in the figure 1  a  and  b  show the processed images. in  a   the rectangles indicate the obstacle candidate regions. 
1 concluding remarks and future work 
as one of the method for representing the interaction between the agent and its environment which enables the situated agents to behave adequately against the external world  we proposed  motion sketch  which is independent of scene components and tightly coupled with motor commands. now  we are planning to develop a new program which tightly connects max video 1 and 

fujitsu tracking module to speed up and finish the final stage of behavior integration. 
