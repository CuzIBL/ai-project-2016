 
multi-level spatial aggregates are important for data mining in a variety of scientific and engineering applications  from analysis of weather data  aggregating temperature and pressure data into ridges and fronts  to performance analysis of wireless systems  aggregating simulation results into configuration space regions exhibiting particular performance characteristics . in many of these applications  data collection is expensive and time consuming  so effort must be focused on gathering samples at locations that will be most important for the analysis. this requires that we be able to functionally model a data mining algorithm in order to assess the impact of potential samples on the mining of suitable spatial aggregates. this paper describes a novel gaussian process approach to modeling multi-layer spatial aggregation algorithms  and demonstrates the ability of the resulting models to capture the essential underlying qualitative behaviors of the algorithms. by helping cast classical spatial aggregation algorithms in a rigorous quantitative framework  the gaussian process models support diverse uses such as directed sampling  characterizing the sensitivity of a mining algorithm to particular parameters  and understanding how variations in input data fields percolate up through a spatial aggregation hierarchy. 
1 introduction 
many important tasks in data mining  scientific computing  and qualitative modeling involve the successive and systematic spatial aggregation and redescription of data into higherlevel objects. for instance  consider the characterization of wcdma  wideband code-division multiple access  wireless system configurations for a given indoor environment. in noisy channels  the performance goal is to quantitatively assess the relationship between the signal-to-noise ratio  snr  and the bit error rate  ber  or bit error probability  bep  of the realized configuration. to improve performance in office environments  characterized by doorways  walls  cubicles   a common trick used is to incorporate space-time transmit diversity  sttd . instead of a single transmitter antenna  
qualitative reasoning 
chris bailey-kellogg department of computer sciences purdue university  in 1  usa cbk cs.purdue.edu 

snrl db 
figure 1: mining configuration spaces in wireless system configurations. the shaded region denotes the largest portion of the configuration space where we can claim  with confidence at least 1%  that the average bit error probability  bep  is acceptable for voice-based system usage. each cell in the plot is the result of the spatial and temporal aggregation of hundreds of time-consuming wireless system simulations. 
the base station uses two transmitter antennas separated by a small distance. if the signal from one of the antennas is weak  the signal from another is likely to be high  and the overall performance is expected to improve. in this application  it is important to assess how the power-imbalance between the two branches impacts the bep of the simulated system  across a range of snrs  see fig. 1;   verstak et al  1  . 
　characterizing the performance of wcdma systems requires the identification of multi-level spatial aggregates in the high-dimensional configuration spaces of wireless systems. the lowest level  input  contains individual monte carlo simulation runs providing unbiased estimates of beps. this space is high-dimensional  e.g. 1   owing to the multitude of wireless system parameters  e.g. channel models  fading characteristics  coding configurations  and hardware controls . wireless design engineers prefer to work in at most two or three dimensions  e.g. to study the effect of power imbalance on system performance  for ease of tunability and deployment. the next level of spatial aggregation thus contains buckets which aggregate data in terms of two dimensions  using various consistency constraints and design specifications. finally  the third level aggregates buckets into regions of constrained shape; the shape of the regions illustrates the nature of joint influence of the two selected configuration parameters on performance. specific region attributes  such as width  provide estimates for the thresholds of sensitivity of configurations to variations in parameter values. 
　the results of such mining are important for both qualitative and quantitative analysis. for instance  when the average snrs of the two branches are equal  the bep is minimal and the width of the mined region in fig. 1 depicts the largest acceptable power imbalance  in this case  approximately 1db . however  the width is not uniform and the region is narrower for smaller values of the snrs. the qualitative result is that system designs situated in the lower left corner of the configuration space are more sensitive to power imbalance in the two branches. 
　each input data point captures the results of a wireless system simulation which takes hours or even days  the simulations in fig. 1 were conducted on a 1-node beowulf cluster of workstations . thus it is imperative that we focus data collection in only those regions that are most important to support our data mining objective  viz. to qualitatively assess the performance in configuration spaces. this requires that we model the functioning of the data mining algorithm  in order to optimize sample selection for utility of anticipated results. modeling data mining algorithms in this manner is useful for closing the loop  characterizing the effects of the data mining algorithm's parameters  and improving our understanding of how variations in data fields percolate up through the layers. a particularly interesting application is to use such modeled structures to design information-theoretic measures for evaluating experimental designs  mackay  1  and for active data selection  cohn et al  1; denzler and brown  1j. 
in order to address these goals  this paper develops a novel 
gaussian process approach to modeling algorithms that mine spatial aggregates. we first overview the spatial aggregation mechanism for spatial data mining and the gaussian process approach to bayesian modeling. we then show how to integrate the two approaches in order to achieve our goal of probabilistically modeling spatial data mining algorithms. we illustrate this ability within the context of identifying pockets underlying the gradient in a field - an application that captures many of the interesting characteristics of more complex studies like the wireless application. 
1 	spatial aggregation 
the spatial aggregation language  sal   bailey-kcllogg et al  1; yip and zhao  1j  provides a set of operators and data types  parameterized by domain-specific knowledge  for uncovering and manipulating multi-layer geometric and topological structures in spatially distributed data. sal applications construct increasingly abstract descriptions of the input data by utilizing knowledge of physical properties such as continuity and locality  expressed with the vocabulary of metrics  adjacency relations  and equivalence predicates. to understand the sal approach  see fig. 1   consider a sal program for analyzing flows in a vector field  e.g. wind velocity or temperature gradient . 
　in the first level  the goal is to group input vectors  a  into paths so that each sample point has at most one predecessor and at most one successor. sal breaks the process into two key steps  one capturing locality in the domain space  i.e. sample location   and the other capturing similarity in the feature space  i.e. vector direction . a neighborhood graph aggregates objects with a specified adjacency predicate expressing the notion of locality appropriate for a given domain. as shown  b   a sample point's neighbors include all other points within some specified radius r. feature comparison then must consider only neighbors in this graph  thereby exploiting physical knowledge to gain computational efficiency while maintaining correctness. here we break feature comparison into a sequence of predicates and graph operations. in particular  we first filter the graph  c   applying a predicate that keeps only those edges whose direction is similar enough 

1 	qualitative reasoning 

 within some angle tolerance 1  to the directions of the vectors at the endpoints. the remaining graph has some  junction  points where vector direction suggests multiple possible neighbors  and the most appropriate path extension from the point must be chosen. a similarity metric sums the distance between the junction and a neighbor  weighted by a constant d  and the difference in vector direction at the junction and the neighbor. the most similar neighbor for the junction is selected   d  and  e   for successor and predecessor junctions  respectively . 
　the remaining graph edges are collected and redescribed as more abstract streamline curve objects  f   for the second level of analysis. again  computation is localized so that only neighboring streamlines are compared. the neighborhood graph here  not shown  uses an adjacency predicate that declares streamlines neighbors if their constituent points were in the first level. it is then straightforward to identify convergent flows  g  with an equivalence predicate that tests when constituent points form a junction in the graph in  c . if desired  these flow bundles can be abstracted and analyzed at an even higher level. 
　sal's uniform spatial reasoning mechanism  instantiated with appropriate domain knowledge  has proved successful in applications ranging from decentralized control design ibailey-kellogg and zhao  1; 1 j  to weather data analysis  huang and zhao  1   to analysis of diffusionreaction morphogenesis  ordonez and zhao  1 . recent work has focused on optimizing sample selection for applications where data collection is expensive  including identifying flows in multi-dimensional gradient fields  bailey-kellogg and ramakrishnan  1   and analyzing matrix properties via perturbation sampling  ramakrishnan and bailey-kellogg  1 . this paper provides the mathematical foundations necessary for the modeling of such sal programs to support the meta-level reasoning tasks outlined in the introduction. 
1 	gaussian processes 
gaussian processes have become popular in the last few years  especially as a unifying framework for studying multivariate regression  rasmussen  1   pattern classification  williams and barber  1   and hierarchical modeling  menzefricke  1 . the underlying idea can be traced back to the geostatistics technique called kriging  journel and huijbregts  1   named after the south african miner danie krige. in kriging  the unknown function to be modeled 
 e.g.  ozone concentration  over a  typically  1d spatial field is expressed as the realization of a stochastic process. a prior is placed over the function space represented by this stochastic process  by suitably selecting a covariance function. given measured function values at sample locations  kriging then proceeds to estimate the parameters of the covariance function  and any others pertaining to the random process . using such values a prediction of the response variable can then be made for a new sample point  typically using map or ml inference. this basic approach is still popular in many tasks of spatial data analysis. 
　even though parameters are estimated in this approach  it is important to note that kriging is fundamentally a memory-
qualitative reasoning 
based technique  since the estimated parameters only describe the underlying covariance function of a stochastic process. thus  predictions of the response variable for new sample points are conditionally dependent on the measured values and their sample points; by unrolling the effect of the parameters of the random process  we can directly express this dependency. 
　kriging is often motivated as a local modeling technique  capable of approximating or interpolating functions with multiple local extrema  and generalizes well to applications exhibiting anisotropics and trends. the stochastic prior is also viewed as a mathematically elegant mechanism to impart any available domain knowledge to the modeling technique. in 1  sacks et al.  sacks et al  1  showed how kriging can actually be used to model processes with deterministic outcomes  especially in the context of computer experiments. the justification for modeling a deterministic code as a stochastic process is often that even though the response variable is deterministic  it may 'resemble the sample path of a suitably chosen stochastic process'  sacks et al.  1 . alternatively  using a stochastic process prior can be viewed as a bayesian approach to data analysis  sivia  1  and this is the idea emphasized by most recent computer science research in gaussian processes  gibbs  1; rasmussen  1 . the stochastic process can be suitably formulated to ensure that the model reproduces the same response value for repeated invocations of a given sample input  i.e.  absence of random error . for instance  the gaussian prior can be chosen so that the diagonal entries of the covariance matrix are 1  meaning that the model should interpolate the data points. 
　in the recent past  gaussian processes have become popular in the statistical pattern recognition community  mackay  1  and graphical models literature  jordan  ed.   1 . neal established the connection between gaussian processes and neural networks with an infinite number of hidden units  neal  1 . such relationships allow us to take traditional learning techniques and re-express them as imposing a particular covariance structure on the joint distribution of inputs. for instance  we can take a trained neural network and mine the covariance structure implied by the weights  given mild assumptions such as a gaussian prior over the weight space . williams motivates the usefulness of such studies and describes common covariance functions  williams  1 . 
　williams and barber  williams and barber  1  describe how the gaussian process framework can be extended to classification  where the modeled variable is categorical. essentially  the idea is to  i  use a logistic function to conduct traditional gaussian regression modeling  and  ii  adopt a softmax function to bin the logistic output into a given set of classes. this means that the logistic function uses a  latent variable  as input in its computation  since its values are not provided by the dataset. 
1 	gaussian processes for spatial aggregation 
sal programs construct multi-layer spatial aggregates based on specified local adjacency relations  similarity metrics  and consistency checks. we describe here how to capture the 

figure 1: modeling the reversal of gradients in a id field using gaussian processes   top  original field   bottom  given measured values of gradient vector angles at specific data points  blue   the model posits that the conditional distribution of the angle at unseen data points is a gaussian  shown in red . 
qualitative behaviors of such aggregates using gaussian processes. the essence of a gaussian process is its covariance structure  so we focus on determining covariance structures in a sal program. for example  in the two-layer sal program of sec. 1  the parameters  r  1  d  impose a covariance structure by specifying the reach of the neighborhood graph  enforcing the similarity of angles in the vector field  and penalizing for the distance at decisions involving junctions. 
1 	covariance structure 
we now describe how to model the covariance structure of a given sal program. we give the mathematical framework for the case of mining a 1d field to determine if there is a reversal of gradient as we move along the spatial dimension  but essentially the same machinery applies to two and higher dimensional spaces. the basic problem is one of classifying id points to determine the qualitative structure of same-direction flows. fig. 1  top  depicts the given input field along the x dimension. as is shown  the field consists of unit vectors with different orientation. the gaussian process approach is first to model an underlying regressed variable and then to use a logistic or softmax function to bin the output into classes. in our application  the regressed variable represents the gradient and can be simply summarized as the angle of unit vector orientation y in fig. 1  top . in other applications  the regressed variable could be an unobserved 'latent' variable. in either case  it is modeled as a function / of the input x. 
　first  assume / to be a gaussian process on x  meaning that the conditional probability distribution of y given a value of x is a gaussian. for instance  fig. 1  bottom  depicts measured values of y superposed with distributions of y at two unseen points. a covariance structure among the y values could  for example  capture the intuition that adjacent values of y should agree more than distant values. the goal of modeling is to determine the extent and stringency of this neighborhood relation - one of the defining parameters of a sal program. specifically  we posit a process such as: 
 1  
the idea then is to estimate a model of the same form as /  on the basis of a given set of observations 
a typical choice for z in 
is a random process with zero mean and covariance 
where scalar is the estimated variance and is a matrix that captures the correlation between the inputs  i.e.  the given locations . notice that even though the input is one dimension  the size of r depends on the number of locations for which gradient measurements are available. the above model for  also includes the constant term this can be estimated based on the k observations  or we can substitute more complex terms  e.g. linear   or even omit it altogether. 
　the functional form of r  including its parameterization  in effect defines the stochastic process and must be carefully chosen to reflect the underlying data's fidelity or any domainspecific assumptions about local variation. the parameters of the process are then estimated using multidimensional optimization involving a suitable objective function. for instance  given the following form for / : 
		 1  
the problem reduces to estimating  from the given data. notice that this formulation for r implicitly enforces that the model exactly interpolate the given data points  since 
　　　　　a common objective function for estimating is to minimize the mean squared error  mse   
between 	and 	. the p that minimizes mse is given by the solution to the optimization problem: 
		 1  
where r is the symmetric correlation matrix formed from for a new sample point a prediction for the regressed variable is given by: 
 1  
where r is the correlation vector between the response at and all the other points  derived from /    is the identity vector of dimension and   is the estimate of a given by: 
 1  the variance in the estimate is given by: 
		 1  
1 	qualitative reasoning in this case  the optimization is one-dimensional due to the presence of the single parameter  with a different parameterization  we will employ multi-dimensional optimization over the entire set of hyperparameters. when dimensionality is large  the hyperparameters are estimated using mcmc methods. once such a modeling is complete  as discussed in the previous section  we can relate a categorical class variable to y using softmax functions. for instance  the reversal of the gradient in fig. 1 can be captured by first using the gaussian process model to make predictions of the gradient at untested points and then determining if  and where  a zero crossing occurs. 
　the above equations extend naturally to a 1d case such as that described in sec. 1. the covariance prior has to be suitably parameterized and we also have the option of taking into account any interactions between the two dimensions  both linear and nonlinear . 
1 	modeling many layers 
when sal programs consist of many layers  we need to develop a sequence of gaussian process models  each with a suitable covariance function  which can then be superposed to yield a composite covariance function. recall that while one could simply assess the covariance of the output field for sample values of the parameters and a given input field  the real purpose of a gaussian process model is to express the covariance of the output as a function of the characteristics of the input. this is the key property that allows reasoning about closing the loop and selecting optimal samples. in addition  gaussian process models help capture the randomness inherent in some of sal's computations  e.g. non-determinism in labeling  and variations due to how ties are broken for aggregation purposes. refer again to sec. 1 for an example of the types of operations that the covariance model must capture. 
at the very bottom of the hierarchy is the input data field. 
for applications characterized by expensive data collection 
 as in the introduction   it can be advantageous to start with a sparse set of sample data. the gaussian modeling approach to regression is ideal for creating surrogate representations of data fields from such a sparse dataset. that is  given a sparse set of samples  interpolate a dense field satisfying those values and incorporating any appropriate domain knowledge  as discussed above regarding kriging. such surrogate functions can then been used as the starting points for qualitative analysis  bailey-kellogg and ramakrishnan  1 . 
　the operators in a sal level deal with both locality  which object locations are close to which other ones  as encapsulated in a neighborhood graph  and similarity  which object features are close to which other ones  as encapsulated in metrics and predicates . for instance  in the example of sec. 1  two points are assigned to the same pocket if they are spatially proximate and their flows converge. here the gaussian process is classification  or more generally  density estimation . a popular covariance structure for an n-dimensional input field captures locality: 
		 1  
where the expression relates the function values at positions and then the covariance function will be positive definite  satisfying the normalization constraints of a posteriori inference. 
　to see how to capture similarity  consider when two sample locations are classified into the same trajectory in sec. 1. in addition to being spatially proximate  as inferred by sal's 
qualitative reasoning 

figure 1: a 1d pocket function. 
neighborhood calculations   the underlying vector fields must also be similar in direction. expressing the covariance in terms of position alone can cause the resulting estimated hyperparameters to be misleading or difficult to interpret  as their effect is confounded with the underlying vector field. one solution is to artificially inflate the dimensionality  so that position and direction together describe the data. besides increasing the dimensionality  this approach spells trouble for estimation using mcmc methods since significant portions of the sample space will remain unsampled and it would be difficult to assess their effects on the minimized functional. an alternative solution is to use the fact that the vector field is itself a surrogate and add a term to the covariance outside the above structure  capturing the contribution due to similarity in the vector field. we place a gamma prior on this term with a shape parameter that ensures that its role is secondary to the covariance structure on position  directional similarity alone is not enough for high covariance at the output; the sample locations also must be spatially proximate . this is recognized in the statistics community as a hierarchical prior and described in detail in  neal  1 . 
1 	experimental results 
in order to test our approach  we studied de boor's pocket function  see fig. 1 : 
at which 
is evaluated  i is the identity n-vector  
and is the l1 norm. this function exploits the fact that the volume of a high dimensional cube is concentrated in its corners and p is designed so that it has a  dip  in each corner. it embodies many aspects of datasets like those encountered in the wireless simulation study  including multiple local extrema  non-systematic variation in the location of the pockets  and regional variation. the pocket function is also important as a benchmark for high-dimensional data exploration  where the goal is to identify the most interesting regions of the design space without necessarily conducting a  costly  global optimization over the entire design space. data mining programs are hence required to identify the most promising regions using as few function evaluations as possible. 


figure 1: modeling a sal program to mine pockets in gradient fields   a  variation in number of pockets mined by the sal program for various values of  b  covariance contribution in dimension for various values of covariance contribution in dimension for various values of  in all charts  varies by group and varies within group. 

　a sal program to identify the number of pockets starts with some samples of the pocket function. the lowest level of modeling involves a kriging interpolation over a uniform grid  we chose size  for testing . then the approach of sec. 1 is applied to the gradient vector field of this scalar field: the second level bundles points into curves  and the third aggregates these into flows. each convergent flow represents one pocket. one could mine the covariance structures for each layer separately; we unfold these mappings here to obtain a single covariance structure summarizing all three layers. this is because the structure  esp. the contributions of each dimension  is easiest to interpret in terms of the original spatial field. 
	we conducted a parameter sweep over 	as: 
and used neaps bayesian modeling software fneal  1  to construct gaussian process classifiers for the flow classes. covariance contributions in the terms  eq. 1  from both the dimensions was estimated using hybrid monte carlo  aggressive schemes to evolve the system state by adding higher order terms . this procedure uses a leapfrog scheme to suppress random walk behavior by selective iteration between gibbs sampling scans and latent value updates. 
　our results indicated a strong positive correlation between the and covariance contributions  bringing out the symmetry in the underlying sal computations. the number of pockets mined was constant across the values of d  other parameters fixed   and one of the goals of our study was to determine if this negligible effect of d is captured in the covariance structures.  the effect of would actually be more pronounced in other spatial fields but not so much in the pocket function due to the inherent symmetry.  parameters r and produced the most variation in the covariance contributions 
 1 causing an abrupt jump in the number of mined pockets. this is due to the rather stringent limit imposed on vector similarity arising from the nonlinearity of the cosine metric. fig. 1 summarizes the results for a 1d pocket function  where we have averaged the covariance contributions across all ds  for given and 
　as the number of pockets increases  fig. 1 a    the covariance contributions increase  fig. 1 b c   approximately quadratically. in other words  as the underlying latent function varies rapidly along the given dimensions  we cannot stray  too far  away from a given sample point when making predictions at test points. the reciprocal of the covariance scale term is often referred to the characteristic length of a dimension. this gives an estimate of  how far  a given dimension's effect holds. when only four pockets are mined  the characteristic length is about 1  meaning pockets occupy a width of   exactly one fourth of the total space . as more pockets are mined  the characteristic length drops to about 1. it is also interesting to note that the abrupt jump in the number of pockets for = 1 is reflected by a similar increase in the covariance contributions for this value. essentially  vector and edge directions have to be so similar that few long  runs  can be aggregated as streamlines. this brings out the capability of the gaussian process approach to capture the essentials of a spatial aggregation algorithm. 
1 	discussion 
this paper has demonstrated a novel gaussian process approach to modeling the qualitative behavior of sal programs; in contrast to much of the literature where gaussian processes are used for pattern classification and regression  rasmussen  1; gibbs  1   our work takes existing data mining algorithms and recasts them in terms of gaussian priors. to the best of our knowledge  this is the first study to completely model a qualitative data mining algorithm in terms of a process framework  summarizing the transformation from data to higher-level aggregates. this is an important step in firmly establishing a probabilistic basis for spatial aggregation computations. the modeling undertaken here  while expensive  is justifiable for studies such as the wireless system characterization described in the introduction. 
1 	qualitative reasoning 　there are several immediate gains from the work presented here; due to space limitations we only mention them briefly. first  the gaussian process model can characterize experimental design criteria such as entropy as a functional w.r.t. the input space  allowing us to use the mined covariance structure to focus sampling at the most informative points  e.g.  see lbailey-kellogg and ramakrishnan  1  . it is important to note  however  that the approach taken in  baileykellogg and ramakrishnan  1  only addresses the lowest 
levels of a hierarchy and is unable to reason about higherlevel  more abstract processes of redescription and aggregation as is done here. second  gaussian process models allow us to study the effects of different sal parameters for a given class of datasets  e.g. the inference above of the negligible role of d in the mining process. finally  it allows us to take algorithms that function in differing ways  and using different sets of parameters  and places them on a common footing  namely the language of covariance structures. this means that we can reason about the applicability of different algorithms by studying the constraints they impose on spatial locality and field similarity. 
   gaussian processes have recently been linked to kernelbased methods  as used in support vector machines  cristianini and shawe-taylor  1 ; we intend to explore this connection in future work. kernel-based methods are attractive in their promise to overcome the curse of dimensionality by the use of nonlinear projections  a facet that is of critical importance for mining data from large parameter sweeps. as the need for data mining in computational science gains prominence  process models will be crucial to achieve effective utilization of data for mining purposes. 
acknowledgements 
the authors thank feng zhao and layne watson for helpful comments. this work is supported by us nsf grants eia1  eia-1  and eia-1. 
