 
we present a learning architecture for lexical semantic classification problems that supplements task-specific training data with background data encoding general  world knowledge . the model compiles knowledge contained in a dictionaryontology into additional training data  and integrates task-specific and background data through a novel hierarchical learning architecture. experiments on a word sense disambiguation task provide empirical evidence that this  hierarchical classifier  outperforms a state-of-the-art standard  flat  one. 
1 introduction 
there is an increasing interest in natural language processing  nlp  and information retrieval  ir  for research on lexical semantics  in particular with respect to word sense disambiguation  yoong and hwee  1   information extraction  riloff and jones  1   named entity recognition  collins  1   and automatic thesaurus extension  hearst  1 . in general terms  the goal in these tasks is that of automatically associating words in text with semantic labels. in information extraction and named-entity recognition noun phrases or proper nouns are assigned to semantic categories such as  organization    person   or  location . in word sense disambiguation and thesaurus extension the goal is to assign words to finer-grained categories defined by existing dictionaries and ontologies. 
　lexical semantic information can be useful in many nlp and ir applications such as text categorization  parsing  and language modeling for speech recognition. furthermore it can be crucial for tasks that require complex inferences involving world knowledge  such as question answering. 
　one of the main difficulties in learning semantic annotations stems from the fact that training instances are often narrowly focused on very specific class labels and relatively few 
   *wc would like to thank our colleagues in the information retrieval and machine learning group  irml  and brown laboratory for linguistic information processing  bllip   as well as jesse hochstadt for his editing advice. this material is based upon work supported by the national science foundation under grant no. 1. 
in number. it thus seems intuitive to supplement task-specific training data  for example  sense-annotated training instances for a specific word  with background data encoding general  world knowledge . the latter are typically available in sufficient quantities and need not to be generated separately for each classification task. to carry out this idea two crucial issues need to be addressed: how exactly can world knowledge be compiled into additional training data  and how can taskspecific and background data be systematically integrated  
　to address the first challenge  we propose to generate additional training data about broader semantic categories by extracting training sentences from a hierarchically structured ontology  wordnet1  fellbaum  1 . we assumed that each example sentence associated with a lexical entry provides evidence for the kind of contexts in which that specific concept and all its ancestors in the hierarchy can appear. as far as the second challenge is concerned  we introduce a novel hierarchical learning architecture for semantic classification. more specifically  we present a simple and efficient on-line training algorithm generalizing the multiclass perceptron of  crammer and singer  1 . 
　finally  we carry out an experimental evaluation on a word sense disambiguation task  providing empirical evidence that the hierarchical classifier outperforms a state-of-the-art standard  flat  classifier for this task. 
　the paper is structured as follows. section 1 introduces the main idea in more detail. in section 1 we introduce wordnet and the simplified ontology derived from it that we used as the source of world knowledge. section 1 deals with the basic multiclass perceptron and the proposed hierarchical multicomponent classifier. finally  sections 1 and 1 describe the data set used and the empirical results  respectively. 
1 word sense disambiguation and world knowledge 
word sense disambiguation is the task of assigning to each occurrence of an ambiguous word in a text one of its possible senses. a dictionary is used to decide if a lexical entry is ambiguous or not  and to specify its set of possible senses. the most widely used lexical resource for this task is wordnet  which we describe in detail in the next section. 
1  in this paper we always refer to wordnet version 1. 

natural language 	1 


figure 1. the simplified two-layer hierarchy for the noun chair. 

　as an illustration consider the noun  chair   which according to wordnet is ambiguous. two possible senses are explained in the following wordnet entries: 
  chairi - a seat for one person  with a support for the back; 
  chairi -  president  chairman  chairwoman  chair  chairperson   the officer who presides at the meetings of an organization; 
　word sense disambiguation is often framed as a multiclass pattern classification task. useftil features include cooccurring words  word bigrams or trigrams  and properties of the syntactic context that contains the target word. most commonly systems are trained on labeled data for a specific word  for each and tested on unseen items of the same word. the set of possible labels is the set of senses of the ambiguous word. one limitation of such a strategy is that the system bases its decision exclusively on what it has been able to learn about a few very specific concepts; e.g.  chair i and chair1. furthermore  since manually sense-tagging words for the required training data is slow and expensive  the data is quite sparse. 
　a great deal of information about objects like  chairs  is indirect and can be derived from more general world knowledge through generalization and inference processes. suppose that the task is to disambiguate between the two simple senses of chair in the following context: 
1   here the quality of the finest chair components is merged with art.  
in this sentence components is a useful hint that we are dealing with the sense chairi. chairs are artifacts  and artifacts can have components. conversely  even though in principle people could  have components  as well  this sounds a little odd. intuitively  if a word sense disambiguation system had access to this type of information - that  chairs  are subordinates of broader concepts like  artifacts  and  people  - and some knowledge about these broader semantic categories  it might achieve a higher accuracy in disambiguating words. notice that the system might never have previously observed any instance of the noun  chair   in either sense  as  having components . 
　the goal hence is to complement specific but limited knowledge about narrow classes with richer  if less specific  knowledge about more general classes. we can easily recover the fact that chairs are kinds of furniture or people from dictionaries and hierarchically organized ontologies like wordnet. learning information about such general concepts  however  is complicated. one source of complication is the very problem we are trying to solve  lexical ambiguity. if we do not know whether something is a person or an artifact we cannot learn reliable information about those more general concepts. one way of addressing these problems is offered by wordnet itself. 
1 	the ontology 
1 wordnet 
wordnet is a broad-coverage  machine-readable dictionary widely used in nlp. the english version contains around 1 entries  mostly nouns  but also verbs  adjectives  and adverbs. wordnet is organized as a network of lexical ized concepts  called synsets  that comprise sets of synonyms. for example  the nouns {president  chairman  chairwoman  chair  chairperson} form a synsct. a word that belongs to several synsets is ambiguous. synsets are linked by semantic relations  the most important of which for nouns and verbs is the is-a relation  or hyponymy; e.g.   car  is a hyponym of  vehicle . the verb and noun databases form is-a hierarchies with a few general concepts at the top and several thousand specific concepts at the leaf level. 
　the hierarchical structure of the database has aroused some interest in nlp  because it can support interesting computational language learning models  for example  in learning predicate selectional preferences  light and greif  1 . we aim to use the hierarchy to improve lexical classification methods. the model we present here can in principle make use of the full hierarchy. however  for the sake of simplicity we have focused on a less complex hierarchy  which has been derived from wordnet as described below. 
1 	a simple two-level hierarchy 
wordnet was built  and is regularly updated  by lexicographers. lexicographers group words together in synsets and individuate the relevant semantic relations between synsets. this process includes the classification of lexical entries into one of 1 broad semantic classes. in this paper we refer to these broad classes with the term supersenses. a few examples of supersense labels are person  animal  artifact  food  location  time  plant  process  attribute  substance  and relation. this set of labels is fairly general and therefore small. at the same time the labels are not too abstract. in other words  these classes seem natural and easily recognizable  and that is probably why lexicographers use them. in fact the level of generality is very close to that used in named-entity recognition   location    person    organization   etc. . 
　each synset in wordnet is associated with one supersense label. as a result the database implicitly defines  in addition to the full hierarchy  a simpler two-layer hierarchy. figure 

1 	natural language 

1 above illustrates the synsets and supersenses chair belongs to. 
1 	the hierarchy as a source of world knowledge 
for a few thousand concepts wordnet lists  among other types of semantic information  one or more example sentences. for the sense of chair above the example sentences are the following: 
  chairi -  he put his coat over the back of the chair and sat down  
  chair1 -  address your remarks to the chairperson  
overall there are 1 of these sentences. since each one is associated with one synset  that is in fact a sense-tagged instance of the word. in other words  wordnet provides a few thousand potential sense-tagged training instances. 
　unfortunately  this additional data in itself would not be of much help: for most of the synsets there are no sentences1  and typically the sentences are very short and do not provide much context. however  the situation appears in a different light if we take into account the hierarchy. considering an example sentence for a synset also as an example sentence for its ancestors  synsets at higher levels in the hierarchy   the number of sentences grows larger at the superordinate levels. if we consider the supersense level  the set of example sentences constitutes in fact a small corpus of supersenseannotated data. our hypothesis is that the several hundred sentences associated with each supersense can provide a useful source of general world knowledge. in the next section we describe a general multicomponent learning architecture that can be used to exploit this supplementary training data. 
1 	multicomponent learning architecture 
the idea of using the hierarchical structure of a domain to overcome sparseness problems has been explored in text categorization. these methods show improved accuracy and efficiency  toutanova et al.  1; dumais and chen  1 . in nlp the hierarchical structure of wordnet has been used to overcome sparseness data problems for estimating class distributions  clark and weir  1   and to exploit morphological information to improve lexical acquisition  ciaramita  1 . 
1 	multiclass perceptron 
the architecture we propose is a generalization of  ultraconservative  on-line learning  crammer and singer  1   which is itself an extension of perceptron learning to the multiclass case. we describe this  flat  version of the classifier first. for each noun w we are given a training set s = 
  where each instance 	and 
y w  is the set of synsets that wordnet assigns to w. thus 
1 summarizes n instances of noun w  where each instance i is represented as a vector of features x{ extracted from the context in which w occurred; d is the total number of features and yi is the true label of  
1 therc are in total around 1 synsets in the noun database. 
1 since some instances are labeled with multiple senses  in cases where the taggers were uncertain  y  may actually be a set of labels. 
natural language 
algorithm 1 multiclass perceptron l: 
1: 1: 1: 
1: 1: 1: 1: 
 1: 1: 1: 
1: 
in general  a multiclass classifier for word it; is a function that maps feature vectors to one of 
the possible senses of . in the multiclass perceptron  one introduces a weight vector for every and defines implicitly by the so-called winner-take-all rule: 
 1  
here refers to the matrix of weights  every column corresponding to one of the weight vectors  
　the learning algorithm works as follows: training patterns arc presented one at a time in the standard on-line learning 
setting. whenever  an update step is performed; otherwise the weight vectors remain unchanged. to perform the update  one first computes the error set el containing those class labels that have received a higher score than the correct class: 
		 1  
an ultraconservative update scheme in its most general form is then defined as follows: u p d a t e w i t h learn-
ing rates fulfilling the constraints  
and 	hence changes are limited to . the sum constraint ensures that the update is balanced  which is crucial to guaranteeing the convergence of the learning procedure  cf.  crammer and singer  1  . we have focused on the simplest case of uniform update weights  . the algorithm is summarized in algorithm 1. 
　notice that the presented multiclass perceptron algorithm learns all weight vectors in a coupled manner  in contrast to methods that perform multiclass classification by combining binary classifiers  for example  training a classifier for each class in a one-against-the-rest manner. 
1 	hierarchical multiclass perceptron 
the hierarchical multiclass perceptron is inspired by the framework for learning over structured output spaces introduced in  hofmann et al.  1 . the key idea is to introduce a weight vector not only for every  leaf-level  class  but also for every inner node in a given class taxonomy. in the current application to word sense disambiguation  the inner nodes correspond to the 1 supersenses 1 and we will hence 
1 

introduce additional weight vectors for   where s w  refers to the subset of supersenses induced by we will use the notation to refer to the supersense corresponding to a synset y. then discriminant functions can be defined in an additive manner by 
		 1  
if one thinks of/ in terms of a compatibility function between an observation vector x and a synset y  then the compatibility score is simply the sum of two independent contributions  one stemming from the supersense level and the other one coming from the more detailed synset level. the multiclass classifier is then again defined using the winner-take-all rule  
		 1  

the complete algorithm is summarized in algorithm 1. 
the first part of the algorithm concerns the different nature of the two types of training data. as we explained in section 1  the supplementary data derived from wordnet only provides annotations at the supersense level. we cannot use this information to perform updates for weight vectors vy  but only to adjust the weights . hence for supersense-annotated training instances we compute the error set on the supersense level as 	and perform the standard multiclass update step tor all with the second part concerns training on the task specific data 
       . if the classifier makes a mistake on pattern error sets are computed for its individual components both at the synset and supersense levels  lines 1 and 1 above   which are updated according to the standard multiclass update rule. 
　as an example  suppose that given a pattern xi of chair the synset error set is while the correct label is  
person. the update vector is subtracted from the vectors relative to the labels in ei while is added to if at the supersense level the error set ef = {artifact}  xi is subtracted from the vector for artifact and added to . therefore  through the supersense weight vectors  the background data affects classification at the synset level. 
1 	data set and features 
1 	the senseval data 
we tested our system on a standard word sense disambiguation data set. the training and test data are those used in the last senseval workshop  senseval-1/acl-1  1   which focused exclusively on word sense disambiguation. the training set consists of 1 paragraphs that contain an ambiguous word whose sense has been manually annotated. the inventory of senses is taken from wordnet. similarly  the test set consists of 1 unlabeled pairs. we only ran experiments on the noun data  which consists of 1 training instances and 1 test instances. each instance consists of a short passage taken from one of various sources: e.g.  the wall street journal  british national corpus  and web pages. the task-specific training data  t y   is typically smaller than the general one  ts  the average ratio  is equal to 
1. 
1 	features 
we used the same feature set described in  yoong and hwee  1   which is compact but includes most of the features that have been found useful in this task: surrounding words  bigrams and trigrams  and syntactic information. yoong and hwee report results for several classifiers broken down by part of speech  which makes it possible to compare our system's performance with that of several others. 
　there are four types of features. the following sentence serves to illustrate them:  the dinner table and chairs are elegant yet comfortable . the feature set is described in greater details in  yoong and hwee  1 : 
 part of speech of the neighboring w o r d s : = cc   = nns  p + 1 = aux ... 
  single words in the surrounding context: c == elegant  c = dinner  c = table  c = the ... 
  bigrams 	and 	trigrams: 	c i +i = and.are  

  head of the syntactic phrase that governs the tar-get: 
g relp1s = l e f t . 
from the syntactic parse trees of the senseval-1 training and 
1 	natural language syntactic features and part of speech tags were extracted 

figure 1. test accuracy of the flat multiclass perceptron  dashed line  and the hierarchical multiclass perceptron  continuous line  on the word sense evaluation data set. 
test data produced using charniak's parser lcharniak  1 . in this way we created the training data ty from the senseval data. in exactly the same way we extracted features from the example sentences in wordnet to produce the additional training set for the supersense-level classes  ts. overall there are around 1 features. 
1 	experiments 
1 	experimental setup 
we tested two models described in section 1: the flat multiclass perceptron  trained and tested at the synset level  and the hierarchical one  trained on both the standard synset data and the training data for the supersenses extracted from wordnet. we also trained and tested a simple  flat  naive bayes classifier. a different classifier was trained and tested for each word. we treated compounds such as easy chair and chair as different words. 
all the results we report are given as accuracy: 

1 	results 
figure 1 shows the performance of the flat perceptron  dotted line  during each iteration. the perceptron in fact converges very quickly. this is probably due to the fact that there are relatively few training items: normally the size of ty is between one and two hundred. to check whether an improvement was due to the algorithm alone and not to the combination of the algorithm and the additional  supersense data set  we also trained a hierarchical perceptron exclusively on the synset data. figure 1 also plots the performance of the hierarchical perceptron trained only on ty. the two curves are virtually indistinguishable  meaning that without additional information not much can be gained from using the hierarchical classifier alone. in other words  with  flat  data a  flat  classifier is as good as a  hierarchical  one. 

figure 1. test accuracy of the hierarchical  continuous line  vs. flat  dashed line  multiclass perceptron. the hierarchical multiclass perceptron was trained using supplementary supersense training data. 
　figure 1 plots the performances of the flat and the hierarchical perceptron when also trained on ts- the two patterns are very different. the hierarchical model converges only after more than 1 iterations. 

table 1. test accuracy on the senseval-1 test data. 
　this might be due to several facts. first  the amount of data is much greater due to the addition of ts  and it takes longer to learn. second  the supersense data and the synset data are probably very different and noisy; as a consequence the weight vectors are continually readjusted  possibly along very different dimensions. the interesting thing  though  is that even in the midst of very wide oscillations there is a clear improvement  particularly between 1 and 1 iterations. 
　we present also a comparative table. table 1 illustrates the results of our systems and other state-of-the-art word sense disambiguation ones. we set the number of iterations to a fixed number for all words equal to 1. given that we set this value  knowing  that it is a good one for both our systems  our results and those of other systems are not really comparable. however  it is reasonable to expect that it is possible to set this stopping criterion well enough using held out data. thus this comparison gives us an approximate idea of where our systems stand with respect to state-of-the-art ones in terms of performance. adaboost is the classifier that gave the best result on nouns in  yoong and hwee  1   best s1  mihalcea and moldovan  1  refers to the bestperforming system on nouns among the senseval-1 workshop systems. these results show that our systems' performance is 

natural language 	1 


table 1. example results on a few words. f = flat  h = hierarchical. 
comparable to that of state-of-the-art ones and that our hierarchical model trained on background and specific data outperforms the flat one. 
   results for a few individual words are presented in table 1. they show that the improvements are not uniform  but vary from word to word. overall we identified 1 nouns. the great majority of these are compounds that typically occur only once in the test data. both systems achieve approximately the same score on these data. on the bulk of the test data  however  the systems perform differently. of the 1 test words on which the classifiers achieve different scores  the hierarchical perceptron is more accurate than the flat one on 1 words  or 1% of the time. 
　this finding suggests a simple improvement for the hierarchical system. the contribution of the individual components of the classifier could be weighted setting the weights  after training  using held out data. in the simplest setting binary weights could be used; e.g.  either the background information is used or not. thus the background model would be used only when useful  otherwise its contributions would be ignored. 
1 	conclusion 
we have presented a learning architecture for lexical semantic classification that supplements task-specific training data with background data encoding general  world knowledge  extracted from a widely used broad-coverage  machinereadable dictionary. the model integrates task-specific and general information through a novel hierarchical learning architecture based on the multiclass perceptron. experiments on a word sense disambiguation task showed that the hierarchical model achieves improved performance over a state-ofthe-art standard  flat  system. 
   this new framework has a number of promising extensions. additional accuracy gains are expected by using more sophisticated perceptron learning algorithms such as the voted perceptron  freund and schapire  1  and by using the dual perceptron with non-linear kernels. we have only made use of the simplest possible form of hierarchy  twostage   in reality the hierarchical structure of wordnet is very complex and much more informative. the model presented here can be extended to include this type of structure as well as other sources of information. in addition  the two-layer model can be applied to all other open-class words in wordnet and a ftill-hierarchy-based model could be applied to verbs and nouns. there is also more information to extract from wordnet  for example  from the glosses  which can potentially be utilized as additional training data. lastly  the ideas we presented here might be used with other learning methods. we leave these topics for future research. 
references 
 charniak  1  e. charniak. a maximum-entropy-inspircd parser. in proceedings of the 1st meeting of the north american chapter of the association for computational linguistics  naacl-1  1. 
 ciaramita  1  m. ciaramita. boosting automatic lexical acquisition with morphological information. in proceedings of the workshop on unsupervised lexical acquisition  acl-1  1. 
 clark and weir  1  s. clark and d. weir. class-based probability estimation using a semantic hierarchy. computational linguistics  1  1. 
 collins  1  m. collins. ranking algorithms for named-entity hxtraction: boosting and the voted perceptron. in proceedings of the 1th meeting of the association for computational linguistics  acl-1  1. 
 crammer and singer  1  k. crammer and y. singer. ultraconscrvative on line algorithms for multiclass problems. technical report  1   school of computer science and engineering  hebrew university  jerusalem  israel  1. 
 dumais and chen  1  s. dumais and h. chen. hierarchical classification of web content. in proceedings ofsigir-1  1rd 
acm international conference on research and development in information retrieval  1. 
 fellbaum  1  c. fcllbaum. wordnet: an electronic lexical database. mit press  cambridge  massachusetts  1. 
 freund and schapire  1  y. freund and r.e. schapire. large margin classification using the perceptron algorithm. machine learning  1  1. 
 hearst  1  m. hearst. automatic acquisition of hyponyms from large text corpora. in proceedings of the nth international conference on computational linguistics  1. 
 hofmann ct al.  1  t. hofmann  i. tsochantaridis and y altun. learning over discrete output spaces via joint kernel functions. in advances in neural information processing systems  workshop on kernel methods  1. 
 light and grcif  1  m. light and w. greitt. statistical models for the induction and use of selectional p