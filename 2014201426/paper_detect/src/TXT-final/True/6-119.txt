 
this paper describes an agent-based evolutionary computing technique called grael  grammar evolution   that is able to perform different natural language grammar optimization and induction tasks. two different instantiations of the graelenvironment are described in this paper: in grael1 large annotated corpora are used to bootstrap grammatical structure in a society of agents  who engage in a series of communicative attempts  during which they redistribute grammatical information to reflect useful statistics for the task of parsing. in grael-1  agents are allowed to mutate grammatical information  effectively implementing grammar rule discovery in a practical context. a combination of both grael-1 and grael-1 can be shown to provide an interesting all-round optimization for corpus-induced grammars. 
1 introduction 
evolutionary computing has seen many interesting applications on a broad range of research domains over the years. its ability to overcome the problem of local maxima in finding a solution to a particular problem  by recombining and mutating individuals in a society of possible solutions  has made it an attractive technique for problems involving large  complicated and non-linearly divisible search spaces. the evolutionary computing paradigm has however always seemed reluctant to deal with natural language syntax  probably because it is essentially a recursive  non-propositional system  dealing with complex issues such as long-distance dependencies and constraints. this has made it difficult to incorporate it in typically propositional evolutionary systems such as genetic algorithms. 
　a limited amount of ga-related syntactic research has focused on linguistic data  smith and witten  1; wyard  1; antonisse  1; araujo  1   but none of these systems are suited to a generic  treebank  grammar optimization task  mainly because the grammatical formalism and evolutionary processes underlying these systems are designed to fit a particular task  such as information retrieval  losee  1 . other work on syntax in the evolutionary computing paradigm has either been involved in studying the origins of grammar in a computational context  batali  1; kirby  1j or the co-ordinated co-evolution of grammatical principles  briscoe  1 . yet so far  little or no progress has been achieved in evaluating evolutionary computing as a tool for the induction or optimization of data-driven parsing techniques. 
　the grael1 environment provides a suitable framework for the induction and optimization of any type of grammar for natural language in an evolutionary setting. in this pa-
per we hope to provide an overview of grael as a grammar optimization and induction technique. we will first outline the basic architecture of the grael environment in section 
1 on the basis of a toy example. next  we introduce grael1  section 1  as a grammar optimization technique that can enhance corpus-induce grammars. by adding an element of mutation in grael-1 we implement a method to extend the coverage of a corpus-induced grammar. we will also describe a combination of both grael-1 and grael-1 which can be shown to provide an interesting all-round optimization technique for corpus-induced grammars. 
1 grael - grammar evolution 
a typical grael society contains a population of agents in a virtual environment. each of these agents holds a number of structures that allows it to produce sentences as well as induce a probabilistic grammar to analyze other agents' sentences. during an extended series of error-driven inter-agent interactions  these grammars are updated over time. while the evolutionary computing approach of grael is able to define the quality of the grammars that are developed over time  the agent-based aspect of grael ensures that the grammar optimization is grounded in the practical task of parsing itself. from an engineering point of view  grael provides a general framework for grammar optimization and induction  but from a more theoretical point of view  grael can also help us to understand the dynamics of grammar emergence and evolution over time. 
　in the data-driven grael experiments described in this paper  the grammatical knowledge of the agents in the society is bootstrapped by using an annotated natural language cor-
pus  marcus et al.  1 . at the onset of such a data-driven 
'grammar evolution 
　

grael society  the syntactic structures of a treebank are randomly distributed over the agents  so that each agent holds a number of tree-structures in memory. these structures enable them to generate sentences  as well as provide grammars that allow them to analyze other agents' sentences. 
　the actual interaction between agents is implemented in language games: an agent  agl  presents a sentence to another agent  ag1 . if ag1 is able to correctly parse agl's sentence  the communication is successful. if on the other hand  ag1 is lacking the proper grammatical information to parse the sentence correctly  agl shares the necessary information for ag1 to arrive at the proper solution. 
　figure 1 displays a toy example of such a language game. in this example  a  treebank  of two structures has been distributed over a society of two agents. the two agents engage in a language game  in which agl presents the sentence  1 offered some bear hugs  to ag1 for parsing. at this point in time  ag1's grammar does not contain the proper grammatical information to interpret this sentence the way agl intended and so ag1 will return an incorrect parse  even though it is consistent with its own grammar. 
　consequently  agl will try and help ag1 out by revealing the minimal correct substructure of the correct parse that should enable ag1 to arrive at a better solution. this information is incorporated in ag1's grammar  who will try to parse the sentence again with the updated knowledge. when ag1 is able to provide the correct analysis  or is not able to after a certain number of attempts  either agl's next sentence will be parsed  or two other agents in the grael society will be randomly selected to play a language game. 
　these interactions  which introduce a concept of errordriven knowledge sharing  extend the agents' grammars fast  so that the datasets can grow very large in a short period of time. a generation-based grael society can be used to allow the society to purge itself of bad agents and build new generations of good parser agents  who contain a fortuitous distribution of grammatical knowledge. this involves the use of fitness functions that can distinguish good agents from bad ones. for a full overview of all the evolutionary parameters in the grael environment  many of which have a significant impact on processing  we would like to refer to  dc pauw  1 . in this paper  we will describe the most relevant subset of experiments that allows us to evaluate grael as a grammar induction and optimization technique. 
1 	grael-1: probabilistic grammar optimization 
historically  most syntactic parsers for natural language have made use of hand-written grammars  consisting of a laboriously crafted set of grammar rules. but in recent years  a lot of research efforts employ annotated corpora to automatically induce grammars  bod  1; collins  1; de pauw  1 . yet  data-analysis of the output generated by these parsers still brings to light fundamental limitations to these corpus-based methods  klein and manning  1 . even though generally providing a much broader coverage than hand-built grammars  corpus-induced grammars may still not hold enough grammatical information to provide parses for a large number of sentences  as some rules that are needed to generate the correct tree-structures are not induced from the original corpus  cf. section 1 . 
　but even if there were such a thing as a full-coverage corpus-induced grammar  performance would still be limited by the probabilistic weights attributed to its rules. a typical data-driven parser provides a huge collection of possible parses for any given sentence. fortunately  we can also induce useful statistics from the annotated corpus that provides a way to order these parse forests to express a preference for a particular parse. these statistics go a long way in providing well ordered parse forests  but in many other cases  it can be observed that the ranking of the parse forest is sometimes counter-intuitive in that correct constructs are often overtaken by obviously erroneous  but highly frequent structures. 
　this can easily be explained by the inherent nature of corpus-based grammars: the initial probabilistic values attached to the grammar rules induced from the annotated data are equal to their relative frequency in the corpus. it might be the case however that  even though they are directly induced from the annotated corpus  the probabilities of these rules are not suited to the disambiguation task as yet. it may therefore be useful to have the parser use the grammar to practice the task of parsing and adjust the probabilistic weights of particular structures according to these test cases. we then need to consider the initial grammar as basic raw material in need of optimization  as it is merely a reflection of the original data set and is not yet optimized for the task of parsing  unseen  sentences itself. 
　typical methods of probabilistic grammar optimization include  among others  bagging and boosting  henderson and brill  1; collins  1   re-estimation of the constituents probabilities  goodman  1; charniak  1  and including extra information sources  belz  1; collins  1 . but we propose an agent-based evolutionary computing method to resolve this issue. grammar optimization using a grahl environment is in this vein related to the aforementioned bagging approach to grammar optimization  albeit with some notable differences. by distributing the knowledge over a group of agents and having them interact with each other  we basically create a multiple-route model for probabilistic grammar optimization. grammatical structures extracted from the training corpus  will be present in different quantities and variations throughout the grael society. while the agents interact with each other and in effect practice the task on each other's grammar  a varied range of probabilistic grammars are optimized in a situation that directly relates to the task at hand. the evolutionary aspects of the system make sure that  while marginally useful grammatical information is down-toned  common constructs arc enforced  providing a better balanced model for statistical parsing. 
　the way grael accomplishes a re-distribution of the original probabilistic values is by using the default grael architecture described in section 1. this type of error-driven learning makes sure that mistakes are being dealt with by transferring difficult grammatical constructs  thereby increasing their probabilistic value in the other agent's grammar. this probabilistic adjustment will be taken into account during subsequent parsing attempts by this agent  hopefully triggering the correct grammatical structure in the future. 
1 	experimental setup 
the overall setup of the grael experiments is displayed in figure 1. baseline accuracy is measured by directly inducing a grammar from the training set to power parser 1  which disambiguates the test set. this grammar takes on the form of a pmpg as outlined in  de pauw  1   cf. infra . the same training set is also randomly and equally distributed over a number of agents in the grael society  who will consequently engage in a number of language games. at some point  established by the halting procedure  cf. infra   the society is halted and the fittest agent is selected from the society. this agent effectively constitutes a redistributed and probabilistically optimized grammar  which can be used to power parser 1. grael-1 accuracy is achieved by having this parser disambiguate the same test set. 
　we used two data sets from the penn treebank  marcus et al.1 . the main batch of experiments was conducted on the small  homogeneous atls-corpus  which consists of a collection of annotated sentences recorded by a spokendialogue system. the larger wall street journal corpus  henceforth wsj   a collection of annotated newspaper articles  was used to test the system on a larger scale corpus. the common division between training set  section 1  and test set  section 1  was used. semantically oriented flags and numeric flags indicating internal relations were removed to allow for more streamlined syntactic processing. 
figure 1: comparing parsers: baseline parser and grael 
　for syntactic processing  the agents use the parsing system pmpg described in  de pauw  1   which integrates a cky parser  chappelier and rajman  1  and a parse forest ranking scheme that employs probabilistic information as well as a memory-based operator to maximize for each parse the number of nodes that can be retrieved from memory. a pmpg takes the form of a simple pcfg-type grammar  enriched with numerical indices that encode contextual information previously observed in a treebank. this memorybased instantiation of data-oriented parsing  bod  1  ensures that larger syntactic structures are used as the basis for parsing  with a minimal loss of computational efficiency over regular pcfgs. the pmpg approach can therefore be considered to introduce a psycho-linguistically relevant memorybased operator in the parsing process. 
　the full experimental run varied society sizes  generation methods  fitness functions and halting procedures  de pauw  1 . the subset of experiments described in this paper employed the sexual procreation method to introduce new generations by combining the grammars of two fit agents in the society to create new generations of parser agents. the fitness of an agent is defined by recording a weighted average of the f-score during inter-agent communication  also see figure 1  and the f-score of the agent's parser on a held-out validation set. this information was also used to try and halt the society at a global maximum and select the fittest agent from the society. for computational reasons  the experiments on the wsj-corpus were limited to two different population sizes and used an approximation of grael that can deal with large datasets in a reasonable amount of time. the test set was not used in any way during actual grael-processing in agreement with blind-testing procedures. 
　

table 1: baseline vs grael-1 results 
　
1 	results 
table 1 displays the exact match accuracy and f-scores for the baseline model  a standard pmpg parser using a grammar induced from the training set  cf. figure 1 . it also displays scores of the grael system for different population sizes. we notice a significant gain for all grael models over the baseline model on the atis corpus  but increasing population size over 1 agents seems to decrease exact match accuracy on the atis corpus. likewise  the small society of 1 agents achieves only a very limited improvement over the baseline method. data analysis showed that the best moment to halt the society and select the fittest agent from the society  is a relatively brief period right before actual convergence sets and grammars throughout the society are starting to resemble each other more closely. the size of the the society seems to be the determining factor controlling the duration of this period. 
　preliminary tests on a subset of the wsj corpus had shown that society sizes of 1 agents and less to be unsuitable for a large-scale corpus  again ending up in a harmful premature convergence. the gain achieved by the grael society is less spectacular than on the atis corpus  but it is still statistically significant. larger society sizes and full grael processing on the wsj corpus should achieve a more significant gain. 
　the experiments do show however  that grael-1 is indeed an interesting method for probabilistic grammar redistribution and optimization. data analysis shows that many of the counter-intuitive parse forest orderings that were apparent in the baseline model  are being resolved after grael1 processing. it is also interesting to point out that we are achieving an error reduction rate of more than 1% over the baseline method  without introducing any new grammatical information in the society  but solely by redistributing what is already there. these experimental results indicate that annotated data can indeed be considered as raw material that can be optimized for the practical use of parsing unseen data. 
　 de pauw  1  also describes experiments that directly compare grael to the similar methods of bagging and boosting  henderson and brill  1   which are summarized in the bottom two lines of table 1. bagging and boosting were shown to obtain significantly lower accuracy figures on almost all accounts. only the f-score for the bagging experiment exceeded that of the optimal grael configuration  but this can be attributed to the fact that an approximation of grael was used for full processing on the extensive wsj dataset. 
1 	grael-1: grammar rule discovery 
the functionality of grael-1 can be extended by only applying minor alterations to the grael system. with grael-1 we wish to provide a grammar rule discovery method which can deal with the problem of grammar sparseness. handwritten and corpus-induced grammars alike have to deal with the fundamental issue of coverage.  collins  1  for example reports that when using sections 1 of the wsj-corpus as a training set and section 1 as a test set  1% of the sentences in the test set require a rule not seen in the training set. even for a large corpus such as the wsj  sparse grammar is indeed a serious accuracy bottleneck. 
　it would therefore be useful to have a method that can take a corpus-induced grammar and extend it by generating new rules. but doing so in a blind manner  would provide huge  over-generating grammars  containing many nonsensical rules. the grael-1 system described in this section  involves a distribute d approach to this type of grammar rule discovery. the original  sparse  grammar is distributed among a group of agents  who can randomly mutate the grammatical structures they hold. the new grammatical information they create is tried and tested by interacting with each other. the neo-darwinist aspect of this evolutionary system will make sure that any useful mutated grammatical information is retained throughout the population  while noise is filtered out over time. this method provides a way to create new grammatical structures previously unavailable in the corpus  while at the same time evaluating them in a practical context  without the need for an external information source. 
　to accomplish this  we need to implement some minor alterations to the grael-1 system. the most important adjustment occurs during the language games. we refer back to the toy example of the language game in figure 1 to the point where agl suggests the minimal correct substructure to ag1. in grael-1 this step introduced a form of error-driven learning  making sure that the probabilistic value of this grammatical structure is increased. the functionality of grael-1 however is different: in this step  we assume that there is a noisy channel between agl and ag1 which may cause ag1 to misunderstand agl's structure. small mutations on different levels of the substructure may occur  such as the deletion  addition and replacement of nodes. this effectively introduces previously unseen grammatical data in the grael society  which 
　
will consequently be optimized over time. 
　preliminary experiments however showed that this does not work as such  since the newly created structures were largely being ignored in favor of the gold-standard corpus structures. we therefore implemented another alteration to the grael-1 system. instead of just presenting the tree-structure originally assigned by the training set  we now require agl to parse the string-only sentence using the grammar acquired during language games  replacing the tree-structure from the training set  with a possibly different tree-structure that incorporates some of the mutated information. this alteration makes sure that the mutated grammatical structures are actively being used  so that their usefulness as grammatical constructs can be measured in a practical context. 
experimental setup and results 
the setup for the grael-1 experiments is the same as for the grael-1 experiments  cf. figure 1 . to test the grammarrule discovery capabilities of grael-1 we have compiled a special worst-case scenario test set for the atis corpus  consisting of the 1 sentences in the atis that require a grammar rule that cannot be induced from the training set. for the wsj-experimentthe normal test set was used. a 1-agent and a 1-agent society were respectively used for the atis and wsj experiments. 
　the baseline and grael-1 methods for the atis experiments trivially have an exact match accuracy of 1%  which also has a negative effect on the f-score  table 1 . grael-1 is indeed able to improve on this significantly  proving that it is indeed an effective grammar rule discovery method. dataanalysis shows however that it has lost the beneficial probabilistic optimization effect of grael-1. 
　we therefore performed another experiment  in which we turned the grael-1 society into a grael-1 society after the former's halting point. in other words: we take a society of agents using mutated information and consequently apply grael-1 's probabilistic redistribution properties on these grammars. figure 1 shows the course of the grael1 experiment. in this figure we see the f-scores recorded during inter-agent communicative attempts. after an almost linear increase during grael-1 processing  the society is halted after an extended period of convergence. next  grael-1 processing resumes  which negatively affects f-scores for a brief period of time  until the society reconverges. even though the f-scores do not seem to improve over those observed before the transition to grael-1  table 1 shows that the fittest agent selected from the society after the transition performs better on the held-out test set. the results on the wsj-corpus are also figure 1: grael1 experiment - f-scores during language games 
interesting in this respect. grael-1 outperforms grael-1 on this data set  but the combination of the two seems to be quite beneficial for parsing accuracy. 
　evaluating a grammar rule discovery method poses an empirical problem in that it can never be clear what grammar rules are missing until we actually need them. the test set we compiled to perform the grael-1 atis-experiments goes a long way in providing a touchstone to see how well grael1 performs as a supervised grammar induction method. and results indicate it performs quite well: mutated information becomes available that is able to create parses for difficult constructions  while the number of structures that constitute noise is limited and is attributed a small enough portion of the probability mass as not to stand in the way of actual useful mutated structures. 
1 	concluding remarks 
this paper has presented one of the first research efforts that introduces agent-based evolutionary computing as a machine learning method for data-driven grammar optimization and induction. in recent years  many researchers have employed ensemble methods to overcome any negative bias their training data might impose on their classifiers. it is indeed important to view  annotated  data  not as an optimally distributed set of examples but as raw material that needs to be pre-processed before it can be used by a machine learning classifier. the bagging and boosting approach for instance  tries to create resamplings of the original data  to overcome the local maxima the data might restrict the classifier to  but we believe grael adds an extra dimension to the task: by splicing the data and incorporating it in a society of communicating agents  we allow for the parallel development of several grammars at once  enhanced in a practical context that mirrors the goal itself: parsing unseen data. 
　we described two instantiations of the grael environment. the basic grael-1 system aims to provide a beneficial re-distribution of the probability mass of a probabilistic grammar. by using a form of error-driven learning in the course of language games between agents  probabilistic values are adjusted in a practical context. this optimizes the grammars for the task of parsing data  rather than reflecting the probability mass of the initial data set. this method favorably compares to established grammar optimization methods like bagging and boosting. 
by adding an element of mutation to the concept of 
　
grabl-1 we were able to extend its functionality and experiment on graf.l-1 as a grammar rule discovery method. the results showed that grael-1 could produce a broadercoverage grammar  but that grahl-1 's ability to optimize the distribution of the probability mass of a grammar was counteracted. a grammar obtained from a graiil-1 society is therefore unsuited to be directly applied to parsing. but combining it again with a grael-1 society however  goes a long way in resolving this issue  providing a grammar that has a broader coverage  as well as a better tuned probability mass distribution over the structures contained therein. we therefore believe the combination of grael-1 and grael-1 to be an interesting optimization toolkit for any given grammar  and corpus-induced grammars in particular. it can achieve a significant optimization over the baseline method  without using an external information source  simply on the basis of knowledge transfer and mutation in a practical and evolutionary context. 
acknowledgments 
the research described in this paper was financed by the fwo  fund for scientific research . 
