 
　　　models of complex phenomena often consist of hypothetical entities called  hidden causes   which cannot be observed directly and yet play a major role in understanding  communicating  and predicting the dynamics of those phenomena. this paper examines the cognitive and computational roles of these constructs  and addresses the question of whether they can be discovered from empirical observations. 
　　　causal models are treated as trees of binary random variables where the leaves are accessible to direct observation  and the internal nodes-representing hidden causes-account for inter-leaf dependencies. in probabilistic terms  every two leaves are conditionally independent given the value of some internal node between them. 
　　　we show that if the mechanism which drives the visible variables is indeed tree-structured  then it is possible to uncover the topology of the tree uniquely by observing pair-wise dependencies among the leaves. the entire tree structure  including the strengths of all internal relationships  can be reconstructed in time proportional to nlogn  where n is the number of leaves. 
i. introduction: causality  conditional independence and trees 
　　　this study is motivated by the observation that human beings  facing complex phenomena  exhibit an almost obsessive urge to conceptually mold these phenomena into structures of cause-and-effect relationships. this tendency is  in fact  so compulsive that it sometimes comes at the expense of precision and often requires the invention of hypothetical  unobservable entities such as  ego    elementary particles   and  supreme beings  to make theories fit the mold of causal schema. when we try to explain the actions of another person  for example  we invariably invoke abstract notions of mental states  social attitudes  beliefs  goals  plans and intentions. medical knowledge  likewise  is organized into causal hierar-
 this work was supported in part by the national science 
foundation  grant #dsr 1 
chies of invading organisms  physical disorders  complications  pathological states  and only finally  the visible symptoms. 
　　　this paper takes the position that human obsession with causation is computationally motivated. causal models are only attractive because they provide effective data-structures for representing empirical knowledge  and their effectiveness is a result of the high degree of decomposition they induce. more specifically  causes are viewed as names given to auxiliary variables which encode a summary of the interaction between the visible variables and  once calculated  would permit us to treat visible variables as if they were mutually independent. 
　　　the dual summarizing-deoampoaing rale of a causal variable is analogous to that of an orchestra conductor; it achieves coordinated behavior through central communication and thereby relieves the players from having to communicate directly with each other. such coordination is characteristic of tree structures and draws its effectiveness from the local nature of the data flow topology. in a management hierarchy  for example  where employees can only communicate with each other through their immediate superiors  the passage of information is swift  economical  conflict-free  and highly parallel. these computational attributes  we postulate  give rise to the satisfying sensation called  in-depth understanding   which people experience when they discover causal models consistent with observations. 
　　　cast in probabilistic terms  central decomposition is embodied by the relation of conditional independence  which we claim constitutes the most universal and distinctive characteristic featured by the notion of causality.  see also  and .  in medical diagnosis  for example  a group of co-occurring symptoms often become independent of each other once we know the disease that caused them. when some of the symptoms directly influence each other  the medical profession invents a name for that interaction  e.g.  complication  pathological state  etc.  and treats it as a new auxiliary variable which again assumes the decompositional role characteristic of causal agents. knowing the exact state of the auxiliary variable renders the interacting symptoms independent of each other. causes invoked to explain human behavior  such as motives and intentions  also induce conditional in-
　　　
1 	j. pearl 
dependence. for example  once a murder suspect confesses to having wished the death of the victim  testimonies proving that he expressed such wishes in public or that he stood to gain from the victim's death are per* ceived to be irrelevant; they shed no further light on whether he actually performed the murder. 
　　　based on these observations we chose to represent causal models as trees of binary random variables  where the leaves are directly accessible to empirical observations and the internal nodes represent hidden causes; any two leaves become conditionally independent once we know the value of some internal variable on the path connecting them. the propagation of updated probabilities in such trees was analyzed by pearl  and kim and pearl . it was shown that the propagation can be accomplished by a network of parallel processors working autonomously  and that the impact of new information can be imparted to all variables in time proportional to the longest path in the tree. 
　　　given that tree-dependence captures the main feature of causation and that it provides a convenient computational medium for performing updating and predictions  we now ask whether the internal structure of the tree can be determined from observations made solely on the leaves. if it can  then the structure found would constitute an operational definition for the hidden causes. additionally  if we take the view that  learning  entails the acquisition of computationally effective representations for nature's regularities  then the procedure of configuring the tree may reflect an important component of human learning. 
　　　a related structuring task was treated by chow and liu   who also used tree-dependent random variables to approximate an arbitrary joint distribution. however  whereas in chow's trees all nodes denote observed variables  the internal nodes in our trees denote dummy variables  artificially concocted to make the representation tree-like. the problem of configuring probabilistic models using auxiliary variables is mentioned by hinton et al.  as one of the tasks that a boltzmann machine should be able to solve. however  no performance results have been reported and it is not dear whether the relaxation techniques employed by the boltzmann machine can easily handle the restriction that the resulting structure be a tree. 
　　　this paper is organized as follows: section 1 presents nomenclature and precise definitions for the notions of star-decomposability and tree-decompotability. in section 1 we treat triplets of random variables and ask under what conditions one is justified in attributing the observed dependencies to one central cause. we show that these conditions are readily testable and  when the conditions are satisfied  that the parameters specifying the relations between the visible variables and the central cause can be determined uniquely. in section 1 
　　　we extend these results to the case of a tree with n leaves. we show that if a joint distribution of n variables has a tree-dependent representation  then the uniqueness of the triplets' decomposition enables us to configure that tree from pair-wise dependencies among the variables. moreover  the configuration procedure takes only 1 nlogn  steps. in section 1 we evaluate the merits of this method and address the difficult issues of estimation and approximations. 

　　　
	j. pearl 	1 

　　　
1 	j. pearl 
that induced by their dependencies cm the third variable; a mechanism accounting for direct dependencies must be 
	the 	criterion 	for 	star-
decomposability we may address a related problem: suppose p is not star-decomposable  can it be approximated by a star-decomposable distribution p that has the same second-order probabilities  
     the preceding analysis contains the answer to this question. note that the 1rd order dependencies are represented only by the term  and this term is confined by eq.  1  to a region whose boundaries are determined by 1nd- order parameters. thus  if we insist on keeping all 1nd-order dependencies of p in tact and are willing to choose so as to yield a stardecomposable distribution  we can only do so if the region circumscribed by  1  is non-empty. this leads to the statement: 
theorem 1: a necessary and sufficient condition for the 1nd order dependencies among the triplet x1 x1 x1 to support a star-decomposable extension is that the six ine-
 1  
iv. a tree-reconstruction procedure 
　　　we are now ready to confront the central problem of this paper: given a tree-decomposable distribution can we recover its underlying topology and 
tree-distribution 
　　　the construction method is based on the observation that any three leaves in a tree have one and only one internal node that can be considered their center  
i.e.  it lies on all the paths connecting the leaves to each other. if one removes the center  the three leaves become disconnected from each other. this means that if p is tree-decomposable then the joint distribution of any triplet of variables is star-decomposable  i.e.  
　　　　i uniquely determines the parameters a fi  tgi as in equations  1    1   and  1   where is the marginal probability of the central variable. moreover  if we compute the star decompositions of two triplets of leaves  both having the same central node w  the two distributions should have the same value for . this provides us with a basic test for verifying whether two arbitrary triplets of leaves share a common center and a 
successive application of this test is sufficient for determining the structure of the entire tree. 
of leaves in r. these 
leaves are interconnected through one of the four possi-
　　　
j. pearl 1 
ble topologies shown in figure 1. the topologies differ in the identity of the triplets which share a common 
center. for example  in the topology of figure 1 a   the pair   1 1    1 1   share a common center and so does the pair   1 1    1 1  . in figure 1 b   on the other hand  the sharing pairs are   1 1    1 1   and   1 1    1 1    and in figure 1 d  all triplets share the same center. thus  the basic test for center-sharing triplets enables us to decide the topology of any 1-tuple and  eventually  to configure the entire tree. 
　　　
1 	j. pearl 
v. conclusions and open questions 
　　　this paper provides an operational definition for entities called  hidden causes   which are not directly observable but facilitate the acquisition of effective causal models from empirical data. hidden causes are viewed as dummy variables which  if held constant  induce probabilistic independence between sets of visible variables. it is shown that if all variables are bi-valued and if the activities of the visible variables are governed by a treedecomposable probability distribution  then the topology of the tree can be uncovered uniquely from the observed correlations between pairs of variables. moreover  the structuring algorithm requires only nlogn steps. 
　　　the method introduced in this paper has two major shortcomings: it requires precise knowledge of the correlation coefficients and it only works when the underlying model is tree-structured. in practice  we often have only sample estimates of the correlation coefficients  and it is therefore unlikely that criteria based on equalities  as in eq.  1   will ever be satisfied exactly. it is possible  of course  to relax these criteria and make topological decisions on the basis of proximities rather than equalities. for example  instead of searching for an equality  we can decide the 1-tuple topology on the basis of the permutation of indices that minimizes the difference pypu - pitpji* experiments show  however  that the structure which evolves by such a method is very sensitive to inaccuracies in the estimates pv  because no mechanism is provided to retract erroneous decisions made in the early stages of the structuring process. ideally  the topological membership of the  i+l th leaf should be decided not merely by its relations to a single triplet of leaves chosen to represent an internal node wf but also by its relations to all previously structured triplets which share w as a center. this  of course  will substantially increase the complexity of the algorithm. 
　　　similar difficulties plague the task of finding the best tree-structured approximation to a distribution which is not tree-decomposable. even though we argued that natural data which lend themselves to causal modeling should be representable as tree-decomposable distributions  these distributions may contain internal nodes with more than two values. the task of determining the parameters associated with such nodes is much more complicated and  in addition  rarely yields unique solutions. unique solutions  as shown in section 1  are essential for building large structures from smaller ones. we leave open the question of explaining how approximate causal modeling  an activity which humans seem to perform with relative ease  can be embodied in computational procedures that are both sound and efficient. 
