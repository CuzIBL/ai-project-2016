 
we overview the development of first-order automated reasoning systems starting from their early years. based on the analysis of current and potential applications of such systems  we also try to predict new trends in first-order automated reasoning. our presentation will be centered around two main motives: efficiency and usefulness for existing and future potential applications. 
　this paper expresses the views of the author on past  present  and future of theorem proving in first-order logic gained during ten years of working on the development  implementation  and applications of the theorem prover vampire  see  riazanov and voronkov  1a . it reflects our recent experience with applications of vampire in verification  proof assistants  theorem proving  and semantic web  as well as the analysis of future potential applications. 
1 theorem proving in first-order logic 
the idea of automatic theorem proving has a long history both in mathematics and computer science. for a long time  it was believed by many that hard theorems in mathematics can be proved in a completely automatic way  using the ability of computers to perform fast combinatorial calculations. the very first experiments in automated theorem proving have shown that the purely combinatorial methods of proving firstorder theorems are too week even for proving theorems regarded as relatively easy by mathematicians. 
　provability in first-order logic is a very hard combinatorial problem. first-order logic is undecidable  which means that there is no terminating procedure checking provability of formulas. there are decidable classes of first-order formulas but formulas of these classes do not often arise in applications. due to undecidability  very short formulas may turn out to be extremely complex  while very long ones rather easy. sometimes first-order provers find proofs consisting of several thousand steps in a few seconds  but sometimes it takes hours to find a ten-step proof. the theory of first-order reasoning is centered around the completeness theorems while in practice completeness is often not an issue due to the intrinsic * partially supported by a grant from epsrc. 
invited speakers 
complexity of first-order reasoning and is often compromised in favor of a faster inference mechanism. 
　however  despite the complexity of theorem proving in first-order logic  an impressive progress in the area has been achieved in recent years. this progress is due to several factors: development of theory  implementation techniques  and experience with applications. modern theorem provers are very powerful programs able to find in a few seconds very complex combinatorial proofs which would require several man-month to be solved by mathematicians. many ideas developed in theoretical papers in this area are on their way to implementation. there is extensive research on aspects of theorem proving vital for applications  such as proofchecking and interfacing with proof assistants. the next generation of theorem provers will incorporate some of these ideas and become even more powerful tools increasingly used for applications which are currently beyond their reach. 
　the aim of this paper is to give a brief overview of firstorder provers and point out new trends in theorem proving which are likely to be implemented in theorem provers of the next generation. 
　currently  theorem provers are used in the following way. the user specifies a problem by giving a set of axioms  a set of first order formulas or clauses  and a conjecture  again  a first-order formula or a set of clauses . if the input is given by first-order formulas  the prover should check whether the conjecture logically follows from the axioms. if the input is given by a set of clauses  the prover should check whether the set of clauses is inconsistent. in either case  for many applications it is desirable that the prover output a proof  if the logical consequence or inconsistency has been established. the proofs should either be human-readable  for example  when the provers are used for proving theorems in mathematics   or machine-checkable  for example  when provers are used as subsystems of proof assistants or verification systems . 
1 applications 
the main application area of theorem provers has been  and continues to be  verification of software and hardware. full applications of this kind usually cannot be directly represented in the first-order form  so provers are normally used to prove subgoals generated by other systems  for example vhdl-to-first-order transformation systems or proof assistants based on higher-order logic or type theories. there are 
too many papers on this subject to be mentioned here. finitestate model checkers and interactive proof assistants are currently prevailing in verification  but with the growing complexity of hardware first-order logic and its extensions are likely to play an increasingly important role. 
　theorem proving in mathematics has been the first application area for theorem provers. provers are not very good at working in structured mathematical theories but they are very efficient in fields of mathematics where combinatorial reasoning is required  for example  in algebra. applications of provers in algebra are numerous. the monograph  mccune and padmanabhan  1  gives an example of how algebra can be developed with the help of a theorem prover. 
　symbolic computation and computer algebra systems need help from theorem provers  for example  when side conditions for applying a simplification rule have to be checked. however  these systems are already very complex and either do not use provers at all or use very primitive ones. interfacing theorem provers and symbolic computation systems is becoming an important application of theorem provers. 
　knowledge bases play a significant role in the semantic web project. some of these knowledge bases use first-order logic or its simple extensions  see  for example   ieee suokif project  1 . 
　teaching logic and mathematics is an area where firstorder provers can serve as valuable tools. the first experiments in teaching mathematics using first-order provers are reported in  mcmath et al.  1 . 
　other application of first-order provers include retrieval of software components  schumann and fischer  1   reasoning in non-classical logics  e.g.  fde nivelle et al.  1    and program synthesis  stickel et ai  1 . first-order theorem provers arc in practice used much more than it is reported in the literature  since many companies and organizations do not disclose proprietary use of provers. 
1 	modern theorem provers 
the main currently supported systems are given in figure 1. 
among these systems  setheo is based on model elimination  while other provers are based on resolution. in the rest of this paper we will overview resolution theorem provers only. 
　in the early papers  e.g.   robinson  1; robinson and wos  1  resolution is defined as a logical system consisting of several inference rules operating on clauses  for example  resolution and paramodulation: 

where mgu denotes the most general unifier. since these rules are local  i.e.  their applicability is identified only by a small number of clauses and the result of a rule application does not change the previously generated clauses  resolution is implemented using saturation algorithms. 
　these algorithms operate on a set of clauses 1  initially the set of input clauses. roughly speaking  they are based on the following loop. 
1. apply inferences to clauses in 1  adding to 1 the conclusions of these inferences. 
1. if the empty clause  is proved  terminate with success. if no inference rule is applicable  terminate with failure. 
the set s is the current search space. however  a simple implementation of this loop would hardly solve even some of the problems considered trivial by modern theorem provers due to the fast growth of the search space. already in the first paper on resolution  robinson  1j it was noted that some clauses can be removed from the search space without losing completeness. 
　more precisely  it has been observed that clauses subsumed by other clauses can be removed from the search space without losing completeness and thus regarded as redundant. later  several other notions of redundancy were discovered. for example   brand  1  proved that the function reflexivity inferences and paramodulation into a variable are redundant. modern theorem provers use many redundancy criteria to prune the search space. these criteria can be divided in two categories: redundant inferences and redundant clauses. many useful notions of redundancy are based on simplification orders introduced in  knuth and bendix  1. these orders are orders  on terms which can be extended to literals and clauses. an example of an inference which is redundant due to the orderings restrictions is a paramodulation inference 
for which in the 1s-1s many notions of redundant inferences and clauses were investigated. in the 
1s a general theory of redundancies was described in  bachmair and ganzinger  1al. nearly all state-of-the-art resolution theorem provers are based on this theory. resolution with redundancy is based on the following saturation algorithm. 
1. apply all non-redundant inferences to clauses in s  adding to s those conclusions of these inferences that are non-redundant. 
1. if the empty clause  is proved  terminate with success. if no inference rule is applicable  terminate with failure. 
1. remove all clauses that become redundant due to the addition of these conclusions of inferences. 
in addition to standard inference rules  these algorithms also operate with simplifications. an inference is called a simpli-
fication if it makes at least one clause in 1 redundant. many modern provers implement an eager search for simplifying inferences  while ordinary generating inferences are applied lazily. 
　in modern provers inference selection is performed via clause selection. for this reason saturation algorithms implemented in such provers are called the given clause algorithms. there are two main concretisations of the saturation algorithm based on the clause selection: the otter algorithm  lusk  1; mccune  1  and the discount algorithm  denzinger et al  1 . these algorithms are described and analyzed in more detail in  riazanov and voronkov  1b . the simple description of these algorithms given above may create an illusion of their simplicity  but in fact  some of these algorithms are extremely complex  and problems related to 

prover affiliation references general-purpose provers otter 
setheo 
spass 
vampire 
gandalf scott 
e 
snark 
bliksem argonne national laboratory munich university of technology 
max-planck institute 
university of manchester 
tallinn university of technology 
australian national university 
munich university of technology 
sri 
max-planck institute imccune  1j 
 moser et al  1  
 weidenbach ef al.  1  
 riazanov and voronkov  1a  
 tammet  1  
 slaney et al.  1  lschulz 1  
 stickel et al.  1  
 de nivelle  1  figure 1: first-order theorem provers 


figure 1: a given clause algorithm of vampire 
the memory management for these algorithms have not been properly solved. for example  one of the given clause algorithms of vampire is schematically shown in figure 1  taken from  riazanov and voronkov  1a  . 
　even proof-search in inference systems with redundancy creates enormously large such spaces. for example  storing 1fi clauses is not unusual. of these clauses 1 can participate in inferences. some operations on clauses are very expensive if implemented naively. for example  subsumption on multi-literal clauses is np-complete and must ideally be performed between each pair of clauses in the search space. it is difficult to imagine an implementation able to perform 1 operations in reasonable time. 
　in the state-of-the-art theorem provers all expensive operations are implemented using term indexing  graf  1; sekar et al  1 . the problem of term indexing can be formulated abstractly as follows. given a set l of indexed terms or clauses  a binary relation r over terms or clauses  called the retrieval condition  and a term or clause t  called the query term or clause   identify the subset m of l that consists of the terms or clauses i such that r l  t  holds. a typical retrieval condition used in theorem proving is subsumption: retrieve all clauses in l subsumed by t. in order to support rapid retrieval of candidate clauses  we need to process the indexed set into a data structure called the index. modern theorem provers maintain several indexes to support expensive operations. for example  vampire  riazanov and voronkov  
invited speakers 
1a  uses flatterms in constant memory for storing temporary clauses  code trees  voronkov  1  for forward subsumption  code trees with precompiled ordering constraints for forward simplification by unit equalities  perfectly shared terms for storing clauses  shared terms with renaming lists for backward simplification by unit equalities  path index with compiled database joins for backward subsumption and some other indexes. 
　serious work with theorem provers requires extensive experiments. every simple modification to a prover should be tested both for bugs and for efficiency. a typical experiment with vampire consists of running it for several hours on over 1 tptp problems in a number of modes on a network of around 1 computers. such experiments require good infrastructure  both software and hardware  to facilitate debugging and evaluation  so vampire is augmented by a number of programs intended for performing large-scale experiments. the necessity of having such programs and interfacing them with vampire adds to the complexity of the system. 
1 	history of development 
since the early work on automated theorem proving  the area witnessed an impressive progress. this progress is due to several factors described below: 
1. development of theory  both of the saturation-based theorem proving with redundancy  see  bachmair and ganzinger  1; nieuwenhuis and rubio  1   and of the tableau and model elimination proof-search  see 
 hahnle  1; letz and stenz  1; degtyarev and voronkov  1 . 
1. development of implementation techniques  term indexing  new algorithms   graf  1; sekar et al  1; nieuwenhuis et al  1 . 
1. growing experience  including experiments with applications  the development of tptp  sutcliffe and suttner  
1   and the annual competitions of theorem provers casc  sutcliffe et al  1 . 
even a naively implemented theorem prover may be powerful enough to solve many problems collected in the tptp library or created automatically by proof assistants. however  there have been a considerable progress in solving difficult problems. every ten years enhancements in the theory and im-

plementation techniques resulted in a large number problems the following built-in theories arise in many application which could be solved several orders of magnitude faster than and are likely to attract attention in automated reasoning. 

by the previous generations of theorem provers. provers implemented around 1 were very inefficient  especially for equality reasoning. in 1 the paramodulation rule has been generally adopted and the general architecture of saturationbased provers understood. by 1 provers started to use routinely simplification orderings and term indexing. the provers used in 1 employ the general theory of resolution with redundancy  and in particular literal selection junctions. the provers of 1 are likely to benefit from a better clause selection  built-in theories  and maybe from parallel or concurrent architectures. 
1 future generation theorem provers 
theorem proving is a very hard problem. the next generation of theorem provers will incorporate new theory  data structures  algorithms  and implementation techniques. their development will be driven by the quest for flexibility and efficiency. flexibility is required to adapt provers to new applications. efficiency can be reformulated as controlling redundancy in large search spaces  lusk  1 . 
　it is unreasonable to expect future theorem provers to be much faster on all possible problems. however  if we can increase performance of provers by several orders of magnitude for a large number of problems coming from applications  many of these problems will be routinely solved  thus saving time for application developers. the development of next generation provers will require: 
1. development of new theory  
1. addition of new features; 
1. development of new algorithms and data structures; 
1. understanding how the theory developed so far can be efficiently implemented on top of the existing architectures of theorem provers; 
this development is impossible without considerable implementation efforts and extensive experiments. 
　there is a large number of research areas to be pursued in automated reasoning in the near future  let me mention some of them. 
　built-in theories. already in the case of equality the naive addition of equality axioms is too inefficient for solving even very simple problems. development of superposition-based equality reasoning resulted in an impressive improvement of theorem provers. there are important theories other than equality which arise in many applications. development of specialized reasoning methods for these theories will be a central problem in theorem proving. this problem is different from the problem of designing decision procedures for theories such as presburger arithmetic because relations and functions used in these theories could be used together with other relations and functions and arbitrary quantifiers. procedures for the combination of decision and unification algorithms can be of some help  nelson and oppen  1; shostak  1; baader and schulz  1; rueb and shankar  1 . 
  ac  the theory of associative and commutative func-tions . these axioms occur in axiomatizations very often. there are many results related to building-in ac in theorem provers but implementation techniques  including term indexing modulo ac  are still in their infancy. associativity and commutativity were built in the eqp theorem prover  mccune  1  with a considerable success but essentially without term indexing. term indexing modulo ac was considered in  bachmair et al  1  but only for a very special case. 
  theories of transitive relations and orders  bachmair and ganzinger  1b . 
  various first-order theories of arithmetic. 
  term algebras and other constructor-based structures. 
　as a first step toward building-in important theories one can consider creation of libraries of axioms/theorems about commonly used data types. an example of such a project is the standard upper ontology  ieee suo-kif project  1. one can also enhance theorem provers by constructs for specifying built-in theories  for example  by constraints or by additional inference rules. 
　inductively defined types. in many applications of interactive proof assistants one has to deal with inductively defined types and functions on these types. the proof assistants such as isabelle  paulson  1   hol  gordon and melham  1   coq  the coq development team  1    twelf  pfenning and schuermann  1  have facilities to define data types and functions inductively. first-order theorem provers have no such facilities. the work on building-in inductively defined types can be developed in the following directions: 
  first-order reasoning on data types given by inductive definitions. 
  first-order reasoning on functions defined over such data types. 
  limited forms of inductive reasoning. 
　working with large axiomatizations. very often theorem provers must work with large axiomatizations containing many irrelevant axioms. this is typical for applications such as reasoning with ontologies  assisting proof assistants  and verification using hierarchically defined theories. recognition of irrelevant axioms is one of the most important problems in theorem proving. many of these axioms are definitions of relations and functions in various forms. recognition of the most typical definitions and efficient work with them play a major role in the future provers. the first steps in this direction are reported in  ganzinger et al  1; afshordel et al  1; degtyarev et al  1 . 
　ontology-based knowledge representation. first-order formulations of many applied problems  if not created manually  often refer to large axiomatizations. this situation is common for knowledge-base reasoning  proof assistants  and 
theorem proving in mathematics. using these large axiomatizations and trying to extract only a small number of relevant axioms can be a hopeless task. the problem arises of structuring these large axiomatizations in a such a way that a relevant subset of them could be easily extracted automatically and that particular properties of the axiomatization can be exploited by provers. for example  provers could use a fact that a subset of axioms axiomatize properties of a relation in terms of other relations. development of knowledge representation formalisms suitable for representing large applied theories and adaptation of provers to such formalisms could become an important problem in automated reasoning. 
　proof checking. strangely enough  modern theorem provers are not always good at producing proofs. some of them print detailed resolution and paramodulation proofs but none gives a proof of the preprocessing steps  such as skolemisation. without proof-checking one cannot use provers in verification and in assisting proof assistants. we expect that in the near future proof-checking components will be added to all major first-order provers. we conjecture that the main approaches to proof-checking will be similar to those used in proof-carrying code  necula and lee  1   so that a proof is built using a number of inference rules  and foundational proof-carrying code  appel  1   so that a proof will given in a system whose language is rich enough to prove even the preprocessing steps  such as hol. 
　reasoning with nonstandard quantifiers  such as those specifying restrictions on the number of elements satisfying a relation  for example    there exists at most 1 . such quantifiers are familiar to the description logic community. one can translate formulas with such quantifiers into firstorder logic with equality but the translation will create formulas of a prohibitive size. it is interesting to develop special ways of reasoning with such quantifiers  although this is not an obvious task. 
　distributed and other non-standard architectures. it has been observed that cooperating heterogeneous theorem provers can perform much better than the mere sum of their components  mp. bonacina  1; denzinger and fuchs  1. however  modern theorem prover architectures are not suited for cooperative or distributed theorem proving. 
　non-resolution inference system. the recent rapid progress of the dpll-based satisfiability checkers  zhang and malik  1  suggests that non-resolution systems could also play a significant role in first-order theorem proving. the first implementation of first-order dpll  baumgartner  1  was not very encouraging  but so were the first implementations of resolution. to be more generally applicable  non-resolution system musts find an efficient solution to the problem of built-in theories  degtyarev and voronkov  1 . 
　satisfiability-checking and model building. for many applications it is desirable to be able to establish satisfiability of sets of clauses and build models for satisfiable sets. satisfiability of first-order is conceptually a much harder problem than unsatisfiability. the set of satisfiable formulas is not recursively enumerable  which means that there is no semidecision procedure for satisfiability-checking. 
　other features. there are other features required of the next generation first-order theorem provers. for example  for 
invited speakers 
the naive users who do not know  and usually do not want to know  much about theorem proving  theorem provers must have a strong auto-mode which will try to select automatically a strategy or strategies best suited for solving the given problem. for more advanced users  there should be options to specify term orderings  literal selection  and clause selection. 
