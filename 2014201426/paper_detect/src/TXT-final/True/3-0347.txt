
we address dimensionality estimation and nonlinear manifold inference starting from point inputs in high dimensional spaces using tensor voting. the proposed method operates locally in neighborhoods and does not involve any global computations. it is based on information propagation among neighboring points implemented as a voting process. unlike other local approaches for manifold learning  the quantity propagated from one point to another is not a scalar  but is in the form of a tensor that provides considerably richer information. the accumulation of votes at each point provides a reliable estimate of local dimensionality  as well as of the orientation of a potential manifold going through the point. reliable dimensionality estimation at the point level is a major advantage over competing methods. moreover  the absence of global operations allows us to process significantly larger datasets. we demonstrate the effectiveness of our method on a variety of challenging datasets.
1 introduction
a number of algorithms for manifold learning from unorganized points have been recently proposed in the machine learning literature. these include kernel pca  scholkopf： et al.  1   locally linear embedding  lle   roweis and saul  1   isomap  tenenbaum et al.  1  and charting  brand  1   that are briefly described in section 1. they aim at reducing the dimensionality of the input space in a way that preserves some geometric or statistic properties of the data. isomap  for instance  attempts to preserve the geodesic distances between all points as the manifold is  unfolded  and mapped to a space of lower dimension. a common assumption is that the desired manifold consists of locally linear patches. we relax this assumption by only requiring that manifolds be smooth almost everywhere. smoothness in this context is the property of the manifold's orientation to vary gradually as one moves from point to point. this property is encoded in the votes that each point casts to its neighbors.
　the representation of each point is in the form of a second order  symmetric  nonnegative definite tensor whose eigenvalues and eigenvectors fully describe the dimensionality and orientation of each point  medioni et al.  1 . the votes are also tensors and their accumulation is a process that is considerably more powerful than the accumulation of scalars or vectors in terms of both the types of structure that can be described  as well as its robustness to noise. a point casts a vote to a neighboring point that carries an estimate of the orientation of the receiver given the orientation of the voter. even in the case of unoriented inputs  meaningful votes can be cast as a function of the relative position of the two points. the result of voting is a new tensor at each point that encompasses the weighted contribution of the point's neighbors.
　the dimensionality d of the manifold at each point in a ddimensional space is indicated by the maximum gap in the eigenvalues of the tensor  while the top d eigenvectors span the normal space of the manifold. the first property makes dimensionality estimation a procedure with no parameters that require tuning. these estimates are reliable without averaging  as shown is section 1  and allow us to handle data with varying dimensionality. this is arguably one of the most important contributions of our approach. moreover  the presence of outliers  boundaries  intersections or multiple manifolds pose no additional difficulties  due to the unsupervised learning ability of our method. furthermore  non-manifolds  such as hyper-spheres  can be inferred since we do not attempt to estimate a global  unfolding .
　we do not attempt to estimate a mapping from the input space to the embedding space  as in  brand  1   teh and roweis  1 . even though certain advantages can be derived from such a mapping  it is not always feasible  as in the case of hyper-spheres or multiple manifolds  and it is not necessary for many tasks  including interpolation. valid paths on the manifold can be followed using the estimated tangent subspaces at the input points by projecting the desired direction on them and moving on the projection.
　since all operations are local  time complexity is o dnlogn  where n is the number of points  enabling us to handle datasets with orders of magnitude more points than the current state of the art without incurring unreasonable computational cost. in terms of storage  the requirements at each point are o d1 . our current implementation is probably limited to d in the order of a few hundreds. as all manifold learning methods  we operate under the assumption that the data are randomly sampled from the manifold in a way that is close to uniform  or at least does not favor certain parts or directions of the manifold.
　the paper is organized as follows: the next section briefly reviews related work; section 1 describes the tensor voting framework; section 1 contains experimental results on classic datasets; finally  section 1 concludes the paper and discusses the directions of our future work.
1 related work
in this section we briefly present recent approaches for learning low dimensional embeddings from points in high dimensional spaces. most of them are extensions of linear techniques  such as principal component analysis  pca  and multi-dimensional scaling  mds   based on the assumption that nonlinear manifolds can be approximated by locally linear patches. in contrast to other methods  scholkopf： et al.  propose kernel pca that attempts to find linear patches using pca in a space of typically higher dimensionality than the input space. correct kernel selection can reveal the low dimensional structure of the input data. for instance a second order polynomial kernel can  flatten  quadratic manifolds.
　roweis and saul  address the problem by locally linear embedding  lle . processing involves computing reconstruction weights for each point from its neighbors. the same weights should also reconstruct the point from its neighbors in the low dimensional embedding space  since the neighborhoods are assumed to be linear. the dimensionality of the embedding however has to be given as a parameter since it cannot always be estimated from the data  saul and roweis  1 . lle assures that local neighborhood structure is preserved during dimensionality reduction.
　isomap tenenbaum et al.  approximates geodesic distances between points by graph distances. then  mds is applied on the geodesic instead of euclidean distances to compute an embedding that forces nearby points to remain nearby and faraway points to remain that way. a variation of isomap that can be applied to data with intrinsic curvature  but known distribution  and a faster alternative that only uses a few landmark point for distance computations have been proposed in  de silva and tenenbaum  1 . isomap and its variants are limited to convex datasets.
　belkin and niyogi  compute the graph laplacian of the adjacency graph of the input data  which is an approximation of the laplace-beltrami operator on the manifold  and exploit its locality preserving properties that were first observed in the field of clustering. the laplacian eigenmaps algorithm can be viewed as a generalization of lle  since the two become identical when the weights of the graph are chosen according to the criteria of the latter. the dimensionality of the manifold however cannot be reliably estimated from the data  as in most of the work reviewed here. donoho and grimes  propose a similar scheme which computes the hessian instead of the laplacian. the authors claim that the hessian is better suited for detecting linear patches on the manifold. the major contribution of this approach is that it proposes a global method  which  unlike isomap  can be applied to non-convex datasets.
weinberger and saul  address the problem of manifold learning by enforcing local isometry. the lengths of the sides of triangles of neighboring points are preserved during the embedding. these constraints can be expressed in terms of pairwise distances and the optimal embedding can be found by semi-definite programming. the method is more computationally demanding than lle and isomap  but can reliably estimate the underlying dimensionality of the inputs by locating the largest gap between the eigenvalues of the gram matrix of the outputs. similarly to our approach  this estimate does not require a threshold  as do approaches that estimate the residual error as a function of the dimensionality of the fitted model.
　an algorithm that computes a mapping  and not just an embedding  of the data is described in  brand  1 . the intrinsic dimensionality of the manifold is estimated by examining the rate of growth of the number of points contained in hyper-spheres as a function of the radius. linear patches  areas of curvature and noise can be discriminated using the proposed measure. affine transformations that align the coordinate systems of the linear patches are computed at the second stage. this defines a global coordinate system for the embedding and thus a mapping between the input space and the embedding space.
　finally  we briefly review approaches that estimate intrinsic dimensionality without any attempts to learn local structure. one such approach was presented in  bruske and sommer  1 . an optimally topology preserving map  otpm  is constructed for a subset of the data  which is produced after vector quantization. then pca is performed for each node of the otpm under the assumption that the data are locally linear. the average of the number of significant singular values at the nodes is the estimate of the intrinsic dimensionality. kegl  estimates the capacity dimension of the mani-＞ fold  which does not depend on the distribution of the data and is equal to the topological dimension. an efficient approximation based on packing numbers is proposed. the algorithm takes into account dimensionality variations with scale and is based on a geometric property of the data  rather than successive projections to increasingly higher-dimensional subspaces until a certain percentage of the data is explained. costa and hero  estimate the intrinsic dimension of manifold and the entropy of the samples using geodesic-minimal-spanning trees. the method is similar to isomap and thus produces a single global estimate. levina and bickel  compute maximum likelihood estimates of dimensionality by examining the number of neighbors included in spheres whose radius is selected in such a way that the density of the data can be assumed constant  and enough neighbors are included. these requirements cause an underestimation of the dimensionality when it is very high.
　the difference between our approach and those of  bruske and sommer  1  brand  1   kegl  1   weinberger＞ and saul  1   costa and hero  1  and  levina and bickel  1  is that it produces reliable dimensionality estimates at the point level  which do not have to be averaged over the entire dataset. we are also able to estimate the local orientation of the manifold without being restricted to simple linear models.
1 tensor voting
in this section we first review the tensor voting framework  medioni et al.  1   which can infer structures from sparse and noisy data via a local voting process. results have been published mostly in 1- and 1-d domains  but the framework can be extended to higher dimensions  as in  tang et al.  1 . we begin by describing the basics of the framework: data representation and voting. then  we present the contribution of this paper to the methodology  which is an efficient implementation  in terms of both space and time  of tensor voting in high-dimensional spaces. we are able to operate in spaces where that seemed impractical or impossible based on the formulation of  medioni et al.  1 .
1 data representation
the representation at each point is in the form of a second order  symmetric  nonnegative definite tensor. the tensor can also be viewed as a matrix or an ellipsoid. it represents the normal space at the point. for instance  a hyper-plane has one normal ~n  which can be encoded in tensor form as ~n~nt. a structure with a normal space of rank d has d normals and is represented by a tensor of the form:
	t = x~nd~ntd	 1 
d
a point without orientation information can be equivalently viewed as one having all possible normals and is encoded as the identity matrix. a tensor in this form represents an equal preference for all orientations and is called a  ball tensor   since the corresponding ellipsoid is a sphere or hyper-sphere. on the other hand  a tensor that contains only one orientation is called a  stick tensor . stick tensors represent the hyperplane of a d-dimensional space  which has one normal and d   1 tangents.
　depending on the type of structure the point belongs to  it has a different number of normals and tangents. the tensor's eigensystem encodes this information. eigenvectors that belong to the tangent space correspond to zero eigenvalues  while those that belong to the normal space correspond to nonzero eigenvalues  typically equal to 1 . therefore  points with known orientation can be encoded in this representation by appropriately constructed tensors  as in  1 . points with unknown orientation are represented by identity matrices  ball tensors  see fig. 1 a  for example tensors in 1-d.
　on the other hand  given a second order  symmetric  nonnegative definite tensor  the type of structure encoded in it can be found by examining its eigensystem. any tensor that has these properties can be decomposed as in the following equation:
	xd	t
	t =	λde de d =
d=1
=  λ1   λ1 e 1e t1 +  λ1   λ1  e 1e t1 + e 1e 1t  
+ .... + λd e 1e t1 + e 1e t1 + ... + e de td   =
xd 1	xd	t	t	t
	  λd λd + 1 	e de d  +λd e 1e 1 +...+e de d  
d=1	k=1
 1 

	 a  example tensors	 b  vote generation
figure 1: tensor voting.  a  the shape of the tensor indicates the type of structure at the location  while its size indicates the confidence of this information. the top tensor has a strong preference of orientation and has higher confidence than the bottom one  which has no preference for any orientation.  b  vote generation as a function of the relative position of a stick voter and the receiver and the orientation of the voter.
where λd are the eigenvalues in descending order and e d are the corresponding eigenvectors. the tensor simultaneously encodes all possible types of structure. the confidence we have in the type that has d normals is encoded in the difference λd   λd+1.
1 the voting process
the tensor voting framework was designed to enforce constraints  such as proximity  co-linearity and co-curvilinearity with human perception in 1- and 1-d in mind. these constraints still hold in high dimensional spaces as long as one is looking for structures that are smooth almost everywhere. in this section we describe how information is propagated from one point  the voter  to another  the receiver.
　during the voting process  each point casts votes to each of its neighboring points. the votes are also second-order  symmetric  nonnegative definite tensors. they encode the orientation the receiver would have  if the voter and receiver were in the same structure. we begin by examining the case of a voter that is associated with a stick tensor of unit length. the magnitude of a vote cast by the stick tensor decays with respect to the length of the smooth circular path connecting the voter and receiver. the circle was selected since it maintains constant curvature. it degenerates to a straight line if the vector connecting the voter and receiver is orthogonal to the normal of the voter. the orientation of the vote is towards the center of the circle defined by the two points and the orientation of the voter. the vote is generated according to the following equation and then transformed to the appropriate coordinate system.
s
s is the length of the arc between the voter and receiver  and κ is its curvature  see fig. 1 b    σ is the scale of voting  and c is a constant. no vote is generated if the angle θ is more than 1o. also  the field is truncated to the extend where the magnitude of the vote is more than 1% of the magnitude of the voter.
　since tensor voting is a function of the position of the voter and the receiver and the orientation of the voter  vote generation by a stick tensor occurs in a 1-d subspace defined by the vector ~v that connects the two points and the normal of the voter. therefore  stick voting is fully defined by fig. 1 b  and eq. 1  regardless of the dimension of the input space. according to  medioni et al.  1   the votes cast by other types of tensors were pre-computed and stored in look-up tables  called voting fields. the presence of a normal space of rank more than one is simulated by a rotating stick tensor that spans the space. since no closed form solution exists  the integration is numerically approximated. this process becomes impractical as the dimensionality of the space increases. space requirements for storing d d-dimensional voting fields with k samples per axis are o dkd . at the same time  the advantage of re-using pre-computed votes vanishes as the dimensionality increases. we propose instead a novel  efficient way to directly compute votes. since integration is infeasible  we seek an approximate solution.
1 efficient voting scheme
the new scheme  proposed here  generates votes that contain the orientation information that should be communicated to the receiver  but not the uncertainty. the latter  in the original framework  was conveyed by the ball component of the vote that was due to the integration of votes cast by the rotating stick tensor. for instance  according to  medioni et al.  1   a voting tensor that encodes a curve in 1-d  that has two normals and a tangent  casts a vote that has a dominant curve component and also some uncertainty. here  we propose to propagate only the curve orientation at the receiver given the orientation of the voter. the uncertainty in the new formulation comes only from the accumulation of votes that are not perfectly aligned.
　let ~v be the vector connecting the voting and receiving points. it can be decomposed into ~vt in the tangent space of the voter  null in the case of a ball voter  and ~vn in the normal space. the new vote generation process is based on the observation that curvature in eq. 1 is not a factor when θ is zero  or in other words  if the voting stick is orthogonal

figure 1: new scheme for vote generation. the voter here is a tensor with two normals in 1-d. the vector connecting the voter and receiver is decomposed into ~vn and ~vt that lie in the normal and tangent space of the voter respectively. a new basis that includes ~vn is defined for the normal space and each basis component casts a stick vote. only the vote generated by the orientation parallel to ~vn is not parallel to the normal space. tensor addition of the stick votes produces the combined vote.
to ~vn. we can exploit this by defining a new basis for the normal space of the voter that includes ~vn. then  the vote is constructed as the tensor addition of the votes cast by stick tensors parallel to the new basis vectors. among those votes  only the one generated by the stick tensor parallel to ~vn is not parallel to the normal space of the voter. see fig. 1 for an illustration in 1-d. this scheme is applicable without changes in the case of ball voters. since two unoriented points define a straight line in any space  the vote from a ball tensor should have d 1 normals and one tangent. since ~vt = ~1  the tensor parallel to ~vn does not cast a vote due to the 1o cut-off rule. the remaining d   1 tensors in the new basis cast votes that are equal in magnitude and span the space orthogonal to ~v  and thus represent a straight line connecting the two points.
　tensors with unequal eigenvalues are decomposed before voting according to eq. 1. then  each component votes separately and the vote is weighted by λd   λd+1  except the ball component whose vote is weighted by λd. this implementation of tensor voting is more efficient in terms of space  since there is no need to store voting fields  and in terms of time since we have devised a direct method for computing votes that replaces the numerical integration of  medioni et al.  1 . the new computation requires at most d computations of a stick vote according to eq. 1  followed by the addition of all the votes  instead of integration over d dimensions. experiments in 1- and 1-d  that cannot be included here due to space limitations  validate the accuracy of the new vote generation scheme.
1 vote analysis
votes are accumulated at each point by tensor addition  which is equivalent to the addition of matrices. after voting is completed  the eigensystem of the new tensor is analyzed and the tensor is decomposed as in eq. 1. in 1-d  the inference of the type of structure that a point belongs is accomplished as follows. the difference between the two largest eigenvalues encodes surface confidence  with a surface normal given by ~e1. the difference between the second and third eigenvalue encodes curve confidence  with a the normals to the curve being ~e1 and ~e1. the smallest eigenvalue encodes junction confidence. outliers receive little and inconsistent support from their neighborhood and can be identified by their low eigenvalues. generalization to higher dimensions is straightforward  considering that there are d manifold types in d dimensions. the dimensionality d is estimated by finding the maximum difference λd   λd+1.
1 experimental results
in this section we present experimental results in dimensionality estimation and local structure inference. all inputs consist of un-oriented points and are encoded as ball tensors.
swiss roll the first experiment is on the  swiss roll  dataset  which is available at http://isomap.stanford.edu/. it contains 1 points on a 1-d manifold in 1-d  fig. 1 . we perform a simple evaluation of the quality of the orientation estimates by projecting the nearest neighbors of each

figure 1: the  swiss roll  dataset in 1-d
point on the estimated tangent space and measuring the percentage of the distance that has been recovered. the accuracy of dimensionality estimation at each point  the percentage of recovered distances for the 1 nearest neighbors and execution times on a pentium 1 at 1 ghz  as a function of σ  can be seen in table 1. performance is the same at boundaries. the number of votes cast by each point ranges from 1 for σ1 = 1 to 1 for σ1 = 1. the accuracy is high for a large range of σ.
σ1correctdist.timedim.  % rec.  %  sec 1.1.11.1.11.1.11.1.11.1.11.1.11.1.1table 1: rate of correct dimensionality estimation and execution times as functions of σ for the  swiss roll  dataset
structures with varying dimensionality the second dataset is in 1-d and contains points sampled from three structures: a line  a 1-d cone and a 1-d hyper-sphere. the hyper-sphere is a structure with three degrees of freedom. it cannot be unfolded unless we remove a small part from it. figure 1 a  shows the first three dimensions of the data. the dataset contains a total 1 points  voting is performed with σ1 = 1 and takes 1 hours and 1 minutes. figures 1 cd  show the points classified according to their dimensionality. performing the same analysis as above for the accuracy of the tangent space estimation  1% of the distances of the 1 nearest neighbors of each point lie on the tangent space  even though both the cone and the hyper-sphere have intrinsic curvature and cannot be accurately approximated by linear models. all the methods presented in sec. 1 would fail for this dataset because of the presence of structures with different dimensionalities and because the hyper-sphere cannot be unfolded.
data in high dimensions the datasets for this experiment were generated by sampling a few thousand points with low intrinsic dimensionality  1 or 1  and mapping them to a medium dimensional space  1- to 1-d  using linear and quadratic functions. the generated points were then rotated

	 a  input	 b  1-d points

	 c  1-d points	 d  1-d points
figure 1: data of varying dimensionality in 1-d. the first three axes of the input and the classified points are shown.
and embedded in a 1- to 1-d space while uniform noise was added to all dimensions. the accuracy of dimensionality estimation after tensor voting can be seen in table 1.
intrinsiclinearquadraticspacedim.dim.mappingsmappingsdim.est.  % 111111111111table 1: rate of correct dimensionality estimation for high dimensional data
1 conclusions
we have presented an approach to manifold learning that offers certain advantages over the state-of-the-art. first  we are able to obtain accurate estimates of the intrinsic dimensionality at the point level. moreover  since the dimensionality is found as the maximum gap in the eigenvalues of the tensor at the point  no thresholds are needed. in most other approaches the dimensionality has to be provided  or  at best  an average intrinsic dimensionality is estimated for the entire dataset  as in  bruske and sommer  1   brand  1   kegl  1 ＞  weinberger and saul  1   costa and hero  1 . second  even though tensor voting on the surface looks like a local covariance estimation  the fact that the votes are tensors and not scalars reveals more information for each point. the properties of the tensor representation  which can handle the simultaneous presence of multiple orientations  allow the reliable inference of the normal and tangent space at each point. due to its unsupervised nature  tensor voting is very robust against outliers  as demonstrated in  medioni et al.  1  for the 1- and 1-d case. this property holds in higher dimensions  where random noise is even more scattered. lack of space did not allow us to include outlier rejection results here.
　an important advantage of tensor voting is the absence of global computations  which makes time complexity o dnlogn . this enables us to process datasets with very large number of points. computation time scales linearly with the number of points assuming that more points are added to the dataset in a way that the density remains constant. in this case  the number of votes cast per point remains constant and time requirements grow linearly. complexity is adversely affected by the dimensionality of the space d  since eigen-decompositions of d 〜d tensors have to be performed. for most practical purposes  the number of points has to be considerably larger than the dimensionality of the space  to allow structure inference. computational complexity  therefore  is reasonable with respect to the largest parameter.
　it should also be noted that the votes attenuate with distance and curvature. this is a more intuitive formulation than using the n nearest neighbors with equal weights  since some of them may be too far or belong to a different part of the structure. for both the  distance and the n nearest neighbor formulations  however  the distance metric in the input space has to be meaningful. this is also the case for all the methods presented in sec. 1.
　the representation by tensors allows us to deal with structures that are not necessarily manifolds. these include nonmanifolds  such as hyper-spheres  intersecting structures and datasets containing structures of different dimensionality. a mapping from the input to the embedding space cannot be computed in any of these cases  but it is not necessary for many tasks including interpolation  function approximation and local orientation  gradient  estimation. one can also view our algorithm as a preprocessing stage that facilitates further processing. its outputs can be used as inputs for tasks such as dimensionality reduction and mapping to a low dimensional space  if desired.
　two issues require further investigation. the first is a more thorough analysis of the required properties of the data distribution. in line with other methods for manifold learning   roweis and saul  1  tenenbaum et al.  1  brand  1    we assume that the data are distributed regularly is space. a uniform distribution is ideal but not necessary  as shown in sec. 1  especially in the last experiment where the points are mapped to the high dimensional space nonlinearly. the second issue is that of data sufficiency for structure inference. as the dimensionality of the space and the intrinsic dimensionality of the structure grow  the number of points necessary to infer the latter correctly also grows. we intend to investigate the minimum requirements in terms of the number of points for inferring a certain type of structure in a space of high dimensions.
acknowledgement
this research has been supported by the u.s. national science foundation grant iis 1.
