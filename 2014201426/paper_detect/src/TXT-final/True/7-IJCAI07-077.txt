
plans provide an explicit expectation of future observed behavior based upon the domain knowledge and a set of action models available to a planner. incorrect or missing models lead to faulty plans usually characterized by catastrophic goal failure. non-critical anomalies occur  however  when actual behavior during plan execution differs only slightly from expectations  and plans still achieve the given goal conjunct. such anomalies provide the basis for model adjustments that represent small adaptations to the planner's background knowledge. in a multi-agent environment where 1 or more individual plans can be executing at any one time  automation is required to support model anomaly detection  evaluation and revision. we provide an agent-based algorithm that generates hypotheses about the cause of plan anomalies. this algorithm leverages historical plan data and a hierarchy of models in a novel integration of hypothesis generation and verification. because many hypotheses can be generated by the software agents  we provide a mechanism where only the most important hypotheses are presented to a user as suggestions for model repair. 
1 	introduction 
planning in most domains requires a description of relevant entities and processes. these are typically described by a set of models. feedback collected during or after plan execution can be used to refine models so that the planner performance becomes more adaptable to the environment and im-
                                                          
     *  this research is sponsored by the united states' defense advanced research projects agency  darpa  under contract fa1-c-1. the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements  either expressed or implied  of the sponsoring institutions  the u. s. government or any other entity. proves with experience. the research presented here shows how small discrepancies between what is expected in a plan and what actually happens during plan execution can determine an effective refinement to the models upon which plans are based and hence improve plan performance.  
　plans themselves provide an expectation about what should happen when the plans execute. an expectation failure occurs when the expected performance diverges from what the models of the domain predict  and such failure provides the basis for learning  birnbaum et al.  1; ram et al.  1 . diagnosis is a symptom to fault mapping that links the expectation failures detected during execution to the model changes necessary to prevent such failures from recurring in the future. however this blame assignment problem is nontrivial  because many factors can contribute to a single error  multiple errors can co-occur  and the diagnostic mapping between symptom and fault can be very indirect. indeed catastrophic plan failure may result from so many factors that diagnosis is not practical. but plans can also fail in nondisruptive ways that result in suboptimal performance while still achieving the goal set. it is this type of expectation failure we term an anomaly.
　the dynamic and continuous nature of our planning domain and the complexity of the models of this domain has led us to develop an agent-based model diagnosis tool that handles multiple types of plan anomalies  e.g.  consumable  spatial  and temporal . the agents use a model hierarchy with spreading-activation likelihood functions to control hypotheses generation. the agents reference historical plan data to recognize past anomaly patterns. when an anomaly has occurred in the past and all planned activities have completed  the agents decrease the likelihood that a model repair is required. in addition  the agents use a temporal link analysis to localize failure symptoms within model processes. the diagnosis system includes a mixed-initiative component that provides model error and repair suggestions  resulting from the most likely hypotheses  to a user who can accept  ignore  or reject them. others have examined agent-based  e.g.  roos and witteveen  1  and modelbased  e.g.  williams and nayak  1  diagnosis for planning as well as mixed-initiative approaches  e.g.  mulvehill and cox  1 . here we present a novel combination of the three.
　in this paper  we describe our technical problem and provide detail on our approach. we also provide the results of a study that compares a hand-coded approach  as described in this paper  with our agent-based approach.  
1 	model-based mission planning and execution monitoring 
our planning domain is focused on military air operations and our tool suite for this domain includes a generative planner  a plan monitor and a model adaptor component. the planner creates a hierarchical task network that allocates resources for specific tasks that achieve a set of input objectives. the monitor tracks the behavior of a simulator that executes the activities in the plan. the model adaptor provides and refines models to account for changes in the world state and to improve plan performance.  
　the domain is defined by two sets of hierarchical models. the entity models describe objects in the domain  and the process models define the actions that entities perform. entities include hundreds of configurations of munitions and fuel tanks that may be loaded on an aircraft for specific missions. processes encode the different mission variations for which the aircraft may be deployed  e.g.  the sequence of activities the aircraft perform varies depending on the mission being performed and the aircraft's configuration load at the time. in addition the entity hierarchy defines specific types of aircraft using default parameters valid for whole families of aircraft types when applicable and uses specific overridden individual parameters when necessary. finding and fixing model problems in such a large and complex model is difficult. 
　as an example  consider a plan servicing 1 objectives  ranging from targets that need to be destroyed to surveillance tracks that must be flown. during execution of this plan  the plan monitor can detect hundreds of anomalies. figure 1 shows a data set of expected and actual fuel values during various activities of a close air support mission flown by a-1 aircraft.  

figure 1. data set 1 
　during takeoff and climb  the first two points   the values are equal  but as the mission continues the values begin to diverge at successive navigation points. the difference between planned  square  values and simulated  triangle  values at point three  however  is not great enough to generate an anomaly by the monitor. at point four and all subsequent locations the difference exceeds a threshold  and the monitor sends anomaly messages to the model adaptor component. yet during the midpoints of the mission and at the very end  the difference between actual and expected fuel levels remains constant  thus indicating that no problem exists in those segments of the mission  despite anomaly messages from the monitor. in actuality  the fuel level discrepancy was introduced by problems in previous mission segments. 
1 	agent-based diagnosis and adaptation 
the problem for the model adaptation component is to map failure symptoms from the monitor such as fuel consumable anomalies to causal faults such as incorrect model parameters like high fuel burn rates on navigate activities for a1s. the reported anomalies can be simple or severe and can be caused by many sources  including the models. as anomalies are detected  they are classified and provided to the model adaptor component. here users  who are familiar with the models  use the model adaptor software to determine if a model is responsible for an anomaly  and if so  repair and republish it to the other planning components. because this particular planning application has the potential to scale to over 1 concurrently executing missions  an agent-based diagnosis system was developed to support anomaly management  interpretation and model repair suggestion generation.  

figure 1. agent-based diagnosis and mixed-initiative model adaptation processes 
the diagnosis system is composed of a set of agents. each agent is a java class and is associated with a hypothesis about a model failure.  the diagnosis manager  detailed in figure 1  is the component responsible for hypothesis generation  along with the creation and execution management of these agents. each agent executes a set of tasks  gathering and performing computations on data from past and current runs in order to determine whether its hypothesis is likely or not. when an agent attains a high degree of confidence in its hypothesis  it will further generalize or refine the hypothesis in a mixed-initiative process with the user. figure 1 shows both the agent-based manager and the mixed-initiative component. specialized agents combine and evaluate hypotheses derived from single agents. these specialized agents output model revision suggestions  if warranted. 
1 hypothesis generation 
currently the diagnosis manager uses four methods to generate new hypotheses and associated agents. first  a set of general hypotheses that should always be investigated is provided at initialization time. examples of such general hypotheses are that the planner is using an old model version  or that there is a non-model related error  such as a faulty sensor or a bug in the world simulator. the second type of hypothesis generation is initiated by the arrival of a plan anomaly message. the diagnosis manager analyzes the anomaly within the context of the plan activity in which the anomaly occurred and within the context of a network of related activities and entities; it generates a list of the model classes and parameters that were used to model the activity. each of these classes and parameters within the model is suspected as a potential source for the error  and an appropriate hypothesis is generated. consider one example plan anomaly referencing an altitude discrepancy on a navigate activity for an a-1 flight group. related models involved in the generation of this planned activity include the a-1 entity model   the navigate activity model  the cas  close air support  mission process model   a set of entity models for objects carried on the a-1  missiles  bombs  and fuel tanks   and a set of geometric object models defining the space through which the aircraft flies. for each of these models  a hypothesis that there is an error in that model is generated by the diagnosis manager. 
　the third type of hypothesis generation is triggered when the planner is unable to satisfy a particular goal. this problem can point to an inconsistency between the capabilities modeled for the actor entity  the process models used by the planner to support reasoning  and a potential change in the world state.   in this case  model error hypotheses will be generated for each available actor model and any related models referenced in the unsatisfied goal. the fourth type of hypothesis generation is hypothesis generalization and refinement. specifically  when an agent's belief that its hypothesis is true  called the agent's likelihood  reaches a certain value  rules can fire that generate more general hypotheses by traversing up the model hierarchy.  
1 hypothesis refinement 
in addition to generalizing hypotheses by utilizing the model hierarchy  agents can generate new hypotheses by refining their current hypothesis. this refinement can take two forms. first  the diagnosis manager can specify what part of the model might be in error  either giving a specific parameter or a specific constraint of a process model. second  the diagnosis manager can specify an interaction between models  such as an error for the a-1 model on a navigate activity flying a cas mission.  
　parameter refinement is accomplished by data mining the model case base to generate a list of parameters that have adaptable values associated with the model in question. for each such parameter we reference a predefined knowledge base to determine if that parameter can cause the anomalies the agent has seen. if so  a new diagnosis agent with a refined hypothesis is generated. the new agents compare the current anomalies with historical data and with expected data for each such parameter type. these newly specified hypotheses are then further refined by looking for interactions between models. for this domain  parameters can have different values for sets of aircraft  activities  and missions.  
　for example  consider consumable fuel anomalies for an a-1 aircraft. when the  a-1 aircraft model error  agent reaches a high likelihood  the hypothesis is refined. an examination of the a-1 model data reveals a list of adaptable parameters  and the causal model narrows this list to the maxfuel parameter  and the fuelburnrate parameter. for fuelburnrate  there are multiple values in the models based on the aircraft  activity  and mission. thus  we can further refine these hypotheses for the a-1 aircraft by tracking the highest likelihood activity and mission. in our example  the consumable anomalies occur on navigate activities for any mission type  so the navigate model error agent has a high likelihood  whereas no mission model error agent is very high. therefore  we refine the hypothesis to suspect an  error with fuelburnrate on a-1 aircraft performing navigate activities on any mission type.  similar refined hypotheses for the other parameter choices are also generated.  
　we call the refined hypotheses with the highest likelihood a suggestion  because they can be presented to the user to support model repair. the operator can accept  reject  or ignore a suggestion and in doing so  provides feedback to the suggestion agent. at this time  the feedback is in the form of a selection from a list of pre-defined options. examples are:  suggestion is too general/specific    suggestion makes no sense   and  source of error is not in the models.  
1 likelihood calculation 
when a new hypothesis and associated agent are generated  the agent is loaded with a predefined set of tasks. the goal of a task is to calculate the belief that the agent's hypothesis is true  false  or uncertain by performing a certain calculation or examining a specific data source. we are experimenting with various forms of evidence combination to merge the belief calculations from various tasks into one single value for the agent  including an application of the dempster-schafer evidence combination theory  dempster  1; shafer  1  and an operator modifiable set of likelihood functions.  
　examples of tasks executed by most agents include:  1  querying the model case base to determine what model changes have been made in the past   1  querying the historical plan case base to determine if similar anomalies adversely affected previously executed missions   1  examining currently executing missions and activities to see if other activities involving this model are completing without error  and  1  performing a statistical analysis of the current anomalies with the involved models. agents with hypotheses about activity models can execute an additional task  1  that examines the temporal successor links in the plan to determine if anomalies can be blamed on the previous activity in the mission. refined hypothesis suggestion agents can execute a task  1  that compares the set of current anomalies and their actual data values with a theoretical set of anomaly data that we would expect to receive if the model error in question were present. this theoretical anomaly data can come from information about previous model errors or from a human-defined knowledge base.  
　for example  for a fuel burn rate error  the expected results  defined in the knowledge base and case base of past fuel burn rate errors  are a decreasing slope for actual values as the mission progresses  with an increasing value for the delta between actual and expected values. if the fuel burn rate is in error  we expect the difference between actual and expected fuel levels to get progressively worse as the mission executes. alternatively  if the maximum fuel level is in error  we expect the difference between expected and actual fuel levels to remain constant   and we expect the actual fuel level value to decrease. 
1 example scenario 
for example  consider an a-1 model error agent. assume that a case base query  task 1  reveals a handful of successful a-1 missions using an older version of the a-1 aircraft model. this discovery would result in a small belief value that the current a-1 model might be in error and a large belief that we are uncertain. the supporting evidence is that previous successful missions were found and the a-1 model was recently updated. next  assume that of the missions in the currently executing plan  the ones without a-1 aircraft are experiencing few anomalies  task 1  and most of the ones with a-1 aircraft are generating fuel value anomalies  task 1 . both of these tasks would result in a significant belief value that the a-1 model has an error. combining these beliefs using the dempster rule of combination  dempster  1  would result in a high likelihood for this a-1 agent.  
　refining this hypothesis gives rise to two new agents hypothesizing that either the maxfuel parameter or the fuelburnrate parameter of the a-1 is in error. examination of the anomaly value data  task 1  shows that the delta between the expected and actual fuel values continued to grow as the mission progressed. historical case base data shows that this is consistent with previous fuelburnrate parameter errors; however the knowledge base data for maxfuel errors show that this delta between expected and actual fuel values should stay constant. thus the  a-1 fuelburnrate error  agent ends up with a higher likelihood than the  a-1 maxfuel error  agent. 
　next  consider the navigate model error agent. similar tasks  tasks 1  1  and 1  would result in a moderate likelihood  because the anomalies additionally occur on many types of activities  such as climb  descend  and orbit. however  examining the temporal link structure of the process model  task 1  shows that the gap between expected and actual fuel values increases mostly on the navigate legs. because both the  a-1 fuelburnrate error  and  navigate error  agents now have a high likelihood  a refined hypothesis from combining the two will be generated.  
　as a result of the agent processing  the user is finally presented with a set of suggestions. the highest confidence suggestion  from the highest likelihood agent is to check the value for fuelburnrate for a-1 aircraft on navigate activities for all mission types. less likely suggestions presented to the user include  check the fuelburnrate parameter for 
a-1 aircraft for all flightactivities   and  check the max-
fuel parameter for a-1 aircraft.   
1 	evaluation 
as discussed earlier  anomalies occur in the form of expectation failures when the expected value of some parameter as specified in a plan differs from an actual value as determined during monitoring of plan execution. because most of our data is proprietary  we created an artificial data set that is representative of the real data for use in this paper. as shown by figures 1  1 and 1  we created three data sets for aircraft fuel levels with which to evaluate the effectiveness of our approach. data set 1  figure 1  represents a typical case where a mismatch occurs  because the fuel burn rate in the model for the a-1 aircraft is mistakenly set too high. thus the plan predicts the fuel at increasingly lower levels than what actually occurs. data set 1  figure 1  represents the case where false anomalies are reported by the plan monitor. in this case  a faulty sensor reports all fuel values to be zero. data set 1  figure 1  reflects the case where a fuel leak causes an a-1 to crash. in the latter two sets the generated plan is correct  but in the first it is not. note also that superficially the third set is similar to a combination of features from the other two.  
　the results from our study compare a na ve method whereby a simple set of hand-coded rules is compared to the diagnosis process described earlier. the rules can be summarized as follows: 
 
if expected current-activity   
   actual current-activity  then  
is-faulty  parameter current-activity  
 
if was-faulty = parameter last-activity   and thisdelta = lastdelta then 
is-faulty  null 
　 where  
thisdelta = actual current-activity  
           -expected current-activity  lastdelta = actual last-activity  
           -expected last-activity  
1 data set 1 
the results between the na ve method and the agent-based diagnosis method are comparable on the first data set of eleven anomalies. with both techniques  the fault is identified as a fuel burn rate parameter for the navigate activity on the a-1 model. the crucial aspect of the anomaly pattern is that the difference between expected and actual values changes for those activities related more directly to the fault. the delta remains the same for an activity that is not faulty. this distinction is evident within figure 1 from the relative slopes of various segments of the two curves. when the slopes are the same and the curves parallel  the differences between expected and actual do not change  and any anomaly can be considered a false positive. the slopes are different in the two segments that correspond to navigation activities  points 1 and 1 . that is  the plan predicts a steeper decline in fuel than actually occurs.  
　the temporal link analysis discussed earlier  task 1 in section 1  forms the basis to determine the slopes and hence identify the correct activity that represents the diagnosed fault given the anomaly symptoms.  
1 data set 1 
not all anomalies detected by the plan monitor will follow the pattern of data set 1. data set 1 represents the case of false symptoms  in this example  where the plan simulator or fuel sensor is bad  and hence provides incorrect data for comparison to the expected values  which are good . this data set cannot be handled correctly by the na ve rules  because for each anomaly input into the system  an expectation failure exists and the deltas progressively vary.  

figure 1. data set 1 
　what is needed is recognition that zero fuel values represent unrealistic data and thus the input is faulty. our diagnosis system can check for such conditions using a knowledge-based sanity check  task 1 in section 1 . data set 1 has a constant actual fuel level  and a decreasing value for the delta between actual and expected fuel values. because this does not match the expected results for fuel burn rate or maximum fuel level errors  neither hypothesis ends up with a high likelihood. the likelihood of one of the default hypothesis agents  the  faulty sensor or simulator bug  agent  is inversely proportional to the maximum likelihood of the other suggestion agents. because no other agent has a high likelihood  the suggestion that there is a faulty sensor or a simulator error is presented to the user. 
1 data set 1 
the third data set cannot be easily analyzed by the agents alone  nor can the rules diagnose the fault. indeed the fault is outside the system and a proper diagnosis relies upon insight provided by a user. in this case  not only does the expectation failure progressively increase  but at a certain point in the input stream  the fuel ends at zero. at this point a series of spatial anomalies enter the input stream.  
　the expectation in the plan specified that the altitude would be a given height  but instead the altitude is zero. using the simple rules results in spatial anomalies being most suspect  but with our system  although the agents are able to focus correctly upon the fuel anomalies as opposed to the spatial anomalies  the complete diagnosis depends upon the human ignoring the spatial anomaly suggestions from the suggestion manager. this is done using the feedback received through the mixed-initiative adaptation process described in figure 1. 

figure 1. data set 1 
1 	conclusion 
our evaluation results indicate that while our agent-based approach is superior to the simple hand-generated method we presented  our approach is unable to handle all classes of faulty anomaly reporting unless there is a human in the loop. however  even with the mixed-initiative approach  we still are faced with the problem of focus. in other words  a user might reject or ignore a suggestion because there is a more important problem being reported by the anomalies or because visual cues from other tools show a particular state that is inconsistent with anomaly reporting. for instance  in the example where the aircraft runs out of fuel and crashes  if the reported anomalies do not result in timely adjustment of the activities to include an aerial refuel  then the plan will fail. the crash will be indicated in a spatial anomaly with a constant altitude value of zero. future versions of the suggestion tool need to account for this situation. 
　in some early work with human modelers  we have determined that our current tool is only satisfying some of the needs of the human modeler. for example  diagnosing a model problem is complicated by the fact that an error in a single parameter for a given modeled entity could result in multiple problems  like the third issue listed below   and by the fact that the models are hierarchical. for example  in the case of an anomaly related to the fuel level for a particular aircraft flying a specific activity of a mission  some questions the human modeler might have are as follows: 
 is the fuel level higher or lower than modeled   - our agents can provide this suggestion. 
 is the fuel burn rate parameter entered in the model for a specific activity for a specific load set too high or too low  - our agents can provide this suggestion. 
 is it a fuel level problem or is the speed specified in the model too high or too low which would cause the aircraft to burn fuel differently than expected  - currently both suggestions would be presented. 
 is it really a modeling problem or does the specific aircraft that is flying have a fuel leak  - in the future version of the anomalies  there will be severity and source data provided so that we can more accurately answer this question. 
 is it just this specific type of aircraft  a subclass in the model hierarchy  that has this problem or is it the whole family of aircrafts of this type  a superclass in the model hierarchy  that has the problem  - in experiments performed to date  we were able to leverage the model hierarchy to satisfy this requirement. 
the diagnosis system we have developed to date produces suggestions that are consistent with some of the types of suggestions required by the human modeler. in addition  the diagnoses that are generated to date are also consistent with the theory of reasoning failure proposed by cox and ram  1 . furthermore we have leveraged the work of lesser  horling  1  to develop our diagnosis hypotheses networks. our use of historical plan data and the generalized reasoning we do with the model case base is our novel contribution as we are unaware of any work of this particular hybrid being done in the past.  
　the experimental data we present here shows how our diagnosis agent approach provides an improvement over a hand-coded method. in other experiments conducted to date  but not reported here  we have found that our diagnosis agents can generate suggestions that are useful to the user. for example  the agents were able to find subtle errors even when the user was distracted with other recurring problems  and we have had some measurable success in providing correct suggestions that were derived from generalizations about model class problems.  
　the next step in increasing fidelity of the suggestion system is keeping track of accepted  rejected  and ignored suggestions to affect the likelihood values calculated by the diagnosis and suggestion system for future anomalies that come in. as the human modeler gives the system feedback about the accuracy of the diagnoses and related suggestions  the system can improve its diagnosis likelihood calculations and future suggestions to the modeler. 
　in this paper  we have introduced a new agent-based diagnostic approach that is being developed to support both offline and real time model diagnosis and repair. we have presented some preliminary experimental results that indicate that this approach is adequate in our specific problem domain.  this diagnostic tool is currently an integral part in a closed loop planning  execution and adaptation framework. our goal is to provide suggestions to human modelers that have such integrity that in the future the human will allow the system to detect  recommend  and in fact automatically repair models. 
1 	