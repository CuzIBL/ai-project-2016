 
this paper presents a generalized associative memory model  which stores a collection of tuples whose components are sets rather than scalars. it is shown that all library patterns are stored stably. on the other hand spurious memories may develop. applications of this model to storage and retrieval of naturallyarising generalized sequences in bioinformatics are presented. the model is shown to work well for detection of novel generalized sequences against a large database of stored sequences  and for removal of noisy black pixels in a probe image against a very large set of stored images. 
1 introduction 
associative memory is an important problem  with numerous applications in ai and in pattern analysis. here we consider this problem in the setting of recurrent connectionist networks. in this setting  the problem is defined as follows  hopfield  1; hertz et al.  1; kamp and hasler  1 . store m vectors each in  in the fixed points of a recurrent neural network in such a way that 
1. most if not all the vectors  are stable  i.e. fixed points of the network. 
1. the stable vectors  have reasonably large basins of attraction around them. 
　a lot of work has been done on this problem  hertz et al.  1; kamp and hasler  1 . the problem has also 
been extended to store  i  real-valued vectors in  1  l  n  hopfield  1; farrell and michel  1  and  ii  vectors in  for any integer !  rieger  1; 
kohring ; jagota et al  1 . these have expanded the set of applications accordingly. 
　in this paper we present a further extension. here we permit a component of a vector to store not merely a scalar value but a set of values. this expands the set of associative memory applications accordingly. 
　one illustrative application involves storing a collection of generalized consensus sequences  gsequences  that describe important signals  i.e. local regions with structure  in {d r}na or protein. consensus sequences are commonly used to describe such signals  haussler  1 . generalized consensus sequences are a refinement in that not one  the most likely  but multiple  the top few  sites at a position are representable. gsequences may also be used to describe multiple alignments of biosequences. multiple alignments are an important procedure for capturing the similarities and differences between several related biosequences  altschul  1 . 
　consider the problem of testing whether a given biosequence x matches any one of m given gsequences  representing signals or multiple alignments . one may store the gsequences in the generalized memory  then input x as a probe  and hope that the appropriate gsequence is recalled. 
applications of a similar nature arise in other settings. formally  the extended problem is defined as follows. 
we are given a positive integer n and fixed alphabets 
 a gtuple of length n  at is a tuple t of length n in which each component ti is an arbitrary subset of the alphabet a binary vector is a special case 
in which 	for all 	i   a n d each compo-
nent of t is a singleton set  i.e.  1 or 1 . the generalized associative memory problem is to store a given collection of m gtuples on given alphabets in such a way that 
1. most if not all the gtuples  are stable. 
1. the stable gtuples tp have reasonably large basins of attraction around them. 
　our main results are as follows. in this paper a recurrent neural network model for the generalized associative memories problem is proposed. the model is shown to have the stable storage property  i.e. all stored gtuples are necessarily fixed points of the network. spurious fixed points may develop however. this is expected to make full retrieval performance poor on large stored collections. here it is demonstrated that two limited types of retrieval-novelty detection and noise removal-work well even on large stored collections. with regards to the first  the following application is demonstrated. a large collection of generalized consensus sequences representing important multiple alignments of protein sequences is stored in the model. the model is shown to work well for detection of novelty in probe generalized sequences 
	duffy and jagota 	1 

against this stored set. with regards to the second  the following application is demonstrated. a very large collection of binary images is stored in the model by coding these images as gtuples. the model is shown to work well in removing noisy black pixels in noisy versions of stored images. 
1 the model 
the model generalizes a model introduced earlier for the special case in which all gtuples are tuples  i.e.  each component of a gtuple is a singleton set   jagota et al.  1 . 
　let and be the minimum and maximum lengths of gtuples on alphabet  to be stored. such gtuples will be called library gtuples. the model structure is described by a  partite  graph g =  v e  whose vertices represent neurons and edges their connectivity in the associated network. for convenience we describe all storage and retrieval operations on this graph; the correspondence to the connectionist network is established in a separate section. the vertex set v of g will be 
 that is  v contains 
a vertex for every possible pair  alphabet-symbol  position  and for the various possible lengths the edge set e of g will depend on the library gtuples 
 to describe the storage process the following notation is useful. for a given gtuple t of length n define n sets 
is an element of component  
the sets are subsets of the vertex set v g  of the graph g. define the join of two sets and as the set of edges joining every vertex 
in 	with every vertex in  
　the vertices in v g  are partitioned into 	which we will call columns. these columns induce an 
partite structure on g. 
　during our exploratory work we found that we were able to assure that the connectionist memory had the 
stable storage property only under the following restriction on the library gtuples. let k =  denote an n1-tuple of fixed positive integer values. we require that component i of every library gtuple contain exactly  elements from such gtuples are called k-uniform gtuples. though this condition is very restrictive  it turns out that collections of arbitrary gtuples can be stored by recoding them to k-uniform gtuples for an appropriate choice of k  see below . 
　we now describe the storage process. the k-uniform library gtuples are presented sequentially  once each  to the storage algorithm. initially  before any gtuple is presented  g has zero edges. when gtuple  of length n is presented  the graph g is modified as follows. 
  for each unordered pair the edges in the join are added to g  if some are already present  they are not added . 
1 	machine learning 
  for each i = 1 ...  n  edges in the join are added to g if previously absent. 
　we now explain how arbitrary gtuples may be recoded to uniform ones. given a collection of gtuples  for each position let denote the largest positive integer such that there exists a gtuple in whose component contains values from the alphabet we now expand the alphabet to include more symbols we then recode 
each library gtuple as follows: whenever component of library gtuple tp contains values we add the values to this component. under this recoding all library gtuples are now k-uniform. 
　from the storage rule  the structure of a stored gtuple as represented in the resulting graph g  after all the recoded gtuples have been stored  is readily apparent. this structure will lead to our definition of memory. for a given gtuple t of length n t   define 

that is  the set of vertices in g associated with t. let denote the subgraph of g induced by v t . if t is a library gtuple  has the following structure: for every is joined with  for every is joined with the vertex labeled 
n t . furthermore  for all i. we will call this structure a k-uniform complete  n t  + 1 -partite subgraph of g. 
　the structure of the previous paragraph leads to the following definition of memory  parametrized by the uniformity vector k. we will say that a subgraph g u  of g induced by a vertex set  is a memory if and only if g u  has the following structure: 
1. u contains no more than one vertex from columns 
 if u contains such a vertex  call 
it un. 
1. u contains no more than ki vertices from column   columns  and 
1. u is a maximal vertex set satisfying conditions 1 and 1 under the constraints imposed by conditions 1 and 1. 
　the memory definition of the previous paragraph is attractive for the following reasons. first  the k-uniform complete  subgraph associated with a 
 recoded  library gtuple t satisfies this definition. hence every  recoded  library gtuple is stored stably in the sense that it is necessarily recorded as a memory. second  a hopfield-type connectionist network may be associated with our graph g in such a way that this definition coincides with the fixed points of such a network. one consequence of this fact is that a memory is retrievable by simple hill-climbing. in the connectionist context it is also worth noting that  by contrast with our scheme  it is 

impossible to store even three binary vectors stably in a 
binary hopfield memory in the worst case  dembo  1; abu-mostafa and jacques  1 . 
　although all library gtuples are stored stably  spurious memories may develop. a spurious memory is a memory  see definition of memory given in an earlier paragraph  that is not associated with a library gtuple. it is the spurious memories that interfere with associative retrieval operations of the model. a good theoretical characterization of how spurious memories emerge in our model  as a function of the library gtuples  appears difficult. in this paper we indirectly assess the damage caused by spurious memories in various application contexts. 
1 	illustrative example 
we now work through an example. consider a multiple alignment of a set of dna sequences of identical length. 
acccat 
acacat ccccgt 
tcccat 
 with some loss of information  we describe this alignment by the generalized consensus sequence 

　consider now a collection of generalized consensus sequences of varying length. 

to store this collection in our model we first determine k =  1 1 . we then recode each gsequence to make it k-uniform in the manner described earlier  and then store the resulting collection according to the storage algorithm described earlier. figure 1 shows the graph formed after all the recoded gsequences have been stored. 
1 	connectionist correspondence 
the graph g formed after the recoded k-uniform gtuples have been stored may be represented as a connectionist  hopfield-type  network in such a way that there is a oneto-one correspondence between the fixed points of the network and the memories of the graph  see the graphstructural definition bf memory in a previous section . the resulting connectionist network is an extension of the one in  jagota et al.  1 . 
　the neurons in the network are the vertices in g. symmetric weights between pairs of neurons are added as follows. 
1. each edge in the graph is represented by a weight of 1. network. 
1. each non-edge in the graph between two vertices not in the same column is represented by a sufficiently negative weight. 
1. each pair of vertices i j in the same column is connected by a weight of -1. 
1. each pair of vertices i  j representing gtuple lengths is connected by a sufficiently negative weight. 

figure 1: graph formed after storing the 
r e c o d e d g s e q u e n c e s 
and solid  dashed  
and dotted lines represent those added to the graph when the first  second  and third of the above gsequences is stored  in this order. this distinction is only for illustration sake; in the graph all edges are identical. 
1. 
all self-weights are 1. 
　biases  constant external inputs  to neurons are as follows. 
1. each tuple-length neuron  neuron labeled  + 
1 ...   has a bias of 1. 
1. each neuron in column i  including the expandedalphabet neurons in that column  has a bias of ki - 1. 
　this yields a hopfield network with binary 1 neurons and a symmetric  zero-diagonal weight matrix  whose fixed points are the local minima of a certain energy function  hopfield  1 . the one-to-one correspondence between these fixed points and the memories of the graph is easily established. the weight condition 1 ensures that in a fixed point if two neurons from different columns are on  then the associated vertices are adjacent in the graph. the weight condition 1 ensures that in a fixed point at most one tuple-length neuron is on. the bias condition 1 taken together with the weight condition 1 ensures that in a fixed point not more than k{ neurons are on in column i. 
1 results 
as noted earlier  when a large number of library patterns are stored  the model is not expected to work 
	duffy and jagota 	1 

well on full retrieval of library patterns from distorted probe patterns. this is due to the compact architecture and distributed storage mechanism  which leads to a low error-correction capacity  hertz et al.  1 . it turns out nevertheless that limited kinds of retrieval on large collections of stored library patterns is feasible. the first kind is novelty detection in which one wishes to test if a probe pattern is novel relative to the collection of library patterns. the second kind is noise removed in which one wishes to filter out noise from a 
probe pattern obtained by adding noise to a library pattern. the precursor of this model has been shown to work well on both these kinds of retrieval  jagota  1; duffy and jagota  1 . here we demonstrate that the generalized model works on these types of retrieval as well. 
1 	novelty detection 
to demonstrate novelty detection  we picked a significant task from bioinformatics. we start with a dataset of 1 blocks  each block representing a multiple alignment of segments of certain proteins  assembled by  henikoff and henikoff  1 . we convert each block to a generalized consensus sequence on the twenty-letter protein alphabet. the resulting collection of 1 gsequences is stored in the model. the average length of a gsequence in this collection is 1; the average size of the set in a position in a gsequence is 1. 
　to assess the model's performance we distorted each library gsequence by replacing each value v in the valueset stored at position j by a random alphabet-symbol with probability p. figure 1 reports  as a function of p  the percent of the 1 probe gsequences that were detected as not in the library collection by the model. for p = 1  as expected  none of the probe gsequences was flagged as novel because all are the library patterns; hence stable. the percentage of probe patterns detected as novel increased as p was increased  which demonstrates that the model works well on this task. 
　it turns out that the model is also able to report  an efficiently-computable  score for each probe gsequence indicating its distance from the stored library collection. the following definition of score generalizes the one in  duffy and jagota  1 . for a vertex set u = u t  associated with a probe gsequence t define score u  as the number of pairs of vertices in u that are in different columns and are non-adjacent divided by the number of pairs of vertices in u in different columns. 
　figure 1 reports  as a function of p  the average score of the probe patterns. we see that the average score increases linearly with p. this validates the suitability of this scoring scheme. 
1 	noise removal 
to demonstrate noise removal  we picked the following task from image processing. we stored in our model a dataset of 1 binary images of handwritten digits. each image was roughly 1 x 1 pixels  with approximately 1 of them being black. to store an image in 
1 	machine learning 

our model we coded it as a gtuple as follows. each column of an image represented a position in the gtuple. the positions of the black pixels in column t in an image was the value-set stored in component t i  of the associated gtuple. 
　as our test set  we picked 1 of these images and distorted them by converting each white pixel into a black pixel with probability 1. on average this added 1 pixels of black noise in the probe image. we then used a 
　noise-removal algorithm in conjunction with our model that simply converts  sequentially  those black pixels to white pixels in the probe gtuple that are maximally nonadjacent to other black pixels. we found that on average this mechanism cleaned up 1 of the 1 noisy black pixels in the probe. 
　this task may also be mapped to the original connectionist model  jagota  1   by treating each library image as a  1 x 1 -bit binary vector. however this has some drawbacks. the resulting network has twice as many neurons. more importantly  this coding is dense  half the neurons in the network participate in each memory . by contrast only about l/1th of the neurons in the network  1 out of 1 x 1  participate in a memory in the generalized network. 
1 current and future work 
one appealing feature of this model is that it may also be used to  efficiently  assign a score to a probe sequence-a type of  distance  from the entire stored library collection. this opens up some interesting application possibilities  that we are now beginning to investigate. 
