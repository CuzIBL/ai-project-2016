 
the paper describes research efforts to develop efficient implementation techniques for large  shared knowledge bases  focusing on efficient concurrent access of large knowledge bases by multiple users. we present an algorithm  called the dynamic directed graph policy  originally proposed in  chaudhri et a/.  1   which allows efficient interleaved execution of transactions against a large knowledge base with the intent of optimizing transaction throughput. the implementation of the policy and experimental evaluation results are also presented and discussed. the paper concludes with discussion on lessons learnt from this research. 
　large knowledge bases containing millions of facts will soon be here  thanks to research efforts such as the knowledge sharing initiative  patil et a/.  1  and the 
cyc project  guha and lenat  1 . however  tools for building knowledge bases do not scale up to accommodate such large knowledge bases. our efforts are focusing on the adoption of database techniques to build knowledge base building tools that do scale up. 
　one of the requirements of such tools is that they accommodate efficient multi-user access of a single  large knowledge base by maximizing throughput  i.e.  the number of user-defined transactions that are executed against the knowledge base per time unit. a comparable requirement for databases is addressed by concurrency control mechanisms that are routinely offered by database management systems  which have been shown to improve throughput by as much as an order of magnitude or more. 
　a comparable concurrency control algorithm  specifically designed for knowledge bases and called the dynamic directed graph policy was proposed in  chaudhri et a/.  1 . the main purpose of this paper is to describe an implementation of that policy and to present 
　*this paper is based on joint work with prof. vassos hadzilacos and prof. ken sevcik. it was supported by the university of toronto  the information technology research center of ontario  the natural science and engineering research council of canada and the institute of robotics and intelligent systems. 
performance results which compare the performance of the proposed policy against the performance of offthe-shelve concurrency control mechanisms designed for databases. the paper summarizes some of our findings and concludes with research directions. 
　the outline of this paper is as follows. in section 1  we begin by motivating the problem. in section 1  we present the dynamic directed graph  ddg  policy. in section 1  we briefly describe some problems that were faced while implementing the policy. in sections 1  we present the evaluation of the algorithm for knowledge base applications  and in section 1  discuss some related work. in section 1  we discuss the lessons learnt from our research and conclude in section 1 with a summary. 
1 	problem statement 
concurrent processing of user requests can lead to large speed ups in user response time as compared to processing the requests of one user at a time  gray and reuter  1 . arbitrary concurrency can  however  lead to inconsistent information in the knowledge base and one must make sure that the concurrent executions are serializable  bernstein et a/.  1 : interleaved execution of a set of transactions must be equivalent to some serial execution of the same collection of transactions in the sense that it leaves the knowledge base in the same state and returns the same answers to the users. 
　most commercial database systems use locktng-based algorithms to ensure serializability. the best known locking algorithm  two-phase locking  1pl   eswaran et a/.  1   works along the following lines. associated with each data item is a distinct  lock . a transaction must acquire a lock on a data item before accessing it. while a transaction holds a lock on a data item no other transaction may access it. a transaction cannot acquire any additional locks once it has started releasing locks  hence the name  two-phase  locking . in a simple generalization of this model  the transactions may hold shared and exclusive locks on data items. the instant when a transaction has acquired all the locks that it will ever need is called its locked point. 
　transactions in a knowledge base system often access large number of data items  for example  while inferencing over long rule chains. such a transaction may potentially access all the nodes that are descendants of the 
	chaudhri and mylopoulos 	1 

goal in the inference graph from which it begins execution. similar long transactions are generated while performing truth maintenance operations in a knowledge base. in such situations  1pl implies that a transaction will have to hold each lock until its locked point thereby locking most of the knowledge base for other users. hence  concurrency is significantly reduced when running such  global  transactions. for this reason  our research has been directed towards the development of new methods that only hold a small number of locks at any one time  even for global transactions. 
　interestingly  knowledge bases generally possess much richer internal structure  e.g.  generalization and aggregation hierarchies  deductive rules  temporal dimensions defined in terms of history or belief time  etc.  than that of traditional databases. information about this structure can be potentially useful in allowing early release of locks. indeed  a concurrency control algorithm does exist for databases that have a directed acyclic graph structure  and is accordingly called dag policy  silberschatz and kedem  1; yannakakis  1 . under the dag policy  a transaction may begin execution by locking any item. subsequently  it can lock an item if it has locked all the predecessors of that item in the past and is currently holding a lock on at least one of those parents. moreover  under the dag policy a transaction may only lock an item once. the dag policy exploits the assumption that there are no cycles in the underlying structure and the structure does not undergo any change. unfortunately  such a policy cannot be adopted for knowledge bases without modifications. the structure of a knowledge base is likely to contain cycles  e.g.  the inference graph generated for a collection of recursive rules  and will undergo change  e.g.  when rules are added or deleted . 
　in summary  neither 1pl nor dag policy is  by itself  appropriate for knowledge bases. 1pl is too conservative  thereby causing reduced concurrency  while dag policy does not provide sufficient functionality. accordingly  we are proposing a new graph-based policy  the dynamic directed graph policy  ddg  that can handle cycles and updates in the knowledge base and also allows release of locks before the locked point of a transaction  thereby promising better performance than 1pl. 
	1 	the dynamic directed graph policy 
the knowledge bases that we have in mind are assumed to support an object-oriented representational framework with an assertional sub-language used for both deductive rules and constraints. also  possibly  they might support facilities for representing special kinds of knowledge  for example  temporal knowledge  incomplete knowledge  etc. . a large class of knowledge bases can be represented in terms of such directed graphs  not only the ones based on semantic nets  frames but also description logics or even logics  plexousakis  1; borgida and patel-schneider  1 . therefore  for the purposes of concurrency control  a knowledge base is a directed graph g v e   where v is a set of nodes  e.g.  employee   and e is a set of edges which are ordered pairs of nodes  e.g.   manager employee  . 
1 	knowledge base technology 
　we first define some properties of directed graphs that are necessary for specifying our algorithm. a root of a 
　directed graph is a node that does not have any predecessors. a directed graph is rooted if it has a unique root and there is a path from the root to every other node in the graph. a directed graph is connected  if the underlying undirected graph is connected. a strongly connected component  scc  g  of a directed graph g is a maximal 
set of nodes such that for each a  b ♀ g   there is a path from a to b. an scc is non-trivial if it has more than one node. an entry point of an scc  g   is a node b such that there is an edge  b  a  in g  a is in g '  but b is not in g -. thus  if a node is an scc by itself  its entry points are simply its predecessors. 
　the dominator d of a set of nodes w is a node such that for each node a g w  either every path from the root to a passes through d or d lies on the same strongly connected component as a. thus  in a rooted * graph  the root dominates all the nodes in the graph including itself. all nodes on a strongly connected component dominate each other. 
　the ddg policy has three types of rules. preprocessing rules convert an arbitrary graph to a rooted and 
connected graph. locking rules specify how each transaction should acquire locks. maintenance rules specify additional operations that must be executed by transactions to keep the structure rooted and connected. the rest of the discussion in this section focuses on locking rules. a detailed description of the ddg algorithm appears elsewhere  chaudhri  1 . 
　a transaction may lock a node in shared or exclusive mode  bernstein et al.  1 . two transactions may simultaneously lock a node only if both lock it in shared mode. the locking rules are as follows: 
li. before a transaction t performs any insert  
delete or write operation on a node a  or an edge 
 a b    t has to lock a  both a and b  in exclusive mode. before t performs a read operation on a node a  an edge  a  b    it has to lock a  both a and b  in either mode. 
l1. a node that is being inserted can be locked at any time. 
l1. each node can be locked by t at most once. l1. the first lock obtained by t can be on any node. if the first node locked by t belongs to a non-trivial scc  all nodes on that scc are locked together in the first step. 
subsequently  
l1. all nodes on an scc are locked together if: 
　l1a. all entry points of that scc in the present state of g have been locked by t in past  and t is now holding a lock on at least one of them  and 
　l1b. for every node a on this scc that is a successor of an entry point  and every path a1 ...  ap a p   1  in the present state of the underlying undirected graph of g  such that t has locked a   in any mode   and a1.     ＊  ap in shared mode  t has not unlocked any of a1 . ..  ap so far. 
　as an example application of the ddg-sx policy consider the knowledge base and the transactions shown in figure 1.  ls and lx respectively denote the acquisition of lock in shared and exclusive mode  u denotes release of lock on a node and  u *  denotes the release of all the locks held by a transaction.  t1 begins by locking node 1  locking rule l1  in shared mode and then locks node 1 in exclusive mode  locking rule l1 . it locks the nodes 1 and 1 which form a strongly connected component in shared mode in one step  locking rule l1 . it is able to do so because the condition l1b is satisfied for each path from node 1 to node 1 and 1. t1 locks node 1 in exclusive mode and finishes execution. t1 begins by locking both nodes 1 and 1  locking rule l1  and then locks node 1 and finishes execution. if t1 adds the edge  1   locking rule li   then t1 will be unable to lock node 1 because in order to do that it must lock node 1 which is a predecessor of node 1 in the current state of the graph  locking rule l1a . t1 must abort and start from node 1. 
theorem: the ddg policy produces only serializable schedules  chaudhri  1 . 
　the ddg policy does not permit concurrency within cycles  see rule l1 above  suggesting that if a knowledge base contains cycles  concurrency will be reduced. we have a version of the ddg policy that permits concurrency within cycles  chaudhri et a/.  1 . we adopted the above version  because the transactions in knowledge bases tend to access all the nodes on a cycle together  and therefore  the cycles are a natural unit of locking. 
　for a transaction to be able to satisfy locking rule l1 for all the nodes that it needs to lock  it has to begin by locking the dominator of all the nodes that it is going to access. this is not a contradiction to locking rule l1  which just says that to lock the first node  no other condition needs to be satisfied. 
1 	implementation of the ddg policy 
the ddg policy has been implemented in the denet  livny  1  simulation environment. the implementation ideas that we present here are independent of any specific system. 
　there are two main issues in the implementation of the ddg policy. first  to enforce the rules of the locking policy  we need to compute and maintain information about several graph properties. second  we need a mechanism to decide the order in which the locks should be acquired and released. 
　to enforce the locking rules  we need information on the dominator relationships and the strongly connected components within the knowledge base graph. in our implementation  the dominator tree of the knowledge base is computed at compile time using a bit vector algorithm  chaudhri  1j. using this information  the dominator of the set of nodes in the transaction can be computed in time linear in the length of a transaction. the dominator information is maintained using an incremental algorithm  caroll  1 . the information on strongly connected components is computed at compile time using depth-first search. there was no algorithm available for incrementally maintaining information on strongly connected components as the knowledge base evolves  and therefore  we developed an algorithm for this purpose  chaudhri  1 . 
　let us describe the order in which a transaction acquires and releases locks. a transaction always begins by locking the dominator of all the nodes that it might access. the dominator is computed on the assumption that a transaction may access all the descendants of the first node on which it requests a lock. subsequently  every time a lock is requested  the locking conditions are checked  and if not enough predecessors are locked  rule l1a   lock requests for them are issued recursively. before a node a can be unlocked by a transaction t  following conditions must be satisfied: 
u1. a is no longer needed by t  and 
u1. releasing the lock on node a does not prevent the locking of any of its successors at a later stage in the execution of t  as required by rule l1a   and u1. for every path a  a1 ...  ap  b in the present state of the underlying undirected graph  such that a is locked  in any mode   a  .. .  ap are locked in shared mode  t intends to lock b in future  t must not unlock any of apa  ...  ap  by locking rule l1b . 
　to implement ul  we require t to send a message to the lock manager when it has finished processing a node. 
　to implement u1  we have to know how many of the descendants might be later locked by t. moreover  of all the predecessors of a node a  only one has to be kept locked until t locks a. therefore  we distinguish one of the predecessors that needs to be locked until all the successors have been locked  and associate with it the number of successors that are yet to be locked. once the number of successors yet to be locked for a node becomes zero  u1 is satisfied. 
　to implement u1  we check all the undirected paths from a to all the nodes that t may lock in future. u1 needs to be checked only when t acquires an exclusive lock or when t locks a node none of whose descendants will be locked by it in future. the check can be made more efficient by observing that if u1 is not satisfied for a node a  it is also not satisfied for descendants of a  
	chaudhri and myl1ul1s 	1 

class 1 


and therefore  the paths passing through them need not be checked. 
	1 	evaluation of the ddg policy 
our performance model is similar to an earlier model  agrawal et a/.  1  and has four components: a source  which generates transactions  a transaction manager  which models the execution of transactions  a concurrency control manager  which implements the details of a particular algorithm; and a resource manager  which models the cpu and i/o resources of the database. 
　performance of the ddg policy was studied on a knowledge base under development for industrial process control  mylopoulos et ai  1 . the application is called advanced process analysis and control system  apacs . the objects represented in the knowledge base  boilers  valves  preheaters  alarms  etc.  are organized into a collection of classes  each with its own subclasses  instances and semantic relationships to other classes. there are several semantic relationships in this knowledge base. the isa relationship captures the classsubclass relationship  the instanceof relationship represents the instances of a class  and the linkedto relationship stores how the components are linked to each other in the power plant. 
　for our experiments  we view this knowledge base as a directed graph. each class and each instance is represented by a node. there are 1 nodes in this graph. there is an edge between two nodes if they have some semantic relationship. for example  there is an edge from node a to node b  if the object represented by a is a part of the object represented by node b. 
　the knowledge base receives two classes of transactions. class 1 transactions are long and traverse the knowledge base along one of its structural relationships. class 1 transactions are short and look-up or update an attribute value and occasionally change the structural 
1 	knowledge base technology 
relationships in the knowledge base. the proportion of the transactions in class 1 was determined to be 1% with the remaining 1% being in class 1. 
　since there are several semantic relationships in the knowledge base  we can selectively use one or more of these as the graph to be used for concurrency control. making a semantic relationship known to concurrency control gives it information on the entities to be accessed by transactions that traverse that relationship  but on the other hand  for transactions that do not traverse this relationship  it may mean locking more nodes than they actually require. we analyzed this tradeoff  chaudhri  1  and concluded that for the apacs workload  using the graph defined by the union of isa and instanceof relationships leads to the best performance  and therefore  used it as the graph with respect to which the locking rules of the ddg policy were applied. 
　in the apacs application  class 1 transactions do not specify the traversal strategy that should be used  and therefore  one can choose a strategy that leads to better performance. we considered two traversal strategies: depth-first traversal  dft  and breadth-first traversal  bft . in general  transactions using a dft hold a lesser number of locks on average as compared to the average number of locks held while using a bft. to quantify the difference  we show  in figure 1  the percentage improvement in the class 1 response times as obtained when class 1 transactions use dft as compared to bft. we do not show the improvement in class 1 response time as it was found to be insignificant. the improvement is shown as a function of multiprogramming level of class 1 transactions and the write probability. multiprogramming level  mpl  is the number of transactions that are concurrently executing at any given time and can be controlled by the system administrator. permitting too few active transactions  or a low mpl  may not exploit full benefits of concurrency and permitting too many active transactions  or a high mpl  may lead to excessive contention and degradation in user response time. write probability is the proportion of accesses performed by a transaction that are updates. if r j bft and r j dft are the mean response times of transactions in the class j then the percentage improvement of dft over bft is computed as  r j bft ~ r j dft /r j bft * 1. we can see that  even at high write probabilities  by using dft  class 1 response times showed consistent improvement. this result shows that in a multi-user knowledge base environment  while using the ddg policy  dft is a more desirable traversal strategy than bft. 
　in figure 1  we plot the percentage improvement in response time of class 1 transactions obtained by using the ddg policy as compared to 1pl. if r{j 1pl and r j ddg are the mean response times of transactions in class j  then the percentage improvement of the ddg policy over 1pl is computed as 1 x  r j 1pl -
r{j ddg /r{j 1pl-
the results for class 1 transactions indicate that when 
class 1 transactions are read only  the performance of the ddg policy is comparable to 1pl. when class 1 transactions also perform some updates  the ddg policy can improve considerably  of the order of 1%  the response time of class 1 transactions that are running concurrently. the results for class 1 transactions are not shown here. we found that at low update probabilities there was slight degradation  about 1%  in class 1 response time by using the ddg policy and at high update probabilities there was no significant difference between the two algorithms. these results make sense  because when there are only shared locks in a transaction  the ddg policy cannot allow any release of locks before its locked point  but incurs extra overhead  and therefore  leads to a slight degradation in response time. on the other hand  if the class 1 transactions are update intensive  they release locks before their locked point  and the extra overhead is more than offset by the increased concurrency obtained due to lock pre-release leading to a net improvement in class 1 response time. 
　in figure 1  we can observe that the improvements are smaller at high multiprogramming levels. at low multiprogramming levels  for example  at mpl=1   any release of locks by a transaction before its locked point contributes to the improvement in response time of concurrently executing class 1 transactions. at higher multiprogramming levels  this may not be necessarily true  because the lock released by a class 1 transaction could be acquired by another class 1 transaction giving no benefit to class 1 transactions. as a result  we see greater improvements in the class 1 response time at low multiprogramming levels of class 1 as compared to improvements at high multiprogramming levels. 
　the overall system response time can be computed as the weighted sum of the individual class response times where class throughputs are used as weights. in the apacs workload the throughput of class 1  1 transactions/second  is much higher than the throughput of class 1  approximately 1 transactions/second   and therefore  the behavior of class 1 transactions dominates the overall response time. on computing the improvements in the overall response time  we found that improvements were very close to the corresponding values for class 1 response time as shown in figure 1. 
　in view of the relative behavior of the two algorithms  we designed an adaptive scheme which can switch between 1pl and the ddg policy depending on the load conditions. such a scheme uses 1pl at low write probabilities and the ddg policy at high write probabilities  thus giving the best of the two algorithms. since simultaneously using the two algorithms may lead to non-serializable schedules the adaptive scheme uses a transition phase while switching from one algorithm to 
	chaudhri and myl1ul1s 	1 


1 	knowledge base technology 

1 	related work 
the present paper summarizes and enhances the results published earlier which focused on details of correctness  chaudhri et a/.  1  and performance  chaudhri et a/.  1  issues. a detailed description of the results can be found in the first author's doctoral dissertation  chaudhri  1 . 
　knowledge base implementations that have tried to address the issue of concurrent access to a shared knowledge base include cycl  guha and lenat  1   krep  mays et a/.  1  and proteus  ballou et a/.  
1 . in cycl  a copy of the whole knowledge base is made and given to each of its users. users independently make changes to the database and submit it to a central server that tries to detect inconsistencies. if it finds any inconsistencies  it passes them along to the persons who made the changes. otherwise  it propagates the changes to all users. the k-rep system uses similar ideas and allows users to work on different versions of the knowledge base and provides facilities for merging the versions. the basic assumption in the cycl and krep approach is that the cost of detecting and repairing inconsistency is much smaller as compared to the cost of preventing it by ensuring serializability. the cycl and k-rep approach is feasible for a small knowledge base but it is unclear if it will scale up to more realistic knowledge base sizes. proteus system uses the concurrency control facilities of an underlying object-oriented system orion. the concurrency control mechanism in orion is 1pl with locking granularities based on aggregation and generalization hierarchies. 
　to deal with long transactions  there have been other proposals  agrawal and abbadi  1; salem et a/.  1  that permit more concurrency than 1pl  but they do not exploit the semantic structure of the database. 
1 	lessons learnt 
an interesting aspect of our results is the integration of knowledge model features with low level implementation issues. the integration is reflected in the design of the ddg policy that abstracts the knowledge model into a directed graph  and in performance evaluation which is done as a function of data model features. 
　for example  the presence of cycles in a knowledge base influences the design of the locking policy and the amount of concurrency permitted by it. this indicates that if there are large cycles in the knowledge base that are repeatedly accessed by a transaction then concurrency will be limited. similarly  we found that a depthfirst traversal strategy led to a better response time as compared to a breadth-first strategy. traditionally  traversal strategy has been under the control of a query processor and the designs of transaction manager and query processor have been considered in isolation. our result on the influence of traversal strategy on concurrency control performance shows that the interaction between query processor and concurrency control can play an important role in the overall performance. 
　to get better advantage of the semantic structure  the knowledge base should be highly structured: it should be almost like a tree  high depth factor  and should not have large fanin and fanout  low fanin and fanout factors  . this has an interesting implication for knowledge base implementations that pre-compute all subsumption relationships. pre-computation of all subsumption relationships is equivalent to creating a large fanin and fanout in the subsumption hierarchy and in such a situation the advantages of using the semantic structure using the ddg policy will be reduced. 
　our research illustrates a general framework for incorporating database functionality into knowledge bases. the solution space to achieve this includes  mylopoulos and brodie  1 : coupling an ai system with a database system  loose or tight coupling  or devising an integrated solution  in an evolutionary or a revolutionary way . we adopt and recommend an evolutionary approach  because it is pragmatic and consistent with software reusability. 
　an evolutionary approach to design an integrated aidb solution should be a two step process. in the first step  one should view the problem as a database tuning problem  shasha  1   and if necessary  one should take the second step and solve a database kernel design problem. by solving a kernel design problem we mean addressing a core database concern such as storage design or query optimization. let us illustrate this approach by considering the problem of concurrent access addressed in the present paper. to support concurrent access to a knowledge base  one would start with a database solution which in this case is 1pl. to tune 1pl for long transactions  one would chop the long transactions into smaller transactions  shasha  1  or impose a correctness criterion which is weaker than serializability. if the long transaction problem still persists one would change the database kernel and augment 1pl with the ddg policy and use an adaptive algorithm that consistently gives a better performance than 1pl. these two steps taken together would give a comprehensive integrated solution for concurrent access in a knowledge base environment. 
　in summary  we feel that to develop a technology for constructing knowledge bases  we need to address the core database issues for knowledge models. a subset of these concerns may be addressed by tuning existing database products but a long term solution would require an integrated approach that makes fundamental changes in the database kernel. 
1 	summary and conclusions 
in this paper  we considered the problem of supporting concurrent access to large knowledge bases. we argued that knowledge base operations such as inference over long rule chains and truth maintenance lead to the problem of long transactions which cannot be efficiently solved by existing database techniques. we argued that we can use the rich semantic structure of knowledge bases to devise a more viable solution for long transactions. we showed this by presenting the design  implementation and evaluation of an algorithm called dynamic directed graph  ddg  policy that gives better response time than 1pl at high update rates. we showed that 1pl can be augmented with the ddg polchaudhri and myl1ul1s 1 
icy to give a hybrid algorithm that consistently performs better than a system that uses only 1pl. we also discussed the impact of our results on the knowledge base design and presented a refinement of the evolutionary paradigm for constructing knowledge base management systems. in our current work  we are addressing the issues of fault-tolerance for the ddg policy. 
　in conclusion  we would like to note that the results presented in this paper are at the intersection of knowledge base and database systems. research combining techniques from these two fields will be of prime importance in future. one of the major technological innovations in the coming years would be development of cooperative information systems involving a large number of intelligent agents distributed over computer/communication networks. developing such systems presents a unique opportunity in which techniques from both knowledge bases and databases will play a crucial role. the results presented in this paper make a 
　modest contribution towards this goal. 
