 
we present a new learning algorithm for pattern recognition inspired by a recent upper bound on leave-one-out error  jaakkola and haussler  1  proved for support vector machines {svms   vapnik  1; 1 . the new approach directly minimizes the expression given by the bound in an attempt to minimize leave-one-out error. this gives a convex optimization problem which constructs a sparse linear classifier in feature space using the kernel technique. as such the algorithm possesses many of the same properties as svms. the main novelty of the algorithm is that apart from the choice of kernel  it is parameterless - the selection of the number of training errors is inherent in the algorithm and not chosen by an extra free parameter as in svms. first experiments using the method on benchmark datasets from the uci repository show results similar to svms which have been tuned to have the best choice of parameter. 
1 	introduction 
support vector machines  svms   motivated by minimizing vc dimension  have proven to be very successful in classification learning  vapnik  1; scholkopf  1; vapnik  1 . in this algorithm it turned out to be favourable to formulate the decision functions in terms of a symmetric  positive definite  and square integrable function k -   referred to as a kernel the class of decision functions - also known as kernel classifiers  jaakkola and haussler  1  - is then given by 
for simplicity we ignore classifiers which use an extra threshold term. 
　recently  utilizing this particular type of decision rule  that each training point corresponds to one basis function  an upper bound on leave-one-out error for svms was proven  jaakkola and haussler  1 . this bound motivates the following new algorithm: find a decision rule of the form in equation  1  that minimizes the bound. the paper is structured as follows: in section 1 we first review the svm algorithm. in section 1 we describe the leave-one-out bound and the leave-one-out support vector machine  loom  algorithm motivated by the bound. in section 1 we reveal the relationship between svms and looms and in section 1 results of a comparison of looms with svms on artificial and benchmark datasets from the uci repository are presented. finally  in section 1 we summarize and discuss further directions. 
1 	support vector machines 
support vector machines  vapnik  1  aim to minimize vc dimension by finding a hyperplane with minimal norm that separates the training data mapped into a feature space via a nonlinear map to construct such a hyperplane in the general case where one allows some training error one minimizes: 

and then uses the decision rule: 

　the tractability of this algorithm depends on the dimensionality of f however  one can remove this dependency by instead maximizing the dual form: where  utilizing that 

	weston 	1 

and that the decision rule is now in the form of equation  1 . 
　alternatively  one can also use the primal dual formulation of the svm algorithm  from  osuna and girosi  1   rather than the usual formulation  which we will describe because of its direct correlation to leave-oneout svms  the svm primal reformulation is the following: minimize 

where one again uses a decision rule of the form in equation  1 . 
1 	leave-one-out 	support vector machines 
support vector machines obtain sparse solutions that yield a direct assessment of generalization: leave-one-out error is bounded by the expected ratio of the number of non-zero coefficients to the number of training examples  vapnik  1 . in  jaakkola and haussler  1  a measure of generalization error is derived for a class of classifiers which includes svms but can be applied to non-sparse solutions. the bound is as follows: 
theorem 1 for any training set of examples 
and labels  for a svm trained by maximiz-
ing equation  1  the leave-one-out error estimate of the classifier is bounded by 
where 
　this bound is slightly tighter than the classical svm leave-one-out bound. this is easy to see when one considers that all training points that have  cannot be leave-one-out errors in either bound. vapnik's bound assumes all support vectors  all training points with  are errors  whereas they only contribute as errors in equation  1  if 

in practice this means the bound is tighter for less sparse solutions. 
　although the leave-one-out bound in theorem 1 holds for support vector machines the motivation behind svms is to minimize vc bounds via structural risk 
1 	machine learning 
minimization  vapnik  1 . to this end  the term in equation  1  attempts to min-
imize vc dimension. if we wish to construct classifiers motivated by theorem 1  that directly attempt to achieve a low value of this expression  we need to consider a different learning technique. 
　theorem 1 motivates the following algorithm: directly minimize the expression in the bound. to do this  one introduces slack variables following the standard approach in  cortes and vapnik  1; vapnik  1  to give the following optimization problem: minimize 

where one chooses a fixed constant for the margin to ensure non-zero solutions. 
　to make the optimization problem tractable  the smallest value for for which we obtain a convex objective function is = 1. this gives us a linear programming problem  and  as in other kernel classifiers  one uses the decision rule given in equation  1 . 
　note that theorem 1 is no longer valid for this learning algorithm. nevertheless  let us study the resulting method which we call a leave-one-out support vector machine  loom . 
1 	relationship to svms 
in this section  we will describe the relationship between looms and svms in three areas: the method of regularization  the sparsity induced in the decision function and the margin loss employed in training. 
1 	regularization 
the new technique appears to have no free regularization parameter. this should be compared with svms which control the amount of regularization with the free parameter c. for svms  in the case of c = one obtains a hard margin classifier with no training errors. in the case of noisy or linearly inseparable datasets1  through noise  outliers  or class overlap  one must accept some training error  by constructing a so called soft margin . to find the best choice of training error/margin tradeoff one must choose the appropriate value of c. in looms a soft margin is automatically constructed. this occurs because the algorithm does not attempt to minimize the number of training errors - it minimizes the number of training points that are classified incorrectly even when 
     1 here we refer to linear inseparability in feature space. both svms and loom machines are essentially linear classifiers. 

they are removed from the linear combination that forms the decision rule. however  if one can classify a training point correctly when it is removed from the linear combination then it will always be classified correctly when it is placed back into the rule. this can be seen 
as  is always the same sign as  all training points are pushed towards the correct side of the decision boundary by their own component of the linear combination. 
1 	sparsity 
like support vector machines  the solutions of the new algorithm can be sparse; that is  only some of the coefficients  are non-zero  see section 1 for computer simulations confirming this . as the coefficient of a training point does not contribute to its leaveone-out error in constraint  1  the algorithm does not assign a non-zero value to the coefficient of a training point in order to correctly classify it. a training point has to be classified correctly by the training points of the same label that are close to it  in feature space   but the training point itself makes no contribution to its own classification. 
1 	m a r g i n loss 
noting that 

where / x  is given in equation  1   one can see that the new algorithm can be written as the following equivalent linear program: minimize 

　in this setting of the optimization problem it is easy to see that a training point xi is linearly penalized for failing to obtain a margin of 
in svms  a training point xi is linearly penalized for failing to obtain a margin of  see equation  1  . thus the margin in svms is treated equivalently for each training pattern. in looms  the larger the contribution the training point has to the decision rule  the larger the value of  the larger its margin must be. thus  the loom algorithm  in contrast to svms controls the margin for each training point adaptively. 
　this method can be viewed in the following way: if a p o i n t i s an outlier  the values of  to points in its class are small and to points in the other class are large  then some  where 
equation  1  have to be large in order to classify correctly. svms use the same margin for such points and they attempt to classify correctly. in 
looms the margin is automatically increased to 1 + for these points and thus less attempt is made to correctly classify thus the adaptive margin provides robustness. moreover  it becomes clear that in looms the points  which are representatives of clusters  centres  in feature space  i.e. those which have large values of to points in their class  will have non-zero 
1 	experiments 
in this section we describe experiments comparing the new technique to svms. we first describe artificial data to visualize the techniques  and then present results on 
benchmark datasets. 
1 	artificial d a t a 
we first describe some toy two dimensional examples to illustrate how the new technique works. figure 1 shows two artificially constructed training problems  left and right  with various solutions  top to bottom of the page . we fixed the kernel to be a radial basis function  rbf  
 1  
with and then found the solution to the problems with leave-one-out machines  looms   which have no other free parameters  and with svms  for which one controls the soft margin with the free parameter c. the first solution  top of the page  for both training problems  left and right  is the solution given by looms  and the other four solutions are svms with various choices of soft margin  parameter c . 
　in the first problem  left  the two classes  crosses and dots  are almost linearly separable apart from a single outlier  a dot . the automatic soft margin control of looms constructs a classifier which incorrectly classifies the far right dot  assuming that it is an outlier. thick lines represent the separating hyperplane and dotted lines represent the size of margin. support vectors  training points with  are emphasized with rings. note also the large margin of the loom classification. the support vector solutions  second picture from top downwards  have parameters c = 1  middle  and c = 1  bottom . constructing a hard margin with c = 1 overfits with zero training error whilst with decreasing c the svm solution tends towards a decision rule similar to the one found by looms. note  however  even with c = 1 the non-smoothness of the decision rule by examining the margin  dotted line . moreover  the outlier here is still correctly classified. 
　in the second training problem  right   the two classes occupy opposite sides  horizontally  of the picture  but slightly overlap. in this case the data is only separable with a highly nonlinear decision rule  as reflected in the hard margin solution by an svm with parameter c = 1  bottom right . again  a reasonable choice of rule  c = 1  middle right picture  can be found by 
	weston 	1 

figure 1: two training problems  left and right pictures  are solved by leave-one-out svms  top left and top right  which have no soft margin regularization parameter and svms for various of c  lower four pictures . for svms  the two problems are solved with  1 = 1  middle row  and c = 1  bottom row . 
svms with the correct choice of free parameter. the 
 parameterless  decision constructed by the loom  top right  however provides a similar decision to the svm with c = 1. note again  the smoothness of the loom solution in comparison to the svm one  even though they are similar. 
　finally  figure 1 gives some insight into how the soft margin is chosen by looms. a simple toy training set is again shown. the first picture  left  has a small cluster of crosses in the top left of the picture and a single cross in the bottom right of the picture. the other class  dots  is distributed almost evenly across the space. looms construct a decision rule which treats the cross in the bottom right of the picture as an outlier. in the second picture  right  we have almost the same problem but near the single cross we add another two training points so there are now two clusters of crosses. now the loom 
1 	machine learning 
we conducted computer simulations using 1 artificial and real world datasets from the uci  delve and statlog benchmark repositories  following the same experimental setup as in  ratsch et a/.  1j. the authors of this article also provide a website to obtain the data1. briefly  the setup is as follows: the performance of a classifier is measured by its average error over one hundred partitions of the datasets into training and testing sets. free parameter s  in the learning algorithm are chosen as the median value of the best model chosen by cross validation of the first five training datasets. 
　table 1 compares percentage test error of looms to adaboost  ab   regularized adaboost  and 
svms which are all known to be excellent classifiers1. the competitiveness of looms to svms and 
 which both have a soft margin control parameter  is remarkable considering looms have no free parameter. this indicates that the soft margin automatically selected by looms is close to optimal. adaboost loses 

the datasets have been pre-processed to have zero mean and standard deviation one  and the exact one hundred splits of training and testing sets used in the author's experiments can be obtained. 
1
the results for ab  and svms were obtained by 
raetsch  et al. 

out to the three other algorithms  being essentially an algorithm designed to deal with noise-free data. 
ab abr svm~ loom banana b. cancer 
diabetes heart 
thyroid 
titanic 1 
1 1 
1 1 
1 1 
1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 1 
1 
1 
1 1 
1 table 1: comparison of percentage test error of adaboost  ab   regularized adaboost  ab r    support vector machines  svms  and leave-one-out machines  looms  on 1 datasets. 
　finally  to show the behaviour of the algorithm we give two plots in figure 1. the top graph shows the fraction of training points that have non-zero coefficients  svs  plotted against   rbf width  on the thyroid dataset. here  one can see the sparsity of the decision rule  cf. equation  1    the sparseness of which depends on the chosen value of  the bottom graph shows the number of training and test errors  train err and test err   the value of  slacks  and the value of the bound given in theorem 1 one can see the training and test error  and the bound  closely match which would be natural for an algorithm which minimized leave-one-out error. the minimum of all four plots is roughly at  indicating one could 
perform model selection using one of the known expressions. note also that for a reasonable range of different rbf widths the test error is roughly the same  indicating the automatic soft margin control overcomes overfitting problems. moreover  the values of which give the best generalization error also give the most sparse classifiers. 
1 	discussion 
in this article  motivated by a bound on leave-one-out error for kernel classifiers  we presented a new learning algorithm for solving pattern recognition problems. the robustness of the approach  despite having no regularization parameter  can be understood in terms of the bound  one must classify points correctly without their own basis function   in terms of margin  see section 1   and through empirical study. we would also like to point out that if one constructs a kernel matrix then the regularization technique employed is to set the diagonal of the matrix to zero  which suggests that one can control regularization through control of the ridge  as in regression techniques. 
acknowledgments 	we would like to thank vladimir 
vapnik  ralf herbrich and alex gammerman for their help with this work. we also thank the esprc for providing financial support through grant gr/l1. 

figure 1: the fraction of training patterns that are support vectors  top  and various error rates  bottom  both plotted against rbf kernel width for leave-one-out machines on the thyroid dataset. 
