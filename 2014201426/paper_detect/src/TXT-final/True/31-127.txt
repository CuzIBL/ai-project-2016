 
today's anti-virus technology  based largely on analysis of existing viruses by human experts  is just barely able to keep pace with the more than three new computer viruses that are written daily. in a few years  intelligent agents navigating through highly connected networks are likely to form an extremely fertile medium for a new breed of viruses. at ibm  we are developing novel  biologically inspired anti-virus techniques designed to thwart both today's and tomorrow's viruses. here we describe two of these: a neural network virus detector that learns to discriminate between infected and uninfected programs  and a computer immune system that identifies new viruses  analyzes them automatically  and uses the results of its analysis to detect and remove all copies of the virus that are present in the system. the neural-net technology has been incorporated into ibm's commercial anti-virus product; the computer immune system is in prototype. 
1 	introduction 
each day  an army of perhaps a few hundred virus writers around the world produces three or more new computer viruses.1 an army of comparable size  the anti-virus software developers  representing an approximately $1 million per year industry   works feverishly to analyze these viruses  develop cures for them  and frequently distribute software updates to users. 
　currently  the battle is roughly even. our statistics  based on observation of a sample population of several hundred thousand machines for several years  kephart and white  1; kephart et a/.  1   suggest that in medium to large businesses roughly 1% of all computers become infected during any given year. the world's computer population has been inconvenienced  but despite dire predictions  tippett  1  it has not been incapacitated. most of the anti-virus products in common usage have been reasonably effective in detecting and removing viruses. within our sample population  only 1% 
l
   this figure is based on the number of distinct new viruses that have been received by us during the last year. 
of all known viruses  about 1 of 1 at the time of writing  have been observed  in the wild  - in real incidents. several viruses that used to be relatively common now qualify for inclusion on an endangered species list. today  computer viruses are a manageable nuisance. 
　several worrisome trends threaten to turn the balance in the favor of computer virus authors. first  the rate at which new viruses are created  already on the verge of overwhelming human experts  has the potential to increase substantially. second  continued increases in interconnectivity and interoperability among the world's computers  designed to benefit computer users  are likely to be a boon to dos and macintosh viruses as well. theoretical epidemiological studies indicate that the rate at which computer viruses spread on a global scale can be very sensitive to the rate and the degree of promiscuity of software exchange  kephart and white  1; 1; kephart et al.} 1; kephart  1b . anticipated increases in both factors threaten to increase substantially the speed of spread and the pervasiveness of these traditional types of virus. in addition  mobile intelligent agents  chess et a/.  1; harrison et a/.  1  will soon navigate the global network  potentially serving as a fertile medium for a new breed of rapidly-spreading virus that exploits the itinerancy of its host by leaving behind copies of itself wherever its host goes. traditional methods of detecting and removing viruses  which rely upon expert analysis by humans and subsequent distribution of the cure to users  would be orders of magnitude too slow to deal with viruses that spread globally within days or hours. 
　to address these problems  we have developed a variety of biologically inspired anti-virus algorithms and techniques that replace many of the tasks traditionally performed by human virus experts  thus permitting much faster  automatic response to new viruses. 
　the term  computer virus   coined by adleman in the early 1's  cohen  1   is suggestive of btrong analogies between computer viruses and their biological namesakes. both attach themselves to a small functional unit  cell or program  of the host individual  organism or computer   and co-opt the resources of that unit for the purpose of creating more copies of the virus. by us-
kephart etal 
ing up materials  memory1  and energy  cpu1   viruses can cause a wide spectrum of malfunctions in their hosts. even worse  viruses can be toxic. in humans  diptheria is caused by a toxin produced by virally-infected bacteria  levine  1 . some computer viruses are similarly toxic  being deliberately programmed to cause severe harm to their hosts. one notorious example  the michelangelo virus  destroys data on a user's hard disk whenever it is booted on march 1th. 
　it is therefore natural to seek inspiration from defense mechanisms that biological organisms have evolved against diseases. the idea that biological analogies might be helpful in defending computers from computer viruses is not original to us  murray  1 . but to our knowledge we are the first to take these analogies seriously  to deliberately design and implement anti-virus technology that is inspired by biology  and incorporate it into a commercial anti-virus product. 
　first  we will briefly describe what computer viruses are  how they replicate themselves  and why their presence in a system is undesirable. then  we shall describe the typical procedures used by human experts to analyze computer viruses  and explain why these methods are unlikely to remain viable a few years from now. then  we shall discuss two complementary anti-virus techniques that are inspired by biological systems that learn: a neural-network virus detector and a computer immune system. 
1 	background 
1 	computer viruses and worms 
computer viruses are self-replicating software entities that attach themselves parasitically to existing programs. they are endemic to dos  macintosh  and other microcomputer systems. when a user executes an infected program  an executable file or boot sector   the viral portion of the code typically executes first. the virus looks for one or more victim programs to which it has write access  typically the same set of programs to which the user has access   and attaches a copy of itself  perhaps a deliberately modified copy  to each victim. under some circumstances  it may then execute a payload  such as printing a weird message  playing music  destroying data  etc. eventually  a typical virus returns control to the original program  which executes normally. unless the virus executes an obvious payload  the user is unlikely to notice that anything is amiss  and will be completely unaware of having helped a virus to replicate. viruses often enhance their ability to spread by establishing themselves as resident processes in memory  persisting long after the infected host finishes its execution  terminating only when the machine is shut down . as resident processes  they can monitor system activity 
1
　　the jerusalem virus increases the size of an executable by 1 bytes each time it infects it  eventually causing it to be too large to be loaded into memory  highland  1 . 
   1 the internet worm caused the loads on some unix machines to increase by two orders of magnitude  eichin  1; spafford  1 . 
continually  and identify and infect executables and boot sectors as they become available. 
　over a period of time  this scenario is repeated  and the infection may spread to several programs on the user's system. eventually  an infected program may be copied and transported to another system electronically or via diskette. if this program is executed on the new system  the cycle of infection will begin anew. in this manner  computer viruses spread from program to program  and  more slowly  from machine to machine. the most successful pc dos viruses spread worldwide on a time scale of months  kephart and white  1 . 
　worms are another form of self-replicating software that are sometimes distinguished from viruses. they are self-sufficient programs that remain active in memory in multi-tasking environments  and they replicate by spawning copies of themselves. since they can determine when to replicate  rather than relying on a human to execute an infected program   they have the potential to spread much faster than viruses. the internet worm of 1 is said to have spread to several thousand machines across the united states in less than 1 hours  eichin  
1; spafford  1   
1 	v i r u s detection  removal and analysis 
anti-virus software seeks to detect all viral infections on a given computer system and to restore each infected program to its original uninfected state  if possible. 
　there are a variety of complementary anti-virus techniques in common usage; taxonomies are given in  spafford  1; kephart et a/.  1 . activity monitors alert users to system activity that is commonly associated with viruses  but only rarely associated with the behavior of normal  legitimate programs. integrity management systems warn the user of suspicious changes that have been made to files. these two methods are quite general  and can be used to detect the presence of hitherto unknown viruses in the system. however  they are not often able to pinpoint the nature or even the location of the infecting agent  and they can sometimes flag or prevent legitimate activity  disrupting normal work or leading the user to ignore their warnings altogether. 
　virus scanners search files  boot records  memory  and other locations where executable code can be stored for characteristic byte patterns  called  signatures   that occur in one or more known viruses. providing much more specific detection than activity monitors and integrity management systems  scanners are essential for establishing the identity and location of a virus. armed with this very specific knowledge  disinfectors  which restore infected programs to their original uninfected state  can be brought into play. the drawback of scanning and repair mechanisms is that they can be applied only to known viruses  or variants of them. furthermore  each individual virus strain must be analyzed in order to extract both a signature and information that permits a disinfector to remove the virus. scanners and disinfectors require frequent updates as new viruses are discovered  and the analysis can entail a significant amount of effort on the part of human virus experts. 
whenever a new virus is discovered  it is quickly distributed among an informal  international group of antivirus experts. upon obtaining a sample  a human expert disassembles the virus and then analyzes the assembler code to determine the virus's behavior and the method that it uses to attach itself to host programs. then  the expert selects a  signature   a sequence of perhaps 1 to 1 bytes  that represents a sequence of instructions that is guaranteed to be found in each instance of the virus  and which  in the expert's estimation  is unlikely to be found in legitimate programs. this signature can then be encoded into the scanner. the attachment method and a description of the machine code of the virus can be encoded into a verifier  which verifies the identity of a virus that has been found by the scanner. finally  a reversal of the attachment method can be encoded into a disinfector. 
　virus analysis is tedious and time-consuming  sometimes taking several hours or days  and even the best experts have been known to select poor signatures - ones that cause the scanner to report false positives on legitimate programs. alleviation of this burden is by itself enough to warrant a serious attempt to automate virus analysis. the anticipated speed with which viruses of the future may spread is an even stronger argument in favor of endowing anti-virus software with the ability to deal with new viruses on its own.1 the rest of this paper describes two techniques for achieving this goal. 
1 	generic detection of viruses 
two methods of computer virus identification have already been introduced: the overly broad  ex post facto detection provided by activity monitors and integrity management systems  and the overly specific detection offered by virus scanners. somewhere in between is the ideal  generic detector : taking a program's code as input  it determines whether the program is viral or nonviral. perfect generic detection is an algorithmically  undecidable  problem: as observed by  cohen  1   it is reducible to the halting problem. however  imperfect generic detection that is good in practice is possible  and is naturally viewed as a problem in automatic pattern classification. standard classification techniques encompass linear methods and non-linear ones such as nearest-neighbor classification  decision trees  and multilayer neural networks. 
　within the problem of the generic detection of viruses  detection of  boot sector viruses  is both an important and relatively tractable sub-problem. a boot sector is a small sequence of code that tells the computer how to 
 pick itself up by its bootstraps . for ibm-compatible pc's  boot sectors are exactly 1 bytes long; their main function is to load and execute additional code stored elsewhere. 
1
　　at the very least  anti-virus software must handle a majority of viruses well enough to prevent them from spreading. for the foreseeable future  it will continue to be important for human virus experts to analyse carefully any viruses that appear in the wild to corroborate the results of the automated analysis and to determine any side effects that the virus may cause in infected systems. 
　although there are over 1 different file-infecting viruses and only about 1 boot-sector viruses  of the 1 viruses most commonly seen 1 are boot viruses  and account for over 1% of all virus incidents. boot viruses similarly dominate the rolls of newly observed viruses  so an ability to detect new boot sector viruses is significant in the war against viruses. 
　detecting boot viruses is a relatively limited pattern classification task. legitimate boot sectors all perform similar functions. viral boot sectors also all perform similar functions  before passing control to a legitimate boot sector loaded from elsewhere. 
　for this application  false positives are critical. false negatives mean missed viruses  and since viruses occur fairly rarely  so will false negatives. also  if a classifier does let a virus slip by  the outcome is no worse than if no virus protection were in place. on the other hand  false positives can occur any time  and will leave a user worse off than she would have been without virus protection. moreover  a false positive on one legitimate boot sector will mean false-positive events on thousands of computers. false positives are not tolerable. 
　nearest-neighbor classification might seem to be a simple  attractive approach to the classification of legitimate and viral boot sectors. natural measures of the difference between two boot sectors include the hamming distance between them  considered as 1-element vectors   or the edit distance  crochemore  1  between them  considered as text strings . to classify a new boot sector  the procedure would find the  nearest  of the 1 known boot sector viruses and 1 legitimate boot sectors  a representative if not comprehensive set that we have collected   and classify the new boot sector as viral if its nearest neighbor is viral  legitimate if its nearest neighbor is legitimate. 
　unfortunately  nearest-neighbor classification performs poorly for this problem. a viral boot sector can be 
just a short string of viral code written over a legitimate boot sector  so in any overall comparison  the virus will be more similar to the legitimate boot sector it happened to overwrite than to any other virus. this says that what makes viral boot sectors viral is not any overall quality but the presence of specific viral functions. 
　these functions can be used to construct a virus classifier. for example  one common action is for a virus to reduce the apparent size of available memory so that space taken up by the virus will not be noticed. although this action may be variously implemented in machine code  most machine code implementations match one of a few simple patterns.  a fictitious pattern typifying the form is c1b****ac1f**1d1 - about 1 fixed bytes and some wildcards.  of the viruses that lower memory in less conventional ways  most still contain a 1-byte pattern weakly indicative of the same function  but more prone to false positives. similar strong and weak patterns describe other common viral functions. 
　using expert knowledge of viral and non-viral boot sectors and several days of extensive experimentation  we hand-crafted an ad hoc classifier  see figure 1 . the classifier scans a boot sector for the presence of patterns that provide strong or weak evidence for any of four viral 
kephart  et al 
　
figure 1: a hand-crafted multi-level classifier network. eliminating the  max  boxes produces a more conventional neural network  but it is inferior  even when the seven weights are optimized. 
functions. one point is credited for weak evidence  and two points for strong evidence. a boot sector is classified as viral if its total score is 1 or higher. this classifier performed well on the 1 examples  with a false-negative rate of about 1% and a false-positive rate too small to measure over the 1 negative examples. that is  1% of viruses were detected  and no legitimate boot sector was classified as viral. 
　we hoped to develop a procedure for automaticallyconstructing a virus classifier  using similar features as inputs to a neural network. since the ad hoc classifier incorporated knowledge of all of the available boot sectors  there was a possibility that it suffered from overfitting  in which case it would generalize poorly on new data. it would be much easier to assess the generalization performance of an automatically constructed classifier. also  we hoped that algorithmic extraction of features and optimization of network weights might give even better classification performance  especially in the false-positive measure. finally  we believed that an automated procedure would adapt much more readily to new trends in boot sector viruses. if substantially new types of boot sector viruses became common  we could simply retrain the classifier - a much easier task than hacking on an ad hoc classifier  or re-writing it from scratch. 
　essentially  what we did was this. we extracted a set of 1-byte strings  or  trigrams   appearing frequently in viral boot sectors but infrequently in legitimate ones. the presence  1  or absence  1  of the strings defined the input vector to a single-layer neural network.  see figure 1.  its weights were  trained  over about half the examples  and the resulting network's performance tested on the other half. during the development of the automatic classifier  we encountered novel challenges in feature pruning and ill-defined learning that we think represent interesting general issues in learning. these will be introduced in the course of a more detailed description of the classifier's construction. 
1 	feature selection 	*. 
the first step in the construction was the selection of byte strings to act as features. where a human expert is able to use high-level understanding of viruses  knowledge of machine code  and natural intelligence to select complex feature patterns containing wildcards  for algorithmic feature generation we contented ourselves with simple 1-byte features. a training set with 1-byte viral boot sectors includes 1  trigrams   of which typically 1 are distinct. 
　this is where the first challenge  feature pruning  comes in. a well known principle in machine learning states that the number of training examples must be considerably larger than the number of adjustable parameters to reliably give good generalization to test examples  hertz et al  1 . with 1 viral and 1 non-viral training examples  a network must have well fewer than 1 weights - say about 1 - implying a lesser or equal number of inputs. somehow the 1 trigrams must be winnowed down to 1. 
　since what is desired are trigrams that are indicative of viral as opposed to legitimate behavior  it is natural to remove trigrams appearing too frequently in legitimate boot sectors. eliminating all trigrams which appear even once in the 1 legitimate training examples reduces the 1 candidate trigrams by only about 1%. on the reasoning that trigrams occurring frequently in 
pc programs in general are analogous to the english word  the  and not salient features  further winnowing can be done. eliminating trigrams with frequency over 1 1  occurring on average more than once in 1 bytes  again reduces the number about 1%  leaving about 1 of the original 1 candidate features. much more drastic pruning is required. 
　it is provided by selecting trigram features which figure importantly in the viral training set. one way to do this would be to select trigrams occurring at least some number of times in the viral training set  but this leaves some viral samples unrepresented by any trigrams. a better approach comes from selecting a  cover  of trigrams: a set of trigrams with at least one trigram representing each of the viral samples. in fact  we can afford something close to a 1-cover  so that each viral sample is represented by 1 different trigrams in the set.  a few 
　
samples have fewer than 1 representatives in the full set of 1 trigrams  in which case they are only required to be 1-covered  1-covered  or singly covered  as possible.  four-covering produces a set of about 1 trigram features  few enough to be used as input to a neural net. 
　 even so  a complete two-layer network with h hidden nodes would have h times as many weights as inputs  which here is prohibitive even for an h of 1 or 1; this is why we used a single-layer network.  
　reassuringly  most of the trigrams were substrings of or otherwise similar to the more complex patterns of the ad hoc classifier. however  there were a few trigrams that could not be related to any of these patterns  and on expert inspection they turned out to define a meaningful new feature class. 
1 	classifier training and performance 
by construction  the selected trigrams are very good features: within the training set  no legitimate boot sector contains any of them  and most of the viral boot sectors contain at least 1. paradoxically  the high quality of the features poses the second challenge  what we have called the problem of ill-defined learning. since no negative example contains any of the features  any  positive  use of the features gives a perfect classifier. 
　specifically  the neural network classifier of figure 1 with a threshold of 1 and any positive weights will give perfect classification on the training examples  but since even a single feature can trigger a positive  it may be susceptible to false positives on the test set and in realworld use. the same problem shows up as an instability when the usual back-propagation  rumelhart et al  1  training procedure is used to optimize the weights: larger weights are always better  because they drive the sigmoid function's outputs closer to the asymptotic ideal values of -1 and 1. 
　in fact all that will keep a feature's ideal weighting from being infinite is the feature's presence in some negative example. since none of the features were present in any negative example  our solution was to introduce new examples. one way is to add a set of examples defined by an identity matrix. that is  for each feature in turn  an artificial negative example is generated in which that feature's input value is 1 and all other inputs are 1. this adds one artificial example for each trigram feature; it might be better to emphasize features which are more likely to appear by chance. 
　to do so  we used 1 bytes of code taken from the initial  entry point  portions of many pc programs to stand in as artificial legitimate boot sectors; the thought was that these sections of code  like real boot sectors  might be oriented to machine setup rather than performance of applications. of 1 such artificial legitimate boot sectors  1 contained some viral feature.  this is about as expected. each selected trigram had generalcode frequency of under 1 1  implying that the chance of finding any of 1 trigrams among 1 bytes is at most 1%; the observed rate for the artificial boot sectors was 1%.  since not all of the 1 trigrams occurred in any artificial boot sector  we used this approach in combination with the  identity matrix  one. 
　at this point the problem is finally in the form of the most standard sort of  single-layer  feed-forward neural network training  which can be done by backpropagation. in typical training and testing runs  we find that the network has a false-negative rate of 1%  and a false-positive rate of 1% as measured on artificial boot sectors.1  given the trigrams' frequencies of under 1 1  if their occurrences were statistically independent  the probability of finding two within some 1 bytes would be at most 1%.  consistent with the 1% false-positive rate  there were no false positives on any of the 1 genuine legitimate boot sectors. 
　there was one eccentricity in the network's learning. even though all the features are indicative of viral behavior  most training runs produced one or two slightly negative weights. we are not completely sure why this is so  but the simplest explanation is that if two features were perfectly correlated  and some are imperfectly correlated   only their total weight is important  so one may randomly acquire a negative weight and the other a correspondingly larger positive weight. 
　for practical boot virus detection  the false-negative rate of 1% or less and false-positive rate of 1% are an excellent result: 1% of new boot sector viruses will be detected  with a tiny chance of false positives on legitimate boot sectors. in fact the classifier  incorporated into ibm antivirus  has caught several new viruses. there has also been at least one false positive  on a  security  boot sector with virus-like qualities  and not fitting the probabilistic model of typical code. rather than specifically allowing that boot sector  less than an hour of re-training convinced the neural network to classify it negatively; this may help to reduce similar false positives. 
　of the 1 or 1% of viruses that escape detection  most do so not because they fail to contain the feature trigrams  but because the code sections containing them are obscured in various ways. if the obscured code is captured by independent means  the trigrams can be passed on to the classifier and these viruses too will be detected. 
1 	a computer immune system 
although generic virus detection works well for bootsector viruses  and may eventually prove useful for file infectors as well  at least two drawbacks are inherent in the technique: 
1. new viruses can be detected only if they have a sufficient amount of code in common with known viruses. 
1. the method is appropriate for viral detection only; it is incapable of aiding in the removal of a virus from an infected boot sector or file. the only way 
　1 comparison of this classifier's 1% detection rate on test data with the 1% rate of the hand-crafted one is more favorable than the numbers suggest. the rate for the neural net was measured over an independent test set  where for the hand-crafted detector there was no training-testing division. measured over all examples  and especially if trained over all examples   the network's detection rate exceeds 1%. 
kephart.etal 
to eliminate the infection is to erase or replace the infected boot sector or file. 
the generic classifier could be viewed as an analog of the 
 innate   or non-adaptive  non-specific immune system that is present in both vertebrates and lower animals. one important component of this innate immunity can be viewed as a sort of generic classifier system  in which the features on which recognition is based include: 
1. the presence of certain proteins that are always present on self-cells  but usually not on foreign 
cells 1 
1. the presence of double-strand rna  which appears in much larger concentrations in a particular class of viruses than it does in mammalian cells  marrack  
1   and 
1. the presence of a peptide that begins with an unusual amino acid  formyl methionine  that is produced copiously by bacteria  but only in minute amounts by mammals  marrack  1 . 
this generic classification is coupled with a generic response to a pathogen that either disables it or kills it. 
　however  vertebrates have evolved a more sophisticated  adaptive immune system that works in concert with the innate immune system  and is based on recognition of specific pathogens.1 it exhibits the remarkable ability to detect and respond to previously unencountered pathogens  regardless of their degree of similarity to known pathogens. this is precisely the sort of defensive capability that we seek against computer viruses. 
　figure 1 provides an overview of our design for an adaptive computer immune system. the immune system responds to virus-like anomalies  as identified by various activity and integrity monitors  by capturing and analyzing viral samples. from its analysis  it derives the means for detecting and removing the virus. many components of the computer immune system are working in the laboratory  and are providing useful data that is incorporated into ibm antivirus  ibm's commercial anti-virus product. 
　the remainder of this section will be devoted to a discussion of the various components of the immune system design  along with their relationship to analogous biological principles. further exploration of some biological analogies can be found in  kephart  1a . first  we shall consider the set of components that are labeled as being currently in ibm antivirus: anomaly detection  scanning for known viruses  and removal of known viruses. then  we shall discuss some of the components 
　　1 these proteins inactivate complement  a class of proteins that bind to cells  and attract the attention of other components of the immune system  which kill the cell  janeway  1 . 1
　　this extra sophistication pits the quick adaptability of the immune system  which occurs within a single individual over the course of a few days  against the similarly quick evolutionary adaptability of pathogens  due to their short life-cycles . due to their much slower life-cycles  it is doubtful that vertebrates could hold their own if their immune systems had to rely on evolution alone. 
that are labeled as being currently in the virus lab: sample capture using decoys  algorithmic virus analysis  and signature extraction. these components are all functioning prototypes. finally  we shall discuss a mechanism by which one machine can inform its neighbors about viral infections. 
1 	a n o m a l y detection 
the fundamental problem faced by both biological and computer immune systems is to distinguish between malignant and benign entities that enter the individual. due to the high degree of stability of body chemistry in individual vertebrates during their lifetimes  their immune systems can replace this difficult task with the much simpler one of distinguishing self from non-self. this is a nice hack  because  self  is much easier to define and recognize than  benign . the biological immune system can simply implement the xenophobic strategy:  know thyself  and reject all else .  this strategy errs on the side of false positives  i.e. false rejection of benign entities   but except in cases of blood transfusions and organ transplants  these mistakes are of little consequence.1 
　in computers  the same xenophobic strategy is an important component of anomaly detection. integrity monitors can use checksums or other methods1 to determine whether an existing executable has changed. however  this is only a partial solution. the nature of  self   i.e. the collection of software on an individual computer  is continually shifting over time - much more so than in biological organisms. people continually add new software to their system  and update existing software by buying new versions or compiling new source code. the fact that an executable is new or has changed is not nearly enough to warrant suspicion. an array of other monitors and heuristics employ a complementary  know thine enemy  strategy: the nature of the anomaly must be strongly indicative of a virus. some components of the anomaly detector trigger on suspicious dynamical behaviors  such as one process writing to an executable or boot record  or unusual sequences of operating system calls  perhaps involving interception of particular interrupts ; others trigger on static properties having to do with the exact nature of a change that has been identified by the integrity monitor. 
1 	scanning for k n o w n viruses 
if the anomaly detector has been triggered  the system is scanned for all known viruses. since there are currently at least 1 known pc dos viruses  this means that exact or slightly inexact matches to approximately 1 signatures  each in the range of roughly 1 to 1 bytes long  are searched in parallel. this is in itself an interesting string matching problem  and efficient search methods are an active area of research for us. much 
   1 another important class of false positives are autoimmune reactions  which are sometimes induced by biochemical changes that occur at puberty  thus changing the nature of  self  . 1  a novel method for integrity monitoring that is based on a close analogy to t cells is described in  forrest et a/.  1 . 
　

kephart et al 
　
more impressive than any string matching algorithm we could ever hope to devise  however  is the parallel search carried out by the vertebrate immune system  in which roughly 1 million different types of t-cell receptors and 1 million different types of antibodies and b-cell receptors are continually patrolling the body in search of antigen  janeway  1 . just as a computer virus scanner recognizes viruses on the basis of  perhaps inexact  matches to a fragment of the virus  the signature   t-cell and b-cell receptors and antibodies recognize antigen by binding  strongly or weakly  depending on the exactness of the match  to fragments of the antigen  consisting of linear sequences of 1 to 1 amino acids  in the case of t cells  janeway  1  . 
　matching to fragments rather than the entire antigen is a physical necessity in the biological immune system; in computers  this strategy is not absolutely necessary  but it has some important advantages. matching to fragments is more efficient in time and memory  and permits the system to recognize slight variants  particularly when some mismatches are tolerated. these issues of efficiency and variant recognition are relevant for biology as well. 
　for both biological and computer immune systems  an ability to recognize variants is essential because viruses tend to mutate frequently. if an exact match were required  immunity to one variant of a virus would confer no protection against a slightly different variant. similarly  vaccines would not work  because they rely on the biological immune system's ability to synthesize antibodies to tamed or killed viruses that are similar in form to the more virulent one that the individual is being immunized against. 
1 	v i r u s removal 
in the biological immune system  if an antibody encounters antigen  they bind together  and the antigen is effectively neutralized. thus recognition and neutralization of the intruder occur simultaneously. alternatively  a killer t cell may encounter a cell that exhibits signs of being infected with a particular infecting agent  whereupon it kills the host cell. this is a perfectly sensible course of action  because an infected host cell is slated to die anyway  and its assassination by the killer t cell prevents the viral particles from reaching maturation. 
　a computer immune system can take the same basic approach to virus removal: it can erase or otherwise inactivate an infected program. however  an important difference between computer viruses and biological viruses raises the possibility of a much gentler alternative. 
　in biological organisms  most infected cells would not be worth the trouble of saving even if this were possible  because cells are an easily-replenished resource. ＜ 
　in contrast  each of the applications run by a typical computer user is unique in function and irreplaceable  unless backups have been kept  of course . since a user would be likely to notice any malfunction  all but the most ill-conceived computer viruses attach themselves to their host in such a way that they do not destroy its 
1
　　neurons are a notable exception  but they are protected from most infections by the blood-brain barrier  seiden  1 . 
function. viruses tend to merely rearrange or reversibly transform their hosts. thus an infected program is usually expressible as a reversible transformation of the uninfected original. 
　when the scanner identifies a particular program as being infected with a particular virus  the first step in our removal procedure is to verify that the virus is identical to a known strain. verification is based upon checksums of regions of viral code that are known to be invariant  perhaps after an appropriate decryption operation  across different instances of the virus. the exact location and structure of the virus must have been derived beforehand  and expressed in terms of a language understood by the verification algorithm. if the verification does not succeed  an attempt to remove the virus by this means is considered too risky  and another more generic virus removal method  beyond the scope of this paper  is brought into play. if the verification succeeds  a repair algorithm carries out the appropriate sequence of steps required for removing that virus  expressed in a simple repair language. the sequence of steps is easily derived from an analysis of the locations  and transformations  if any  of all of the portions of the original host. 
　although the analysis required to extract verification and removal information has traditionally been performed by human experts  we shall discuss in a later subsection an automated technique for obtaining this information. 
1 	decoys 
suppose that the anomaly detector has found evidence of a virus  but that the scanner cannot identify it as any of the known strains. most current anti-virus software will not be able to recover the host program unless it was deliberately stored or analyzed1 prior to becoming infected. ideally  one would like to have stronger evidence that the system really is infected  and to know more about the nature of the virus  so that all instances of it  not just the one discovered by the anomaly detector  can be found and eliminated from the system. 
　in the computer immune system  the presence of a previously unknown virus in the system can be established with much greater certainty than can be provided by the anomaly detector. the idea is to lure the virus into infecting one or more members of a diverse suite of  decoy  programs. decoys are designed to be as attractive as possible to those types of viruses that spread most successfully. a good strategy for a virus to follow is to infect programs that are touched by the operating system in some way. such programs are most likely to be executed by the user  and thus serve as the most successful vehicle for further spread. therefore  the immune system entices a putative virus to infect the decoy programs by executing  reading  writing to  copying  or otherwise manipulating them. such activity attracts the attention of many viruses that remain active in memory even after they have returned control to their host. to 
1
　　 generic disinfection methods can store a small amount of information about an uninfected program  and use this information to help reconstruct it if it subsequently becomes infected. 
　
catch viruses that do not remain active in memory  the decoys are placed in places where the most commonly used programs in the system are typically located  such as the root directory  the current directory  and other directories in the path. the next time the infected file is run  it is likely to select one of the decoys as its victim. from time to time  each of the decoy programs is examined to see if it has been modified. if any have been modified  it is almost certain that an unknown virus is loose in the system  and each of the modified decoys contains a sample of that virus. these virus samples are stored in such a way that they will not be executed accidentally. now they are ready to be analyzed by other components of the immune system. 
　the capture of a virus sample by the decoy programs is somewhat analogous to the ingestion of antigen by macrophages  paul  1 . macrophages and other types of cells break antigen into small peptide fragments and present them on their surfaces  where they are subsequently bound by t cells with matching receptors. a variety of further events can ensue from this act of binding  which in one way or another play essential roles in recognizing and removing the pathogen. capture of an intruder by computer decoys or biological macrophages allows it to be processed into a standard format that can be interpreted by other components of the immune system  provides a standard location where those components can obtain information about the intruder  and primes other parts of the immune system for action. 
1 	a u t o m a t i c virus analysis 
typically  a human expert applies a deep understanding of machine instruction sequences to virus analysis. sometimes  this is combined with observation of the effects of the virus on a program. 
　our automatic virus analysis algorithm is much less sophisticated in its knowledge of machine code  but makes up for this deficiency by making use of more data: specifically  several samples of the virus. once a few samples of the virus have been captured  the algorithm compares the infected decoys with one another and with the uninfected decoys to yield a precise description of how the virus attaches to any host. the description is completely independent of the length and contents of the host  and to some extent can accommodate self-encrypting viruses. a pictorial representation of one particularly simple infection pattern is presented in fig. 1. 
　automatic virus analysis provides several useful types of information: 
1. the location of all of the pieces of the original host within an infected file  independent of the content and length of the original host. this information is automatically converted into the repair language used by the virus removal component of ibm antivirus. 
1. the location and structure of all components of the virus. structural information includes the contents of all regions of the virus that are invariant across different samples. this information has two purposes: 
1 	automatic signature extraction 
the basic goal of automatic signature extraction is to choose a signature that is very likely to be found in all instances of the virus  and very unlikely to be found accidentally in uninfected programs. in other words  we wish to minimize false negatives and false positives. false negatives are dangerous because they leave the user vulnerable to attack. false positives are extremely annoying to customers  and so infuriating to vendors of falsely-accused software that they have led to at least one lawsuit. 
　to minimize false negatives  we first start with the contents of the invariant regions that have been identified by the automatic virus analysis procedure. however  it is quite conceivable that not all of the potential variation has been captured within the samples. as a general rule  non-executable  data  portions of programs  which can include representations of numerical constants  character strings  work areas for computations  etc.  are inherently more likely to vary from one instance of the virus to another than are  code  portions  which represent machine instructions. the origin of the variation may be internal to the virus  or a virus hacker might deliberately change a few data bytes in an effort to elude virus scanners. to be conservative   data  areas are excluded from further consideration as possible signatures. although the task of separating code from data is in principle somewhat ill-defined  there are a variety of methods  such as running the virus through a debugger or virtual interpreter  which perform reasonably well. 
　at this point  there are one or more sequences of invariant machine code bytes from which viral signatures could be selected. we take the set of candidate signatures to be all possible contiguous blocks of 1 bytes found in these byte sequences  where s is a signature length that is predetermined or determined by the algorithm itself.  typically  s ranges between approximately 1 and 1.  the remaining goal is to select from among the 
kephart etal 
　
candidates one or perhaps a few signatures that are least likely to lead to false positives. 
　we have formulated the problem of minimizing the false positive probability as follows. for each candidate signature  estimate the probability for it to match a random sequence of length s that is generated by the same probability distribution that generates legitimate software on the relevant platform.  of course  machine code is written by people or compilers  not probability distributions  so such a probability distribution is a theoretical and somewhat ill-defined construct  but we estimate its statistics from a set of over 1 dos and os/1 programs  constituting half a gigabyte of code.  then  we select the candidate signature for which the estimated probability is the smallest. 
　in slightly more detail  the key steps of the algorithm are as follows: 
1. form a list of all n-grams  sequences of n bytes; 1   n   n max  contained in the input data.  n max is typically 1 or 1.  
1. calculate the frequency of each such n-gram in the  self  collection. 
1. use a simple formula that chains together conditional probabilities based on the measured n-gram frequencies to form a  false-positive  probability estimate for each candidate signature  i.e. the probability that it matches a random s-byte sequence chosen from code that is statistically similar to  self. 
1. select the signature with the lowest estimated falsepositive probability. 
characterizations of this method  kephart and arnold  
1  show that the probability estimates are poor on an absolute scale  due to the fact that code tends to be correlated on a longer scale than 1 or 1 bytes. however  the relative ordering of candidate signatures is rather good  so the method generally selects one of the best possible signatures. in fact  judging from the relatively low false-positive rate of the ibm antivirus signatures  compared with that of other anti-virus vendors   the algorithm's ability to select good signatures may be better than that achieved by human experts. 
　in a sense  the signature extraction algorithm combines elements of outmoded and current theories of how the vertebrate immune system develops antibodies and immune-cell receptors to newly encountered antigen. the template theory  which held sway from the mid1 until the early 1's  was that antibodies and receptors molded themselves around the antigen. the clonal selection theory holds that a vast  random reper-
toire of antibodies and receptors is generated  and those that recognize self are eliminated during the maturation phase. of the remaining antibodies and receptors  at least a few will match any foreign antigen that is encountered. the clonal selection theory gained favor in the 1's  and is currently accepted  paul  1 . 
　our automatic signature extraction method starts out looking like the template theory. instead of generating a large random collection of signatures that might turn out to be useful someday  we take the collection of code 
1 	invited speakers 
for a particular virus as our starting point in choosing a signature. however  we do share one important element with the clonal selection theory: elimination of self-recognizing signatures. in fact  the automatic signature extraction method is even more zealous in this endeavor than clonal selection  in that it only retains the  best  signature. 
1 	immunological m e m o r y 
the mechanisms by which the vertebrate immune system retains a lifelong memory of viruses to which it has been exposed are quite complex  and are still the subject of study and debate. 
　by contrast  immunological memory is absolutely trivial to implement in computers. during its first encounter with a new virus  a computer system may be  ill   i.e. it will devote a fair amount of time and energy  or cpu cycles  to virus analysis. after the analysis is complete  the extracted signature and verification/repair information can be added to the appropriate known-virus databases. during any subsequent encounter  detection and elimination of the virus will occur very quickly. in such a case the computer can be thought of as  immune  to the virus. 
1 	fighting self-replication w i t h self-replication 
in the biological immune system  immune cells with receptors that happen to match a given antigen reasonably well are stimulated to reproduce themselves. this provides a very strong selective pressure for good recognizers  and by bringing a degree of mutation into play  the immune cell is generally able to come up with immune cells that are extremely well-matched to the antigen in question. 
　one can view this as a case in which self-replication is being used to fight a self-replicator  the virus  in a very effective manner. one can cite a number of other examples in nature and medical history where this strategy has been employed  such as the deliberate use of the myxoma virus in the 1's to curtail an exploding rabbit population in australia  mcneill  1; levine  1 . 
　the self-replicator need not itself be a virus. in the case of the worldwide campaign against smallpox  launched by the world health organization in 1  those who were in close contact with an infected individual were all immunized against the disease. thus immunization spread as a sort of anti-disease among smallpox victims. this strategy was amazingly successful: the last naturally occurring case of smallpox occurred in somalia in 1  bailey  1 . 
　we propose to use a similar mechanism  which we call the  kill signal   to quell viral spread in computer networks. when a computer discovers that it is infected  it can send a signal to neighboring machines. the signal conveys to the recipient the fact that the transmitter was infected  plus any signature or repair information that might be of use in detecting and eradicating the virus. if the recipient finds that it is infected  it sends the signal to its neighbors  and so on. if the recipient is not infected  it does not pass along the signal  but at least it has received the database updates  effectively immunizing it against that virus. 
　theoretical modeling has shown the kill signal to be extremely effective  particularly in topologies that are highly localized or sparsely connected  kephart and white  1; kephart  1b . 
1 	conclusion and perspective 
the development of the generic virus detector and the computer immune system were primarily motivated by practical concerns: human virus experts are on the verge of being overwhelmed  and we need to automate as much of what they do as possible. 
　the generic virus detector was incorporated into ibm antivirus in may  1  and since that time it has successfully identified several new boot viruses. it is the subject of a pending patent. most of the components of the computer immune system are functioning as very useful prototypes in our virus isolation laboratory; we use them every day to process the large sets of new viruses that arrive in the mail from other virus experts around the world. the immune system itself is the sub-
ject of a pending patent  as are several of its components  including automatic virus analysis and automatic signature extraction. 
　our eventual goal is to incorporate the immune system into ibm antivirus and  a few years from now  in networks inhabited by itinerant software agents. more implementation and more invention  guided in part by the biological metaphor  lie ahead. 
　although our primary motivation for developing a computer immune system is practical  it is interesting to adopt a more philosophical perspective. 
　consider the history of how humans have handled disease. for millions of years  our sole defense against infectious disease was our immune system  and it has done a good job of defending us from most infectious diseases. when we are suffering from the common cold  we may experience a few days of discomfort while the immune system figures out how to recognize and eradicate the virus  but we usually survive the attack. however  a minority of diseases  like smallpox or aids  are not handled effectively by the immune system. fortunately  during the last few centuries  we have made tremendous advances in our understanding of infectious diseases at both the macroscopic and microscopic levels  and medical practices based on this understanding now augment the capabilities of our natural immune system. 
　a few hundred years ago  disease began to be understood at the macroscopic level. in 1  daniel bernoulli  the founder of mathematical physics  was interested in determining whether a particular form of inoculation against smallpox would be generally beneficial or harmful to society. formulating and solving a mathematical model  he found that inoculation could be expected to increase the average life expectancy by three years. his work founded the field of mathematical epidemiology  bailey  1 . observational epidemiology received a major boost from john snow  who in 1 was able to deduce the origin of a severe cholera outbreak in london by plotting the addresses of victims on a city map  bailey  1 . 
　the macroscopic approaches of snow and bernoulli proved fruitful even before bacteria and viruses were identified as the underlying cause of infectious disease in the late 1th century. during the 1th century  research at the microscopic level has supplemented epidemiology. electron microscopy and x-ray crystallography brought the structure of viruses into view in the 1's  and the fascinating complexities of their life cycle and biochemistry began to be studied intensively in the mid-1's. these advances established terra firm a on which mathematical epidemiologists could build their models. 
　today  epidemiologists  in the detective role pioneered by john snow  discover new viruses  garrett   1 . biochemists  molecular biologists  and geneticists work to elucidate the secrets of viruses  and to create safe and effective vaccines for them. epidemiologists use intuition and mathematics to develop plans for immunizing populations with these vaccines. the eradication of smallpox from the planet in 1 is probably the greatest triumph of this multi-disciplinary collaboration. 
　interestingly  the history of man's defense against computer viruses is almost exactly reversed. computer viruses were first understood at the microscopic level  thanks to the pioneering work of fred cohen in the early 1's  cohen  1 . as soon as the first dos viruses began to appear in 1  highland  1   they were dissected in great detail  and the first primitive anti-virus software was written. it was not until 1 that the first real attempts were made to understand the spread of computer viruses from a macroscopic perspective  kephart and white  1; 1; tippett  1; 1 . finally  in the mid-1's  we are proposing to give computers what humans and other vertebrates have always relied upon as a first line of defense against disease: an immune system. 
　the center for disease control does not get worked up when a new strain of the common cold sweeps through a population. instead  they concentrate their limited resources on finding cures for horrible diseases such as aids. currently  the world community of anti-virus researchers  the computer equivalent of the cdc  squanders lots of time analyzing the computer equivalents of the common cold. our hope is that a computer immune system will deal with most of the standard  run-of-themill viruses quietly and effectively  leaving just a small percentage of especially problematic viruses for human experts to analyze. 
