
effective representation of web search results remains an open problem in the information retrieval community. for ambiguous queries  a traditional approach is to organize search results into groups  clusters   one for each meaning of the query. these groups are usually constructed according to the topical similarity of the retrieved documents  but it is possible for documents to be totally dissimilar and still correspond to the same meaning of the query. to overcome this problem  we exploit the thematic locality of the web- relevant web pages are often located close to each other in the web graph of hyperlinks. we estimate the level of relevancebetween each pair of retrieved pages by the length of a path between them. the path is constructed using multi-agent beam search: each agent starts with one web page and attempts to meet as many other agents as possible with some boundedresources. we test the system on two types of queries: ambiguous english words and people names. the web appears to be tightly connected; about 1% of the agents meet with each other after only three iterations of exhaustive breadth-first search. however  when heuristics are applied  the search becomes more focused and the obtained results are substantially more accurate. combined with a content-driven web page clustering technique  our heuristic search system significantly improves the clustering results.
1 introduction
clustering of web search results has been in the focus of the information retrieval community since the early days of the web  hearst and pedersen  1; zamir and etzioni  1 . the reasons for clustering of search results are two-fold. the first is that the ir research community has long recognized the validity of the cluster hypothesis  van rijsbergen  1  in top-ranked documents  i.e. similar documents tend to be relevant to the same requests. a second  related  reason is that the ranked list is usually too large and contains many documents that are irrelevant to the particular meaning of the query the user had in mind. thus  it would be beneficial to group search results by various meanings of the query. recently  successful academic  zeng et al.  1  and industrial  vivisimo.com  attempts have made the clustering of search results plausible for many www users. however  it is still not accurate enough to attract an average user. the main drawback of many web page clustering methods is that they take into account only the topical similarity between documents in the ranked list.
모topical similarity metrics between web pages would not help solving the clustering problem in at least two cases:  a  when there is not enough contextual information on a page: for example  within 1 first hits on a query jaguar one can find a web site savethejaguar.com  which presents a large picture of the wild cat on the background  but does not contain enough topical words to automatically associate the page to the correct group;  b  when web sites are contextually different but actually refer to the same meaning of the query. for instance  given a query michel d뫣ecary  one can retrieve web pages of at least three individuals: a computer scientist  www.zoominfo.com/micheldecary   a lawyer  www.stikeman.com/cgi-bin/profile. cfm pid=1   and a chansonnier www.decary.com . all three studied at the university of montre뫣al and at mcgill university in canada. are they three different people or actually one person 
모these problems can be resolved by exploiting the thematic locality of the web graph  the directed graph in which nodes are web pages and edges are hyperlinks . hypothetically  if page a hyperlinks page b  then the creator of page a intentionally raised the topic of page b in the context of page a which indicates that pages a and b are semantically close. davison  empirically justifies this hypothesis. in an average case  if two static web pages are located in a short proximity to each other in the web graph  then they stand in a  probably vague  semantic relation. for the two examples presented above  the site savethejaguar.com hyperlinks wcs.org  the wildlife conservation society  which reveals its topic; while the pages of michel d뫣ecary the scientist and the chansonnier point to cogilex.com  michel's previous enterprise  which implies that two of the three individuals are in fact the same person. note that no language modeling method would resolve this dilemma because the two pages are strictly different  they are even written in different languages .
모link analysis has been successfully applied to various web mining tasks. a related task is identification of web communities  see  e.g.  gibson et al. 1   which are defined as heavily connected components of the web graph. research methodsfor this task are primarilygraph-theoretic graphpartitioning  network flow etc. . unfortunately  these methods are inapplicable to our task  because we cluster isolated  unconnected web pages retrieved by an arbitrary query. the most relevant previous work is he et al.   who build a web page clustering system that exploits the hyperlink structure of the web: they consider two web pages to be similar if they are in parent/child or sibling relations in the web graph. we propose a more general framework that incorporates both topical and topological closeness: web pages belong to the same cluster if they are similar in content or close to each other in the web graph.
모to approximate the distance between two pages in the web graph  we apply the heuristic search paradigm  pearl  1 . to our knowledge  this paper is the first work that applies heuristic search  specifically  beam search  in the domain of the web graph. heuristics have been used for focused web crawling  e.g.  davidov and markovitch  1    where the goal is to collect as much useful information as possible while crawling the web  and the heuristics estimate the amount of information available in a particular web sub-graph. in contrast  we use heuristics to estimate the utility of expanding the current node in terms of leading to the target node.
모since the heuristic search can be computationally hard  we perform bidirectional search: we start searching from both source and target nodes and expand hyperlinked nodes until the two search frontiers meet at a common node or until the resources are depleted. in this setting  the computational complexity is no longer an issue: after only three search iterations  we can construct paths of length up to 1 which are long enough to potentially diminish any semantic relation between the starting nodes. thus  since short searches are acceptable in our case and since the out-degree of web pages is on average just about 1  kleinberg et al.  1   even exhaustive breadth-first search methods are feasible. moreover  modern search engines store the adjacency table of most of the web  i.e. no web crawling is required for the heuristic search. we use heuristics not to reduce the search time  but to improve the search accuracy. as we discuss below  the modern web is tightly interconnected  so heuristics are used as filters to prune branches of search trees that are likely to establish undesired connections between unrelated web pages.
모to distribute the heuristic search  we build a multi-agent system: given n web pages in the ranked list  we construct n collaborative web agents each of which is assigned one page of the initial dataset. each agent then performs heuristic search to traverse the web graph in order to meet as many other agents as possible. if an agent reached a dead end and cannot continue the search  it can move up in the hierarchy of web directories which would presumably lead to a more general page that has more hyperlinks. pages whose agents managed to meet within the given budget on resources are then placed in the same cluster. by this  we construct a set c of topological clusters  we consider only k largest clusters of those constructed . in parallel to that  we apply a traditional topical clustering method that assigns each document from the original ranked list into one cluster from a set c of topical clusters. after that  for each cluster ci 뫍 c we find its closest cluster from = argmax.
each cluster ci is then enriched with elements of that do not appear in any cluster of c. by this  we construct larger clusters ci that contain documents that are either topologically or topically related. this technique shows excellent results both in terms of precision and recall  see section 1 .
모besides the clustering of web search results  the proposed system can be applied to various information retrieval and web mining tasks  such as web appearance disambiguation  bekkerman and mccallum  1   acronym disambiguation  pakhomovet al.  1   interactive information retrieval  leuski and allan  1   web search with web pages provided as queries  dean and henzinger  1   as well as for homeland security analysis and other important problems.
모in this study  we test our system on two applications: search result clustering and web appearance disambiguation. in the latter  the goal is to identify web appearances of particular people with potentially ambiguous names. the problem is solved given a few names of people who are known to belong to the same social network. disambiguation of each person's name is allowed by the presence of other names that are likely to correlate with it. we represent web appearance disambiguation as a special case of the search result clustering task: given m queries of people names  we construct one cluster of web pages that mention the people of our interest  while disregarding pages that mention their unrelated namesakes. we generalize our multi-agent heuristic search by constructing m 뫄 n agents that search for each other in the web.
1 multi-agent heuristic search
we propose two multi-agent heuristic search algorithms for topologically clustering n pages  we call them source pages . algorithm 1 is called sequential heuristic search  shs . we start with n singleton clusters of the source pages. we create a collection of n web agents each of which is assigned one source page. each agent maintains a search frontier: a list of nodes  urls  to be expanded  initially  the url of its source page . at any search iteration each agent obtains urls hyperlinked from the nodes of its search frontier. it then applies heuristics to select potentially good urls to become its new search frontier. after that  we intersect the sets of urls obtained by all agents. if a common url is found for two source pages  we merge the clusters they belong to. the system stops after a predefined number of iterations  usually  a small number of 1 or 1  as discussed in section 1 .
모the shs algorithm  while being simple and intuitive  suffers from one crucial drawback: there is no possibility to control the topology of the constructed clusters. in a worst case  after l search iterations  if a path is found between page a and b  as well as between pages b and c  and between pages c and d  while no other links are found   then pages a and d
input:
s = {s1 s1 ... sn} - urls of source pages
 l - number of search iterations output:
clusters c1 ... ck
for each si 뫍 s do
initialize agent ai's search frontier f1 ai  뫹{si}
initialize agent ai's set of extracted urls t1 ai  뫹{si}
for each j = 1 ... l do
---distributed search phase:--- for each si 뫍 s do
construct fj+1 ai  뫹 extracturls fj ai  
filter fj+1 ai  using a set of heuristics
update tj+1 ai  뫹 tj ai  뫋 fj+1 ai 
---result collection phase:---
construct all pairs
initialize singleton clusters ci 뫹{si}
for each 
ifthen
mergealgorithm 1: sequential heuristic search  shs .
will be placed in the same cluster despite that the semantic relation between them is probably weak  as their distance in the web can be 1l 1 which is too long even if l = 1. a method for building tightly connected clusters should be proposed.
모solving the web appearance disambiguation problem  bekkerman and mccallum  noticed that matching hyperlinks of the source pages leads to a small but clean cluster of relevant pages  called the core cluster . we adopt this idea and propose another multi-agent heuristic search algorithm  called incremental heuristic search  ihs -see algorithm 1. in ihs  we start with a set of core clusters generated at iteration 1 of shs. 1 the distributed search phase of ihs is exactly the same as of shs  but at the result-collection phase we now select only pairs where one member belongs to a core cluster while the other does not  so we add it to the corresponding core cluster. we ignore pairs in which both members belong to different core clusters. proceeding incrementally  we keep track of the diameter of each constructed cluster  which is now independent of the cluster's size.
1 useful heuristics
two types of heuristics can be proposed in the web domain: topology-driven and content-driven. topology-driven heuristics are based on the layout of the web graph  while contentdriven heuristics are based on features extracted from the interior of web pages. in this section we propose one topologydriven and two content-driven heuristics  all of which are fairly straightforward  but still prove to be effective when used in our framework of heuristic search in the web graph. in our future work  we will explore other heuristics as well.
input:
s = {s1 s1 ... sn} - urls of source pages
 cc1 ... cck - core clusters obtained at iteration 1 of shs l - number of search iterations output:
enlarged core clusters cc1 ... cck
for each si 뫍 s do
initialize agent ai's frontier f1 ai  뫹 extracturls si 
initialize ai's set of extracted urls t1 ai  뫹{si}뫋 f1 ai 
for each j = 1 ... l do
---distributed search phase:--- for each si 뫍 s do
construct fj+1 ai  뫹 extracturls fj ai  
filter fj+1 ai  using a set of heuristics
update tj+1 ai  뫹 tj ai  뫋 fj+1 ai 
---result collection phase:---
for each pair  do
addalgorithm 1: incremental heuristic search  ihs .
모our topology-drivenheuristic is high-degree node elimination  or  in short  high-degree heuristic : after each search iteration  from the search frontiers we remove high out-degree and high in-degree urls that often connect between semantically unrelated pages. for example  both macromedia. com and historians.orgpoint to google.com  which does not imply that there is a tight semantic relation between macromedia inc. and the american historical association. for the graphical interpretation of the high-degree heuristic  see figure 1 a . to detect high out-degree urls  we simply count the number of hyperlinks at each page. to detect high in-degree urls  we use google's link: operator.
모a successful content-driven heuristic is the person name heuristic. figure 1 b  illustrates the idea. an agent has a good chance to meet another agent  if it expands a page that shares a person name with a page expanded by another agent. to extract person names from expanded web pages  we first remove markup  and then apply ner  wei li's named entity tagger  see mccallum and li  1 . we extract only entities tagged as person and consider people names that consist of two  three or four words. we exclude people names that are too common  again  we use google's link: operator .1
모in analogy to the person name heuristic  we also propose the anchor text heuristic that matches snippets of anchor text extracted from the web pages. we ignore too common anchor texts  such as contact us or copyright. eiron and mccurley  perform a comprehensive analysis of anchor texts and show that they usually summarize the content of the hyperlinked web pages. such summarization can be very useful in our case  when we attempt to predict a possible benefit of expanding pages from the search frontier. note that in contrast to people names  anchor snippets can be easily identified by shallow parsing of the pages' markup language.

figure 1: an illustration of applying heuristics  at the first search iteration. black nodes are the source pages.  left  high-degreeheuristic. gray nodes are high in- or out-degree pages  eliminated fromsearch frontiers.  right person name heuristic. a hyperlink path between two nodes is constructed over which a person name is also shared.
1 datasets
we use two datasets for evaluation of our methods: one for web appearance disambiguation and another for clustering web search results.
1 web appearance disambiguation dataset
we downloaded bekkerman's web appearance disambiguation dataset from www.cs.umass.edu/몲ronb. it consists of 1 web pages retrieved on 1 names of people from melinda gervasio's social network  mostly  sri engineers and university professors . the dataset is labeled according to the person's occupation. two of the 1 people appear to be unique in the web  while the rest have relatively common names. some of the names are extremely ambiguous  e.g. given a query ''tom mitchell''  1 different tom mitchells are found within the first 1 google hits. the dataset contains pages of 1 unique people overall  while only 1 of them are relevant  mentioned at 1 pages . for the statistics on the dataset as well as for the preprocessing procedure  see bekkerman and mccallum .
모we crawled the web starting with these 1 pages  source pages . we retrieved all available pages hyperlinked from the source pages  as well as the pages located one level above the source pages in the hierarchy of web directories. we continued this process until all the pages within three hops of the original dataset were retrieved. in order not to produce a priori weak connections and to still preserve a reasonable size for our dataset  we did not retrieve pages located at extremely popular domains  such as amazon.com. we also ignored pages of non-textual format. at each crawling iteration our dataset grew almost an order of magnitude: we downloaded 1 pages at the first hop  1 pages at the second hop and 1 pages at the third hop  resulting in 1 unique web pages overall.
1 jaguar dataset
we built a new dataset for the problem of clustering web search results. we retrieved and labeled 1 first google hits obtained on the query jaguar. we found 1 different categories within the 1 retrieved pages: the largest ones are obviously the car  the wild cat and the mac operating system  version 1 . table 1 presents statistics on this dataset.
모exactly as for the web appearance disambiguation dataset  we crawled three hops off the jaguar source pages  retrieving 1 pages on the first hop  1 pages on the second hop and
category	# of pages	category	# of pages
car1cornell project1mac os1metal band1wild cat1movie1biotech firm1photo gallery1youth org1atari game1maya culture1guitar1resin models1tv channel1web hosting1web designer1reef lodge1e-commerce firm1book1game archieve1singer1aircraft1emulator1table 1: statistics on the jaguar dataset.
1 pages on the third hop. at each iteration the dataset grew on average by a factor of 1  which corresponds surprisingly well to the growth of the web appearance disambiguation dataset and to the findings of kleinberg et al. .
1 results and discussion
1 web appearance disambiguation
first  we apply both sequential and incremental search algorithms on the web appearance disambiguation data in an exhaustive manner  i.e. without applying heuristics. surprisingly  we discover that the dataset is heavily interconnected. after each iteration of the sequential search  the connected pages compose one large cluster of size 1  1  1  and 1  1% of the entire dataset  respectively. some connections are extremely weak: 1% of 1 hyperlink paths found at the last iteration go through www.adobe.com/ products/acrobat/readstep1.html  a page with over 1 google hits on it.
모on this data  we report on precision  recall and f-measure of constructing one cluster of documents that mention relevant people. precision/recall curves in figure 1 show that the exhaustivesequential and incremental algorithms do quite poorly on this data  with slight advantage to the incremental approach. after four iterations  we end up with above 1% recall  but the precision is very low  under 1% . however  the performance is improved when we apply the high-degree heuristic. we set the threshold of in/out hyperlinks at 1- all pages with more than 1 google hits and pages containing more than 1 hyperlinks are filtered out. we also tried other thresholds  such as 1 and 1  without any significant change in the performance. note that only short paths are effective: the precision drops at the second and third hops of the source pages. the reason for such a drop is that the highdegree heuristic is topology-driven-it ignores the content of the pages  which introduces a lot of noise while moving far away from the source pages.
모the person name heuristic turned out to be more effective. we notice that since we perform short searches  up to three hops from the source pages   there is no need in narrowing the search beam with the heuristic. moreover  such narrowing may hurt the recall of our system. instead  we apply the heuristic as a filtering method: at each iteration

figure 1: precision/recall curves for four algorithms on the web appearance disambiguation dataset. h/d means highdegree heuristic  names means person name heuristic. four nodes in each curve correspond to search iterations 1  1  1 and 1. at iteration 1  only original nodes expanded  the core cluster is built  for the ihs algorithm .
j  we first use an incremental exhaustive search in order to find pages that are linked with the core cluster  and then we apply our information extraction module that extracts people names from pages expanded during the search. for each source page si we build two sets: tj si  of all urls found during the search and nj si  of all people names extracted from the search tree. for the core cluster cc we construct  and.
we put page si into cc if there is a hyperlink path from si to cc and a common person name is found:  tj si  뫌
. note that the only
difference from algorithm 1 is that the common person name may not be on the constructed hyperlink path between si and cc. this method shows good results on our data  see figure 1 . the best f-measure  1%  is achieved at the second iteration  while at the third one the precision drops by almost 1%  which implies that two iterations are enough. we also tried to apply the high-degree and the person name heuristics together  but did not see any improvement in precision  while hurting recall.
모to compare our results with the ones reported by bekkerman and mccallum   we use their topical clustering method called agglomerative/conglomerative distributional clustering  a/cdc   which is a state-of-the-art informationtheoretic technique. the results are shown in table 1  a/cdc result is by bekkerman and mccallum  1 . we see that after the first iteration the heuristic search method is competitive with a/cdc in precision  but is inferior in recall. however  when combining the two methods  we obtain excellent results in terms of both precision and recall. after the second search iteration the precision trades off against the recall  more noise is added  and the f-measure slightly decreases.
모heuristic search allows addition of 1 previously undiscovered documents to the topical cluster  1 of which refer to adam cheyer and steve hardt. bekkerman and mccallum  notice that these two researchers work in industry so their pages use different vocabulary than most of other academic-stylepages in the cluster. our heuristic search method is especially designed to overcome this problem.
	method	precision	recall	f-measure
web appearance disambiguation
topical  a/cdc 1%1%1%ihs  iteration 1 1%1%1%hybrid  iteration 1 1%1%1%ihs  iteration 1 1%1%1%hybrid  iteration 1 1%1%1%
clustering of web search results
topical  a/cdc 1%1%1%ihs  iteration 1 1%1%1%hybrid  iteration 1 1%1%1%ihs  iteration 1 1%1%1%hybrid  iteration 1 1%1%1%table 1: results of topical clustering  a/cdc   topological clustering  ihs  and their hybrid  on two datasets. the ihs clustering  and the hybrid  results are obtained after the first and second iterations of heuristic search  hyperlink paths of length up to 1 and up to 1 respectively .
1 clustering of web search results
in contrast to web appearance disambiguation  the problem of clustering web search results is not a one-class problem. we evaluate our system on k largest classes of the data. for our jaguar dataset we chose k = 1  so we build three clusters  of cars  mac os  and wild cats . let cci be one of these clusters and cli be its corresponding class. let corri be a set of pages from cli that have been correctly assigned into cci by our system. then the micro-averaged precision and recall of the system are:
.
모on the jaguar dataset  the sequential exhaustive search fails: after three iterations  1 of the 1 pages are all connected together. however  the incremental algorithm shows better results  see figure 1 : at the first iteration it obtains 1% precision but then the precision drops. when applying the high-degree heuristic  with the threshold at 1 hyperlinks   the result is even better  especially after the first iteration  1% precision . we use the three clusters constructed at this iteration as the core clusters  instead of using the core clusters constructed at the previous iteration-this design choice leads to a higher recall   and add the anchor text heuristic  which improves the precision . the resulting system demonstrates good performance  while the f-measure is consistently improved from 1% to 1% and then to 1% at the third hop from the source pages. this is the only result we could obtain that shows usefulness of expanding pages at the third hop.
모when comparing the heuristic search method with topical clustering  we observe exactly the same trend as for the web appearance disambiguation task  see table 1 . the best performance  1% f-measure  is obtained by the combination of the two methods after the first heuristic search iteration  which is a strong result for an unsupervised method on a multi-class task.

figure 1: precision/recall curves on the jaguar dataset  the three-class problem of recognizing pages related to cars  mac os and wild cats . h/d means high-degree heuristic  anchors means anchor text heuristic. four nodes in the curves of ihs and ihs+h/d correspond to search iterations 1  1  1 and 1. for ihs+h/d+anchors only 1 iterations are performed  with the clusters of ihs+h/d  iteration 1  taken as its core clusters.
1 conclusion and future work
to our knowledge  this paper is the first study of heuristic search in the web graph. the proposed framework is highly promising: various information retrieval and web mining tasks can be tackled in this framework. the main contribution of this paper is thus in making the heuristic search viable in the vast domain of the www and applicable to clustering of web search results and to web appearancedisambiguation.
모we show that the web is highly interconnected: heuristics are used not to seek hyperlink paths but rather to prune many irrelevant ones. despite that we obtain good results with our heuristic search method  it is still inferior to a state-of-theart machine learning topical clustering technique. however  the two methods find different types of connections between web pages. we empirically prove that the highest benefit is in combination of topological and topical clustering methods that demonstrates commercially acceptable performance.
모clustering web pages using heuristic search in the web graph might be considered burdensome but it actually is not. modern search engines store link structure of a large part of the web  so neither retrieval nor parsing of web pages should be performed in real time. since bidirectional search for hyperlink paths between clustered pages is applied  only one or two  maximum three  search iterations are usually enough to construct meaningfulclusters. the process is fully distributed so the map-reduceparadigm dean and ghemawat  1  can be employed. while the person name heuristic can be difficult to compute  the other two heuristics proposed  high-degree and anchor text  are straightforwardly applicable.
모the framework of heuristic search in the web graph poses a wide variety of interesting research problems. how to adapt heuristic search to various real-world tasks  which heuristics are the best for these tasks  could heuristics estimate the distance between two nodes in the web graph  which search control strategies should be played by multiple agents while exploring the web graph  our results open up a range of theoretical and practical opportunities yet to be addressed.
1 acknowledgments
we thank tsuyoshi murata and kevin mccurley for fruitful discussions. this work was supported in part by the center for intelligent information retrieval and in part by the defense advanced research projects agency  darpa  under contract number hr1-c1. any opinions  findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.
