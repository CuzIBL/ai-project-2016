
within the larger area of automatic acquisition of knowledge from the web  we introduce a method for extracting relevant attributes  or quantifiable properties  for various classes of objects. the method extracts attributes such as capital city and president for the class country  or cost  manufacturer and side effects for the class drug  without relying on any expensive language resources or complex processing tools. in a departure from previous approaches to large-scale information extraction  we explore the role of web query logs  rather than web documents  as an alternative source of class attributes. the quality of the extracted attributes recommends query logs as a valuable  albeit little explored  resource for information extraction.
1 introduction
despite differences in the types of targeted information  as well as underlying algorithms and tools  a common theme shared across recent approaches to information extraction is an aggressive push towards large-scale extraction. documents spanning various genres  news corpora  or the web are readily available  providing significant amounts of textual content towards the acquisition of relations such as instanceof  pantel and ravichandran  1   country-capitalof-city  cafarella et al.  1   companyheadquartersin-location  agichtein and gravano  1   person-authorof-invention  lita and carbonell  1  or person-bornin-year  pas ca et al.  1 . as a next step beyond extracting instances of a target relation that is specified in advance  this paper aims at acquiring sets of relations that constitute prominent attributes  or quantifiable properties  of a given class of instances. as an illustration  desirable class attributes targeted here include  among others:
　  manufacturer  cost  side effects  pharmacokinetics and dosage  for drug ;
　  capital city  population  gross domestic product and prime minister  for country ;
　  paintings  artistic style  influences and birth place  for painter .
　from a pure artificial intelligence perspective  class attributes recommend themselves as building blocks towards the appealing  and yet elusive goal of constructing large-scale knowledge bases automatically. yet our work is originally motivated by an activity that permeates modern societies and is undertaken by millions of people every day  namely web search. named entities constitute a large fraction of the queries submitted to the largest web search engines. in response to a query that refers to a named entity  web search engines could augmenttheir results with a compilation of specific facts  based on the set of attributes extracted in advance for the class to which the named entity belongs.
　another  more immediate application of class attributes in web search is to process a query stream to identify and flag queries that request factual information. this is particularly useful in the more frequent  but difficult case when likelyfactual queries are keyword-based rather than expressed in natural language. for example  if the search engine has specialized functionality for answering simple natural-language questions such as  what is the altitude of guadalajara    the knowledge that altitude is a prominent attribute of the class city enables the search engine to handle keyword-based queries such as  altitude guadalajara  and  guadalajara altitude  similarly to their cleaner  natural-language counterpart.
　in this spirit  and in a significant departure from previous approaches to large-scale information extraction  the target information  in this case  class attributes  is not mined from document collections. instead  we explore the role of web query logs  rather than documents  as an alternative source of class attributes. to our knowledge this corresponds to the first endeavor in large-scale knowledge acquisition from query logs. sections 1 and 1 explain why we believe that query logs represent a valuable  albeit unexplored  resource for information extraction. section 1 introduces a robust and scalable method for extracting quantifiable attributes of arbitrary classes. the method initially relies on a small set of linguistically motivated extraction patterns applied to each entry from the query logs  then employs a series of web-based precision-enhancement filters to refine and rank the candidate attributes. section 1 presents evaluation results when applying the method to a sample of approximately 1 million queries selected from a larger set of queries collectively submitted by web users to the google search engine.
1 strategies for extraction
1 class-driven extraction
if the target class is specified by mentioning only the class name  e.g.  country or drug/medicine   a possible extraction strategy is to identify authoritative documents that explicitly enumerate the attributes of the class. for the class car  an ideal enumeration could be  the most important properties of a car are its class  quality of handling  engine displacement  top speed  fuel economy  ..  . unfortunately  the likelihood of such enumerations occurring in natural language for arbitrary classes is quite low  even within web-scale collections. moreover the scale and noisy character of the web rule out the use of any expensive tools that would be required for the necessary levels of text understanding. for these reasons  class-driven extraction is both unlikely to produce useful attributes  and impractical to apply to large amounts of text.
1 instance-driven extraction
since a class  concept  is traditionally a placeholder for a set of instances  objects  that share similar attributes  properties   dowty et al.  1   the attributes of a given class can be derived by extracting and inspecting the attributes of individual instances from that class. with this strategy  the attributes of the class car are extracted by inspecting attributes extracted for chevrolet corvette  toyota prius  volkswagen passat  etc. this is particularly appealing with large sources of open-domain text  e.g.  the web  for two reasons. first  it is straightforward to obtain high-quality sets of instances that belong to a common  arbitrary class by either: a  acquiring a reasonably large set of instances through bootstrapping from a small set of manually specified instances  agichtein and gravano  1 ; or b  selecting instances from available lexicons  gazetteers and web-derived lists of names; or c  acquiring the instances automatically from a large text collection  including the web   based on the class name alone  shinzato and torisawa  1 ; or d  selecting prominent clusters of instances from distributionally similar phrases acquired from a large text collection  lin and pantel  1 . in other words  specifying the target class as a set of instances rather than the class name is simply a matter of formatting  or converting  the input class  rather than a limiting assumption. second  named entities are well represented on the web  whether documents or queries   whereas it is relatively much harder to find common-sense knowledge such as the fact that  all countries have a capital city   especially in an explicit natural-language form that lends itself to robust extraction.
1 sources of data
1 extraction from document collections
previous studies in textual informationextraction consistently use some form of a document collection as their preferred data source. in particular  many recent studies take advantage of the increasing amount of textual content available in web documents. similarly  for the task of extracting attributes for arbitrary classes  web documentsrepresent an obviouschoice based on a simple observation:
　hypothesis 1: let c be a class  and i （c be instances of that class. if a is a prominent attribute of c  then some of the documents that contain information about i are also likely to contain mentions of a in the context of i.
1 extraction from query logs
at first sight  choosing queries over documents as the source data may seem counterintuitive due to disadvantages in several key aspects: ability to convey rather than inquire about information; sheer size of available data; and availability of contextual clues. indeed  common wisdom suggests that textual documents tend to assert information  statements or facts  about the world in the form of expository text. comparatively  search queries can be thought of as being nothing more than noisy  keyword-basedapproximationsof oftenunderspecified user information needs  interrogations . in fact  even with a large document collection there is no guarantee that any relevant documents exist that provide an answer  confirm  or refute the interrogation to which a random query corresponds. from a strictly quantitative standpoint  the amount of text within query logs is at a clear disadvantage against the much larger textual content available within the web documents. finally  additional contextual information is usually available in web documents both internally  i.e.  the context in which an instance occurs in text  and externally  e.g.  through anchor text associated with incoming links. in contrast  little  if any additional context is available in web queries since their median length is only two words. despite these disadvantages  our choice of exploring query logs is motivated by the following general hypothesis about availability and use of knowledge.
　assume an abstract search scenario  in which a pool of users has access to a large body of common-sense knowledge - so large that no user can possibly memorize each individual assertion from the knowledge base. in order to find new knowledge that they need  users can search the existing knowledge base. each query that they submit is an inquiry for new information. however  users formulate their queries based on the common-sense knowledge that they already possess at the time of the search. therefore  search queries play two roles simultaneously: in addition to requesting new information  they also indirectly convey knowledge in the process:
　hypothesis 1: if knowledge is generally prominent or relevant  people will  eventually  ask about it.
　the hypothesis becomes increasingly useful as the number of users and the quantity and breadth of the available knowledge increase. indeed  to switch from an abstract to a more concrete scenario  if it is common-sense knowledge that something  e.g.  top speed  is a prominent attribute of a given class  e.g.  car   then a fraction of the set of web search queries submitted by people from around the globe are likely to capture that knowledge:
　hypothesis 1: let c be a class  and i （c be instances of that class. if a is a prominent attribute of c  then a fraction of the queries about i are likely to ask about a in the context of various instances i.
　among the possible choices in data sources  web documents vs. query logs  and format of the information about the target class  class-driven vs. instance-driven   we opt for an instance-driven extraction method from query logs  which is presented in the next section.
query logs	raw candidate attributes
fuel economy of honda civic  the maker of solaris	 honda civic  fuel economy    solaris  maker  france's capital  what is the population of iquitos	 france  capital    iquitos  population  department of commerce  schnauzer average weight	 1 	 commerce  department   schnauzer  average weight 	 1  founder of ebay  resection of 1th rib  chrysler's ceo	 ebay  founder    1th rib  resection    chrysler  ceo  pictures of schnauzer  who is the mayor of helsinki	 schnauzer  pictures    helsinki  mayor  battle of algiers  toyota of reno  meaning of poodle	 algiers  battle    reno  toyota    poodle  meaning 
extraction patterns	target classes	capitalization within web documents
what is the a of i	c1 = {helsinki  iquitos  rome  reno  ...}	... an initiative in the archdiocese of chicago ... who was a of i	c1 = {chevrolet corvette  honda civic  ...}	... hosted by the archdiocese of denver and ... the a of i	c1 = {schnauzer  fox terrier  poodle  ...}	... as the founder of ford  he contributed ... who is i's a	c1 = {linux  unix  mac os  solaris  ...}	... but the founder of ebay  the world's ... what are the a of i	c1 = {tuvalu  peru  france  thailand  ...}	... behind the toyota of glendale dealership ... i's a	c1 = {new york times  ford  ebay  ...}	... took place at the toyota of hollywood ...
resources and target classes
ranked class attributes	filtered class attributes  after filter 1 
a1 = {population  mayor  latitude  flag  suburbs  ...}	 c1  fuel economy    c1  maker    c1  capital 
a1 = {top speed  fuel economy  reliability  price  ...}	 1 	 c1  population    c1  average weight    c1  founder 	 1 
a1 = {average weight  shape  life span  fur color  ...}	 c1  ceo    c1  mayor    c1  map  a1 = {maker  system requirements  kernel size  ...}	 c1  logo 
extraction steps
unfiltered class attributes
 c1  fuel economy    c1  maker    c1  capital 
 c1  population    c1  average weight    c1  founder 
 c1  ceo    c1  pictures    c1  mayor    c1  battle   c1  toyota    c1  meaning    c1  archdiocese 
 c1  map    c1  logo    c1  meaning    c1  story 
 c1  detailed map    c1  logos    c1  meaning 
 1 
filtered class attributes  after filter 1 
 c1  fuel economy    c1  maker    c1  capital 
 c1  population    c1  average weight    c1  founder   c1  ceo    c1  pictures    c1  mayor    c1  meaning 
 c1  map    c1  logo    c1  meaning    c1  story 
 c1  detailed map    c1  logos    c1  meaning 
 1 
filtered class attributes  after filter 1 
 c1  fuel economy    c1  maker    c1  capital 
 c1  population    c1  average weight    c1  founder 
 c1  ceo    c1  mayor    c1  map    c1  logo 
figure 1: overview of data flow during class attribute extraction c1  detailed map    c1  logos 

1 extraction method
1 selection of candidate attributes
our extraction method consists of three stages: selection of candidate attributes for the given set of target classes  steps 1 and 1 from figure 1 ; filtering of the attributes for higher quality  steps 1 through 1 ; and ranking of the attributes that pass all filters  step 1 .
　for robustness and scalability  a small set of linguisticallymotivated patterns extract potential pairs of a class label  or instance  and an attribute from query logs. this corresponds to step  1  from figure 1. for each pair  a weighted frequency is computed as a weighted sum of the frequencies of the input queries within the query logs. the weights are set such that full-fledged natural-language queries  e.g.   what is the population of iquitos   have a higher contribution towards the weighted frequencies. in a useful extension that better captures frequency data available within query logs  the logs are scanned again for any keyword-based queries that contain the elements from the collected pairs  in any order  and nothing else  e.g.   population iquitos  or  iquitos population  . the motivation of this step is that keyword-based queries are used much more often by web users as compared to naturallanguage queries. the frequencies of those matching queries are added to the counts computed initially for the pairs.
　figure 1 illustrates how step  1  converts the collected pairs into a smaller set of yet-unfiltered attributes that apply only to instances of the target classes. for example  honda civic is an instance of the target class c1. it also appears in the first pair shown as collected after step  1 . therefore the attribute  fuel economy  is extracted for c1. the weighted frequency of each pair output by step  1   e.g.   c1  capital   aggregates the weighted frequencies of the source pairs  e.g.   france  capital    peru  capital    thailand  capital   etc. .
1 filtering of candidate attributes
due to the simplicity of the extraction patterns applied during pre-processing  and the relative low quality of the average web query  the candidate attributes selected after step  1  are quite noisy. a series of filters successively improve the quality of the sets of class attributes.
　the first filter - step  1  from figure 1 - identifies and discards attributes that are proper names or are part of longer proper names. since many users of search engines make no upper vs. lower-case distinction when they type their queries  it is not feasible to detect the true case of the candidate attributes based on query logs alone. however  it is relatively straightforward  although computationally expensive  to recover case information heuristically  by scanning web documents for mentions of the candidate attributes. more specifically  let a be an attribute to be checked  e.g.  toyota  and i be instances  e.g.  helsinki  glendale  hollywood  of the class. if the pattern  the upper-case-a of i   e.g.   the toyota of glendale   occurs more frequently in web documents than the pattern  the lower-casea of i   e.g.   the founder of ford   does  then the attribute is discarded as upper-case  unless it is a known person title such as queen ; otherwise it is retained. although simple  this technique proves quite effective in discarding spurious attributes such as the abovementioned toyota  as well as archdiocese  battle  etc.
　the second filter corresponds to step  1  from figure 1. it discards candidate attributes that are deemed to be generic  e.g.  meaning  story  picture  pictures and summary   in that they are simultaneously associated with many target classes.
　the third filter  step  1  from figure 1  aims at reducing the number of attributes that are semantically close to one another within a class  thus increasing the diversity and usefulness of the overall set. for the sake of simplicity  we prefer a fast heuristic that flags attributes as potentially redundant if
classsizeexamples of instancesdrug1ibuprofen  tobradex  priloseccompany1ford  xerox  longs drug storespainter1georgia o'keeffe  ossip zadkinecity1hyderabad  albuquerque  tokyocountry1yemen  india  paraguay  egypttable 1: size and example of instances in each target class
they have a low edit distance to  or share the same head word with  another attribute already encountered in the list.
1 ranking of candidate attributes
in summary  the weighted frequencies of the pairs of a class and an attribute that pass all filters are derived successively from the original frequencies in query logs. the computation takes the frequencies from query logs  weights them based on which pattern they match  and then adds frequencies together as different entries are collapsed into identical pairs during pre-processing and then selection of attributes.
　using the notation wf c a  for the weighted frequencyof the attribute a within the class c  the first formula frequency  for scoring the attributes is:
score c a  = wf c a 
　in other words  the first formula exploits the weighted frequencies directly to determine the relative order of the returned attributes within each class. to better capture attributes that are specific to a particularclass  a second formula  reranked  computes pmi-inspired  turney  1  scores:
w c a	n
where n is the total frequency over all pairs  and sf c a  is a smoothing factor to avoid an over-emphasis of rare attributes. with either formula  the top attributes in each class are returned in step  1  from figure 1.
1 evaluation
1 experimental setting
query logs: the data source for attribute extraction experiments is a random sample of around 1 million unique  fullyanonymized queries from larger query logs collected by the google search engine in the first few months of 1. all queries are in english  and are accompanied by their total frequency of submission within the logs.
target classes: due to the time intensive nature of manual accuracy judgments often required in the evaluation of information extraction systems  agichtein and gravano  1; cafarella et al.  1   we restricted ourselves to a conservative number of classes for testing. the test set comprises five target classes  namely drug  company  painter  city  and country  which respectively correspond to artifacts  organizations  roles  and two geographic entities. each of these classes is specified as a set of representative instances  details on which are given in table 1. 1 as explained earlier in

figure 1: precision as a function of rank. dotted lines represent lists that have been through heuristic filtering  section 1   black denotes lists reranked based on the pmiinspired formula  and gray represents frequency-ranked lists.
labelvalueexamples of attributesvital1 country  president    drug  cost okay1 city  restaurant    company  strengths wrong1 painter  diary    drug  users table 1: correctness labels for the manual assessment
section 1  our apparent reliance on pre-specifying the target class through a set of instances is not a limiting factor.
1 results
precision: four distinct lists of attributes are evaluated for each class  corresponding to the combination of the use of one of the two ranking methods  frequency or pmi-reranked  with either applying all filters  e.g.  city-frequency-filtered  or none of them  e.g.  city-frequency . each of the first 1 elements of the extracted lists of attributes is assigned a correctness label. similarly to the methodology previously proposed to evaluate answers to definition questions  voorhees  1   an attribute is vital if it must be present in an ideal list of attributes of the target class; okay if it provides useful but non-essential information; and wrong if it is incorrect. to compute the overall precision score  the correctness labels are converted to numeric values as shown in table 1. precision at rank n in a given list is thus measured as the sum of the assigned values of the first n attribute candidates  divided by n. with frequency ranking and with all filters enabled  table 1 illustrates the top attributes returned for the five target classes. a few of these attributes that are deemed to be wrong include long term use for drug  and best for city. table 1 summarizes the resulting precision values at various ranks.
　figure 1 gives a detailed illustration of the effects of filtering  described earlier in section 1  and reranking when applied to the classes city and drug. the accuracy varies among different classes. somewhat surprisingly  using the reranked scoring formula does not bring significant benefits over the simpler frequency ranking  as seen in the lack of separation between gray versus black lines. on the other hand  the heuristic filtering increases accuracy significantly  as illustrated by the difference between dotted and bold lines  es-
country: capital  population  president  map  capital city  currency  climate  flag  culture  leaderdrug: side effects  cost  structure  benefits  mechanism of action  overdose  long term use  price  synthesis  pharmacologycompany: ceo  future  president  competitors  mission statement  owner  website  organizational structure  logo  market sharecity: population  map  mayor  climate  location  geography  best  culture  capital  latitudepainter: paintings  works  portrait  death  style  artwork  bibliography  bio  autobiography  childhoodtable 1: top ten attributes acquired from query logs
classprecision 1 1 1 1 1country11111drug11111company11111city11111painter11111table 1: precision at various ranks n
pecially within the crucial 1 top ten entries. recall: in general  the measurement of recall requires knowledge of the complete set of items  in our case  attributes  to be extracted. unfortunately  this number is often unavailable in information extraction tasks  hasegawa et al.  1; cafarella et al.  1 . following a scenario that matches well the intended use of our extracted attributes  an unbiased gold standard collects attributes of the class country from the first 1 main-task questions used in past editions of the question answering track of the text retrieval conference  voorhees and tice  1 . table 1 gives these attributes sorted by frequency of appearance among the questions  along with their respective ranks as exact strings within our own extracted attribute list. for example  the goldstandard attribute capital city  collected from questions such as  what is the capital city of algeria    is present among the extracted attributes at rank 1.
alternate sources of evaluation data: in a more holistic evaluation of recall  starting from a few provided sample attributes for the classes dog and planet  survey participants were asked to provide a set of what they considered to be important attributes for each of our five target classes. each of our extracted lists of attributes are searched by hand in order to find candidates that are deemed semantically equivalent to what participants provided. 1 a sample of these results are provided in table 1. note that  city  quality of living  is not an attribute that we would consider to be vital  as compared to some equivalent version of  drug  is it addictive  or  company  is it nonprofit . from the survey results it was primarily these sort of binary valued attributes that we failed to extract  leading us to consider if in the future alternate patterns may potentially improve our performance.
specifically for the class country  table 1 contains exam-
attributerankattributerankcapital1emperor-population1date of independence1prime minister1currency1leader1area1capital city1queen1president1gdp1size1table 1: gold-standard attributes for country from trec questions  with their rank in the filtered  frequency-rankedlist
attributer.attributer. painter  nationality 1 painter  influences 1 drug  is it addictive - painter  awards - drug  side effects 1 country  income 1 company is it nonprofit - country  neighbors - company  competitors 1 city  mayor 1 city  quality of living - city  taxes 1table 1: sample of attributes given by survey participants and their ranks in the filtered  frequency-ranked list  r. = rank 
ples of attributes mentioned by textual descriptions found in the cia factbook. 1 the table also shows where the attributes occur in our own extracted list. unsurprisingly   common  attributes such as flag and map are more attainable via query logs versus specific  yet equally correct  gold-standard attributes such as household income consumption by percentage share or manpower reaching military service age annually. in fact  we feel that the automatic extraction of such detailed   summarizing  attributes from natural-languagetext is beyond current state of the art in information extraction.
1 comparison to previous work
although our work naturally fits into the larger goal of building knowledge bases automatically from text  craven et al.  1; schubert  1   to our knowledge we are the first to explore the use of query logs for the purpose of attribute extraction. similar goals motivated a few other recent studies  although the scale  underlying methods and data sources differ significantly. in  chklovski  1   the acquisition of attributes relies on web users who explicitly specify class attributes given a class name. in contrast  we may think of our approach as web users implicitly giving us the same type of information. the method proposed in  tokunaga et al.  1  applies lexico-syntactic patterns to text within a small collection of web documents. the resulting attributes are evaluated through a notion of question answerability  wherein an attribute is judged to be valid if a question can be formulated about it. more precisely  evaluation consists in users manually assessing how natural the resulting candidate attributes are  when placed in a wh- question. comparatively  our evaluation is stricter. indeed  many attributes  such as long term uses and users for the class drugs  are marked as wrong in our evaluation  although they would easily pass the question answerability test  e.g.   what are the long term uses
attributerankattributeranknatural resources1irrigated land-terrain1administrative divisions-climate1infant mortality rate-table 1: sample of country attributes from the cia factbook and their rank within the filtered  frequency-ranked list
of prilosec    used in  tokunaga et al.  1 .
　we might view at least a fraction of the web query logs as a collection of interrogations corresponding to pregenerated  natural-sounding questions  except that they have been stripped of most of their syntax  and have been mixed in with queries that were never questions to begin with. from this perspective  our task is related to classifying queries into questions vs. non-questions.
1 conclusion
recent studies in large-scale knowledge acquisition from text are fueled by the belief that the web as a whole represents a huge repository of human knowledge. taking this idea further  this paper presented a method for extracting a particular type of knowledge  namely class attributes  based on the hypothesis that web search queries as a whole also mirror a significant amount of knowledge. the knowledge is implicitly encoded in obfuscated queries that are usually ambiguous  keyword-based approximations of often-underspecified user information needs. in one of the first attempts to systematically decode and exploit a very small part of the information that web queries wear on their sleeves  we introduced a robust model for extracting class attributes from query logs  producing average precision levels of 1% and 1%  over the top ten and top thirty extracted attributes respectively.
　while the approach relies heavily on query logs  we have begun exploring how to incorporate additional evidence from more traditional sources  such as natural language text and semi-structured text  e.g.  tables   which may help further improve the quality of the output.
　building upon recent contributions towards large-scale knowledge acquisition  we see our efforts as part of a larger program aimed at constructing large fact databases. in our vision  successive text mining stages start from unlabeled natural language text and 1  identify clusters of instances grouped into semantic classes  cf.  lin and pantel  1  ; 1  assign labels to the corresponding classes  cf.  pantel and ravichandran  1  ; 1  acquire the most prominent attributes of each class  e.g.  through our method ; and 1  identify the values of the attributes for various instances of the class  based on current state of the art in answer extraction .
