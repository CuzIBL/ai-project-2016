 
we present a framework foreasoning under uncertainty using mass distributions. our basic concept is the flow of evidence masses from supersets to subsets which is specified by experts. the expert domain is assumed to have various characteristics so that it can be considered to be a product space with one coordinate for each characteristic. encoding the qualitative knowledge structure in a dependency network  hypertree  it is possible to describe an efficient propagation algorithm providing mass distributions on each characteristic. this algorithm bases on the computation in  small  subspaces of the product space which reduces its complexity in time as well as in space. it is also easy to make use of parallel computer architectures to improve the efficiency of the algorithm. 
1 	introduction 
the purpose of this paper is to provide a suitable theoretical tool for the treatment of uncertain information which is an important problem in the range of knowledge based systems  buchanan and shortliffe  1  kruse et a/.  1  pearl  1   so subject to our investigations are statements being not simply true or false but with a validity which is a matter of degree. this may be caused by the fact that the statements' truth depends on some underlying random process  that their 'reliability' reflects the subjective estimates of a human decision maker or that the concept of truth itself has to be weakened. 
　throughout this paper we will restrict ourselves to the treatment of subjective valuations of evidence which requires the use of belief functions  gordon and shortliffe  1  kohlas  1  kruse et a/.  1  lowrance et a/.  1  shafer and logan  1  shafer et a/.  1  smets  1  measuring the credibility of information although our concept of specialisation is very general and can be applied to probabilities as well as to possibility measures. 
the expert domain is assumed to be a product space 

nonempty set. the qualitative dependency structure can be described by a dependency network reflecting dependencies like  in the subs pace 
quantitative knowledge about the domain is encoded in mass distributions and specialisation matrices quantifying the flow of evidence masses  the concept we prefer to dempster's rule of conditioning  shafer  1  often applied in the calculus of belief functions. 
　section 1 to provides a short outline about mass distributions and belief functions. in section 1 we present our main concept: the flow of evidence masses given by a specialisation matrix  kruse and schwecke  1 . this concept can be applied to revision. 
　in order to structure the knowledge about the qualitative dependencies we make use of dependency networks in section 1. section 1 describes the propagation algorithms we use to draw conclusions from the knowledge. this algorithm was implemented on a ti-explorer making use of the kee-shell. it was developed in cooperation with dornier gmbh  friedrichshafen  germany. applications in the field of data fusion are implemented. some conclusions concerning the capabilities and restrictions of our tool are presented in the last section. 
1 	knowledge representation with mass distributions 
as mentioned in the introduction belief functions aim to model a human decision maker's subjective valuation of evidence. for this purpose we consider a finite sample space ft containing the possible atomic events. the decision maker or 'expert' distributes one unit of 'belief which we can imagine as movable 'evidence mass' among the elements of ft with respect to his knowledge about the actual situation  attributing greater amounts to the more likely elements. we will describe such distribution by a mapping  where the subjective probability of some event is given by unfortunately in practice the expert often is unable to make definite statements  i.e. he fails to specify such distribution in detail. he is only able to valuate complex events  but not to determine the distribution of that amount of belief committed to a onto its elements. in such cases he specifies a mass distribution m which is a mapping from  to the unit interval.  in this context is named the frame 

1 	qualitative reasoning 

	kruse  schweke  and klawonn 	1 

	1 	qualitative reasoning 

1 reasoning in product spaces and dependency networks 
as mentioned in the introduction the universe of discourse can be viewed as a product space of different characteristics of the domain. the quantitative knowledge is encoded by mass distributions on  and also by specialisation matrices. the aim of the knowledge acquisition process is to provide a set of possible mass distributions given by the experts. 
   typically  such a set of mass distributions can be viewed as revisions or more generally specialisations of an a priori distribution. 
　one cannot expect the experts to give information about the whole product space . usually only dependencies of small groups of characteristics x   i   can be described by the experts. so  what we obtain is knowledge about certain subspaces w h e r e t h u s it is important to know the qualitative structure of the domain which describes the dependencies between characteristics and subspaces. 
   the first step of the knowledge acquisition process therefore is to elucidate the qualitative structure of the knowledge resulting in a dependency network with characteristics  as vertices and dependency relations as edges. an edge between two vertices . and  represents a direct dependency between 
these two characteristics which is specified by the experts in the subspace the dependency network is directed because of the knowledge being given by rules 

we also assume that all characteristics influence each other mutually  at least indirectly . otherwise the universe of discourse could be split into different domains which could be modelled independently. 
   to obtain a propagation algorithm which propagates the knowledge about the subspaces through the dependency network it is important that the network is acyclic. methods of avoiding cycles can be found in  kruse and 
schwecke  1  shafer and shenoy  1 . for a formal definition of a dependency network see e.g.  kruse et a/.  1 . 
   the dependency networks enable us to make local computations of the validations given by the experts  shenoy  1 . 
1 	the propagation algorithm 
in this section we describe the way in which the experts' knowledge is encoded via specialisation matrices and how an 'evidence' obtained from absolutely reliable observations induces new  revised  mass distributions on  
   first the dependency structure of the expert domain has to be represented by a dependency network. then the experts specify mass flows in certain subspaces by specialisation matrices. 
   for each dependency of the form   the experts provide a specialisation matrix which we consider to be an orthogonal extension matrix  that means the matrix specifies the flow of evidence masses from cylinder sets a in  to sets 

　the acquisition of the different matrices is done according to the causal consistent ordering of the dependency network. starting point is the state of total ignorance  where nothing is known but the 'closed world assumption' mentioned before  which is represented by the mass distribution  the only mass distribution on ignorance  i.e. before any expert has been asked  all mass distributions on are considered possible  because the projection of any mass distribution to yields 
　in addition to the experts' knowledge reliable observations have to be considered.  if nothing is observed with respect to we may assume representing the closed world assumption.  this reliable information initiates a revision process revising the possible mass distributions given by experts with respect to . . o f course  the computation is not performed in the product space but in appropriate subspaces. the results are mass distributions on which are projections of the possible revised 
mass distributions on  
   the basis of the algorithm we present in the sequel is the idea of independent node processors  exchanging messages. for each quantitative dependency of the 
form 	of the dependency network there is one processor 	which stores 
the respective orthogonal extension matrix  quantifying the dependencies of the 	characteristics  
 ad node processors are of the same structure. we have 
  an input port to receive messages  from the user  concerning his observation of characteristic .  
  for each predecessor 1 a pair of input/output ports to receive a message and to send a message 
  for each successor u a pair of input /output ports to receive a message and to send a message 
  an output port to provide the marginal revised mass distribution for the user. 
the structure of root and leaf nodes is somewhat different. for root nodes no facilities for the communication with fathers are required and similarly leaf nodes need not to communicate with sons. 
   in the sequel we assume the knowledge acquisition process to be completed  i.e. at every node the orthogonal extension matrices  resp. the marginal mass distribution at the root nodes  are available. in the beginning all observations  are assumed to be equal to the respective domain  that means thete is no observational knowledge  i.e. for initialization process we have  to bring up the system each node has to send its messages to all neighbours. since the node processors are assumed to 
	kruse  schweke  and klawonn 	1 
work independendly  this can be done in parallel. the local algorithm for every node is as follows. 
   if the node processor  receives the observation e   j   from the user it has to perform the following activities 
 i  the calculation of the new marginal revised mass distribution. 
 ii  the calculation of the new messages for the father.  iii  the calculation of the new messages for son u  
for details see  kruse et a/.  1 . there is no feedback  the algorithm terminates after a well defined number of steps. the maximum path length a flow of messages has to cover is therefore given by the diameter of the dependency network. so if a dependency network on the basis of belief functions has to be installed  then at first the qualitative structure of the domain has to be determined by the construction of a dependency network  then the dependencies have to be quantified by the orthogonal extension matrices and after that the dependency network is ready for operation. 
   besides the algorithm presented before there are several further aspects  which are important for an useful software tool for the representation of uncertain knowledge on the basis of belief function. for the structure and the man-machine-interface of such system there are some requirements it has to meet  if a succesful application is aspired. 
   first the process of knowledge acquisition has to be supported. the expert should be able to determine first of all the qualitative structure of the domain in from of a hypergraph  whose edges represent the dependencies between the corresponding characteristics. in order to avoid cycles a hypertree covering of the hypergraph has to be constructed  shafer and shenoy  1 . this should be graphically supported. the change from an arbitrary hypergraph to a dependency network by the 
join of nodes should be performed either by direct user interventions or automatically. another possiblity to assure that a dependency network arises from this first part of the knowledge acquisition process is to provide only those tools for the construction of the dependency graphs  on the screen  which cannot perform any action destroying the dependency network property. the second step the expert has to perform is to determine the domains for the different characteristics. if a node represents a compound characteristic the single domains should be specified. 
   after that the expert has to quantify the qualitative dependencies he stated to exist. that means he has to determine the orthogonal extension matrices. recall that a typical orthogonal extension matrix has the form 

no expert can determine 1  1 numbers  so he has to be powerfully supported. one possibility to do this is to start with a default assignment of v where 

which represents total ignorance. only if the expert explicity specifies changes of these default values they are modified. 
   a b o the user needs support. firstly he should have a graphical representation of the dependency network  which enables him to bring in his observations  secondly there should be a graphical presentation of the results to facilitate the correct interpretation of the results. the system should not provide directly the mass distribution but the induced belief- and plausibility function  but not of all sets but of those the expert specified to be of interest. 
in addition he should be able to ask for arbitrary sets. 
moreover it might be useful to defer the normalization to the output of the results and to display the portion of evidence mass  on which the result is based  e.g. 1%  to have a measure of the degree of 'pathology' of the actual case under consideration. 
   the above example shows the limits of this approach. the complexity of these matrices explodes if the size of the involved sets increases  especially if compound characteristics or nodes with multiple fathers are considered. but if the domains are slightly small  e.g. 
 and the dependency net-
work is mainly a tree then this approach can yield useful results. 
   an additional increasing on efficiency can be obtained by omitting the graphical surface in process control applications or by efficient algorithms for the representation of orthogonal extension matrices  making use of the fact that they tend to be sparse. 
1 	conclusions 
the main objective of this article is to introduce a software tool for handling uncertainty about imprecise data. note that in contrast to the work on belief function done by shafer  shenoy and smets where the rule of combination plays a central role we use a mass flow concept where the masses  or probabilities  are attached to the imprecise data  subsets of a frame of discernment . 
   we assume the qualitative knowledge  about dependencies  to be encoded in terms of hypertrees  shafer and shenoy  1 . this allows to construct a propagation algorithm which gaines efficiency by using methods similar to the ones considered by  shenoy  1 . furthermore we assume the quantitative expert knowledge 

1 	qualitative reasoning 

to be encoded in cylinder sets of the product space which leads to specialisation matrices with only a few non-zero entries reducing the expenditure of computer memory. 
   the result of the reasoning process are mass distributions on the sets  describing the knowledge about characteristics x  and being projections of mass distributions on the whole product space. 
　the method has been implemented on a ti explorer under k e e and can be used f.e. for data fusion problems. 

