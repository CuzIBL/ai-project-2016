computational complexity of hypothesis assembly 
dean allemang  michael c. tanner  tom bylander  john josephson 
laboratory for artificial intelligence research 
department of computer science 
the ohio state university 

a b s t r a c t 
     the problem of finding a best explanation of a set of data has been a topic of much interest in artificial intelligence. in this paper we present an approach to this problem by hypothesis assembly. we present this approach formally so that we can examine the time complexity and correctness of the algorithms. we then examine a system implemented using this approach  which performs red blood antibody identification. we use this domain to examine the ramifications of the assumptions of the formal model in a real world situation. we also briefly compare this approach to other assembly approaches in terms of time complexity and reliance on assumptions. 
i . i n t r o d u c t i o n 
     the problem of abductive reasoning  as proposed by the philosopher c.s. peirce  has been a topic of much recent interest in artificial intelligence  miller 1  reggia 1  charniak 1 . the general task faced by an abductive reasoning system is to find the best explanation of a set of data or observations  i.e.  the best way to account for a set of data. most of the work in artificial intelligence in this area has focused on a specific kind of abduction  which we call hypothesis assembly. the hypothesis assembly task assumes as given a set of hypotheses with some knowledge about what sorts of data each can account for  and finds the subset of these hypotheses that best accounts for the problem data. at the ohio state laboratory for artificial intelligence  josephson et. al.  1  have been developing an approach for abduction based upon hypothesis assembly. 
     in this paper we will begin by presenting a mathematical idealization of this approach. from this we will analyze the complexity and correctness of the algorithm. then we will examine how well this idealization matches with real 
     this work has been supported by the national library of medicine under grant lm-1  the national science foundation through a graduate fellowship  and the defense advanced research projects agency  r a d c contract f1-c-1. computer facilities were enhanced through gifts from xerox corporation. 
reasoning 
world concepts of abduction. in particular  we will examine a system called r e d   based upon this approach  which performs antibody identification in the domain of red blood cell typing  as described in  josephson 1    josephson 1  and  smith 1   and show how the general mathematical results respond to questions that have been raised  mostow 1  about its complexity. 
i i . m a t h e m a t i c a l i d e a l i z a t i o n o f a b d u c t i o n 
d e f i n i t i o n s 
in order to motivate the following definitions  we will examine briefly the domain of the current implementation of red  the domain of blood bank antibody analysis. the primary data consists of results of several lab tests on blood samples. the blood bank technologist knows how antibodies can account for various reactions. the lab tests have the property that if antibody a accounts for some reaction r  and antibody b accounts for reaction q  then the presence of both antibodies a and b accounts for both reactions r and q. this property of a domain will be referred to as independence of hypotheses. 
     more formally  we define a domain for hypothesis assembly as a the triple  h  m e   where h is a finite set of hypotheses  m is a finite set of manifestations  and c is a map from subsets of h to subsets of m. e s  is interpreted as the explanatory power of a set of hypotheses  and is the set of manifestations for which those hypotheses can account. an assembly problem is specified by a subset mo m. mo is interpreted as the set of observed manifestations1. in these terms  we have the independence assumption: 
if s and t are subsets of h  then 

     although many domains satisfy the independence assumption  we wish to strengthen our result by replacing 

1
　　 in what follows  we will use the notation e s  where we should  strictly speaking  write  for the restriction of e s  to the observed manifestations. 
the independence assumption with two assumptions  which when taken together are weaker than the independence assumption. 
	the 	monotonicity assumption: 
if 1 and t are subsets of h  

     clearly any domain that satisfies the independence assumption also satisfies the monotonicity assumption. 
the accountability assumption 
     for a hypothesis  it is possible to know what manifestations h can account for. formally  the function 


can be computed as easily as e  polynomial calls to e . 
     notice that if the independence assumption holds  then  so the accountability assumption holds. 
     central to the success of any hypothesis assembly algorithm is the following assumption: 
the computability assumption: 
for any subset 1 of h  e s  can be computed. 
     clearly the complexity of this computation is central to the complexity of the assembly task  since it is difficult to ask for a set of hypotheses that accounts for some set of manifestations if it is difficult to compute what is accounted for by a set of hypotheses. in cases in which independence holds  this computation can be done in linear time by simply having a table that tabulates e for each individual hypothesis  so that e 1  can be computed by taking the union of e s  for s  this has allowed the current red mechanism  the set covering algorithm of reggia  reggia 1   and internist  miller 1  to safely ignore the complexity of the computation of e. in fact  the incremental nature of all these algorithms allows them to compute e s  in constant time  since the usual case is that some set t is to be extended with another hypothesis s  with e t  already known  so that the computation of e can be done by simply looking up e {s} . for the complexity analysis which follows  we will assume an oracle for the function e  that is  we make the calculations counting one step for each evaluation of e. 
b. description of the hypothesis assembly algorithm 
　　we now describe the algorithm for abduction by hypothesis assembly. the algorithm has four phases; screening  hypothesis collection  parsimony and critique. 
     the input to the algorithm is the set of observed manifestations  mo- the output is a set of hypotheses that constitute a 'best' explanation of the data. 
　　the screening phase determines plausibilities  i.e.  prima facie likelihoods  for the veracity of the hypotheses. hypotheses with very low plausibilities are ruled out for further consideration. in this discussion we wish to examine and analyze the assembly phases of the algorithm  so we will omit a description of the screening phase. when we discuss applications  we will see one way to accomplish screening. 
the collection phase works in the following way: 
be a domain for hypothesis assembly; let 
be a problem in that domain. 
m'  h' and m are variables in the following algorithm. 
let m' = mo be the observed manifestations  the manifestations to be explained . 
let h' be the empty set  working set of hypotheses . 
until a/' is empty  do the following: 
let m be the most salient hypothesis in m'. 
find the most plausible hypothesis h that offers to account for m  i.e.   h  f adjoin h to h'. compute the reactions accounted for by  '. 
 is the set of mani-
festations yet to be explained. 
the set h' is the assembled explanation of the observed manifestations. 
the parsimony phase works in the following way: 
for each hypothesis h in h'  the working hypothesis set   starting from the least plausible to the most plausible  do the following:  then h is superfluous  so set 
     h' is now a parsimonious set of hypotheses  that is  it has no redundant parts. note that this does not guarantee that h' is a complete explanation of least cardinality. 
　　the critique phase determines whether a hypothesis is essential to the explanation  that is  there is no way to ac-
count for the observed data without using that hypothesis. 
this phase works as follows: 
for each hypothesis h in h' do the following: 
set g :=  repeat the collection algorithm  above  using g as working hypothesis  with hypothesis h marked as unusable. 
	allemang  tanner  bylander  and joaephaon 	1 

if collection completes  that is  there is a composite hypothesis that does not use h which explains a/o   then h is not essential. 
if collection fails to complete  then hypothesis h is essential. 
c  complexity of the assembly mechanism 
we begin by analyzing the time complexity of the assembly phase. 
the collection algorithm can be analyzed as follows: 
     let h be the number of hypotheses that were not ruled out; let m be the number of manifestations observed. 
     for each manifestation  we select a hypothesis that offers to explain that manifestation  and evaluate the resulting composite. there are m such manifestations  and selection of a hypothesis takes at most h calls to the oracle a  for a linear search . the computation of m' e h'  can be done in o mlogm  steps  sort both sets  and merge the results . 
thus  the complexity of this step is 
     each time a new hypothesis is added to the working composite hypothesis h'  it is necessary to evaluate e h'   so there are 1 m  calls to the oracle e. 
next we analyze the parsimony algorithm: 
     at most h hypotheses are considered for redundancy  so there are 1 h  calls to the oracle e. at each such step  we must compare the explained set of manifestations to the observed set  that can be done in time 1 m log m   first sort both sets  then compare elementwise   so the entire complexity is 1 /imlogm . 
     notice that this algorithm does not guarantee that we have found a compound hypothesis of smallest size that accounts for the observed data. this problem  which is the task of the reggia et. al. set covering program  reggia 1   can be shown to be np-complete  reduction to vertex cover . this algorithm simply guarantees that the parsimonious solution has no proper subset that is also a solution. 
now we analyze the critic for essential hypotheses: 
     the critic takes each of the hypotheses in the working hypothesis and marks it as unusable  then repeats the assembly algorithm. there are at most h hypotheses in the composite hypothesis  so this is at most h times the complexity of assembly  that is  
d. correctness of the algorithm 
     that the collection mechanism produces a composite hypothesis that accounts for all of the observed findings  provided  of course  that such a collection exists  is obvious from the loop condition. it is also clear that the critique 
reasoning 
algorithm will find all indispensable hypotheses  and all hypotheses deemed to be essential are in fact indispensable. the parsimony algorithm needs some justification. 
     we say that a composite hypothesis is a parsimonious explanation of some findings if no proper subset of it can account for those findings. the parsimony mechanism ensures that there is no single hypothesis that can be excluded from the composite without losing the capability of explaining some finding. it is possible to construct a domain in which a composite hypothesis has a proper subset that can account for all findings  that is  the composite hypothesis is not parsimonious   yet no single hypothesis is by itself redundant  that is  there is no hypothesis such that it alone can be removed without losing explanatory power . if the monotonicity assumption is true  this can not happen. 
     suppose that 1 is a non-parsimonious composite hypothesis. then for some subset r of s  
r is itself redundant  that is  the red 
mechanism will find a redundant hypothesis in 1. hence we conclude that if 1 is not parsimonious  then red will eliminate some hypothesis from it. 
i i i . adapting the formal model to real-world abduction 
a. the abductive answer 
an abductive problem solver applied to actual problems tries to produce a best explanation for the observed data. unfortunately  there is no hard and fast rule for demonstrating that a particular explanation is best. human problem solvers argue about whether one explanation is better than another. hence the best we can do to argue that the answer given by this algorithm is best is to list the features which recommend it. 
     the answer given by this approach is guaranteed to cover all the data. every observation will be explained. 
     the answer will be parsimonious in the sense that it will have no superfluous parts. it is not necessarily the smallest answer  i.e.  the answer with the least parts. 
     the answer will be plausible  since more plausible hypotheses are preferred over less plausible ones. the algo-

rithm prefers a large number of plausible hypotheses over a smaller number of less plausible ones. the algorithm will identify essential hypotheses  so when part of the answer is provably correct  the algorithm determines this. 
     again  we cannot prove that such an answer is a 'best' explanation in any given situation  these are just the points by which one could argue for this answer. in the long run  the best answer is the one that is in fact the true situation. so now let us examine an application of this method to an actual domain. 
b. a real-world application red blood antibody identification 
1. description of the domain 
     one of the jobs done by a blood-bank technologist is to identify antibodies in a patient's serum to antigens that might appear on red blood cells. this is typically done by combining samples of patient serum with red blood cells that contain certain antigens. some of these combinations will show reactions  others will not. the presence of certain antibodies in the patient serum will account for certain reactions. the reactions are additive in the sense that if the presence of one antibody explains one reaction  and presence of another antibody explains another  then the presence of both antibodies explains both reactions. if both antibodies can account for a weak result in some reaction  then the presence of both can account for a strong result in that reaction. also  some pairs of antibodies cannot occur together. the task of the pathologist is to decide what antibodies are present  given a certain reaction pattern. 
1. satisfaction of assumptions 
     we now examine how well this task meets the assumptions of the formal model presented in the first part of this paper. 
     computabtlity of e- the pattern of antigens appearing on the test cells is available in table form. this information determines which antibodies account for any of the reactions. more detailed knowledge of the chemistry of the reactions allows us to build a table to tell the strengths of the reactions. 
     independence - the additive nature of the reactions means that for separate reactions  the independence assumption is met. since the model does not allow for parts of manifestations to be explained  we cannot really say that the additivity of reaction strengths is included in the independence assumption. 
     mono tonicity - if we view a weak result for some reaction as a separate result from a strong result for the same reaction  then we can say that the phenomenon of additive reaction strengths satisfies the monotonicity assumption. that is  each antibody alone explains the manifestation of 
'weak reaction'. together  they can explain either a 'weak reaction'  or a 'strong reaction'. 
accountability - since the domain nearly satisfies the independence assumption  it is not surprising that we can calculate a nearly as easily as we can in the independent situation. that is  if an antibody can account for any strength result for some reaction  then it can potentially contribute to an explanation of any other strength result for that reaction. 
     incompatibility of hypotheses - implicit in the formal model is the assumption that any collection of hypotheses is possible. this domain has a restriction that invalidates this assumption. it can be shown that hypothesis collection in general under such a constraint is an np-complete problem  see appendix . there are two reasons why this is not a problem in this domain. one is that the collection problem is not exponential in the number of hypotheses  or even the number of hypotheses left after screening  but in the number of incompatible pairs remaining after screening. this tends to be quite a small number. also  since the screening rates hypotheses by plausibility  the algorithm is likely to stumble upon the correct answer early in the collection process. 
1. implementation 
     the algorithm for this domain is presently implemented as follows: 
     screening - this phase is done by a hierarchical classification system similar to the mdx system  chandra 1 . 
     collection - this phase is done as in the text  with the exception that when a hypothesis is to be entered that is incompatible with some hypothesis already in the collection  the conflict is resolved by throwing out the hypothesis which is already in the set. a check is made for loops each time a hypothesis conflict arises. this is  in the worst case  a most plausible-first  brute force search. the computa tions of e and a are done from a table of singleton values; since the independence assumption holds  almost   we can compute the value of e of a set from the value of c at each element. 
     parsimony and critique these phases are done as described in the text. 
     the collection and parsimony phases are repeated  using the essential hypotheses as a starting point. 
     this algorithm matches the one in the text  except for the problem of incompatible hypotheses. so  with that exception  that was discussed above   the time complexity of the red algorithm is polynomial in the number of antibodies and reactions. 
1. availability of knowledge 
     we have seen that in the blood typing domain that the assumptions required by this model are easily satisfied. we now examine these assumptions to estimate how likely it is that they will extend to yet other domains. 
     without the computabtlity of e  it would be impossible to evaluate whether an answer constituted a complete 
	allamang  tanntr  bylandar  and joaaphaon 	1 

explanation  much less to construct such an answer. many existing abduction systems  e.g. set-covering model  reg gia 1  and internist  miller 1   depend strongly upon this assumption. 
     the independence assumption makes computations easy  since we only need to know values of e for singleton sets. diagnostic domains are likely to satisfy this assumption  since devices are often decomposable into independent parts  and hypotheses about these parts are often independent. the set covering model  reggia 1  relies strongly on this assumption. internist  miller 1  relies on this  but it would only take a simple change to internist to cope with more unruly domains  provided that they satisfy the monotonicity and accountability assumptions. 
     the monotonicity assumption is weaker than the independence assumption  hence is more likely to hold. it allows for some interaction of hypotheses  so long as no manifestations are canceled. 
     the accountability assumption is also weaker than independence  so is more likely to hold. this simply requires that we know to what manifestations a hypothesis can contribute. 
     in a more unruly domain  in which the monotonicity assumption fails to hold  that is  when several hypotheses together fail to account for data that some of them do separately  e.g.  due to subtractive interactions between the hypotheses   all of these assembly techniques will have difficulty. it can be shown that certain types of subtractive interaction problems are np-complete in such cases  it may be fruitful to find different knowledge so that the problem is no longer one of assembly. one approach along this line is abel  a system described in  patil  1   which attempts to make this problem tractable by making use of elaborate causal structures. 
conclusion 
     an important characteristic of problems that are dealt with in artificial intelligence is that domain knowledge is necessary to make the computations feasible. if an approach is to be widely applicable  it is necessary to know what knowledge is necessary to ensure that the approach will yield correct answers in a reasonable time. 
     in this paper  we have constructed a formal model of the approach to abductive reasoning illustrated by the red system. from this model  we have identified when the available knowledge will allow application of this approach. under the assumption that the knowledge is of this form  that is  it satisfies the assumptions of monotonicity  accountability and computability   we have demonstrated the tractability and correctness of the computation. 
reasoning 
appendix i 
proofs of theorems 
np-completeness of the reggia algorithm 
after reggia  1   using the notational conventions of this paper   we have the following definitions: 
a diagnostic problem is a 1-tuple  h/  a/  c  m1 . 
     h  hi are finite sets of hypotheses and manifestations respectively  

   an explanation for a diagnostic problem is a set   	   for any 	s.t. 
     we wish to show that given a diagnostic problem  finding an explanation is np-complete. we give a reduction from minimal node cover. 
   minimum node cover: given a graph   find a set  of smallest cardinality s.t. has an endpoint in v. 
     we form a diagnostic problem from the graph g as follows: 
set 

     clearly an explanation of this diagnostic problem is also a cover of the graph  1  of minimum cardinality. also clearly this transformation is polynomial in the size of g. 
np-completeness of hypothesis assembly with incompatible hypotheses 
     an hypothesis assembly problem under constraints of incompatible hypotheses can be specified as a 1-tuple 
 h a/ c / afu   where 
     h and a/ are finite sets of hypotheses and manifestations respectively  
	c is a relation  	a table for computing e  
/ is a set of unordered pairs of elements of h  and 
	for a subset 	for the set 
   the interpretation of these sets is that h is a set of hypotheses that can account for certain manifestations in a/      .-x. e h  means that h can account for m. 
means than hypotheses h and h1 are incom-patible. mo is the se
plete explanation is a set such that    and such that there is no pair 

     we prove np-completeness of this problem by reduction from 1sat  garey 1 . 
     1sat: given a statement in propositional calculus in disjunctive normal form  in which each term has at most three factors  find an assignment of the variables that makes the statement have value 'true'. 
     we form an hypothesis assembly problem from a 1sat problem by mapping factors in a 1s at problem to hypotheses and mapping terms to manifestations as follows: 
     let p be a statement in propositional calculus in disjunctive normal form  where each term has at most three factors. let 	{ 	be the variables used in p  let t be the number of terms in p. 
     we now construct an instance of the hypothesis assembly problem under incompatibility constraints with 1n hypotheses and t manifestations. 


     if we find a complete explanation s for this problem  then we claim that the assignment 

will be a solution to p. 
with 
since ' e h1   we have  from the construction of c  and hence  e   that u1 is a factor in the jth term of p  hence the 
jth term of p has value true. 
and hence u1 is false  and since m; 
- i u   is a factor in the jth term of p  hence again the jtk term of p has value true. 
since every term of p has value true  so does p. 
     a similar argument shows that there exists a valid assembly corresponding to each satisfying assignment. 
     clearly the construction of this problem from p is a deterministic polynomial process. also it is clear that complete explanation construction under incompatibility constraints is np  so hypothesis assembly with incompatible hypotheses is np-complete. 
