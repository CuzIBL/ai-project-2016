
we investigate three parameterized algorithmic schemes for graphical models that can accommodate trade-offs between time and space: 1  and/or adaptive caching  aoc i  ; 1  variable elimination and conditioning  vec i  ; and 1  tree decomposition with conditioning  tdc i  . we show that aoc i  is better than the vanilla versions of both vec i  and tdc i   and use the guiding principles of aoc i  to improve the other two schemes. finally  we show that the improved versions of vec i  and tdc i  can be simulated by aoc i   which emphasizes the unifying power of the and/or framework.
1 introduction
this paper addresses some long-standing questions regarding the computationalmerits of several time-space sensitive algorithms for graphical models. all exact algorithms for graphical models  either search or inference based  are time and space exponentially bounded by the treewidth of the problem. for real life networks with large treewidth  the space limitation can be quite severe  therefore schemes that can trade space for time are of outmost importance.
　in the past ten years  four types of algorithms have emerged  based on:  1  cycle-cutset and w-cutset  pearl  1; dechter  1 ;  1  alternating conditioning and elimination controlled by induced-width w  rish and dechter  1; larrosa and dechter  1; fishelson and geiger  1 ;  1  recursive conditioning  darwiche  1   which was recently recast as context-based and/or search  dechter and mateescu  1 ;  1  varied separator-sets for tree decompositions  dechter and fattah  1 . the question is how do all these methods compare and  in particular  is there one that is superior  a brute-force analysis of time and space complexities of the respective schemes does not settle the question. for example  if we restrict the available space to be linear  the cycle-cutset scheme is exponential in the cycle-cutset size while recursive conditioning is exponential in the depth of the pseudo tree  or d-tree  that drives the computation. however some graphs have small cycle-cutset and larger tree depth  while others have large cycle-cutsets and small tree depth  e.g.  grid-like chains . the immediate conclusion seems to be that the methods are not comparable.
　in this paper we show that by looking at all these schemes side by side  and analyzing them using the context minimal and/or graph data structure  mateescu and dechter  1b   each of these schemes can be improved via the and/or search principle and by careful caching  to the point that they all become identically good. specifically  we show that the new algorithm adaptive caching  aoc i    inspired by the recently proposed and/or cutset conditioning  mateescu and dechter  1a   improving cutset  and w-cutset schemes   can simulate any execution of alternating elimination and conditioning  if the latter is augmented with and/or search over the conditioning variables  and can also simulate any execution of separator controlled treeclustering schemes  dechter and fattah  1   if the clusters are augmented with and/or cutset search  rather than regular search  as was initially proposed.
　all the analysis is done assuming that the problem contains no determinism. when the problem has determinism all these schemes become incomparable  as was shown in  mateescu and dechter  1b   because they exploit the deterministic information in reversed ordering of variables.
1 preliminaries
this section provides the basic definitions.
definition 1  graphical model  a graphical model is a 1tuple is a set of variables; d = {d1 ... dn} is the set of their finite domains of values; f = {f1 ... fr} is a set of real-valued functions defined on variables from x.
definition 1  primal graph  the primal graph of a graphical model is an undirected graph  g =  x e   that has variables as its vertices and an edge connecting any two variables that appear in the scope  set of arguments  of the same function.
definition 1  pseudo tree  a pseudo tree of a graph g =  x e  is a rooted tree t having the same set of nodes x  such that every arc in e is a back-arc in t  i.e.  it connects nodes on the same path from root .
definition 1  induced graph  induced width  treewidth 
an ordered graph is a pair  g d   where g is an undirected graph  and d =  x1 ... xn  is an ordering of the nodes. the width of a node in an ordered graph is the number of neighbors that precede it in the ordering. the width of an ordering d  denoted by w d   is the maximum width over all nodes. the induced width of an ordered graph  w  d   is the width of the induced ordered graph obtained as follows: for each node  from last to first in d  its preceding neighbors are connected in a clique. the induced width of a graph  w   is the minimal induced width over all orderings. the induced width is also equal to the treewidth of a graph.
1 description of algorithms
in this section we describe the three algorithms that will be compared. they are all parameterized memory intensive algorithms that need to use space in order to achieve the worst case time complexity of o n kw    where k bounds domain size  and w  is the treewidth of the primal graph. the task that we consider is one that is #p-hard  e.g.  belief updating in bayesian networks  counting solutions in sat or constraint networks . we also assume that the model has no determinism  i.e.  all tuples have a strictly positive probability .
　the algorithms we discuss work by processing variables either by elimination or by conditioning. these operations have an impact on the primal graph of the problem. when a variable is eliminated  it is removed from the graph along with its incident edges  and its neighbors are connected in a clique. when it is conditioned  it is simply removed from the graph along with its incident edges.
　the algorithms we discuss typically depend on a variable ordering d =  x1 ... xn . search proceeds by instantiating variables from x1 to xn  while variable elimination processes the variables backwards  from xn to x1. given a graph g and an ordering d  an elimination tree  denoted by t  g d   is uniquely defined by the variable elimination process. t  g d  is also a valid pseudo tree to drive the and/or search. note however that several orderings can give rise to the same elimination tree.
1 and/or search space
the and/or search space is a recently introduced  dechter and mateescu  1; mateescu and dechter  1b; 1a  unifying framework for advanced algorithmic schemes for graphical models. its main virtue consists in exploiting independencies between variables during search  which can provide exponential speedups over traditional search methods oblivious to problem structure.
　given a graphical model m   its primal graph g and a pseudo tree t of g  see figure 1 for an example of a pseudo tree   the associated and/or search tree has alternating levels of or and and nodes  see figure 1 for an example of or and and nodes; however  the figure shows a search graph  not a tree . the or nodes are labeled xi and correspond to the variables. the and nodes are labeled
  or simply xi  and correspond to the value assignments in the domains of the variables. the structure of the and/or search tree is based on the underlying pseudo tree t . the root of the and/or search tree is an or node labeled with the root of t . the children of an or node xi are and nodes labeled with assignments. the children of an and node are or nodes labeled with the children of variable xi in the pseudo tree t .
　the and/or search tree can be traversed by a depth first search algorithm  thus using linear space. it was already shown  freuder and quinn  1; bayardo and miranker  1; darwiche  1; mateescu and dechter  1a  that:
theorem 1 given a graphical model m and a pseudo tree t of depth m  the size of the and/or search tree based on t is o n km   where k bounds the domains of variables. a graphical model of treewidth w  has a pseudo tree of depth at most w  logn  therefore it has an and/or search tree of size o n kw  logn .
　the and/or search tree may contain nodes that root identical conditioned subproblems. these nodes are said to be unifiable. when unifiable nodes are merged  the search space becomes a graph. its size becomes smaller at the expense of using additional memory by the search algorithm. the depth first search algorithm can therefore be modified to cache previously computed results  and retrieve them when the same nodes are encountered again. some unifiable nodes can be identified based on their contexts  darwiche  1 . we can define graph based contexts for both or nodes and and nodes  just by expressing the set of ancestor variables in t whose assignment would completely determine the conditioned subproblem. however  it can be shown that using caching based on or contexts makes caching based on and contexts redundant  so we will only use or caching.
definition 1  or context  given a pseudo tree t of an and/or search space  the context of an or node x  denoted by context x  =  x1 ...xk   is the set of ancestors of x in t ordered descendingly  that are connected in the primal graph to x or to descendants of x.
　it is easy to verify that the context of x d-separates  pearl  1  the subproblem below x from the rest of the network. the context minimal and/or graph is obtained by merging all the context unifiable or nodes. an example will appear later in figure 1. it was already shown that  bayardo and miranker  1; dechter and mateescu  1 :
theorem 1 given a graphical model m  its primal graph g and a pseudo tree t   the size of the context minimal and/or search graph based on t is o n kwt   g    where wt   g  is the induced width of g over the depth first traversal of t   and k bounds the domain size.
1 and/or cutset conditioning - aocutset i 
and/or cutset conditioning  aocutset i    mateescu and dechter  1a  is a search algorithm that combines and/or search spaces with cutset conditioning. the conditioning  cutset  variables form a start pseudo tree. the remaining variables  not belonging to the cutset   have bounded conditioned context size that can fit in memory.
definition 1  start pseudo tree  given a primal graph g and a pseudo tree t of g  a start pseudo tree tstart is a connected subgraph of t that contains the root of t .
　algorithm aocutset i  depends on a parameter i that bounds the maximum size of a context that can fit in memory. given a graphical model and a pseudo tree t   we first find a start pseudo tree tstart such that the context of any node not in tstart contains at most i variables that are not in tstart. this can be done by starting with the root of t and then including as many descendants as necessary in the start pseudo tree until the previous condition is met. tstart now forms the cutset  and when its variables are instantiated  the remaining conditionedsubproblemhas induced width boundedby i. the cutset variables can be explored by linear space  no caching  and/or search  and the remaining variables by using full caching  of size bounded by i. the cache tables need to be deleted and reallocated for each new conditioned subproblem  i.e.  each new instantiation of the cutset variables .
1 algorithm aoc i  - adaptive caching
the cutset principle inspires a new algorithm  based on a more refined caching scheme for and/or search  which we call adaptive caching - aoc i   in the sense that it adapts to the available memory   that caches some values even at nodes with contexts greater than the bound i that defines the memory limit. lets assume that context x  =  x1 ...xk  and k   i. during search  when variables x1 ... xk i are instantiated  they can be regarded as part of a cutset. the problem rooted by xk i+1 can be solved in isolation  like a subproblem in the cutset scheme  after variables x1 ... xk i are assigned their current values in all the functions. in this subproblem  context x  =  xk i+1 ...xk   so it can be cached within space bounded by i. however  when the search retracts to xk i or above  the cache table for x needs to be deleted and will be reallocated when a new subproblem rooted at xk i+1 is solved.
definition 1  i-context  flag  given a graphical model  a pseudo tree t   a variable x and context x  =  x1 ...xk   the i-context of x is:
	 	if	i   k
xi is called the flag of i-context x .

 algorithm aoc i  input :
output: updated belief for x1
1 let t = t  g d  // create elimination tree for each x （ x do
1 allocate a table for i-context x 
1 initialize search with root of t ;
1 while search not finished do
1 pick next successor not yet visited	// expand;
1 purge cache tables that are not valid;
1 if value in cache then
1 retrieve value; mark successors as visited;
1 while all successors visited do	// propagate
1 save value in cache;
1 propagate value to parent;

　the high level pseudocode for aoc i  is given here. the algorithm works similar to and/or search based on full

figure 1: context minimal graph  full caching 
   
  - a 
  - ab 
 a - bc 
 a - bd 
 b - de 
 b - df 
  - bg 
figure 1: aoc 1  graph  adaptive caching 
   
 a 
 ab 
 ab - c 
 ab - d 
 b - de 
 b - df 
 b - g 
figure 1: aocutset 1  graph  and/or cutset 
context. the difference is in the management of cache tables. whenever a variable x is instantiated  when an and node is reached   the cache table is purged  reinitialized with a neutral value  for any variable y such that x is the flag of i-context y    line 1 . otherwise  the search proceeds as usual  retrieving values from cache if possible  line 1  or else continuing to expand  and propagatingthe values up when the search is completed for subproblem below  line 1 . we do not detail here the alternation of or and and type of nodes.
example 1 we will clarify here the distinction between and/or with full caching  and/or cutset and and/or
adaptive caching. we should note that the scope of a cache table is always a subset of the variables on the current path in the pseudo tree. therefore  the caching method  e.g.  full caching based on context  cutset conditioning cache  adaptive caching  is an orthogonal issue to that of the search space decomposition. we will show an example based on an or search space  pseudo tree is a chain   and the results will carry over to the and/or search space.
　figure 1 shows a pseudo tree  with binary valued variables  the context for each variable  and the context minimal graph. if we assume the bound i = 1  some of the cache tables don't fit in memory. we could in this case use aocutset 1   shown in figure 1  that takes more time  but can execute in the bounded memory. the cutset in this case is made of variables a and b  and we see four conditioned subproblems  the four columns  that are solved independentlyfrom one another  there is no sharing of subgraphs . figure 1 shows aoc 1   which falls between the previous two. it uses bounded memory  takes more time than full caching  as expected   but less time than aocutset 1   because the graph is smaller . this can be achieved because adaptive caching allows the sharing of subgraphs. note that the cache table of h has the scope  bg   which allows merging.
1 variableeliminationandconditioning-vec i 
variable elimination and conditioning  vec   rish and dechter  1; larrosa and dechter  1  is an algorithm that combinesthe virtues of both inferenceand search. one of its remarkably successful applications is the genetic linkage analysis software superlink  fishelson and geiger  1 . vec works by interleaving elimination and conditioning of variables. typically  given an ordering  it prefers the elimination of a variable whenever possible  and switches to conditioning whenever space limitations require it  and continues in the same manner until all variables have been processed. we say that the conditioning variables form a conditioning set  or cutset  this can be regarded as a w-cutset  where the w defines the induced width of the problems that can be handled by elimination . the pseudocode for the vanilla version  called vec-or i  because the cutset is explored by or search rather than and/or  is shown below:

algorithm vec-or i 

output: updated belief for x1
1 if  context xn  ＋ i  then
1 eliminate xi;
1 call vec-or i  on reduced problem
1 else for each xn （ dn do
1 assign xn = xn;
1 call vec-or i  on the conditioned subproblem

　when there are no conditioning variables  vec becomes the well known variable elimination  ve  algorithm. in this case aoc also becomes the usual and/or graph search  ao   and it was shown  mateescu and dechter  1b  that:
theorem 1  ve and ao are identical  given a graphical model with no determinism and a pseudo tree  ve traverses the full context minimal graph bottom-up by layers  breadth

figure 1: primal graph and pseudo tree
first   while ao is a top-down depth-first search that explores  and records  the full context minimal graph as well.
1 tree decomposition with conditioning - tdc
one of the most widely used methods of processing graphical models  especially belief networks  is tree clustering  also known as join tree or junction tree algorithm  lauritzen and spiegelhalter  1  . the work in  dechter and fattah  1  presents an algorithm called directional join tree clustering  that corresponds to an inward pass of messages towards a root in the regular tree clustering algorithm. if space is not sufficient for the separators in the tree decomposition  then  dechter and fattah  1  proposes the use of secondary join trees  which simply combine any two neighboring clusters whose separator is too big to be stored. the resulting algorithm that uses less memory at the expense of more time is called space based join tree clustering 
　the computation in each cluster can be done by any suitable method. the obvious one would be to simply enumerate all the instantiations of the cluster  which corresponds to an or search over the variables of the cluster. a more advanced method advocated by  dechter and fattah  1  is the use of cycle cutset inside each cluster. we can improve the cycle cutset scheme first by using an and/or search space  and second by using adaptive caching bounded by i  rather than simple and/or cutset in each cluster. we call the resulting method tree decomposition with conditioning  tdc i  .
1 aoc i  compared to vec i 
we will begin by following an example. consider the graphical model given in figure 1a having binary variables  the ordering d1 =  a b e j r h l n o k d p c m f g   and the space limitation i = 1. the pseudo tree corresponding to this ordering is given in figure 1b. the context of each node is shown in square brackets.
　if we apply vec along d1  eliminating from last to first   variables g  f and m can be eliminated. however  c cannot be eliminated  because it would produce a function with scope equal to its context   abehlkdp   violating the
   
 l 
 ln 
 abe  lno 
 abj  ej  ok 
 abjr 
figure 1: components after conditioning on c
bound i = 1. vec switches to conditioning on c and all the functionsthat remain to be processed are modified accordingly  by instantiating c. the primal graph has two connected components now  shown in figure 1. notice that the pseudo trees are based on this new graph  and their shape changes from the original pseudo tree.
　continuing with the ordering  p and d can be eliminated  one variable from each component   but then k cannot be eliminated. after conditioning on k  variables o  n and l can be eliminated  all from the same component   then h is conditioned  from the other component  and the rest of the variables are eliminated. to highlight the conditioning set  we will box its variables when writing the ordering  d1 =  a b e j r  h l n o  k d p  c m f g .
　if we take the conditioning set  hkc  in the order imposed on it by d1  reverse it and put it at the beginning of the ordering d1  then we obtain: d1=c k h a b e j rh  l n o  d p	 m f g 
c
where the indexed squared brackets together with the underlines represent subproblems that need to be solved multiple times  for each instantiation of the index variable.
　so we started with d1 and bound i = 1  then we identified the corresponding conditioning set  hkc  for vec  and from this we arrived at d1. we are now going to use d1 to build the pseudo tree that guides aoc 1   given in figure 1. the outer box corresponds to the conditioning of c. the inner boxes correspond to conditioning on k and h  respectively. the context of each node is given in square brackets  and the 1-context is on the right side of the dash. for example  context j  =  ch-ae   and 1-context j  =  ae . the context minimal graph corresponding to the execution of aoc 1  is shown in figure 1.
　we can follow the execution of both aoc and vec along this context minimal graph. after conditioning on c  vec solves two subproblems  one for each value of c   which are the ones shown on the large rectangles. the vanilla version vec-or is less efficient than aoc  because it uses an or search over the cutset variables  rather than and/or. in our example  the subproblem on a b e j r would be solved eight times by vec-or  once for each instantiation of c  k and h  rather than four times. it is now easy to make the first improvement to vec  so that it uses an and/or search over the conditioning variables  an algorithm we call vec-ao i   by changing line 1 of vec-or to:

figure 1: pseudo tree for aoc 1 

algorithm vec-ao i 

...
1 call vec-ao i  on each connected
component of conditioned subproblem separately;

　let's look at one more condition that needs to be satisfied for the two algorithms to be identical. if we change the ordering to d1 =  a b e j r  h l n o  k d p f g  c-
 m    f and g are eliminated after conditioning on c   then the pseudo tree is the same as before  and the context minimal graph for aoc is still the one shown in figure 1. however  vec-ao would require more effort  because the elimination of g and f is done twice now  once for each instantiation of c   rather than once as was for ordering d1. this shortcoming can be eliminated by defining a pseudo tree based version for vec  rather than one based on an ordering. the final algorithm  vec i  is given below  where ng xi  is the set of neighbors of xi in the graph g . note that the guiding pseudo tree is regenerated after each conditioning.

 algorithm vec i  input :
output: updated belief for x1
let t = t  g d  // create elimination tree ; while t not empty do
if    xi leaf in t  … |ng xi |＋i   then eliminate xi else pick xi leaf from t ; for each xi （ di do
assign xi = xi; call vec i  on each connected component of conditioned subproblem
break;

based on the previous example  we can prove:
theorem 1  aoc i  simulates vec i   given a graphical model with no determinism and an execution of vec i   there exists a pseudo tree that guides an execution of aoc i  that traverses the same context minimal graph.
proof: the pseudo tree of aoc i  is obtained by reversing the conditioning set of vec i  and placing it at the beginning of the ordering. the proof is by induction on the number of conditioning variables  by comparing the corresponding contexts of each variable.

figure 1: context minimal graph　basis step. if there is no conditioning variable  theorem 1 applies. if there is only one conditioning variable. given the ordering d =  x1 ... xj ... xn   let's say xj is the conditioning variable.
　a  consider x （ {xj+1 ... xn}. the function recorded by vec i  when eliminating x has the scope equal to the context of x in aoc i .
　b  for xj  both vec i  and aoc i  will enumerate its
domain  thus making the same effort.
　c  after xj is instantiated by vec i   the reduced subproblem  which may contain multiple connected components  can be solved by variable elimination alone. by theorem 1  variable elimination on this portion is identical to and/or search with full caching  which is exactly vec i  on the reduced subproblem.
　from a   b  and c  it follows that vec i  and aoc i  are identical if there is only one conditioning variable.
　inductive step. we assume that vec i  and aoc i  are identical for any graphical model if there are at most k conditioning variables  and have to prove that the same is true for k + 1 conditioning variables.
　if the ordering is d =  x1 ... xj ... xn  and xj is the last conditioning variable in the ordering  it follows  similar to the basis step  that vec i  and aoc i  traverse the same search space with respect to variables {xj+1 ... xn}  and also for xj. the remaining conditionedsubproblemnow falls under the inductive hypothesis  which concludes the proof. note that it is essential that vec i  uses and/or over cutset  and is pseudo tree based  otherwise aoc i  is better. 
1 aoc i  compared to tdc i 
we will look again at the example from figures 1 and 1  and the ordering d1. it is well known that a tree decomposition

figure 1: tree decompositions: a  for d1; b  maximal cliques only; c  secondary tree for i = 1 corresponding to d1 can be obtained by inducing the graph along d1  from last to first   and then picking as clusters each node together with its parents in the induced graph  and connecting each cluster to that of its latest  in the ordering  induced parent. because the induced parent set is equal to the context of a node  the method aboveis equivalentto creating a cluster for each node in the pseudo tree from figure 1  and labeling it with the variable and its context. the result is shown in figure 1a. a better way to build a tree decomposition is to pick only the maximal cliques in the induced graph  and this is equivalentto collapsing neighboringsubsumed clusters from figure 1a  resulting in the tree decomposition in figure 1b. if we want to run tdc with boundi = 1  some of the separators are bigger than 1  so a secondary tree is obtained by merging clusters adjacent to large separators  obtaining the tree in figure 1c. tdc 1  now runs by sending messages upwards  toward the root. its execution  when augmented with and/or cutset in each cluster  can also be followed on the context minimal graph in figure 1. the separators  af    ar  and  cd  correspond to the contexts of g  f and m. the root cluster  chabejdr  corresponds to the part of the context minimal graph that contains all these variables. if this cluster would be processed by enumeration  or search   it would result in a tree with 1 = 1 leaves. however  when explored by and/or search with adaptive caching the context minimal graph of the cluster is much smaller  as can be seen in figure 1. by comparing the underlying context minimal graphs  it can be shown that:
theorem 1 given a graphical model with no determinism  and an execution of tdc i   there exists a pseudo tree that guides an execution of aoc i  that traverses the same context minimal graph.
proof: algorithm tdc i  is already designed to be an improvementoverspace based join tree clustering  section 1   in that it uses and/or adaptive caching  rather than cutset conditioning  inside each cluster to compute the messages that are sent. tdc i  is based on a rooted tree decomposition  which can serve as the skeleton for the underlying pseudo tree. each cluster has its own pseudo tree to guide the and/or adaptive caching. each message sent between neighboring clusters couples all the variables in its scope  therefore the scope variables have to appear on a chain at the top of the pseudo tree of the cluster where the message is generated  and also they have to appear on a chain in the pseudo tree of the cluster where the message is received. this implies that two neighboring clusters can agree on an ordering of the common variables  since we assume no determinism  the order of variables on a chain doesn't change the size of the context minimal graph . all this allows us to build a common pseudo tree for two neighboring clusters  by superimposing the common variables in their respective pseudo trees  which are the variables in the separator between the clusters . in this way we can build the pseudo tree for the entire problem. now  for any variable xi in the problem pseudo tree  i-context xi  is the same as it was in the highest cluster  closest to the root of the tree decomposition  where xi is mentioned. from this  it follows that aoc i  based on the pseudo tree for the entire problem traverses the same context minimal graph as tdc i . 
1 conclusion
we have compared three parameterized algorithmic schemes for graphical models that can accommodate time-space tradeoffs. they have all emerged from seemingly different principles: aoc i  is search based  tdc i  is inference based and vec i  combines search and inference.
　we show that if the graphical models contain no determinism  aoc i  can have a smaller time complexity than the vanilla versions of both vec i  and tdc i . this is due to a more efficient exploitation of the graphical structure of the problem through and/or search  and the adaptive caching scheme that benefits from the cutset principle. these ideas can be used to enhance vec i  and tdc i . we show that if vec i  uses and/or search over the conditioning set and is guided by the pseudo tree data structure  then there exists an execution of aoc i  that is identical to it. we also show that if tdc i  processes clusters by and/or search with adaptive caching  then there exists an execution of aoc i  identical to it. and/or search with adaptive caching  aoc i   emerges therefore as a unifying scheme  never worse than the other two. all the analysis was done by using the context minimal data structure  which provides a powerful methodology for comparing the algorithms.
　when the graphical model contains determinism  all the above schemes become incomparable. this is due to the fact that they process variables in reverse orderings  and will encounter and exploit deterministic information differently.
acknowledgments
research supported in part by the nsf grant iis-1.
