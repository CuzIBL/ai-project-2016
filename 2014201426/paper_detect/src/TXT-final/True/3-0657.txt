
we presenta transcriptionsystem that takes a music signal as input and returns its musical score. two stages of processing are used. the first employs a fundamental frequency detector and an onset detector to transform input signals into a sequence of sound events. the onset detection is inherently noisy. this paper focuses on the second stage  going from sound events to a notated score. we use a family of graphical models for this task. we allow the results of onset detection to be noisy  necessitating a search over possible segmentations of the sound events. we use a large corpusof monophonic vocal music to evaluate our system. our results show that our approach is well-suited to the problem of music transcription. the initial onset detection reduces the number of observations and makes the system less instrumentspecific. the search over segmentations corrects the errors in the onset detection. without such reasoning  these errors are magnified in subsequent rhythm transcription.
1 introduction
music transcription is the task of transforming a music signal into a symbolic representation of the musical events it contains. there are three levels of representation of music that are relevant here. on the third level is the musical score in common music notation. this consists of musical events with attributes notated pitch  such as  a  in octave 1   notated duration  such as quarter note   and notated rhythmic position  such as the first beat of measure 1 . on the second level is the representation of performance in terms of sound events. this is the level of midi data. the rhythmic attributes of events on this level are the actual onset time  such as 1 seconds from the beginning of the piece  and the actual duration  such as 1 seconds . the pitch attribute on this level could be discrete  such as midi note 1  or continuous  such as 1 . on the first level is the representation of the sound signal. it is a common practice to transform the signal into a sequence of frames  through short-time fourier transform   which are snapshots of the signal over a short period of time  e.g. 1 msecs . a frame captures the composition of the sound by specifying the amplitude for every frequency.
　the third level is the target representation of transcription. previous authors  raphael  1b; cemgil and kappen  1  have proposed using graphical models for automated music transcription  but their systems take midi data as input and only go from the second to the third level. our goal is to build an end-to-end transcription system that goes all the way from a first level audio signal to a third level musical score. in this work we focus on monophonic transcription from a complex instrument such as voice. monophonic transcription is a difficult enough task  and well worthy of study. it raises many issues that need to be addressed before polyphonic transcription can be attempted. going from level 1 to 1 is a much more challenging task than going from level 1 to 1. in going from level 1 to 1  there is a one-to-one mapping between musical events. furthermore  the input is often midi data where the pitch is given and does not need to be detected. therefore the only work that needs to be done is to track the tempo of the music  thereby transforming actual onset times and durations into notated rhythmic positions and durations.
　going from level 1 directly to level 1 is too hard. frames are too small a unit and even short music signals will be represented by very long sequences of frames. each musical event consists of many frames that need to be grouped together. furthermore  the information in a frame needs to be summarized. instead of a frequency-amplitudespectrum  we need to know the fundamental frequency of a given frame. therefore we use a preprocessing step in which we go from level 1 to level 1. this step consists of a fundamental frequency detector  which determines the fundamental frequency sounding in each frame  and an onset detector  which detects the beginnings of musical events from the frame sequence. once a musical event has been identified  an overall pitch for the event can be deduced by analyzing all the frames constituting the event. we have previously built an onset detector capable of handling the soft onsets prevalent in vocal music  in which there is a smooth pitch change  or a vowel change  kapanci and pfeffer  1 . while achieving state-of-the-art performance  it is not completely reliable  and no perfect onset detector exists. it might miss some onsets  causing it to merge musical events  while reportingother spuriousonsets  causing it to separate events. furthermore  the pitch data produced by the onset detector is continuous-valued and noisy.
　this paper focuses on the transformation from level 1 to 1 when the level 1 input is noisy. we do not assume a one-toone mapping between events at the two levels. instead  we allow for the possibility that reported onsets are false positives. the onset detector can be tuned to trade off the rate of false positives with false negatives. we use a setting with a very low false negative rate  so that most of the errors in onset detection are false positives. we also detect the pitch  allowing the tuning of the performance to change over time. so we need to simultaneously  1  detect false positives in the segmentation   1  do tempo tracking and rhythm transcription  and  1  do tuning tracking and pitch transcription.
　we propose a family of graphical models to accomplish these tasks. in the models  the properties of sound events depend probabilistically on the properties of notated events to which they are related. because there may be false onsets  several sound events may actually be related to the same notated event. the segmentationdetermines which soundevents are grouped together into a single note event  and therefore it determines the structure of the graphical model. we have a different graphical model corresponding to each possible segmentation. we use markov chain monte carlo  mcmc  sampling to search over the different structures and parameters.
　we evaluate our model using a corpus of monophonic vocal music. our results show that in the absence of perfect onset detectors  reasoning about segmentation is necessary for a signal-to-score system. without such reasoning  the errors in the segmentation are magnified in subsequent rhythm transcription. we also show that the transcription benefits from musical knowledge  which is easily incorporated into our graphical model as prior distributions over the scores.
1 problem statement
the input of our system is a music signal  and the output is a musical score. the music signal is first processed using an onset detector to yield the sound event observations for the graphical model. each sound observation consists of an observed duration and a frequency . the task is to find a segmentation and a score . each score event consists of a notated duration and a pitch
　. the segmentation variable is the number of observed events related to the th score event. thus the segmentation determines the mapping between score events and observed events. is the number of observations  fixed   while is the number of note events  dependent on the segmentation .
　the goal is to find the segmentation and the score that maximize the posterior probability of the segmentation and score given the observed data:
 1 
assuming independence of the prior distributions over score and segmentation. the graphical model detailed in section
1 defines . we have a style specific score prior   and a segmentation prior based on the onset detector's outputs. the output of the system is the score and the mapping of observations to the score events . we evaluate it using a metric presented in section 1.
　for our onset detector  we use our previously published hierarchical method that detects onsets in a signal by comparing frames one to another  kapanci and pfeffer  1 . a comparison function based on the fundamental frequencies and amplitudes returns the dissimilarity of two frames. if this value is abovea threshold  an onset is declared overthe region delimited by the two frames. the system begins by comparing adjacent frames  and then gradually increases the distance between the compared frames. by comparing frames some distance apart  the detector is able to detect soft onsets that happen smoothly over time.
　to reduce the number of missed onsets  we use a low dissimilarity threshold. that way  we get many extraneous onsets  false positives  which need to be eliminated in later stages  but missed onsets  false negatives  are infrequent. in our experiments  1% of the onsets were extraneous  while only 1% of the onsets were missed.
　once the onsets are marked  the signal is segmented by cutting it at these onsets. a continuous pitch attribute is computed for each event by the average fundamental frequency. the comparison value for each detected onset is also stored to be used in assigning a prior segmentation probability to each observation   as described in the next section.
1 model
the likelihood of a sequence of observed sound events is given by a family of graphical models. due to the possibility of false onsets  there are many possible groupings of sound events  observations  into note events. the segmentation determines which sound events are grouped together  and therefore it determines the structure of the graphical model. for each segmentation   the likelihood model defines the probability of observations given
                 . the probabilistic dependency of sound events on the notated events is the same in different structures. the difference is in the connections between the two layers as illustrated in figures 1 and 1: the leaf nodes correspond to the observations  and the segmentation determines the structure of the graph above them. the mapping between observation indexes   s  and note event indexes   s  is given by	  where	.
　the tempo and tuning are continuous processes changing smoothly over time: we model both as first-order gaussian processes  equations 1 and 1 . the tempo determines how notated durations become observeddurations: the notated duration is multiplied by the tempo to produce the observed duration. in addition  we model the fact that durations will deviate without reflecting a change in tempo. this is accomplished  first of all  by including a gaussian noise in the model for the observed duration  equation 1 .
　we further model the fact that as the tempo remains constant  successive deviations are likely to compensate for each other  so that a note that sounds for shorter than its written duration will be followed by a note that sounds for longer  and vice versa. this may be the result of both intentional and unintentional deviations. it is common in various styles to intentionallyproducethis effect. for example  in swing rhythm  given two successive eighth notes  the first is lengthened and

figure 1: rhythm model

figure 1: pitch model
the second is shortened. as another example  double-dotting is a common effect in baroque music. here a dotted note is lengthened and the subsequent note is shortened. this compensation model is also a natural model of unintentional deviation. the performer is likely to maintain a constant beat  so that the notes that subdivide a beat have a given amount of time to fill. as a result  shorter notes will be compensated for by longer notes to fill up the required time. we capture this effect by including a first-order stealing process  equation 1   which allows successive durations to deviate in opposite directions without affecting the overall tempo. this is accomplished by having a negative value for the stealing coefficient . the value was used in our experiments. given the note duration s  the observed durations depend linearly on the tempo and stealing processes  equation 1 .
　for the pitch process  the tuning is an additive offset that is added to the notated pitch to produce the observed frequency  equation 1 . in addition  we allow the frequency to deviate independent of the tuning  so equation 1 includes a gaussian noise term . the noise terms in equations 1 and 1 model observational errors as well as local deviations.
	tempo:	for	 1 
	stealing:	for	 1 
	duration:	 1 
	tuning:	for	 1 
frequency:	s.t.	 1 
we assume that	is	 	is	  is	 	is	 	is	 	is
　　　　 	is	and	is	  all independent of each other. the variance values used in the experiments were:	 	 	 
	 	 	 	and
.
　note that we don't observe directly  but only through the durations of the individual sound events  where
　　　　　. on the other hand  the model directly specifies the observed frequency of the individual sound events.
	we define the rhythm prior	by a hidden
markov model  hmm   learned offline using rhythm sequences of a corpus of music in the same style as our evaluation data. for the pitch prior we use an -gram model  with =1   which assumes that each element in a sequence only depends on the previous elements. we define the prior over intervals rather than individual pitches since this allows for generalization to different keys. the probabilities in the model are estimated offline by the -gram frequencies in the training data.
　the segmentation prior indicates  for each observed sound event  the probability that there is truly an onset immediately after the event. to obtain the segmentation prior  we use the dissimilarity values returned by the onset detector at each observation. the goal is to individually estimate onset   the probability that there is an actual onset right after the th observation  given its dissimilarity to the following observation. although not a formal density estimation method  sigmoid fitting provides an appropriate solution  platt  1 . this was suggested to transform a classifier's output values into probabilities. using a training corpus for onset detection  we learned the following function:	. the shape of the function makes sense: as the dissimilarity increases  it assigns increasing probabilities in  1  to the presence of an onset. in order to avoid probabilities close to 1 and 1  we used a weighted average of and as our prior: onset	.
　once individual segmentation priors of all observations are computed  the prior probability of a segmentation is computed by multiplying these together:
	onset	onset
1 inference
our goal is to compute the maximum a posteriori  map  segmentation-score configuration as given in equation 1. let be the set of possible note durations and the set of possible pitches. for a given observation sequence   there are possible segmentations and possible scores for each segmentation with note events. the exponential number of the possible pairs makes exact computation of the map configuration intractable. we therefore use mcmc methods  gilks et al.  1  to sample from the space of possible pairs.
　our inference procedure is similar to the handling of reference uncertainty in first-order probabilistic models  pasula and russell  1 . we alternate between mcmc steps to change the segmentation  and gibbs steps to sample the duration and pitch for a given segmentation. when changing the segmentation  reversible jump mcmc  green  1  is employed since the model dimension changes whenever we split or merge events.
1 reversible jump mcmc
mcmc methods are used to generate samples from a complex target distribution where analytical or numerical techniques are not applicable. in our case  the target is the posterior distribution of the transcription given the observations 
                   and we are trying to find the mode of this distribution. there are well-known methods to construct a markovchain whose stationary distributionis guaranteed to be the target distribution. in the metropolis-hastings algorithm  the candidate samples are generated from a proposal distribution and are accepted with probability
	 .	this is not applicable
when does not have a fixed dimensionality  such as in our problem: different segmentations may correspond to different numbers of notes  and thereforehave different numbers of parameters. the reversible jump mcmc method allows us to sample from a target distribution even when the dimensionality of the model is not fixed. the acceptance probability of moving from one model to another model of different dimension is given by:
		 1 
where and are model indicators that determinethe number of notes  and are the parameter vectors of sizes and respectively  is the prior probabilityof model   and is the probability of moving from model to . the function is a mapping between the parameters of the two models. this mapping may entail random decisions to generate the parameters of given the current parameters of . these decisions are denoted by the variable   whose probability distribution is .
　since we assume that the segmentation  rhythm and pitch priors are independent  for the first two terms of the fraction in equation 1 we have:
 1 
the terms in the right hand side of equation 1 are respectively given by the segmentation prior  the rhythm prior  the pitch prior and the likelihood model.
　during our sampling  we allow two kinds of moves that change the model dimension: merging two neighboring notes together  or splitting a single note into two. the rhythm and pitch components are sampled using gibbs sampling without a change in model dimension  so we don't discuss the acceptance probability for those here and instead focus on the segmentation component of . we have two possible values for : it is equal to split when and to merge when . the two functions merge and split define the mappings between the models before and after  where the move is a merge and a split respectively:
merge	 1  split	 1 
in equation 1  because we choose the splitting point randomly from all possible boundaries between the observations currently assigned to a note  is a random variable with uniform density split . since merging is a deterministic move  we don't have a random variable in equation 1 so we simply have merge . finally  it's easy to see from
equations 1 and 1 that both	merge	and	split	are equal to 1.
　putting everything together  we have the following two acceptance probabilities for merge and split moves:
split
	merge	
merge
 1 
merge
	split	
split
 1 
where is either the total number of observations associated with the two notes we are merging  or it is the number of observations associated with the note we are splitting.
1 generating samples
given a sample   new samples are generated as follows:
　1. pick an index	.
　1. resample the th note  keeping the segmentation and all other notes the same:
	sample a value for	from	.
　let . it is accepted automatically as a sample since it was generated by gibbs sampling.
　1. change segmentation in one of two ways:
　a. with probability merge  merge the th note with its successor:
	pick a value for	using gibbs sampling.
　b. with probability split merge  split the th note into two at   an integer chosen uniformly at random from  only possible if  .
	jointly sample values for	and	from
.
　1. let . it is a candidate sample that has a different segmentation than . the acceptance probability of as the next sample is given by one of equations 1 and 1  decided by whether it corresponds to a merge or a split move.
　in step 1 above  although we could pick a random   processing the events sequentially     allows computational savings. we can factorize the likelihood as:
 1 
where denotes the hidden variables     and . the first factor is computed incrementally as new samples are taken for to . the second factor depends only on the future observations  and can be efficiently computed in advance for all s by a backward filtering pass.
　given   the relationships between the continuous variables of our model are linear  and the additive change or noise variables are all gaussian. so  it is a linear dynamical system where the integration over the hidden variables  in equation 1  can be computed analytically using kalman filtering. all the necessary probabilities can be represented by gaussian potentials  over which multiplication  conditioning and marginalization are three basic operations.
　a problem with this individual sampling of s is that sometimes the neighboring notes will be such that it is very unlikely to sample a good value for either one with the current assignment of the other. to overcomesuch deadlocks we randomly sample some score pairs jointly in step
1. steps 1 and 1 are not performed for joint samples. ideally  one would like to sample all events jointly given a segmentation  but this would result in an exponential sampling space.
　we define an mcmc iteration as one sequence of samplings from to . after some iterations  we randomly propose a global sampling step. we have two global operations: one that shifts all the pitches in the sample by   and one that halves or doubles all durations. if the result of halving produces a duration not in   we randomly select the closest element in . when a zero duration is assigned to an observation  it is grouped with one of its neighbors. in the experiments we used 1 chains with 1 iterations per chain. each chain starts with an initial sample that is obtained by matching the observations as closely as possible: if the segmentation prior of an observation is less than 1  we group it with its successor. the durations are rounded to the closest duration in   and the frequencies are rounded to the closest pitch in . starting with this sample  as opposed to a random one  allows us to get to a good transcription faster.
1 experiments
1 training and evaluation data
a corpus of monophonic vocal music was used to evaluate the system and its various components. the corpus consisted of two parts. one part contained both scores and audio data  and was used to evaluate the system. the test data had a total duration of 1 minutes  composed of 1 audio segments containing a total of 1 notes. all pieces were by the renaissance composer g.p. palestrina  1th century   and sung by the same singer. the second part of the corpus consisted only of scores without audio data. this part was used to learn the rhythm and pitch priors. there were 1 pieces by palestrina in this part  none of which appeared in the first part. two audio fragments were removed from the first part for the purpose of tuning the eight variance parameters of the graphical model and the two sigmoid parameters of the segmentation prior. these were not used in the evaluation.
1 evaluation metric
given the true score	and segmentationfor obser-vations	  we evaluate the system's resultandas follows:
segmentation: we count the number of correct onset/noonset decisions over the number of possible segmentation locations    .
musical score: since the system's segmentation might be different from the reference segmentation  comparing directly to is not very meaningful. we instead evaluate the score by looking at the number of rhythm and pitch errors remaining once the segmentation errors are fixed. for instance if a half note is transcribed as two successive quarter notes of the same pitch  we consider this to be a segmentation error  extra onset   and not a rhythm transcription error. similarly  if two successive quarter notes of the same pitch are transcribed as a half note  this is only a segmentation error  missed onset . if  on the other hand  two successive quarter notes of different pitches are transcribed as a half note  there is a pitch error in addition to the segmentation error. this is because even after correcting the missed onset by splitting the half note in two  our transcription would have a pitch different from the real score.
　returning to the extra onset case  if a half note is transcribed as two separate quarter notes of different pitches  then when the segmentation error is corrected the two quarter notes are replaced by a half note with a single pitch. we need to choose one of the pitches of the two quarter notes as the pitch of the half note. there may or may not be a pitch error in addition to the segmentation error  depending on which pitch is chosen for the half note. we choose to go with the pitch whose related observation is longer in duration. if this is equal to the real pitch  there is no pitch error. this generalizes naturally to cases where a single true note is transcribed as multiple notes.
　for the evaluation of durations  if a note is transcribed as multiple notes  extra onsets   we compare the sum of these durations to the reference duration. if multiple notes are transcribed as a single note  missed onset   we compare the duration of this note to the sum of the reference durations. we can extend this idea to evaluate cases of misplaced onsets  where the computed transcription contains an onset in a different place from the reference transcription. in both the reference and computed transcriptions  we only keep the onsets that appear in both  and compute durations for these new segments by summing the durations of their constituents. these are then compared to evaluate the rhythm transcription.
1 results
we present the results for the evaluation of our system in tables 1 and 1. table 1 contains the results obtained by the sampling process described in section 1. this process generates samples from the posterior distribution of the transcription.
table 1: results
seg.pitchdur.comb.timeall11111all-r11111all-p11111all-sg11111all-st11111all-j11111none11111all-rg11111none-rg11111table 1: results  non-reversible 
seg.pitchdur.comb.timeall11111all-r11111all-p11111all-sg11111all-st11111all-j11111none11111all-rg11111none-rg11111however  we are mainly interested in the mode of this distribution  so we do not really need to ensure that our sampling has the posterior as its stationary distribution. we could employ any search process over the space of transcriptions  and output the one with the highest posterior. since we know that our input is over-segmented  a simple modification is to bias our sampler towards merging segments. we implement this by removing the factors from equations 1 and
1. other approaches such as simulated annealing can also be used to generate samples concentrated around the modes. however  we were able to obtain slightly higher posteriors with the same number of iterations using this simpler modification. the results using the modification are presented in table 1  and we use that table to discuss our results. it is important to keep in mind that both tables 1 and 1 use samples to approximate the map transcription. it is likely that the exact map configurations would have higher posteriors  with possibly higher or lower evaluation scores. however  given the high number of iterations  the values are still representative of the performance of their respective models.
　in both tables 1 and 1  the first three columns contain the success rates for the segmentation  pitch transcription and rhythm quantization  using the metric defined in section 1. the next column contains the harmonic mean1 of these three as a combined metric. the last column gives the average time per iteration  in msecs  for a 1 seconds input signal. the first row  all  refers to the system with all of the priors described in section 1. the next three rows show the results when either prior is removed: rhythm  all-r   pitch  all-p   or segmentation  all-sg . we then present results where we don't include the stealing process given in equation 1  allst   and where we don't allow joint sampling  all-j . the next row contains the results when none of these five components is used  none . the final two rows  all-rg and none-rg  show the results when we don't allow regrouping of observations  so that every observation is a separate segment. this is what we would get if we did not reason about segmentation  and instead assumed that the onset detector is perfect. notice that these two rows are the same in the two tables since the segmentation is fixed.
　the most instructive comparison is between all and allrg. we see that not only does all-rg do significantly worse on segmentation  which is expected  its score for duration is also muchlower. it seems to be the case that errorsin segmentation spill over  causing significant errors in rhythm as well. surprisingly  however  all-rg does a little better on pitch transcription. this may be because it is not subject to missed onset errors  which as we saw earlier always results in a pitch error when two notes of different pitches are transcribed as a single note. comparison of none and none-rg is similar. again  the system that allows regrouping greatly outperforms the one that does not on segmentation and rhythm  but does slightly worse on pitch.
　now we consider the effect of individual components of the model. we see that the rhythm prior greatly improves the performance on duration  while slightly hurting the segmentation and pitch scores. it could be that without a rhythm prior  the segmentation prior and observed pitches have a greater effect on the segmentation  and the pitches can match the observations more closely. overall  however  the small negative effect on pitch and segmentation is well compensated for by the improvement in duration. the pitch prior has only a very small effect. it leads to a slight improvement in pitch  and a slight degrading of the segmentation and duration scores. the segmentation prior and joint sampling lead to an improvement in all three scores. the stealing process has little effect  increasing the pitch score and slightly decreasing the other two scores. it might be more useful in other styles of music.

figure 1: transcription example　in figure 1  we show the transcription results for a short audio segment. the true transcription is given first  followed by the transcriptions returned by each of the systems in table 1. the errors are indicated by the arrows above them. the complete system  all  is able to transcribe the piece with no mistakes. we see that by removing the rhythm prior  all-r   we get many stylistic errors: the notated durations of the first  forth and fifth errors are very unlikely  and they induce an unlikely rhythmic pattern. removing the segmentation prior  all-sg  results in segmentation errors  which are often accompanied by pitch and/or rhythm mistakes. we see this in the second error  where the pitch has been transcribed incorrectly once the note is split into two. the stealing prior provides flexibility to transcribe local expressive or motor deviations correctly. without it  all-st   we are unableto correctly transcribethe first noteat the beginningof the fourthmeasure: its elongated performancerequires a too large deviation in the global tempo to be transcribed correctly. in the bottom two rows of figure 1  we clearly see that the two systems without reasoning about the segmentation have many mistakes  and the quality of transcription is really poor  especially for the last row. even in the all-rg system that includes rhythm and pitch priors  we can see that segmentation errors result in many additional pitch and rhythm errors. so  we clearly see the benefits of reasoning about the segmentation in this example.
1 related work
transcription of music signals to scores is a difficult problem. most related work consists of addressing subproblems such as pitch tracking  onset detection  beat tracking  level 1-to1  audio to midi or piano-roll   and level 1-to-1  midi to score .
　there are a number of related works on the level 1-to-1 problem. most systems transform the signal into a timefrequency representation  followed by heuristic methods to segment the signal and label segments with pitch or chord names  see  klapuri  1  for a more detailed literature review . sterian  1  proposes heuristics to pick peaks from the time-frequency image. he then uses a kalman filter to extract partial tracks and searches over hypotheses for the association of the tracks to notes. marolt  1  presents a connectionist approach: oscillator networks for partial tracking and neural networks for note recognition. raphael  1a  computes a set of features from each frame of the timefrequency image  and takes these as observations from an hmm. cemgil et al.  1 propose a graphical model whose observation is directly the time domain signal.
　beat tracking is another problem related to rhythm transcription. dixon  1  proposes a rule-based system to estimate beats from a sequence of onset times. goto and muraoka  1  take an audio signal as input and use dsp methods to construct and select from multiple beat hypotheses. we consider beats to lie somewhere between levels 1 and 1: given perfect onset times  the beats could be used to assign discrete durations to observed events. however  onset detection accuracy is not crucial for beat trackers. on the contrary  poor onset detection seems to help beat tracking by filtering out less salient onsets. this is clearly undesirable for a transcription system.
　there have been systems employing graphical models for level 1-to-1 problem: given a list of onset times  cemgil and kappen  1  use mcmc methods to sample most likely note durations. raphael  1b  proposes an exact method for the joint estimate of the tempo and note durations. he introduces a  thinning  operation that removes a value from consideration for a discrete variable  if that variable is guaranteed not to have that value in the map configuration. this seems to keep the search space from growing exponentially. however  both of these systems assume that their list of onset times is accurate  so they have a one-to-one mapping between observations and score events. in our case  the number of mappings alone is exponential  which is why we chose approximate methods.
　we believe that exploiting the advances in dsp to get to level 1  even a noisy one   and then using graphical models for level 1-to-1 is appropriate. level 1 data is much more instrument independent  and it is much shorter. however  without perfect onset detectors in sight  level 1 data will inevitably have errors  and our experiments have shown that these errors are magnified in the subsequent analysis to get to level 1. the most important contribution of our system is its ability to accept and reason about noisy input. to the best of our knowledge  it is the first music transcription system to go from level 1 to level 1 without assuming a perfect level 1.
1 discussion and conclusion
we have presented a transcription system that takes a music signal as input and returns its musical score. we use a noisy onset detector as our front-end  which computes the input to a family of graphical models. each regrouping of this input corresponds to a specific model from this family. our inference procedure jointly searches over models and scores that may have produced the observed sound events. our results show that this is a very well-suited approach to the problem of music transcription.
　the constraint on our onset detection noise is that we can have extra onsets but not many missed onsets  since these will be reflected as merged notes in the final score. this is a reasonable requirement  as most detectors can be tuned to reduce missed onset at the cost of increased extra onsets. we can therefore use a variety of onset detectors as our frontend. also  in theory we could apply our system to individual frames  although the inference would take much longer.
　in our models  the pitch and rhythm variables are conditionally independent given the segmentation. this keeps the sampling over the two spaces separate. a more sophisticated model could combine the rhythm and pitch processes as well as the priors. also  the rhythm and pitch priors we learn are style-specific due to the homogeneity of the training data. this makes sense  as a trained musician would transcribe the same audio signal differentlyunderdifferentstylistic assumptions. ultimately  we would like to have rhythm priors for various styles and make the choice of style a variable that needs to be optimized as well.
