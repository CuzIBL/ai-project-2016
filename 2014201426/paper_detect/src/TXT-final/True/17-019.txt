 
arguments are presented in favor of the answer  yes . 
a quantitative approach which overcomes the usual need for a priori probabilities is presented. some of the practical advantages of using probabilities in a production system are described. 
¡ì1. introduction 
     the panel on uncertain reasoning at aaai-1 considered the question of whether or not implementations of non-monotonic reasoning should be probabilistic. a variety of  generally unsupported  claims were made to the effect that probabilities arc unintuitive  that the numbers needed arc unavailable  and that the method generally is inappropriate. the counterclaims that probabilities are intuitive  available and appropriate were similarly unsupported. 
     my intention here is to present some results that deal with these questions. let me stress that it is precisely the question posed in that last paragraph that interests me: should probabilities be used to implement non-monotonic reasoning systems  the easier question of whether probabilities can be used to implement some types of non-monotoic reasoning has been answered rather conclusively by mycin and its offspring; more difficult questions involving the nature or definition of probability itself have been grappled with by philosophers for centuries  and i am content to leave them to it. 
     i will attempt to address the issues of whether the numbers required by a probabilistic theory can in general be made available to a reasoning system  and whether or not probabilistic methods arc effective. the first of these is principally a theoretical issue  while the second is more one of pragmatics. 
¡ì1. a priori probabilities 
     a standing objection to the use of probabilities in ai systems corresponds to the question   where do the numbers come from   bayesian methods require the existence of initial estimates for the probabilities in question  and it seems impossible to arrive at these estimates without a 
     research supported by the office of naval reserach under grant number n1-k-1. 
great deal of knowledge about the domain being considered. 
     i have a great deal of sympathy with this objection. it has been pointed out  however  that by considering ranges of probabilities instead of specific values  it is possible to encode information not only about the strength of our belief in a given proposition  but also about our confidence in our estimate of that strength  1 . a precise formulation of this observation will be the principle result of this section. 
     when we think of a statement as corresponding not to a precise probability but to a range  x y   we can think of y - x as corresponding to the uncertainty we have in our probabilistic estimate. thus a specific range {x} =  x x  implies total confidence in our probabilistic knowledge  while the maximal range  1  corresponds to total ignorance-the statement that a certain probability p lies in the range  has no informational content at all. 
     more generally  a probability range  x y  with corresponds to partial knowledge. furthermore  it is possible to use dempster-shafer theory to combine probability ranges of this sort; an application of this to semantic nets is described in . 
     in order to see how to obtain the ranges from observational data  or the lack thereof   suppose that the probability of some specific default rule is p  although this value need not be known to us. now fix some  gullibility   and suppose that we test the default rule ex-
perimentally t times. then there is some  such that the probability of our observing no more than tpmin successful applications of the default rule among the t trials is equal to g. intuitively  if the  real  probability is p  we require that the chance that the observed probability be at least pmin be at least g. thus if we get the extremely cautious approximation ; 
	we can define 	similarly. having done so  
if some default rule d has been tested t times with 1 successes  we can approximate the overall probability to be assigned to the rule by s/t  and consider the probabilistic range  conversely  given a probability range  x y   we can use this expression to recover s and t  for g fixed . 
     the details of the calculation require us to solve a familiar problem from probability theory: given a series of t trials in an experiment where the probability of success 
     
1 	m.ginsberg 
on each trial is p  what is the probability that the observed probability of success will be in the range  pmin  pmax   this problem is discussed in   among other places; there is no exact solution in closed form  but results can be obtained either by using tchebychev's approximation or by approximating the relevant binomial distribution with a gaussian. tchebychev's approximation gives 
 1  
 1  
if g = 1  we get a probability range of  1  l  independent of s and t  not very gullible at all!   as we do if s = t = 1. in the large t limit  we get the singleton s/t as expected-our confidence in our estimate increases as docs the amount of data. 
     alternatively  we can use the gaussian approximation  so that we need to solve 

     to solve the inverse problem  in either case   suppose we are given a probability range  x y . then if we set 
	 1  

¡ì1. implementation issues 
     existing formalisms of non-monotonic reasoning generally proceed by attempting to determine whether or not a default inference will be valid before drawing it. thus  before concluding that the bird tweety can fly  we first try to prove that he can't; if the proof fails  we draw the inference that he can. 
     there are well known difficulties with this. the first is that the problem of proving that twccty can't fly is only semi-decidable  and implementations of this scheme therefore tend to be painstakingly slow  at best! . the second is that the need to use the appearance of a new datum  such as the fact that twccty is an ostrich  to reverse an earlier conclusion requires the introduction of a new formalism  such as truth maintenance . probabilities provide a way around both of these difficulties by marking the conclusion of a proof to indicate that it may be subsequently reversed in the presence of stronger contradictory evidence. we are not claiming here that they can replace a truth maintenance system; it will still be necessary to store information regarding either the use to which information has been put  in a forward-chaining system  or the source from which information was obtained  in a backward-chaining one . 
     in the presence of an adequate rule for probabilistic combination  many of the attractive properties of a reason maintenance system can be incorporated into a probabilistic one. when the truth value of some conclusion changes as a result of the appearance of new evidence  earlier inferences made using this conclusion can be repeated  with the change in probability therefore propagating to the results that were derived from it. 
1 tags 
     suppose that we are in fact considering ranges of probabilities instead of specific values  and let p be the set of all closed subintcrvals of  1 . then there are six natural 
     
mappings from p to  1   given by: 

intuitively  t corresponds to the extent to which a given statement is confirmed by the available evidence  and nil to the extent to which it is discontinued  mass reflects the completeness of our probabilistic information  and unc the incompleteness of it. finally  poss and poss-not correspond to the degrees to which the statement might be true or false respectively. 
     we will refer to these six functions as tags; they provide a natural and uniform framework in which to consider either the truth or falsity of any given proposition  or the extent of our knowledge about it  
1 use of probability to limit inference 
     non-numerical inference techniques must of necessity run to completion; there seems to be no way to use qualitative information to terminate the inference process. this can be avoided if quantitative methods are used. 
     there are two ways in which a probabilistic inference can be shortened. suppose that we arc trying to prove some proposition p; the first cutoff can be implemented by not including in our analysis any inferences which will affect the eventual probability of p by jess than some small value c1. for example  it never rains in southern california  or at least only very rarely   1j; if we are trying to show that our beach party will be a success  we do not need to consider rain as a reason for it not to be. 
     a second and independent way to shorten a probabilistic inference is to assume that if the probability exceeds some value c1  alternatively  if the result of applying some tag to the probability range exceeds c1   the inference is complete. if the all-star game is being played in los angeles on the same day as our beach party and we have a friend who is giving away tickets to it  then we are probably better off picking another day for the party than looking for an esoteric proof that it will be successful after all. 
     it is worth considering the effects on the inference procedure if we select extremal values for c1 or c1. taking c1 = 1 allows allows all relevant information to be considered  while c1 = 1 ensures that the entire deduction will not be stopped early. this combination therefore results in all attempted derivations running to their eventual conclusions as described at the beginning of this section. 
	m.ginsberg 	1 
 and as such  is no more efficient than any of the more conventional techniques for non-monotonic reasoning.  if we select c1 - 1  then only monotonic inferences will be considered  while c1 = 1 results in the rather preemptive strategy of considering only the first bit of applicable information. finally  the combination c1 = c1 = 1 allows us to perform standard monotonic reasoning using a proba-
bilistic database. 
1 probabilistic resolution 
     the inference technique of resolution can be extended to deal with probabilistic information. consider the derivation of flies  twcety  from bird tweety  and bird x  flies x : 
unifying the above two expressions by substituting tweety for x and resolving the results gives  flies tweety . 
     in general  we will view resolution as the combination of expressions of the form 
 1  
to obtain 
where p and s are either positive literals or conjunctions thereof  and r and t are either positive literals or disjunctions thereof. 
     intuitively  if p and s hold  then from the first rule being resolved in  1   either r holds  in which case the conclusion of the resolution is valid   or q holds  in which case the second rule can be applied to conclude that t holds. the likelihood of the conclusion being probabilistically valid is therefore at least the product of the likelihoods of each of the original rides being valid. 
     the situation is complicated in the probabilistic case by the need to treat negation in a uniform fashion. the reason for this is that the probability range assigned to the statement bird x  flies x  will also contain information about the likelihood of a randomly selected bird not being able to fly-in other words  about the validity of the statement bird{x  flies x . 
since the bird x  flies x  can be rewritten as the effect of this is that it is possible 
to resolve pairs in which the same clause appears in each conclusion. in general  the the implication 
implies and we can resolve this with to get it follows that given the pair of rules 
     
1 	m.ginsberg 
there will be two contributions to the confirmation of given by the product of the confirmation of and the disconfirmation of  as above   and by the product of the disconfirmation of and the confirmation of 
1 implementation results 
     the ideas described in this paper have been implemented in the expert system building tool mrs at stanford. we will conclude by describing some of the details of this implementation. additional details can be found in 
 1. 
     mrs  is a logic-based expert system building tool. it supports a variety of inference methods  including forward- and backward-chaining. information is currently entered into mrs on two  levels . the meta-level is used to store information regarding control of inference or procedural attachments for the various mrs primitives  a demon is a procedural attachment to the primitive that stashes an item in the database  for example . the base level is used to store more conventional expert system-type information about the domain in question. although the inference methods for the two levels are distinct  all of the information is stored in a single database. 
     the probabilistic implementation associates to each fact in the database a pair  c . d  corresponding to the probability range  c  1 - d . the probability ranges are thought of as the  truth values  of the propositions  and are combined using dempster's rule as described in   
     tags arc used to reduce the probability ranges to specific values  as described in section 1. this has the immediate advantage of unifying the treatment of negation within mrs itself-where the two propositions  not  ost r i c h fred   and  ostrich fred  had previously been considered to be unrelated  they arc now simply differing apects of the same object  and interact more conveniently with  for example   known  ostrich fred   or  unknown  ostrich fred  . 
     reason maintanrnce facilities have been implemented in the forward chainer only. when a rule of inference is invoked  the truth value of the instantiated version of the premise is stored  along with information concerning the instantiation itself. the next time the rule is invoked  if the mass of the difference between the previous truth value and the current one is no greater than the inference cutoff c1  no action is taken. the effect of this is to avoid propagating a change in the database to a point where it will have no significant effect on the probabilities of the statements involved. 
     the backward chainer litis been implemented using the pair of cutoffs described in the previous section. timing tests done with c1 = c1 = 1  standard monotonic inference only  indicate that the incorporation of the-probabilistic facilities has at most a small effect  perhaps 1%  on the system's monotonic performance. 
     the most important experiment remains. comparable implementations of a large-scale non-monotonic problem using both probabilistic and non-probabilistic methods are needed; it is only when comparisons can be made that it will be possible to draw secure conclusions. 
¡ì1. conclusion 
     the efficacy of using probabilities in a non-monotonic inference system is both a theoretical and an experimental question  and we have attempted to address both issues in this paper. our theoretical arguments dealt with the possibility of using probability ranges and dempster-shafer theory to sidestep the bayesian need for a priori probabilities. 
     the experimental question may well be more interesting  but cannot be settled until a great deal more work is done on full-size non-monotonic systems that do and do not use probabilistic inference methods. the work we have completed at stanford seems to support the arguments we have presented  but no hard and fast conclusion can be drawn without a great deal more experimental evidence. 
acknowledgement 
     the author would like to thank mike genesereth and john mccarthy for many enlightening  if heated  discussions  and russ greiner for the care with which he examined an early version of this paper. 
