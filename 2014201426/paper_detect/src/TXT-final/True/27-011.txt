 
we argue that many ai planning problems should be viewed as process-onented  where the aim is to produce a policy or behavior strategy with no termination condition in mind  as opposed to goal-onented the full power of markov decision models  adopted recently for ai planning  becomes apparent with process-onented problems the question of appropriate opdmallry criteria becomes more cnncal in this case  we argue that average reward optimahty is most suitable while construction of averageoptimal policies involves a number of subtleties and computational difficulties  certain aspects of the problem can be solved using compact action representations such as bayes nets in particular  we provide an algorithm that identifies the structure of the markov process underlying a planning problem - a crucial element of constructing average optimal policies - without explicit enumeration of the problem state space 
1 	introduction 
tbe traditional ai planning paradigm requires an agent to denve a sequence of actions that leads from an initial state to a goal state while much planning research has focussed on rather unrealistic models that assume complete knowledge of both states and actions  increasingly  research in planning has been directed towards problems in which the initial condinons and the effects of actions are not known with certainty and in which multiple  potentially conflicting objectives must be traded against one another to determine optimal courses of action in particular there has been much interest in decision theoretic planning  dtp   dean and wellman 1  
　tbe theory of markov decision processes  mdps  has found considerable popularity recently both as a conceptual and computational model for dtp  dean et al 1 boutiher and dearden 1  indeed  much recent research has emphasized the complementary nature of work in  for example  operations research  or  on the foundations and computauormlaspects of mdps  and planning models used in ai perhaps most important is the exploitation of structure in solving 
 thu research wu nipported by nserc research grant 
ogp1 and the nce iris-n program project ic-1 
f
thus research wu supported by nserc grant ogp1 
1 	learning 
m a r t i n l. puterman t 
faculty of commerce 
university of bntish columbia 
vancouver  bc v 1 t 1  c a n a d a marty markov commerce ubc ca 
hnp //acme commerce ubc ca/puterman/puurman-html 
mdps using compact representations of actions  such as influence diagrams or strips operators  one can often group together large classes of calculations with great savings if the domain possesses many regularities  tatman and shachter 1  boutiher  dearden and goldszmidt 1  we will exploit representations of this form below 
　an important disnnction that arises when one considers the use of mdps for planning problems is that between goaloriented planning problems and process-oriented problems a goal-onented problem is one in which an agent must construct a plan that will change the world from some initial state to one of a specified set of goals states for example constructing a plan to achieve a goal proposition g is a goalonented problem. implicit in such problems is tbe assumption that the evolution of the system  once the goal is achieved  ceases to be of interest the agent must be given another goal to achieve in order to begin planning and acting again such problems have received the bulk of the attention from the planning community  even when uncertainty is involved  though a relaxed definition of success may be used  kushmenck  hanks and weld 1   in decision-theoretic settings 
goal based approaches are also common  with utilities used often to discriminate feasible plans  dean et al 1  
a process-onented problem is one in which there does not 
 necessarily  exist a goal state of the type described above more specifically  there may be no state  or goal  such that the agent should stop acting once that state is reached  or the goal is true  such problems require mat the planning agent construct an on-going plan that proceeds indefinitely while we focus on dtp  where these often occur naturally  processonented problems can also arise in more  classical  settings  for example  one might require an agent to construct a plan or policy that continuously alternates between states satisfying goals g1 and g1 if exogenous events can cause these goals to become false  then such a plan proceeds indefinitely 
　mdps are excellent models for such process-onented problems techniques such as policy iteration  howard 1  can be used to denve optimal plans for infinite hortzon problems of this type under uncertainty unfortunately  the emphasis m recent work using mdps for dtp has been on goal-onented problems  dean et al 1  boutiher and dearden 1   albeit conditional and decision-theoretic this is not to say that these algorithms don't work for process-onented problems  but no consideration has been given to the issues and special circumstances that might anse when an ongoing process is involved the full power of mdps only comes to hght 

when we have problems that exhibit this continual basis one aim of this paper is to survey the unique challenges that arise when we attempt to solve process-onented problems issues that take on added importance include representation of exogenous events  design of reward functions  and appropriate optimality cnteria. 
　this last issue  the design of appropriate optimality criteria  has been paid little attention in dtp mdps have been used for planning and reinforcement learning quite extensively  and most models measure the goodness of policies using dis counted total reward  one exception is  singh 1   however  hole thought seems to have been given to this choice of opbmality measure or to good discounting rates in fact  for many ongoing processes it seems that the correct  or most useful  measure of a policy is the average reward it accrues per unit time discounting admits conceptually simpler policy construction algorithms  but small discounting rates introduce unacceptable bias toward quick rewards at the expense of long-term gam  while large  close to one  discounting rates cause algorithms to converge quite slowly 
　unfortunately finding average-optimal policies comph cates most policy construction algorithms some algorithms such as value iteration  bellman 1  puterman 1  will work in almost the same form as for discounted problems  but only if one can establish the underlying  reachability  or communicating structure of the process the second aim and key technical contribution of this paper is the development of an algorithm mat determines this structure using a compact representation of the mdp's dynamics unlike existing algorithms for structure classification  fox and land  1   our algorithm exploits the problem representation lo avoid enumeration and traversal of the underlying state space this is an important feature because the planning state space grows exponentially with the number of variables or features present. 
　in section 1 we will sketch a rather simplified but in many respects realistic example lo illustrate these considerations we argue that many realistic problems ought to viewed as process-onented rather than goal-onented we emphasize the importance of exogenous events  especially user com mands  and considerations of appropnate reward structure in section 1  we describe the basic mdp model and policy construction techniques in addition  we discuss compact representations of mdps  the separation of events from actions  and point to ways in which these can be used to speed up pol icy construction in section 1  we argue that average reward opomality is often appropnate for such problems and point out the difficulties involved in computing average-optimal policies we also present the mam technical contribution of the paper  namely  an algorithm that determines the underlying communicating structure of an mdp  a crucial step in the computation of average-optimal policies by exploiting our action representation   potentially exponential  reductions in time and memory requirements are possible for many problems  as compared to traditional stale-space algonthms 
1 	a process-oriented planning problem 
oft-used  gopher** domains are commonly viewed as goalonented planning problems we have an agent  say a robot  that is designed to perform certain tasks for its owner  the user  most planning algonthms suggest that the user will ask the robot to perform some task or achieve some goal' the robot will construct a plan to achieve that goal  and then execute the plan when that goal is achieved the robot waits  doing nothing  until another request is issued this cycle of  get goal  achieve goal  is pervasive in classical and decision-theoretic models however  this cycle of achieving goals in order is rather unrealistic for a number of reasons 
1 many goals are not specifiable in this manner consider simple maintenance goals such as  keep the lab tidy   this is not a goal that can be achieved then abandoned though maintenance goals are used in classical planning  they typically specify constraints  such as subgoals and safety constraints  that the agent is not permitted to violate while achieving a primary goal these serve a somewhat different purpose than true maintenance goals 
1 a user should not have to wait until a previous goal is satisfied before issuing another request  or if the robot stores requests in the order issued  it may not be desirable to have the robot delay achievement of later goals while completing earlier ones a new goal may preempt previous goals - and there is no reason to expect some goals not to be preempted indefinitely 
1 we should not expect an agent's actions at any given time to be directed toward the achievement of a single goal proposmon should multiple objectives be obtainable more readily  or at lower cost  by interleaving or sharing certain actions to achieve those objectives  an architecture that forces consideration of a single goal at any one time will produce suboptimal behavior 
1 an agent should plan not only for its current objectives  but also in anticipation of new goals or contingencies an agent whose raison d itre is mail delivery may be well-served by positioning itself near the mailroom at certain times  if u has no other pressing tasks  
it should be clear that many of the problems to which classical goal-onented planning techniques are currently applied may more naturally be thought of as process-onented prob1lems while point 1 indicates that some objectives are truly ongoing  points 1 suggest that even multiple or recurring goals extended in time interact in ways that make the processonented perspective most suitable 
　to make our discussion more concrete  we will focus on a particular example of a' gopher  robot with three pnmary responsibilities to pickup and deliver mail to a user  to deliver coffee to the user  and to keep the user's lab tidy this is not a goal-onented problem in the classical sense keeping the lab tidy is certainly an on-going process mail amves continually as does the user's need for coffee 1 to formalize this 
problem we assume the six domain variables hoc  the loca tion of the robot  takes one of five values lo  ll  lm  lh  lc  office  lab  mailroom  haljway  and coffeeroom  connected in a cyclic fashion  t indicates lab tidiness with five values to  messiest  to ta  tidiest  we also have four boolean vanables denoting whether there is mail in the user's box  a/   an outstanding coffee request by the user  cr   the robot has 
   'for example software agents as commonly conceived  often have this flavor j in  boutiber and puterman 1  we give a full desenption of this problem and further details of our algonthms 
	b1utllier and puterman 	1 

mail  hrm   or the robot has coffee  hrc this gives rise to a problem with 1 states 
   process-onented problems typically arise in systems that change in certain ways independendy of the agent's actions changes that demand the agent's continuing attention require that we model exogenous events that change the state of the system. an especially important class of such events will be user commands so our agent can react to requests  we treat user commands as particular exogenous events that cause facts like  there is an outstanding request to do x  to become true these are not goals in the classical sense  however  for an agent is under no obligation to drop what it is doing and immediately  or ever  satisfy the request. requests must be balanced with other objectives in the derivation of an optimal course of action for the agent the variable cr above serves this purpose - it indicates whether the user has issued a coffee request that remains unfulfilled in order to model out problem  we assume three exogenous events occur occasionally the arrival of mail  causing m   the user requesting coffee  causing cr  and the lab becoming untidy  causing t to decrease one unit  we assume the probability of any of these events occurring at a given time is known clearly  optimal plans vary with these probabiliues for instance the robot may  hang out  at the mailroom if mail arrival is likely 
　our robot has a number of actions at its disposal it can move through its domain in either direction  actions gou and goo   it can pickup mail  pum  successfully if in the mailroom and there is mail  it can deliver mail  delm  in its possession to the user; it can pour coffee  pc  if in the coffee room  it can deliver this coffee  delc  to the user in the office  causing a request cr to be fulfilled   it can tidy the lab  tidy  by one unit  and it can do nothing  stay  
　to construct a plan  an agent must be able to predict the system state after execution of an action here however these predictions must account for the possible occurrence of exogenous events a common technique for incorporating events is to  roll in  the probability of exogenous event occurrences and their effects into the action description for example when the robot considers the effect of goo not only will it know that its location changes  it expects mail to arrive with some probability as well however the natural specification of the problem suggests that a user should be permitted to specify exogenous events and their effects independendy of the action specificanon so in addition to the eight actions  we assume that the three events described above  denoted arrm  reqc  and mess  are specified independently in much the same format as actions unlike acnons  whose occurrence is controlled by the agent  events must also come with a description of the conditions under and probabilities wiih which they may occur for instance  we might assume that arrm occurs with probability 1 at any  stage   see below  
in order to construct a plan or policy  we can automatically 
 roll in  the event probabilities and effects into the action descriptions this is usually a straightforward process  but problems arise when an action and an event affect the same variable in different ways for instance  suppose the action pum is executed at a certain stage in the plan  causing 1  and the event arrm occurs at the same stage  causing m  there are no general principles by which the  true  effect of the action-evem pair can be constructed from the information provided thus we assume mat for any such conflicts  die 
1 	learning 
user is willing to specify the  net effect  on the variable in question in our domain  most action-event pairs have predictable effects on van able s and the few contentious cases are resolved explicitly for example lfarrm occurs concurrently with pum  m is true  there is more mail to pick up  we describe action-event merging formally in the next section 
　also taking on added importance in process-onented models is the representation of goals and objectives if goals are classical  discrete propositions   how should one represent the fact that one goal should be achieved before another  or that a goal has been achieved and that the next can be pursued  in a decision-theoretic setting  how should one assign rewards or costs to fulfillment of objectives  or lack thereof   in a process-onented problem  the usual approach of assigning rewards to states in which objectives are satisfied becomes problematic - since the objective may remain true in subsequent states  there is a danger of  over-compensating  an agent for satisfying an objective once on the other hand  associating rewards with state transitions  e g   a transition to a good state from a bad one  has its own difficulties we discuss these issues in detail in  boutilier and puterman 1  
　for this problem  and many in which there are separate objectives to be balanced a useful reward model is one where penalties are associated with states in which objectives are unsatisfied for instance at any state where there is an outstanding user request cr  the agent is penalized such request variables become false when die objective  in this case  successful coffee delivery  is met the magnitude of the penalty reflects the relative importance of the objective in our example  we associate  additive  penalties with the following propositions cr  an outstanding coffee request   m v hrm  undelivered mail   and tn if n   1  with penalties varying with degree of tidiness  the magnitudes of the penalties capture the relative priority of mail  coffee and tidiness optimal plans vary considerably with the relative importance of these objectives for example  the robot may move to the mailroom if there are no current tasks and mail has high priority 
1 	mdps and their representation 
 we model a dtp problem as a completely observable mdp these are ideal for representing stochastic domains without classical goals  and especially process-onented problems we assume a finite set of states 1  a set of actions a and a reward function r an action takes an agent from one state to another with each transition corresponding to a stage of the process the effects of actions cannot be predicted with certainty  hence we write  to denote the probability that s1 is reached given that action a is performed in state s 1 these transition probabilities can be encoded in an  matrix for each action 1 complete observability 
entails that the agent always knows what state it is in we assume a bounded  real valued reward function r  with r s  denoting the  immediate  utility of being in state b 1 for our purposes an mdp consists of s  a  r and the set of transition distributions 
	a plan or policy is a mapping tt 	   de-
notes the action an agent will perform whenever it is m state 
1 we assume any action can be attempted in any stole 
1 cosu can also be associated with actions in general 

s1 policies naturally encode strategies suited for processoncnted problems  there is no notion of a bote sequence of actions or termination condition as in the classical setting given an mdp  an agent ought to adopt a policy that maximizes the expected value of its  potentially infinite  trajectory through the state space typically value depends in a compositional way on the stales  in particular  the rewards r{s   through which an agent passes the most common value  and ophmahty  criterion m dtp for mfinite-honzon problems is discounted total reward the current value of future rewards is discounted by some factor  and we want 
1 maximize the expected accumulated discounted rewards over an infinite time period the expected value  under this measure  of a fixed policy n at any given state s can be shown to satisfy  howard 1  

the value of at any initial state js can be computed by solving this system of linear equations a policy is optimal 

　techniques for constructing optimal policies for discounted problems have been well-studied while algorithms such as modified policy iteration  puterman and shin 1  are often used in practice  an especially simple algorithm is value it eration based on bellman's  1   principle of optimality  we discuss value iteration because it can  under certain conditions be used directly for average-reward problems as we describe below algorithms such as policy iteration may be much more complex in average-reward settings 
　we start with a random value function v＜ that assigns some value to each i given value estimate v for each stale s we define 
the sequence of functions v' converges linearly to the optimum value in the limit. after some finite number n of iteranons  the choice of maximizing action for each s forms an optimal policy and approximates its value 1 
　the above specification of mdps requires that one spell out the transition matrices for each action and a reward function over the explicit state space s even for a relatively simple problem like the  gopher  example  with 1 states this can be prohibitive clearly  we do not expect users to specify problems in such an explicit form. recently  a number of action representations such as strips and influence diagrams have been applied to the problem of representing stochastic actions and mdps generally  kushmenck  hanks and weld 1  bounber and dearden 1  tatman and shachter 1  
we adopt the  two-slice  temporal bayes network  dean and kanazawa 1  for each action  we have a bayes net with one set of nodes representing the system state prior to the action  one node for each variable  another set representing ihe world after the action has been performed  and directed arcs representing causal influences between the these sets  see 

j
   such policies arc stationary- acuon choice depends only on the state  and not the stage for the problems we consider opumal stationary policies always exisl 
1 we discuss stopping criteria in section 1 see  puierman 1   boublier  dearden and goldsznndt 1  for a more detailed discussion of this representation  
figure 1 shows the specification of the action network for 
pum  describing the effect of pum independent of any event occurrences the tables for the postacbon variables describe the effects of the action nodes labeled persist are unaffected and retain their preaction value  persistence tables are constructed automatically  
　the event network for arrm m figure 1 has a somewhat different form. while the effects of events are specified as with actions  we omit persistence variables for conciseness   we must also indicate the probability of the event occurring the arrm network contains a double-circled node denoting the occurrence of the event in question  with an unconditional probability table the parents of event nodes  though this example has none  are those variables that influence the probability of the event occurrence  e g   arrm could depend on the time of day  
　finally  the net effect network for pum is shown we notice that its effect on hoc  hrc and hrm is the same its effect on cr and t is altered  corresponding to the events reqc  mess; but the combination is derivable automatically the contention between the effect of pum and arrm on the variable m has to be resolved by the user - in this case  we assume more mail arrives  1 e   the robot picks up mail at the beginning of the period  implicit in this type of specification is the modeling assumption that the action and event networks simply describe what hold at the endpomts of a given stage the acuon network for pum says that if the robot is in the mailroom and there is mail at the beginning of a stage  the robot has the mail at the end of the stage it makes no assumptions about how this effect is manifest during the intervening interval therefore  when combined with the event arrm  interpreted similarly  we cannot predict the interactions of their effect on the contentious variable m the user must resolve the conflict we do  however  assume that explicit effects lake precedence over  persistence' variables 
　we note that these tasks should not be viewed as classical goals depending on the event probabilities and the importance of it objectives  under some circumstances tasks can be ignored for example  if mail is far more important than tidiness and mail constantly arrives  the robot will never stop to ndy the lab under the optimal policy 
1 	average reward optimality 
with goal-oriented problems  there is a straightforward measure of success in many decision theoretic problems  such as finite-horizon influence diagrams one can sum the expected utility per stage of the policy but for infinite-horizon processoriented problems the total accumulated reward typically diverges  making any direct comparison between policies meaningless fhus discounting factors are often introduced with a discounting rate less than one  total discounted reward will be bounded and comparisons can be carned oul 
　unfortunately the choice of discounting rate can have a drastic influence on optimal policies a discounting rate such as 1 is hard to justify in our robot example and can induce an unacceptable bias toward quick rewards this essentially means that a unit reward achieved at stage of the process is  currently  worth 1% of the value of a unit reward achieved at stage n - the motivation for discounting is pn-
	boutilier and puterman 	1 


figure 1 action  event and net effect networks 

manly economic but it is difficult 1 provide an economic 
justification for discounting in problems such as these 
　in process-onented problems  we are primarily interested in the steady-state performance of our agent as such  expected average reward per stage is the most appropriate measure of a policy by choosing discount raies very close to one  optimal discounted policies may be similar to average-optima  policies  however  discounted algorithms may converge very slowly  e g   value iteration  or involve ill-conditioned systems  e g   policy iteration  furthermore  directly computing average-optimal policies conforms closely to our intuitions about long-term processes we present a brief summary of average-optima try and its computation  but refer to  puterman 1  for a detailed expositioon 
the expected average reward or gain of a policy is 
where is  be expected total reward when ir is used for n stages starting at state a intuitively  the gain describes the steady-state average reward one can expect of a policy when starting in state s a policy is average  or gam  optimal if it is not dominated by another policy in the usual sense  according to this measure in our finite state setting  average-optimal stationary policies always exist 
　computing average-optimal policies involves a number of subtleties that make approaches such as policy-iieration rather complex however one of the interesting aspects of this optimality measure  which can be exploited for computational gain is its sensitivity to the chain or communicating structure of the mdp we can classify an mdp according to the markov chains induced by the stationary policies it admits for a fixed markov chain  we can group states into maximal recurrent classes such that each state reaches every other state in that class eventually states belonging to no recurrent class are called transient an mdp is recurrent if each policy induces a markov chain with a single recurrent class an mdp is umchain if each policy induces a single recurrent class with  possibly  some transient states an mdp is communicating 
　　1 we assume this limit exists. this may not be the case if the mdp admits policies that are periodic in this case the definition may use a slightly more robust cesaro limit  putennan 1  
1 	learning 
if for any pair of states s  t  there is some policy under which s can reach t we call other policies noncommumcating b 
　umchain and recurrent mdps are especially well-behaved the gain of every stationary policy is constant 
is identical for all  and methods such as policy and value iteration can be used in a relatively straightforward way but planning problems will seldom exhibit this structure to be recurrent  we must know the agent will visit each state infinitely often no matter what policy it adopts it will almost always be the case that an agent can choose to avoid certain states as soon as we have a domain where an agent can move to a certain sections of the state space and remain there  e g stay   the mdp will not be unichain or recurrent. 
   while not quite so well-behaved  communicating models have the nice feature that optimal policies  though not all policies  must have constant gam while policy iteration becomes much more complicated in this case  value iteration can be used directly to construct an optimal policy  we run value iteration as described above with / stopping when the span1 of the difference between two consecutive estimates is small in other words  value iteration stops when  for some small thus when the difference between two value estimates is nearly constant  we are close to an average optimal policy however  this algorithm can only be used under conditions when we know the optimal gain is constant otherwise the algorithm may not converge 1 otherwise more complex methods are required thus  the identification of the underlying chain structure of an mdp becomes an important computational tool for constructing average optimal policies 
　we note that the techniques of  bouulier  dearden and goldszmidt 1  can be applied in this setting  allowing value iteration to work on groups of states instead of com-
　'in the full paper we ductus weakly communicating mdps  which share nice features with communicating mdps. 

　　1d thc algorithm may also not converge if the mdp admits pen odic chains  but apenodicity transformations that introduce a small amount of noise can be used. note also that setting & - 1 is not problematic relative value iteration can be used if undiscounted values get too large see  putemun 1  for these details. 
puting over an explicitly enumerated state space if n can be factored  e g   using a bayes net  thus  our representation can be exploited for computational gain as well 
1 	discovering communicating structure 
we expect many dtp problems to be communicating these problems are such that an agent could with positive probability reach any state from any other slate however noncommunicating problems are not rare in planning domains  e g   if there are 'irreversible choices  such as a robot going down  unclimbable  stairs  or an agent breaking an egg  thus we must take care to classify an mdp before attempting lo construct an average optimal policy if the mdp is communicating value iteration can be used directly the classification algorithm we use has the added advantage that it can be used to apply value iteration  piecewise  to general mdps  as we sketch below  
　an efficient algorithm for classifying markov chains known as the fox-landi algorithm  fl   fox and landi 1  can be extended to the classification of mdps by considering the  reachability  matnx for the mdp roughly  we construct a single transition matnx that assigns positive probabdity to entry i  j if there is any action that moves the process from state i to state j with nonzero probability fl works by constructing paths through the state space using this reachability matrlx  producing a labeling and grouping of all states roughly a start state i is chosen and a path is constructed by adding a state j reachable from i a stale reachable from j and so on if  he path ever loops the entries in the loop are merged into one supers late   note a path can always be extended although n might form a cycle if the cycle  or superstate  cannot be extended  i e   all states reach only other stales in the cycle  then the states in the cycle are grouped into the same recurrent class ail states on the path leading to  but not part of  the cycle are classified as transient then a new unclassified start state is chosen if in path extension a previously classified state  either recurrent or transient  is ever reached all slates on the path are transient  and we begin again if fl classifies all states as recurrent and puts them in the same recurrent class  then the mdp is communicating and value iteration can be used to solve it 
　this form of the fl algonthm requires explicit enumeration of the state space  and fails to exploit regularities captured in our representation of the system dynamics to avoid this we present a structured fox landi algonthm  sfl  that uses the action descnplions directly sfl can be used to classify an mdp directly  or more generally classify any compactly represented markov chain furthermore  in conjuncnon with a structured implementation of value iteration it can be used to compute average-optimal policies for arbitrary mdps  regardless of chain structure  
　schematic states and paths the key feature of the sfl algonthm is its use of a schematic representation for states  paths and cycles  allowing entire groups of paths to be extended in a single operation the schematic path building and cycle detection operation then itself involves a number of crucial components  which we bnefly describe 
　schematic states  s-states  represent groups of states corresponding lo a partial variable assignment. for example  we use  ll   to capture a state where ll  lab  is true  and the other variables  m  t  etc   have some fixed value in general  an s-state consists of n slots to represent values of n domain variables a slot can be filled in various ways it can have a fixed value such as ll  or an arbitrary fixed value from a certain set  denoted  ll lo  this represents any fixed state with one of the specified values we abbreviate all values of a variable using a dot as shown above  and we use an overhne to denote the complement of the value set 
　schematic paths  s-paths  are constructed by applying actions to s-states - since atons have local effects  only certain portions of an s-state are affected this can be viewed as implicitly extending every state consistent with the s-state for example in figure 1 a  the s-state above is extended to the state {lo   this is reached by applying action coo  whose effect can be read from its network  this s-path of length two actually represents the 1 true paths induced by assignment to the variables an s-path with fixed values represents the set of paths where the variable has some fixed value everywhere in that path  unless a different value occurs later in the path  we can also represent cycles schematically as single states the notation  in figure 1 c  means thai any value in that set is  reachable  from any other value 
thus  it captures a cycle between states where to and t  hold  all else equal  abbreviates a cycle among all possible values of variable loc  see figure 1 a   
　a key element in path construction and cycle detection is unification to test whether two s-states intersect  1 e   share stales  unification is straightforward - it identifies the stales shared by two s-slates  the unifier  as well as those they do not  in a symbolic fashion it is used to join two s-paths or form a cycle  but in general when two paths are joined at an s-state the unification is not complete  1 e   there will be states that are not shared  in this case  the s-paths will split a concatenated s-path will be formed using the unifier  common states  and the remaining states will be split off symbolically leaving two more specific s-paths  we see this below  a detailed exposition of path splitting is not possible here 
　finally  because an s-path represents a group of paths  and can be split into more specific s-paths  we must keep track of partially constructed s-paths that have not been extended to completion unlike ordinary fl  which only ever builds one path we must keep an open list of such partial s-paths when extending the current path  we will try to unify the head with earlier states in the path  to create cycles  or an existing path on the open list- by creating cycles whenever possible  the problem representation tends to stay compact 
   structured fox-landl algorithm we give a high-level sketch of the sfl algorithm  figure 1  and describe its application to our example  figure 1  we defer a detailed description to  boutilierandputerman 1  along with more formal definitions and a proof of correctness the example here blurs a number of steps in the algorithm for conciseness 
　we begin by choosing the initial s-state in figure 1 a  it is called the current paih  and the main loop of the algorithm constantly extends the current path by applying an action and choosing some possible outcome of that action in this case the action gco is applied several times  extending the palh to length 1 the sixth application returns to the initial state in the path this is delected by the unification procedure during cycle detection whenever the current path is extended  the new head state is compared to all  unclassified  visited sstales  either those earlier in the path or those on the open list 
	b1util1er and puterman 	1 


figure 1 the structured fox-landi algorithm in this case a cycle is detected and the path is collapsed into the s-cycle at the bottom of figure 1 a  thus  the robot can  with nonzero probability  reach any location from any other without disturbing other variables 
　we continue in figure 1 b  by specializing this cycle  the head of the current pain  with the value t1 the  rest  of the s-cycle is still valid it 1` split from the current path and added to the open list for extension in the future we apply the action slay several times under which the lab  with nonzero probability  gets messier  giving us the current path in figure 1 b  at each point  one fewer  instance  of t is left on the open list  since the head state at each path extension step unifies with a specializanon of the s-state on the open list  detected in path joining  by the end  the open list is empty 
　in figure 1 c   the action tidy is applied at the head of the current list while tidy only has the desired effect when ll holds  the condition  ensures that the necessary condition ll is reachable but alter the action  ll remain true cycle detection discovers that this new state unifies with a previous state on the path  and the new cycle is formed  the second path in figure 1 c   with several more applications of tidy  we easily get to the state 
　it is worth noting  at this point  that we have discovered that the  subprocess  consisting of variables loc and t is now known to be communicating  although we haven't explicitly constructed a path through all 1 states  1  of this process 
1 	boutilier and puterman 
instead  we have shown that all values of loc communicate and that all values of t communicate under some value of loc this simple subprocess illustrates the spirit of sfl we expect that problems that can be decomposed into groups of variables that have strong mutual influence  within groups   but relatively constrained influences between groups  will be very well-suited for sfl  see  heuristics  below  
　we continue in figure 1 d  by considering the variable m we start with value m  and extend it with stay  making m true due to possible mail arrival   this unifies with the initial open list  making it empty in an effort to form a quick cycle  we apply acbon pum the condition lm is satisfied by {l*}  and holds following the acbon another effect however in hrm this unifies with the initial state but forces the current path to split only hrm becomes part of the cycle  nothing in pum can force the robot to lose the mail  the split chain hrm stays apart from the cycle finally  in figure 1 e  we extend this chain with the delm action if lo holds then hrm becomes false and the path collapses into a cycle the variables cr and hrc will behave similarly and thus our 
mdp is communicating 
　for other problems  the algorithm is somewhat more complex here we notice that each s-state can be extended to a novel s-state by some action until the obvious final step if there are multiple recurrent classes  when we complete the construction of a maximal cycle  some effort is required to ensure that it is a maximal class in particular  we must ensure that no action can move the system out of that class of states however  given the schematic representation of cycles and paths and the structured acbon representations  this can usually be verified quite readily even in the worst case  with no exploitable structure   the effort is no more than that needed to construct the reachability matrix for fl 
　heuristics we note that in our example the algorithm verifies the communicating structure in under 1 steps of path extension even with the overhead of unification  this is considerably better than the i steps  in this case  roughly 1  required by fl ofcourse  we have exploited  good  action and outcome choices in performing the algorithm here a crucial aspect of sfl is the use of heuristic information encoded in the action representation when choosing the  direction  in which to extend a path 
　the mam guiding principle is that we attempt to find the  local communicating structure  of individual or small groups of 


figure 1 influence graph for example problem 
variables that are  to some extent  shielded from the influence of other variables in particular  we try to find short s-cycles in small groups of variables  choosingparticular variables and outcomes that will unify with earlier states we choose the variables to extend using an influence graph that describes influences between variables  see figure 1  in our example hoc is expanded first since no other variables under any action influence the probability of hoc  as indicated by the graph  the structure of loc is independent of any other conditions in our example this means under all circumstances it can be ignored when determining the structure of other variables all variables are partially ordered by the graph and are expanded roughly reflecting this order 1 
1 	exploiting communicating structure 
our algorithm has three outcomes of interest either a sin gle recurrent class is discovered  a single class plus transient states  or more than one recurrent class  plus possibly transient states  if our aim is to simply categorize an mdp as communicating or not  the algorithm can be terminated as soon as any transient states  or multiple recurrent classes  are identified if identified as communicating a simple algonihm like value iteration  or related methods based on structured representations  boutiher dearden and goldszmidt 1   can be used to determine the average optimal policy 
　if the algonthm discovers more than one recurrent class then the mdp is multichain  i e general  if a single recurrent class is discovered together with transient states  then it may be weakly communicating or multichain weakly communicating mdps also have constant gain and can be solved using value iteration however  determining this fact requires examination of individual policies something our algonthm does not currently do if the process is multichain  more complex methods may have to be used 
　however  ross and varadarajan  1  have proposed a method for decomposing general mdps we are currendy adapting this method for use with sfl to constructing average optimal policies using  piecewise  value lteradon roughly  the recurrent classes identified by sfl can be  solved  independently using value iteration  since they must have constant gain  then these states are  eliminated   transient states are reclassified in this reduced mdp  and fl is run again on the remainder of the state space  ignonng these recurrent classes  the second level of fl provides new recurrent classes for which optimal gain  in the sub-problem  is constant these can be pieced together with the previously classified states 
　 the precise meaning of the graph and lis construcuon are de scribed in  boutiher and puterman 1  
to determine a new policy if the gam in the subproblem is greater  these states adopt actions that keep them from the earlier states the procedure continues until all states are classified 
1 	concluding remarks 
we have argued that many planning problems are processonented and that special consideration must be given to these especially in the choice of reward and acoon representation we also claim that average-optimality is the most appropriate measure of performance for many process problems  and have presented the sfl algonihm to determine the communicating structure of an mdp  an important part of constructing average-optimal policies  using compact action representations we are currently explonng further heunsucs for the algonthm conducting experiments to determine general problem characteristics that predict good performance of sfl as compared to standard fl  and extending our approach to multichain problems 
　future research includes applying these ideas to semimarkov models where actions can take varying amounts of time and the use of more genera  modeling assumptions for 
events the discovery of weakly-commumcatmg mdps using structured paths is also of interest 
