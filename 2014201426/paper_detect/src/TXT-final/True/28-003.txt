 
the creation of a complex web site is a thorny problem in user interface design. first  different visitors have distinct goals. second  even a single visitor may have different needs at different times. much of the information at the site may also be dynamic or time-dependent. third  as the site grows and evolves  its original design may no longer be appropriate. finally  a site may be designed for a particular purpose but used in unexpected ways. web servers record data about user interactions and accumulate this data over time. we believe that ai techniques can be used to examine user access logs in order to automatically improve the site. we challenge the ai community to create adaptive web sites: sites that automatically improve their organization and presentation based on user access data. several unrelated research projects in plan recognition  machine learning  knowledge representation  and user modeling have begun to explore aspects of this problem. we hope that posing this challenge explicitly will bring these projects together and stimulate fundamental ai research. success would have a broad and highly visible impact on the web and the ai community. 
1 	introduction 
the world wide web is becoming a key medium for information dissemination  entertainment  and communication. examples include personal home pages  on-line malls  university course information  and much more. many web sites quickly sprout intricate collections of pages and hyperlinks as they begin to mirror the complexity of the information they convey. 
   *this research was funded in part by office of naval research grant 1-j-1  by arpa / rome labs grant f1-1  by a gift from rockwell international palo alto research  and by national science foundation grant iri1. 
1 	ai challenges 
　designing a rich web site so that it readily yields its information can be tricky. unlike the oyster that contains a single pearl  a web site often contains myriad facts  images  and hyperlinks. many different visitors approach a popular web site - each with his or her own goals and concerns. consider  for example  the web site for a typical computer science department. the site contains an amalgam of research project descriptions  course information  lists of graduating students  pointers to industrial affiliates  and much more. each nugget of information is of value to someone who would like to access it readily. one might think that a well organized hierarchy would solve this problem  but we've all had the experience of banging our heads against a web site and crying out  it's got to be here somewhere... . 
　the problem of good web design is compounded by several factors beyond the fact that different visitors have distinct goals. first  the same visitor may seek different information at different times. second  many sites outgrow their original design  accumulating links and pages in unlikely places. third  a site may be designed for a particular kind of use  but be used in many different ways in practice; the designer's a priori expectations may be violated. too often web site designs are fossils cast in html  while web navigation is dynamic  time-dependent  and idiosyncratic. we challenge the ai community to address this problem by creating adaptive web sites: web sites that automatically improve their organization and presentation by learning from user access patterns. 
　in essence  web design is a problem in user interface design. however  in contrast with vendors of shrinkwrapped software  few web site designers can afford to subject their web sites to formal usability testing in special labs. fortunately  web users interact directly with a server maintained by the inventors of the service or authors of the content being served. as a result  data on their behavior is recorded in web server logs  see figure 1 . because this raw data is overwhelming for an overworked webmaster to process regularly  web server logs are ripe targets for automated analysis. 
　our challenge then is this: how can we build a web site which improves itself over time in response to user interactions with the site  this challenge poses a number of 


figure 1: typical user access logs  these from a computer science web site. each entry corresponds to a single request to the server and includes originating machine  time  and url requested. note the series of accesses from each of 

two users  one from sfsu  one from umn . 
difficult  but not impossible  questions: 
  what kinds of generalizations can we draw from user access patterns and what kinds of changes could we make  suppose we maintain a web site containing information about various automobiles  organized by manufacturer. we observe that visitors who look at the ford windstar minivan page also tend to look at the dodge caravan and mazda mpv minivan pages. we might therefore create a new page for mini vans  which cuts across the existing manufacturer-based organization and provides a new view of the site. 
  how do we design a site for adaptivity  we might specifically design parts of the site to be changeable. for example  we might present our users with a  tour guide   as in  armstrong et a/.  1   and have changes to the site be presented as the agent's suggestions. alternatively  we might annotate our html with directives stating where and how changes can be made. or we may provide semantic information about the entire site  allowing the agent to reason about the relationships between everything  perhaps representing the entire site as a database  see  fernandez et a/.  1  . 
  how do we effectively collaborate with a hu-man webmaster to suggest and justify potential adaptations  suppose the human webmaster is still responsible for the final product. instead of changing web pages directly  our system might accumulate observations and suggested changes and present them to the webmaster  clearly explaining its observations and justifying the changes it recommends. 
  how do we move beyond one-shot learning algorithms to web sites that continually improve with experience  over time  our adaptive web site will accumulate a great deal of data about its users and should be able to use its rich history to continually evolve and improve. 
our department maintains a web site for its introductory computer science course. this site contains schedules  announcements  assignments  and other information important to the hundreds of students who take the course every quarter. enough information is available that important documents can be hard to find or entirely lost in the clutter. imagine  however  if the site were able to determine what was important and make that information easiest to find. important pages would be available from the site's front page. important links would appear at the top of the page or be highlighted. timely information would be emphasized  and obsolete information would be quietly moved out of the way. 
　there are several factors that make this challenge both appropriate and timely for the ai community. first  the growing popularity and complexity of the web underscores the importance of the challenge. second  virtually all existing web sites are not adaptive  yet data to support the learning process is readily available in web server logs. clearly  here is an opportunity for ai! finally  a number of disconnected projects in machine learning  armstrong et a/.  1   data mining  knowledge representation  plan recognition  kautz  1; pollack  1   and user modeling  fink et a/.  1  have begun to explore aspects of the problem. framing the problem explicitly in this paper could help bring these disparate approaches together. 
　we pose our challenge as a particular task to be accomplished by any means available. many advances in artificial intelligence  both practical and theoretical  have come about in response to such task-oriented approaches. the quest to build a better chess-playing computer  for example  has led to many advances in search techniques  e.g.   anantharaman et a/.  1  . the autonomous land vehicle project at cmu  thorpe  1  has resulted in not only a highway-cruising vehicle but also breakthroughs in vision  robotics  and neural networks. the quest to build autonomous software agents has similarly led to both practical and theoretical advances. for example  the internet softbot project has yielded both deployed softbots and advances in planning  knowledge representation  and machine learning  etzioni  1 . 
　we believe that the goal of creating self-improving web sites is a similar task: one whose accomplishment will require breakthroughs in different areas of ai. in this paper we discuss possible approaches to this task and how to evaluate the community's progress. in section 1  we present two basic approaches to creating an adaptive 
	perkowitz & etzioni 	1 

web site. we illustrate both with ongoing research and examples. in section 1  we discuss how to evaluate research on this challenge  discussing practical alternatives as well as open questions. throughout  we pose challenge questions intended to suggest research directions and illustrate where the open questions lie. 
1 	approaches to adaptive web sites 
sites may be adaptive in two basic ways. first  the site may focus on customization: modifying web pages in real time to suit the needs of individual users. second  the site may focus on optimization: altering the site itself to make navigation easier for all. we illustrate these two basic approaches with examples drawn from current ai research. whether we modify our web pages online or offline  we must use information about user access patterns and the structure of our site. much of this information is available in access logs and in the site's html  but this may not be sufficient; we also discuss how to support adaptivity with meta-information - information about page content. finally  we examine other issues that arise in designing adaptive web sites. 
1 	customization 
customization is adjusting the site's presentation for an individual user. customization allows fine-grained improvement  since the interface may be completely tailored to each individual user. one way for a site to respond to particular visitors is to allow manna/ customization: allowing users to specify display options that are remembered during the entire visit and from one visit to the next. the microsoft network  at http://www.msn.com   for example  allows users to create home pages with customized news and information displays. every time an individual visits her msn home page  she sees the latest pickings from the site presented according to her customizations. 
　path prediction  on the other hand  customizes automatically by attempting to guess where the user wants to 
go and taking her there more quickly. a path prediction system must answer at least the following questions. 
  what are we predicting  we may try to predict the user's next step. for example  if we can predict what link on a page a particular user will follow  we might highlight the link or bring it to the top of the page. alternatively  we may try to predict the user's eventual goal; if we can determine what page at the site a visitor is looking for  we can present it to her immediately. 
  on what basis do we make predictions  we might use only a particular individual's actions to predict where she will go next. on the other hand  we might generalize from multiple users to gather data more quickly. 
  what kinds of modifications do we make on the basis of our predictions  we may do as little as highlighting selected links  by making them 
1 	ai challenges 
bold or putting graphics around them  for example  or as much as synthesizing a brand new page that we think the user wants to see. 
　the webwatcher  armstrong et a/.  1   see http://www.cs.cmu.edu/ ~webwatcher/  learns to predict what links users will follow on a particular page as a function of their specified interests. webwatcher observes many users over time and attempts to learn  given a user's current page and stated interests  where she will go next. a link that webwatcher believes you are likely to follow will be highlighted graphically and duplicated at the top of the page. visitors to a site are asked  in broad terms  what they are looking for. before they depart  they are asked if they found what they wanted. webwatcher uses the paths of people who indicated success as examples of successful navigations. if  for example  many people who were looking for  personal home pages  follow the  people  link  then webwatcher will tend to highlight that link for future visitors with the same goal. 
　instead of predicting a user's next action based on the actions of many  we might try to predict the user's ultimate goal based on what she has done so far. goal recognition  kautz  1; pollack  1  is the problem of identifying  from a series of actions  what an agent is trying to accomplish. lesh and etzioni  lesh and etzioni  1  pose this problem in a domain-independent framework and investigate it empirically in the unix domain: by watching over a user's shoulder  can we figure out what she is trying to accomplish  and offer to accomplish it for her   they model user actions as planning operators. assuming users behave somewhat rationally  they use these actions' precondition/postcondition representation to reason from what a user has done to what she must be trying to do. in the web domain  we observe a visitor's navigation through our site and try to determine what page she is seeking. if we can do this quickly and accurately  we can then offer the desired page immediately. 
i challenge: 	can we formalize user navigation of i the web as a planning process that is amenable to goal recognition  do user actions on the web carry 
i enough evidence of their purpose  | 
　the avanti project  fink et al.} 1   see http://zeus.gmd.de/ projects/avanti.html  focuses on dynamic customization based on users' needs and tastes. as with the webwatcher  avanti relies partly on users providing information about themselves when they enter the site. based on what it knows about the user  avanti attempts to predict both the user's eventual goal and her likely next step. avanti will prominently present links leading directly to pages it thinks a user will want to see. additionally  avanti will highlight links that accord with the user's interests. avanti is illustrated on a hypothetical louvre museum web site. for example  when a disabled tourist comes to the site  links regarding disabled access and tourist information are emphasized. avanti relies on users providing some 

information about themselves in an initial dialogue; the site then uses this information to guide its customization throughout the user's exploration of the site. avanti also attempts to guess where the user might go based on what she has looked at so far. for example  if our disabled tourist looks at a number of paintings at the site  avanti will emphasize paintings links as it continues to serve pages. as with the webwatcher  we might ask if we can avoid avanti's requirement that users explicitly provide information. 
1 	optimization 
whereas customization focuses on individuals  optimization tries to improve the site as a whole. instead of mak-
ing changes for each user  the site learns from all users to make the site easier to use. this approach allows even new users  about whom we know nothing  to benefit from the improvements. 
　we may view a web site's design as a particular point in the vast space of possible designs. improving the site  then  corresponds to searching in this space for a  better  design. assuming we have a way of measuring  better   we may view this as a classical ai search problem. one possible quality metric would be to measure the amount of effort a visitor needs to exert on average in order to find what she is looking for at our site. effort is defined as a function of the number of links traversed and the difficulty of finding those links. for example  a site whose most popular local page is buried five links away from the front page could be improved by making that page accessible from a readily obvious link on the front page. we can navigate through this space by performing transformations on the site - adding or removing links  rearranging links  creating new web pages  etc. if we guarantee that each transformation improves the quality of the site  we are performing a hillclimbing search. 
i challenge: how large is this search space and i what is an appropriate search strategy  can we restructure the space to avoid searching large portions 1 of it  
　in  perkowitz and etzioni  1  we sketch the design of a system with a repertoire of transformations that aim to improve a site's organization; transformations include rearranging and highlighting links as well as synthesizing new pages. our system learns from common patterns in the user access logs and decides how to transform the site to exploit those patterns and make the site easier to navigate. for example  the web site for our department's introductory computer science course contains a web page for each homework assignment given during the course. after each assignment's due date  a solution set for that assignment is made available. our system would observe that after an assignment's due date many visitors look at the solution set; in fact  the most recent solution set is one of the most popular pages at the site. this observation would lead the system to promote the solution set by giving it a prominent link on the front page. promotion - making the link to a page more prominent - is a simple but effective transformation. we have implemented a form of promotion on an existing web site and have found that approximately 1% of our 1-1 daily page accesses are through automatically generated links; roughly 1% of all visitors click through at least one such link. of course  we note that promoting a link may be a self-fulfilling prophecy - making a page more prominent may increase its popularity  artificially inflating the site's apparent success at adaptation. 
　a more ambitious transformation is clustering - synthesizing a brand new web page that contains links to a set of related objects. from available data  the system must infer that a set of pages at the site are related and group them together. this inference might be based on content  e.g.  when a number of pages cover the same topic  or on user navigation patterns  e.g.  when visitors to one page are particularly likely to visit certain others . as final exams approach  students tend to look at multiple solutions sets on each visit. even though the solution pages are not linked together directly  visitors navigate from one to another  via intervening pages  on their own. this pattern suggests that the solution sets form a meaningful group in our visitors' heads  which does not appear on our web site - solution sets are only linked to from their respective assignment pages. our system would create a new page with a link to each solution set and make this new page available to visitors to the site. we are currently implementing clustering transformations based on user navigation data. 
1 	meta-information 
a web site's ability to adapt can be hampered by the limited knowledge about its content and structure provided by html. for example  suppose that a page contains a list of links. is it appropriate to add a new link at the top of the list  the answer depends on the contents of the list - an adaptive site should not add a link to a course's home page to a list of links to faculty home pages; furthermore  if the list is in alphabetical order then a new item can only be added at the appropriate point. clearly  a site's ability to adapt could be enhanced by providing it with meta-information: information about its content  structure  and organization. in this section  we discuss means of providing an adaptive site with this sort of information. 
　one way to provide meta-information is to represent the site's content in a formal framework with precisely defined semantics such as a database or a semantic network. this approach is pioneered by the strudel web-site management system  fernandez et o/.  1  which attempts to separate the information available at a web site from its graphical presentation. instead of manipulating web sites at the level of pages and links  web sites may be specified using strudel's view-definition language. in addition  web sites may be created and updated by issuing strudel queries. for example  a corporation might create home pages for its employees 
	perkowitz & etzioni 	1 

by merging data from its  manager  and  employee  databases. a page would be created for every person in either database. furthermore  each manager's page would have links to her employees  and vice-versa. 
　this approach would facilitate adaptivity because strudel would enable a site to reason about its logical description and detect cases where adaptations would violate the existing logic. furthermore  an adaptive site could easily transform itself by issuing strudel queries; strudel provides the mechanisms to automatically update the site appropriately. the drawback of the strudel approach is that it requires the site's entire content to be encoded in a set of databases or in wrappers that map web pages and other information sources into strudel. the cost of constructing such wrappers for existing web sites  and particularly for relatively unstructured sites  appears to be high. 
　a lighter-weight approach is to annotate an existing web site with meta-content tags. in this approach  a formal description of the content coexists with html documents. we may choose how much of the site to annotate and how complex our annotations will be. yet  meta-content annotation still facilitates reasoning about the connections between parts of the site and still provides guidance as to where and how to make changes. 
one approach of this type is apple's meta-content 
format  see http://mcf.research.apple.com . mcf is an attempt to establish a standard for meta-content annotation for the web. when a user visits an mcfenhanced site with an mcf-enabled browser  she can choose to navigate the site in a three-dimensional representation of the site's structure  as determined from the site's mcf annotation. shoe  luke et a/.  1   at http://www.cs.umd.edu/projects/plus/shoe/   takes a different tack. shoe is a language for adding simple ontologies to web pages. shoe adds basic ontological declarations to html; a page can refer to a particular ontology and declare classifications for itself and relations to other pages. in their example  a man's home page is annotated with information about him  such as the fact that he is a person  his name  his occupation  and his wife's identity  she has her own home page . shoe is designed to facilitate the exploration of agents and the workings of search tools  but ontological annotation could also support adaptation. 
　while lighter-weight than strudel  meta-content tagging also has clear disadvantages. first  because the metarcontent annotation is separated from the actual content  it has to be updated manually as the content changes. second  since the meta-content is attached to existing html  it provides no direct support for automatic adaptation; any adaptation must still modify the original html. 
　each of the approaches described so far require a fair amount of effort to build and maintain the content descriptions. if we wish only to facilitate adaptation  this effort may be overkill. an alternative that we are actively investigating is to use an extremely lightweight annotation system designed specifically for 
1 	ai challenges 
adaptivity. these annotations would be in the form of directives to the adaptive system telling it where it may  or may not  make changes and what kinds of changes it might make. for example  we might add a list tag to html to allow us to describe the elements in a list and how they are ordered. a list might be declared as  list order unordered    which tells the system it may reorder the list in any way it chooses. or a list might be declared  list order=''popularity'   in which case the system will draw upon data from access logs to determine how to present the list. a list declared  list order=  alphabet ical   or  list order= chronological'  can be modified by additions or deletions so long as its original ordering constraint is preserved. 
　we present tags of this sort as part of an  adaptive html  language called a-html in  perkowitz and etzioni  1 . our intention is to extend html to a higher level of abstraction  allowing a web designer to describe objects in terms of their time-relevance  organization  and interrelationships. note that this approach does not require the global establishment of an a-html standard; the adaptive site uses a server capable of interpreting a-html and translating it into standard html at runtime. only the resulting html is served in response to page requests. 
1 	open questions 
the quest for a self-improving web site raises a number of related questions. an adaptive site will be active twenty-four hours a day  seven days a week. the site will constantly be ingesting and analyzing data  adjusting its concepts and models  and updating its own structure and presentation. over time  this constant cycle will reflect many hours of experience and refinement. in the past  ai research has focused on single trials and short-lived entities: systems that run their experiments and shut down  to start again the next day with a blank slate. although such an approach may be applied to the adaptive site challenge  the most intelligent site will surely be one that continually accumulates knowledge about pages  users  content  and itself. 
　user interface design is difficult enough for human beings to perform well. yet an adaptive web site will have to take into account all the artistry of good design in its self-improvements. we can limit the scope of the system's ability to change itself  thus ensuring that it cannot do too much harm  but this means we also limit its scope for improvement. on the other hand  giving the system free rein for radical transformation might mean giving it free rein for radical screwup. 
i challenge: how do we formalize the concept of i good design  how do we limit the potential for harm i without overly limiting the potential for good  
　we might instead put the ai system in the role of advisor to a human master. instead of making changes under cover of night  our ai system must now intelligently present suggestions to a human being  complete with ex-

planation and justification. such a solution frees us from the problem of changing details without changing design but presents us with a new interface challenge. 
i challenge: 	how does our adaptive web site com- i 
i municate its suggestions to a webmaster  
1 evaluation 
although the problem of measuring the quality of a web site design is thorny  we have identified several preliminary approaches. progress on the design of adaptive web sites will include more sophisticated methods of evaluating a site's usability. we propose a basic metric for how usable a site is: how much effort must a user exert on average in order to find what she wants  as discussed in section 1  effort can be defined as a function of the number of links traversed and the difficulty of finding the links on their pages. the standard daily access log may be used to approximately measure user effort. 
　however  standard log data is not sufficient to know everything about visitor navigation. for example  standard logs do not distinguish between individuals connecting from the same location or record which link a user followed. however  software is available to provide more complete information. webthreads  for example  see http://www.webthreads.com   allows a site to track an individual user's progress  including both pages visited and links followed. along with analysis of our site's structure  data from a system like webthreads is sufficient for us to measure user effort. 
　analysis of our user logs provides much information about how users interact with the site. in addition  we may use controlled tests with subjects. such tests have the advantage of allowing us to observe users as they interact with the site - we get much more information than is encoded in user access logs. as subjects perform tasks such as finding information  downloading software  or locating documents  we may gather data such as: 
  whether the subject succeeded at the task  or real-ized it was not solvable . 
  how long the subject took to solve the goal. 
  how much exploration was required. 
careful observation of test subjects would complement the limited access data we get on all of the site's regular visitors. of course  we can also rely on intermediate measures such as encouraging users to fill out feedback forms and send e-mail messages. 
1 conclusion 
this paper posed the challenge of using ai techniques to radically transform web sites from today's inert collections of html pages and hyperlinks to intelligent  evolving entities. adaptive web sites can make popular pages more accessible  highlight interesting links  connect related pages  and cluster similar documents together. an adaptive web site can perform these self-improvements autonomously or advise a site's webmaster  summarizing access information and making suggestions. the improvements can happen in real-time as a visitor is navigating the site  or offline based on observations culled from many visitors. 
　this paper juxtaposed a number of disconnected projects from knowledge representation  machine learning  and user modeling that are investigating aspects of the problem. we believe that posing the challenge explicitly  in this paper  will help to cross-fertilize existing efforts and alert new researchers to the problem. success in the next two years will have a broad and highly visible impact on the web and the ai community. 
