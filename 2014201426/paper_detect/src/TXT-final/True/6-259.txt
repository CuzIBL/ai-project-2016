 
markov decision processes  mdps  and contingency planning  cp  are two widely used approaches to planning under uncertainty. mdps are attractive because the model is extremely general and because many algorithms exist for deriving optimal plans. in contrast  cp is normally performed using heuristic techniques that do not guarantee optimality  but the resulting plans are more compact and more understandable. the inability to present mdp policies in a clear  intuitive way has limited their applicability in some important domains. we introduce an anytime algorithm for deriving contingency plans that combines the advantages of the two approaches. 
1 introduction 
two closely related decision-theoretic planning paradigms have emerged in the area of planning under uncertainty: markov decision processes  mdps  have become a general framework for planning and reinforcement learning. contingency planning  cp  on the other hand emphasizes compactness and understandability of the solution. in both cases  an agent is manipulating an environment by performing actions with uncertain outcomes. in each move  the agent receives some reward and the overall goal is to maximize the cumulative reward. 
　mdps are solved by deriving a policy which maps domain states to actions  typically represented as a large table. several existing algorithms can construct optimal policies  but the results are not easy to visualize or understand. cp is another widely used approach in stochastic domains which allows plans to include branches that may depend on arbitrary memory states. 
　it is clear why optimal plans are desirable  but the issue of understandability is less obvious. in fact  for some applications  a plan represented as a large table mapping states to actions may be perfectly suitable. however  the lack of clarity has limited the adoption of mdp planning in some application domains such as space exploration with unmanned rovers. in this domain  communication with the agent is restricted and due to high mission costs and associated risks  understanding and verifying plans are crucial. 
　we examine the relationship between policies and contingency plans and define a precise measure of complexity of contingency plans. then  we introduce a technique for automated contingency plan generation and briefly describe our experimental results. finally  we conclude with a summary and future research directions. 
1 formal problem description 
a markov decision process  mdp  with goal states is a tuple  1  a  p  r  s1  g   where s = s1 x       x sn is a factored state space and si is the  finite  domain of feature i. a is a set of actions  p the transition probabilities  r the reward expectations  so a known initial state and g a  possibly empty  set of absorbing terminal states. 
	a policy 	: s 	a is a mapping from states to ac-
tions. following the notation in ilittman et al.  1   a contingency plan for an mdp is a tuple where {v e  is a directed graph with start state associates and action with each node of the graph  labels each edge with a set of states. 
　an interval label descriptor maps state sets to sets of intervals over the feature domains defined as follows: 

　the complexity of a contingency plan does not reflect the computational complexity of constructing it  but is a 
　measure of how hard it is to understand. formally: com-
   abf   alds  where abf is 
the average branching factor of the graph and alds is the average label descriptor size. the size of a single label descriptor is the number of constrained intervals. 
　the value of a policy reflects the expected discounted return when the agent selects action using n  starting from the initial state. it can be computed with standard algorithms such as value iteration. similarly  the value of a contingency plan is the expected discounted return when the agent selects action using p  starting from the initial node. by considering the markov chain induced by the state set s x v  the same methods used to evaluate mdp policies can be used to evaluate a contingency plan. 

1 	poster papers 

1 	automated contingency plan generation 
our approach to automatically generate more understandable 
 less complex  contingency plans relies on solving first the underlying mdp  obtaining an optimal policy  and creating an equivalent initial contingency plan from it: v = 

　by performing a depth-first search over the set of all reachable edges between the state-node pairs in s x v  a reachability analysis can be applied to remove states from label descriptors  edges from nodes and nodes from the graph. 
　since a benefit of interval label descriptors is the ability to capture easily large sets of states and simplify the branch conditions  n-dimensional boxes to describe them more compactly can be generated by merging intervals along the dimensions of the state space. this is in particular very effective if the order of feature values corresponds to some natural ordering. 
　by trading off optimality for clarity  interval descriptors can be merged even if there are a few states that make the merge operator not admissible. 
　most importantly  the node-split operator splits a node of the plan graph into two and adapts the incoming and outgoing edges according to a given partition of the state space. this gives contingency plans their real strength: the current node of the graph then represents partial information about the current state   context information    so less details of the actual state may be needed to make the branch decision on the next step. for instance  with a state set s = {u  v  w  x  y  z  and the partition s  - {u v w}  s1 = {x y z}  the result of a split operator is illustrated below: 

1 	experimental results 
it turns out that combining these operators can significantly reduce the complexity of a contingency plan. this leads to the following search problem: given the initial contingency plan defined above  find a contingency plan with the lowest complexity  not giving up more that a certain fraction e of the optimality of the policy n. unfortunately  two major obstacles make an exhaustive search intractable: the node-split operator is parameterized by a partition of the mdp state space  leading to a large number of possible splits and thus to a large branching factor. in addition  the computational complexity of reachability analysis is fairly high. 
　as a result  we decided to relax the requirement of minimizing complexity and developed a method to combine the operators and automatically generate some understandable plans. performance improvement can be obtained by choosing a partition of the state space  split the nodes according to this partition  perform a reachability analysis and merge the interval descriptors subsequently. we implemented a hillclimbing algorithm that used some domain-specific heuristics for choosing a partition and performed random restarts  leading to an interruptible anytime algorithm that performed well on a simple set of test problems. 
　for instance  in a maze domain with stochastic move actions  the algorithm took advantage of bottlenecks  tunnels and alternative branches. in a simulated planetary rover domain with uncertain action durations  some hints for good split partitions lead to dramatically decreased complexity. the resulting plan instructed the agent to perform certain actions until a resource  one feature of the factored state space  drops below a certain threshold. this was very easy to understand for humans unlike the optimal policy given in the form of a huge lookup-table. 
1 	conclusions 
the main objective of this work has been to find solutions for decision-theoretic planning problems that are optimal  or near-optimal  and also compact and understandable. we defined a measure of the complexity of contingency plans  reflecting their size  branching factor and the size of the label descriptors. the results we gained by experimenting with several plan-transformation operators are encouraging and show the potential of automated generation of understandable contingency plans. 
　there are several interesting directions for future research. first  a better measure of the plan complexity could be developed. second  additional operators could be added. third  the language for representing edge labels could be enriched. finally  methods for finding good split partitions have to be found to create more reliable algorithms. the results reported in this paper provide a good framework for future exploration of these research directions. 
acknowledgments 
support for this work was provided in part by nasa under grant nag-1. any opinions  findings  and conclusions or recommendations expressed in this material are those of the authors and do not reflect the views of nasa. 
