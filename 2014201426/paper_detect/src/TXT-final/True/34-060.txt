 
　　　a program framework has been designed in which the linguistic facts and heuristics necessary for generating fluent natural language can be encoded. the linguistic data is represented in annotated procedures and data structures which are designed to make english translations of already formulated messages given in a primary program's internal representation. the messages must include the program's intentions in saying them  in order to adequately specify the grammatical operations required for a translation. 
　　　the pertinant questions in this research have been: what structure does natural language have that allows it to encode mutifaceted messages; and how must that structure be taken into account in the design of a generation facility for a computer program. 
　　　this paper describes the control and data structures of the design and and their motivation. it is a condensation of my master's thesis  1   to which the reader is retered for further information. work is presently underway on implementing the design in lisp and developing a grammar for use in one or more of the domains given below. 
introduction 
　　　at the present time  there are intelligent  interactive programs under development which will require greater fluency in generating natural language than any current system can offer. three such programs  in particular  are a personal scheduling program  goldstein  1    a programmer's assistant  rich and shrobe  1    and the macsyma advisor  geneserith  1  . 
　　　a characteristic of all these programs is that they wifl employ models of their users - of their habits  and the things they are likely to know in various situations. they will also maintain models of themselves and their intentions as they reason and interact with their users. this will have a large effect on the design of a suitable generating facility for them. 
level of fluency desired 
　　　the sort of conversations we hope these programs will eventually be able to have are typified by the short example below  between a scheduling program  p  playing the part of a 
　　　secretary scheduling appointments for a professor  and a student 
 s . 
 s  1 want to see professor winston sometime in the next few days. 
 p  he's pretty busy this week. can it wait  
 h  no  i need his signature on my petition before friday. 
 p  well  maybe he can squeeze you in tomorrow morning. give me your name and check back in an hour. 
　　　these are very fluent answers by human standards. the wording is colloquial -  pretty busy    squeeze you in  - but at the same time  it deliberately conveys useful information; casual speech can give an impression of flexibility. impressions like this can be as important to the total message as the sentence's prepositional content. similarly  the use of  well  at the beginning of a reply can signal an admission on the part of the speaker that the answer which follows may not be adequate. this realizes a need within the discourse situation that is proper y part of c o n v e r s a t i o n s among people  and should be included in converstattons between computers and people. 
a separate linguistic component 
　　　the total process of generating language involves making a 
　　　large variety of decisions and having available the information on which to base them. the initial urge to speak comes from somewhere to fulfill some need which must then be made more precise. models of the audience  their present knowledge and expectations  must be consulted. the available procedures for putting together an utterance will never be totally adequate and therefore compromises must be reached. finally  the most appropriate linguistic representations must be found and organized  probably requiring more compromise. despite the intricacy of this process  i believe it is both meaningful and profitable to divide it two: decision making which requires cognitive/domain knowledge  versus decisions requiring linguistic knowledge. 
in some early communicating programs  winograd  1   woods 
 1    linguistic and domain knowledge were freely mixed. this was possible because the programs only worked in a very small number of situations where the relevant linguistics could be  built in . however  if  as seems to be the case  an extensive amount of  linguistic reasoning  is required to meet the needs of more sophisticated programs  and people   the the programming difficulties of designing a mixed system become insurmountable   n.b. the more recent systems  goldman  1   and slocum  1   incorporate essentially the same division as my own.  
　　　a possible objection to this compartmentalization is that people don't work that way. a grammar that is organized in a different way than human grammar may have difficulties representing the same rules. people often consider the potential impact on their audience of their use of particular words  or of the ordenng of their phrases. poets  in particular  are certainly as conscious of choosing syntax and meter as they ara of choosing cognitive content. 
　　　when i say that the generation process should be divided in two  i do intend the strongest interpretation: namely  that the  cognition  wifl have totally developed the message to be communicated before any linguistic processing is done. futhermore  the linguistic processing should not change the meaning in any way that the cognitive domain cares about  and the message  once determined  should not be modified. this design will  indeed  not be able to behave as people do on the tasks above. 
　　　however  i believe that computer programs will not be able to motivate such behavior for a long time  and that the division in the design is a useful one for the purpose of current research.  in my thesis   1   i describe how the design might be upgraded to handle increasingly sophisticated human linguistic behavior  and what additional information such behavior would require the cognitive  component  to have.  
the translation process 
　　　in this design  the generation of english utterances in context is seen as involving two  computationally separate components.  am of this work has been done in english  though there should not be any difficulty in applying it in other languages.  since there are two components  there must be a communications channel between them and a language which the cognitive component uses to describe to the linguistics component what it wants said  the use of the term linguistics component  in this paper only refers to generation processes and not to interpretive processes  see  1  . 
　　　
1 
　　　
messages 
　　　before the linguistic component is called in  the main program  for example  1   1   or  1   has as total a picture as it needs of what is to be said. it knows that it wants to mention certain particular entites and certain relations among them  and to achieve   certain effect upon its audience. to communicate with the linguistic component  this information is collected into a data structure  a  message . a message can be viewed as consisting of two sorts of things. 
1. a collection of pointers to the internal objects that are to be talked about. the pointers ere annotated to describe how the objects relate to each other and the message as a whole. 
1. a list of features which characterize the primary program's communicative intent in making that message. 
　　　the following structure is en example of what a message might look like. the word on the left of each pair is the annotation  and the phrases on the right in angle-brackets represent objects in the main program with roughly the meaning of those phrases. 
this message may be translated into the sentence:  professor winston will probably be busy all afternoon . 
the character of the translation 
　　　translation from the internal representation of a computer program to natural language is very much like translating from one language to another  and the same problems arise. basically  the problems are that the same concepts may not exist as primitives in each language  and that the conventions of the target language may require additional information that was not in the source language. translation  therefore  cannot be simply one for one 
　　　what english phrase should be used for a particular element in the program's message will vary as a function of what is in the rest of the message and of what the external context is. to allow these factors to be adequately considered  the translation of an element is carried out by special procedures called  composers  which may take into account a wide variety of phenomena as they do their translation. 
　　　evary concept  name  structure  process  or other entity which the main program might employ in a message will be associated with such a describing procedure. the association might be a direct pointer from a unique name in the program to its composer  or it might be derived by examining  is-a  links or  type  features associated wtth the object  e.g. all things of type  event  might share a composer . composers are run at predetermined points in the translation process and are designed to expect a particular computational/grammatical context at that point. 
　　　the translation to english  then  does not employ a single  unified grammar  such as an atn   1   1  . instead  the grammatical information is distributed among the individual composers. this is in part a matter of programming aesthetics  and in part due to a belief that the attendant increase in modularity and flexibility will make the grammar more tractable and easier to improve. 
control structure 
　　　with the grammar distributed among a very large number of separate processes  the task of coordinating their actions becomes of paramount importance. roughly speaking  the translation process has this character: the intentions and objects in the message will suggest  via their composers  strategies for realizing themselves in english. however  these strategies may be blocked or modified by general linguistic information in  the grammar   or by the effects of decisions made by earlier strategies.  the term  the grammar  refers to the collective information in the composers and their data structures  rather than some central body of constraints.  
　　　the control structures required to implement this process are themselves  very simple  this is because the descriptive apparatatus of the grammar can provide a rich enough description of the situation to direct the actions of the composers and keep them  in effect  from tripping over each other's feet. 
　　　this would not be possible if it were not for the fact  hat natural languages are very complex entities with rich structures. in more concrete terms  this is to say that languages are made up of a relatively large number of types of structures  noun phrases  function words  inflectional endings  modifiers  etc.  and that the possible arrangements of these structures are very highly restricted - only a few combinations are possible. by encoding this information into a system of features and data structures  and then writing a grammar for the composers in terms of that system  a tremendous  implicit coordination should be achieved. 
this coordination does not come automatically of course. 
since situations are defined in terms of features  a composer will recognize where it is by using conditional statements involving those features. the larger the number of possible situations that a particular composer may be run in  the more intricate its conditionals will be  and the harder the composer wilt be for the human designer to write 
　　　part of the job of cutting down on the complexity can be done in the grammar by increasing each feature's descriptive power  and probably adding to their total number . the more information that a given feature codes for  then the more decisions that can be made solely on its basis. an even more effective technique is to closely control when the individual composers will be run. this can provide implicit situational information. also  if it can be arranged that a decision  once made  seldom has to be reconsidered  then a considerable overhead in mechanism and composer code will be saved. 
the requirement of linear order 
　　　one of the fundamental characteristics of natural language is that utterances are necessarily made up of linear strings of words. this requirment is an inescapable fact of the  physics  of natural language and accordingly  it has been given a large role in conveying information. it can realize propositional meaning and rhetorical intent  and permit abbreviations throughout the utterance tor example. 
　　　fortunately for fhe program designer and the linguist  those things that are ordered are not arbitrary clumps of words  but rather structural units  noun phrase  adverb  etc.  with two important characteristics. 
1. they seem  not coincidentally  to describe categories of experience which are natural to us as people and which we will probably want to introduce into our computer programs. 
1. linguistically speaking  they are very modular and can usually be  moved  to several different positions within an utterance to achieve rhetorical effects  and require only minimal  well specified structural changes in each position. 
　　　the majority of the descriptive composers in the lexicon wilt describe their corresponding internal entities with just such coherent grammatical structures. most of the syntactic details of these structures vary with position in the sentence  if these composers can be given a guarantee that the position of their object will not be shifted as the utterance is further developed  there will be a considerable savings in the complexity of individual composers and in the overhead required to manage them. 
two phases 
　　　to provide this quarantee  the translation process is divided into two phases. during the first phase  the message as a whole is examined according to the intentions given for it and the 
annotation for each object that it mentions. a  plan  is selected 
1 
　　　
for it  see below  which embodies the syntactic structure of the ultimate utterance and which has  slots  in it into which the largely  unexpended  objects of the message are transfered. in this phase  all of the elements in the message which will involve ordering conventions in their realization  translation  are found end the plan modified to accomodate them. 
　　　during the second phase  the developed plan  which is essentially a constituent structure tree with the possible positions explicitly labeled  is  walked  from left to right and top down - as it would be spoken - and the objects in it are described by their composers as they are encountered. with the  proto-utterance  represented in its surface structure form during this phase  relationships become apparant which could not otherwise be seen. these include the possibilites for pronominalizing elements  and the actual scope of quantifiers. these can be dealt with by syntactic procedures associated with the features and grammatical units in the plan 
data-directed processing 
　　　in both phases  control is data-directed  lr the first phase  the data structure being interpreted is the message  and in the second  it is the plan that was chosen and filled in during the first phase. this is another source of coordination for the composers since the control of their order of execution is now governed by structures which can be written to be very rich in grammatical information 
　　　let me summarize what has been said so far. this design proposes that a very loose  modular framework can be used in language generation; that it will be most convenient if the domain and audience specialists in the program be allowed to work out their message independently of its ultimate linguistic details; and that when the time comes to consider the linguistics  the process should be viewed as a translation which is performed by a large number of specialist procedures associated with the possible things which may appear in a message. the operation of these specialists can be coordinated implicitly by the grammar and data structures that are developed. 
　　　in the rest of this paper  1 will describe some of operations and structures of this design in more detail 
plans 
　　　all natural language programs and linguistic theories employ  in one form or another  a tree structured constituent analysis of their sentences in terms of the traditional grammatical units. my design is no different  except that i have found it necessary to augment the usual descriptive framework to make it capable of the task at hand. this has resulted in the data structure 1 call a  plan . 
　　　the principle function of plans is to mark the possible positions in a grammatical unit  in terms of a fixed vocabulary of slots auch as  subject    mam verb    post-verb-modifiers   and so on. the slots in a plan are arranged in a fixed order corresponding to the normal english surface structure. for example  if we used the grammar developed by winograd in his shrdlu program  1   the slots in a noun phrase would be as follows. 

　　　since the slots are named  they can be referred to directly from within the grammar  rather than requiring some complicated tree walk and string matching operation as in programmar  1   or in transformational grammars. the grammar can  for example when it is doing verb agreement  ask what is the number of the object in the subject slot  or  when considering using extraposition  ask if the subject will be described using a clause. 
　　　the major motivation for naming the possible positions within a grammatical unit involves more than convenience in writing grammatical rules. the basic operation during the first phase is to insert another element from the message into an established plan. to do this  the composing procedure must know: 1  what positions are open in this plan where it is grammatically leasable to put this element; and 1  if more than one position is available  in what ways do their properties vary so that a reasoned decision can be made as to which one is best in this case. 
　　　when the possible positions in an utterance are marked with unique names  it becomes possible to associate grammatical information with them to use in situations such as above. most of this information will probably reside directly in the relevant composers  but some will be used by functions which mediate the insertion of an object into a slot. 
　　　the function of such mediation is to relieve the composers of the need to know low level syntactic information. the verb group is a prime example. because of the intricacy of its syntax  it will be convenient to have only one slot  vg  in a clause or verb phrase plan  and let a function associated with vg manage a full verb group plan below it. the function determines what sub-slot should be filled  perhaps even changing the actual configuration of the slots  and adjusts the features of the group if necessary. this way  the bulk of the composers no longer need to know about details such as:  if you add a modal verb   would   to a verb group  you have to add a marker to the main verb to inhibit the later morphological expression of tense  
　　　plans are associated with grammatical units  with possibly a separate plan for each set of grammatical features that a unit might have  reflecting the different slots that may be present in each case. by knowing the features of a unit  a composer will know exactly what slots to expect it to have. the next section has examples of how plans are used. 
translating a message 
　　　most of the work in the first phase is done by organizational composers associated with the intentional features on the messages. each such composer will include code which understands the possible annotations that typically are mentioned with such intentions  and which will govern their insertion into a 
　　　plan at the proper time. consider the example message given earlier and repeated here. 

mere  the organizing composer will be associated with the feature  prediction . typically  one element of the message will be most important and is translated first. the others will probably refer to it and may need to be realized inside the plan that it was translated into. in this case  the prime element is the one annotated  proposition   an object of the sort  status of a person . plan selection is done by the descriptive composer for this sort and is guided by further characteristics of the object. the lexicon will record that the type property  busy  must be realized as a predicate adjective. this leads to the following  partially filled in plan. 
  
1 

　　　the rest of the message is transferred by the prediction composer chunk by chunk. predictions are of future events  so  will  is added to the verb group; the  hedge    1% chance  will be realized as an adverb  say   probably   and so it is added to the adverb slot in the verb group; and  time-predicted  is a time modifier to the clause  making it part of the post-sententialmodifiers. with the entire message transfered  the plan looks like this. 
annotating composers 
　　　to properly fit the pointers/objects in a message into a plan  the organizing composer must know what sort of grammatical object they will be. this can not always be directly deduced from the nature of the annotation on the message. for example  in this s e n t e n c e   the  hedge  might well have been an object corresponding to the phrase  unless something comes up   which is a bound clause and would have to go at the end of the posteentential-modifiers. 
　　　the necessary information can be maintained by each descriptive composer as a permanant annotation in the form of a feature list which describes what sort of grammatical unit it constructs. to check this for an object in the message  an organizing composer will look up the object m the lexicon to see what composer will describe it  and then read the annotation on that composer. in this case  the features might be   adverb eventmodifer    versus   clause bound conditional   
an example of a composer 
　　　each of the objects m a message will eventually be described by a general descriptive composer which is keyed to the sort of object that they are  plus additional information associated with the names in each object. as there are  sorts  of objects in a 
　　　program  there will be descriptive composers in the lexicon. some sorts might be reasons  actions  people  appointments  activities  times of the day  and so on. this design makes no restrictions on the possible composers; only that they should reflect what properties objects have in common and common ways that they can be described. individual objects will usually only supply parameters to their composers  but some may be idiosyncratic and instead point to complete words or phrases or to specially tailored composing procedures. 
actions 
　　　actions are things that something does:  making an appointment    evaluating a procedure    defending a chess piece   etc. the function of  the action composer  is to set up the syntactic environment that all actions have in common. in this analysis  actions are realized as verb phrases  with the internal name of the action indicating in the lexicon what the verb should be  and the objects asociated with the name  if any  becoming its syntactic objects in the phrase. 
　　　an example of an action in a likely internal representation might be the following  in a programmer's assistant  
we might see this in an utterance like it is necessary to set the value of switchl to nil before leaving this routine . 
　　　the first thing any composer does when it begins to run is find out where it is. in the above utterance  the location would be in the second phase  with the action-object in the  complement  slot. other slots where actions could occur are  subject   and as the mam-proposition in an answer to a question. with the situation known  the composer might dispatch to a particular block of code which handles that situation  but in this case  the only difference is that complements must have infinitival verbs. this is done by adding a feature to the verb group at the end of the operation. 
　　　all actions yield verb groups  so the composer begins by replacing the pointer in the complement slot with a syntactic node for a verb group. it must then get a plan for this verb group  and fill in the appropriate slots of that plan with the subcomponents of the object. then it is finished and the node and plan are in turn refined by their own composers as the second phase controler walks along the plan to them. 
　　　the information on what plan to use and what transfers to make is part of an object's specific lexical entry. to get at it  the action composer must know what property of the object describes its structure  of course  the programmer must see to it that such a property exists  and then follow tt into the lexicon for a plan and a mapping of properties on the object to slots in the plan. for the example action this entry is given below. 

note that this plan is not so much a grammatical skeleton as a 
variablized english phrase. with such information in the lexicon  the action composer can employ straightforward pattern substitution functions to finish its job. 
the syntactic environment 
　　　the primary operation in the second phase is to describe the chunks of the message that have been embedded in the plan. this is done by walking the plan with a simple controler to run the composer for each object as it is encountered; print out the words given  literally  and ignore any empty slots. since a plan is esentially a constituent structure tree  walking it topdown  from left to right results in words being uncovered  and  spoken   in the same order as would occur if a human were making the utterance. at the same time  parts of the plan further on  which have not yet been walked  retain their unexpanded character  presenting those characteristics which may be important to know in decisions involving the whole utterance while hiding those details that are unimportant. 
　　　this points out that plans can be viewed as providing an environment ihat composers can ask questions of. some questions are easy because their answers are represented directly   what is the transitivity of the main verb    and others are much harder because they must be computed ctrt there any intervening noun groups between me and that previous occurance of me way back 
　　　
1 
　　　
there   - needed for reflexive pronouns . however  in environments with different structures than that of plans  answering such question could become simple. 
　　　such additional environments could be created as a sideeffect of the construction and walking of a plan. because the walking follows the temporal order of the generation of an utterance  it readily marks what the audience can be presumed to know at any given point. 
　　　i have not yet done any work on determining just what such parallel environments should look like. that will come as grammars are written for this design. however  it is clear that they must encode some very subtle aspects of what the audience knows  and should describe the syntactic situation in such a way as to guide pronominalization and  deletion  of later structures. 
pronominalization 
　　　pronominalization is only one instance of a very general phenomena in language which  encourages  the speaker to abbreviate their utterance wherever possible. languages contain conventional structures which themselves mark the relationships that are going on  so that the actual words need not be physically present  e.g. equi-np-deletion:  john is ready to please  - the subject  john  does not need to be repeated with each verb . 
　　　often conventions which allow potential descriptions to be omitted take into account semantic information that the audience is assumed to share. for example consider the sentence  white's knight can take a pawn . what is interesting here is that there is no need to say  ... take a black pawn . presumably  it is what we know about the semantics of  take  in chess games - that its subject and object will be pieces of opposite colors - which has taken effect here. 
every composer describing an object will have to examine the 
 discourse  environment to see if it would be most appropriate to use a pronoun or otherwise cut down on the normal amount of description. 
quantifier scope 
　　　certain relationships become apparent during the second phase that can not be seen at other times. cic very important one is quantifier scope certain accidental misreadings can be generated as the plan is walked  precisely because the individual composers work independently of each other. this can be corrected by introducing  global  syntactic processes associated with the grammatical units  which can  monitor  the activities of the composers and insert corrective patches when necessary. 
　　　situations in the grammar where such accidents are possible must be identified and routines designed for them. then  when any syntactic unit is entered by the second phase controler  a check will first be make for any monitoring routines  which are then run before going on. 
　　　one situation that would be checked for would be that of a verb followed by a conjoined object. the monitor would be associated with the verb group and go to work if it saw that a 
　　　conjoined noun phrase followed. the problem is that the structure   are  not a  and  b    is usually misinterpreted by people as   are not  a and b    with the scope of  not  inadvertently taking in 1 as well. the monitor must watch as the first conjunct ts described  and if it begins with  not   it should patch the construction by copying the  are  after the  and  -   are not a and are b  . a repetoire of such monitors and patches witl be required in the grammar. 
present directions 
　　　the design that i have described here  see  1  for greater detail  represents some contentions about what a very fluent  generation grammar  for english must deal with  and what control and data structures will be convenient to write that grammar in. at this writing  june 1   a lisp implementation of the design is well under way  and it is anticipated that part of a grammar can be completed before the end of the summer. however  until a working grammar exists  and the generator has been interfaced with some primary program  many of the things described in this paper remain contentions which i believe to be true  but which may turn out to be without substance  necessitating possibly drastic changes in the design. in particular  the program has made these assumptions. 
1. that the candidate primary programs will have a sufficiently rich organizing structure that very general composers can be written  cutting down on the bulk of the lexicon  and that associating objects with composers will be a straightforward thing to do. 
1. that the messages constructed by a main program will naturally be translatable without editing. some problems in a message could be patched by the grammar  but others  like too much necessary embedding  could not be fixed without going directly back to the program and  explaining  that some material must be cut  letting the mam program decide what is to be left out. 
1. the proposed grammar depends on having good information at all times  otherwise  the composers may thrash and will continually find themselves in unanticipated situations. this information will be encoded in a system of features and possible plans and slots. it must be possible to devise an adequate grammatical system  or else the resulting inefficiences may swamp the generator. 
1. the grammar will organize linguistic constructions in terms of the reasons why speakers use them. however  the reasons for using the bulk of the constructions in english are poorly understood. it is hoped that a combination of the fact that programs are presently rather simple minded compared to humans  and that initial hunches about the use of those grammatical constructions which are called tor will be close to correct  will make it possible to write a grammar without unmanagable gaps in it. 
some people in a.i. have said that language generation is 
 easy . basically i agree with them. i think that the structure of language is well enough understood that we should be able to have our programs speak in very fluent english without excessive research. as in many things  however  to make a system  easy  to work with seems to require first introducing a rather complicated structuring framework in order to separate out its component influences into managable chunks  and let the messy interfacing details work themselves out  away from our view. 

　　　
1 
