 
there have been many proposals for first-order belief networks  i.e.  where we quantify over individuals  but these typically only let us reason about the individuals that we know about. there are many instances where we have to quantify over all of the individuals in a population. when we do this the population size often matters and we need to reason about all of the members of the population  but not necessarily individually . this paper presents an algorithm to reason about multiple individuals  where we may know particular facts about some of them  but want to treat the others as a group. combining unification with variable elimination lets us reason about classes of individuals without needing to ground out the theory. 
1 introduction 
belief networks or bayesian networks  pearl  1  are a popular representation for independencies amongst random variables. they are  however zeroth-order representations; to reason about multiple individuals  we have to make each property of each individual into a separate node. 
　there have been proposals to allow for ways to lift belief networks to a first order representation. the first tradition 
 breese  1; horsch and poole  1; wellman  breese and goldman  1  essentially allowed for parameterized belief networks  which could be grounded for each individual. the second tradition  poole  1; koller and pfeffer  1; pfeffer  koller  milch  and takusagawa  1  allowed for richer first order probabilistic representations that have belief networks as special cases. in all of these the only individuals assumed to exist are those that we know about. 
　there are many cases where we want to reason about a set of individuals as a group. we'd like avoid explicit reasoning about each individual separately. there was a great advance in theorem proving in the 1s with the invention of of resolution and unification  robinson  1 . the big advance was that doing resolution on clauses that contain free variables implements a potentially unbounded number of resolutions on the grounded representation. the goal of the current work is to allow for similar savings in probabilistic reasoning. 
　probabilistic reasoning is more challenging than logical reasoning for a number of reasons: 
  we have to use all of our information when making a probabilistic inference; new information can change old conclusions. 
  we don't want to double count evidence. if we have some probabilistic information about all people and we use it for one particular individual  say fred  then we can't reuse that information for fred. 
  there are cases where the size of the domain affects the probability. in the example developed below  determining the probability that a person is guilty of a particular crime depends on the population; the population size affects the number of other people who could be guilty. 
　the following example shows why we may need to reason about the individuals we don't know about as well as the individuals we do know about and shows why we need to consider population size. 
example 1 a person in a town was seen committing a crime. 
this person had the same  unusual  hair colour and car colour as joe  both purple  and the person was very tall and we know joe has big feet  and being tall is correlated with having big feet . what is the probability that joe is guilty  we need to model the probabilities of the observations and their dependence  which would lead us to consider belief networks as a representation. the probability joe is guilty also depends on the population. if the population of the town is very small then he is probably guilty  as it is unlikely there is anyone else fitting this description . if the town was a city containing a large number of people then he is probably innocent  as we would expect many other people to also fit this description . 
　this example points to a number of features that have been ignored in first-order probabilistic models. not only do we need to take population sizes into account but we need to be able to reason about the individuals we know about as well as the individuals who we know exist  but we don't know anything particular about. we don't have to ground out our theory and reason about millions of people in a city separately  but we also cannot ignore the relevant information about the people we know something about. 
1 representation 
probabilistic inference 	1 we assume that the domain is represented as a parametrized belief network  where the nodes are parametrized with domain-restricted types  or populations . such an idea has been explored previously |horsch and poole  1; kersting and de raedt  1   and is similar to the plates of buntine   1 . we will not dwell on the semantics in this paper. we just assume that the program means the grounding: the substitution of constants for each individual in the domain of a parameter. 
　this paper is explicitly bayesian. in terms of the first-order probability of halpern   the probabilities are all degrees of belief  not statistical assertions about proportions of a population. we are not quantifying over random individuals  as does bacchus    but over all individuals  i.e.  with standard universal quantification . all of the individuals about which we have the same information have the same probability. it is this property that we exploit computationally. 
　this paper is built on two different traditions  namely that of logic programming and theorem proving on one side and that of probabilistic reasoning on the other side. unfortunately they use the same terms or notations  e.g.   variable    domain for very different things  or at least they are used in very different ways in this paper . in order to avoid confusion  we use neutral terms  but use :he traditions of the different communities as appropriate. 
　a population is a set of individuals. a population corresponds to a domain in logic. the cardinality of the population is called the population size. for example  the population may be the set of people living in vancouver  and the population size is 1 million people. the population size can be finite or infinite. 
　we allow for random variables to be parametrized  where parameters start with an upper case letter and constants start with a lowercase letter. parameters correspond to logical variables  e.g.  as a variable in prolog . all of the parameters are typed with a population. a parametrized random variable is of the form where is a functor  either a function symbol or a predicate symbol  and each is a parameter or a constant. each functor has a set of values called the range of the functor. 
　examples of parametrized random variables are hair colour x   likes x y   likes joe x  or townjeonservativeness  the latter being a functor of no arguments   where hairjcolour  likes and townjeonservativeness are functors. 
　given an assignment of a constant to each parameter  a parametrized random variable represents a random variable. for example  suppose hair colour has range {black  blond  red  grey  purple  orange  none  multicoloured}. then hair colour{sam  is a random variable with domain  black  blond  red  grey  purple  orange  none  multicoloured . thus a parametrized random variable represents a set of random variables  one for each parameter assignment. different parameter assignments for the variables in a parametrized random variable result in different random variables; for example  hair colour jred  is a different random variable to hairjcolour  joe . 
　a parametrized primitive proposition is an expression of the form which means that parametrized random variable has value a parametrized proposition is built from the parametrized primitive propositions using the normal 

figure 1: robbing example parametrized belief network  example 1  using plates and parametrized random variables. 
logical connectives. 
a probabilistic assertion is of the form: 
where 	are parameters  dx are populations  c is a set of inequality 	constraints 	on 	are parametrized propositions using only the parameters 	  /; specifies a probability distribution over a. we omit the corresponding 
syntactic constructs when   
a parametrized belief network consists of a dag where the nodes are parametrized random variables  
an association of a population to each parameter  an assignment of a range to each functor  a set of probability assertions for each node given its parents. 
example 1 to formalize example 	1  	consider the parametrized belief network of figure 1. here we have shown it using the plates1  buntine  1  as well as with parametrized random variables. we assume that the hair-colour of the different people are not independent; they depend on how conservative the town is. associated with this network are conditional probabilistic assertions such as 1: 

　if we knew as background knowledge that sam was an exception and has purple hair with probability 1  we would replace the last probabilistic assertion with: 

p hair colour x =purple conservative  = 1 
   *herc we use plates just as a visual help; the real meaning is as probabilistic assertions. the plates get difficult to interpret when there are complex interactions between the parameters  but we can always interpret these as probabilistic assertions. 

p hair colour sam =purple  = 1 
 we will not use this in our continuing example . 
　a grounding of a parametrized belief network is a belief network that consists of all instances of the parametrized random variables where each parameter is replaced by an individual in the population  or a ground term denoting that individual . 
　intuitively  a parametrized belief network represents a huge belief network where the parametrized random variables are repeated for each individual in the population associated with the parameter for which the constraint is true. the above example  if there were 1 people  would represent a belief network with 1 nodes. 
1 first order variable elimination 
the problem that we consider is: given a parametrized belief network  a set of observations  and a  possibly parametrized  query to determine the conditional probability of the instances of a query given the observations. we assume that the evidence is a conjunction of existentially quantified parametrized primitive propositions. 
　we consider the problem of parametric probabilistic inference in two stages: first  where every parameter that appears in the parents of a node also appears in the node and where the observations are variable-free. in this case  when we ground out the theory  each random variable only has a limited number of parents. in section 1 we present the second case where we have parents that contain extra parameters; in this case we need a way to aggregate over populations. that section also considers existential observations and queries. 
　the algorithm is based on variable elimination  ve   zhang and poole  1   where we eliminate the non-observed nonquery random variables one at a time. for first-order ve  fove  we eliminate all the instances of a functor at once1. 
　there is a strong relationship between this work and lifting in theorem proving  chang and lee  1 : given a ground proof procedure  construct a proof procedure with logical variables  or in our case with parametrized random variables . in general  correctness can be shown by proving that we get the same answer as if we first grounded the theory and then carried out variable elimination. see figure 1. 
1 	parametric factors 
in ve  a factor is the unit of data used during computation. 
a ve factor is a function from a set of random variables into a non-negative real number. the initial factors are the conditional probabilities. the main operations are multiplying factors and summing out random variables from factors. after conditioning on the observed random variables and summing out the non-observed  non-query random variables  we can extract posterior probabilities from the remaining factors by multiplying them and normalizing the remaining factor. 
　in first-order variable elimination  we use a generalization of a factor where we want to treat the many instances of factors 

figure 1: we design fove so that we get the same answer as if we had grounded the representation and carried out variable elimination. 
as a unit. we only instantiate parameters when we need to. in general we reason with all of the individuals  except the ones that we know extra information about  as a unit. 
　a parametric factor or parfactor is a triple where c is a set of constraints on parameters  v is a set of parametrized random variables and ms a table representing a factor from the random variables to the non-negative reals. 
　intuitively the parametric factor represents all of the ground instances of the factor where the instantiation of the parameters satisfies the constraints. 
example 1 a parametric factor that represents one of the conditional probability tables of figure 1 is: 

where t is the table that represents a function from hair colour and conservativeness into non-negative numbers  t is not indexed by x. t looks like: 
hairjcolour conservativeness val purple purple blue blue conservative liberal conservative liberal 1 
1 
1 
1 　when there are two instances of the same parametrized random variable in v; in this case we need to mark the random variable in the table to distinguish these instances. example 1 suppose that whether person x is a friend of y depends on whether x likes y and whether y likes x. there are two distinct cases here  the first is when x = y  in which case 
friends{x  y  has one parent  and the second is when 
in which case friends x  y  has two parents. to represent this  we could use 

where the subscripts in likes1 and likesn represent different instances of likes in table t1- that is  is a factor on friends  likesi and likesi. when we eliminate all of the likes relationships  we have to consider likes  and ukes1. 
1 splitting 
the foundation of parametric variable elimination is the splitting operation. splitting plays the analogous role to applying substitutions in theorem proving  except that we not only have to be concerned about the instance created  but also about the instances left over. 

probabilistic inference 	1 

definition 1 suppose parametric factor contains parameter x. a split of on x = w h e r e y is either a 
constant or another parameter  and c does not contain results in the two parametric factors: 
where is the same as v  but with y replacing every instance of x  i.e.  we substitute for x . the second parametric factor is called a residual parametric factor. 
a substitution is of the form where the are distinct variables and the are terms. we assume that all 
substitutions are in normal form: does not contain for any and the substitutions resulting from standard unification algorithms are in normal form  chang and lee  1 . if a substitution is in normal form  we get the same result from replacing each x  by the corresponding f  sequentially  in any order  or in parallel. 
　instead of just applying substitutions as in normal theorem proving  we need to split. the general idea is that whenever applying a substitution to a parfactor restricts the set of ground instances  we need to split the parfactor. we don't need to split when just renaming variables or substituting a value for a variable that doesn't appear in the parfactor. 
　we can split a parfactor  on substitution by totally ordering the elements of the substitution1  by carrying out the following procedure which results in a final instance of and a set of residual parfactors: for i from 1 to k 
note that the final value of 	is the same as if we had applied the substitution to   but we also create residuals. 
　there is a close relationship between the splitting on equality in this paper and the splitting on the value of a variable in contextual variable elimination fpoole and zhang  1 . 
1 	observations 
when we observe a ground value  we carry out the analogous operation to ve. we project the tables onto the observed values. however we must first split to ensure that we only affect the appropriate ground variables. 
example 1 if we condition on the fact that joe has purple hair  a purple car  and a shoe size of 1  we now reason separately about joe than we do about the other individuals who can be treated as a group. 
　the parametric factor of example 1 becomes the two parametric factors: 

1 	multiplying parametric factors 
in ve  when we eliminate a variable  we multiply all of the factors that contain that variable then sum out the variable from the resultant factor. 
　as in variable elimination  we need to multiply the factors that contain the variable to be eliminated. in parametric variable elimination  given two parametric factors  some instances may need to be multiplied  and some instances may not. also  the dimension of the resulting factors can be different for different instances. 
　the product of two parametric factors  in general  results in a set of parametric factors. intuitively  we keep splitting the parametric factors  and renaming parameters  until we can guarantee that we get parametric factors that  if grounded  result in the same factors as if we were to first ground the factors and then multiply. 
determining which parfactors to multiply 
suppose there are two parfactors 	and 
with variables renamed to be differ-
ent  wherepi and/ 1 are unifiable instances of 
where is to be eliminated. let we will split on and split on putting all the residuals in the set of all parfactors. all instances of the resulting non-residual 
parfactors would be multiplied in ve. 
　the following abstract example is designed to show what needs to be considered when multiplying parfactors. it isn't meant to be meaningful. 
example 1 suppose we were to eliminate p and multiply the two parametric factors: 
 1  
　　　 1  if we were to ground the parameters some of the instances of these would be multiplied in ve and some of them wouldn't. unification finds the most general instances that are identical. resulting in the substitution 
 1  
    1  parametric factor  1  is a residual parametric factor. no instance of parametric factor  1  ever needs to be multiplied by any instance of parametric factor  1  when eliminating and doesn't participate further in the product. 
similarly  we can split  1  on 1 resulting in: 
  1  
  1  parametric factor  1  is a residual factor and doesn't participate further in the elimination of p. all instances of  1  and  1  would be multiplied together when eliminating p b  a  in variable elimination. 
determining the dimensionality of the product 
in example 1  all instances of parametric factors  1  and  1  would be multiplied if we were to ground parametric factors  1  and  1  and carry out ve. however  not all of the product factors have the same dimension; some have two different instances  and some have one. we need to do more splitting to ensure that all of the products have the form of parametric factors. 
　suppose we have parfactors that need to be multiplied. if for all and either / ' and p  are identical or non-unifiable or if mgu p  p   is incompatible with the constraints  we know that all instances of their product has the same dimension. if there is a p'  and p v  that are not identical but unify with mgu consistent with c' and c   we can split and on mgu p  /   . the resulting instances and residuals either have identical instances of// and/   or non-unifiable instances. we then multiply each instance created from by each instance created from 1 . 
when we know all the instances have the same dimension  we create the parfactor where is the product of the 
tables for where we maintain one dimension for each member of thus  those members in common in v and are treated as the same variable in the product  but those members that don't unify  even with the same functor  in and are treated as different variables in the product table. 
example 1 when we need to multiple parfactors  1  and 
 1   we notice that 	and 	unify  with unifier 
　　　　　　we thus split parametric factor  1  on and rename w to k  replacing  1  with: 
 1  
 1  
we can now multiply parametric factors  1  and  1  producing: 
 1  
where is the product of factors. parametric factor  1  represents all of the factors of dimension four created by multiplying factors that are instances of parametric factors  1  and  1 . 
we also multiply parametric factors  1  and  1  producing: 
 1  
where is the factor   but with abelled as q  and is the factor but with labelled as thus is a factor on parametric factor  1  represents all of the factors of dimension five created by multiplying factors that are instances of parametric factors  1  and  1 . 
　while this may seem very complicated  remember that parametric factor  1  represents mr m - l 1 factors  assuming that all populations have size  even if is 1  this is 1 factors. 
　when we have multiplied all of the appropriate parfactors we are ready to sum out the variables being eliminated. we must remember that when we are eliminating p we are not just eliminating one random variable  but we are eliminating a number of variables equal to the product of the population sizes of  if some parameters only appear in the parametrized random variable that is being eliminated  in the grounding we are multiplying the number of factors equal to the effective population size of those parameters. 
the effective population size is the product of the populations of the parameters less the number excluded by the constraints. thus we have to take each element in the table and put it to the power of the effective population size. 
1 	aggregation over populations 
when we have parameters in the parents of a node and not in the node itself  the number of parents grows with the population size  and we need to specify how a node is a function of its parents. there are many possibilities  such as a node being the  logical or  of it's parents  the  logical and   the max of its parents  the average if its parents  true iff greater than k of its parents are true  or the vote of its parents  according to some voting scheme  for example when the majority wins   or some other function. zhang and poole  give an analysis where there is an arbitrary associative and commutative operator between the parents. in order to make the presentation simpler  in this paper we assume that the operator is a logical  or   diez and galan  1 . it is straightforward to use the techniques of zhang and poole  to extend this to other operators. 

probabilistic inference 	1 

　we can transform the problem into one of the form with parents where is the  logical or   over all of the values. 
　to eliminate   we multiply all of the compatible clauses that contain and as we eliminate each  we accumulate the probability over the   as in  diez and galan  1; zhang and poole  1  . the only difference is when there is a free  perhaps with inequality constraints  parameter in the 
 in this case we need to do the  or  over the effective population size. we can determine the effective population size by determining the effective population size of each free parameter by subtracting the number of excluded values from the population  e.g.  if we have and with a population size of 1  we have an effective population size of 1=1  and multiplying by the effective populations of all of the free parameters. 
　if we have an effective population size of and each one contributes a probability of then  assuming they are all independent  the probability that they are all false is so the probability that at least one is true  i.e.  the logical  or   is 1 - if the effective population size is countably infinite  the probability that at least one is true is 1 if 1 and is 1 if 
　however  we cannot assume the instances of the q's are independent. they are only dependent if they either have  1  a common ancestor in the grounding or  1  a common observed descendent. if we condition on the observation after eliminating the we can get around the second condition. the only way that the can have a common ancestor is if there is an ancestor that doesn't involve one of the parameters. if we make sure that we eliminate the before we eliminate the common ancestors that separate the other common ancestors  then we can just use the equation above that assumes independence. effectively we are doing the logical  or  for each value of the ancestors  and we know they are independent for each value of the ancestors. 
　if we have an existential observation or query  e.g.  conditioning on the fact that someone who fits a certain description is guilty   we need to construct the  or  and either condition on or query the resulting node. 
example 1 let's return to example 1  see figure 1 . suppose that as well as observing the hair-colour  car-colour  and shoe size of joe  we also observed that there exists a person who is guilty and fits a certain description of hair-colour  car-colour  height. this is depicted in figure 1. 
　suppose we want to compute whether joe is guilty given joe's hair-colour  car-colour and shoe size and given the witness description. we can first instantiate all of the observed random variables except the witness observation. this splits off joe as a special case. we can now eliminate all of the random variables except for townjconservativeness  guilty  joe   descn{x   for x = joe and and witness. this results in two parametric factors: 
we 
the instances are independent given conservativeness. this 

 figure 1: robbing example with the witness observation. results in the parametric factor: 

we can now sum out conservativeness and condition on witness and end up with a parametric factor 

we can now determine the probability that joe is guilty from normalising t1. if we were to carry out this computation leaving the population as a parameter  the probability of guilty  joe  can be computed as a ftmction of the population. we get a result that looks like figure 1. the graph has this shape because it is the linear combination of exponential functions  for each value of townjoonservativeness we have an exponential distribution . 
1 conclusion 
this paper has made three main contributions: 
  a way to do inference over populations without grounding out the theory and a way to use unification  where as well as the unifiers we also need to take into account the residuals; 
  the idea that we need to take population size into account when we have aggregation over populations either as part of the model or as an observation; and 
  a way to handle existential observations where we know someone exists but don't know who it is. 
this paper extends the inference in object-oriented bayesian networks  pfeffer et al.  1  where the reasoning with 


figure 1: probability of guilty joe  for various populations. 
generic class models corresponds to reasoning with free parameters in this paper. they do not use the power of unification as in this paper. 
　this paper contradicts the pessimistic conclusions of jaeger 1   but not the results. while in the worst case we may effectively ground the representation  in many cases we can do much better. how much we save in practice is still an open question. 
there are still a number of open questions: 
  what are good elimination orderings  we don't have to eliminate all instances of a functor at once. 
  how to utilize other combination rules  apart from  or  . while the description here was in terms of noisy-or  diez and galdn  1  or inter-causal independencies  zhang and poole  1   the actual use of splitting is much closer to the work on contextual independence  poole and zhang  1 . 
  we are interested in combining the work in this paper with richer languages  e.g.  poole   1    where much of the power comes from mixing contextual independence and free variables and allowing for function symbols and recursion. 
  it is also interesting to think about combining this with mcmc  pasula and russell  1   however it may not be straightforward because we are representing a set of individuals as a unit  and so are completely dependent   which may cause problems when we separate one individual from the class and reason about that individual separately. 
acknowledgements 
this work was supported by nserc research grant og-
p1 and the institute for robotics and intelligent systems. thanks to valerie mcrae and mark paskin for valuable comments. 
