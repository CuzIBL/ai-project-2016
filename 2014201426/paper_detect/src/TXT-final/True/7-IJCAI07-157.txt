
the main objective of this paper is to present and evaluate a method that helps to calibrate the parameters of an evolutionary algorithm in a systematic and semi-automated manner. the method for relevance estimation and value calibration of ea parameters  revac  is empirically evaluated in two different ways. first  we use abstract test cases reflecting the typical properties of ea parameter spaces. here we observe that revac is able to approximate the exact  hand-coded  relevance of parameters and it works robustly with measurement noise that is highly variable and not normally distributed. second  we use revac for calibrating gas for a number of common objective functions. here we obtain a common sense validation  revac finds mutation rate pm much more sensitive than crossover rate pc and it recommends intuitively sound values: pm between 1 and 1  and 1 ＋ pc ＋ 1.
1 introduction
appropriate calibration of evolutionary algorithm  ea  parameters is essential to good ea performance. nevertheless  current practice in ea calibration is based on ill-justified conventions and ad hoc methods. without exaggeration  one could state that one of the canonical design problems in evolutionary computing  ec is: how to choose operators and parameters for an evolutionary algorithm to ensure good performance  the related questions can concern relevance of operators and parameters as well as the values of  relevant  parameters. for instance  the question whether crossover is a relevant operator is still open  or rather  the answer depends on the application at hand  eiben and smith  1 . relevance of parameters is also an issue  e.g.  tournament size can be a highly relevant parameter whose value must be chosen well for good performance  while mutation rate could be less relevant in the sense that its values do not affect ea performance too much.1
　in contemporary practice  ea parameters are set by common  wisdom   e.g.  mutation size should be 1 divided by the chromosome length  or by statistical hypothesis testing  e.g.  comparing ea performance for a number of different setups  eiben et al.  1 . this is typically a laborious  ad hoc procedure involving much handwork and heuristic choices. so far  no procedure has yet been established that can do this systematically  exploring the full combinatorial range of possible parameter settings. another problem is that studies on design aspects like confidence intervals for good parameter values and a sensitivity analysis for parameter robustness are scarce. in this paper we discuss a design method that calibrates the parameters of an ea in a robust way  regardless of the distribution of the results. for each parameter the method produces a distribution over the parameter's range that give high probability to values leading to good ea performance. in this scheme  a distribution with a narrow peak indicates a highly relevant parameter  whose values largely influence ea performance   while a broad plateau belongs to a moderately relevant parameter  whose values do not matter too much .
　related work includes meta-gas as an early attempt to automate ga calibration  greffenstette  1   and  eiben et al.  1  that established parameter control in eas as one of the big challenges in ec. czarn et al.  discuss current problems in ea design and use polynomial models of a response curve to estimate confidence interval for parameter values. franc ois and lavergne  estimate response curves for eas across multiple test cases to measure generalizability. the groundwork of statistical experimental design was laid by r.a. fisher in the 1s and 1s. response surfaces methods that exploit sequential sampling were introduced in box and wilson . a paradigm shift that emphasizes parameter robustness is due to g. taguchi . a survey of optimization by building probabilistic models is given in  pelikan et al.  1 .
1 the revac method
the revac method is based on information theory to measure parameter relevance. instead of estimating the performance of an ea for different parameter values or ranges of values1  the method estimates the expected performance when parameter values are chosen from a probability density distribution c with maximized shannon entropy. this maximized shannon entropy is a measure of parameter relevance. in information theoretical terms we measure how much information is needed to reach a certain level of performance  and how this information is distributed over the different parameters. in these terms the objectives of the revac method can be formulated as follows:
  the entropy of the distribution c is as high as possible for a given performance
  the expected performance of the ea in question is as high as possible
　technically  the revac method is an estimation of distribution algorithm  eda   pelikan et al.  1  that is specifically designed to measure maximized entropy in the continuous domain. given an ea with k  strategy  parameters the revac method iteratively refines a joint distribution
over possible parameter vectors. beginning with a uniform distribution c1 over the initial parameter space x  the revac method gives a higher and higher probability to regions of x that increase the expected performance of the associated ea. this is to increase the ea performance. on the other hand  the revac method maximizes entropy by smoothing the distribution c. for a good understanding of how revac works it is helpful to distinguish two views on a set of parameter vectors as shown in table 1. taking a horizontal view on the table  a row is a parameter vector and we can see the table as m of such vectors.
taking the vertical view on the columns  column i shows m values for parameter i from its domain. these values naturally define1 a marginal density function d x  over the domain of parameter i  hence we can see the k columns of the table as k marginal density functions x = {d1 ... dk}.
d1，，，di，，，dk，，，，，，......{x1j，，，xij，，，xkj}......{x1m，，，xim，，，xkm}table 1: a table x of m vectors of k parameters
　roughly speaking  revac works by iteratively improving an initial table x1 that was drawn from the uniform distribution over x. creating a new table xt+1 from a given xt can be described from both the horizontal and the vertical perspective.
　looking at the table from the horizontal perspectivewe can identify two basic steps:
1. evaluating parameter vectors: given a parametervector x we can evaluate it: the utility of x is the performance of the ea executed with these parameter values.
1. generating parameter vectors: given a set of parameter vectors with known utility we can generate new ones that have higher expected utility.
　step 1 is straightforward  let us only note that we call the performance that an ea achieves on a problem using parameters x the response. response r is thus a function ; the surface of this functionis called a response surface. as for step 1  we use a method that is evolutionary itself   but should not be confused with the ea we are calibrating . we work with a population of m parameter vectors. a new population is created by selecting n   m parent vectors from the current population  recombining and mutating the selected parents to obtain a child vector  replacing one vector of the population.
　we use a deterministic choice for parent selection as well as for survivor selection. the best n vectors of the population are selected to become the parents of the new child vector  which always replaces the oldest vector in the population. only one vector is replaced in every generation. recombination is performed by a multi-parent crossover operator  uniform scanning  that creates one child from n parents  cf.  eiben and smith  1 . the mutation operator-applied to the offspring created by recombination-is rather complicated  it works independently on each parameter i in two steps. first  a mutation interval is calculated  then a random value is chosen from this interval. to define the mutation interval for mutating a given xij all other values for this parameter in the selected parents are also taken into account. after sorting them in increasing order  the begin point of the interval can be specified as the h-th lower neighbor of xij  while the end point of the interval is the h-th upper neighbor of xij. the new value is drawn from this interval with a uniform distribution.
　from the vertical perspective we consider each iteration as constructing k new marginal density functions from the old set xt = {dt1 ... dtk}. roughly speaking  new distributions are built on estimates of the response surface that were sampled with previous density functions  each iteration giving a higher probability to regions of the response surface with higher response levels. each density functions is constructed from n uniform distributions over overlapping intervals. in this context  the rationale behind the complicated mutation operator is that it heavily smoothes the density functions. like all evolutionary algorithms  revac is susceptible for convergingon a local maximum. by consistently smoothing the distribution functions we force it to converge on a maximum that lies on a broad hill  yielding robust solutions with broad confidence intervals. but smoothing does more: it allows revac to operate under very noise conditions  it allows revac to readjust and relax marginal distributions when parameters are interactive and the response surface has curvedridges  and it maximizesthe entropy of the constructed distribution. smoothing is achieved by taking not the nearest neighbor but the h-th neighbors of xij when defining the mutation interval1. choosing a good value for h is an important aspect when using the revac method. a large h value can slow down convergence to the point of stagnation. a small h value can produce unreliable results. we prefer h 「 n/1.
　because the revac method is implemented as a sequence of distributions with slowly decreasing shannon entropy  we can use the shannon entropy of these distributions to estimate the minimum amount of information needed to reach a target performancelevel. we can also measure how this information is distributed over the parameters  resulting in a straightforward measure for parameter relevance. this measure can be used in several ways. first  it can be used to choose between different operators  nannen and eiben  1 . an operator that needs little information to be tuned is more fault tolerant in the implementation  easier to calibrate and robuster against changes to the problem definition. second  it can be used to identify the critical parts of an ea. for this we calculate relevance as the normalized entropy  i.e.  the fraction of total information invested in the particular parameter. when an ea needs to be adapted from one problem to another  relevant parameters need the most attention. with this knowledge  the practitioner can concentrate on the critical components straight away. third  it can be used to define confidence intervals for parameterchoices. given a distribution that peaks out in a region of high probability  except for the early stage of the algorithms the marginaldistributions have only one peak   we give the 1th and the 1th percentile of the distribution as a confidence interval for the parameter. that is  every value from this range leads to a high expected performance  under the condition that the other parameters are also chosen from their respective confidence interval.
　further documentation  a matlab implementation and graphical demonstrations are available on the web sites of the authors: http://www.few.vu.nl/゛volker/revac and http:/www.few.vu.nl/゛gusz
1 empirical assessment of revac
for an elaboration on how to evaluate the revac method method we can distinguish 1 layers in using an ea:
application layer the problem s  to solve.
algorithm layer the ea with its parameters operating on objects from the application layer  candidate solutions of the problem to solve .
design layer the revac method operating on objects from the algorithm layer  parameters of the ea to calibrate .
　a real in vivo assessment requires that we select a problem  or a set of problems  and execute real runs of an ea on this problem s  for each evaluation of given parameter vectors. this can lead to excessive running times for revac and would deliver information on how revac works on the given problem  s . as a second stage  we would need to make a generalization step to claim something about how revac works in general. alternatively  we can make the generalization step first and create an abstract response surface that is representative for ea parameter spaces. hereby we abstract from the application and algorithm layers and reduce run times enormously. either way  there is a generalization step involved.
　in this paper we perform both types of experiments  but put the emphasis on the second option. to define abstract response surfaces resembling the response surface of a typical ea we identify five essential properties:
1. low dimensionality: the number of parameters to calibrate is in the order of magnitude of 1.
1. non-linearity  epistasis  cross-coupling : parameters interact in non-linear ways  implying that the response curves are non-separable  values for one parameter depend on the values of other parameters.
1. smoothness: small changes in parameter values lead tosmall changes in ea performance.
1. low multi-modality  moderate ruggedness : the response surface has one or few local optima  i.e.  few regions of the parameter space have high ea performance.
1. noise: depending on the application  the performanceof a ga can be highly variable and can follow a distribution that is far from the normal distribution.
in section 1 we present experiments on abstract response surfaces having these five properties. thereafter  in section 1 results obtained with concrete objective functions are given. in all cases revac is used with a population of m = 1 parameter vectors  from which the best n = 1 are selected for being a parent. we smooth by extending the mutation interval over the h = 1 upper and lower neighbors. in all experiments  revac is allowed to perform 1 evaluations. for a test with a concrete objective function f this means 1 runs of the ea on f  evaluating 1 different parameter vectors .
1 results on abstract ea response surfaces
we present three experiments: two on the accuracy of the relevanceestimation  without noise  and one on the resistance of the method to noise. we use k = 1 parameters that can take values from the range  1 .
　experiment 1: hierarchical dependency. in this experiment we pre-define and hard-code the dependency between parameters. the response  showing the utility of x  is calculated as the sum over the responses per parameter. for the first parameter value x1 the response r1 is equal to 1 minus the distance to a target value  randomly chosen at the start of the experiment. the response of each consecutive parameter value is ri = ri 1   |xi   xi 1|  （  1 . the response of parameter i is higher if its value is closer to that of parameter i   1. but because it is multiplied by the response of that parameter  we have hierarchical dependencies. the better the first parameter is calibrated  the more the calibration of the second parameter can contribute  and so forth. knowledge of such a hierarchy of dependencyand the associated hierarchy in relevance is very useful for the practitioner  and we are interested if the revac method can reveal it.
	relevance per parameter	response / performance	entropy of parameter 1  1  1
	1	1	1	1	1	1	1	1	1	1
figure 1: utility and relevance of parameters. the left graph shows the relevance  normalized entropy  per parameter at the end  after 1 revac evaluations. the middle graph shows how the measured response  parameter utility as the abstract ea performance  increases through the calibration process. the rightmost graph shows the development of entropy for parameters 1  1  and 1. for both  the x-axis shows evaluations 1  the first 1 concern the initial population .
	calibrating parameter 1	calibrating parameter 1	calibrating parameter 1
1 	1 	1 
1.1
1 1 
	1	1	1	1	1	1	1	1
figure 1: calibration of parameters illustrated. the y-axis shows the value for the 1th  1th and 1th percentile of the distribution. the calibration effect is achieved by taking parameter values from the range between the 1th and 1th percentile. note that the length of this confidence interval indicates parameter relevance: narrow/broad = highly/moderately relevant. here　figures 1 and 1 show the results for one run of revac  1 evaluations . the main quality indicator for revac is parameter 1 shows highly relevant  parameter 1 does not.
the leftmost histogram in figure 1 that shows how the relevance estimates for consecutive parameters approximate the predefined linear relationship. the rightmost graph of figure 1 and the three graphs of figure 1 show various aspects of the calibration process for parameters 1  1  and 1.
　experiment 1: predefined relevance distribution. in this experiment we control the response contribution of each parameter directly to see whether the revac method can reveal arbitrary relationships in relevance. to this end we create a response curve by placing 1 peak randomly in the 1dimensional unit space. again  total response is a sum over individual responses  which is 1 minus the distance to this peak per parameter i.e.  hamming distance . we weight each distance with a predefined target relevance. in this way  an irrelevant parameter has no contribution to the response  no matter how close its value is to the peak. on the other hand  the higher the target relevance of a parameter  the more its calibration contributes to the overall response.
　using three sets of weights we specify three predefined relevance distributions. these are given together with the experimental results in figure 1  cf. black vs. white columns. the shown outcomes are from single runs of revac  1 evaluations . the accuracy of results can be improvedby averaging over the results of several runs. as can be seen in figure 1  the revac method can reproducea predefined relevancedistribution quite accurately. the relationship in the right graph of figure 1 is the most typical situation when calibrating real eas  also known as the sparcity of effects principle: relevance is concentrated on only a few parameters. as can be seen  the revac method can identify the more relevant ones and also reproduce the correct order in relevance. however  it does not reproduce the fact that the most relevant parameter is twice as relevant as the follow up. this can not be repaired by averaging the results of several runs of revac.
　experiment 1: measurement noise. in this experiment we study how the reliability of a relevance estimation changes with noise of increasing variance. to measure the quality of the estimate we use the mean squared error between a relevance estimate to the target distribution  which we plot against the variance of the noise.
　the abstract response surface we use here is the third one  non-linear increase  from experiment 1  see figure 1  right. addition of noise turns the abstract response curve into a stochastic equation  where η is an i.i.d. random value. the noise is drawn from a pareto distribution
	p x   x  = cx γ 	x   logγ c
with exponent γ = 1. c is a normalization value and also controls the variance σ1 of the noise. such a distribution is also called a power law distribution and can be found in many physical and social systems. it has frequent outliers  a slowly convergent variance  and is generally incompatible with statistical methods that require a normal distribution.
　the variance of i.i.d. noise can be reduced by averaging over multiple evaluations. the revac method however does not average over multiple evaluations. rather  if allowed more evaluations  it uses them to get a better coverof the sampling space. noise effects are only treated during the smoothing of the distributions.
	parameter 1 and 1 are irrelevant	linear increase in relevance	non-linear increase in relevance

	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1
figure 1: comparing the relevance estimation of the revac method  white  to the hard-coded target relevance  black .

figure 1: impact of noise. the left graph shows the mean squared error of the relevance estimate as a function of the variance of the noise. the x-axis shows the increasing variance  from 1 to 1.  1 is very high for a function with a range between 1 and 1 . the y-axis shows the results of 1 different runs of the revac method over these noise levels.  the expected mean squared error of a completely random relevance estimate is 1.  the middle graph shows the estimate voor σ1 = 1. the right graph is averaged over the last 1 evaluations. note how averaging over multiple runs of revac recovers the main features of the target distribution  black columns in the rightmost graph of figure 1  right above it .　figure 1 shows the results of this experiment. as we increase the variance of the additive noise term  the mean squared error of the revac estimate increases roughly linearly with the variance. this error can be reduced by averaging over multiple runs of revac. that means that under noisy conditions revac can give a quick first approximation that can be refined by averaging over repeated calibrations. this leads to an effective use of all evaluations. the fact that the noise follows a non-normal distribution does not form a significant obstacle.
1 results on concrete objective functions
here we present the outcomes of in vivo experiments  as discussed in section 1. to this end we need to define an ea and some specific objective functions. for both purposes we rely on  czarn et al.  1  who apply rigorous statistical exploratory analysis to a simple ga to study the effect of calibrating the mutation and crossover operators. the ea here is a generational ga with 1 bits per variable  gray coding  probabilistic ranked-based selection  single point crossover with pc  bit flip mutation with pm  and a population of 1 chromosomes. the four test functions are rather standard: sphere  saddle  step  schaffer's f1. our results are summarized in table 1 and figure 1. these can be considered from two perspectives  compared with the  usual  ga settings  and with the work of czarn et al. as table 1 shows the values found by revac are consistent with the conventions in ec: pm between 1 and 1  and pc between 1 and 1.
 1 
 1 
 1 
 1 
table 1: revac results after 1 evaluations.
optimumconfidenceentropyrele-valueintervalvancef1 pm11 - 1-11pc11 - 1-11f1 pm11 - 1-11pc11 - 1-11f1 pm11 - 1-11pc11 - 1-11f1 pm11 - 1-11pc11 - 1-11	entropy	final pdf: mutation	final pdf: crossover
1	1	1	1	1	1	1	1	1	1	1	1	1
figure 1: calibrating schaffer's f1. the left graph shows the development of entropy for pm and pc. the entropy of pm shows significant decrease  indicating its relevance; that of pc does not. the other two graphs show the final probability density function  pdf  over the parameter values. note the extremely sharp needle  high sensitivity  for pm.　a direct comparison with  czarn et al.  1  is difficult  because of the different types of outcomes. as for the method  they use screening experiments to narrow down the space of feasible parameter settings  partition this space into equally spaced discrete levels  repeatedly measure the performance for each level and use anova to calculate the significance of mutation and crossover rates. then they procede to approximate the response curve for both parameters by fitting low order polynomials to the performancemeasures  suggesting to calculate confidence intervals from these approximations. as a main result  czarn et al find that the marginal response curves for crossover and mutation are linear and quadraticand that mutationis more significant than crossover. by contrast  the revac method uses no screening and does not partition the parameter space into discrete levels. it studies the complete and continuous parameter space. we can narrow the solution space to any arbitrarily small subspace and can directly read off confidence intervals for any given level of performance. our global outcomes  however  are in line with those in  czarn et al.  1 : pm is much more peaked and relevant than pc.
1 conclusions
in this paper we propose and evaluate a specialized estimation of distribution algorithm  eda  that estimates parameter relevance by normalized shannon entropy. the method finds a distribution  marginal density function  for each ea parameter simultaneously such that parameter values yielding higher ea performance have higher probabilities. the method shows the relevance of a parameter graphically by the width of the peak s : a distribution with a narrow peak indicates a highly relevant parameter  whose values have great influence on ea performance   while a broad plateau belongs to a moderately relevant parameter  whose values do not matter too much . parameter calibration is achieved in a straightforward way  the values with a high probability are the ones to be used. in particular  for each parameter we choose choose the 1th percentile as the robust optimum and the 1th and 1th percentiles as the confidence interval.
　our empirical validation is twofold. first  we use abstract test cases representing typical response surfaces of ea parameter spaces and observe that estimates reproduce the target values to a satisfiable degree. we also show that revac can work with significant levels of highly variable noise. the error of relevance estimates increases roughly linearly with the variance of the noise and can be reduced by averaging over several estimates. second  we use revac to calibrate a ga on four common test functions and find that the recommended values are much in line with the usual settings   optimized  by collective experience of the field.
　ongoing research is carried out along two lines. the first one concerns real-life testing  where we use the method to evaluate policies in complex economic simulations. the second one is a comparativestudy  where we calibrate genetic algorithms on a number of objective functions by revac and compare it to a meta-ga  greffenstette  1 .
