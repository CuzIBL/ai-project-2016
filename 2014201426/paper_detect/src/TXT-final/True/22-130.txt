 
ideally  definitions induced from examples should consist of all  and only  disjuncts that are meaningful  e.g.  as measured by a statistical significance test  and have a low error rate. existing inductive systems create definitions that are ideal with regard to large dis-
juncts  but far from ideal with regard to small disjuncts  where a small  large  disjunct is one that correctly classifies few  many  training examples. the problem with small disjuncts is that many of them have high rates of misclassification  and it is difficult to eliminate the errorprone small disjuncts from a definition without adversely affecting other disjuncts in the definition. various approaches to this problem are evaluated  including the novel approach of using a bias different than the  maximum generality  bias. this approach  and some others  prove partly successful  but the problem of small disjuncts remains open. 
1 	the problem of small disjuncts 
systems that learn from examples do not usually succeed in creating a purely conjunctive definition for each concept. instead  they create a definition that consists of several disjuncts  where each disjunct is a conjunctive definition of a subconcept of the original concept. table 1  column 1  shows the number of disjuncts in definitions induced by several different systems. 
　the  coverage  of a disjunct is defined as the number of training examples it correctly classifies. a disjunct is called  small  if its coverage is low. table 1  column 1  shows the coverage of disjuncts in induced definitions. 
　there are several reasons for paying special attention to the methods by which small disjuncts are created. first  many concepts include rare or exceptional cases and it is desirable for induced definitions to cover these cases  even if they can only be covered by augmenting 
   'support for this research was provided by the army research office under grant aro-daag1-k-1 and the national science foundation under grants iri-1 and iri-1. 
   *this research conducted while visiting the university of texas at austin department of computer sciences. 
problem of small disjuncts* 
liane e. acker and bruce w. porter 
department of computer sciences 
university of texas at austin 
austin  texas 1 
the definitions with small disjuncts. secondly  small disjuncts constitute a significant portion of an induced definition  in the sense that often they collectively match more than 1% of the examples that satisfy a definition. 
　the problem with small disjuncts  and the main reason for reviewing the methods by which they are created  is that they are much more error prone than large dis-
juncts. table 1 illustrates this phenomenon with the definitions created by cn1  clark and niblett  1  in a chess endgame domain  shapiro  1 . the error rate of small disjuncts is high  whereas the error rate of large disjuncts is almost zero. disjuncts of coverage 1 or less commit 1% of the errors  column 1  even though they match only 1%  of the examples  column 1 . this pattern of errors is not unique to cn1  or to this domain. a similar pattern occurs in the definitions created by id1  quinlan  1  in this domain  and in the definitions created by cn1 in the lymphography domain  clark and niblett  1   see table 1 . 
　ideally  induced definitions should consist of all  and only  disjuncts that are meaningful  e.g.  as measured by a statistical significance test  and have a low error rate. definitions created by existing methods are ideal with regard to large disjuncts  but far from ideal with regard to small disjuncts. the remainder of this paper evaluates three approaches to eliminating error-prone small disjuncts from a definition without adversely affecting other disjuncts in the definition. 
1 	approach 1: eliminate all small disjuncts 
the most direct means of eliminating error-prone small disjuncts is to eliminate all small disjuncts by explicitly refusing to create disjuncts whose coverage is below a certain threshold.1 an immediate objection to this policy is that it has the undesirable effect of creating definitions that do not include the unusual cases of a concept  represented by small but significant disjuncts . 
　a second objection to eliminating all small disjuncts from a definition is that doing so may significantly in-
1
　　the nmin parameter in cart  breiman et a/.  1  is this type of threshold. assistant  cestnik et a/.  1  specifies this threshold as a percentage of the original training set. in both cart and assistant this cutoff is used in building a decision tree that is subsequently pruned. 
	holte  acker and porter 	1 


table 1: this table indicates the number of disjuncts and the coverage of disjuncts in definitions induced by different systems in different domains. this information is gathered from several sources  see  tlolte et a/.  1  for details . aql 1 is d escribed in  michalski and chilausky  1   photos in  bareiss et a/.  1  and  porter et a/.  1   and metadendral in  buchanan et a/.  1 . 

table 1: data about disjuncts of different coverages in the definitions produced by cn1  with a significance threshold of 1%  in the chess endgame domain. these numbers do not include the disjuncts corresponding to cn1's default rule  which all are small and have error rates around 1%. 1 training sets of 1 examples each were independently drawn from the dataset of 1 examples. the definitions produced were evaluated on the entire dataset. column 1 gives the number of test examples matched  column 1 the number of misclassifications  by disjuncts with the coverage. these numbers are the totals over all the definitions. column 1 gives the ratio of misclassifications to matches. column 1 gives the percentage of test examples matched by disjuncts whose coverage is equal to or less than the value in column 1. this value  for row x  is calculated by 
summing the entries in column 1 in rows x and above  and dividing by the sum of all entries in column 1. column 1 gives the percentage of misclassifications made by disjuncts whose coverage is equal to or less than the value in column 1. 
1 	machine learning 


table 1: data corresponding to columns 1  1  and 1 in table 1 for the definitions produced by id1 in a chess endgame domain  left side   and the definitions produced by cn1  with a significance threshold of 1%  in the lymphography domain  right side . this version of id1 did no pruning. in the chess endgame domain  1 training sets of 1 examples each were independently drawn from the same dataset used in the experiment in table 1. the 1 definitions produced were evaluated on the entire dataset. in the lymphography domain  1 runs were made. in each run  the dataset of 1 examples was divided 

into two equal parts  one for training  the other for testing. 
crease the definition's error rate. the net effect of eliminating all small disjuncts is difficult to predict  because it depends on the fate of the  emancipated  examples - the examples that were classified by the disjuncts that have been deleted. table 1  column 1  and table 1  columns 1 and 1   indicate the percentage of examples emancipated by eliminating all disjuncts up to a certain coverage. 
　some emancipated examples will match disjuncts that have not been deleted. these may be classified correctly or incorrectly  and a disjunct's error rate on emancipated examples may be much higher than its original error rate. emancipated examples that fail to match any disjunct may be assigned a default classification  or allowed to pass as errors of omission. most existing systems have rules  called default rules  for assigning a default classification. these rules often have very high error rates. consequently  in these systems  there is a considerable chance that emancipated examples will be misclassified. the only examples that ought to be emancipated are those that match disjuncts with high error rates  say  1% or more. small disjuncts  although much more error-prone than large disjuncts  do not consistently have error rates high enough to justify a policy of eliminating all small disjuncts. 
　an error of omission occurs when a test example is not assigned a classification. it is a indication that several classifications of the example are equally strongly supported by the training set. in many circumstances  errors of omission are more desirable than extremely error-prone default classifications. for this reason  the discussion of  approach 1  gives equal consideration to definitions with default rules and those without. 
1 approach 1: eliminate undesirable disjuncts 
techniques that directly measure  or estimate  the significance and error rate of disjuncts are used in several systems  e.g.  cn1  cart  breiman et al  1   assistant  cestnik etal  1   and recent versions of id1 . these techniques reliably eliminate undesirable large disjuncts  i.e.  ones that are not meaningful  or have a high error rate   but  as currently used  do not reliably eliminate undesirable small disjuncts. this section considers the prospects of strengthening these techniques so that they reliably eliminate undesirable disjuncts  small and large alike. 
1 	significance testing 
tests of statistical significance are used in some systems to determine whether or not to include a disjunct in a definition. definitions produced using these tests tend to have fewer disjuncts  larger disjuncts  and slightly lower error rates than definitions produced without using them. 
　disjuncts whose coverage is too low do not pass significance tests. the coverage at which disjuncts become  insignificantly small  is determined by the significance threshold chosen for the test  typically 1%   the number of concepts  and the distribution of training examples among concepts. for example  if a training set has an equal number of examples of two concepts  a dis-
junct is 1% significant if and only if its coverage is 1 or more. because significance tests eliminate all small dis-
juncts  they are subject to the objections raised in the preceding section. 
　a further problem arises with systems that do not use true significance tests. most systems use tests that accurately approximate significance tests only for large dis-
juncts. some of these systems  such as cn1  apply the approximate tests to small disjuncts despite their inaccuracy. others  such as id1  refrain from testing the significance of small disjuncts1. in any case  the significance of small disjuncts is not reliably estimated  with the undesirable result that significant small disjuncts may be 
　　1  quinlan  1  page 1 . the action taken in lieu of a significance test is not described. 
	holte  acker and porter 	1 

eliminated and insignificant ones retained. this problem is not insuperable:  niblett  1  gives an exact test for significance. 
1 	e r r o r - r a t e 	e s t i m a t i o n 
error rate cannot be tested exactly: it can only be estimated. like approximate tests of significance  techniques for estimating error rate are not entirely reliable for small disjuncts. for example   cestnik et a/.  1  reports that the technique of niblett and bratko1  seems to make rather pessimistic estimation about the information contained in learning data  it overestimates the error rate of subtrees  ... post-pruning is too drastic when the number of learning examples per class per attribute is low.   page 1 . 
1 t h e n e e d f o r b o t h significance a n d e r r o r - r a t e t e s t i n g 
no existing system tests both significance and error rate.  pre-pruning  systems use significance testing;  postpruning  systems use error-estimation  niblett  1 . indeed  post-pruning systems have a rather strong disregard for the significance of disjuncts  their sole objective being to eliminate from a definition as many disjuncts as possible without suffering too great an increase in error rate. a one-test approach is sufficient to eliminate undesirable large disjuncts  because for large disjuncts  significance and error rate are highly correlated. 
   however  in order to eliminate all undesirable small disjuncts  it is necessary to test both significance and error rate. this is because  for small disjuncts  error rate is not related to significance in any simple way. neither is it related to  entropy   a measure that is often used in conjunction with significance tests. the lack of a simple relation between error rate and entropy is evident in the definitions produced by cn1. cn1 creates disjuncts one at a time  evaluating  at each step  the entropy of all passible new disjuncts on the portion of the training set not covered by existing disjuncts. the new disjunct with the lowest entropy is included in the definition only if it passes a significance test. thus  the disjunct selected at a given step had lower entropy  at that step  than the disjuncts selected at later steps  and it was statistically significant. if error rate were related to entropy  disjuncts in neighbouring steps would have similar error rates. that this does not occur is evident in the following data  which describes a typical definition. 

1 approach 1: make small disjuncts highly specific 
the techniques considered in the previous sections have all been based on properties  coverage  significance  entropy  and error rate  that are defined in terms of the set of training examples that match a disjunct. there will usually be many different disjuncts that match the same set of training examples  and these will all be indistinguishable by the previous techniques. that is  they will all have identical estimates of error rate  significance  entropy  and so on. 
   to select among disjuncts that are indistinguishable on the basis of the training set  inductive systems employ an extra-evidential preference criterion  or  bias   mitchell  1 . definitions produced using different biases  will usually have different error rates and different distributions of errors across disjuncts. it is possible that the problem of error-prone small disjuncts is caused by the use of the  maximum generality'1 bias  defined below . this bias is used by many inductive systems  including id1 and cn1. the use of a different bias might result in definitions in which all disjuncts  large and small alike  have low error rates. this approach has been explored experimentally  by comparing the definitions produced by cn1 when it is biased in different ways. 
   three biases are compared in this section. all are defined in terms of a disjunct's  specificity   which is defined as the number of conditions in a disjunct  recall that a disjunct is the conjunction of one or more conditions .  generality  is the opposite of specificity. to compare the definitions produced by the different biases  a training set of about 1 examples was drawn at random from the 1 examples in the kpa1kr  chess endgame  dataset. cn1  using each bias  was run on the training set. the definitions produced were evaluated using the entire dataset. this procedure was repeated for 1 independently drawn training sets. the cumulative results of these 1 runs are given in table 1  see  acker  1  for more details . 
   cn1's original bias is the maximum generality bias. an inductive system using this bias  having decided to create a disjunct that matches a particular set of training examples  selects a maximally general disjunct that matches those examples and no others. the definitions produced by cn1 using this bias are described in table 1 

1 	machine learning 

and the top  row of table 1. the problem of error-prone small disjuncts is evident in this data. small disjuncts  coverage 1 or less  have an error rate of 1%  whereas large disjuncts have an error rate of 1 . 1 % . ignoring examples classified by the default rule  small disjuncts commit about 1%  of the errors even though they match only about 1%  of the examples. 
   the maximum generality bias works well for large disjuncts  but not for small disjuncts. this suggests restricting its use to large disjuncts  and using a different bias for small disjuncts. the maximum specificity bias seems  on the face of it  to be appropriate for small disjuncts. an inductive system using this bias  having decided to create a disjunct that matches a particular  small  set of training examples  selects the disjunct con-

sisting of all the conditions that are satisfied by those examples. 
　the middle row in table 1 describes the definitions produced by cn1 using the maximum generality bias for large disjuncts and the maximum specificity bias for small disjuncts. in these definitions  the large disjuncts are identical to those in the original definitions  but the small disjuncts are maximally specific instead of being maximally general. the small disjuncts created using this bias match many fewer examples than are matched by the small disjuncts created using the original bias  1 compared to 1 . the error rate of small disjuncts has decreased considerably  indicating that the 1 examples emancipated by using maximally specific disjuncts were a major source of error. 
　unfortunately  use of the maximum specificity bias for small disjuncts has adverse affects on other parts of the definition. the emancipated examples  1% of which are classified by large disjuncts  are misclassified at a rate of almost 1%  which is double the rate at which they were misclassified by the small disjuncts. consequently  there is a net increase in the error rate of the definitions that is unacceptably large. 
　the maximum specificity bias moves in the right direction  but it goes too far. using a  selective specificity  bias  an inductive system  having decided to create a disjunct that matches a particular  small  set of training examples  would select the disjunct consisting of the conditions that are satisfied by those examples and that meet certain other requirements. these other requirements are what make the specificity selective. a disjunct produced using this type of bias may be maximally specific  maximally general  or neither  depending on whether all  none  or some of the conditions meet the requirements. 
　the particular selective specificity bias used in this experiment required the conditions in the disjunct for subset s of training set t to match no more than 1%  of the examples in t - s whose class differs from that of the majority of 1. for example  suppose there are two classes  c1 and c1  that the majority of examples in s are in c1  and that g is a maximally general disjunct for s. then a condition matching all the examples in s is added to g  according to this selective specificity bias  if and only if it matches fewer than 1% of the c1 examples in t - s. 
　the bottom row in table 1 describes the definitions produced by cn1 using the maximum generality bias for large disjuncts and the selective specificity bias for small disjuncts. the small disjuncts produced using the selective specificity bias are superior to those produced using the other biases. they have a reasonably low error rate  which they did not have when the maximum generality bias was used  and they are doing a significant amount of the classification  which they did not do when the maximum specificity bias was used. when the selective specificity bias is used for small disjuncts  large disjuncts have a slightly higher error rate than when the maximal generality bias is used. likewise  the error rate of definitions is slightly higher using the selective specificity bias than it is using the maximum generality bias. however  this difference is due entirely to the error rates of the default rules. ignoring the default rules  the definitions produced by both biases match almost the same number of examples  1% of the test set  and have almost identical error rates  1% . thus  the problem of error-prone small disjuncts is solved  to a significant degree  by using the maximum generality bias for large disjuncts and the selective specificity bias for small disjuncts. 
　the success of this approach depends  to some extent  on having defined  small  as coverage   1. table 1 gives the results of repeating the preceding experiments with  small  defined as coverage   1. these results are similar to the previous ones in three important ways. first  the error rate of small disjuncts is reasonably low when the selective specifity bias is used for small disjuncts but not when the maximal generality bias is used. secondly  both biases produce large disjuncts with low error rates. thirdly  the error rate of definitions is higher when the selective specifity bias than when the maximal generality bias is used  and this difference is entirely due to the increased use and error rate of the default rule. 
　there are also significant differences between the definitions produced using the different definitions of  small . the error rate of definitions is considerably lower using coverage   1. however  if default rules are ignored  the opposite is true. using coverage   1  the collective error rate of large and small disjuncts is only 
	holte  acker and porter 	1 

1%  compared to 1% using coverage   1. on the other hand  using coverage   1  the large and small dis-
juncts collectively match only 1% of the test examples  
compared to 1%  using coverage   1. 
1 	conclusions 
this paper has demonstrated that existing concept learning systems do well at creating large disjuncts  but poorly at creating small ones. some of the causes of this poor behaviour have been identified. improvements that are suggested by this analysis are  1  use exact significance tests;  1  test both significance and error-rate; and  1  use errors of omission instead of default classifications whenever possible. a fourth suggestion  that different biases ought to be used for large and small dis-
juncts  was investigated experimentally. the use of the maximum generality bias for large disjuncts and a selective specificity bias for small disjuncts partly solved the problem of small disjuncts. this result is relatively insensitive to the exact definition of  small . 
acknowledgements 
we are especially grateful to peter clark  of the turing institute  glasgow   who provided the source for cn1  the definitions it produced on the lymphography dataset  and the kpa1kr dataset generated by alen shapiro shapiro  1 . peter also answered many questions  and gave helpful criticism of early drafts of this paper. we also wish to to thank ray mooney  for his reconstruction of 1  ray bareiss  for providing the definitions produced by protos  and larry rendell  for his comments on an early draft of this paper. 
