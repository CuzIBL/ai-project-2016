 
we present a description of three different algorithms that use background knowledge to improve text classifiers. one uses the background knowledge as an index into the set of training examples. the second method uses background knowledge to reexpress the training examples. the last method treats pieces of background knowledge as unlabeled examples  and actually classifies them. the choice of background knowledge affects each method's performance and we discuss which type of background knowledge is most useful for each specific method. 
1 using background knowledge 
supervised learning algorithms rely on a corpus of labeled training examples to produce accurate automatic text classifiers. an insufficient number of training examples often results in learned models that are suboptimal when classifying previously unseen examples. numerous different approaches have been taken to compensate for the lack of training examples. these include the use of unlabeled examples  bennet and demiriz  1; blum and mitchell  1; nigam et al  1; goldman and zhou  1   the use of test examples  joachims  1  and choosing a small set of specific unlabeled examples to be manually classified  lewis and gale  1 . 
　our approach does not assume the availability of either unlabeled examples or test examples. as a result of the explosion of the amount of data that is available  it is often the case that text  databases and other sources of knowledge that are related to the text classification task are readily available from the world wide web. we incorporate such  background knowledge  into different learners to improve classification of unknown instances. the use of external readily available textual resources allows learning systems to model the domain in a way that would be impossible by simply using a small set of training instances. for example  if a text classification task is to determine the sub-discipline of physics that a paper title should belong to  background knowledge such as abstracts  physics newsgroups  and perhaps even book reviews of physics books can be used by learners to create more accurate classifiers. 
haym hirsh 
rutgers university 
1 frelinghuysen road piscataway  nj 1 hirsh cs.rutgers.edu 
　we present three methods of incorporating background knowledge into the text classification task. each of these methods uses the corpus of background knowledge in a different way  yet empirically  on a wide variety of text classification tasks we can show that accuracy on test sets can be improved when incorporating background knowledge into these systems. we ran all three methods incorporating background knowledge on a range of problems from nine different text classification tasks. details on the data sets can be found at  www.cs.csi.cuny.edu/~zelikovi/datasets; each varied on the size of each example  the size of each piece of background knowledge  the number of examples and number of items of background knowledge  and the relationship of the background knowledge to the classification task. 
1 methods 
in our first approach we use naive bayes and em as in  nigam et al  1 . we can substitute more general background knowledge for unlabeled examples  and obtain improvements in accuracy on text classifiers that arc created using both the training set and the set of background knowledge. naive bayes classifiers make the assumption that examples  both labeled and unlabeled  have been generated by a mixture model that has a one-to-one correspondence with classes. even if this assumption is true for the labeled data and the test data  by its very nature  background knowledge should not fit this assumption at all. however  the interesting observation that we make is that to gain leverage out of unlabeled examples  the unlabeled data that we have need not be specifically and accurately unlabeled examples. as long as the vocabulary and classification structure closely resembles the training/test data  background knowledge can improve classification accuracy in textual data using the em algorithm. 
1 	poster papers 　a second approach that we take is based upon a nearest neighbor text classifier using whirl  cohen  1; cohen and hirsh  1 . instead of simply comparing a test example to the corpus of training examples  we use the items of background knowledge as  bridges  to connect each new example with labeled training examples. a labeled training example is useful in classifying an unknown test instance if there exists some set of unlabeled background knowledge that is similar to both the test example and the training example. we call this a  second-order  approach to classification  zelikovitz and hirsh  1; 1   in that data are no longer directly compared but rather  are compared one step removed  through an intermediary. 
　finally we use the background knowledge to redescribe both the training and the test examples. to do this  we add the background knowledge documents to the training set  to create a large  sparse term-by-document  t x d  matrix. we then use latent semantic indexing  lsi   deerwester et al.  1 to automatically redescribe textual data in a new smaller semantic space using singular value decomposition. the original space is decomposed into linearly independent dimensions or  factors   and the terms and documents of the training and test examples are then represented in this new vector space  zelikovitz and hirsh  1; 1j. documents with high similarity no longer simply share words with each other  but instead are located near each other in the new semantic space. since this semantic space was created by incorporating the background knowledge  the model of the domain that it creates reflects both the training set and the background knowledge. 
1 comparison of approaches 
different types of background knowledge are most useful for each of these three systems. the system based upon whirl performs best on the problems where the form and size of the background knowledge is substantially different than the training and test data. for example  we classify names of companies by area using yahoo! pages as background knowledge. these background pieces of data are not really classifiable  in the sense that they do not necessarily belong to any specific class. since this whirl-based method does not attempt to classify the background knowledge  but merely uses it to index into the training corpus  it makes the best use of 
this background knowledge. 
　for the data sets where the background knowledge fits very closely to the training and test classification task  em outperforms the other systems. for example  em performed best when classifying physics papers by subdiscipline using abstracts as background knowledge. this is consistent with the way em makes use of background knowledge. since em actually classifies the background knowledge  and uses the background knowledge to decide on the parameters of its generative model  the closer the background knowledge is to the training and test sets  the better em will perform. ideally  for em  we wish the background knowledge to be generated from the same model as the training and test sets. 
　reexpressing the data and background with lsi seems to be most effective when there is very limited training data. on the smallest data sets  it outperforms all the other methods in many domains. when very few training examples exist  this method can still build a space that correctly models the domain by using the available background knowledge. 
　we are currently looking at methods to evaluate sets of background knowledge to determine the amount of background knowledge as well as the measure of relevance that it must have to the training set to be useful for each of these learners. 
