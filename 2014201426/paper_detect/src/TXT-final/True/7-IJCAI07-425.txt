
in this paper we deal with the problem of mining large distributed databases. we show that the aggregation of models  i.e.  sets of disjoint classification rules  each built over a subdatabase is quite enough to get an aggregated model that is both predictive and descriptive  that presents excellent prediction capability and that is conceptually much simpler than the comparable techniques. these results are made possible by lifting the disjoint cover constraint on the aggregated model and by the use of a confidence coefficient associated with each rule in a weighted majority vote.
1 introduction
this paper deals with the problem of mining several large and geographically distributed databases  dbi  with the goal of producing a set of classification rules that explains the various groupings found in the observed data. the result of this mining is both a predictive and descriptive meta-classifier. in other words  we aim at producing a model which is not only capable of predicting the class of new objects  but which is also able to explain the choices of its predictions. we believe that this kind of models  based on classification rules  should also be easy to understand by humans  which is also one of our objectives. also  our application context is one where it is impossible to gather all these databases on the same site  and this  either because of downloading time  or because of the difficulty to mine the aggregated database.
모in the literature  we find very few distributed data mining techniques which are both predictive and descriptive. the majority of them try to produce a meta-classifier in the form of a set of rules with disjoint cover  i.e.  where an object is covered by one and only one rule. we will show in this paper that this constraint of disjoint cover is not necessary to produce a reliable meta-classifier and furthermore introduces unnecessary complexity to the mining technique. therefore  we propose a simpler technique where an object can be covered by several rules  and where the lifting of this constraint enables us to produce a conceptually very simple classifier with good prediction capability as will be shown below.
모the performance of our meta-classifier  from a prediction point of view  is compared to c1 applied on the whole database db = 뫋idbi; it is used only as a reference to assess the potential loss of accuracy of our method since  by assumption  we stated that we could not process db because of download/processing time constraints.
모this paper proceeds as follows. we present in sections 1 a survey of some well known model aggregation techniques. then  in section 1  we present our solution for distributed data mining  ddm  by model aggregation  ddm-ma  based on a majorityvote that is ponderedbysome confidencecoefficient. section 1 introduces a conceptual comparison between our method and those found in the literature. in section 1  we present experimental results which prove the viability of our method. we will show that it bares comparable accuracy rates while being simpler that other ddm methods. we finally present a conclusion and our future work.
1 model aggregation existing techniques
as we present in this paper a technique developed in a distributed data mining perspective  we will ignore some non relevant techniques as the ruler system  fayyad et al.  1   fayyad et al.  1  that was developed for the aggregation of several decision trees build on the same data set in a centralized system  the distributed learning system  sikora and shaw  1  developed in a context of information management system that builds a distributed learning system  and the fragmentation approach  wu몮thrich  1  which uses probalistic rules. also  we will ignore purely predictive techniques such as bagging  breiman  1   boosting  schapire  1   stacking  tsoumakas and vlahavas  1   and the arbiter and combiner methods  chan  1    prodromidis et al.  1 .
1 the mil algorithm
the mil algorithm  multiple induction learning  was initially proposed by williams  williams  1  in order to resolve conflicts between conflictual rules in expert systems. authors of  hall et al.  1a; 1b  took again the technique of williams  williams  1  to aggregate decision trees built in parallel and transformed beforehand into rules. the process of aggregation proposed by these authors is a regrouping of rules accompanied by a process of resolution of the possible conflicts between them. it should be noted that this resolution of the conflicts treats only one pair of conflict rules at a time. two rules are considered in conflict when their premises are consistent while they produce two different classes  williams  1   called conflict of type i    or when the conditions of the premises overlap partially  hall et al.  1a   called conflict of type ii  or when the rules have the same number of predicates with different values for conditions and they classify objects into the same class  hall et al.  1b   called conflict of type iii .the conflict resolution consists in either specializing one or the two rules in conflict  conflicts type i and ii   or in adjusting the value of the condition  i.e.  the test boundary  for the conflicts of type ii and iii and eventually in combining the two rules in conflict  conflict of type iii . in certain cases  conflicts of type i and ii   new rules are added based on the training sets to recover the cover lost by the specialization process.
1 the drl system  distributed rule learner 
the drl technique  distributed rule learner   provost and hennessy  1 was conceived based on the advantageof the invariant-partitioningproperty  provost and hennessy  1 . the drl technique begins by partitioning the training data e into nd disjoined subsets  assigns each one  ei  to a different machine  and provides the infrastructure for the communication between different learners  named rl . when a rule r satisfies the evaluation criterion for a subset of the data  i.e.  being an evaluation function1 of a rule
and c a constant   it becomes a candidate to satisfy the global evaluation criterion; the extended invariant-partitioningproperty guarantees that each rule which is satisfactory on the whole data set will be acceptable at least on one subset. when a local learner discovers an acceptable rule  it sends the rule to the other machines so that they update its statistics on the remainder of the examples. if the rule meets the global evaluation criterion  f r e  뫟 c; f being the principal evaluation function and c a constant   it is asserted as a satisfactory rule. in the opposite case  its local statistics are replaced by the global statistics and the rule is made available to be specialized some more. the property of invariant-partitioning guarantees that each satisfactory rule on the whole data set will be found by at least one of the rls.
1 combining rule sets generated in parallel
the work presented by  hall et al.  1  is a mixture of the last two techniques presented above  i.e.  that of  williams  1    hall et al.  1b  and  provost and hennessy  1 . in details  they associate to each rule a measurement of its  quality  which is based on its prediction precision as well as on the number and the type of the examples that it covers.
모the technique suggested in  hall et al.  1  is based on the use of what  provost and hennessy  1  proposes  see 뫫1   with a small difference where the deletion of the rule from the space of rules under consideration is made only when the rule classifies all the data of the various distributed bases  which is the case when its measure f r e  is lower than a certain threshold. it should be noted that each rule does not  travel  alone from one site to another  but is indeed accompanied by the values necessary to calculate the measure associated with each rule.
모however  in  hall et al.  1   the authors show that in the extreme case the property of invariant-partitioning could not be satisfied. thus  they prove that the precision of the aggregate rule set can be very different from the precision of the rules built on the training set. moreover  the authors show that conflicts between rules can be solved  as described by  hall et al.  1b  and  williams  1 .
모in addition   hall et al.  1  proposes a new type of conflict between rules: a rule whose premise contains some interval that overlaps an interval that is contained in the premise of a second rule. in this case  a more general rule is created by combining the two conflicting rules and by adjusting the border values of these intervals.
1 the proposed model aggregation technique
the proposed technique is very simple. we build in parallel over each distributed dbi a model  i.e.  a set of classification rules ri  called base classifier. figure 1 shows an example of such rules.
if adoption of the budget resolution = n
if physician fee freeze = y then class: republican
if adoption of the budget resolution = u
if physician fee freeze = y then class: democrat
figure 1: an example of rules contained in a base classifier.
모then  we compute for each rule a confidence coefficient  see below for details . finally  in a centralized site  base classifiers are aggregated in the same set of rules  r = 뫋iri  which represents our final model  called meta-classifier. the global algorithm of our distributed data mining technique is described by figure 1.
1. do in parallel over each database dbi
 a  apply on dbi a classification algorithm producing a set of disjoint cover rules. the produced set is
ri = {rik | k 뫍  1..ni }
where ni is the number of rules;
 b  compute for each rik a confidence coefficient crik  see hereafter ;
1. in a central site create:
r =   ri
i=1...nd
where nd is the number of distributed databases.
figure 1: algorithm of the proposed ddm technique.
since different rule sets are going to be merged together whereas they are issued from different data subsets  and since each rule r has its proper error rate er and coverage n  we compute for each rule a confidence coefficient cr. this confidence coefficient is computed in straightforwardmanner from the lower bound of an error rate confidence interval proposed by  langford  1 . we defined it as one minus the worst error rate in  1   붻  of the time :

cr = 1   bin  n ner 붻 
		def
where bin  n k 붻  = min{r : 1   bin n k r  뫟 붻} and
모since r is not a set of disjoint cover rules where an object is covered by a unique rule  we explain hereafter how we can use this meta-classifier as a predictive and descriptive model.
1 the use of r as a predictive model
the set r represents the aggregation of all base classifiers  r = 뫋iri . this rule set is used as a predictive model as well as a descriptive one. from a predictive point of view  the predicted class of a new object is the class predicted by a majority vote of all the rules that cover it  where the rules are weighted by their confidence coefficients1. it should be noted that  contrarily to what is identified in the literature  see 뫫1   we have restricted the notion of rules in conflict to be those that cover the same object but with different classification results. if several rules cover the same object and predict the same class  we do not consider them as being in conflict.
모it is to be noted that any object can be covered by at most nd rules  knowing that nd is the number of sites.
1 the use of r as a descriptive model
as a classification system is often developed as support to decision-making  the different rules covering an object may be proposed to the user who could then judge  from his expertise  of their relevance  helped by a confidence coefficient. presenting to a decision maker more than one rule may have its advantages since it may provide a larger and more complete view of the  limits  of each class. we bring to mind  that in machine learning  the limit which defines separation between various classes is generally not unique nor clear cut  and consequently  several rules producing the same class can represent the  hyper-planes  separating the various classes  providing various views on these data.
1 a conceptual comparison
1 the mil technique
the mil technique  hall et al.  1a   hall et al.  1b  suffers from several problems. first of all  the process of conflict resolution only specializes the rules based on the classification rules data sets. the generated rules could show poor classification ability when they are applied to new objects  especially in the case of very noisy training data. in addition  the adaptation of the technique of williams  williams  1  in order to treat distributed bases implies an increase in the volume of data exchanged between the various sites. indeed  on the one hand  each rule travels accompanied by the index of the covered objects and  on the other hand  in the event of conflict  all the objects covered by one of the two rules in conflict must be downloaded from the training site to the site resolving the conflict.
1 the drl system
the most significant disadvantage of the drl system  provost and hennessy  1  is its execution time. indeed  when a rule is considered to be acceptable by a given site  it must go across to all the other sites. in other words  any acceptable rule on a site must classify all the data of all the other sites. thus  the rule must  on the one hand  travel  through all the sites  and on the other hand  classify the data of each site. if a rule is not considered to be satisfactory on the whole data set  this rule is specialized and the process starts again if it is considered to be locally acceptable. it is clear that this process could be very time consuming.
1 combining rule sets generated in parallel
as for the system of combining rule sets generated in parallel  it is identical to the previous one with a little difference: any rule generated in a given site must cross over to all the other sites. thus  the number of rules traveling between the various sites is more significant than the number of rules of the drl system. consequently  it is clear that this technique is slower than the preceding one.
1 the proposed technique
to overcome the problems of mil technique  the proposed one is based on a majority vote that is known to be rather a robust model against noisy data. indeed  the prediction process gives good results especially in noisy bases  see 뫫1 below .
모in the proposed technique  see 뫫1  rules  travel  only in one way  from the distributed database site to the central site and the amount of data is almost minimal where a rule is augmented by no more than its confidence coefficient. thus the problem of excess communication found in the drl system and its successor is avoided.
모from an execution point of view  an asymptotic analysis was conducted in  aounallah  1  and  aounallah and mineau  1  of our technique and those presented in 뫫1 of this paper. this asymptotic analysis shows clearly that in the worst case our technique is faster than existing ones.
모in the best case  we expect our technique to be at least comparable to existing ones. since having no conflicts between different base classifiers is very rare  we believe that our technique is faster than existing ones because our technique does not conduct any conflict resolution step.
moreover  the proposed technique is no more than a sim-
ple aggregation of base classifiers. consequently  there is no doubt that it is conceptually by far simpler that existing comparable ones  i.e.  it should be faster and simpler to implement than those found in the literature  see 뫫1 .
1 an empirical comparison
to evaluate the performance of our ddm technique  we conducted some experiments in order to assess its prediction  accuracy  rate. we compared it to a c1 algorithm built on the whole data set  i.e.  on the aggregation of the distributed databases. this c1  produces a rule set r  which is used as a reference for its accuracy rate since we assumed in the introduction that it is impossible to gather all these bases onto a single site  and this  either because of downloading time  or because of the difficulty to learn from the aggregated base because of its size. the rule set r is considered to be the ideal case  where theoretically it is not possible to perform better than a model built on the whole data set.
모the conducted experiments have been tested on nine data sets: chess end-game  king+rook versus king+pawn   crx  house-votes-1  ionosphere  mushroom  pima-indiansdiabetes  tic-tac-toe  wisconsin breast cancer  bcw  mangasarian and wolberg  1  and wisconsin diagnostic breast cancer  wdbc   taken from the uci repository  blake and merz  1 . the size of these data sets varies from 1 objects to 1 objects  objects with missing values have been deleted . furthermore  in order to get more realistic data sets  we introduced noise in the nine aforementioned databases  and this by reversing the class attribute1 of successively 1%  1%  1% and 1% of objects. hence  since for each data set we have  in addition to the original set  1 other noisy sets  giving a total number of databases of 1.
모in order to simulate a distributed environment the data sets have been divided as follows. we divided each database into a test set with proportion of 1. this data subset was used as a test set for our meta-classifier and for r  our reference classifier. the remaining data subset  of proportion 1   was divided randomly into 1  1  1 or 1 data subsets in order to simulate distributed databases. the size of these bases was chosen to be disparate and in such a way so there was a significant difference between the smallest and the biggest data subset. as an example of such subdivision see figure 1.

mush1.test 1obj .  mush1.data + mush1.data mush1.test 1obj .  mush1.data + mush1.data mush1.test 1obj .  mush1.data + mush1.data
figure 1: example of subdivision for a database from the uci.
모for the construction of the base classifiers we used c1 release 1  quinlan  1   quinlan  downloaded in 1  table 1: comparison between  original data sets .

lower b.upper b.rcmp.bcw1%1%1%1%chess1%1%1%1%-crx1%1%1%1%iono1%1%1%1%mush1%1%1%1%pima1%1%1%1%tic-tac-toe1%1%1%1%vote1%1%1%1%wdbc1%1%1%1%
	table 1: comparison between	 1% noise .

lower b.upper b.rcmp.bcw1%1%1%1%chess1%1%1%1%-crx1%1%1%1%-iono1%1%1%1%mush1%1%1%1%pima1%1%1%1%+tic-tac-toe1%1%1%1%vote1%1%1%1%wdbc1%1%1%1%
which produces a decision tree that is then directly transformed into a set of rules. the confidence coefficient of each rule was computed using the program offered by langford  langford  downloaded in 1  with a basis of 1% confidence interval  i.e.  붻 = 1 .
모in order to assess the prediction capability of our technique we compared its prediction rate to the one of r over the 1 aforementioned data sets. table 1 to 1 detail the results obtained. the third and the forth columns of these tables contain respectively the lower and the upper bound of r error rate confidence interval computed at 1% confidence. the last column contains:
   +  if our technique outperforms r 
   -  if r outperforms our meta-classier and
  a blank if the two techniques are statistically comparable.
모from these tables  we can see that our meta-classier performance is very comparable to the one of r since in 1 cases table 1: comparison between  1% noise .

lower b.upper b.rcmp.bcw1%1%1%1%chess1%1%1%1%crx1%1%1%1%iono1%1%1%1%mush1%1%1%1%pima1%1%1%1%tic-tac-toe1%1%1%1%vote1%1%1%1%+wdbc1%1%1%1%
	table 1: comparison between	 1% noise .

lower b.upper b.rcmp.bcw1%1%1%1%chess1%1%1%1%crx1%1%1%1%iono1%1%1%1%mush1%1%1%1%pima1%1%1%1%tic-tac-toe1%1%1%1%vote1%1%1%1%+wdbc1%1%1%1%
	table 1: comparison between	 1% noise .

lower b.upper b.rcmp.bcw1%1%1%1%chess1%1%1%1%+crx1%1%1%1%+iono1%1%1%1%mush1%1%1%1%+pima1%1%1%1%tic-tac-toe1%1%1%1%vote1%1%1%1%+wdbc1%1%1%1%
over 1 its error rate is statistically comparable and only in 1 cases it is worst than r. moreover  surprisingly  our metaclassifier could outperformr in 1 cases; this is especially the case when noise in distributed data sets is important. in these cases  we could easily see the advantage of using a noise robust model as the weighted majority vote in a non disjoint cover rule set  instead of using a single model with disjoint cover rule set.
모these results  prove also the viability of the confidence coefficient proposed in this paper.
1 conclusion
the objective of this paper is to present a very simple distributed data mining technique  ddm  by model aggregation  ma . with this intention  we presented  on the one hand  a rapid survey of existing model aggregation techniques which are most comparable to ours. and on the other hand  we presented a description of our ddm-ma technique.
모throughout this paper  we have shown that the proposed ddm technique is conceptually by far simpler that existing comparable techniques. indeed  it consists of a simple aggregation of distributed models  base classifiers .
모experiments demonstrate that our technique  from a prediction point of view  performs as well as or even better  than a classifier built over the whole data set  r  the theoretically ideal case since it is built over the whole data. our metaclassifier r could outperform r due to the weighted majority vote pondered by a confidence coefficient associated to each rule. this confidence coefficient is based on the lower bound of an error rate confidence interval proposed by  langford  1 . in other words  such a mojority vote over imperfect rules gives very good predictive results because the confidence coefficient used in the process uses these rules with a weight that reflects their individual prediction power.
모moreover  since the granularity of the majority vote is at rule level  instead of classifier level   the meta-classifier r can be used as a descriptive model  where the predictive class of an object is described by the rules covering it.
모due to these good results  we can imagine that our technique could be applied on very large centralized databases that could be divided into smaller ones before applying our technique  rather than applying a data mining tool over the centralized database; this is what we propose to explore in a near future.
모furthermore  we propose  on the one hand  to test the proposed technique on n-ary databases and  on the other hand  to compare it experimently to exiting techniques.
