 
this paper presents a machine designed for compact representation and rapid execution of lisp programs. the machine language is a factor of 1 to 1 more compact than s-expressions or conventional compiled code  and the.compiler is extremely simple. the encoding scheme is potentially applicable to data as well as program. the machine also provides for user-defined data structures. 
introduction 
pew existing computers permit convenient or e f f i c i e n t implementation of dynamic storage allocation  recursive procedures  or operations on data whose type is represented e x p l i c i t l y at run time rather than determined at compile time. this mismatch between machine and language design plagues every implementor of languages designed for manipulation of structured information. neither of the usual software solutions to this problem is e n t i r e l y satisfactory. interpretive systems are easy to build and f l e x i b l e   but i n t r i n s i c a l l y i n e f f i c i e n t ; compilers which approach the efficiency of those for conventional languages are hard to write and often force the implementor  and user  to sacrifice valuable but expensive language features for the sake of efficiency. on many machines compiled code also occupies at least as much space as a structured representation of the source program. 
an alternative approach to this problem is to design machines whose code structure more closely resembles that of their major programming language s . this approach of t a i l o r i n g the machine to the language was f i r s t used in the burroughs b1 and successor machines  which were designed to execute algol 1 programs.' in recent years  the a v a i l a b i l i t y of microprogrammed processors and the continuing decline in the cost of processor hardware and design have prompted several experiments of t h i s sort at universities* and at least one successful experiment by a large company* and one unsuccessful new commercial venture. 1 
the present paper describes a machine design for e f f i c i e n t representation and execution of bbn-lisp programs. bbn-lisp is an interactive system developed from the lisp language.* 1 readers unfamiliar with lisp should consult weissman's excellent primer 1 ; some details particular to bbn-lisp appear in the next sections of t h i s paper. a complete and well-maintained but voluminous reference manual for bbn-lisp is also a v a i l a b l e .   the machine design presented here w i l l be referred to as microlisp  a name intended to connote both code compactness and possible microprogrammed implementation. 
data types 
lisp has many data types  e.g. l i s t   symbolic atom  integer  but no d e c l a r a t i o n s . the usual implementation of languages with this property affixes a tag to each datum to indicate i t s type  in lisp  however  the   vast majority of data are pointers to l i s t s or atoms  and it would be wasteful to leave room for a f u l l word plus tag  the space needed for an integer datum  for example  in every place where a datum can appear such as the car and cdr of l i s t cells. consequently  in bbn-lisp every datum is a pointer; integers  strings  etc. are a l l referenced i n d i r e c t l y . storage is allocated in quanta  and each quantum holds data of 
only one type  so what type of object a given pointer references is just a function of the object's address  i . e . the pointer i t s e l f . 
the chief drawback of t h i s scheme is that every b u i l t - i n function which produces a number as a result  such as plus  the addition function  must allocate a word to hold the r e s u l t . this leads to frequent  time-consuming garbage collections. bbn-lisp circumvents t h i s problem for the most part by permanently storing a l l the integers from -1 to +1 in consecutive 
cells and just returning a pointer to one of these c e l l s if a numerical result is in t h i s range  rather than allocating a new c e l l . in microlisp  which is intended as a 
reasonably e f f i c i e n t numerical language  data on the stack  temporary results and 
variable bindings  carry a type tag 
identifying them as integers  f l o a t i n g point numbers  or pointers. in this way  long numerical calculations can take place without any consumption of allocated space. 
no existing lisp system permits the user to define his own packed data structures. microlisp includes such a f a c i l i t y   since it can be made inexpensive when implemented in the machine language and since i t s absence from lisp is one of the reasons most frequently cited for choosing other languages for complex symbolic computation. the details are presented in appendix a  since they are somewhat peripheral to the rest of this paper. it is worth noting that the scheme could be implemented within bbn-lisp and even lends i t s e l f to e f f i c i e n t compilation in the usual case. 
control and binding structure 
microlisp uses a single stack structure for both control and variable bindings  essentially as described in a recent paper. 1 a function c a l l allocates a  basic frame  for the arguments and a  frame extension  for control information and temporary values* the basic frame contains the function name  the argument values 
1 
 bindings   and a pointer to the argument names. the frame extension holds a pointer to the c a l l e r ' s frame extension and a variety of other bookkeeping information. the funarg capability of lisp 1  i . e . the a b i l i t y to construct a data object comprising a function and a binding environment  is provided through a primitive function which creates an  environment descriptor  pointing to a specified frame. as long as there are accessible references to t h i s descriptor  the frame continues to exist. environment descriptors also allow the user to construct cooperating sequential processes  coroutines ; the stack becomes tree-structured rather than linear  as in the burroughs b1* 
bbn-lisp  l i k e most programming languages  recognizes two kinds of accesses to variables:  load  and   s t o r e   . this duality actually exists for data structures as well  car-rplaca  get-pot  etc.  but is not treated systematically. microlisp systematizes t h i s concept by allowing a function to have  in e f f e c t   two d e f i n i t i o n s   one for the  normal   load  context  one for the  store  context. the set function is extended so that if the f i r s t argument is a l i s t 
      fn argl . . . argn  rather than a variable  the function fn is called in  store  mode with arguments argl . . . argn and newvalue  the second argument of set . setq is also extended in the obvious way  but is not p a r t i c u l a r l y useful. 
a more useful function is 
      setfq  f n argl . . . argn  newvalue  which quotes the function name and evaluates everything else. 	this allows rplaca  for example  to be defined as 
 lambda  x y   setfq  car x  y   -
the semantics of variables are simple in p r i n c i p l e : search the current basic frame  then the c a l l e r ' s frame  etc. for a binding of a variable with the desired name; if none is found  consult the  value c e l l   of the variable; if t h i s contains the special value nobind  the variable is unbound.  in f a c t   the search follows a chain through an  access l i n k   pointer in the frame extension rather than the caller pointer or  control l i n k     to cover application of funargs.  microlisp  and compiled bbn lisp  actually use three variations of t h i s searching strategy depending on the situation. searching for the arguments of the current function is pointless: their relative locations in the basic frame are known to the compiler and they can be accessed by indexing. searching for variables which are set at the top level and never rebound is time-consuming: there is a compiler declaration to force references to specific variables to bypass the search and go d i r e c t l y to the value c e l l . repeated searches for a variable referenced more than once in a given function are wasteful; in microlisp the search always occurs at the time of the f i r s t reference and is not repeated thereafter. 
in both bbn-lisp and microlisp  a l l variable bindings appear in the basic frame. in bbn-lisp half of each word in the basic frame is reserved for the name  in microlisp  the basic frame contains a single pointer to a table of names  the lnt; see below . either scheme requires that any prog or open lambda which does not constitute the entire body of a function be made a separate subfunction  since prog variables are bound at the time the frame is created  i . e . when the function is entered. the microlisp scheme may slow down free variable searches  since a name table may not be in core any longer when the search wants to scan i t . its advantages are that it is not necessary to insert the name of each variable at function entry time  and that the entire word is available for holding the binding  which  with the help of a few type b i t s elsewhere in the frame  may thus be a full-word integer or real number. 
code design 
conventional machines generally take the attitude that it must be convenient for any instruction to reference any word in the overall address space. this approach tends to produce instruction formats in which a large fraction  half or more  of the bits are devoted to a memory address. microlisp takes advantage of the observed fact that a given lisp function references rather few functions and variables and therefore can make do with very short addresses which just index a global table  of commonly used functions  or a function-local table  of local variables and less common functions . furthermore  a given name is usually only used as either a function or a variable  not both. microlisp tags each name in the tables with a function/variable f l a g   which eliminates the need for levels of l i s t structure as a syntactic device  and tags functions with an argument count  which eliminates the need for sublists as scope delimiters. thus microlisp code is essentially a string of byte-sized instructions  representing the o r i g i n a l s-expression in postfix form  where most bytes reference either a  global name table   gnt  or a  local name table   lnt  as just described. 
the lnt actually has additional internal structure: argument names come f i r s t   then prog and free variables  then everything else. the  binding  of a free variable is a pointer to the true binding  and the variable searching algorithm uses this knowledge: since a l l the bindings in a 
given frame are i d e n t i f i e d by a single pointer from the basic frame to the associated lnt  the searching process can t e l l from the tag if the match was on a free variable  and if so  follow the pointer one more step to obtain the value if desired. 
microlisp programs  l i k e lisp programs  are structured into functions. each function hab a header which gives the expected number of arguments and the length of the lnt. the former determines the size of the basic frame. the l a t t e r determines the function's entry point  since the lnt immediately follows the header and precedes the code  and also fixes the range of byte values that addresses the lnt: larger byte values address the gnt  after being adjusted downward by the size of the lnt. 
each gnt or lnt entry consists of a it-bit tag and a datum  pointer  whose interpretation depends on the value of the tag. to accommodate the usual organization of memories i n t o words  each nt is organized into blocks of entries: the arrangement for a 1-bit memory  for example  appears below. 

the algorithm for computing the location of the i ' t h name in a n is atually quite simple and only involves addition and s h i f t i n g . the possible tag values are presented immediately below and discussed in the following paragraphs. 
	const 	gvar 	ivar 	fvar 
	fno 	fn1 	fn1 	fn1 	fn1 	fn1 	fn1 
fn* 
function tag values must include the number of supplied arguments; the datum holds the function name. the tags fno . . . fn1 represent function c a l l s with the most common argument counts. fn* represents a function c a l l with more than 1 arguments: the actual argument count is supplied as the last argument  and the machine removes it before constructing the new frame. the primitive functions apply and apply* provide the a b i l i t y to c a l l a function whose name is computed: t h i s a b i l i t y is not represented d i r e c t l y by a tag value. 
the four variable tags represent different strategies for obtaining the value of the variable. a l l variable references eventually result in pushing the value of the variable onto the end of the current frame extension; a function c a l l severs the appropriate number of arguments from the end of the old frame extension for incorporation in the new basic frame. const  constant  simply pushes the datum i t s e l f . gvar  global variable  pushes the contents of the value c e l l of the variable whose name is the datum  or traps if the value c e l l contains nobind. ivar  indexed variable  does not 
use the datum: it just pushes the n'th value from the basic frame  where n is the actual byte value. fvar  free variable  
works s i m i l a r l y   but takes the value as a pointer to the true binding; if the pointer has not been set up  a stack search occurs f i r s t to f i n d the nearest binding and set the pointer to i t . 
a few primitive operations  such as returning from a function  cannot be represented by function c a l l s   so a few byte values are reserved for them. these are the only real  opcodes  in microllsp. some of them are followed by displacements or other parametric information in the next byte or bytes; a few  store  dstore  are followed by an ordinary variable reference which is interpreted specially. the convention followed in the description of the opcodes  and also in the examples in appendix b  is that upper-case words l i k e store represent opcodes; lower-case words represent parameter bytes; upper-case words in  brackets  represent references to functions; lower-case words in brackets represent references to variables. 
data movement 
store    v   
this causes the top value on the stack  a  to be stored. the interpretation depends on the tag of v: 
ivar  gvar: 
the value in the binding is replaced by 
z. 
fvar: 
the value in the addressed binding is replaced by z. 
const: 
     error  trap . fno . . . fn1: 
the function is called at i t s  store  entry point with one more argument than i t s tag specifies. 
fn*: 
the function is called at i t s  store  entry point with one more argument than the count  immediately below z on the stack  specifies. 
dstore    v   
performs the same action as store followed by pop. 
addrx  n1  n1; addrxx  n1  n1 n1 these serve to increase the range of addresses. the 1-byte or 1-byte parameter is interpreted as an address in the lnt or gnt as appropriate. 
pop 
removes the top item from the stack. 
copy 
pushes the top value on the stack onto the stack  only apparent use is for selectq. 
arg 
if n is the top value on the stack  an integer   replaces n by the n'th argument of the function. 
setarg 
if z is the top item and n is the next item  an integer   sets the n'th argument of the function to z and removes n from the stack  but retains z  squeezing n out . 
1 
control 
the jump opcodes are followed by a parameter byte  d  which is interprete as a 1's complement address displacement r e l a t i v e to the opcode i t s e l f . if positive  d is adjusted by +1 to eliminate meaningless small values. a few values of 1 are reserved to indicate extension into a second byte to provide a larger range of displacements. 
jump  d 
always jumps d bytes relative to the instruction. 
tjump  d 
tests the top value on the stack and pops i t ; then jumps if the datum was true  not nil  . 
fjump  d 
the inverse of tjump  jumps if nil . 
ntjomp  d 
like tjump  but pops the value only if the jump f a i l s  value is nil . this is for cond's with clauses lacking a consequent  where the value of the test becomes the value of the cond if t r u e . 
typejump  t  d 
the bottom b i t s of t give a type number; the top b i t of t selects jumping on true or false. 	the top value on the stack is removed  then jump or no jump depending on i t s type. 
gotoself 
c a l l s t h e c u r r e n t f u n c t i o n r e c u r s i v e l y b y jumping t o i t s e n t r y p o i n t a f t e r r e p l a c i n g t h e arguments  i . e . a progiter-type c a l l . 
return 
returns the top value on the stack as the value of the current function. 
conclusions and comments 
microlisp programs are consistently one-third to one-fourth the size of bbn-lisp compiled programs  and the nicrolisp compiler is about one-third the size of the corresponding part of the bbn-lisp compiler. some of the former advantage is due to design decisions in bbn-lisp which result in bulky code: its lisp 1   for example  is remored to produce code one-third the size of bbn-lisp or only one-third larger than microlisp. however  t h i s compactness is achieved at the expense of many of the a t t r a c t i v e features of bbn-lisp: r e c a l l the observations about compilers in the introduction  since no microlisp machine exists  there are no comparable timing data. however  a microprogrammed implementation and a software interpreter are in preparation. 
microlisp has been presented as a machine language  but s l i g h t additions would permit unambiguous decompilation into the o r i g i n a l s-expression for e d i t i n g . this approach is only feasible in general when the machine language closely resembles the source code: compilers for conventional machines must 
rearrange and suppress the o r i g i n a l program structure extensively to achieve e f f i c i e n t execution. interpretive systems  of course  generally do reconstruct the source text from an intermediate representation  often using their knowledge of the program structure to advantage  e.g. indenting to indicate depth of l o g i c a l nesting . 
several factors prompted the author to investigate the type of design just presented. one was the feeling that the constant demands from the a r t i f i c i a l intelligence community for larger primary memories were based as much on disinclination to spend time contemplating alternatives to t r a d i t i o n a l machine and program organization as on a real need to deal with larger amounts of information. another was the hope  based on an e a r l i e r experience with a small computer1  that a lisp minicomputer could provide  at a fraction of the cost  the kind of f a c i l i t i e s now available only through large  expensive time-shared i n s t a l l a t i o n s   a recent product announcement for a desktop basic machine is 
encouraging in this regard. 
realizing t h i s hope for less expensive lisp systems requires compressing the data as well as the program. one approach is to provide f a c i l i t i e s for the user to define his own packed data structures; a simple proposal along this l i n e is described in an appendix. another is to consider  compiling  data in a manner similar to programs. a careful reading of the microlisp design reveals that the encoding scheme works on a r b i t r a r y l i s t s   not just programs. the essential ideas are: eliminating cdr pointers by forcing l o g i c a l l y successive data to be physically consecutive; eliminating non-atomic car pointers by associating an operand count with each operator  so the end of a sublist 
      subexpression  is defined i m p l i c i t l y ; compressing atoms by use of tables  on the assumption that some few atoms  different for different contexts  w i l l account for most of the 