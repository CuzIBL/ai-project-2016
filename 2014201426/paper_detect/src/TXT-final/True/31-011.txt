 
　this paper presents a prototype of an interface system with an active human-like agent. in usual human communication  non-verbal expressions play important roles. they convey emotional information and control timing of interaction as well. this project attempts to introduce multi modality into computer-human interaction. our human-like agent with its realistic facial expressions identifies the user by sight and interacts actively and individually to each user in spoken language. that is  the agent sees human and visually recognizes who is the person  keeps eye-contacts in its 
facial display with human  starts spoken language interaction by talking to human first. 
key words : ai application  multimodal interface  autonomous agent  spoken dialogue  visual recognition  facial display 
1 	introduction 
　in normal human communication  face-to-face communication in particular  humans activate many communication modes/channels in parallel and exchange verbal and non-verbal signals. sight and hearing are the examples of such modes. as a result  a message conveyed by such modes can be reinforced each other  so that communication becomes highly flexible and meaningful. 
　on the other hand  numerous researchers have studied and proposed a variety of intelligent agents . it can be said that an essential common to all such research is to develop agents which engage and help all types of end users. therefore  agents should have the capacity for user-friendly and easy-to-use interaction with users. 
* hasegawa et 1 .go.jp 
　recently  in view of these facts  research into multi modal interaction has become popular. such projects attempt to understand the characteristics and utilization of human modes  and to introduce knowledge regarding them into human-computer interaction. however  these aspects have  until now  remained open problems. 
　we have been working on these problems. for example  we have collected and analyzed data on human behavior during interactions with a simulated spoken dialogue system . we are on the way to the development of a mathematical model which describes the activation  integration  recognition and learning process of multi modal information. however  it is necessary to evaluate the mathematical model developed. for mathematical modeling and its evaluation  a multi modal interaction system should be developed and examined by all types of end users. 
　in this paper  we describe an active agent oriented multimodal interface system with image and speech recognition/synthesis functions as the first prototype of the research project. 
　the prototype displays a moving human-like agent with realistic facial expressions to promote smooth interaction with users  see section 1 . the agent can identify the user on sight and provide active interaction to users in spoken language. that is  the agent can execute such tasks as follows. 
1. the agent starts spoken language interaction by talking to human first. 
1. the agent sees human and visually recognizes who is the person. 
1. the agent keeps eye-contacts in its facial display with human. 
　as a result  the agent can provide individual interaction with each user. in other words  the agent can respond differently to each individual user. these are achieved by the integration of image and speech recognition/synthesis technologies. 
1 	related works 
　the idea of introducing a human-like agent into human-computer interaction was proposed in the mid 1's by alan kay. however  at that time  it was hard to develop agents which could provide  ordinary  interaction modes for users because the computing power and costs in the mid 1's were not prohibitive. 
　john sculley  for example  proposed a human-like agent called phil in the   knowledge navigator  in 1  but it was demonstrated only in the concept video  and the utilization of visual functions was not considered. 
　nevertheless  as the 1's move into the 1's  realistic agents with some interaction modes have begun to appear. in the following  some representative researches which utilizes human-like agents are refered to. 
　takebayashi et al. have developed a spoken dialogue system  sds  with a cartoon like but moving facial display  agent    as their system does not have visual functions  it tries to find a user using a special switch in a floor mat. 
　nagao et al. have developed a sds with a humanlike agent which joins human-human conversations and presents beneficial information for users  1  1 . the agent is texture mapped with a real human texture  and the appearance is realistic. however  as this system does not have visual functions  the agent determines the presence of the user s  from his/her voice. 
　maes et al. are developing human-like agents which assist users with daily computer-based tasks . in their framework  multi-agent collaboration is discussed based on learning agents. however  only simple caricatures are used to convey the state of the process  agent  to users. as for input from users  only the standard devices  keyboards and others  are used. therefore  the channels between human and computer are not sufficient for natural interaction with users. 
　the apple newton with its agent software and general magic's messaging agents are  will be  marketed as commercial products which employ humanlike agents  but neither visual or speech recognition functions are supported. 
　the prototype system proposed in this paper can provide multi interaction modes  sight  hearing  speaking  and facial expressions  which are essential for intimate communication between humans and computers. 
　the facial image synthesis technique applied here is also used in videophone and video conference communication. however  our facial display is an autonomous agent  not a duplication of the user's face. 
　 this paper does not deny the utilization of keyboards and pointing devices. the important point is whether the interaction system can give users opportunities to select acceptable modes or devices  or not.  
1 	why human-like agent  
　in a previous research project  an experiment was carried out to collect data on human behavior in interaction with computers . in this experiment  forty subjects were requested to speak with a simulated spoken dialogue system.  the system used in this experiment did not display any visual agents.  
　after the experiment  information was obtained from the subjects by questionnaires. the following results were obtained: 
1. seven subjects voluntarily requested to display facial symbols to speak with. 
1. if the subjects feel that the system behaves like a human  they feel intimacy towards the system. 
　based on this information  it can be said that realistic human-like agent promotes human-computer interaction. as a result  it was decided to apply a realistic and human-like agent as an interface surface between humans and the computer. 

	hasegawa etal 	1 
　

fig.1 facial texture and fitted sd model. 
1 prototype system 
1 	system architecture 
　the developed system consists of three work stations for real time image and speech processing  one auto-focus ccd camera and one microphone. these are all standard hardware and equipment. 
　figure 1 illustrates an outline of the system architecture. it consists of the following four sub-systems and a interaction manager. 
1. a facial display sub-system that generates threedimensional facial images. 
1. a vision sub-system that recognizes and distinguishes users' facial images. 
1. a speech recognition sub-system  that recognizes speaker- independent continuous speech. 
1. a speech synthesis sub-system that generates voice output. 
1. a interaction manager that controls inputs and outputs of the sub-systems. 
　the details of these sub-systems are described in the following subsections. 
1 	facial display sub-system 
　the face of the agent is composed of approximately 1 polygons and is modeled three-dimensionally . 
　the appearance of the face is rendered by the texture mapping technique which is commonly used in computer graphics. the facial texture employed in our system is taken from a photograph of a young man. figure 1 illustrates a 1-d facial model fitted onto the texture used in the system. 

fig.1 samples of synthesized communicative facial displays  the agent    a neutral   b happiness   c anger   d sadness   e surprise   f sleep. 
　facial displays are synthesized by local deformations and rotations of the polygons. currently  eyebrows  eyeballs  eyelids  mouth and head orientation of the facial model are controllable. as a result  the action units in facial action coding units  facs   are available on the system. 
　moreover  the system can control both action degrees and action speeds of each facial part indepen-
　
dently to provide realistic moving facial displays. figure 1 shows samples of synthesized communicative facial displays  the agent . these are neutral  happiness  anger  sadness  surprise  and sleep. 
　it is common knowledge that  eye-contact  plays a vital role in human communication. for this reason  eye-contact was introduced into the prototype system  so that the agent may become friendly with users. 
　the current system controls the agent's eyes continuously so as to look at users during the interaction. figure 1 shows a comparison between facial displays with and without eye-contact. 
　the number of parameters for the facial display is 1 for the current system. the base performance of the facial display sub-system is approximately 1 frames per second on sgi indigo1. 
　currently  the system in-corporates thirteen parameter-sets  command sequences  for moving facial displays  considering the correspondence with tasks of the system  see subsection 1 . 
　the quality of the texture-mapped facial images is high  making them much more realistic than standard rendered images or animations. 

fig.1 comparison between facial displays  a  with eye-contact and  b  vnthout it. 
1 	vision sub-system 
　the purpose of the vision sub-system is not only to detect the presence of a user  but also to identify a facial image as a specific person. 
　user's facial images are taken from a standard video camera connected to the system. the camera is set beside the monitor  which displays the agent  so as to face the user. 
　the algorithm implemented is based on  kurita et al.  so that real-time and robust processing will be available on the prototype system. the method employs higher order local autocorrelation features as primitive features at the first stage of feature extraction. those features are then linearly combined using linear discriminant analysis or multiple regression analysis to identify the user. 
　thus  a moving face in input image sequence is recognized and identified in real-time. the background of the input images does not need to be constrained or segmentation free. 
　the recognition rates of the prototype system were approximately 1% for identification of 1 persons. the recognition speed was about 1 frames per second on a sun sparcstation 1. this appears to be satisfactory for a human-computer interactive system. 
1 	speech recognition sub-system 
　user's utterances are obtained via a microphone set in front of the monitor displaying the agent. speakerindependent and continuous speech recognition is implemented in the prototype system. the speech recognition sub-system recognizes a  short  sentences one after another. the algorithm is based on  itou et al. . 
　the recognition rate was approximately 1% spontaneous speech of for 1 subjects  1 utterances   but the experiments were carried out before the sub-system was integrated with the other subsystems. calculation time required for speech recognition ranges from 1 sec after the end of each user's utterance. 
　currently  the sub-system can deal with a vocabulary of approximately 1 words and reject utterances that have low likelihood scores. 
1 	speech synthesis sub-system 
　the speech of the agent is synthesized by the speech synthesis sub-system in a male voice. the sub-system synthesizes a speech consisting of one or more sentences at a time. however  it cannot control output timing precisely. 
1 	interaction manager 
　currently  the whole system is controlled by the interaction manager. 
　the interaction manager receives messages  recognition results  both from the vision and speech recognition sub-systems in parallel. in order to obtain a high level of accuracy  it examines the order of the received messages and discards inadequate ones by following the state of the dialogue. 
	hasegawa etal 	1 
　then it analyzes the massages received and generates control commands for the facial display and speech synthesis sub-systems. 
　followings are available basic tasks controlled by the interaction manager on the current system. in interaction  these tasks are executed in parallel. all tasks are executed with moving facial displays of the agent. 
1. the agent finds a facial image  identifies it with a specific user and speaks to him/her actively. 
1. the agent answers questions concerning the date and the time by speech. 
1. the agent gives notice of incoming e-mail messages by speech. 
1. the agent sets and explains the user's time schedule. 
1. the agent records/replays messages from/to a specific user. 
1. the agent sleeps between tasks. 
1 example interaction with the prototype system 
1 preliminary arrangements 
　the experimental interactions are executed in an ordinary office environment. 
　before interactions  the agent requires a learning process of users' facial images and a background. as described in subsection 1  the background need not be constrained. in the learning process  approximately 1 images taken from a video camera are required for every user and the background. this process completed in a few minutes. 
　as for speech recognition  no preliminary arrangements are necessary. 
1 example of interaction 
　this section describes an example interaction between two users and the prototype system.  the original dialogue is in japanese.  in the following  a  ul and u1 denote the agent  user 1 and user 1  respectively. 
 ul sits in front of the system and looks at the monitor displaying the agent. 
　
1 discussions 	