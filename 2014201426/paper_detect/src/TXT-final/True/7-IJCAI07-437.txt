
statistical relational learning  srl  algorithms learn statistical models from relational data  such as that stored in a relational database. we previously introduced view learning for srl  in which the view of a relational database can be automatically modified  yielding more accurate statistical models. the present paper presents sayu-vista  an algorithm which advances beyond the initial view learning approach in three ways. first  it learns views that introduce new relational tables  rather than merely new fields for an existing table of the database. second  new tables or new fields are not limited to being approximationsto some target concept; instead  the new approach performs a type of predicate invention. the new approach avoids the classical problem with predicate invention  of learning many useless predicates  by keeping only new fields or tables  i.e.  new predicates  that immediately improve the performance of the statistical model. third  retained fields or tables can then be used in the definitions of further new fields or tables. we evaluate the new view learning approach on three relational classification tasks.
1 introduction
most statistical relational learning  srl  algorithms are constrained to operate with the specific representation they are given-a prm  friedman et al.  1  must use the schema of the input database  a logic-based system must use the predicates provided. yet  in many cases the original relationalrepresentation was not designed to empower the learning algorithm. for example  the schema for a medical database may have been chosen to simplify billing; biological databases often have a hierarchical schema  forcing an srl system to use long slot-chains or long clause bodies to find related data.
　in some cases an srl user may have the freedom to modify the representation for the sake of learning. even so  considering all relevant features or relations to include for a task is a difficult job by itself. ideally  we would like the learning algorithm to be able to discover and incorporate relevant  intermediate concepts into the representation. for instance  consider the well-known task of predicting whether two citations refer to the same underlying paper. the coauthor relation is potentially useful for disambiguating citations; for example  if s. russell and s.j. russell both have similar lists of coauthors  then perhaps they are interchangeable in citations. but the coauthor relation may not have been provided to the learning system. furthermore  coauthor can be used as a building block to construct further explicit features for the system  such as a new predicate sameperson.
　a start already has been made in change of representation for srl systems  under the name of view learning. initial approaches to view learning  davis et al.  1b; 1a  showed that even an application with a single table can benefit from new views: views can represent connections between related examples. moreover  a greedy approach to view learning can be useful. the present paper introduces important extensions for change of representations in srl. first  it provides a mechanism for learning a new view as a full new relational table  such as coauthor. second  it permits a newly-invented relation  or predicate  to be used in the invention of other new relations  such as sameperson. such re-use goes beyond simply introducing  short-cuts  in the search space for new relations; because the new approach also permits a relation to be from aggregates over existing relations  re-use actually extends the space of possible relations that can be learned by the approach. because this new work extends sayu by providing a mechanism for view invention by scoring tables  we call the resulting system sayuvista.
1 ilp and sayu
inductive logic programming  ilp  is a popular approach for learning in a relational environment. given a set of positive and negative examples and background knowledge  an ilp system finds a logical description of the underlying data model that differentiates between the positive and negative examples. this description is a set of first-order logical rules or clauses  which form a logic program.
　sayu  davis et al.  1a  is an srl system that combines ilp with bayesian network learning  by default with tree-augmented naive bayes learning  friedman et al.  1 . sayu is an acronym for  score as you use.  unlike other approaches to using ilp for defining new features  sayu scores each clause not by a standard ilp measure  but by how much it helps the model in which the clause is used. another system  known as nfoil  was developed in parallel with sayu and has this same property  landwehr et al.  1 . in the sayu approach  we start from an empty model  or a prior model . next  an ilp system generates rules. each generated rule represents a new feature which is added to the current model. we then evaluate the generalization ability of the model extended with the new feature  where generalization ability is measured as the area under the precision recall curve on a held-aside subset of the data. we retain the new model if the inclusion of the new feature significantly improves the model's generalization ability; otherwise we remain with the original model. this results in a tight coupling between feature construction and model building.
　sayu needs an ilp system to propose rules. in our work  we use aleph  srinivasan  1   which implements the progol algorithm  muggleton  1  to learn rules. this algorithm induces rules in two steps. initially  it selects a positive instance to serve as the  seed  example. it searches the background knowledge for the facts known to be true about the seed example. the combination of these facts forms the example's most specific or saturated clause. the key insight of the progol algorithm is that some of these facts explain this example's classification. thus  generalizations of those facts could apply to other examples. aleph defines the search space to be clauses that generalize a seed example's saturated clause  and performs a general to specific search over this space.
　sayu greedily searches for new fields or variables  defined by first-order logic clauses  that improve the prediction of the class field. sayu modifies the standard aleph search as follows. instead of using coverage  aleph passes each clause it constructs to sayu  which converts it to a binary feature. the feature is added to the current training set and sayu learns a new bayes net  a tan network in our case  incorporating this new feature. we measure performance by looking at the area under the precision recall curve on a heldaside tune set. if the feature degrades the performance of the network  sayu discards the feature and reverts back to the old classifier. then sayu returns control to aleph to construct the next clause. if the new feature improves the score of the network  then sayu retains the feature in the network. in contrast to aleph  after accepting a rule  sayu randomly selects a new seed example and reinitializes the search. thus  for a given seed  sayu does not search for the best rule  but only the first rule that helps. however  nothing prevents the same seed from being selected multiple times during the search.
1 learning new predicates
the initial approach to view learning  davis et al.  1b   suffers from two important drawbacks. first  it only creates new fields  not new tables. the new field definition has the same arity as the target predicate. second  the new fields are just learned approximations to the target concept. sayuvista addresses both shortcomings. it creates new predicates with arity greater than one  some capturing many-tomany relations  which require a new table to represent. furthermore these new predicates are no longer approximations to the target concept  but may be any concept that improves the statistical model.
　the original motivation for view learning centered on learning a statistical expert system to provide decision support to radiologists  davis et al.  1b . there we used srl because the learned statistical model sits on top of the national mammography database  nmd  schema  a standard established by the american college of radiology  acr  1 . the goal of the data set is to predict which abnormalities on a mammogram are malignant. we will use mammography as a running example to help illustrate the key components of the algorithm.
　sayu-vista  nfoil and sayu all learn definite clauses and evaluate clauses by how much they improve the statistical classifier. the key difference in the algorithms rests in the form that the head of the learned clauses takes. in nfoil and sayu  the head of a clause has the same arity and type as the example  allowing us to precisely define whether a clause succeeds for a given example and hence whether the corresponding variable is true. in the mammography domain  a positive example has the form malignant ab1   where ab1 is a primary key for some abnormality. every learned rule has the head malignant a  such as in the following rule: malignant ab1  if: archdistortion ab1 present   same study ab1 ab1   calc finelinear ab1 present .
the bayesian network variable correspondingto this rule will take value true for the example malignant ab1  if the clause body succeeds when the logical variable a is bound to ab1.
　sayu-vista removes the restriction that all the learned clauses have the same head. first  sayu-vista learns predicates that have a higher-arity than the target predicate. for example  in the mammography domain  predicates such as p1 abnormality1  abnormality1   which relate pairs of abnormalities  are learned. subsection 1 discusses scoring predicates that have higher arities than the target relation. second  sayu-vista learns predicates that have types other than the example key in the predicate head. for example  a predicate p1 visit   which refers to attributes recorded once per a patient visit  could be learned. in order to score predicates of this form  we introduce the concept of linkages  which are discussed in subsection 1. after discussing how to evaluate these types of predicates  we will present the full sayu-vista algorithm.
1 scoring higher arity predicates
sayu-vista can learn a clause such as:
p1 ab1 ab1  if: density ab1 d1   prior-abnormality-same-loc ab1 ab1   density ab1 d1  
d1   d1.
this rule says that p1 is true of a pair of abnormalities
ab1 and ab1 if they are at the same location  ab1 was observed first  and ab1 has higher density than ab1. thus p1 may be thought of as  density increase.  unfortunately  it is not entirely clear how to match an example  such as malignant ab1   to the head of this clause for p1. sayu-vista maps  or links  one argument to the example key and aggregates away any remaining arguments using existence or count aggregation. the next section describes the approach used for linkage; the remainder of this paragraph discusses aggregation. in existence aggregation  the clause succeeds for the given example  key  if there exists any bindings of the remaining variables for which the clause succeeds. count aggregationcomputes the number of bindings for these remaining variables for which the clause succeeds. currently  sayu-vista discretizes aggregatedfeatures using a binning strategy that creates three equal-cardinality bins  where three was chosen arbitrarily before the running of any experiments.
1 linkages
so far we have simplified matters by assuming that the first argument to the learned predicate has the same type as the example key. in our examples so far  this type has been abnormality id. there is no need to enforce this limitation. for example  in predicting whether an abnormality is malignant  it might be useful to use the following clause  where visit is a key that refers to all abnormalities found on a given mammogram:
p visit  :visit visit ab   massesshape ab oval .
predicate p is true of a visit  or mammogram  that contains at least one abnormality with an oval shape.
　linkage declarations are background knowledge that can establish the connection between objects in the examples and objects in the newly invented predicates. when these objects are of the same type  the linkage is trivial; otherwise  it must be defined. for mammography  we use linkage definitions that link an abnormality to its patient or to its visit  mammogram . the linkages for the other datasets we use are equally straightforward and are presented when we describe those datasets.
1 predicate learning algorithm
at a high level sayu-vista learns new predicates by performing a search over the bodies of definite clauses and selecting those bodies that improve the performance of the statistical model on a classification task. we use tree-augmented naive bayes  tan   friedman et al.  1  as our statistical model.
　the predicateinventionalgorithmtakes several inputs from a user. first  it needs a training set  which is used to learn the statistical model  and a tuning set  which is used to evaluate the statistical model. the user provides a pre-defined set of distinguished types  which can appear in the head of a clause. the user provides background knowledge  which must include linkage definitions for each distinguished type. the algorithm using an improvement threshold  p  to decide which predicates to retain in the model. a new predicate must improve the model's performance by at least p% in order to be kept. we used p =1 in all experiments. optionally  the user may input an initial feature set to the algorithm. algorithm 1 shows pseudocode for the sayu-vista algorithm.
　the clause search proceeds as follows. we randomly select an arity for the predicate. to limit the search space  we restrict the arity to be either the arity of the target relation  or the arity of the target relation plus one. next  we randomly select the types for the variables that appear in the head of the clause. the clause search uses a top-down  breadth-first refinement search. we define the space of candidate literals to add using modes  as in progol  muggleton  1  or aleph  srinivasan  1 . we score each proposed clause by adding it as variable in the statistical model. to construct the feature  we first link the predicate back to the example key as described in subsection 1. then we perform the necessary aggregation  discussed in subsection 1  to convert the clause into a feature. by default  the algorithm first tries existence aggregation and then tries count aggregation. the clause search terminates in three cases:  i  it finds a clause that meets the improvementthreshold; ii  it fully explores the search space;  iii  it exceeds the clause limit. after satisfying one of these conditions  the algorithm re-initializes the search process. every clause that meets the improvement threshold is added into the background knolwedge. therefore  future predicate definitions can re-use previously learned predicates. as in prior work  davis et al.  1a   the algorithm terminates when it exceeds the global time limit.
1 data and methodology
cora. the objective of this dataset is to predict whether two citations refer to the same paper. the dataset was originally constructed by mccallum et al. . we used the same version of the data as kok and domingos . cora includes 1 citations to 1 computer science papers  resulting in 1 positive examples and 1 negative examples. the background knowledge includes data on title  venue  author s   and year for each citation. we defined paper  title  venue  author and year as keys that can appear in heads of clauses. we link a paper to its title  venue  author s  and year fields. we aggregate over papers and authors.
　uw-cse. this common srl dataset was constructed by richardson and domingos  and is publicly available. the goal is to predict the advisor of a graduate student. the information comes from the university of washington cs department and contains 1 positive examples versus 1 negative examples. we defined students  professors  courses and publications as keys that could appear in the head of a clause. we link a course to a graduate student by the ta relationship  and we link papers to a graduate student by the author relationship. we link a course to a professor by the teaches relationship and we link papers to a professor by the author relationship. we aggregate over students  professors  papers and courses.
input: train set labels t  tune set labels s  distinguished types d  background knowledge b  improvement threshold p  initial feature set finit
output: feature set f  statistical model m
f = finit;
bestscore =1; while time remains do
randomly select the arity of predicate to invent;
randomly select types from d for each variable in the head of the predicate; selectedfeature = false; while not selectedfeature  do
predicate = generate next clause according to breadth first search;
/*link the predicate back to the target relation */ ;
linkedclause = link predicate  b ;
/* convert the linkedclause into a feature that the statistical model can use */;
newfeature = aggregate linkedclause t  s ; new = f “ newfeature;
new = buildtannetwork t  fnew ;
newscore =areaunderprcurve m  s  fnew ;
/*retain this feature*/ ;
if newscore   p   bestscore then
f = fnew;
bestscore = newscore;
m = mnew;
   add predicate into background knowledge; selectedfeature = true; end
end
algorithm 1: sayu-vista　mammography. the objective of this dataset is to predict whether an abnormality on a mammogram is benign or malignant  davis et al.  1b . this dataset consists of a radiologist's interpretation of a mammogram and not the raw image data. the dataset contains 1 positive examples and 1 negative examples. we used the same version of the data as davis et al.  1b . we define abnormality  visit and patient as keys that can appear in the head of the clause. we aggregate over abnormalities.
1 experiments and results
we compare sayu-vista to two srl systems in our experiment. first  we compare sayu-vista to sayu  davis et al.  1a  as it is the state-of-the-art view learning implementation  a follow-up to the original view learning paper  davis et al.  1b . however  sayu only learns additional fields for existing tables; these fields are defined by learned rules that are approximations to the target concept. we also compare sayu-vista against another leading srl system-one that already has been applied with success  as measured by cross-validated precision-recall curves  to two of our application tasks and that has been receiving considerable attention: markov logic networks  richardson and domingos  1   publicly available as the alchemy system. finally  we compared these three srl systems against aleph on all three data sets  and the srl systems significantly outperformed aleph on all three data sets. therefore  to simplify the presentation we limit the remaining discussion to the three srl systems.
　all three srl systems are evaluated by precision-recall curves estimated by cross-validation with significance of differences tested by a paired two-tailed t-test on areas under the precision-recall curves  aucpr  across the different folds. we are careful to repeat any tuning of parameters on each fold of cross-validation  without looking at the test set for that fold  by dividing the data into a training set and tuning set. in this we follow the methodology of the developers of both mlns and sayu. for sayu-vista  as for sayu  we use the training set to learn the network parameters  while we use the tuning set to score potential clauses. for all datasets we use aucpr as our score metric. however  we only look at aucpr for recalls − 1. we do this for two reasons. first  precision can have high variance at low levels of recall. second  in domains such as mammography we are only interested in high levels of recall. a practicing radiologist would need to achieve at least this level of recall. a clause must improve the aucpr  for recall − 1  by at least 1% in order to be retained in the network. this is an arbitrary parameter setting; in fact we did not try any other thresholds. we had a time-based stop criteria for both sayu and sayuvista. for uw-cse each fold was given two hours to run  whereas for mammographyand cora each fold received three hours runtime. we gave uw-cse less time because it was a smaller data set. in practice  the time is not a limiting factor because few changes occur after the first 1 minutes for any of the tasks. mln runs were not time-bounded. to offset potential differences in computer speeds  all experiments were run on identically configured machines.

ecall
figure 1: precision-recall curves comparing sayu-vista and sayu on cora
　we employed the default structure learning algorithm for mlns and performed limited manual tuning of the parameters of the system to maximize aucpr  while maintaining acceptable execution times. we report the best aucpr values we obtained  over all attempted parameter settings. note that we did not do any parameter tuning for sayu-vista. the average  per-fold run-times for mlns were all significantly longer than for either sayu or sayu-vista. the average  per fold run time for learning structure were five hours for cora  seven hours for uw-cse and three hours for mammography.
1 discussion of results
cora. following kok and domingos  we performed two-fold cross validation on this dataset for five different random train-test splits. we divided the training set in half  to form a new training set and a tuning set. each fold received three hours of cpu time to run. sayu and sayu-vista could evaluate up to 1 clauses  before selecting a new seed or clause head.
　table 1 reports the average aucpr  recall −1  for cora and the p-value for a two-tailed paired t-test between sayuvista and the other two algorithms. sayu-vista performs significantly better than sayu on this domain. figure 1 shows precision-recall curves for all algorithms on this dataset. we pooled results across all folds to generate the curves. sayu-vista dominates sayu throughout precision-recall space. however  mlns have a slightly higher average aucpr than sayu-vista does  although the difference is not significant. mlns received an advantage over sayu and sayu-vista in this task  as mlns started with an expert knowledge base.
　uw-cse. following richardson and domingos   we performed five-fold cross validation on the uw-cse dataset. we used two folds for the training set and two folds for a tuning set. each approach could evaluate up to 1 clauses  before either selecting a new seed  sayu  or a new

ecall
figure 1: precision-recall curves comparing sayu-vista  mlns  and sayu on uw-cse
predicate head  sayu-vista .
　table 1 reportsthe averageaucpr for uw-cse and the pvalue for a two-tailed paired t-test comparing sayu-vista to the other approaches. sayu-vista comes close to performing significantly  1   p   1  better than sayu on this domain. although performance varies widely between the 1 folds  sayu-vista had a higher aucpr than sayu on each fold. sayu-vista also comes close to outperforming mlns on this data set  winning on four out of five folds.
　figure 1 shows precision-recall curves for sayu  sayuvista and mlns on this dataset. we pooled results across all five folds to generate the curves. even though we measured aucpr for recall − 1  sayu-vista dominates sayu for most levels of recall. however  mlns dominate sayu-vista for low levels of recall  whereas sayuvista tends to dominate for the high levels of recall. we also compared the performance of sayu-vista  average aucpr of 1  and mlns  average aucpr of 1  for aucpr for all levels of recall. again  there is no significant difference. sayu-vista has a higher variation in per fold aucpr score than mlns do. one reason for sayuvista's increased performance for high recall is that we are expressly optimizing for this metric. mlns also receive one advantage over sayu and sayu-vista in this domain  in that they start with an expert defined knowledge base.
　mammography. following davis et al.  1b  we performed ten-fold cross validation on this dataset. we used four folds for a training set and five folds as a tuning set. each algorithm could evaluate at most 1 clauses for a given seed  sayu  or clause head  sayu-vista .
mlnsayusayu-vistap-value  vs sayu p-value  vs. mln cora1.1.1.1.1uw-cse11111mammography11111 1table 1: average aucpr for recall − 1 for each task using tan as the statistical　for the previous two datasets  we initially started with a bayesian network that only contained a feature for the target predicate. however  in the mammography domain we have access to a set of expert defined features  from the nmd . furthermore  we could define a set of aggregate features as davis et al.  davis et al.  1b  did. opposed to starting model.

ecall
figure 1: precision-recall curves comparing sayu-vista 
mlns and sayu on mammography
with an empty network structure  we begin with a network that contained both nmd features and the aggregate features. table 1 reports the average aucpr over all folds. we use a two-tailed paired t-test to compute significant results  and the p-value for the test can also be found in table 1. we find no significant difference between sayu-vista and sayu on this task  yet sayu-vista does not perform any worse than sayu. however  both sayu and sayu-vista significantly outperform mlns on this domain.
　figure 1 shows precision-recall curves for both algorithms on this dataset. we pooled results across all folds to generate the curves. on this dataset  sayu-vista and sayu have comparableperformancefor all levels of recalls. sayuvista and sayu both dominate mlns for all levels of recall. note that  as in the uw-cse domain  mlns tend to have better performancefor low levels of recall. we feel there are several potential reasons that sayu-vista did not perform significantly better than sayu on this domain. first  for this application  mlns and sayu receive a large number of features-the precomputed aggregates-that sayu-vista could potentially learn  but mlns and sayu cannot easily capture. second  this domain contains many more constants than other domains  thus by leveraging aleph  sayu has a smaller and more directed search. finally  the mammography domain contains only three relations  while the other domains each have approximately twenty relations. thus  on those domains there is more room to exploit the ability to learn predicates that  1  have differenttypes in the head and  1  represent new tables.
　in order to allow mlns to run on this domain  we had to do drastic sub-sampling of the negative examples. mlns struggled with having to ground out a network with the large number of examples that this data set contains. another possible explanation for sayu and sayu-vista's better performance is that we seed the algorithm with an initial feature set. however  we ran the experiments where we started sayu and sayu-vista with an empty network structure and it still significantly outperformed mlns.
1 further investigation of sayu-vista
sayu-vista adds several components to sayu. first  it adds count aggregation: the ability to handle many-to-many and one-to-many relationships by adding a feature to the statistical model that counts the number of satisfying assignments for a predicate. second  linkages allow us to learn entirely new tables. third  we allow for previously invented predicates to appear in the definitions of new predicates. without linkages  sayu-vista reduces to sayu. to discover the extent to which the other two features contribute to sayu-vista's performance  we consider removing the first and third components from sayu-vista and observed the resulting performance.
　the first component  counting the number of satisfying assignments  does not help in either cora or the mammography domain. it is never used in cora and it is only used twice in mammography. consequently  we do not need to consider removing it on these domains. however  it appears 1 times  or about twice per fold in the uw-cse domain. removing it reduces the aucpr for this domain from 1 to 1. this degrades performance on four out of five folds  yet  the change is not significant  having a p-value of 1. however  it seems that even though counting does not help on two out of three domains  it can potentially be useful for an srl system.
　the other component of sayu-vista we remove is the third  that of adding the learned predicates into background knowledge. disabling this feature slightly improves performance in mammography  increasing aucpr from 1 to 1. however in cora it decreasing aucpr from 1 to 1 and in uw-cse the performance declines from 1 to 1. across all these experiments none of the changes are significant.
　on cora  the benefit comes only from the introduction of linkages. on uw-cse  the benefit comes from both linkages and the count aggregation. in a sense linkages are the key innovation of sayu-vista. linkages allow us to both learn new tables and to learn concepts that are not simply approximations to the target concept. reusing learned predicates does not seem to provide a win. asserting each learned predicate might unnecessarily widen the search space.
1 related work and conclusions
we already have discussed how the present paper advances the state-of-the-art in view learning. the paper also is related to propositionalization within ilp  lavrac et al.  1   particularly to propositionalization approaches that incorporate aggregation  krogel and wrobel  1; knobbe et al.  1; popescul et al.  1; popescul and ungar  1 . in these approaches clause bodies are constructed that define new features or propositions. the value of such a feature for a data point  or example  is obtained by binding one of the variables in the clause body to the example's key  and then aggregating over the remaining features. in this fashion  the definition of a feature is equivalent to a definite clause whose head is  p x    where p is an arbitrary predicate name and x is the body variable that is bound in turn to each example's key. both existential and count aggregation have been employed before  krogel and wrobel  1 . in fact  all the approaches cited above have used more complex aggregations than does sayu-vista  and these could be incorporated easily into sayu-vista. the novel properties of sayu-vista relative to propositionalization by aggregation are the following. first  subsets of the variables in the clause body may be mapped back to an example's key  via the domain-specific linkagerelations  thus enabling new tables or non-unarypredicates to be learned  having different arities and types than the examples. second  each time a potential new table or predicate is scored  an entire statistical model is constructed  and the new predicate is retained only if yields an improved model. third  once a predicate is learned  it is available for use in the definitions of further new predicates. although one piece of work cited above  popescul and ungar  1  does in fact allow some further use of some learned predicates  these new predicates are based on clustering and are constructed in an initial pre-processing step  before the learning of predicates to define new features for the statistical model; the latter features are never re-used.
　other general areas of related work are of course constructive induction  rendell  1  and predicate invention  muggleton and buntine  1; zelle et al.  1   as well as learning latent or hidden variables in bayesian networks  connolly  1 . predicate invention is a specific type of constructive induction  where a new predicate is defined not based directly on examples of that predicate  but on the ability of that predicate to help in learning the definitions of other predicates for which examples are available. the classic difficulties with predicate invention are that  unless predicate invention is strongly constrained:  1  the search space of possible predicates is too large   1  too many new predicates are retained  thus reducing efficiency of learning  and  1  the ability to invent arbitrary new predicates leads to overfitting of training data.
　the present work can be seen as a type of predicate invention  because arbitrary clauses are constructed whose heads do not have to unify with the examples-they may have arities and types different from the examples. sayu-vista is analogous to chillin  zelle et al.  1  and closed world specialisation  srinivasan et al.  1 . both of those systems search for an intensional definition of a clause based on existing predicates  just like sayu-vista. one key difference is that those systems don't directly search for new predicates. chillin is demand driven  and closed world specialisation invents predicates to handle exceptions to the theory  whereas sayu-vista directly searches for new predicates. the other important differenceis how the systems evaluate new predicates. the other systems use traditional ilp metrics  such as compaction. the approach in the present paper is to constrain predicate invention by requiring invented predicates to be of immediate value to the statistical learner in order to be retained for further use. the empirical success of sayu-vista-that it does not hurt performance and it sometimes helps-indicates that this efficacy test is a successful constraint on predicate invention.
　the topic of learning bayesian network structures with the introduction of new  latent  variables faces similar obstacles to predicate invention. because the new variables are unconstrained by the data  their introduction into bayesian network structure learning permits overfitting of the training data  in addition to increasing search complexity. sayu-vista may be seen as introducing new variables into the structure learning task; nevertheless  by requiring these new variables to be defined using existing  pre-defined or recently learned  relations  these variables are partially constrained. the empirical success of sayu-vista provides some evidence that this constraint on the new variables helps to avoid overfitting. sayu-vista's use of tan bayes nets also helps to reduce the search space. both predicate invention and bayes net learning with the introduction of new variables are widely noted to be extremely difficult tasks. this paper provides some evidence that attempting to address both tasks at the same time  within an srl framework  can actually make both tasks somewhat easier.
acknowledgements
this work was supported in part by u.s. national science foundation grant iis 1 and by an nlm training grant to the computation and informatics in biology and medicine training program  nlm 1lm1 . jan struyf is a postdoctoral fellow of the fund for scientific research of flanders  fwo-vlaanderen . we would also like to thank pedro domingos  stanley kok and the rest of alchemy team for answering our questions regarding the alchemy system and for providing the cora dataset.
