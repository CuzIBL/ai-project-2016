 
we present an overview of symbolic-statistical modeling language prism whose programs are not only a probabilistic extension of logic programs but also able to learn from examples with the help of the em learning algorithm. as a knowledge representation language appropriate for probabilistic reasoning  it can describe various types of symbolic-statistical modeling formalism known but unrelated so far in a single framework. we show by examples  together with learning results  that most popular probabilistic modeling formalisms  the hidden markov model and bayesian networks  are described by prism programs. 
1 	introduction 
we can make programs probabilistic by incorporating probabilistic elements  which is rather obvious. what is less obvious would be the existence of a general learning algorithm for these probabilistic programs. in this paper  we present an overview of symbolic-statistical modeling language prism  programming in statistical modeling . prism was born as the integration of logic programming and the general learning algorithm with which programs can change their behaviors  a posteriori  by learning from examples. prism is a new type of programming language designed for symbolic-statistical modeling of complex objects and the real world. 
　the theoretical basis of prism is fixed point  least model  semantics and probability theory  but the development was spurred by the noticeable success of h m m  hidden markov model  in speech recognition  and their applications to genetic information processing   rabiner 1  asai et al. 1   a rapid surge of the interest in statistical methods in nlp  natural language processing   charniak 1   well-developed uncertainty handling mechanisms in bayesian networks  pearl 1  and 
   *this paper is partly based on a report submitted for computational logic network news letter. 
  email: sato cs.titech.ac.jp 
*email: kame cs.titech.ac.jp 
1 	probabilistic reasoning 
very encouraging results from ilp  inductive logic programming   muggleton 1 . they are all symbolic systems able to learn from statistical data. 
　prism offers a common vehicle for these diverse research fields in symbolic-statistical modeling  and also gives us hopes for building even more complex and intelligent systems. this is because prism programs can be arbitrarily complex  no restriction on the form or size   and what is more  regardless of the complexity  there is  at least in theory  a method for prism programs to learn from positive/negative examples. to put it differently  we have the possibility of training arbitrarily large programs so that they behave as we desire. 
　in the rest of paper  we describe prism in stages. we first describe the semantic aspect of prism programs in section 1  then move to an example in section 1 where a program modeling human blood types is presented. section 1 explains learning and the em  expectation maximization  algorithm. prism as a programming system is described in section 1  followed by examples of the hidden markov model and bayesian networks in section 1. section 1 contains related work and section 1 conclusion. 
1 	prism programs and distributional semantics 
prism programs are roughly defined as logic programs with a probability distribution given to facts. so we can compute any recursive functions as a special case of nonprobabilistic facts. to capture prism programs mathematically however  we need a probabilistic generalization of fixed point semantics. 
　a prism program db is a set of definite clauses written as db = fur where f is a set of facts  unit clauses  and r is a set of rules  non-unit clauses . in the theoretical setting  we always equate clauses with the set of their ground instances  and allow db to be countably infinite. 
　what makes prism programs differ from usual logic programs is a basic joint probability distribution pf given to f. it means that ground unit clauses a1   a1 ... belonging in f are probabilistically true and the probabilities are determined by the joint probability distribution 

1 	a blood type example 
as a typical symbolic-statistical model  we present a blood type program. it is very like a prolog program and as usual  variables begin with upper case letters. we hope that the following comments will help the reader understand the program. 
1. this program describes how one's blood type  bloodtype/1  is determined from the genes inherited from the parents. it happens to be nonrecursive but there is no problem in writing recur-

1. sampling all bsw/1 atoms determines which ground bsw/1 atom is true  thereby determines which ground gene/1 atom is true. non-unit clauses together with these true ground atoms jointly determine the truth of the remaining atoms  i.e. genotype/1 and blood bloodtype/1. in this way  we get a sampling from the joint probability distribution for all ground atoms. 
1 	learning 
the above program is a faithful representation of genetic knowledge concerning how one's blood type is determined. it is a computational model  runnable  
1
bsw id n r  is called bs atom in  sato 1 . 
1
 currently  we admit only the class of bs programs as 
prism programs. expanding the class of bs programs is a future task. 
	sato & kameya 	1 

and at the same time  a statistical model  i.e. provides a probability distribution for bloodtype/1. since blood type in the real world has a probability distribution  our learning task is to approximate it in terms of pdb{bloodtype   = 1}  the distribution for bloodtype/1 defined by our program  by assigning each binary switch bsw/1 a suitable parameter value. 
　let {bloodtype a   bloodtype a   bloodtype b   bloodtype  o  ...} be our observations of people's blood types. parameters associated with bsw/1 are determined by a maximum likelihood method; they are calculated as the ones that maximize the probability of the conjunction of these observed atoms. 
　an algorithm we use for maximization is the em  expectation maximization  algorithm  tanner 1 . it is an iterative method in statistics for maximizing likelihood and has been used in various fields  in particular in the baum-welch learning algorithm for hmms in speech recognition  rabiner 1 . 
　we successfully combined the em algorithm with bs programs to derive a general learning algorithm for bs programs  sato 1 . it should be emphasized that the derived learning algorithm is valid for the entire class of bs programs  and second that the class of bs programs seems fairy large. for example  it covers bayesian networks  hmm and pcfg  probabilistic context fvee grammar   currently known as the most powerful symbolic-statistical formalism in ai. for more details  see  asai et al. 1  charniak 1  charniak et al. 1  pearl 1  poole 1  rabiner 1 . 

figure 1: prism programming system 
1 	prism programming system 
prism programming goes through three phases: programming  learning and execution. since the learning 
1 	probabilistic reasoning 

figure 1: a prism program 
phase and the execution phase require rather different treatment of an original program  prism translates it into two specialized programs  one for execution and the other for learning. the latter works in the learning phase cooperating with the built-in em learning routine to perform maximum likelihood estimate. 
1 	structure of a prism program 
a prism program is comprised of three parts  a model  a utility program and the control declarations  figure 1 . the model part is just a logic program whose purpose is to generate possible proof trees of a target atom which represents our observations. those trees may or may not contain special built-in predicate bsw/1. since bsw/1 is probabilistic  so can be the target atom  and the problem is to make the distribution of the target atom as close to the observed  empirical  distribution as possible. this is achieved in the learning phase by tuning the parameters of bsw/1 atoms. 
　the utility part contains a logic program that makes use of the distribution pdb with special built-ins such as learn/1 for learning and prob/1 and cprob/1 for calculation of probabilities. the model part and the utility part should be conceptually distinguished  but actually  when combined  they look like just a logic program with special built-ins. 
　the control declarations give information required for the learning phase. currently  we have target declaration for specifying a target atom  and data declaration for specifying a file containing teacher data  randomly sampled target atoms . 
1 	learning phase 
the learning phase starts with commands learn/1. prior to them  we have to specify a target atom by target declaration  and the data file by data declaration. 

　teacher data are expressed as ground literals containing the target atom. they are henceforth called goals. what prism does in the learning phase is to adjust statistical parameters  associated with bsw/1  to maximize the conjunctive probability of these goals. presently for practical reasons  only positive goals are allowed as teacher data. 
　prism  given the goals in the data file  builds a table to keep the records of the correspondence between a goal and the conjunctions of bsw/1 atoms that appear in one of the proof trees of the goal. it means that prism computes all solutions  by top-down exhaustive search  and hence sometimes becomes computationally inefficient compared to specialized algorithms developed for specific tasks. anyway  after completing the table  prism sets random values  between 1 and 1  to statistical parameters associated with bsw/1 atoms in the table. the em learning routine then starts to update these parameters iteratively until convergence to attain the maximum likelihood estimators. 
1 	e x e c u t i o n phase 
after learning  we run the learned program to check how it learned or how it behaves. there are three execution modes  i.e. sampling  answer with probability and answer with formula. 
　a special command is used to specify the execution mode. sampling is done by the command sample/1. for example  for query i  - sample  bloodtype x   the system returns the answer x=a  x=b  x=o or x=ab according to the distribution of bloodtype/1. the commands prob/1 and cprob/1 are used for the answer with probability mode; for the query i  prob bloodtype a  prob  we have prob=1 for instance. in the case that the given formula is false with probability 1 such as bloodtype a  & bloodtype b  we have answer no. 
　the commands proof/1 and probf/1 give answers with formula. the formula is a dnf of bsw/1 atoms which logically explains the given formula. for example  the answer with formula for bloodtype a  is 

1 	examples 
in this section  we present modeling examples by prism programs  together with learning results. 
1 	blood type 
the first one is the blood type program in section 1. 
in case of japan  the ratio of blood types is a:1:b:ab = 1:1. we artificially generated random data with this ratio and used them as teacher data for the program to estimate the probability distribution of gene/1. the result is shown in table 1. conv shows converged parameters values of the bsw/1 named id. 

1 	h m m 
the hidden markov model  hmm   rabiner 1   which has long been a basic tool for speech recognition  stands for a class of finite state automata in which transition is probabilistic and an alphabet is emitted on each transition. what we can observe from outside is only an output string consisting of emitted alphabets while the state transition is not observable  hence the name of hmm. figure 1 is an example of hmm that has a state set {so  s1  s1} and output alphabets {a b}. we can see from the figure that the transition probability from so to so is 1 whereas that of so to s1 is 1. we can also see that on a transition from so  a is emitted with probability 1 and b with probability 1 etc. 
　this hmm is described by a prism program on next page  the utility part is not shown . for learning  we limit the length of output strings to 1 and leave all parameters associated with bsw/1 undefined. 
　a learning experiment was conducted using 1 teacher data1 generated from the h m m in figure 1. we then let the program learn the data placed on the file hmm.dat by the prism's built-in em learning routine. convergence is judged when an increment of the logarithmic likelihood of the conjunction of all hmm/1 atoms  teacher data  becomes less than 1 - 1 . the result is shown in table 1  smp shows original parameter values . 
　in prism  the probabilities of atoms are computed by prob/1 predicate and the probability of pro hmm  a a b a b      is 

this experiment exemplifies the expressive power of 
prism and the learnability of hmms. 	it is however 
1
　　these data were obtained from running the hmm program in sampling mode. 

	sato & kameya 	1 

  i d smp conv ! 1 
! 1 
1 
1 
1 1 
1 
1 
1 
1 1 
1 
1 
1 
1 represent statistical dependencies among random variables. we confine ourselves to a case where random variables are binary  i.e. propositions. dependencies are expressed as a directed acyclic graph where nodes represent propositions and links indicate direct probabilistic dependencies quantified with probabilities. the graph as a whole represents a joint probability distribution of propositions. for example1  in figure 1  it holds that 
p  tamering  fire  alarm  smoke  learning  report  = p report | leaving p leaving   alarm  
p alarm   tampering  fire p smoke   fire  p  fire  p  tampering  
　following  poole 1   we describe this bayesian network as follows: 
not sufficient to claim practical usability of prism  because the practical merit of using hmms resides in the availability of three efficient algorithms for three basic problems. namely  the forward-backward algorithm  rabiner 1  for calculating the probability of a given string  the viterbi algorithm  rabiner 1  for deciding the most likely state transition sequence for the given string and the baum-welch learning algorithm  rabiner 1  to estimate the probability parameters of an h m m . we expect that since prism is a general programming language  it will not be very difficult to write prism programs for these algorithms. 
table 1: result of h m m learning 
1 	bayesian networks 
now we turn to bayesian networks  pearl 1 . bayesian networks are a knowledge representation language to 
1 	probabilistic reasoning 

table 1: result of bayesian learning 

　an experiment was conducted using 1 teacher data sampled from the above program with parameter values shown as smp in table 1. convergence is obtained after two iterations of the em learning routine and the converged parameter values are shown as conv in table 1. 
　after learning  we can check various probabilities by using built-in predicates such as prob/1 and cprob/1. for instance  p fire  smoke -*alarm   the probability of a fire breaking out while smoke is observed but the alarm is not ringing  is 

1 	related work 
prism is a general programming language with the ability of learning  and we don't have many predecessors with the same character and power. probably  most directly related one is poole's probabilistic horn abduction  poole 1 . his semantics however excludes large part of usual logic programs1. furthermore probabilities are considered only for finite cases  no joint distribution for infinitely many random variables . accordingly there is no way to express markov chains such as hmms. 
　ng and subrahmanian proposed probabilistic logic programming  ng 1 . their approach is based on intervals. they assign probability ranges to atoms in the program and check  using linear programming technique  if probabilities satisfying those ranges actually exist or not. the use of linear programming confines their approach to a finite domain. neither of poole's proposal or ng and subrahmanian' proposal mentions learning. 
　charniak and goldman proposed a special language frail1 for construction of bayesian networks  charniak et al. 1 . although their rules look much like definite clauses annotated with probability dependencies  the semantics is not very clear and no learning mechanism is provided for their programs. 
　hashida  hashida 1  proposed a rather general framework for natural language processing as probabilistic constraint logic programming. he assigned probabilities to between literals and let them denote the degree of the probability of invocation. he has shown constraints are efficiently solvable by making use of these probabilities. 
1
    this is due mainly to the acyclicity assumption made in  poole 1 . 
1 	conclusion 
we have presented prism which is a new modeling language for symbolic-statistical phenomena. it not only has general computing power combined with probabilistic semantics but also has a general learning mechanism that enables any prism program to learn from examples. below are two potential application areas. 
　prism programs can define markov chains such as hmms with mathematical rigor. using this property  and taking advantage of prism's first order expressiveness  it looks feasible to describe markov decision processes controlled by complex symbolic reasoning. this might contribute to modeling agents with rich knowledge  interacting and learning one another. 
　nlp is another promising area because of the obvious need for describing statistical correlations between syntactic structures and semantic structures. it is also noticeable that a large corpus is already available for learning. 
　computation power and learning power should be unified to give a new dimension to programming. we hope that prism brings us one step closer to the crossfertilization of computation and learning. 
