	a n 	e f f i c i e n t 
c o n t e x t - f r e e 	p a r s i n g 	a l g o r i t h m 
	f o r n a t u r a l 	l a n g u a g e s 1 
masaru tomita 
computer science department 
carnegie-mellon university 
pittsburgh  pa 1 

a b s t r a c t 
¡¡this paper introduces an efficient context-free parsing algorithm and emphasizes its practical value in natural language processing. the algorithm can be viewed as an extended lr parsing algorithm which embodies the concept of a  graph-structured stack.  unlike the standard lr  the algorithm is capable of handling arbitrary non cyclic context-free grammars including ambiguous grammars  while most of the lr parsing efficiency is preserved. the algorithm seems more efficient than any existing algorithms including the cocke younger kasami algorithm and earley's algorithm  as far as practical natural language parsing is concerned  due to utilization of lr parsing tables. the algorithm is an all-path parsing algorithm; it produces all possible parse trees  a parse forest  in an efficient representation called a  shared-packed forest.  this paper also shows that earley's forest representation has a defect and his algorithm cannot be used in natural language processing as an all-path parsing algorithm. 
1 introduction 
¡¡in past decades  many context-free parsing algorithms have been developed  and they can be classified into two groups: algorithms for programming languages and algorithms for general context-free languages. the former group of algorithms are intended to handle only a small subset of context-free grammars sufficient for programming languages. such algorithms include the ll parsing algorithm  the operator precedence parsing algorithm  the predictive parsing algorithm and the lr parsing algorithm. they can handle only a subset of context free grammars called ll grammars  operator precedence grammars  predictive grammars and lr grammars  respectively . these algorithms are tuned to handle a particular subset of context free grammars  and therefore they are very efficient with their type of grammars. in other words  they take advantage of inherent features of the programming language. 
 the other group of algorithms  often called general context-free parsing algorithms  are designed to handle arbitrary context-free grammars. this group of algorithms includes earley's algorithm  1  and the cocke younger kasami algorithm  1  1 . general context-free languages include many difficult phenomena which never appear in programming languages  such as ambiguity and cycle. algorithms in this group have not been widely used for programming languages  because their constant factors are too large to be used in practical compilers  as earley admitted in his thesis . this is not surprising  
¡¡this research was sponsored by the defense advanced research projects agency  dod . arf'a order no. 1  monitored by the air force avionics 
laboratory under contract f1 -k 1. the views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies  either expressed or implied  of the defense advanced research projects agency or the us government. 
because those algorithms are not tuned for any particular subset of context-free grammars  and must be able to handle all difficult phenomena in context-free grammars. in other words  they do not take advantage of inherent features of the programming language. intuitively speaking  algorithms in this group are efficient for  hard  grammars by sacrificing efficiency on  easy  grammars. 
 no parsing algorithm has been designed that takes advantage of inherent features of natural languages. because natural languages include slightly more difficult phenomena than programming languages  we cannot simply use the first group of algorithms for natural languages. natural languages are a little 
 harder  than programming languages  but they are still much  easier  than general context-free languages as we have seen above  we have context-free parsing algorithms at two extremes. the one is very efficient but not powerful enough to handle natural languages. the other is too powerful and it turns out to be inefficient. we need something in between. 
 this paper introduces such a context-free parsing algorithm  which can be viewed as an extended lr parsing algorithm which embodies the concept of a  graph-structured stack.  the fragile point of the standard lr parsing algorithm is that it cannot handle a non-lr grammar  even if the grammar is almost lr. unlike the standard lr parsing algorithm  our algorithm can handle non-lr grammars with little loss of lr efficiency  if its grammar is  close  to lr. foriunateiy  natural language grammars are considerably  close  to lr  compared with other general context-free grammars. 
 a primitive version of the algorithm was described in the author's previous work . because the primitive algorithm used a  tree-structured stack   exponential time was required  whereas the current algorithm uses the  graph-structured stack  and runs in polynomial time. also  the primitive algorithm was a recognizer; that is  it did not produce any parses  while the current algorithm produces all possible parses in an efficient representation. a  graph-structured stack  was proposed in the author's more recent work . the algorithm was previously called the mlr parsing algorithm. all ideas presented in those two previous papers are included in this paper  and the reader does not need to refer to them to understand the current discussion. 
1 the standard lr parsing a l g o r i t h m 
 the lr parsing algorithms  1  1  were developed originally for programming languages. an lr parsing algorithm is a shiftreduce parsing algorithm which is deterministically guided by a parsing table indicating what action should be taken next. the parsing table can be obtained automatically from a context-free phrase structure grammar  using an algorithm first developed by deremer 1  1 . i do not describe the algorithms here  referring the reader to chapter 1 in aho and ullman  1. i assume that the reader is familiar with the standard lr parsing algorithm  not necessarily with the parsing tabie construction algorithm . 
 the lr paring algorithm is one of the most efficient parsing algorithms. it is totally deterministic and no backtracking or search is involved. unfortunately  we cannot directly adopt the lr parsing technique for natural languages  because it is applicable only to a small subset of context-tree grammars called lr grammars  and it is almost certain that any practical natural language grammars are not lr. if a orammar is non-lr  its parsing table will have multiple entries ; one or more of the action table entries will be multiply defined. 
 figures 1 and 1 show an example of a non-lr grammar and its parsing table. grammar symbols starting with  *  represent preterminals. entries  sh n  in the action table  the left part of the table  indicate the action  shift one word from input buffer onto the stack  and go to state n . entries  re n  indicate the action  reduce constituents on the stack using rule n . the entry  ace  stands for the action  accept   and blank spaces represent  error . goto table  the right part of the table  decides to what state the parser should go after a reduce action. the exact definition and operation of the lr parser can be found in aho and ullman . 
 we can see that there are two multiple entries in the action table; on the rows of state 1 and 1 at the column labeled  *prep . it has been thought that  for lr parsing  multiple entries are fatal because once a parsing table has multiple entries  deterministic parsing is no longer possible and some kind of nondeterminism is necessary. however  in this paper  we extend a stack of the lr parsing algorithm to be  graph-structured   so that the algorithm can handle multiple entries with little loss of lr efficiency. 
1 handling multiple entries 
 as mentioned above  once a parsing table has multiple entries  determimst'c parsing is no longer possible and some kind of nondeterminism is necessary. we handle multiple entries with a special technique  named a graph-structuied stack. in order to introduce the idea of a graph-structured stack  i first give a 
 simpler non determinism  and make refinements on it. subsection 1 describes a simple and straightforward nondeterminism  i.e. pseudo-parallelism  breath-first search   in which the system maintains a number of stacks simultaneously. i call the list of stacks stack list. a disadvantage of the stack list is then described. the next subsection describes the idea of stack combination  which was introduced in my earlier research   to make the algorithm much more efficient. with this idea  stacks are represented as trees  or a forest . finally  a further refinement  the graph-structured stack  is described to make the algorithm even more efficient; efficient enough to run in polynomial time. 
1 with stack list 
the simplest idea is to handle multiple entries non-
deterministically. i adopt pseudo-parallelism  breath first search   
	m.tomita 	1 
maintaining a list of stacks called a stack list. the pseudoparallelism works as follows. 
 a number of processes are operated in parallel. each process has a stack and behaves basically the same as in standard lr parsing. when a process encounters a multiple entry  the process is split into several processes  one for each entry   by duplicating its stack. when a process encounters an error entry  the process is killed  by removing its stack from the stack list. all processes are synchronized; they shift a word at the same time so that they always look at the same word. thus  if a process encounters a shift action  it waits until all other processes also encounter a  possibly different  shift action. 
 figure 1 shows a snapshot of the stack list right after shifting the word  with  in the sentence  i saw a man on the bed in the apartment with a telescope  using the grammar in figure 1 and the parsing table in figure 1. for the sake of convenience  i denote a stack with vertices and edges. the leftmost vertex is the bottom of the stack  and the rightmost vertex is the top of the stack. vertices represented by a circle are called state vertices  and they represent a state number. vertices represented by a square are called symbol vertices  and they represent a grammar symbol. each stack is exactly the same as a stack in the standard lr parsing algorithm. the distance between vertices  length of an edge  does not have any significance  except it may help the reader understand the status of the stacks. 
 we notice that some stacks in the stack list appear to be identical. they are  however  internally different because they have reached the current state in different ways. although we shall describe a method to compress them into one stack in the next section  we consider them to be different in this section. 
 a disadvantage of the stack list method is that there are no interconnections between stacks  processes  and there is no way in which a process can utilize what other processes have done already. the number of stacks in the stack list grows exponentially as ambiguities are encountered. for example  these 1 processes in figure 1 will parse the rest of the sentence  the telescope  1 times in exactly the same way. this can be avoided by using a tree structured stack  which is described in the following subsection. 
1 with a tree-structured stack 
 if two processes are in a common state  that is  if two stacks have a common state number at the rightmost vertex  they will behave in exactly the same manner until the vertex is popped from the stacks by a reduce action. to avoid this redundant operation  these processes are unified into one process by combining their stacks. whenever two or more processes have a common state number on the top of their stacks  the top vertices are unified  and these stacks are represented as a tree  where the top vertex corresponds to the root of the tree. i call this a treestructured stack. when the top vertex is popped  the treestructured stack is split into the original number of stacks. in general  the system maintains a number of tree-structured stacks 1 	m.tomita 
in parallel  so stacks are represented as a forest. figure 1 shows a snapshot of the tree-structured stack immediately after shifting the word  with . 
 although the amount of computation is significantly reduced by the stack combination technique  the number of branches of the tree-structured stack  the number of bottoms of the stack  that must be maintained still grows exponentially as ambiguities are encountered. the next subsection describes a further modification in which stacks are represented as a directed acyclic graph  in order to avoid such inefficiency. 
1 with a graph-structured stack 
 so far  when a stack is split  a copy of the whole stack is made. however  we do not necessarily have to copy the whole stack: 
even after different parallel operations on the tree-structured stack  the bottom portion of the stack may remain the same. only the necessary portion of the stack should therefore be split. when a stack is split  the stack is thus represented as a tree  where the bottom of the stack corresponds to the root of the tree. with the stack combination technique described in the previous subsection  stacks are represented as a directed acyclic graph. figure 1 shows a snapshot of the graph stack. it is easy to show that the algorithm with the graph-structured stack does not parse any part of an input sentence more than once in the same way. this is because if two processes had parsed a part of a sentence in the same way  they would have been in the same state  and they would have been combined as one process. 
so far  we have focussed on how to accept or reject a sentence. 
in practice  however  the parser must not only simply accept or reject sentences  but also build the syntactic structure s  of the sentence  parse forest . the next section describes how to represent the parse forest and how to build it with our parsing algorithm. 
1 an efficient representation of a parse forest 
 our parsing algorithm is an all-path parsing algorithm; that is  it produces all possible parses in case an input sentence is ambiguous. such all-path parsing is often needed in natural language processing to manage temporarily or absolutely ambiguous input sentences. the ambiguity  the number of parses  of a sentence grows exponentially as the length of a sentence grows. thus  one might notice that  even with an efficient parsing algorithm such as the one we described  the parser would take exponential time because exponential time would be required merely to print out ail parse trees  parse forest . we must therefore provide an efficient representation so that the size of the parse forest does not grow exponentially. 
 this section describes two techniques for providing an efficient representation: sub-tree sharing and local ambiguity packing. it should be mentioned that these two techniques are not completely new ideas  and some existing systems already adopted these techniques  either implicitly or explicitly. to the author's knowledge  however  no existing system has explicitly adopted both techniques at the same time. 
1 sub-tree sharing 
 if two or more trees have a common sub-tree  the sub-tree should be represented only once. for example  the parse forest for the sentence  i saw a man in the park with a telescope  should be represented as in figure 1. our parsing algorithm is very well suited for building this kind of shared forest as its output  as we shall see in the following. 
 to implement this  we no longer push grammatical symbols on the stack; instead  we push pointers to a node1 of the shared 

forest. when the parser  shifts  a word  it creates a leaf node labeled with the word and the preterminal  and instead of pushing the pre-terminal symbol  a pointer to the newly created leaf node is pushed onto the stack. if the exact same leaf node  i.e. the node labeled with the same word and the same preterminal  already exists  a pointer to this existing node is pushed onto the stack  without creating another node. when the parser  reduces  the stack  it pops pointers from the stack  creates a new node whose successive nodes are pointed to by those popped pointers  and pushes a pointer to the newly created node onto the stack. 
 using this relatively simple procedure  our parsing algorithm can produce the shared forest as its output without any other special book-keeping mechanism  because the algorithm never does the same reduce action twice in the same manner. 
1 local ambiguity packing 
 i define that two or more subtrees represent local ambiguity if they have common leaf nodes and their top nodes are labeled with the same non-terminal symbol. that is to say  a fragment of a sentence is locally ambiguous if the fragment can be reduced to a certain non-terminal symbol in two or more ways. if a sentence has many local ambiguities  the total ambiguity would grow exponentially. to avoid this  we use a technique called local ambiguity packing  which works in the following way. the top nodes of subtrees that represent local ambiguity are merged and treated by higher-level structures as if there were only one node. such a node is called a packed node  and nodes before packing are called subnodes of the packed node. examples of a sharedpacked forest is shown in figure 1. 
 local ambiguity packing can be easily implemented with our parsing algorithm as follows. in the graph-structured stack  if two 

	m.tomita 	1 
or more symbol vertices have a common state vertex immediately on their left and a common state vertex immediately on their right  they represent local ambiguity. nodes pointed to by these symbol vertices are to be packed as one node. in figure 1 for example  we see one 1-way local ambiguity and two 1-way local ambiguities. 
 the algorithm will be made clear by an example in the next section. 
1 the example 
 this section gives a trace of the algorithm with the grammar in figure 1  the parsing table in figure 1 and the sentence  i saw a man in the park with a telescope.  
 at the very beginning  the stack contains only one vertex labeled 1  and the parse forest contains nothing. by looking at the action table  the next action  shift 1  is determined as in standard lr parsing. 

figu re 1: trace of the parser 
 when shifting the word  i   me algorithm creates a leaf node in the parse forest labeled with the word  i  and its preterminal  *n   and pushes a pointer to the leaf node onto the stack. the next action  reduce 1  is determined from the action table. 

we reduce the stack basically in the same manner as standard 
lr parsing. it pops the top vertex  1  and the pointer  1  from the stack  and creates a new node in the parse forest whose successor is the node pointed to by the pointer. the newly created node is labeled with the left hand side symbol of rule 1  namely  np . the pointer to this newly created node  namely  1   is pushed onto the stack. the action  shift 1  is determined as the next action. now  we have figure 1. 

1 m. tomita 
figure 1: trace of the parser  cont.  
 the next action is  reduce 1 . it pops pointers   1  and  1m  and creates a new node in the parse forest such that node 1 and node 1 are its successors. the newly created node is labeled with the left hand side symbol of rule 1  i.e.  np . the pointer to this newly created node   1   is pushed onto the stack. we now have figure 1. 

figu re 1: trace of the parser  cont.  
at this point  we encounter a multiple entry   reduce 1  and 
 shift 1   and both actions are to be executed. reduce actions are always executed first  and shift actions are executed only when there is no reduce action to execute. after executing  reduce 1   the stack and the parse forest look like the following. the top vertex labeled  1  is not popped away  because it still has an action which is not yet executed. such a top vertex  or more generally  vertices with one or more actions yet to be executed  are called active. thus  we have two active vertices in the stack above: one labeled  1   and the other labeled  1 . the action  reduce 1  is determined from the action table  and is associated with the latter vertex. 

figu re 1: trace of the parser  cont.  
 because reduce actions have a higher priority than shift actions  the algorithm next executes  reduce 1  on the vertex labeled  1 . the action  shift 1  is determined from the action table. 

figu re 1: trace of the parser  cont.  
 after about 1 steps  figure 1   the action  accept  is finally executed. it returns  1  as the top node of the parse forest  and halts the process. the final parse forest is shown in figure 1. 


	m.tomita 	1 
1 comparison with other a l g o r i t h m s 
 there have been several general parsing algorithms that run in polynomial timo. theoretically speaking  the fastest algorithm at present is valiant's algorithm. valiant  reduced the contextfree parsing problem to the boolean matrix multiplication problem   and his algorithm runs in time 1 n b1 . this algorithm is  however  of only theoretical interest  because the coefficient of n1 is so large that the algorithm runs faster than conventional n algorithms only when an input sentence is tremendously long. practically speaking  on the other hand  the most well-known parsing algorithm is earley's algorithm  1  1 1   which runs in time 1 n1 . 
 all other practical algorithms seem to bear some similarity with or relation to earley's algorithm. another algorithm which is as well-known as earley's algorithm is the cocke-younger-kasami  cyk  algorithm  1  1  1 . graham ei al.   however  revealed that the cyk algorithm is  almost  identical to earley's algorithm  by giving an improved version of the cyk algorithm which is very similar to earley's algorithm. the chart parsing algorithm is basically the same as the cyk algorithm. the active chart parsing algorithm is basically the same as earley's algorithm  
although it does not necessarily have to parse from left to right. bouckaert et al.  extended earley's algorithm to perform tests similar to those introduced in ll and lr algorithms. improved nodal span  and lingol  1 are also similar to earley's algorithm  but both of them require grammars to be in chomsky normal form  cnf . 
these all practical general parsing algorithms seem to be like 
earley's algorithm  in that they employ the tabular parsing method; they all construct well formed substring tables . in chart parsing  such tables are called charts. the representation of one well formed substring is called an  edge  in active chart parsing  a  state  in earley's algorithm  a  dotted rule  in grahams algorithm and an  item  in aho and ullman. throughout this paper  we call a well-formed substring an item. 
1 recognition time 
 no existing general parsing algorithm utilizes lr parsing tables. all of the practical algorithms mentioned above construct sets of items by adding an item to a set  one by one  during parsing. our algorithm  on the other hand  is sufficiently different; it precomputes sets of items in advance during the time of parsing table construction  and maintains pointers  i.e.  state numbers  to the precomputed sets of items  rather than maintaining items themselves. 
 because of this major difference  our algorithm ha1 the following three properties. 
  it is more efficient  if a grammar is  close  to lr: that is  if its lr parsing table contains relatively few multiple entries. in general  less ambiguous grammars tend to have fewer multiple entries in their parsing table. in an extreme case  if a grammar is lr  our algorithm is as efficient as an lr parsing algorithm  except for minor overheads. 
  it is less efficient  if a grammar is  densely  ambiguous as in figure 1. this kind of grammar tends to have many multiple entries in its lr parsing table. our algorithm may take more than 1 n1  time with  densely  ambiguous grammars. 

1 	m.tomita 
  it is not able to handle infinitely ambiguous grammars and cyclic grammars1  figure 1 and 1   although it can handle e grammars and left recursive grammars. if a grammar is cyclic  our algorithm never terminates. the existing general parsing algorithms can parse those sentences  figure 1  1 and 1  still in time proportional ton.. 

produce a parse forest in 1 n1  space  but they require their grammars to be chomsky normal form  cnf . theoretically speaking every context free grammar can be mechanically transformed into cnf. practically speaking  however  it is usually not a good idea to mechanically transform a grammar into cnf  because the parse forest obtained from a cnf grammar will make little sense in practical applications; it is often hard to figure out a parse forest in accordance with its original grammar. 
1 defect of earley's forest representation 
this subsection identifies the defect of earley's representation. 
consider the following grammar g1 and the sentence in figure 1. figure 1 is the parse forest produced by earley's algorithm. the individual trees underlying in this representation are shown in figure 1. they are exactly what should be produced from the grammar and the sentence. 

 it is certain that no natural language grammars have infinite ambiguity or cyclic rules. it is also extremely unlikely that a 
 natural language grammar has dense ambiguity such as that shown in figure 1. it is therefore safe to conclude that our algorithm is more efficient than any existing general parsing algorithms in terms of recognition time as far as practical natural language grammars are concerned. 
1 parse forest representation 
 some of the existing general parsing algorithms leave a wellformed substring table as their output. in my opinion  these wellformed substring tables are not appropriate as a parser's final output  because it is not straightforward to simply enumerate all possible parse trees out of the tables; another search must be involved. thus we define a parse forest as a representation of all possible parse trees out of which we can trivially enumerate all trees without any substantial computation. 
 for most natural language grammars  our shared-packed forest representation  described in section 1  takes less than or equal to o n1  space. this representation  however  occasionally takes more than 1 n1  space with densely ambiguous grammars. for example  it takes 1 n1  space with the grammar in figure 1. 
 earley  on the other hand  gave in his thesis  a parse forest representation which takes at most 1 n1  space for arbitrary context-free grammars. however  the next subsection shows that his representation has a defect  and should not be used in natural language processing.1 there exist some other algorithms that 
those two kinds of grammars are equivalent. 
¡¡several existing chart parsers seem to build a parse forest by adding pointers between edges. since none of them gave a specification of the parse forest representation  we cannot make any comparisons in any event  however  if they adopt earley's representation then they must have the defect  and if they adopt my representation then they must occasionally take more than 1 n   time. 


 similarly  out of the sentence 'xxxx' with the same grammar g1  the algorithm produces a representation which over-represents 1 trees including 1 wrong trees along with 1 correct parse trees. 
 a grammar like g1 is totally unrealistic in the domain of programming language  and this kind of defect never appears as a real fault in that context. productions like 
¡¡¡¡¡¡s -  ss in g1 look rather tricky and one might suspect that such a problem would arise only in a purely theoretical argument. 
 unfortunately  that kind of production is often included in practical natural language grammars. for example  one might often include a production rule like 
¡¡¡¡¡¡n --  nn to represent compound nouns. this production rule says that two consecutive nouns can be compounded as a noun  as in 'file equipment' or 'bus driver.' this production rule is also used to represent compound nouns that consist of three or more nouns such as 'city bus driver or 'ibm computer file equipment.' in this case  the situation is exactly the same as the situation with the grammar g1 and the sentence 'xxx' or 'xxxx'  making the defect described in the previous section real in practice. 
another defective case is that using conjunctive rules such as 
np -  np conj np vp -  vp conj vp 
which are even more often included in practical grammars. the same problem as that above arises when the algorithm parses a sentence with the form: 
np and np and np. 
 yet another defective case which looks slightly different but which causes the same problem is that with the following productions: 
np -  hp pp 
pp -  prep np 
¡¡we could think of an algorithm that takes the defective representation as its argument  and enumerate only the intended parse trees  by checking the consistency of leaf nodes of each tree. such an algorithm would  however  require the non-trivial amount of computation  violating our definition of parse forest. 
m. tomita 1 
these represent prepositional phrase attachment to noun phrases. the fault occurs when the algorithm parses sentences with the form: 
np prep np prep np 
 as we have seen  it is highly likely for a practical grammar to have defective rules like those above  and we conclude that earleys representation of a parse forest cannot be used for natural languages. 
1 concluding remarks 
 our algorithm seems more efficient than any of the existing algorithms as far as practical natural language parsing is concerned  due to its utilization of lr parsing tables. our sharedpacked representation of a parse forest seems to be one of the most efficient representations which do not require cnf. 
 the following extensions of this paper can be found in my doctorate dissertation : 
  the algorithm is implemented and tested against four sample english grammars and about 1 sample sentences  to verify the feasibility of the algorithm to be used in practical systems. 
  earleys algorithm is also implemented and practical comparisons are made. the experiments show that our parsing algorithm is about 1 to 1 times faster than earleys algorithm  as far as natural language processing is concerned. 
  the algorithm's precise specification  as well as the source program  is presented. 
  multi part-of speech words and unknown words are handled by the algorithm without any special mechanism. 
  an interactive disambiguation technique out of the shared-packed forest representation is described. 
  an application to natural language interface  called left-to-right on-line parsing  is discussed  taking advantage of the algorithms left-to-right-ness. 
a c k n o w l e d g e m e n t 
 i would like to thank jaime carbonell  phil hayes  herb simon and ralph grishman for thoughtful comments on an earlier version of this paper  and cynthia hibbard for helping to produce this document. 

1 m. tomita 
