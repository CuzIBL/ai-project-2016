 
   human conversational participants depend upon the a b i l i t y of their partners to recognize their intentions  so that those partners may respond appropriately. in such interactional the speaker can encode his intentions that the hearer act in a variety of sentence types. instead of telling the hearer what to do. the speaker may just state his goals  and expect a response that meets these goals. this paper presents a new model for recognizing the speaker's intended meaning in determining a response. we show that this recognition makes use of the speaker's plan  his beliefs about the domain and about the hearer's relevant capacities.* 
1. statement op the problem 
   human conversational participants depend upon the a b i l i t y of their partners to recognize their intentions  so that those partners may be capable of responding appropriately. for instance  in the dialogue below  
d1 si: i want to see the drawing of the new design layout. 
1 s1: ok. here it i s .  shows sheet with new design  
1 s1: there i s n ' t room to put in the color code charts at the bottom of the picture. can you move up the main layout  
1 s1: sure  i ' l l bring back the new design in half an hour. 
the speaker's desires are encoded in a variety of sentence types. instead of t e l l i n g the hearer what to do  the speaker states his goals  and expects a response that meets them  at least partway. this paper presents a model in which recognizing the speaker's intended meaning plays a fundamental part in determining a response. 
1. introduction 
   one of the goals of the knowledge representation and natural language group at bbn has been to provide powerful general tools for natural language processing and to build a language understanding system for a decision maker using a graphics display. we envision a decision maker aocessing information from a database that can be represented visually: he/she needs to collect information from the database  add to i t   change it and define new features. 
　　a special feature of this language understanding system is the assumption that the human user expresses him/herself naturally. he/she can utter more than the direct imperatives and can ask questions besides the direct questions in typical of most current ai language systems. 
   natural communication with a partner is possible  however  only if the latter can reason both about the intent behind the speaker's utterances and about its own responses to those utterances. such reasoning involves - at least  1  bringing to bear the kinds of knowledge people have before they enter into a given discussion  and  1  making use of the knowledge they gain in the discussion. 
　　an organizing framework for a system that can reason about the the speaker's intentions has been explored in our group and is reported elsewhere  brachman et a l . 1 . in that framework we experimented with an implementation of allen's  model of a plan-based approach to speech act recognition. this model provided us with a perspective within which to use models of speaker's beliefs and wants  cohen  1   as well as a 
　　framework for reasoning about the speaker's plans. we found that we needed to expand on allen's model in two ways:  1  by recognizing a richer form of plans  and  1  by making explicit the connection between the speaker's intentions as structured by his plans and the response intended by the speaker. 
　　in this paper we w i l l describe the new theoretical model for recognizing speaker's intended meaning and the computational tools we are currently using to implement that model. we think the model is particularly powerful because it is 

　*acknowledgements: the research reported in this paper was supported in part by the advanced researoh projects agency and was monitored by the office of naval research under contract no. n1-c-1. 
the basis of a system that oan reason about the kinds of planning bugs the acknowledgement of  and the response to whioh are often refleoted in conversational exchanges. 
1. depinino intended speaker meaning 
   our goal is to provide a computational model of the hearer's interpretation of the speaker's intended meaning. the intended meaning of an utterance we define as that set of  propositional attitude  e.g.  belief  want  intend   propoaitional content  pairs that the speaker wants to induce in the hearer by means of the utterance. 
   the notion of the intended meaning of an utterance oan be illustrated by contrasting it with that of semantic meaning. the semantic meaning of 
1 

a declarative utterance is the propositlonal content assigned to the type of the utteranoe by the semantic rules of the language. for instance  if someone says   you're a prince   the semantic meaning is  roughly  that the person addressed by the speaker la tne son of a king. by contrast  the intended meaning is dependent on  inter alia  the individual psychological state of the speaker at the time and place of utterance. the speaker may mean that he thinks the hearer is a really nice guy and wants to t e l l him so  or he may be saying something quite different. the speaker  using irony  may mean that the addressee is just the opposite of a nice guy. 
　　this  in some ways extreme  example demonstrates that speaker's intended meaning  though correlated  is not in general identical  with semantic meaning. comprehending the semantic meaning of the utterance forms the oasis for discerning the intended meaning  but the latter  typically  also requires the use of the following kinds of belief on the part of the hearer: 
　　1. beliefs about the characteristics of the current situation  
　　1. beliefs 	about 	the 	speaker's 	beliefs 	and 
goals  
　　1. beliefs about the context of discussion  the discourse context  as a special aspect of  1   
　　1. beliefs about what conventions for action exist between the speaker and hearer * 
　　1. beliefs about what is mutually believed  as between speaker and hearer  with respect to  1  through  1 . 
　　a sample exchange w i l l indicate the role of these kinds of belief. in the example below  the user is interacting with our system to display some information  in this case  a portion of an atn network . the user's f i r s t two utterances are simple direct imperatives that indicate that the user wants the system to display a part of the net and then move the focus to a subpart of the display. 
d1 u: 	display 	the 	clause 	level network. 
1 s:  display of network  ok. 
1 u: 	now 	focus 	on 	the 	pre 	verbal constituents. 
1 s:  display of subnet  ok. 
1 u: no  i want to see s/aux. 
what 	does 	the user mean by her third utterance 
 utteranoe 1   the answer depends on what she believes about the net objects to which she has referred. suppose she thinks that s/aux is part of the preverbal constituents. then she is communicating that the display la wrong and what's wrong with i t ; she intends for s/aux to be included in the display with the other constituents. suppose  alternatively that she thinks that s/aux is not part of the preverbal constituents. she is s t i l l indicating that she wants to see s/aux  but also that she has changed her mind about the display in some way and intends s/aux to be visible.  as to the nature of her change of mind  this depends on whether she has realized the error of her ways  or at least the mismatch between her views on s/aux and the system's.  
　　in ohoosing among i t s available responses  the system must u t i l i z e i t s model of the user's beliefs about the domain and i t s model of what the user takes to be mutually believed between the two of them about that domain. for example  the user might have thought that s/aux was one of the 
　 this  of course  is relevant only to a special class of situations; a olass which includes the kind of interaction the bbn system must handle. 
p
reverbal constituents  and thought the system elleved this also. she would then have expected and intended the system to include that state in the display. if the user had been right about this  the system would indeed have included i t . but the user's  no  indicates to the system some bug in her plan  a bug stemming either from a faulty model of the domain i t s e l f or from faulty expectations about the system's model.  for simplicity  we assume that the system is omniscient about the atn grammar.  
　　i f   on the other hand  the system doesn't oonclude that the user takes s/aux to be among the preverbal constituents  and if it believes that she takes that to be mutually believed  then the system must again use its models of her and of her model of i t s e l f to determine what action is intended by the user  e.g.  should it compress the current display to make room for s/aux; should it erase the current display and bring up a new one  centered on s/aux  etc.  . this decision may depend on the kinds of conventions alluded to in  1  above. in general  of course. people's behavior in conversational situations also depends on the relative status of the conversational partners  on what the participants think w i l l benefit themselves  as well as not harm others  and the l i k e . these social considerations are significant to human interaction  but for the remainder of this paper  we'll assume that the system responds in a slavishly cooperative way  that l a   it has no interest beyond serving the user. 
　　there are two ways to view the intentions of another agent. the f i r s t is simply in terms of one's beliefs about what the other person wants and believes. this is keyhole recognition  see cohen et al  1l  . one person decides what he thinks another intends simply by observing him through a keyhole.  e.g. i decide that you are looking for your umbrella  on the basis of your looking around the room wltn your coat on  when i believe you believe that i t ' s raining outside . keyhole recognition of a user's wants is central to genesereth's macsyma advisor ; it also forms the basis of plan recognition in both schmidt et al'a believer  1 system and in wilensky's story understanding work l1 . 
　　the intended recognition of what someone is doing  on the other hand  la relevant for communicative situations  orlce 1  allen.1 . a speaker says something to a hearer  and intends that the hearer recognize the intention that lies behind the utterance. the speaker is attempting to  give the hearer a piece of i t s mind  and i t ' s essential to the success of the speaker's attempt that the hearer recognize it as such. 
1. a model of recognition of intended meaning 
   the hearer's taak in recognizing what the apeaker meant by an utteranoe la to be understood as follows: 
1. to produce an explanation for the utterance  stated in terms of the speaker's beliefs and wants. 
1. to use the explanation aa a basis for a response. 
we use the term  explanation  because the hearer is trying to answer the question  why did the speaker say that to me * the answer to this question - the proffered explanation of the speaker's act in uttering what he/she did - in turn produces new beliefs about the speaker; theae w i l l form part of the basis of the hearer's response. 
   the explanation  in general  w i l l have the form of a set of pairs of propositlonal attitudes and propositlonal contents attributed by the hearer to the speaker.  e.g.  bellef  that s1aux is part of 

1 

the preverbal oonstituents   want  that i display a l l ooaponents of the preverbal conaituents . etc.j certain beliefs play a central role in explaining why the speaker said what he did: 
explanatory beliefs 
1. beliefs about the speaker's goal and the plan to achieve i t   
that pc s1 . 
　　 in the above   p  is a schematic letter which takes verbs of proposltional attitude as substituends;  pc   a schematic letter which takes declarative sentences as substituends.  we oan apply this theory directly to the sample dialogues. for example  let us consider a sample utterance from the dialogue d1  understood  however  as the i n i t i a l utterance of a discourse: 

1. beliefs about the hearer's capacities  
1. beliefs about the hearer's dispositions to act given information about the speaker's wants. 
the problem we pose for ourselves is determining how to infer beliefs of these kinds  and how to use them to distinguish  between intended  and helpful  but unintended  responses. we want our system to recognize and produce the intended response whenever possible  and to be able to produce a helpful response when appropriate. 
　　to model the construction of the required explanation  we begin with grice's theory of speaker meaning  1  1 . grice notes that there are certain kinds of evidence which are normally available to an audience on the basis of which the audience  is intended  to draw certain conclusions about the speaker's intended meaning. these include the features of the utterance  mappings between those features and proposltional attltude-proposltlonal content pairs that the audience  assumed to be a competent speaker/hearer of the language  is supposed to be able to  and is intended to grasp  etc. for example  the feature: declarative w i l l be mapped to the speaker's wanting the hearer to believe the speaker believes the proposltional content of the utterance; while imperatives w i l l be mapped to the speaker's wanting the hearer to believe the speaker wants the hearer to bring about the state of affairs expressed by the proposltional component of the utterance. 
   somewhat more formally: an audience  for the utterance of a certain sentence s1  who la believed by the speaker to have certain attributes a  is expected to be able to recognize certain features of the utterance and to be able to draw from those features certain conclusions about what the speaker intended in uttering s1 in that context.  one such audience attribute  of course. is competence in the language of s1: others are both more interesting and more situation-specific.  these conclusions are  or at least inolude : 
intended conclusions 
si: i want to see s/aux. 
intuitively  we would like the theory to allow us to show how an audience  even a computer system  would conclude that the user wants to see s/aux  and that the user wants it to believe that he/she has this desire. 
　　the set of relevant features f  attributes a and mappings c are  or anyway include : 
o f1 = s1 is in declarative mood o 	f1 = s1 was uttered intentionally by u o 	f1 = s1 was intentionally directed at s 
o a1 = s is a computer system with a graphics display  and u knows this o a1 = s believes u is sincere 
o c1 s f1 maps to u's wanting the intended audience to believe that u believes that u wants to see s/aux. 
our proposed system w i l l make default assumptions guaranteeing f1  f1  a1 and a1  recognize that f1  and apply cl to s1. the system can then u t i l i z e 
the intended conclusions and infer directly that: 
1. u intended  s to reoognlze  that s1 is correlated with u's wanting s to believe that u believes that u wants to see s/aux  derived from intended conclusion 1 and 
c1 . 
1. by sincerity: u believes that u wants to see s/aux. 

1. s1 has certain features  call them f1 ... fn . 
1. s1 is correlated  in virtue of such features and the rules of the language  with the pair  p   pc sd . 
1. the speaker intends the audience to believe that the speaker p's that pc s1 . 
1. by sincerity  see below   the speaker does p that pc s1   
1. the speaker intended that the hearer p* 
　 actually the hearer may be intended to have a different proposltional attitude p' toward a related proposition. for simplicity  we'll assume these are the same. 
	1. 	by r e l i a b i l i t y : u wants to see s/aux.* . 
this  of course  is what  on i n t u i t i v e grounds  wanted the system to conclude. 
　 simply stated these rules  for the case of belief  are: sincerity! if x wants y to believe that x believes that q  then x believes that q. reliability: if y believes that x believes that q and that x is reliably informed about q  then y w i l l believe that q. the basis for these rules is the intuition that the speaker is sincere about his beliefs  and that what he believes he believes reliably  at least for certain subjeot matters  such as his own present state of mind. 
1 


1 

plan. 
　　to account for the bug  the system can reason in either of two ways*. on the one hand  if it now has reason to believe that the user believes that the preverbal constituents include s/aux  it w i l l conclude that there is a bug in the user's plan. this is a private  not mutual  belief; but it prevents the system from responding in the way intended by the user  to display s/aux  for not enough of the user's intentions are clear to decide how to do the display  e.g.  to include s/aux or to show it alone . henoe the system w i l l respond by indicating what the bug la and by asking about the particular mode of display desired. 
　　on the other hand  if the system believes that the user believes the preverbal constltutents are  mutually believed to be  disjoint from s/aux  then the system w i l l oonolude that the user has scrapped his current plan  and that this conclusion is one the system is intended to deduce. in this case  displaying s/aux is the intended response  but the system must s t i l l ask how to display i t   since it is not clear whether the user intended it to be displayed alone or with the subnet. a person. in such circumstances  would probably conclude that s/aux should be displayed alone  because s/he oould deduce that in general if a plan is scrapped  effects of its partial realization are no longer desired. however  this heuristic may be too general for systems which s t i l l have limited reasoning capacities  and hence we have chosen not to include such rules. 
　　this example demonstrates two aspects of our system: i t s use of plan-attributions  inferred in the course of interpreting the user's intentions  to recognize bugs in plans  and i t s use of private as well as mutual beliefs to determine i t s response when what the user intends is unclear due to a  buggy  plan. 
　　the next example is a variation of d1. the task here is to interact with a user when a graphics display is available for representing information about a database. the database is represented in kl-owe  brachman et a l   1   and the display consists of a graphic representation of xl-one concepts. 
d1 u: i want to see the generic concept named employee. 
1 s: 	ok. 	 displays concept in midscreen  
1 u: i can't f i t a new individual concept below i t . 
1 can you move it up  
1 s: 	sure. 	 moves 	up the generic concept  
the system's problem in responding to  can you move it up   la to determine whether the user meant his utterance directly as a question about the system's a b i l i t i e s   or intended rather to be taken to be directing the system to move the conoept under discussion. i t s decision depends on inferring the speaker's plan and  in particular  on what it believes the user's model of i t s own capacities to be. 
   this example illustrates a feature of natural interchanges: a user may have a plan in mind  and carry out a part of i t   without considering ossible undesired side effect; when one occurs  t may be recognized and eliminated. in d1 the user is carrying out the plan of accessing the concept for employee so that a/he can add a new employee to the database. s/he wants the system to 
　 actually there is a third case-the system was wrong about what  preverbal constituents  refers to. as mentioned above  we ignore this case. 
display the employee concept  but has not foreseen that i t s display looatlon might be i n f e l i c i t o u s . after the inappropriateness is discovered  the user indloatea the d i f f i c u l t y and expects it to be corrected. just how the bug in d1 is corrected depends on whether the user already believes the system can move things up and intends the system to do so  or has to find this out f i r s t . 
 from the system's point of view  the decision about what the user means may cause it to respond differently in various cases. suppose the system thlnjcs the user believes that the system can move up concepts on the screen. then when the user indloatea that his plan has a flaw  d1   the system must conclude that the user's plan is blocked by the lack of space for a new concept. when the question about moving the employee concept is raised  the system w i l l oonolude that the user intends to t e l l the system to perform the move by asking about a precondition of the action s/he wants  a precondition which consists in the system's having a capacity it is mutually believed to possess. the system is intended to move up the conoept  not simply to answer the question. 
　　a different scenario is as follows. suppose the system thinks the user is uaware that the system is oapable of moving up the conoept. then when the user indicates that his plan has a flaw and asks about moving the employee concept  the system w i l l conclude that the user intends to find out whether it has that a b i l i t y   as part of finding a means of resolving the bloolc. in this case  if the system moves the concept  that is a b i t of helpful behavior  one not intended to be recognized as intended by the user. 
   ve w i l l outline in some detail how our system reasons in such contexts by showing what plans are deduced  what rules are needed and how the reasoning proceeds in the case of d1 and 1. the relevant user plan i s : 
add-data  user   netplece   data   acreen-looation  
consider-aapect  user   netpiece  
put user  data  at  aoreen-location  
　　the add-data plan states that to add data  a user must consider some aspect of a network part  netplece  and then put some data at a screen location. even after recognizing that the user wants some data displayed after d1  the system can not deduoe that add-data is the user's plan. since there are many ways to oonsider some aspect of a net  ask for a display  think about i t   ask to be informed about i t s contents   as well as many other plans for which displaying a netpiece is a f i r s t step  the user cannot be understood to have intended the system to recognize that his plan was to add-data . a l l the system can oonolude is that the user wants the employee concept displayed  and it responds accordingly. 
　　in reasoning about d1   i can't f i t a new individual oonoept below i t     the system concludes that among the speaker's intentions mutually presumed to be recognized is that the user produoed a declarative utteranoe with the propositional content that the user cannot f i t a new individual conoept  abbrievate e1  below the generic concept  abbreviated e1 : 
belief1;  say user system  declarative 
 not  can user  pit user e1 
 below e1       
prom t h i s   the system ooncludes that the user wants the system to believe that the user believes that it can't f i t e1 below e 1   and that the user in fact believes that he can't. the system then infers the embedded proposition   not  can . . .         and that the user intended that that proposition be mutually believed. using a  default type  rule to the effect that whenever a user says that it can't 

1 

bring about a certain atata-of-affaira or parform a cartain action  tha user la tailing tha ayataa that it wanta that stata-of-affairs brought about  tha ayataa concludaa that it la intandad to baliava that tha uaar wanta it to baliava 

　 baaad on rulaa about ha1ping out whan ona haa an approprlata capacity. aa with othar hauriatica  it ＊ay ba wiaa to aonltor oarafully what tha ayataa doas to ba halpful alnoa aoaa halpful actiona  if not in faot daalrad by tha apaakar can ba aaally undooa  but otnara hava non-trivial aida~affacta. 
  ic  and  rola with valua  ara kl-owe taraa 
ballafa about tha ayataa'a oapaoltlaa affaot tha ayataa*a dataralnatlon of tha uaar'a intantiona  but alao that tha f u l l axplanation of aaon uttaranoa daapana tha ayataa*a undaratandlng of tha uaar*a goala and aubaaquant uttaranoaa. 

