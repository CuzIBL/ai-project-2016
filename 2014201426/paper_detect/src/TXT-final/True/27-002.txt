 
the standard approach t decision tree in duction is a top-down greedy agonthm that makes locall} optimal irrevocable decisions at each node of a tree in this paper we empir call} study an alternative approach in which the algorithms use one-level loo k a l i e to deride what test to use at a node weystematically compare using a very large number of rfal and artificial data sets the quality of dmsion trees induced by the greedv approach to that of trees induced using lookahead the main observations from our experments are  1  the greedv approach consistently produced trees that were just as at curate as trees produced with the much more expensive lookahead step and  n  we observed manv instances of pathology  l e   lookalnad producrd trees that were both larger and less accurate than trees produced without it 
1 	introduction 
th  standard algorithm for constructing decision trees from a set of examples is greedv induction - a tree is induced top-down with locally optimal  hoices made at each node without lookahead or backup as the greed  approach can produce suboptimal trees m terms of tree siz  and depth  it is natural!} ofintest si to explore wa s to improve the greedv strategy 
　fixed depth lookahead starch is a standard technique for improving greed} algorithms  sarkar ct al   1  lookahead is largely unexplored in tin dmsion tree literature barring a few scattered attempts discussed in section 1 the advantagt-s  or lack thereof of lookahead search have not been s}steinaticallv quantified in the context of decision tree or rule induction 
　with the rapid increases in computing power in rect nt years  limited lookahead is now feasibh for moderately large data sets the question that therefore arises is what are the benefits  tf any  thai we might gain from employing this more costly approach1 in the current paper  we attempt to answer this question  impincally we compare greedily induced trees with those induced with one-level lookahead  using two large classes of synthetic 
in decision tree induction 
steven salzberg department of computer science johns hopkins universitybaltimore m d 1 u s a balzberg c*; jhu edu 
data and eight real-world data sets from the uc1 ma chine learning repositor   murphy and aha  1  the results suggest that 
  limited lookahead search does not produce signifi-cantly better decision trees on average  it produces trees with approximately the same classification accuracy and size as greed} induction 
  limited lookahead s  arch produces inferior decision trees in a significant number of cases i e decision tree induction exhibits the same  pathology that has been observed in game trees  nan  1  
  tree post-processing techniques such is pruning arc at least as beneficial as limited lookahead for a variety of real-world data sets in this context  we describe a new post-processing technique decision tree balancing 
　section 1 describes our experimental method sec lions 1 and 1 present the results of our experiments with synthetic and real world data respectivel} section 1 summarizes related work in the literature and discusses open questions 
	1 	experimental m e t h o d 
the algorithms we used in all uur experiments  greedy and look  are described below look performs one level of lookahead to decide what test to use at a decision tree node while gretdy decides based only on local considerations in the pseudocode below  s is the set of training examples where each example is assumed to compnse i set of nummc features and a class label 
algorithm greed  s  
1 if s contains examples from onl} one class halt 
1   onsider all distinct tests t of the form r   k on i he features of s the ls are chosen to be the midpoints between adjacent feature values c hoose the test t* that is the best according lo a prc-d -fin  d goodiu ss measure 
1 split s into two subsets si and s1 using t  
1 recursivel} run this procedure on si and s1 
algorithm look s  
1 execute step 1 of greedy 
1 for each test t of the form x   k do 
	murthy and salzberg 	1 
 a  split s into sets s  and s1 using t 
 b  find the best split of si into sets s l l and sl1 using steps 1 of algorithm g r e e d y 
 c  repeat  b  on s1 forming set'  s1 and s1 
 d    ompute the goodness of splitting s into si 1 s1 s1  and s1 using the same goodness measure as g r e e d y this is 1 s goodness 
	1 	execute steps 1 of greedy 
   we experimented with two pre-defined goodness mea mires namely the gini index of diversity  breiman e1 al 1  and information gain  quinlan 1  ' this gave us four algorithms for our experiments which we named greedy-gim greedy-info look gini and lookinfo note that gmedy-gini is essentiall  identical to tlhe c a r t algorithm  breiman ei al 1  ind greedy info to the id'i algorithm  quinlan 1b  
   our experiments with s nth tic data  section 1  systematically compare the trees induced with one level lookahead to those induced greedily  our entire clasies of decision trees  ~ we define below two classes of decision trees that are small enough to be amenable to systematic experimentation on the entire clss and general enough to be interesting we first generated a training set train and a test set test 1 r a    has 1 exam pies and test 1 examples with two real valued attributes for each example all attribute values were generated uniformly al random in the interval  1  the same unlabeled training and test sets art used in all the experiments each experiment tested a different 

	1 	1 
figure 1 class c consists of all balanced decision tries on a 1 x 1 grid such that each tree has three lepit  internal  nodes and all test nodes are non-lnvial  in the sense thai they split heterogeneous point sets is there are two classes 1 and 1 
element of the concept class and the eximples were al beled accordingly trees built onlis possible advantages more specifically the complex- traj  were tested on the root in class  . will always hnd the ophmal decision tree in terms of both size and depth frees in this class realistically occur in many situations as subtrees of a larger tree and it is reasonable to ask if we should con stantl} check one level ahead while building such a tree in order to see if we can finish off a subtree because e en one level of lookalit ad is ver  costl}  we wish to quantily 

iest for every concept in each class 
   trees are compared to each other throughout this paper using three quality measures - accuracy size and depth accuracy is the percentage of correct classification on tebt size is the. number of leaf nodes depth is the length of the longest path in the tree 
1 	experiments with synthetic data 
1 	c exhaustive vs greedy search 
we designed our first set of experiments to measure how close lo optimal are the trees produced by greedy induction on a fixed concept class more precisely  we consider a class of concepts c in which one-level lookahead is equivalent to exhaustive search  for this class lookahead always gives us the optimal tree while greedy induction may not we systematically evaluate the effectiveness of greedv induction over this entire class 
   c is a class of binary decision trees defined as in fig 1 and has a total of 1 distinct trees   frees that are equivalent except for having their class labels swapped are not considered distinct   one level of lookahead from 
　　we chose gini index and information gain because they have been widely used for real world applications expen mente with other goodness measures may be interesting  but we suspect the results would be similar 
1
　　thie style of empirical investigation is made possible by the existence of extremely fast inexpensive computers see 
 murphy and pazzani 1  for another example of this slyle 
1 	- 	iparninh 
it} of the standard greedy algorithm is 1 dn logn  at a node  for d allnbutes and n examples one level lookalit ad has complexity 
　using the experimental method defined in section i we built 1 trees on the set train with each of lh  four algorithms i hus one tree was  induced by each algo nlhm for every possible element of c as ont level lookd head is the same as exhaustive search on c look-info and look-gim produce identical trees figure 1 summarizes the differences between the decision tries induced by grftdy-gim greedy info  and exhauslive search  u ther look-info or look gtni  over the entire class c the figure figure shows the mean and one quartile ranges of the accuracy  tree size and maximum depth  one quartih range is the interval that includes 1% of the samples above and below the mean   
　as the figure shows the differences between giecdygtm greedy info  and look are quite small  in spite of the fact that greedy induction uses only about    1 limes as much search as exhaustive search the average 
number of candidate splits evaluated per tree in c art. 
grtedy-gim 	1  greedy-info 	1  look 	1 
the differences in accuracy between the greedy algorithms and look are negligible the difference in tree size between greedy-info and exhaustive search is 1 nodes  less than one standard deviation the difference of 1 between the average tree size of grcedy-gini and look is slightly more pronounced but still not significant the only measure for which greedily induced trees are significantly worse than the optimal trees is maximum depth exhaustive search produces trees whose longest 

mean and one quartilt ranges for accuncv tree size and maximum depth are shown for dntdy gam one-level lookahead and greedy-htfo the accuricies shown are the amounts above a baseline value of 1% 
piths an on average one level  shorter t h i n what is product d with the greed} algorithms ' 
   figure { shows the effects of one level lookahead  equivalent!} exhaustive search  for class c in more de tail the horizontal axis plots the impiovement due to lookahead the  line for accuracy shows the ;increase in arcurac} whereas the lines for tres size and depth show the decrease in these measures when lookahead is used the ve rtical axis plols the number of trees in which lookahead causes a particular improvement hur points on   - 1 lookahead had no effect and for points to the right of   = 1 lookahead was beneficnl tor points to the left of  he line 1i = it greed} induction was better thin lookahead only tin measurement for information gain are shown due to space constraints 
   i ig i offers several interesting insights first each of the three lines has a single prominent peak the peaks it   - 1 for accuracy and tree size lines show that for a large number of trees lookahead did not make any difference in terms of these measures the depth peak at   = 1 shows that the maximum d pth of most of the greedily induced trees is exactly oik more than optimal lo understand why the greedy approarh builds trees with unnecessarily long paths we looked at several of these trees individually and found that man} trees were unbalanced that is  there were several trees in which nodes could bi moved around without altering the original partitioning and accuracy to cut short the maximum depth of the tree appendix a describes a simple post-proceasing step to rebalance a greedily m-
   1note that the effect of lookahead on average or expected depth may not be the same as that on maximum depth the expected depth of a greedily induced decision tree has been observed to be very close to that of the optimal tree  murthy and salzberg  1  
figure   eifect of one level iookahead in trees produced with information gam for class c improvements in accuracy  si e and maximum depth are shown  along with the number of trees in which these improvements occur negative values on the x-axis mean that iookahead produced inferior trees 
duced tree  in order to reduce its worst-case classification cost use of this decision tree balancing procedure filled some of the gap between the greedy and iookahead trees in all our experiments 
　second  it is interesting to note that iookahead ac lually hurts accurat  in almost as many trees as those in which it enhances accuracy this property where iookahead search finds inferior solutions is known as pathology in the context of game trees  nau 1 mutchler 1  we discuss pathology for decision trees further in section 1 where this t rend is exhibited more prominently pathology cannot occur for tree size or depth for class c1  because one-level iookahead is equivalent to exhaustive search however our next class cs includes deeper trees and limited iookahead can and does produce trees that are worse in terms of size and depth 
　third  we tan see from figure 1 that there are some greedily induced trees that have as manv as 1 leaves more than the optimal we looked at all such large trees  and found that thev always had several  mimmallv useful splits  splits that were separating very few points such splits cin be easily avoided with a simple stop-splitting rule  narrowing the gap between iookahead and greed  induction further 
1 	c$ 	a class of larger trees 
thi1 section extends class c to a class cs  which contains slightly larger trees each tree in cs is obtained from a different tree in c  as follows 
1 remove t from c 
1 randomly choose a leaf node l of t 
	murthyandsalzberg 	1 

         summary of experiment with class cs mean and one quartile ranges for accuracv tree size and maximum depth are shown for greedy  gins  look-gim greedy-info and look-info the accuracies shown are the amounts above a baseline value of   r- % 
j split l with a randomly chosen non-lmial spill *s of the form x    k where it is an integer in the range  1  if no valid split exists  go to step i and choose a different l 
1 assign one side of $ to class 1 randomlv and assign the other side to class 1 
1 add t to cs 
each decision tree in cs is a binary trt-  with four test  internal  nodes and has a maximum depth of 1 for these trees one level lookahead is not sufficient to find the optimal tree note that while cs has 1 trees the same as c  another run of the above procedure would create a different definition of cs because of the ran domized steps using exhaustive enumeration in place of these random choices would produci a class that is vastly larger too large for systematic t expenmentation the experimental method used for cs was identical to that used for c one important difference is  since onelevel lookahead is not equivalent to exhaustive search on cs  look-gmi and look info do not produce identical trees for this class 
　the experimental results with class cs strengthen the conclusions drawn from experiments with class c figure 1 summarizes the differences in accuracy tree size and maximum depth between greedy-gim  look-gtni  greedy-info and look info on class cs it can be seen that there is no significant improvement in accuracy due to lookahead the differences in accuracy due to lookahead are actually smaller here than they were for class c  despite the fact that the relative cost of lookahead search was higher for this class the average number of candidate splits considered per tree in cs were greedygmi 1  greedy info 1  look gini 1 and look-info 1 despite these enormous differences in computational effort  the differences in tree size are 
1 	learning 
figure 1 effect of one level lookahead for trees in class 
cs lmprovermnte in accuracy size and maximum depth of trees buill using look-info versus greedy-info art shown negative values on the x-axis mean that lookahead produced inferior trees 
less than one standard deviation the onlv quantity for which one-level lookahead caused any noticeable improvement was maximum depth where trees were on average 1 c levels shallower when lookahead was used 
pathology results figure 1 shows the effect of onelevel lookahead for class cs m more detail for greedyinfo the svnlax of this figure is the same as that of figure 1 i e points to the left of 1 on the horizontal axis represent instances of pathology  where lookahead was worse than no lookahead lookahead hurt accuracv for a large number of trees in cs just as it did for c in addition it produced worse trees in lerms of tree size and depth figure 1 shows a data set in cs for which information gain exhibits pathology in terms of accuracy  size and depth 
1 	experiments with real world data 
in addition to the synthetic data  we also experimented with eight real world data sets  for which the underlying concepts are unknown we augmented our algorithms  greedy-gini  look-gtni  greedy-info and lookinfo  with pruning for these experiments  using cost complexity pruning with the one standard error rule  breiman el al   1   reserving 1% of the training data as the pruning set all results for real world data are averages of ten 1-fold cross validation experiments 
　the choice of the domains is important if a greedy method can induce a highly accurate  concise classifier for a domain  e g   the well-known ins data   lookahead is not likely to produce significant benefits we used a survey of results  holte  1  to choose six  difficult  domains for our experiments - domains for which the best 

was 1 1 

known accuracy is al mosl 1% 1 houeji  he low accuracies may be due to factors other than the inadequacy of greedy induction such as an overly small or noisy training set there is no sir ughtforward w *y of knowing i his a piwn the six difficult domains an the breast canc r ncurrence datahase  bi    the   leveland heart disase data  c l   uci cleyeland d a t i   glass identific ation data  ctl   hepatitis diagnosis  jit  c inadian la hor negotiations data  la  and lymphography diagnosis 
 l     1  i-lymph-dati   	in addition in these domains figure 1 	lffect of one level lookahead on classification accuracy for eight real-world databases 	t h t accuracies 
with and without lookahead and with and without pruning are shown for information gam 
in fig 1 is that lookahead doesn t affect accuracy signifi 
cantly for these domains and that pruning is both much cheaper and more effective at creating accurate trees 
　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　shadowed by. the benefits of pruning for all domains we experimented with lun   an in is    1 and   1  of tthe congressional voting records- clatd vois is used in nor-   now consider the tree size plot  shown in fig 1 lookahead does reduce the tree size  bv a small amount in most domains these benefits however  are over-

ton  norton 1  for his lookahead   penments the    data  holle 1  is identical to  the  1 data e   | t that the best attribute physici in ft   freeze is removed all the data se|s were taken from the t   l machme learning repository  murph  and aha l1  our abbrevations for the d a t t s t t s an consistent with those of holle  holle 1  
   all experimental resuits reporttd in this section were obtained with information gam resuits with gmmdex look very similar and art omitted for space considerations figures 1 and 1 summarize lhe results for accuracy and tree size respective } i he plot for tree depth looks almost identical to that for tree size and is omitted in each figure we plot the values of lhe measure obtained using four induction methods  1  greedy-info   n  lookhifo  in  greedy-info with pruning and  iv  look-info 
with pruning there are eight linos in each figure corresponding to the eight data sets 
   consider the accuracy plot m fig 1 the first obser vation is that the accuracies do not vary much between various induction methods on closer observation  accuracy drops for six out of the fight databases  all except vi and gt   when lookahead is used in addition greedy-info with pruning produces more accurate trees than look info for five data sets pruning almost always  1 out of 1 times  works better when it is used without lookahead  as can be seen from the third and fourth columns our overall conclusion from tin accuracy plot except the i a data  which has a verv small tree to begin with}  pruning helps produce substantially smaller 
irees than lookahead 
   the results of our experiments with real data support our results with the artificial data a limited lookahead did not help significantly in terms of classification accu rac   size or depth  despite the fact that it is enormously mon expensive it helped produce shallower trees  but tree post-processing techniques much less expensive than lookahead  pruning in this case  were  adequate to reap comparable if not larger  benefits finalh both of the goodness measures we used  gini index and information gain  exhibited pathology on the real world domains also 
1 	discussion 
several versions of the optimal decision tree induction problem are known to be np-complete  hvafil and pivest  1 murph  and mccraw 1  as a re suit  virtually all implemented decision tree systems use a heuristic greedy approach there have been however some exceptions to this rule m o n t  morel 
1  surveys early induction systems that used dynamic 
1
　　 note that ill of our  difficult  data sets happen to he quite small  probably inherently inadequate for learning the experiments with real data are given only to substantiate the earlier observations on the artificial data we would not make strong conclusions from the uci data alone 
	murthy and salzberg 	1 


figure 1 effect of one levei lookahead on tree size for eight real world databases the tree sizes with and with out lookahead and with and without pruning are shown for information gam 
programming and brancli-and-bound methods to prodme optimal trees hart mann at al  hartmann of al 
1  describe generalized optimum testing algorithm  gota  an algorithm based on an information the retic criterion between branching levels in a tree with the appropriate parameter settings go i a can do fixeddepth lookahead  different depths of lookahead al differ ent branching levels or even exhaustive search though hartmann et al did offer a concist framework for doing arbitrary level lookahead the  did not evaluate the of-fects fects of lookahead on tree quality the ideas in gota motivated norton & idx system  norton 1  which is a variant of qumlan & id1 that performs lookahead norton conducted experiments on the congressional voting records database  see section 1   and found that lookahead reduced decision tree depth on average with a few exceptions though the advantages of lookahead were very small in norton s experiments ragavan and rendell considered using lookahead for feature construction in symbolic domains  ragavan and ren dell 1   and pointed out that lookahead is beneficial when there is concealed attribute interaction 
　the emphasis of the current paper differs significantly from the existing work on lookahead hirst our experiments are aimed to offer insights on whether or not to use lookahead when little is known about domain characteristics or attribute interactions second  though existing papers do contain some remarks about whether lookahead did or did not help  no work has vet attempted to systematically quantify how often lookahead helped 
how often it did not make a difference and how often it hurl tree quality 
　the pathology results are particularly interesting  since they have not been previously reported for decision trees intuitively  doing more search  lookahead  should produce better decision trees  just as deeper search in 
1 	learning 
game trees  e g for chess  produces better game-playing programs however it has been observed that for some games  deeper search can actually produce an inferior program both with two players  nau  1  and with multiple players  mutchler  1  decision trees  one can argue  are analogous to a one-player game tree our discovery that deeper search can lead to inferior decision tree1; thus extends the earlier pathology results to a new domain 
　it is possible that pathology is a side-effect of the way heuristic goodness measures are defined greedy methods grow a decision tree by optimizing entropy or classdivergence based measures at each node of the tret our pathology results indicate that each such optimization it  not necessanl  improving the tree globally in terms of generalisation accuracy  tree size or depth goodman and smyth  showed that greedily maximizing the average mutual information should result in trees thai are near optimal in terms of average depth although our experimental results are consistent with this work pathologically deep trees indicate that locallv optimizing information gain can in fact make a tree deeper 
　we considered only one-level lookahead in this paper one can attempt to evaluate the benefits of lookahead as a function of se arch depth we feel that such a syslematic evaluation is not only going to be computationally prohibitive  butl also probably not very useful norlon  norton 1  presents e- xpenments comparing one and two level lookahead on one data set 
　observing incidences of pathology  as we did in this paper  is only the first st  p in several interesting research directions concept i lasses for which a particular good ness measure exhibits pathology can be sludied  analytically or quantitatively to determine when pathologv might occur on the other hand  one can attempt to isolate characteristics of data which have bearing on when lookahead is likely to help as we have only studie-d two concept classes  several othe r interesting concepts remain to be explored another interesting question for further study is whethtr there exist effective goodness measures that guarantee no pathology 
a 	decision tree balancing 
 i he main benefit of lookahead search for classes c and cs was that lookahead produced trees with shorter longest paths on closer observation  we found that several greedily induced trees had identical partitions as the ones induced with lookahead  but the latter were shallower because the trees were better balanced this trend suggests the following problem given a decision tree d for a training set train we want to produce a tree dr that induces the same partitioning as d on train  but has less worst-case cost  or maximum depth  
　although little work has been done on balancing decision trees  a great deal of research has considered balanced search trees  e g  nakamura ct al   1   roughly speaking  this literature deals with techniques to restructure search trees when elements are inserted or deleted  in order to restrict the depth of these trees to a logarithmic function of the number of search keys an axis-parallel decision tree in a continuous space can be 


figu e 1 left and right rotations of a binary decision tree dotation operators can help reduce the expected rosi of classification of a derision tree without changing lis acuracy the leaf nodes i 1 l1 tt  in this figure can ik replaced with arbi rar  subtrees 
interpreted as a inulti dimensional binay  search tree such in interpretation makes il possible to use search tree balancing techniques on decision trees 
　the mam primitives used for rebalancing a trte in balanerd seach tree methods are rotations rotations are operations in which the parent child links of some nodes in the tree art rearranged locally while guaranteeing that the functionality of th  whole from romans invarant anl   t have adopted two simple tree rotation operators left rotate and right rolate to decision trees thest operators are illustrated in tigure 1   r found that a heuristic top-down tree balancing procedure using rola lion operators recursively at the tree nodes significantly reduces the maximum depth of greedily induced trees for classes ♀ and c& 
