
the concept of stereotype seems to be really adapted when wishing to extract meaningful descriptions from data  especially when there is a high rate of missing values. this paper proposes a logical framework called default clustering based on default reasoning and local search techniques. the first experiment deals with the rediscovering of initial descriptions from artificial data sets  the second one extracts stereotypes of politicians in a real case generated from newspaper articles. it is shown that default clustering is more adapted in this context than the three classical clusterers considered.
introduction
conceptual clustering  michalski  1  is a fundamental machine learning task that is applied in various areas such as image analysis  analytical chemistry  biology  sociology. it takes a set of object descriptions as input and creates a classification scheme. the conceptual descriptions of clusters are of particular interest to reason about the categories themselves  to compare different data sets and to predict new observations. this work focuses on the extraction of such conceptual descriptions  especially in the specific context of missing data.
모automatic inductive techniques have to deal with missing information  due to voluntary omissions  human error  broken equipment  newgard and lewis  1 . in the context of sparse data  i.e. with a huge amount of missing values  the concept of stereotype seems more appropriate than the usual one of prototype to describe data clusters. therefore  our goal is to extract stereotype sets that represent the data sets as well as possible. by analogy to default logic  reiter  1   which is a specific logic for default deduction  we make use of default subsumption  which is a specific logic for default induction  to build such stereotypes.
모section 1 presents a new approach to conceptual clustering when missing information exists. section 1 proposes a general framework in the attribute-value formalism. the new notion of default subsumption is introduced  before seeing how the concept of stereotype makes it possible to name clusters. a stereotype set extraction algorithm based on local search techniquesis then presented. section 1 concerns experiments  first on artificial data sets and secondly with a real data case generated from newspaper articles.
1 conceptual clustering with sparse data
1 dealing with missing values
this paper proposes a clustering method that deals with high rates of missing values. but contrary to algorithms such as kmodes  categorical version of k-means  or em  that can easily lead to local optima  we have chosen to achieve the clustering using a combinatorial optimization approach  like in  figueroaet al.  1 or  sarkar and leong  1 . note that our goal is not only to cluster examples but also and mainly to describe the cluster in a way that is simple and easy to understand. the problem can thus be stated as finding readable  understandable  consistent and rich descriptions from the data.
1 overview of default logic
during the eighties  there were many attempts to model deductive reasoning when missing information exists. a lot of formalisms were developed to encompass the inherent difficulties of such models  especially their non-monotony: closeworld assumption  circumscription  default logic  etc. since our goal is to deal with missing values  it seems natural to take advantage of this work. the default logic formalism  introduced by r. reiter in 1  was chosen because it seemed to correspond well to our problem.
모this logic for default reasoning is based on the notion of default rule  through which it is possible to infer new formulas when the hypotheses are not inconsistent with the current context. more generally  a default rule always has the following form: a : b1 b1 ...bn/c where a is called the prerequisite  bi the justifications and c the conclusion. this default rule can be interpreted as follows: if a is known to be true and if it is consistent to assume b1 b1 ...bn then conclude c. for instance  let us consider the default rule below related to the experiments of the last section:
politician x  뫇 introducedabroad x  :  diplomat x 

traitor x 
 this rule translates an usual way of reasoning for many people living in france at the end of the 1th century. it states that the conclusion traitor x  can be derived if x is a politician who is known to be introduced abroad and that we cannot prove that he is a diplomat.
모the key idea here is to use similar observations and their descriptions to infer new information instead of default rules  but the underlying mechanism is the same. the following subsection explains the transition from default logic to default induction.
1 default clustering
e. rosch saw the categorization itself as one of the most important issues in cognitive science  rosch  1 . she introduced the concept of prototype as the ideal member of a category. whereas categorization makes similar observations fit together and dissimilar observations be well separated  clustering is the induction process in data mining that actually build such categories. more specifically  conceptual clustering is a machine learning task defined by r. michalski  michalski  1  which does not require a teacher and uses an evaluation function to discover classes that have appropriate conceptual descriptions. conceptual clustering was principally studied in a probabilistic context  see  for instance  d. fisher's cobweb algorithm  fisher  1   and rarely on really sparse data sets. for instance  the experiments done by p.h. gennari do not exceed 1% of missing values  gennari  1 .
모as seen above  default logic is a logic for deduction depending on background knowledge. this paper proposes a new technique called default clustering which uses a similar principle but for induction when missing information exists. the main assumption is the following: if an observation is grouped with other similar observations  you can use these observationsto complete unknowninformationin the original fact if it remains consistent with the current context. whereas default logic needs implicit knowledge expressed by default rules  default clustering only uses information available in the data set. the next section presents this new framework. it shows how to extract stereotype sets from very sparse data: first it extends classical subsumption  see section 1   next it discusses stereotype choice  see section 1   and finally it proposes a local search strategy to find the best solution  see section 1 .
1 logical framework
this section presents the logical framework of default clustering in the attribute-value formalism  an adaptation to conceptual graphs can be found in  ganascia and velcin  1  . the description space is noted d  the descriptor space  i.e. the values the attributes can take  v and the example set e. the function 붻 maps each example e 뫍 e to its description 붻 e  뫍 d.
1 default subsumption
contrary to default logic  the problem here is not to deduce  but to induce knowledge from data sets in which most of the information is unknown. therefore  we put forward the notion of default subsumption  which is the equivalent for subsumption of the default rule for deduction. saying that a description d 뫍 d subsumes d1 뫍 d by default means that there exists an implicit description d1 such that d1 completed with d1  i.e. d1 뫇 d1  is more specific than d in the classical sense  which signifies that d1 뫇 d1 entails d. the exact definition follows:
모definition 1 d subsumes d1 by default  noted d 뫞d d1  iff   dc such that dc 1=뫐 and d 뫞 dc and d1 뫞 dc where t 뫞 t1 stands for t subsumes t1 in the classical sense. dc is a minorant of d and d1 in the subsumption lattice.
모to illustrate our definition  here are some descriptions based on binary attributes that can be compared with respect to the default subsumption:
모d1 = { traitor = yes   internationalist = yes } d1 = { traitor = yes   connectionwith jews = yes } d1 = { patriot = yes } d1 뫞d d1 and d1 뫞d d1 because   dc such that d1 뫞 dc and d1 뫞 dc: dc = { traitor = yes   internationalist = yes   connectionwith jews = yes }.
모however  considering that a patriot cannot be an internationalist and vice-versa  i.e.    patriot=yes  뫇  internationalist=yes    which was an implicit statement for many people living in france at the end of the 1th century  d1 does not subsume d1 by default  i.e.   d1 뫞d d1 .
모property 1 the notion of default subsumption is more general than classical subsumption since  if d subsumes d1  i.e. d 뫞 d1  then d subsumes d1 by default  i.e. d 뫞d d1. the converse is not true.
모property 1 the default subsumption relationship is symmetrical  i.e.  d  d1 if d 뫞d d1 then d1 뫞d d.
모note that the notion of default subsumption may appear strange for people accustomed to classical subsumption because of the symmetrical relationship. as a consequence  it does not define an ordering relationship on the description space d. the notation 뫞d may be confusing with respect to this symmetry  but it is relative to the underlying idea of generality.
1 concept of stereotype
in the literature of categorization  rosch introduced the concept of prototype  rosch  1; 1  inspired by the family resemblance notion of wittgenstein  wittgenstein  1   see  shawver  1  for an electronic version and  narboux  1  for an analysis focused on family resemblance . even if our approach and the original idea behind the concept of prototype have several features in common  we prefer to refer to the older concept of stereotype that was introduced by the publicist w. lippman  lippman  1 . for him  stereotypes are perceptive schemas  a structured association of characteristic features  shared by a group about other person or object categories. these simplifying and generalizing images about reality affect human behavior and are very subjective. below are three main reasons to make such a choice.
모first of all  the concept of prototype is often misused in data mining techniques. it is reduced to either an average observation of the examples or an artificial description built on the most frequent shared features. nevertheless  both of them are far from the underlying idea in family resemblance. especially in the context of sparse data  it seems more correct to speak about a combination of features found in different example descriptions than aboutaverage or mode selection. the second argument is that the notion of stereotype is often defined as an imaginarypicture that distorts the reality. our goal is precisely to generate such pictures even if they are caricatural of the observations. finally  these specific descriptions are better adapted for fast classification  we can even say discrimination  and prediction than prototypes. this last feature is closely linked to lippman's definition.
모in order to avoid ambiguities  we restrict the notion of stereotype to a specific description d 뫍 d associated to  we can say  covering   a set of descriptions d   d. however  the following subsection does not deal just with stereotypes but with stereotype sets to covera whole descriptionset. the objective is therefore to automatically construct stereotype sets  whereas most of the studies focus on already fixed stereotype usage  rich  1; amossy and herschberg pierrot  1 . keeping this in mind  the space of all the possible stereotype sets is browsed in order to discover the best one  i.e. the set that best covers the examples of e with respect to some similarity measure. but just before addressing the search itself  we should consider both the relation of relative cover and the similarity measure used to build the categorization from stereotype sets.
1 stereotype sets and relative cover
given an example e characterized by its description d = 붻 e  뫍 d  consider the following statement: the stereotype s 뫍 d can cover e if and only if s subsumes d by default. it means that in the context of missing data each piece of information is so crucial that even a single contradiction prevents the stereotype from being a correct generalization. furthermore  since there is no contradiction between this example and its related stereotype  the stereotype may be used to complete the example description.
모in order to perform the clustering  a very general similarity measure msim has been defined  which counts the number of common descriptors of v belonging to two descriptions  ignores the unknownvalues and takes into account the default subsumption relationship:
msim: d뫄d	뫸 n+
 di d   1뫸 msim di d   = |{v 뫍 d/d = di 뫇d }|
모모모모모모모모모msim di dj  = 1 if   di 뫞d dj   where di 뫇 dj is the least minorant of di and dj in the subsumption lattice.
모let us now consider a set s = {s  s1 s1 ...sn}   d of stereotypes. s  is the absurd-stereotype linked to the set e . then  a categorization of e can be calculated using s with an affectation function that we called relative cover:
모definition 1 the relative cover of an example e 뫍 e  with respect to a set of stereotypes s = {s  s1 s1 ...sn}  noted cs e   is the stereotype si if and only if:
  si 뫍 s 
  ms 붻 e  si    1 
   k 뫍  1 n  k 1= i msim 붻 e  si    msim 붻 e  sk .
모it means that an example e 뫍 e is associated to the most similar and  covering-able  stereotype relative to the set s. if there are two competitive stereotypes with an equal higher score or if there is no covering stereotype  then the example is associated to the absurd-stereotype s . in this case  no completion can be calculated for e.
1 stereotype extraction
in this paper  default reasoning is formalized using the notions of both default subsumption and stereotype set. up to now  these stereotype sets were supposed to be given. this section shows how the classification can be organized into such sets in a non-supervised learning task. it can be summarized as follows. given: 1. an example set e.
모1. a description space d.
모1. a description function 붻: e  뫸 d which associates a description 붻 e  뫍 d to each example belonging to the training set e.
the function of a non-supervised learning algorithm is to organize the initial set of individuals e into a structure  for instance a hierarchy  a lattice or a pyramid . in the present case  the structure is limited to partitions of the training set  which corresponds to searching for stereotype sets as discussed above. these partitions may be generated by  n + 1  stereotypes s = {s  s1 s1 ...sn}: it is sufficient to associate to each si the set ei of examples e belonging to e and covered by si relative to s. the examples that cannot be covered by any stereotype are put into the e  cluster and associated to s .
모to choose from among the numerous possible partitions  which is a combinatorial problem  a non-supervised algorithm requires a function for evaluating stereotype set relevance. because of the categorical nature of data and the previous definition of relative cover  it appears natural to make use of the similarity measure msim. this is exactly what we do by introducing the following evaluation function he:
	definition	1	e	being	an	example	set 	s	=
{s  s1 s1 ...sn} a stereotype set and cs the function that associates to each example e its relative cover  i.e. its closest stereotype with respect to msim and s  the evaluation function he is defined as follows:
he s  = x msim 붻 e  cs e   e뫍e
모while k-modes and em algorithms are straightforward  i.e. each step leads to the next one until convergence  we reduce here the non-supervisedlearning task to an optimization problem. this approach offers several interesting features: avoiding local optima  especially with categorical and sparse data   providing  good  solutions even if not the best ones  better control of the search. in addition  it is not necessary to specify the number of expected stereotypes that is also discovered during the search process.
모there are several methods for exploring such a search space  hill-climbing  simulated annealing  etc. . we have chosen the meta-heuristic called tabu search which improves the local search algorithm. remember that the local search process can be schematized as follows: 1. an initial solution sini is given  for instance at random . 1. a neighborhood p is calculated from the current solution si with the assistance of permitted movements. these movements can be of low influence  enrich one stereotype with a descriptor  remove a descriptor from another  or of high influence  add or retract one stereotype to or from the current stereotype set . 1. the
	sc 뫹 {s } ; the current	solution
	sb 뫹 sc ; the best up-to-now	solution
t 뫹   ; the list that contains tabu-attributes for i 뫹 1 to nstep do {
let sc be {s  s1 s1 ...sk}
p 뫹   ; initialize the neighborhood for all ai 뫍/ t do {
for m 뫹 1 to k do {
for all v 뫹  ai = vij  do
	if v	of sc {
	뫹	1	m 1 m	m+1 .	k
   p 뫹 p 뫋 s } if v a stereotype of sc {
뫹 m
if s1=   then s 뫹 {s1 ...sm 1 s1 sm+1 ...sk}
else s 뫹 {s1 ...sm 1 sm+1 ...sk}
if s 1=   then p 뫹 p 뫋 s }
}
for all v 뫹  ai = vij  if v does not belong to a stereotype of sc do {
sn 뫹 {v} s 뫹 {s1 s1 ...sm sn}
p 뫹 p 뫋 s }
}
sn 뫹 argmax s뫍p he s  
t is updated  depending on the chosen attribute ai that permits to pass from sc to sn. sc 뫹 sn if he sc    he sb  then sb 뫹 sc }
return sb
figure 1: the default clustering algorithm
best movement  relative to the evaluation function he  is chosen and the new current solution si+1 is computed. 1. the process is iterated a specific number of times nstep and the best up-to-now discovered solution is recorded. then  the solution is the stereotype set sb that best maximizes he in comparison to all the crossed sets.
모as in almost all local search techniques  there is a tradeoff between exploitation  i.e. choosing the best movement  and exploration  i.e. choosing a non optimal state to reach completely different areas. the tabu search extends the basic local search by manipulating short and long-term memories which are used to avoid loops and to intelligently explore the search space. this meta-heuristic is detailed in  glover and laguna  1  and its application to clustering can be found in  al-sultan  1 . note that only the short-term memory was used at this stage of our work.
1 default clustering algorithm
fig. 1 is the main frame of the default clustering algorithm. it is based on a very basic version of tabu search that tries to maximize our function he. ai denotes the ith attribute and e is the example set. sc and sb stands respectively for the current and for the best solution. nstep is the maximal number of iterations  p the current neighborhood and t the tabu-list that contains tabu-attributes. if an attribute ai is in the tabu-list then no descriptor  ai = vij  can be used to calculate the neighborhood of the current solution.
모a  no-redundancy  constraint has been added in order to obtain a perfect separation between the stereotypes. in the context of sparseness  it seems really important to extract contrasted descriptions which are used to quickly classify the examples  as does the concept of stereotype introduced by lippman.
모a new constraint called cognitive cohesion is now defined. it verifies cohesion within a cluster  i.e. an example set ej   e  relative to the correspondingstereotype sj 뫍 s. cognitive cohesion is verified if and only if  given two descriptors v1 and v1 뫍 v of sj  it is always possible to find a series of examples that makes it possible to pass by correlation from v1 to v1. below are two example sets with their covering stereotype. the example on the left verifies the constraint  the one on the right does not.
s1 : a1   b1   d1   f1   h1s1 : a1   b1   d1   f1   h1e1 : a1           h1 e1 : a1  b1          e1 :       d1       e1 :    b1  d1  f1    e1: a1     d1      e1 : a1  b1          e1 :          f1    e1 : a1  b1          e1:       d1     h1 e1:       d1     h1hence  with s1 it is never possible to pass from a1 to d1  whereas it is allowed by s1  you begin with e1 to go from a1 to b1  and then you use e1 to go from b1 to d1 . it means that  in the case of s1  you are always able to find a  correlation path  from one descriptor of the description to another  i.e. examples explaining the relationship between the descriptors in the stereotype.
1 experiments
this section presents experiments performedon artificial data sets. this is followed by an original comparison in a real data case using three well-known clusterers. default clustering was implemented in a java program called press  programme de reconstruction d'ensembles de ster뫣 eotypes뫣 structures뫣  . all the experiments for k-modes  em and cobweb were performedusing the weka platform  garner  1 .
1 validation on artificial data sets
these experiments use artificial data sets to validate the robustness of our algorithm. the first step is to give some contrasted descriptions of d. let us note ns the number of these descriptions. next  these initial descriptions are duplicated nd times. finally  missing data are artificially simulated by removing a percentage p of descriptors at random from these ns 뫄 nd artificial examples. the evaluation is carried out by testing different clusterers on these data and comparing the discovered cluster representatives with the initial descriptions. we verify what we call recovered descriptors  i.e. the proportion of initial descriptors that are found. this paper presents the results obtained with ns = 1 and nd = 1 over 1 runs. the number of examples is 1 and the descriptions are built using a langage with 1 binary attributes. the tabulist length is equal to 1 and nstep to 1. note that the first group of experiments are placed in the missing completely at random  mcar  framework.
모fig. 1 shows firstly that the results of press are very good with a robust learning process. the stereotypes discovered correspond very well to the original descriptions up to 1% of missing descriptors. in addition  this score remains good  nearly 1%  up to 1%. whereas cobweb seems stable relative to the increase in the number of missing values  the results of em rapidly get worse above 1%. those obtained using k-modes are the worst  although the number of expected classes has to be specified.

figure 1: proportion of recovered descriptors.
1 studying social misrepresentation
the second part of the experiments deals with real data extracted from a newspaper called  le matin  at the end of the 1th century in france. the purpose is to automatically discover stereotype sets from events related to the political disorder in the first ten days of september 1. the results of press are comparedto those of the three clusterers k-modes  em and cobweb. it should be pointed out that our interest focuses on the cluster descriptions  which we call representatives to avoid any ambiguity  rather than on the clusters themselves.
모the articles linked to the chosen theme were gathered and represented using a language with 1 attributes. the terms of this language  i.e. attributes and associated values  were extracted manually. most of the attributes are binary  1 accept more than two values and 1 are ordinals. the number of extracted examples is 1 and the rate of missing data is nearly 1%  which is most unusual.
1 evaluation of default clustering
in order to evaluate press  a comparison was made with three classical clusterers: k-modes  em and cobweb. hence  a non-probabilistic description of the clusters built by these algorithms was extracted using four techniques:  1  using the most frequent descriptors  mode approach ;  1  the same as  1  but forbidding contradictory features between the examples and their representative;  1  dividing the descriptors between the different representatives;  1  the same as  1  but forbidding contradictory features. two remarks need to be made. firstly  the cluster descriptions resulting from k-modes correspond to technique  1 . nevertheless  we tried the other three techniques exhaustively. secondly  representatives resulting from extraction techniques  1  and  1  entail by construction a redundancyrate of 1%. the comparisonwas made according to the following three points:
모the first approach considers the contradictions between an example and its representative. the example contradiction is the percentage of examples containing at least one descriptor in contradiction with its covering representative. in addition  if you consider one of these contradictory examples  average contradiction is the percentage of descriptors in contradiction

figure 1: comparative results on le matin.
with its representative. this facet of conceptual clustering is very important  especially in the sparse data context.
모secondly  we check if the cognitive cohesion constraint  see 1  is verified. the rate of descriptor redundancy is also considered. these two notions are linked to the concept of stereotype and to the sparse data context.
모finally  we consider the degree of similarity between the examples and their covering representatives. this corresponds to the notion of compactness within clusters  but without penalizing the stereotypes with many descriptors. the function he seems really adapted to give an account of representative relevance. in fact  we used a version of he normalized between 1 and 1  by dividing by the total number of descriptors.
1 results
fig. 1 gives the results obtained from the articles published in le matin. experiments for the k-modes algorithm were carried out with n = 1...1 clusters  but only n = 1 results are presented in this comparison. the rows of the table show the number n of extracted representatives  the two scores concerning contradiction  the result of he  the redundancy score and whether or not the cognitive cohesion constraint is verified. the columns represent each type of experiment  k-modes associated with techniques  1  to  1   em and cobweb as well  and finally our algorithm press .
모let us begin by considering the contradiction scores. they highlight a principal result of default clustering: using press  the percentage of examples having contradictory features with their representative is always equal to 1%. in contrast  the descriptions built using techniques  1  and  1   whatever the clusterer used  possess at least one contradictory descriptor with 1% to 1% of the examples belonging to the cluster. furthermore  around 1% of the descriptors of these examples are in contradiction with the covering description  and that can in no way be considered as a negligible noise. this is the reason why processes  1  and  1  must be avoided  especially in the sparse data context  when building such representatives from k-modes  em or cobweb clustering. hence  we only consider techniques  1  and  1  in the following experiments.
모let us now study the results concerning clustering quality. this quality can be expressed thanks to the compactness function he  the redundancy rate and cognitive cohesion.
모press marked the best score  1  for cluster compactness with six stereotypes. that means a very good homogeneity between the stereotypes and the examples covered. it is perfectly consistent since our algorithm tries to maximize this function. the redundant descriptors rate is equal to 1%  according to the no-redundancy constraint. furthermore  press is the only algorithm that is able to verify cognitive cohesion. em obtains the second best score and redundant descriptor rate remains acceptable. however  the number of expected classes must be given or guessed using a cross-validation technique  for instance. k-modes and cobweb come third and fourth and also have to use an external mechanism to discover the final number of clusters.
모note that the stereotypes extracted using press correspond to the political leanings of the newspaper. for instance  the main stereotype produces a radical  socialist politician  corrupted by foreign money and freemasonry  etc. it corresponds partly to the difficulty in accepting the major changes proposed by the radical party and to the fear caused in france since 1 by the theories of karl marx. we cannot explain here in more detail the semantics of discovered stereotypes  but these first results are really promising.
1 conclusion
conceptual clustering is seldom studied with such a high number of missing values. however  it is really important to be able to extract readable  understandable descriptions from such type of data in order to complete information  to classify new observations quickly and to make predictions. in this way  default clustering presented in this paper tries to provide an alternative to the usual clusterers. moreover  based on local optimization techniques  it proposes a very general easy-to-extend framework for stereotype set discovering: new movements  constraints added relative to the problematic chosen  adapted control indexes  etc. the results obtained  on both artificial data sets and a real case extracted from newspaper articles  are really promising and should lead to other historical studies concerning social stereotypes.
모for instance  we are currently applying these techniques to the study of social representations  a branch of social psychology introduced by s. moscovici  moscovici  1 . more precisely  this approachis really useful forpress content study which up to now is done manually by experts. hence  future work could be done on choosing key dates of the dreyfus affair and automatically extracting stereotypical characters from different newspapers. these results will then be compared and contrasted with the work of sociologists and historians of this period.
