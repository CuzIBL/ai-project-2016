a portfolio approach to algorithm select 
kevin leyton-brown and eugene nudelman and galen andrew and jim mcfadden and yoav shoham 
{kevinlb;eugnud;galand;jmcf;shoham} cs.stanford.edu  stanford university  stanford ca 1 

1 introduction 
although some algorithms are better than others on average  there is rarely a best algorithm for a given problem. instead  it is often the case that different algorithms perform well on different problem instances. not surprisingly  this phenomenon is most pronounced among algorithms for solving aa'p-hard problems  because runtimes for these algorithms are often highly variable from instance to instance. when algorithms exhibit high runtime variance  one is faced with the problem of deciding which algorithm to use; in 1 rice dubbed this the  algorithm selection problem  . in the nearly three decades that have followed  the issue of algorithm selection has failed to receive widespread attention  though of course some excellent work does exist. by far  the most common approach to algorithm selection has been to measure different algorithms' performance on a given problem distribution  and then to use only the algorithm having the lowest average runtime. this  winner-take-ah  approach has driven recent advances in algorithm design and refinement  but has resulted in the neglect of many algorithms that  while uncompetitive on average  may offer excellent performance on particular problem instances. our consideration of the algorithm selection literature  and our dissatisfaction with the winner-take-all approach  has led us to ask the following two questions. first  what general techniques can we use to perform per-instance  rather than per-distribution  algorithm selection  second  once we have rejected the notion of winner-take-all algorithm evaluation  how ought novel algorithms to be evaluated  we offer the following answers: 
1. algorithms with high average running times can be combined to form an algorithm portfolio with low average running time when the algorithms' easy inputs are sufficiently uncorrected. 
1. new algorithm design should focus on problems on which the current algorithm portfolio performs poorly. 
¡¡readers familiar with the boosting paradigm in machine learning  will recognize that boosting uses similar ideas: combining weak classifiers into a much stronger ensemble by iterativcly training new classifiers on data on which the ensemble performs poorly. 
1 algorithm portfolios 
in our previous work  we demonstrated that statistical regression can be used to learn surprisingly accurate algorithmspecific models of the empirical hardness of given distributions of problem instances. in short  the method proposed in that work is: 
1 

figure 1: algorithm and portfolio runtimes 

	figure 1: optimal 	figure 1: selected 
1. use domain knowledge to select features of problem instances that might be indicative of runtime. 
1. generate a set of problem instances from the given distribution  and collect runtime data for the algorithm on each instance. 
1. use regression to learn a real-valued function of the features that predicts runtime. 
¡¡given this existing technique for predicting runtime  we now propose building portfolios of multiple algorithms as follows: 
1. train a model for each algorithm  as described above. 
1. given an instance: 
 a  compute feature values 
 b  predict each algorithm's running time 
 c  run the algorithm predicted to be fastest 
1 wdp case study: past work 
our case study is based on the data collected in our previous work . in that work we have constructed models for predicting the running time of the cplex solver on the winner determination problem  wdp   which is an np-complete combinatorial optimization problem formally equivalent to weighted set-packing. we have also created models for two other wdp algorithms: gl  gonen-lehmann    a simple 
branch-and-bound algorithm with cplex's lp solver as its heuristic  and cass   a more complex branch-and-bound algorithm with a non-lp heuristic. the data set consists of 1 instances of a fixed size  1 goods and 1 nondominated bids   sampled uniformly from cats  instance generator. since our methodology relies on machine learning  we split the data into training  validation  and test sets. we report our portfolio runtimes only on the test set that was never used to train or evaluate models. runtime data was collected on 1 mhz pentium xeons  running linux 1. 
1 w d p case study: portfolios 
we now describe new results. fig. 1 compares the average runtimes of our three algorithms to that of the portfolio. note that cplex would be chosen under winner-take-all algorithm selection. the  optimal  bar shows the performance of an ideal portfolio where algorithm selection is performed perfectly and with no overhead. the portfolio bar shows the time taken to compute features  light portion  and the time taken to run the selected algorithm  dark portion . despite the fact that cass and gl are much slower than cplex on average  the portfolio outperforms cplex by more than a factor of 1. further  neglecting the cost of computing features  our portfolio's selections take only 1% longer to run than the optimal selections. figs. 1 and 1 show the frequency with which each algorithm is selected in the ideal portfolio and in our portfolio. they illustrate the quality of our algorithm selection and the relative value of the three algorithms. while our portfolio does not always make the right choice  most of the mistakes occur when algorithms have very similar running times. these mistakes are not very costly  which explains why our portfolio's choices have a running time so close to optimal. these results show that our portfolio methodology can work very well even with a small number of algorithms  and even when one algorithm has superior average performance. 
1 focused algorithm design 
once it has been decided to select algorithms from a portfolio on a per-instance basis  it is necessary to reexamine the way we design and evaluate algorithms. since the purpose of designing new algorithms is to reduce the time that it will take to solve problems  designers should aim to produce new algorithms that complement an existing portfolio given a distribution d reflecting problems that will be encountered in practice. the instances with the greatest potential for improvement will be hard for the portfolio  common in d  or both. the full version of this paper describes a technique for using rejection sampling to automatically generate such instances. in figures 1 and 1 we show how our techniques are able to automatically skew two of the easiest cats instance distributions towards harder regions. in fact  for these two distributions we generated instances that were  respectively  1 and 1 times harder than anything we had previously seen! moreover  the average runtime for the new distributions was greater than the observed maximum running time on the original distribution. 

1 conclusions 
learned runtime models may be used to create algorithm portfolios that outperform their constituent algorithms. these models can also be used to induce harder benchmark distributions for use in the development and evaluation of new algorithms. our case study on combinatorial auctions demonstrates that a portfolio composed of cplex and two olderand generally much slower-algorithms outperforms cplex alone by about a factor of 1. in the full version of this paper we describe our methodology in more detail  and also: 
  show how to reduce the time spent computing features without substantially degrading portfolio performance; 
  demonstrate ways of using response variable transfor-mations to focus portfolios on metrics other than average running time; 
  explain how to induce distributions with characteristics other than hardness  e.g. realism ; 
  compare our approach to previous work that executes algorithms in parallel ; uses classification instead of regression ; or considers the problem as an mdp . 
