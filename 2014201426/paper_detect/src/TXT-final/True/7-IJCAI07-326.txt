
two noteworthy models of planning in ai are probabilistic planning  based on mdps and its generalizations  and nondeterministic planning  mainly based on model checking . in this paper we:  1  show that probabilistic and nondeterministic planning are extremes of a rich continuum of problems that deal simultaneously with risk and  knightian  uncertainty;  1  obtain a unifying model for these problems using imprecise mdps;  1  derive a simplified bellman's principle of optimality for our model; and  1  show how to adapt and analyze state-of-art algorithms such as  l rtdp and ldfs in this unifying setup. we discuss examples and connections to various proposals for planning under  general  uncertainty.
1 introduction
planning problems can be classified  based on the effects of actions  in deterministic  probabilistic or nondeterministic. in this paper we are concerned with action dynamics under general forms of uncertainty; indeed  we are interested in planning under both risk and knightian uncertainty. we show how to use these concepts to express probabilistic and nondeterministic planning  and combinations thereof  as markov decision processes with set-valued transitions  mdpsts .
　similar generalizations of markov decision processes  mdps  have appeared before in research on artificial intelligence. for example  givan et al.  use intervals to encode a set of exact mdps  which is used to conduct space state reduction of mdps. their bounded-parameter mdps  bmdps  form neither a superset nor a subset of mdpsts. buffet and aberdeen  use bmdps to produce robust policies in probabilistic planning. they also show that real-time dynamic programming  rtdp  can be used in bmdps. our perspective is different: we wish to unify various strands of planning that have proven practical value  using a theory that has a behavioral basis on preferences and beliefs - otherwise  we do follow a similar path to buffet and aberdeen's in that we exploit rtdp in our models. another recent work that should be mentioned is perny et al.'s   where transitions must only satisfy a few algebraic properties. our models are a strict subset of their algebraic mdps  amdps ; we forgo some generality because we want to employ models with solid behavioral justification  and to exploitthe specific structureof combinedprobabilisticnondeterministic planning. even though general algorithms such as amdp-based value iteration are useful theoretically  we find that a more specific approach based on real-time dynamic programming leads to encouraging results on computational complexity.
　this necessarily brief review of closest literature should indicate the central motivation of this work: we strive to work with decision processes that have solid behavioral foundation and that can smoothly mix problems of practical significance. a similar approach has been proposed by eiter and lukasiewicz   using nonmonotoniclogic and causal semantics to define set-valued transitions in partial observable mdps  and leaving algorithms for future work . we offer a model that assumes full observability  and we obtain algorithms and complexity analysis for our model.
　the remainder of this paper is organized as follows. in section 1 we discuss how mdpsts capture the continuum of planning problems from  pure  probabilistic to  pure  nondeterministic planning. in section 1 we show that mdpsts are markov decision processes with imprecise probabilities  mdpips   a model that has received attention in operations research and that displays a solid foundation. we also comment on the various relationshipsbetween mdpsts and other models in the literature. in section 1 we show that mdpsts lead to importantsimplifications of their  minimax bellmanstyle equation  we note that such simplifications are mentioned without a proof by buffet and aberdeen  for bmdps . we obtain interesting insights concerning computational complexity of mdpsts and related models. section 1 investigates algorithms that produce minimax policies for mdpsts. although our results yield easy variants of value and policy iteration for mdpsts  we are interested in more efficient algorithms based on rtdp. in section 1 we derive the conditions that must be true for rtdp to be applied. section 1 brings a few concluding remarks.
1 background
1 varieties of planning
we start reviewing a few basic models of planning problems  attempting to unify them as much as possible as suggested by recent literature  bonet and geffner  1 :
m1 a discrete and finite state space s 
m1 a nonempty set of initial states s1   s 
m1 a goal given by a set sg   s 
m1 a nonempty set of actions a s    a representing the actions applicable in each state s 
m1 a state transition function f s a    s mapping state s and action a （ a s  into nonempty sets of states  i.e.
|f s a | − 1  and
m1 a positive cost c s a  for taking a （ a s  in s.
adapting m1  m1 and m1  one can produce:
  deterministic models  det   where the state transition function is deterministic: |f s a | = 1. in  classical  planning  the following constraints are added:  i  |s1| = ; and  iii   s （ s a （ a s :c s a  = 1.
  nondeterministic models  nondet   where the actions may result in more than one successor state without preferences among them.
  probabilistic models  mdps   where actions have probabilistic consequences. not only the function |f s a | − 1 is given  but also the model includes:  mdp1  a probability distribution p1 ，  over s1; and  mdp1  a probability distribution p ，|s a  over f s a  for all s （ s a （ a s .
　for any of these models  we expect that a solution  e.g. a policy  is evaluated on its long-term costs. the cost of a solution can be evaluated in a finite-horizon  in which the maximum number of actions to be executed is limited to k （ r+. an alternative is to consider discounted infinite-horizon  in which the number of actions is not bounded and the cost of actions is discounted geometrically using a discount factor 1   γ   1. since it is difficult to find an appropriate k for each problem  in this paper we assume the discounted infinite-horizon framework.1
　due to the assumption of full observability and discounted infinite-horizon cost  a valid solution is a stationary policy  that is  a function π mapping states s （ s into actions a （ a s . bellman's principle of optimality defines the optimal cost function v   s  = mina（a s  qv   s a   bellman  1   where:
 for det   for nondet  and
qv  s a = 	s （f s a 	 1  for mdps.
:
 this principle characterizes v    also called optimal value function  and induces the optimal policy for each model: π  s  = argmina（a s  qv   s a . the definition of qv  s a  clarifies the guarantees of each model. in det  guarantees given by π  do not depend on its execution; in nondet guarantees are on the worst-case cost; and in
mdps guarantees are on expected cost. there are algorithms that compute the optimal policies for each one of these models  and algorithms that can be specialized to all of them  bonet and geffner  1 . however  we should emphasize that previous unifying frameworks do not intend to handle smooth  mixtures  of these planning problems. in fact  one of our goals in this paper is to provide a framework where nondet and mdps are the extreme points of a continuum of planning problems.
1 varieties of uncertainty
probability theory is often based on decision theory  berger  1   a most appropriate scheme in the realm of planning. thus a decision maker contemplates a set of actions  each one of which yields different rewards in different states of nature. complete preferences over actions imply that a precise probability value is associated with each state - a situation of risk  knight  1; luce and raiffa  1 . an obvious example of sequential decision making under  pure  risk is probabilistic planning. however  often preferences over actions are only partially ordered  due to incompleteness in beliefs  or lack of time/resources  or because several experts disagree   and then it is not possible to guarantee that precise probabilities represent beliefs. in those cases  a set of probability measures is the adequate representation for uncertainty; such sets are often referred to as credal sets  levi  1; kadane et al.  1; walley  1 . even though terminology is not stable  this situation is said to contain knightian uncertainty  other terms are ambiguity or simply uncertainty . an extreme case is nondeterministic planning  where no probabilities are specified.1
　note that actual decision making is rarely restricted to either  pure  risk nor  pure  knightian uncertainty; in fact the most realistic scenario mixes elements of both. not surprisingly  such combinations are well studied in economics  psychology  statistics  and philosophy. we note that credal sets have raised steady interested in connection with artificial intelligence  for example in the theory of probabilistic logic  nilsson  1   in dempster-shafer theory  shafer  1   in theories of argumentation  anrig et al.  1   and in generalizations of bayesian networks  cozman  1; fagiuoli and zaffalon  1 .
　the usual prescription for decision making under risk is to select an action that maximizes expected utility. in the presence of knightian uncertainty  matters become more complex  as now a decision maker carries a set of probability measures and consequently every action is associated with an interval of expected costs  walley  1 . thus a decision maker may choose one of several criteria  such as minimaxity  maximality  e-admissibility  troffaes  1 . in this paper we follow a minimax approach  as we are interested in actions that minimize the maximum possible expected cost; we leave other criteria for future work.

figure 1: an mdpst representing the example 1. dotted lines indicate each one of the reachable sets. cost of taking actions d1  d1 and ht in the example 1. states with  -  indicates the action is not applicable. action noop represents the persistenceaction for the absorbing states.
1 markov decision processes with set-valued transitions
in this section we develop our promised synthesis of probabilistic and nondeterministic planning. we focus on the transition function; that is  on m1. instead of taking f s a    s  we now have a set-valued f s a    1s  ; that is  f s a  maps each state s and action a （ a s  into a set of nonempty subsets of s. we refer to each set k （ f s a  as a reachable set. a transition from state s given action a is now associated with a probability p k|s a ; note that there is knightian uncertainty concerning for each successor state  k. we refer to the resulting model as a markov decision process with set-valued transitions  mdpsts : transitions move probabilistically to reachable sets  and the probability for a particular state is not resolved by the model. in fact  there is a close connection between probabilities over f s a  and the mass assignments that are associated with the theory of capacities of infinite order  shafer  1 ; to avoid confusion between p k|s a  and  we refer to the former as mass assignments and denote them by m k|s a .
thus an mdpst is given by m1  m1  m1  m1  m1  mdp1 
mdpst1 a state transition function f s a    1s   mapping states s and actions a （ a s  into reachable sets of s  and
mdpst1 mass assignments m k|s a  for all s  a （ a s   and k （ f s a .
　there are clearly two varieties of uncertainty in a mdpst: a probabilistic selection of a reachable set and a nondeterministic choice of a successor state from the reachable set. another important feature of mdpsts is that they encompass models discussed in section 1:
  det: there is always a single successor state:  s （ s a （ a s  : |f s a | = 1 and  s （ s a （ a s  k （ f s a  : |k| = 1.
  nondet: there is always a single reachable set  but selection within this set is left unspecified  nondeterministic :  s （ s a （ a s  : |f s a | = 1  and  s （ s a （ a s  k （ f s a  : |k|   1.
  mdps: selection of k （ f s a  is probabilistic and it resolves all uncertainty:  s （ s a （ a s  : |f s a |   1  and  s （ s a （ a s  k （ f s a  : |k| = 1.
example 1 a hospital offers three experimental treatments to cardiac patients: drug d1  drug d1 and heart transplant  ht . state s1 indicates patient with cardiopathy. the effects of those procedures lead to other states: severe cardiopathy  s1   unrecoverable cardiopathy  s1   cardiopathy with sequels  s1   controlled cardiopathy s1   stroke  s1   and death  s1 . there is little understandingabout drugsd1 and d1  and considerable data on heart transplants. consequently  there is  partial  nondeterminism  that is  there is knightian uncertainty  in the way some of the actions operate. figure 1 depicts transitions for all actions  indicating also the mass assignments and the costs. for heart transplant  we suppose that all transitions are purely probabilistic.
1 mdpsts  mdpips and bmdps
in this section we comment on the relationship between mdpsts and two existing models in the literature: markov decision processes with imprecise probabilities  mdpips   white iii and eldeib  1; satia and lave jr  1  and bounded-parameter markov decision processes  bmdps   givan et al.  1 .
　an mdpip is a markov decision process where transitions are specified through sets of probability measures; that is  the effects of an action are modelled by a credal set k over the state space. an mdpip is given by m1  m1 m1  m1  m1  mdp1 and
mdpip1 a nonempty credal set ks a  for all s （ s and a （ a s   representing probability distributions  over successor states in s.
in this paper we assume that a decision maker seeks a minimax policy  that is  she selects a policy that minimizes the maximum cost across all possible probability distributions . this adopts an implicit assumption that probabilities are selected in an adversarial manner; other interpretations for mdpips are possible  troffaes  1 . under the minimax interpretation  the bellman principle of optimality is  satia and lave jr  1 :
;
 1 
moreover  this equation always has a unique solution that yields the optimal stationary policy for the mdpip. to investigate the relationship between mdpsts and mdpips  the following notation is useful: when k （ f s a   we denote by

figure 1: this figure illustrates two examples of planning under uncertainty modeled through mdpsts and bmdps. example 1 is the heart example from perny et al.  1 . example 1 is a simple example in which none of the models can express the problem modelled by the other one.
d k s a  the set of states such that.
thus d k s a  represents all nondeterministic effects of k that belong only to k. we now have:
proposition 1 any mdpst
is expressible by an mdpip.
proof  detailed in  trevizan et al.  1   it is enough to prove that  s （ s a （ a s   f s a   mdpst1  and m s a   mdpst1  imply ks a   mdpip1 . first  note that mdpst1 bounds for allthe probability of being in state s after applying action a in state s as follows

 to see that  use the definition of reachable sets: let k （
f  then it is not possible to select s as a
nondeterministic effect of a. 
　from mdpst1 and mdpst1 it is possible to bound the sum of the probabilitiesof each state in a reachableset k （ f s a  and in the associated set d k s a :

the set of inequalities  1  and  1  for s （ s and a （ a s  describe a possible credal set ks a  for mdpip1.  
definition 1 the mdpip q obtained through proposition 1 is called the associated mdpip of p.
as noted in section 1  bmdps are related to mdpsts. in-
tuitively  bmdps are markov decision processes where transition probabilities and rewards are specified by intervals  givan et al.  1 . thus bmdps are not comparable to mdpips due to possible imprecision in rewards; here we only consider those bmdps that have real-valued rewards. clearly these bmdps form a strict subset of mdpips. the relationship between such bmdps and mdpsts is more complex. figure 1.a and 1.b presents an mdpst and an bmdp that are equivalent  that is  they represent the same mdpip . figure 1.c and 1.d presents an mdpst that cannot be expressed

figure 1: relationships between models  bmdps with precise rewards .
as an bmdp  and an bmdp that cannot be expressed as an mdpst. as a technical aside  we note that the f s a  define choquet capacities of infinite order  while transitions in bmdps define choquet capacities of second order  walley  1 ; clearly they do not have the same representational power.
　the results of this section are captured by figure 1. in the next two sections we present our main results  where we explore properties of mdpsts that make these models rather amenable to practical use.
1 a simplified bellman equation for mdpsts
we now present a substantial simplification of the bellman principle for mdpsts. the intuition behind the following result is this. in equation  1   both minima and maxima are taken with respect to all combinations of actions and possible probability distributions. however  it is possible to  pull  the maximum inside the summation  so that less combinations need be considered.
theorem 1 for any mdpst and its associated mdpip   1  is equivalent to:

proof define and as a shorthand for the values obtained through  respectively   1  and  1 . we want to prove that for all mdpst  its associated mdpip  and
 s （ s  vip   s  = vst   s . due to the proposition 1  we have that the probability measure induced by  for k （ f s a  in  1  is a valid choice according to ks a   therefore. now  it is enough to show that  s （ s vst   s  ＋ vip   s  to conclude this proof.
　for all s  （ s  we denote by fs  s a  the set of reachable sets {k （ f s a | s  = argmax. the proof of proceeds by contradiction as follows. for all s （ s and all a （ a s   let p ，|s a  be the probability measure chosen by the operator max in vip   s  and suppose that vst   s    vip   s . therefore  there is a s （ s such that
p s|s a ; as p ，|s a  is a probability measure  there is also a  and
vip   s . now  let p ，|s a  be a probability measure defined

that   a contradiction by the definition of p ，|s a . thus  the rest of this proof shows that satisfies proposition 1.
　due to the definition of p ，|s a   we have that the left side and the right side of  1  are trivially satisfied  respectively  by
. to treat the other case for both s and
s  it is sufficient to define  as follows:
.
 using this definition  we have that  by hypothesis; since  1  gives a lower and an upper bound to the sum of p ，|s a  over  respectively  k （ f s a  and d k s a    k.
if  then nothing changes and these bounds remain valid. there is one more case for each bound that its satisfaction is trivial too:
 i  for the upper bound when; and  ii  for the lower bound when . a nontrivial case is for the lower bound in  1  when 
bound still holds because	 by hy-
pothesis  and 

  using proposition
　the last remaining case  to prove that equation  1  is true for p ，|s a   happens when s （ d k s a  and 
d k s a  for the upper bound in  1 .	this case is valid because there is no  such that 
 by the definition of reachable set  thus p s|s a  ＋  by hypothesis. if there is not a  s.t.  this upper bound still holds  else  choosing  will validate all the bounds. since p ，|s a  respects proposition 1  we get a contradiction because 
 but by hypothesis p ，|s a 	= argmax.	therefore 
  what completes the proof.	 
　an immediate consequence of theorem 1 is a decrease of the worst case complexity order of mdpips algorithms used for solving mdpsts. consider first one iteration of the bellman principle of optimality for each s （ s  one round  using equations  1 . define an upper bound of |f s a | for all s （ s and a （ a s  of an mdpst instance by

f = maxs（s{maxa（a s  |f s a |} ＋ 1|s|. in the mdpip obtained through proposition 1  computation of v   s  consists of solving a linear program induced by the max operator on  1 . because this linear program has |s| variables and

its description is proportional to f  the worst case complexity of one round is   for p − 1 and q − 1.
the value of p and q is related to the algorithm used to solve this linear program  for instance  using the interior point algorithm  kojima et al.  1  leads to p = 1 and q = 1  and the karmarkar's algorithm  karmarkar  1  leads to p to 1 and q to 1 .
however  the worst case complexity for one round using
equation  1  is. this is true because the probability measure that maximizes the right side of equation
 1  is represented by the choice  in equation  1   avoiding the cost of a linear program. in the special case of an mdp modelled as an mdpst  i.e.  s （ s a （ a s   |f s a | ＋ |s| and  k （ f s a   |k| = 1  this worst case complexity is o |s|1|a|   the same for one round using the bellman principle for mdps  papadimitriou  1 .
1 algorithms for mdpsts
due to proposition 1  every algorithm that finds the optimal policy for mdpips can be directly applied to mdpsts. instances of algorithms for mdpip are: value iteration  policy iteration  satia and lave jr  1   modified policy iteration  white iii and eldeib  1   and the algorithm to find all optimal policies presented in  harmanec  1 . however  a better approach is to use theorem 1. this proposition gives a clear path on how to adapt algorithms from the realm of mdps - algorithms such as  l rtdp  bonet and geffner  1  and ldfs  bonet and geffner  1 . these algorithms are defined for stochastic shortest path problems  ssps   bertsekas  1   ssps are a special case of mdps  in which there is only one initial state  m1  and the set of goal states is nonempty  m1  . to find an optimal policy  an additional assumption is required: the goal must be reachable fromeverystate with nonzeroprobability the reachabilityassumption . for mdpsts  this assumption can be generalized by requirement that the goal be reachable from every state with nonzero probability for all probability measures in the model. the following proposition gives a sufficient  however not necessary  condition to prove the reachability assumption for mdpsts.
proposition 1 if  for all s （ s  there exists a （ a s  such that  for all  then it is sufficient to prove that the reachability assumption is valid using at least one probability measure for each s （ s and a （ a.
proof if the reachability assumption is true for a specific sequence of probability measures  then there exists a policy π and a history h  i.e. sequence of visited states and executed actions  induced by π and p such that is minimum and.
since si+1 can always be reached  because there exists an action a （ a si  such that p si+1|si a    1  then  for any sequence of probability measures in the model  every history h induced by π contains h  i.e.  reaches sn （ sg.  
example 1 consider the planning problem in the example 1 and the cost of actions in figure 1. we have obtained the following optimal policy for this mdpst:
s1s1s1s1s1s1s1d1d1htnoopnoopnoopnoopπ  =
1 conclusion
in this paperwe have examinedapproachesto planning across many dimensions: determinism  nondeterminism  risk  uncertainty. we would like to suggest that markov decision processes with set-valued transitions represent a remarkable entry in this space of problems. mdpsts are quite general  as they not only capture the main existing planning models of practical interest  but also they can represent mixtures of these models - we have emphasized throughout the paper that mdpsts allow one to combine nondeterminism of actions with probabilistic effects. it is particularly important to note that mdpsts specialize rather smoothly to det  nondet or mdp; if an mdpst belongs to one of these cases  its solution inherits the complexity of the special case at hand. such a  smooth  transition to special cases does not obtain if one takes the larger class of mdpips; the general algorithms require one to perform bilevel programming  linear programs are nested  as one linear program is needed to compute the value  and do not treat efficiently the special cases.
　in fact  mdpsts are remarkable not only because they are rather general  but because they are not overly general - they are sufficiently constrainedthat they display excellentcomputational properties. consider the computation of an iteration of the bellman equation for a state s  a round . this is an essential step both in versions of value and policy iteration and in more sophisticated algorithms such as  suitably adapted  rtdp. as discussed in section 1  rounds in mdpsts have much lower complexity than rounds in general mdpips - in essence  the simplification is the replacement of a linear program by a fractional knapsack problem.
　finally  we would like to emphasize that mdpsts inherit the pleasant conceptual aspects of mdpips. they are based on solid decision theoretic principles that attempt to represent  as realistically as possible  risk and knightian uncertainty. we feel that we have only scratched the surface of this space of problems; much remains to be done both on theoretical and practical fronts.
acknowledgements
we thank fapesp  grant 1-1  and cnpq  grants 1-1  1-1  1-1  for financial support and the four anonymous reviewers for the suggestions and comments.
