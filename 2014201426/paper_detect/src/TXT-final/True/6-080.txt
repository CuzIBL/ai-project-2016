 
we extend multiclass svm to multiple prototypes per class. for this framework  we give a compact constrained quadratic problem and we suggest an efficient algorithm for its optimization that guarantees a local minimum of the objective function. an annealed process is also proposed that helps to escape from local minima. finally  we report experiments where the performance obtained using linear models is almost comparable to that obtained by state-of-art kernel-based methods but with a significant reduction  of one or two orders  in response time. 
1 introduction 
automatic multiclass classification  i.e. the process to automatically assign exactly one from a prefixed set of labels to a stream of input instances  is a central task for many real world problems like speech recognition  ocr  text categorization etc. many supervised learning methods have been studied that help on these tasks. recently  kernel-based methods  like svm  have been well studied especially for binary settings  and they yielded state-of-art performance in many different tasks. svm searches for a high margin linear discriminant model in a very high dimensional space  feature space  where examples are implicitly mapped via a function . since kernel-based algorithms need only of dot products in feature space  it is possible to resort to the so called kernel trick if these dot products can be computed efficiently with a kernel function k x  y  =  . when the number of examples  or support vectors  is large  these approaches tend to be less efficient w.r.t. the time spent for classifying new vectors since they require to work with the implicit characterization of the discriminant model. 
　one of the most effective svm extension able to deal with general multiclass problems has been recently provided  crammer and singer  1  and it has shown very good results. in this paper  we extend the multiclass svm to the multi-prototype setting where for each class there can be more than one prototype vector. this allowed us to get very expressive decision functions by using simple models without necessarily requiring the use of kernels. 
alessandro sperduti 
dept. of pure and applied mathematics 
university of padova  italy sperduti math.unipd.it 
1 	preliminaries and notation 
　　　　　　　　　　　　be a set of n training examp	l	e	s	 	a 	single-label multi-class classifier is a f u n c t i o n t h a t maps instances 	to labels 	we focus on the class of winnemake-all linear classifiers with form: 
		 i  
where mr is the r-th prototype vector  r is the set of prototype indexes and c r  the function returning the class associated to the prototype indexed r. given a classifier hm x  and a training example i we will say that misclassifies 
　in the following  we denote by the constant that is equal to 1 i f o t h e r w i s e . for a given example x1   we denote by the set of  positive  prototypes indexes and by '  the set of  negative  prototypes indexes. finally  the real value 
   x is referred to as similarity score  or simply score  of the r-th prototype on the instance x. 
1 single-prototype multi-class svm 
an effective multi-class extension to svm has been already proposed in  crammer and singer  1 . the resulting classifier is of the same form of  1  where each class has associated exactly one prototype  i.e. 
the solution is obtained through the minimization of a convex quadratic constrained problem. in this section  we present a simpler derivation of an equivalent formulation that will be extended in the next section to the multi-prototype framework. 
　in the multiclass setting  in order to have a correct classification of the instance  the prototype of the correct class should have a score that is greater than the maximum among the scores of the prototypes associated to incorrect classes. we can formally write the constraints for a correct classification of the example  with a margin greater or equal to 1 by requiring: 

where yi; = d is the index of the prototype vector associated to the correct class for the example x . to allow for margin 

learning 	1 

violations  for each example  we introduce soft margin slack variables 

where denotes the hinge loss equal to x if x   1 and 1 otherwise. notice that the value is an upper bound to the 1 - 1 loss for the example xi  and consequently its average value over the training set will be an upper bound for the empirical error. 
　motivated by the structural risk minimization  srm  principle in  vapnik  1   we want to search for a set of prototype vectors m = {mi ..  m c } with small norm such to minimize the empirical error on the training set. for this  we can formulate the problem in a svm-style by requiring a set of prototypes of minimal norm that fulfill the soft constraints given by the classification requirements. thus  the single-prototype multiclass svm  sprotsvm  will result in: 
 1  
notice that  as desired  the optimal solution for will be the maximum value among the negative scores for the instance xi. this problem is convex and it can be solved in the standard way by resorting to the optimization of the wolfe dual problem. in this case  the lagrangian is: 
lagrangian with respect to the primal variables and imposing the optimality conditions we obtain the set of constraints  kkt conditions  that the variables must fulfill in order to be an optimal solution: 

 1  
　by using the fact  and substituting conditions  1  in  1  and omitting constants that do not change the solution  the problem can be restated as: 
		 1  
　the next section includes an efficient optimization procedure for the more general multi-prototype setting that includes the single-prototype case as a particular instance. 
1 multi-prototype multi-class svm 
in this section  the sprotsvm model previously defined will be extended to the case of multiple prototypes per class. the basic idea is that  given multiple prototype vectors  we have a correct classification iff at least one of the prototypes associated to the correct class has a score higher than the maximum of the scores of the prototypes associated to incorrect classes. 
　in this case  we write the constraints for a correct classification of the example xi with a margin greater or equal to 1 requiring: 

　to allow for margin violations  for each example xi  we introduce soft margin slack variables  1  one for each positive prototype  s.t. 

given a pattern xi  we arrange the soft margin slack variables f j* in a vector let us now introduce  for each example xi a new vector   whose components are all zero except one component that is 1. in the following  we refer to as the assignment of the pattern xt. notice that the dot product is always an upper bound to the 1 loss for the example x; independently from its assignment. 
　now  we are ready to formulate the multi-prototype problem by requiring a set of prototypes of small norm and the best assignment for the examples able to fulfill the soft constraints given by the classification requirements. 
thus  the mprotsvm formulation will result in: 
 1  
unfortunately  this is a mixed integer problem that is not convex and it is a difficult problem in general but  as we will see in the following  it is prone to an efficient optimization procedure that approximates the global optimum. at this point  it is worth noticing that  since the effective formulation is itself an  heuristic  approximation to the structural risk minimization principle  a good solution  although not optimal  can anyway give good results. as we see after  this claim is confirmed by the results obtained in our experimental work. 
　let suppose now that the assignments are kept fixed. in this case the reduced problem becomes convex and can be solved as above by resorting to the optimization of the wolfe dual problem. in this case  the lagrangian is: 

tiating the lagrangian of the reduced problem and imposing 

1 	learning 

if the values of a1p and aft  after being updated  would turn 

　it can be trivially shown that this formulation is consistent with the formulation for sprotsvm's given above. 
1 	optimization with static assignments 
when patterns are statically assigned to the prototypes via constant vectors  the convexity of the associated mprotsvm problem implies that the optimal solution for the primal problem in  1  can be found through the maximization of the lagrangian in  1 . 
　assuming an equal number q of prototypes per class  the dual involves n x m x q variables which lead to a very large scale problem. anyway  the independence of constraints among the different patterns allows for the separation of the variables in n disjoint sets of m x q variables. 
　the algorithm we propose for the optimization of the problem in  1  is inspired by the ones already presented in  crammer and singer  1; aiolli and sperduti  1a  and consists in iteratively selecting patterns from the training set and optimizing with respect to the variables associated to that pattern. from the convexity  each iteration leads to an increase of the lagrangian until no more improvement is possible since the optimal solution for the lagrangian has been found. after selecting a pattern xi  to enforce the constraint two elements from the set will be optimized in pair 
while keeping the solution inside the feasible region. in particular  let wi and w1 be the two selected variables  we restrict the updates to the form with 
optimal choices for p. iterating the application of this basic step over different pairs and then doing the same for different patterns will guarantee to reach the optimum for the overall problem. 
　let now compute the optimal value for p. first of all  we observe that the update will affect the squared norm of the prototype vector mr of an amount 

　now  we show how to analytically solve the associated problem with respect to an update involving a single variable and the variable since does not influence 
learning 
out to be not feasible for the constraints arp   1 and aft   1  we select the value for p such to fulfill the violated constraint bounds at the limit. 
　now  we show how to analytically solve the associated problem with respect to an update involving a pair of variables a 1  ap1 such that r   r1 e ni and =   r-i. since  in this case  the update must have zero sum  we have: 
similarly to the previous case  if the values of the arp1  after being updated  would turn out to be not feasible for the constraints  we select the value for p such to fulfill the violated constraint bounds at the limit. 
　iterating multiple times the basic step described above on pairs of variables chosen among that associated to a given pattern it is guaranteed to find the optimality condition for the pattern. this has been exploited in  aiolli and sperduti  1a  to devise an incremental algorithm that uses the method on the reduced problem of a single example and then iterates over different examples. this optimization procedure can be considered incremental in the sense that the solution previously found for a given pattern forms the initial condition when the pattern is selected again for optimization. in this version  the basic step of optimization of the reduced problem can require the optimization over all the  pairs of variables not 
constrained to 1 associated with the selected pattern. thus the complexity of the optimization of the reduced problem is where / is the number of iterations. 
　now  we perform a further step by giving an algorithm that at each iteration has a complexity o m x q . for this  we observe that for the variables  associated to the pattern 
1 
xp to be optimal  the feasible analytic solution p must be 1 for each pair. 
　in particular  for the first case above  in order to be able to apply the step  it is required that  one of the following two conditions are verified  i.e.: 
while for the second case we must have: 
　notice that these facts can be checked in linear time. it is easy to show that the solution obtained at a certain step can be improved iff one of these facts are verified. this suggests an efficient procedure  shown in figure l top   that tries to greedily fulfill the previous condition of optimality. 
1 optimization of general mprotsvm 
in this section  we present an efficient procedure that guarantees to reach a local minimum of the objective function of the problem in  1  associated to mprotsvm. this procedure will try to optimize also with respect to the assignments 
　let suppose to start with a given assignment i for the patterns. in this case  as we have already seen  the associated problem is convex and can be efficiently solved by using the algorithm given in section 1. once that the optimal value for the primal  has been reached  we observe that the solution can be further improved by updating the assignments in such a way to assign each pattern to a positive prototype with minimal slack value  i.e. by setting the vector  1  such to have the unique 1 corresponding to the best performing positive prototype. however  with this new assignment 1r 1   the variables might not fulfill the second kkt condition in eq.  1  anymore. if it is the case  it simply means that the current solution  is not optimal for the new assignment. thus  a lagrangian optimization done by satisfying constraints dictated by kkt conditions for the new assignment is guaranteed to obtain a new with a better optimal primal value . for the optimization algorithm to succeed  however  the kkt conditions on the must be restored in order to return back to a feasible solution and then finally resuming the lagrangian optimization with the new assignment 1r 1 . we restore the kkt conditions by setting whenever there exists such that 
　performing the same procedure over different assignments  each one obtained from the previous one by the procedure just mentioned  implies the convergence of the algorithm to a local solution for the primal problem when no improvements are possible and the kkt conditions are all fulfilled by the current solution. this is due to the fact that every step induces an improvement on the primal value. 
　the first problem with this procedure is that it results onerous when dealing with many prototypes since we must perform many lagrangian optimizations. for this  we observe that for the procedure to work  at each step  it is sufficient to stop the optimization of the lagrangian when we find a value figure 1: top: algorithm for the optimization of the variables associated with a given pattern xp and a tolerance e. bottom: algorithm for the optimization of mprotsvm. 

1 	learning 

for the primal better than the last found and this is going to happen for sure since the last solution has been found not optimal. this requires only a periodic check of the primal value when optimizing the lagrangian. 
　the second and more stringent problem is that the procedure will lead to a local minimum that can be very far from the best possible. for this  we suggest to update the assignments on the basis of a stochastic annealed function instead of the hard decision function described above. 
　specifically  let us view the value of the primal as an energy function 

let suppose to have a pattern xi having slack variables  and suppose that the probability for the assignment to be in the state s  i.e. with the 1-th component set to 1  follows the law 
i 
where t is the temperature of the system and the variation of the system energy when the pattern xi 
is assigned to the s-th prototype. by multiplying every term by the normalization term exp and considering that probabilities over alternative 
　thus  when assigning the pattern xi  each positive prototype s will be selected with probability pi{s . notice that  when the temperature of the system is low  the probability for a pattern to be assigned to a prototype different from the one having minimal slack value tends to 1 and we obtain a behavior similar to the not annealed version of the algorithm. the simulated annealing is typically implemented by decreasing the temperature as the number of iterations increases by a monotonic decreasing function t = t t t1 . the full algorithm is depicted in figure 1  bottom . 
1 experimental results 
we tested our model against three datasets that we briefly describe in the following: 
n1st: it consists of a 1-class task of 1 digits randomly taken from the nist-1 dataset. the training set consists of 1 randomly chosen digits  while the remaining 1 digits are used in the test set. 
usps: it consists of a 1-class ocr task  digits from 1 to 1  whose input are the pixels of a scaled digit image. there are 1 training examples and 1 test examples. 
letter: it consists of a 1-class task  alphabetic letters a-z . inputs are measures of the printed font glyph. the first 1 examples are used for training and the last 1 for testing. 
　even if any kernel function can be in principle used  for the following preliminary experiments  we used the linear kernel k x  y  =  x - y + 1 . this allowed us to write an optimized code for training and classification that works directly with the explicit  compact  version of the model m. 
　since we are primarily interested into the evaluation of mprotsvm w.r.t. sprotsvm  for each dataset  we performed validation on the parameter for the sprotsvm model and we re-used the obtained value for training every mprotsvm generated for the same dataset. this approach seems us anyway a pessimistic estimate of the performance of mprotsvm. 
　the annealing process required by mprotsvm has been implemented by decreasing the temperature of the system with the exponential law: 

where 1   r   1 and t1   1 are external parameters. we used to = 1 for all the following experiments. 

table 1: comparison of generalization performances between lvq and mprotsvm increasing the number of codebooks/prototypes on the nist dataset 

table 1:  a  test error of mprotsvm on the usps dataset 
  with an increasing 
number of prototypes;  b  test error of mprotsvm on the letter dataset  with an increasing number of prototypes. 
　a set of experiments have been performed to compare the generalization performance of our  linear  model versus lvq  kohonen et al.  1  which seemed to us the most comparable model. for this  we have reported the best results that have been obtained by lvq on the nist dataset. specifically  they have been obtained with the lvq1 version of the algorithm  see  sona and sperduti  1  . as it is possible to see in table 1  mprotsvm performs significantly better when few prototypes per class are used  while the difference gets lower when the number of prototypes per class increases. this can be due to the more effective control of the margin for svm w.r.t. lvq models. on the same dataset  the tangentdistance based tvq algorithm  aiolli and sperduti  1b  has obtained the best result  a remarkable 1% test error  and polynomial svm's have obtained a 1% test error. in 

learning 	1 


table 1: primal values and generalization error obtained with different configurations varying the parameter r  usps dataset . 

addition  we tested the mprotsvm on the uci irvine usps and letter datasets. as it is possible to see in table 1  by combining a reasonably high number of linear prototypes  we have obtained performances almost comparable with the ones obtained in literature by using non-linear models. in fact  on the usps dataset  we have been able to get a 1% error  using a sprotsvm with polynomial kernel of degree 1 and without preprocessing the data  while for letter  we refer to the 1% obtained in  crammer and singer  1  by sprotsvm with exponential kernel. although obtained with a slightly different split of the letter dataset  1 examples for training and 1 for test   we would like to mention the results reported in  michie et al.  1  where lvq and k-nn yielded a 1% and 1% error  respectively. 
　notice that mprotsvm returns far more compact models with respect to state of the art non-linear kernel methods  thus allowing a  one or two order  reduced response time in classification. defining a sort of model complexity factor  = 1 x  m x q /n  i.e. the number of prototypes produced as a fraction of the cardinality of the training set  the above experiments have shown very low value for  e.g. 1 x 1 prototypes in the letter dataset gives = 1% and 1 x 1 prototypes for usps gives = 1% . notice that  can be directly compared with the fraction of support vectors in kernel machines. thus  mprotsvms also give us a way to decide  before training  the complexity of the model. 
   finally  in table 1 we have reported the values of the objective function of the primal problem in  1  along with their corresponding test errors obtained with the usps dataset using different configurations and lowering the parameter r. as expected  fixed a raw in the table  better values for the primal can be obtained with lower values of r. moreover  as the number of prototypes per class increases  the choice of small r tends to be more crucial. anyway  higher values for r  and thus not optimal values for the primal  can neverthless lead to good generalization performances. this can be due to the fact that the primal value is just a way to approximate the theorethical srm principle. 
1 conclusion 
we have proposed an extension of multiclass svm able to deal with several prototypes per class. this extension defines a non-convex problem. we suggested to solve this problem by using a novel efficient optimization procedure within an annealing framework where the energy function corresponds to the primal of the problem. experimental results on some popular benchmarks demonstrated that it is possible to reach very competitive performances by using few linear models per class instead of a single model per class with kernel. this allows the user to get very compact models which are very fast in classifying new patterns. thus  according to the computational constraints  the user may decide how to balance the trade-off between better accuracy and speed of classification. finally  it should be noted that the proposed approach compares favorably versus lvq  a learning procedure that  similarly to the proposed approach  returns a set of linear models 
per class. 
