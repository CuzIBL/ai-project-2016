 
this paper describes gib  the first bridgeplaying program to approach the level of a human expert.   g i b finished twelfth in a handpicked field of thirty-four experts at an invitational event at the 1 world bridge championships.  we give a basic overview of the algorithms used  describe their strengths and weaknesses  and present the results of experiments comparing gib to both human opponents and other programs. 
1 	introduction 
of all the classic games of mental skill  only card games and go have yet to see the appearance of serious computer challengers. in go  this appears to be because the game is fundamentally one of pattern recognition as opposed to search; the brute-force techniques that have been so successful in the development of chess-playing programs have failed almost utterly to deal with go's huge branching factor. indeed  the arguably strongest go program in the world  handtalk  was beaten by 1dan janice kim  winner of the 1 fuji women's championship  in the 1 hall of champions after kim had given the program a monumental 1 stone handicap. 
　card games appear to be different. perhaps because they are games of imperfect information  or perhaps for other reasons  existing poker and bridge programs are extremely weak. world poker champion howard lederer  texas hold'em  1  has said that he would expect to beat any existing poker program after five minutes1 play.*1 perennial world bridge champion bob hamman  six-time winner of the bermuda bowl  once summarized all of the commercial bridge programs by saying that  
 they would have to improve to be hopeless. + 
　in poker  there is reason for optimism: the gala system  koller and pfeffer  1   if applicable  promises to produce a computer player of unprecedented strength by 
   1many of the citations here are the results of personal communications. such communications are indicated simply by the presence of a + in the accompanying text. 
1 	computer game playing 
reducing the poker  problem  to a large linear optimization problem which is then solved to generate a strategy that is nearly optimal in a game theoretic sense. schaeffer  author of the world champion checkers program c h i n o o k  schaeffer  1   is also reporting significant success in this domain  billings et al.  1 . 
　the situation in bridge has been bleaker. in addition  because the american contract bridge league  acbl  does not rank the bulk of its players in meaningful ways  it is difficult to compare the strengths of competing programs or players. 
　in general  performance at bridge is measured by playing the same deal twice or more  with the cards held by one pair of players being given to another pair during the replay and the results then being compared.1 a  team  in a bridge match thus typically consists of two pairs  with one pair playing the north/south  n/s  cards at one table and the other pair playing the e/w cards at the other table. the results obtained by the two pairs are added; if the sum is positive  the team wins this particular deal and if negative  they lose it. 
　in general  the numeric sum of the results obtained by the two pairs is converted to international match points  or imps. the purpose of the conversion is to diminish the impact of single deals on the total  lest an abnormal result on one particular deal have an unduly large impact on the result of an entire match. 
　jeff goldsmith+ reports that the standard deviation on a single deal in bridge is about 1 imps  so that if two roughly equal pairs were to play the deal  it would not be surprising if one team beat the other by about this amount. it also appears that the difference between an average club player and a world class expert is about 1 imps  per deal played . the strongest bridge playing programs thus far appear to be slightly weaker than average club players. 
　progress in computer bridge has been slow. a recent incorporation of planning techniques into bridge baron  
1
　　space restrictions prevent my describing the rules of bridge. descriptions can be found in other ai papers dealing with bridge  and there are many excellent texts available  sheinwold  1 . articles on chess-playing programs never describe the rules; hopefully bridge will be treated similarly as it becomes a more regular topic for ai research. 
　
for example  appears to have led to a performance increment of approximately 1 imp per deal  smith et al.  1 . this modest improvement still leaves bridge baron far shy of expert-level  or even good amateurlevel  performance. 
　existing programs have attempted to duplicate human bridge-playing methodology in that their goal has been to recognize the class into which any particular deal falls: finesse  end play  squeeze  etc. smith et.al.'s work uses planning to extend this approach  but the plans continue to be constructed from human bridge techniques. in retrospect  perhaps we should have expected this approach to have limited success; certainly chess-playing programs that have attempted to mimic human methodology  such as paradise  wilkins  1   have fared poorly. 
　g i b works differently. instead of modeling its play on techniques used by humans  gib uses brute-force search to analyze the situation in which it finds itself. monte carlo techniques are then used to suggest plays by combining the results of analyzing instances of bridge's perfect-information variant. this approach appears to have been first suggested by levy  levy  1 . 
　card play is only half of bridge; there is bidding as well. it is possible to use search-based techniques here also  although there is no escaping the fact that a large database of bids and their meanings is needed by the program.  bidding is  after all  a communicative process; the meanings of the bids need to be agreed upon.  gib's success here has been more modest; the overall approach is promising but is  for technical reasons that we will describe  unusually vulnerable to gaps or other inaccuracies in the bidding database itself. 
g i b currently seems to be about halfway between 
bridge baron and world class  beating bridge baron by something over 1 imps per deal played and losing to strong human players by approximately half that. unlike previous programs  however  it it still improving rapidly; there are many straightforward additions that are likely to enhance its performance substantially. 
　the outline of this paper is as follows: we begin in the next section by describing a monte carlo approach to card play  outlining its strengths and weaknesses  and providing details on its performance. section 1 describes the use of a similar approach to bidding  explaining why it is so vulnerable to database errors and describing several possible ways around this vulnerability. we end with a summary of the gib project  including details on its overall performance and suggestions for future work. 
1 	card play 
in order to understand the card play phase of a bridge deal  consider first bridge's perfect information variant  the game where all of the players are playing  double dummy  in that they can see which cards the other players hold. in this case  the game tree is a fairly straightforward minimax tree  although there are some minimizing nodes with minimizing children  since the player playing last to one trick may well play first to the next. the raw branching factor of the tree appears to be about four; alpha-beta pruning and the introduction of a transposition table bring it down to about 1. augmenting the move ordering heuristic to exploit narrowness1 reduces the branching factor further to approximately 1  corresponding to a search space of some 1 nodes per deal. the introduction of partition search  ginsberg  1  and the killer heuristic reduce the space further to some 1 nodes per deal. 
　one way in which we might now proceed in a realistic situation would be to deal the unseen cards at random  biasing the deal so that it was consistent both with the bidding and with the cards played thus far. we could then analyze the resulting deal double dummy and decide which of our possible plays was the strongest. averaging over a large number of such monte carlo samples is one possible way of dealing with the imperfect nature of bridge information. 
a l g o r i t h m 1  monte carlo card selection  to select a move from a candidate set m of such moves: 
1. construct a set d of deals consistent with both the bidding and play of the deal thus far. 
1. for each move m m and each deal d d  evaluate the double dummy result of making the move m in the deal d. denote the score obtained by making this move s m d . 
1. return that m for which  is 	maximal 
　the monte carlo approach has drawbacks that have been pointed out by a variety of authors  including roller+ and others  frank and basin  1 . most obvious among these is that the approach never suggests making an  information gathering play.  after all  the perfect-information variant on which the decision is based invariably assumes that the information will be available by the time the next decision must be made! instead  the tendency is for the approach to simply defer important decisions; in many situations this may lead to information gathering inadvertently  but the amount of information acquired will generally be far less than other approaches might provide. in spite of this  gib's card play is at the level of a human expert. 
　performance was measured initially using bridge master  bm   a commercial program developed by gitelman. bm contains 1 deals at 1 levels of difficulty. each of the 1 deals on each level is a problem in declarer play. if you misplay the hand  bm moves the defenders' cards around if necessary to ensure your defeat. 
　bm was used for the test instead of randomly dealt deals because the signal to noise ratio is far higher; good plays are generally rewarded and bad ones punished. every deal also contains a lesson of some kind; there are 
1
　　the narrowness heuristic suggests placing early in the move ordering those moves to which the opponents have few legal responses  thereby keeping the size of the game tree small. this heuristic is apparently well known in the chess community but is poorly cited in the academic literature. a recent paper  plaat et al.  1  suggests that the idea is rooted in that of conspiracy search  mcallester  1 . 
	ginsberg 	1 
　
no completely uninteresting deals where the line of play is irrelevant or obvious. there are drawbacks to testing gib's performance on nonrandomly dealt deals  of course  since the bm deals may in some way not be representative of the problems a bridge player would actually encounter at the table. 
   the test was run under microsoft windows on a 1 mhz pentium pro. as a benchmark  bridge baron  bb  version 1 was also tested on the same deals using the same hardware.1 bb was given 1 seconds to select each play  and gib was given 1 seconds to play the entire deal with a monte carlo sample size of 1 new deals were generated each time a play decision needed to be made. 
   these numbers approximately equalized the computational resources used by the two programs; bb could in theory take 1 seconds per deal  ten seconds on each of 1 plays   but in practice took substantially less. g i b was given the auctions as well; there was no facility for doing this in bb. this information was critical on a small number of deals.  the auction is the sequence of bids made by the players.  
here is how the two systems performed: 
level bb gib 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 total 1 1 1% 1% each entry is the number of deals that were played successfully by the program in question. 
   gib's mistakes are illuminating. while some of them are of the sort that have already been mentioned  failing to gather information   most are quite different. 
   g i b is very good  nearly optimal  in fact  at identifying specific possibilities that will allow a contract to be made or defeated  since such possibilities are overlooked only if they don't appear in the monte carlo sample being used. what it is weak at is combining such possibilities. as an example  suppose that you are playing a hand and you can take one of four possible lines. each of the first two banks on a specific  but different  distribution of the opposing cards. the third line simply defers the guess by doing something random  and the fourth line is a clever one that succeeds in either of the first two cases  independent of which actually transpires. 
   g i b chooses randomly between the third and fourth possibilities in this situation  assuming that if it can defer the guess  it will make it correctly in the future!  and 
1
　　the current version is bridge baron 1 and could be expected to perform guardedly better in a test such as this. bridge baron 1 does not include the smith enhancements. 
1
　　gib's monte carlo sample size is fixed at 1 in most cases  which provides a good compromise between speed of play and accuracy of result. 
1 	computer game playing 
on a double dummy basis  it would.  this pattern accounts for virtually all of gib's mistakes; as bm's deals get more difficult  they more often involve combining a variety of possibly winning options and that is why gib's performance falls off at levels 1 and 1. 
   at still higher levels  however  bm typically involves the successful development of complex end positions  and gib's performance rebounds. this appeared to happen to bb as well  although to a much lesser extent. it was gratifying to see gib discover for itself the complex end positions around which the bm deals are designed  and more gratifying still to witness gib's recent discovery of a maneuver that had hitherto not been identified in the bridge literature. 
   experiments such as this one are tedious  because there is no text interface to a commercial program such as bridge master or bridge baron. as a result  information regarding the sensitivity of glb's performance to various parameters tends to be only anecdotal. 
   g i b solves an additional 1 problems  bringing its total to 1%  given additional resources in the form of extra time  up to 1 seconds per play  although that time was very rarely taken   a larger monte carlo sample  1 deals instead of 1  arid hand-generated explanations of the opponents' bids and opening leads. each of the three factors appeared to contribute equally to the improved performance. 
   other authors are reporting comparable levels of performance. forrester  working with a different but similar benchmark  blackwood  1   reports1 that gib solves 1% of the problems given 1 seconds/play  and 1% of them given 1 seconds/play. deals where gib has outplayed human experts are the topic of a series of articles in the dutch bridge magazine imp  eskes  1  and sequels .1 based on these results  gib was invited to participate in an invitational event at the 1 world bridge championships in france; the event involved deals similar to bridge master's but substantially more difficult. g i b joined a field of 1 of the best card players in the world  each player facing twelve such problems over the course of two days. g i b was leading at the halfway mark  but played poorly on the second day  perhaps the pressure was too much for it   and finished twelfth. 
   the human participants were given 1 minutes to play each deal  although they were penalized slightly for playing slowly. gib played each deal in about ten minutes  using a monte carlo sample size of 1. michael rosenberg  the eventual winner of the contest and the pretournament favorite  in fact made one more mistake than did bramley  the second place finisher. rosenberg played just quickly enough that the time penalties gave him the victory. the scoring method thus favors gib slightly. 
   there are two important technical remarks that must be made about the monte carlo algorithm before proceeding. first  note that we were cavalier in simply saying   construct a set d of deals consistent with both the 
1
posting to rec.games.bridge on 1 july 1. 
http://www.imp-bridge.nl 
　
bidding and play of the deal thus far.  
　to construct deals consistent with the bidding  we first simplify the auction as observed  building constraints describing each of the hands around the table. we then deal hands consistent with the constraints using a deal generator that deals unbiased hands given restrictions on the number of cards held by each player in each suit. this set of deals is then tested to remove elements that do not satisfy the remaining constraints  and each of the remaining deals is passed to the bidding module to identify those for which the observed bids would have been made by the players in question. this process typically takes one or two seconds to generate the full set of deals needed by the algorithm. 
   to conform to the card play thus far  it is impractical to test each hypothetical decision against the cardplay module itself. instead  gib uses its existing analyses to identify mistakes that the opponents might make. 	as an example  suppose gib plays the 	the analysis indicates that 1% of the time that the next player  say west  holds the  it is a mistake for west not to play it. if west in fact does not play the  bayes' rule is used to adjust the probability that west holds the at all. 	the probabilities are then modified further to include information revealed by defensive signalling  if any   and the adjusted probabilities are finally used to bias the monte carlo sample  replacing the evaluation  with  where  is the weight assigned to deal d. 	more heavily weighted deals thus have a larger impact on gib's eventual decision. 
　the second technical point regarding the algorithm itself involves the fact that it needs to run quickly and that it may need to be terminated before the analysis is complete. for the former  there are a variety of greedy techniques that can be used to ensure that a move m is not considered if we can show  for some m'. the algorithm also uses iterative broadening  ginsberg and harvey  1  to ensure that a lowwidth answer is available if a high-width search fails to terminate in time. results from the low- and high-width searches are combined when time expires. 
　also regarding speed  the algorithm requires that for each deal in the monte carlo sample and each possible move  we evaluate the resulting position exactly. knowing simply that move mi is not as good as move m1 for deal d is not enough; m1 may be better than m1 elsewhere and we need to compare them quantitatively. this approach is aided substantially by the partition search idea  where entries in the transposition table correspond not to single positions and their evaluated values  but to sets of positions and values. in many cases  m1 and m1 may fall into the same entry of the partition table long before they actually transpose into one another exactly. 
1 	bidding 
the purpose of bidding in bridge is twofold. the primary purpose is to share information about your cards with your partner so that you can cooperatively select an optimal final contract. a secondary purpose is to disrupt the opponents' attempt to do the same. 
　in order to achieve this purpose  a wide variety of bidding  languages  have been developed. in some  when you suggest clubs as trumps  it means you have a lot of them. in others  the suggestion is only temporary and the information conveyed is quite different. in all of these languages  some meaning is assigned to a wide variety of bids in particular situations; there are also default rules that assign meanings to bids that have no specifically assigned meanings. any computer bridge player will need similar understandings. 
　bidding is interesting because the meanings frequently overlap; there may be one or more bids that are suitable  or nearly so  on any particular set of cards. existing computer programs have simply tried to find the bid that is the best match for the cards that the machines hold  but world champion chip martel reports* that human experts take a different approach.1 
　although expert bidding is based on a database such as that used by existing programs  close decisions are made by simulating the results of each candidate action. this involves projecting how the bidding is likely to proceed and evaluating the play in one of a variety of possible final contracts. an expert gets his  judgment  from a monte carlo-like simulation of the results of possible bids  often referred to in the bridge-playing community as a borel simulation. glb takes a similar approach. 
a l g o r i t h m 1  borel simulation  to select a bid from a candidate set b  given a database z that suggests bids in various situations: 
1. construct a set d of deals consistent with the bidding thus far. 
1. for each bid b  b and each deal d  d  use the database z to project how the auction will continue if the bid b is made.  if no bid is suggested by the database  the player in question is assumed to pass.  compute the double dummy result of the eventual contract  denoting it s b d . 
1. return that b for which s b d  	is maximal. 
　as with the monte carlo approach to card play  this approach does not take into account  the fact that bridge is not played double dummy. human experts often choose not to make bids that will convey too much information to the opponents in order to make the defenders' task as difficult as possible. this consideration is missing from the above algorithm. 
unfortunately  there are more serious problems also. 
suppose that the database z is somewhat conservative 
1
　　the 1 rosenblum cup world team championship was won by a team that included martel and rosenberg. 
1
　　frank suggests  frank  1  that the existing machine approach is capable of reaching expert levels of performance. while this appears to have been true in the early 1's  lindelof  1   modern expert bidding practice has begun to highlight the disruptive aspect of bidding  and machine performance is no longer likely to be competitive. 
	ginsberg 	1 
　
in its actions. the projection in step 1 leads each player to assume his partner bids conservatively  and therefore to bid somewhat aggressively to compensate. the partnership as a whole ends up ot/ercompensating. 
　　worse still  suppose that there is an omission of some kind in z; perhaps every time someone bids the database suggests a foolish action. since is a rare bid  a bidding system that matches its bids directly to the database will encounter this problem infrequently. 
   g i b   however  will be much more aggressive  bidding  often on the grounds that doing so will cause the opponents to make a mistake. in practice  of course  the bug in the database is unlikely to be replicated in the opponents' minds  and gib's attempts to exploit the gap will be unrewarded or worse. 
　this is a serious problem  and appears to apply to any attempt to heuristically model an adversary's behavior: it is difficult to distinguish a good choice that is successful because the opponent has no winning options from a bad choice that appears successful because the heuristic fails to identify such options. 
　there are a variety of ways in which this problem might be addressed  none of them perfect. the most obvious is simply to use gib's aggressive tendencies to identify the bugs or gaps in the bidding database  and to fix them. because the database is large  some 1 rules  1 this is a slow process. 
　another approach is to try to identify the bugs in the database automatically  and to be wary in such situations. if the bidding simulation indicates that the opponents are about to achieve a result much worse than what they might achieve if they saw each other's cards  that is evidence that there may be a gap in the database. unfortunately  it is also evidence that gib is simply effectively disrupting its opponents' efforts to bid accurately. 
　finally  restrictions could be placed on gib that require it to make bids that are  close  to the bids suggested by the database  on the grounds that such bids are more likely to reflect improvements in judgment than to highlight gaps in the database. 
　all of these techniques are used  and all of them are useful. gib's bidding is substantially better than that of earlier programs  but not yet of expert caliber. 
　the bidding was tested as part of the 1 baron barclay/okbridge world computer bridge championships. each program bid deals that had previously been bid and played by experts; a result of 1 on any particular deal meant that the program bid to a contract as good as the average expert result. there were 1 deals in the contest; although card play was not an issue  the deals were selected to pose challenges in bidding and a standard deviation of 1 imps/deal is still a reasonable estimate. one standard deviation over the 1 deal set could thus be expected to be about 1 imps. 
   gib's final score in the bidding contest was +1 imps  as it narrowly edged out the expert field against which it 
　　g i b uses the database that is distributed with meadowlark bridge. 
1 	computer game playing 
was compared.1 the next best program finished with a score of -1 imps  not dissimilar from the -1 imps that had been sufficient to win the bidding contest in 1. 
1 	overall remarks 
1 	g i b compared 
g i b participated in the 1 world computer bridge championships  along with six other computer programs  including bridge baron. the event consisted of a complete round robin  with each program playing each other and the results being converted to  victory points.  after the round robin  the four leading programs advanced to a knockout phase  which was designed to favor slightly the program that won the round robin. 
　g i b won every match it played in the round robin  accumulating 1 out of a possible 1 victory points. in the knockout phase  it beat bridge baron by 1 imps over 1 deals  a 1 standard deviation event had the programs been evenly matched  and then beat q-plus bridge in the finals by 1 imps over 1 deals  a 1 standard deviation event . g i b also played a 1 deal demonstration match against human world champions zia mahmood and michael rosenberg1  losing by a total of 1 imps  a 1 standard deviation event . g i b also plays on okbridge  an internet bridge club with some 1 members.1 after playing thousands of deals against human opponents of various levels  it is losing at the rate of 1 imps/deal. 
1 	other games 
this has been very much a paper about bridge; i have left essentially untouched the question of to what extent the basic monte carlo technique could be applied to other games of imperfect information. although i can make educated guesses in this area  the experimental work on which this paper is based deals with bridge exclusively. 
　the primary drawback of the monte carlo approach appears to be that it does not encourage information gathering actions  instead tending to defer decisions on the grounds that perfect information will be available later. this leads to small but noticeable errors in glb's cardplay. hearts appears to be similar to bridge in this area  and i would expect it to be possible to translate gib's success from one game to the other. 
　the monte carlo approach is known to be successful in both backgammon and scrabble  where the strongest machine players simulate possible dice rolls or tile draws 
　　1  this is in spite of the earlier remark that gib's bidding is not of expert caliber. gib was lucky in the bidding contest in that all of the problems involved situations that it understood. when faced with a situation that it does not understand  gib's bidding deteriorates drastically. 
1
　　mahmood and rosenberg have won  among other titles  the 1 cap volmac world top invitational tournament. as remarked earlier  rosenberg would also go on after the 
gib match to win the par competition in which gib finished 1th. 
	* 	http://www.okbridge.com 
　
several moves ahead in order to select a move. these games clearly meet the criteria of the previous paragraph  since it is impossible to gather information in advance about the stochastic processes underlying the game. 
　for other games  however  the problems may be more severe. poker  for example  depends heavily on the ability to make information gathering maneuvers. how effective monte carlo techniques are in cases such as this remains to be seen. 
1 	future work 
g i b has matured to the point that new ideas can be tested by having it play itself overnight over 1 deals. the chess community has already observed that it is easy to use this approach to overfit  so gib's self-testing is used only to evaluate coarse features of the approach such as the question of whether a monte carlo simulation be used during the bidding at all.1 
　there are a variety of straightforward extensions to gib that should also improve its performance substantially. principal among these is the further development of gib's  i.e.  meadowlark's  bidding database  and the inclusion of a facility that allows gib to think on its opponents' time. none of these modifications requires substantial technical innovation; it's simply a matter of doing it. martel has predicted that gib will achieve expert levels of performance around 1  and be stronger than any human player within two or three years after that. the prospects for doing this seem fairly bright. 
acknowledgement 
the gib work has been supported by just write  inc; during its development  i have received invaluable help from members of both the bridge and computer science communities. i am especially indebted to chip martel  rod ludwig  alan jaffray  hans kuijf and fred gitelman  but also to bob hamman and eric rod well  to david etherington  bart massey and the other members of girl  to jonathan schaeffer and rich korf  and to jeff goldsmith  thomas andrews and many other members of the rec.games.bridge community. 
the work has also been supported by darpa and 
afrl under agreements f1-1 and f1-
1-1. the u.s. government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation hereon. the views and conclusions contained herein are those of the author and should not be interpreted as necessarily representing the official policies or endorsements  either expressed or implied  of darpa  afrl  or the u.s. government. 
