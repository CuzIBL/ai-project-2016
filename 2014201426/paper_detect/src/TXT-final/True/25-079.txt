 
we describe a new approach to default  reasoning  based on a principle oi indifference among possible worlds. we interpret default rules as extreme statistical statements  thus obtaining a knowledge base kb comprised of statistical and first-order statements. we then assign equal probability to all worlds consistent with kb in order to assign a degree of belief to a statement φ. the degree of belief can be used to decide whether to defeasibly conclude φ. various natural patterns of reasoning  such as a preference for more specific defaults  indifference to irrelevant information  and the ability to combine independent pieces of evidence  turn out to follow naturally from this technique. furthermore  our approach is not restricted to default reasoning; it supports a spectrum of reasoning.  from quantitative to qualitative. it is also related to other systems for default reasoning. in particular  we show that the work of |goldszmidt et al.  1   which applies maximum entropy ideas to --semantics  can be embedded in our framework. 
1 	introduction 
default reasoning  i.e.  reasoning to plausible but. deductively invalid conclusions  has been an important research area in ai for over a decade. work in this area shows us how  given that we accept the default  birds typically fly  and the fact  tweety is a bird   we can arrive* at the reasonable  although possibly incorrect  belie! that  tweety flies . 
   *the work of fahiem bacchus was supported by nserc under their operating grants program and by iris. the work of adam crove  joseph halpern  and daphne koller was sponsored in part by the air force office of scientific re search  afso   under contract f1-c-1. adam  trove's work was performed while at stanford university  where he was also supported by an ibm graduate fellowship. some of this research was performed while adam grove and daphne koller were at ibm alrnaden research center. the united states government  is authorized to reproduce and distribute reprints for governmental purposes. 
joseph y. halpern 
ibm almaden research center 
1 harry road 
 san jose  ca 1 halpern  almaden. ibm.com daphne koller 
computer science dept. 
stanford university 
　stanford  ca 1 daphne es. stanford.edu 　a different reasoning paradigm  which has been studied for an even longer period  is direct inference. direct inference is concerned with reasoning to conclusions about particular individuals from general statistical knowledge. for example  from a knowledge base consisting of the statistical information  1% of birds fly  and the fact  tweety is a bird1  theories of direct inference would allow us to conclude that our degive of belief in  tweety files  should be 1. different systems for direct inference that have been suggested include  bacchus  
1; bacchus et al  1; kyburg  1; levi  1; pollock  1; reichenbaeh  1  salmon  1 . 
　direct inference ana default reasoning share a number of important characteristics. first  neither is a logically sound inference system. neither statistical knowledge nor defaults about the class of all birds permit us to deduce anything for certain about a particular bird such as tweety: both  tweety flies  and  tweety does not  fly  are logically consistent with  1% of birds fly  or  birds typically fly - second  both direct inference and default reasoning are nonmonotonic. if we learn that penguins do not fly  and that tweety is a penguin  direct inference would generate a different degree of belief in tweety flying. similarly  default reasoning systems would retract the conclusion that tweety flies. and third  various properties  such as ignoring irrelevant information and preferring more specific information  are considered to be desirable in both default reasoning and direct infer-
ence. 
　so how deep is the connection between direct inference and ai default reasoning  some applications of defaults seem to have little to do with statistics  mccarthy  1 . but equally often  defaults have .some basis in statistics. for example  the default  birds typically fly  appears to have as one of its justifications the statistical fact that most birds fly. thus  it seems reasonable to adapt techniques from direct inference to reason with defaults of this type. cur theme in this paper is that this plausible connection between direct inference and default reasoning can be made precise. in particular  we show in section 1 that a new method for direct inference  first introduced in  bacchus et al.  1; grove et al.  1b   can provide many of the features considered desirable in default reasoning. among other things  it provides a preference1 for more specific defaults as well as the ability to ignore irrelevant information. 
	bacchus et al. 	1 
　this is particularly important a s there is a tension between these  two requirements. most theories of default reasoning fail to capture both of them simultaneously  sec  for example   geffner and pearl  1; lehmann and magidor  1; pearl  1; reiter  1  . what is even more important is that for us these properties follow directly from an independently motivated semantics; they are not the result of adopting an ad hoe theory of irrelevance. 
　in our method  we presume that there is a knowledge base that consists of information about the world  in the form of first-order statements  such as  all penguins are birds   and statistical information. the statistical information might be quantitative  e.g.   1% of birds fly    or it might be in the form of qualitative default information. we interpret a default statement such as  birds typically fly  as the statistical assertion  almost all birds fly   which is given a precise semantic interpretation within our formalism. this interpretation of defaults has a number of benefits. the first is simply that we understand what our knowledge base means. many default theories will tell us how to reason with  birds typically fly . but  as pointed out by  neufeld  1   there is far less work telling us when we should adopt this default in the first place. specifically  what is there about the world that makes this a good default  for us  the true proportion of flying birds offers a guide to how reasonable our approximation  almost all birds fly  really is. in addition  the semantics imposes natural constraints on the defaults. for example  in our formalism the default  birds typically fly  is inconsistent with both the default  birds typically do not fly  and the logical assertion  no bird flies . 
　a major advantage of our approach is that it allows for rich knowledge bases  with arbitrary first-order information and statistical information. thus  it can support both quantitative and qualitative reasoning. in section 1  we demonstrate the advantages of being able to perform both types of reasoning in a unified framework  by considering both the lottery paradox and the nixon diamond example. 
　we are certainly not the first to apply a probabilistic semantics to nonmonotonic logic  see  pearl  1  for an overview . however  while all the other probabilistic approaches we are aware of use the statistical interpretation as a motivation for using probabilities  none make explicit use of statistical assertions. nevertheless  there are close technical connections between our approach and e-semantics  adams  1; geffner and pearl  1 . in particular  we show in section 1 that the approach of goldszmidt  morris  and pearl   which extends e-semantics by applying ideas of maximum entropy  can be embedded in our framework. besides providing further justification for the use of maximum entropy in  goldszmidt et al.  1   this embedding allows us to use the algorithms they have developed to calculate degrees of belief for formulas in a fragment of our full language. 
1 	the formalism 
we assume that the knowledge base consists of sentences written in a formal language that allows us to express both statistical information and first-order information. we use the probability logic presented in  grove et al   1b   which is a variant of logics developed in  bacchus  1; halpem  1 . 
　this logic augments first-order logic by allowing proportion expressions of the form  this term denotes the proportion of domain elements satisfying we actually allow an arbitrary set of variables in the subscript. thus  for example   describes  for a fixed y. the proportion of domain elements that are sons of y;  describes  for a fixed r  the proportion of domain elements whose son is x  and describes the proportion of pairs of domain elements that are in the son relation. we also allow conditional proportion expressions of the form  which denotes the proportion of domain elements satisfying ψ from among those elements satisfying  a rational number is also a proportion expression  and the set of proportion expressions is closed under addition  subtraction  and multiplication. 

1
　　 we discuss the issue of conditioning on an event with probability zero in the full paper. 
1
　　in  bacchus et al.  1  the use of approximate equality was suppressed in order to highlight other issues. 






approximately equals relation    since the approach of  goldszmidt et al.  1  uses the same e for all default rules. moreover  they all involve only unary predicates. under this translation  we can prove the following theorem  using techniques similar to  grove et al.  1b . 
t h e o r e m 1 . 1 : let c be a constant symbol. using the translation described above  for any set r of defeasible is an me-plausible consequence of r iff 
thus  all the computational techniques and results described in  goldszmidt. et al.  1  carry over to this special case of our approach. 
   it is very encouraging that the results of  goldszmidt et al.  1  can be arrived at in two quite different ways. our result formalizes a connection between entropy and indifference  well known in other contexts like statistical thermodynamics  in the context of an agent reasoning by default. it shows that if one feels that it is reasonable* for an agent to be indifferent between possibilities left open by its knowledge  then one has an independent reason for accepting the theory of irrelevance generated by maximum entropy. 
   it should also be noted that our approach  which does not appeal to entropy maximization directly  has the advantage of being much more general. most importantly  it can deal sensibly with languages that have predicates of arbitrary arity. it is unlikely that an approach that uses entropy directly could be extended to deal such languages. once we have even a single binary predicate in the language  all connection between our approach and maximum entropy disappears. as discussed in  grove et. a/.  1b   we cannot even find a suitable probability space to take entropy over. results of  grove et a/.  1a  showing that  with a binary predicate in the language  degrees of belief are in general uncomputable support the conjecture that there is none to be found. 
1 	discussion and conclusions 
we have shown that a logic that allows statistical and first-order assertions  together with a principled approach for obtaining degrees of belief from a knowledge base expressed in this logic  can give a general approach for capturing many aspects of default reasoning. our framework has the added advantage of being able to deal with both default  quantitative  and statistical  quantitative  information. our results demonstrate the close connection between default reasoning and direct inference. 
   we close by briefly discussing two criticisms that have been made of entropy-based reasoning systems: language and syntax dependence  and the treatment of causality  pearl  1 . while the random-worlds method is not entropy-based  the relationship we observed in section 1 suggests that similar problems may arise. 
　with regard to causality   goldszmidt et a/.  1; pearl  1  and  hunter  1  h ave observed that knowledge about causal relationships greatly affects our intuitions concerning the  right  answers to various problems  and that the naive maximum entropy approaches do not take this causal information into consideration. we would argue that this only shows that this information is not properly captured by the straightforward encoding of defaults  and that we may therefore have to include information about causality when expressing defaults in the knowledge base.  hunter  1  presents one possibility for encoding causal information within the maximum entropy approach. in  bacchus et al.  1   we present a more general approach within the random-worlds framework  and show that it deals with many of the problematic aspects of causal reasoning. 
   the language problem is more subtle. maximumentropy methods can draw different conclusions from knowledge bases that seem to reflect the same information about the world. 'phis is a serious issue  because the choice of the  right  representation of our information is not always clear. in general  we believe that the form in which our information is written down encodes knowledge it reveals our biases and expectations. it is perfectly reasonable that our bias should affect inductive reasoning. in certain cases  our bias is sufficiently clear that the choice of representation becomes obvious. in physics  for example  the choice of language is sometimes based on the criterion of time-invanance. moreover  in physics and in many other applications of maximum entropy  there is an objective ''reality check  -we can compare the answers given by the formalism to reality  and thus independently verify the reasonableness of our representation. in many ai applications  however  there might not be an obvious representation  nor an appropriate reality check. in these cases  we will have to formulate criteria for choosing the right formal knowledge base  given a natural-language specification of our knowledge. this is an important research problem  which we intend to investigate. the fact that our approach can deal with causality leads us to hope that it will be able to deal with the language problem as well in a satisfactory way. 
acknowledgements 
we would like to thank moshe vardi for helpful comments. 
