the complexity of theory revision 
russell g r e i n e r * 
siemens corporate researcli 	princeton  nj 1 
greiner scr siemens com 

a b s t r a c t 
a knowledge-based system uses its database 
 a k a its  theory   to produce answers to the queries it receives unfortunately  these an swere may be incorrect if the underlying theory is faulty standard  theory revision  systems use a given set of  labeled queries   each a query paired with its correct answer  to transform the given theory  by adding and/or deleting either rules and/or antecedents  into a related theory that is as accurate as possible after formally denning the theory revision task and bounding its sample complexity  this paper addresses the task's computational complexity it first proves that  unless p = np  no polynomial time algorithm can identify the opti mal theory  even given the exact distribution of queries  except in the most trivial of situations it also shows that  except in such trivial situations  no polynomial-time algorithm can produce a theory whose inaccuracy is even close  j e   within a particular polynomial factor  to optimal these results justify the standard practice of hill-chmbing to a locally-optimal 
theory  based on a given set of labeled samples 
1 	i n t r o d u c t i o n 
there are many fielded knowledge-based systems  ranging from expert systems and logic programs to production systems and database management systems  lev1  each such system uses its database of general informa tion  a k a its  theory   to produce an answer to each given query  this can correspond to retrieving information from a database  eg  finding x such that   mak a acne x  &  color x red    or to providing a diagnosis or repair  based on a given set of symptoms unfortunately  these responses may be incorrect if the underly ing theory includes erroneous information if we observe that some answers are incorrect  e g   if the patient does 
    i gratefully acknowledge receiving helpful comments from george drastal  r bharat rao  narendra gupta  tom han cock  sheila mcllraith  edoardo amaldi  dan roth and rom khardon 
1 	learning 
not get better  or the proposed repair does not correct the device's faults   we can then ask a human expert to supply the correct answer theory revision is the process of using such correctly-answered queries to modify the initial theory  to produce a new theory that is more accurate  i e   which will not make those mistakes again  while remaining correct on the other queries 
　most theory revision algorithms use a set of transformations to hill-chmb through successive theories  until reaching a theory whose accuracy is  locally  optimal  based on a set of correctly-answered queries  c/   pol1  mb1  coh1  om1  wp1  cs1  ldrg1  this report addresses the obvious question is there a better approach  which will directly yield the globally optimal theory1 
　section 1 first states the theory revision objective more precisely as finding the theory with the highest accuracy from the space of theories formed by applying a sequence of transformations to a given initial theory  here each transform involves either adding or deleting either a rule or an antecedent it also proves that a polynomial number of training  labeled queries   each a specific query paired with its correct answer  is sufficient  i e  they provide the information needed to identify a transformation-sequence that will transform the given theory into a new theory whose accuracy is arbitrarily close to optimal  with arbitrarily high probability section 1 then addresses the computational complexity of the task of finding the optimal  or even near-optimal  revised theory it first proves that the task of computing the optimal theory within this space of theories is intractable  even in trivial contexts - eg  even when dealing with propositional horn theories  or when considering with only atomic queries  or when considenng onl  a bounded number of transformations  etc 1 we then show that this task cannot even be approximated i e   that no efficient algorithm can find a theory whose inaccuracy is even close to  i e   within a particular small polynomial of  optimum' we also prove that these negative results apply even when we are only generalizing  or only specializing  the initial theory we also discuss the efficiency of other restricted variants of theory revision  
   throughout  we will assume that p=/np  gj1   which implies that any np-hard problem is intractable this also implies certain approximation claims  presented below 

providing sharp boundaries that describe exactly when this task is  versus is not  tractable 
　we view these results as sanctioning the standard approach of using a set of transformations to hill-climb to a local optimum  based on a set of samples the labeled training samples are required to obtain the needed distribution information  and the realization that no tractable algorithm will be able to find the global optimum justifies hill-chmbing to a local optimum  within the space formed using specified transformations 
we close this section by describing related research 1 
related research while most learning systems begin with an  empty theory  and attempt to learn a target function  perhaps a decision tree  or a logic program   theory revision processes work by modify mg a given initial theory there are several implemented theory revision systems most use essentially the same set of transformations we describe - e g   
audrey  wp1   fonte  mb1   either  om1  and 
a  ldrg1  all consider adding or deleting antecedents or rules our analysis  and results  can easily be applied to many other types of modifications - e g   specializing or generalizing antecedents  om1   using  n-of-m rules   bm1   or merging rules and removing chains paths of rules that produced incorrect results  coh1  coh1  1 while these projects provide empirical evidence of the effectiveness of their specific algorithms  and deal with classification  j e   determining whether a given element is a member of some target class  rather than general derivation  our work formally addresses the complexities inherent in finding the best theory  for handling arbitrary queries 
　finally  note that  in some cases  our task can require extracting the best consistent sub-theory from a given inconsistent theory from this perspective  our work is related to  knowledge representation  form of theory revision  a la gardenfors  gar1  agm1   katsuno and mendelzon  km1l  and many others our work differs by using the notion of expected accuracy to dictate which of the  revisions  is best 
1 	f r a m e w o r k 
we define a  theory  as a collection of  propositional or first order  horn clauses  where each clause is a disjunction of literals  at most one of which is positive borrowing from  lev1  dp1l   we also view a theory t as a function that maps each query to its proposed answer  hence  t q-  a  where q is a  possibly infinite  set of horn queries  and a = { no  yes } is the set of possible 

the wrong answer 
   this a t    function measures t's accuracy for a sin gle query in general  our theories must deal with a range of queries we model this using a stationary  but unknown  probability function pr   where 
pt q  is the probability that the query q will be posed given this distribution  we can compute the  expected accuracy  of a theory  t 

   we will consider various sets of possible theories  where each such t contains the set of theories 
1
　　to simplify the presentation  the bulk of this paper will deal only with proportional logic  section 1 below describes the extensions needed to deal with predicate calculus 
	greiner 	1 

formed by applying various sequences of transformations to a given initial theory  see section 1 below our challenge is to identify the theory whose expected accuracy is optimal  i e   
		 1  
　there are two challenges to finding such optimal theories the first is baaed on the observation that the expected accuracy of a theory depends on the distribution of queries  which means different theories will be optimal for different distributions while this distribution is not known initially  it can be estimated by observing a set of samples  each a query/answer pan   drawn from that distribution section 1 below discusses the number of samples required to be confident of obtaining the information needed to identify a good  with high probability 
　we are then left with the challenge of computing the best theory  once given this distributional estimate section 1 addresses the computational complexity of this process  showing that the task is not just intractable 1 but it is also not approximatable - i e   no efficient algorithm can even find a theory whose expected accuracy is even close  in a sense defined below  to the optimal value 

1 	learning 


	greiner 	1 

figure 1 tractability of theory revision tasks 

even when there is a horn theory that correctly labels all of the queries  
the observation that determining such  it-step perfect theories  is np-hard leads immediately to 
corollary 1 it is np-hard to compute the minimalcost transformation sequence required to produce a per-
fect theory  i e   to compute the smallest k for which there is c  such that 
even m the propositional case when considering only atomic queries ft is also np-hard to compute the  minimal-length  transformation  where the length of the transformation sequence is simply k - 
j e   when each transformation has  unit cost  
　this negative result shows the intractability of the obvious proposal of using a breath-first transversal of the space of all possible theory revisions first test the initial theory to against the labeled queries  and return t1 if it is 1% correct if not  then consider all theories formed by applying a single  unit-cost  transformation  and return any perfect and if not  consider all theories in  formed by applying sequences of transformations with cost at most two   and return any 
perfect  and so forth 
1 	a p p r o x i m a t a b i l i t y 
many decision problems correspond immediately to optimization problems  for example  the m i n g r a p h c o l o r decision problem  given a gtaph g =  n} e  and a positive integer a   can each node be labeled by one of k colors in such a way that no edge connects two nodes of the same color  see  gj1  pl1 chromatic number    corresponds to the obvious minimization problem find the minimal coloring of the given graph g we can simdarly view the decision droblem as either the 
maximization problem  find the whose accuracy is mammal  or the minimization problem  find the  whose inaccuracy is minimal   where a 
theory's inaccuracy is obviously i n a   t   = 1 - a  t   
1 	learning 
 while the maximally accurate theory is also mini mally inaccurate  these two formulations can lead to different approximatability results   for notation  let 
  resp    refer to the 
maximization  resp   minimization  problem 
　now consider any algorithm b that  given any instance x =  t  s  with initial theory t and labeled training sample s  computes a syntactically legal  but not necessarily optimal  revision 
then b's  performance ratio for the instance is defined as 

	bounded 	 all notpolyapprox     	 all notpolyapprox     	 all notpolyapprox     

　the companion paper  gre1a  considers other related cases  including the above special cases in the context where our underlying theories can use the not     operator to return yes if the specified goal cannot be proven  hard  this problem is clearly in np 
1
　　there are such constants for some other np-hard minimization problems for example  there is a polynomialtime algorithm that computes a solution whose cost is within a factor of 1 for any travelingsalesman with-
tr!angle equality problem  see  gj1  theorem 1  
1 e   using negation-as-failure  cla1  it also considers the effect of re-ordenng the rules and the antecedents  in the context where such shufflings can affect the answers returned in most of these cases  we show that the cor responding maximization problem is not approximatable within a particular polynomial 
	greiner 	1 

    the extended  gre1b  explainb the asymmetry between trprop perf ersus t r p r o p perf e+r   and discusses how these results relate to both inductive logic programming  and to default theories   
1 conclusion 
a knowledge-based system can produce incorrect answers to quenes if its underlying theory is faulty a  theory revision  system attempts to transform a given theory into a related one that is as accurate as possi ble  using a given set of correctly answered  training quenes  this report describes both the sample and computational complexity of this task it first provides the number of samples required to obtain the statistics needed to identify a theory  from within a class of theories defined by applying various standard transformations to a given initial theory  whose accuracy will be 
within e of the optimal theory in this class  with probability at least 1 - 1 it then shows that  in general  the task of computing this globally optimal theory is intractable - and worse  that no polynomial time algorithm can be guaranteed to find a solution that is even close to optimal  given the standard p =/ np assumption  we also present special cases of these tasks  which pin-point exactly when the task becomes tractable 
