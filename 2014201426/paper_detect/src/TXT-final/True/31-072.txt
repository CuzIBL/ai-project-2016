 
given a subproblem  s  of a constraint satisfaction problem  we can decompose the problem into a set of disjoint subproblems one of which will be s. this decomposition permits exploitation of problem-specific metaknowledge  a priori or acquired knowledge  about s. if we know that s is unsolvable  for example  the decomposition permits us to extract and then discard s  restricting the search for a solution to the remaining subproblems. a variety of potential uses for the decomposition method are discussed. a specific method that dynamically discards failed subproblems during forward checking search is described  and its utility demonstrated experimentally. 
1 introduction 
1 	overview 
constraint satisfaction problems  csps  involve finding values for problem variables subject to restrictions  constraints  on what combinations of values are allowed. they have wide application in artificial intelligence  in areas ranging from planning to machine vision. 
　fig. la shows a sample csp. the problem is to assign a color to each vertex of the graph satisfying the restrictions that vertices joined by an edge cannot be assigned the same color. coloring problems are useful for illustration purposes  and actually have applications  e.g. to scheduling . they are in general np-complete  though obviously the example here is a trivial one. the csp variables in this coloring problem are the vertices of the graph. the domain of values for each variable is the set of colors available for the vertex; in this case colors a  b  c  aquamarine  black and coral  are available at each vertex. the constraints are the  not same color   i.e.  not equal   restrictions corresponding to each edge. call any choice of a value for each variable  e.g. a for x  b for y  a for z  a possibility. a possibility that satisfies all the constraints is a solution. 
　*this material is based on work supported by the national science foundation under grant no. iri-1. 
1 	constraint 	satisfaction 

　at times we may have special knowledge of a csp subproblem and wish to extract it from the csp  to decompose the csp in a manner that permits us to consider the subproblem and the remainder of the csp separately. this paper provides a mechanism for doing so for a large class of subproblems. 
　for example  fig lb shows a subproblem of the coloring problem. this is an example of what we call here a subdomain subproblem  essentially one in which the domains of some variables have been reduced. this particular subproblem obviously has no solution. it represents a special case of the easily inferred conclusion that a complete graph of n vertices  one in which each pair of vertices is joined by an edge  cannot be colored with n-1 colors. the subproblem contains 1 of the 1 possibilities in the original problem  almost 1 per cent of the total. it would be nice if we could extract this subproblem  and then discard it. but what exactly would be left  well  in fact  what is left are the three subproblems shown in fig. lc. one contains 1 possibilities  one 1  one 1. together they contain precisely the 1 possibilities that remain when the subproblem in fig. lb is removed. 
　our general extraction procedure provides a mechanism for exploiting many forms of  metaknowledge  about subproblems; it is described in section 1. we use this scheme in a specific  new method that dynamically factors out  extracts and removes  failed subproblems  repeatedly during search; this is described in section 1. section 1 evaluates the new method experimentally on hard homogeneous and inhomogeneous problems. section 1 speculates on further uses for the general mechanism; section 1 is a brief conclusion. 
　
1 	relation to previous work 
the subproblem extraction method is a generalization of the idc decomposition employed in  freuder and hubbe 1 .  thus the idc algorithm already provides one successful application of the extraction approach.  the idc algorithm extracts and discard subproblems that may contain solutions  and thus is not appropriate when seeking all solutions  but is guaranteed not to discard all solutions . the basic decomposition step of the extraction method  dividing a problem into two subproblems  utilizes domain splitting  mackworth 1; van hentenryck 1 ; the extraction method combines several splitting operations for a specific new purpose. 
　factoring out failed subproblems provides a new approach to profiting from experience during search  which 
we will imbed in a new algorithm called fof for  factor out failure . there has  of course  been considerable work on learning or remembering new constraints discovered during search in the csp literature  e.g.  frost and dechter 1  schiex and verfaillie 1   with connections to the truth maintenance literature   see  smith and kelleher 1  . the closest to fof is probably the learning method of charman  charman 1   but fof has considerably more potential for pruning. 
　the failed subproblem could be recorded as a new kary constraint. however  the overhead of remembering and employing such constraints is problematic. initial 
experiments along this line did not demonstrate an impressive performance  though this alternative approach may prove useful for problems with an appropriate structure  verfaillie 1 . in fact  our approach is almost the opposite in spirit to traditional learning or tms approaches. there individual  nogoods   inconsistent ktuples  are recorded and consulted for future pruning. with our approach entire subproblems are discarded  rather than remembered. 
1 	extraction 
for simplicity we restrict our attention here to binary 
csps  where the constraints involve two variables. a value for variable u and one for variable v are said to be consistent if they satisfy the constraint between u and v  i.e. the pair of values is permitted by the con-
straint. tightness is a measure of how many pairs satisfy a constraint  the higher the tightness  the fewer consistent pairs. density is a measure of how often there is a constraint between pairs of variables  the higher the 
density  the more constraints there are. 
　the process that produced the decomposition of the example in the introduction is shown in figure 1. the original problem appears at the root of the decomposition tree. the decomposition consists of the leaves; since one of the subproblems is unsolvable here  it can be discarded. problems are represented by the variable domains  where the variables are listed in lexicographic order. 
　we say that the decomposition extracts the target subproblem s from the original problem. the method can be 
generalized to extract any subdomain subproblem from any csp. s is a subdomain subproblem of p if s can be obtained from p by eliminating some values from some of 
	freuder and hubbe 	1 
　
　each time we move down in the search tree  to try and extend the partial solution we are accumulating to another variable  v  we begin keeping track of the largest failed subproblem  f  encountered as we try different values for v. when we try another value for v  and are faced with a new subproblem  s  we factor out the possibilities that s has in common with the failed f  to avoid retesting them. they would still fail with the new value for v. the agenda is maintained as a stack and the remainder problem in a decomposition is placed on the stack first 
to implement a depth-first search. 
　the factoring out decomposition does not require any more constraint checks than a forward checking decomposition. nevertheless  it was found to be undesirable 
                                                 to utilize the fof decomposition all the time. fof can introduce effort in two ways. first  of course  is the overhead associated with the algorithm. second  there is a 1 	constraint satisfaction 
　
certain amount of redundancy introduced by the decomposition. while the decomposition cannot contain any more possibilities than the original problem  it may contain partial possibilities redundantly. for example  in figure 1  the partial possiblity of b for the first variable and a for the third variable is contained in two subproblems. this can lead to some redundant effort. 
　in our implementation we only employ the fof decomposition when the ratio of the size of the problem fed to extract to the size of the decomposition returned is greater than a threshold. otherwise  we proceed with standard forward checking. as we have noted  forward checking itself can be represented within the disjunctive decomposition schema; thus the schema easily accomodates an integrated algorithm in which the choice of whether to use forward checking or fof decomposition is determined by a threshold. the value we use for this threshold  1  was arrived at experimentally. we will refer to the algorithm as fof. 
　there is some evidence that an algorithm that establishes and maintains full arc consistency can often be preferable to forward checking  which maintains partial arc consistency  sabin and freuder 1 . however  it should be possible to apply the factor out failure insight to this algorithm as well  and to other extensions of forward checking. 
1 	experiments 
it is increasingly understood that csp methods are often not competitors  but can be combined cooperatively 
 prosser 1 . thus the question for a new method is less  can it beat x  than it is  can it profitably be added to x . fof it self may be viewed as a refinement of forward checking  fc . standard fc is a good benchmark for comparison as it has itself been compared with many algorithms  generally to its advantage . we have also added fof decomposition to the combination of forward checking and constraint-based backjumping  fccbj   a combination that has proven especially successful recently  prosser 1   and compared fc-cbj with fc-cbj-fof. in all cases we use a proven variable ordering heuristic that dynamically chooses a minimal domain size variable to instantiate next  dmd   haralick and elliott 1 . 
　random problems have often been used as benchmarks. there is a well-known hard problem  ridge'' in  density/tightness space  for random csps  cheeseman et al. 1  williams and hogg 1 . in our first experiment  figure 1  we looked at several points along this ridge.  our points may not be at the precise peak of the ridge.  at each point we averaged ten problems.  it should be noted that there may be a wide variation in difficulty within a problem set.  the problems all have 1 variables with 1 values. for a range of tightness values we looked for a density that put us on the ridge.  our probabalistic problem generator permits some variation in actual density and tightness values  especially locally within a problem.  
　we counted constraint checks  a standard measure of csp algorithm performance  and cpu time in seconds. in the figure  checks are shown first  then time  on a dec alpha 1xl  is shown in parentheses. adding fof reduced constraint checks in almost every case. the improvement of fof over fc approaches an order of magnitude in constraint checks at the highest tightness. fc-cbj-fof-dmd had the fewest constraint checks in every case  and the best time at three points out of four. 
　the improvement increases as the problems become sparser. we expect that problems of lower density  where variables are involved in fewer constraints  will be better candidates for fof  since there will be fewer subproblems in the decompositions and less opportunity for redundancy.  constraints where different values are likely to support the same values at other variables will also lead to fewer subproblems . the lowest density translates into an average number of constraints per variable of 1.  note  however  that the threshhold for fof decomposition avoids bad behavior for fof even at much higher densities.  we will call the average number of constraints per variable the degree  this has to do with the standard representation of csps as  constraint graphs1' . 
　as problems become sparser they tend to become easier anyway  however  other things being equal. there are fewer constraints to check. also there may be deeper theoretical reasons to expect loosely constrained problems to be harder  van beek 1   and other things being equal loosely constrained problems must be denser to stay on the hard problem ridge. however  by increasing the number of problem variables we can obviously encounter harder problems  and real-world problems may well be large  sparse problems . 
　we do just this in the second experiment  figure 1  taking off from the point where we obtained the best result in the first experiment. at this point the parameters were: a tightness of .1  1 variables with 1 values  and a degree of 1. we keep these parameters fixed except for the number of problem variables  which we increase from 1  to 1 and 1.  our analysis suggests that by maintaining a fixed degree  as opposed to a fixed density  we will in theory remain on the hard problem ridge.  in this experiment we restrict our attention to the two best algorithms from the first experiment. the savings become quite significant. 
　so far  although our generator allows some variation  we have been lookly at basically homogeneous random problems. one might expect that application problems would involve more heterogeneity in their structure. in the third experiment we introduced more divergence in structure by removing or loosening constraints. we started with a set of 1  ridge  problems with 1 variables  a domain size of 1  tightness .1 and density .1.  this density corresponds to an average degree of approximately 1.  for each of these we generated a sequence of problems by randomly choosing variables  five at a time  with degree greater than 1  and reducing their degree to three  by randomly removing constraints involving the variables . we call this process introducing weak spots into the problem. figures 1a-e shows these five problem sequences.  note that the scales differ.  in figure 1f we induce weak spots  in the problem of figure 
1a  in a different manner  by loosening the constraints 
	freuder and hubbe 	1 
　

1 	constraint satisfaction 

	freuder and hubbe 	1 
　
around variables rather than removing them  so that the tightness times the degree is less than 1 . 
　we do not show the fc-dmd results; in the first problem for example fc-dmd peaks at close to 1 million constraint checks  way off the chart shown in figure 
1a. our idc algorithm was tested on these problems in  freuder and hubbe 1 ; fof-dmd actually performs somewhat better.  idc has not been combined with fc-cbj.  
　the general lesson of these figures seems to be that cbj and fof complement each other nicely in coping with the induced inhomogeneity. the weak spots may also hinder standard forward checking  by reducing the pruning that forward checking can accomplish. this may lead to larger failed subproblems  which in turn provide an opportunity for fof. although we did not design these problem sequences specifically for fof  these approaches to inducing inhomogeneity would admittedly appear to favor fof. however  the larger point is that in inhomogeneous problems with a range of degrees and tightnesses there may well be areas with a structure for which fof is particularly well suited  and fof seems well able to take advantage of such local opportunities. 
1 	potential 
as the idc algorithm demonstrates  a primary potential application of the extraction method arises in situations where we can determine that a subproblem s has no solution  or does not have all the solutions - s can then be discarded. we might know s has no solutions from previous experience  e.g. with a similar problem. we might infer it from domain knowledge. the coloring problem illustration used in the first section is an example. we can  create  unsolvable subproblems. take any inconsistent pair of values a and b for x and y. the subproblem obtained by reducing the domain of x and y to a and b  can be factored out. taking this further: if the sets of values a for x and b for y are such that no pair of values from a and b is consistent we can factor out the larger subproblem where x and y are only reduced to a and b. we could look for pairs of value subsets  a and b  which are optimal in the sense that any pair of values in their cartesian product is inconsistent  and no other pair of value subsets produces a larger cartesian product in which all value pairs are inconsistent. optimal pairs are probably too hard to find  but we might have heuristics for finding good ones.  contrast this approach with the work in  hubbe and freuder 1   which utilizes the cartesian product of consistent pairs of values.  to some degree we are trying to  extract  the tight constraint  or part of it. this may in some sense loosen the a-b constraint  making it perhaps more likely that what remains will succeed. 
　call a subproblem  s  involving some of the values  for some  but not all  of the variables  a subset  subdomain subproblem. s extends to a subdomain subproblem  s1 involving all of the variables  in which the domains of the variables not in s are not reduced at all. call s1 the extension of s. if s is unsolvable  its extension will be also. if we can identify an unsolvable subset  subdomain subproblem we can factor out its extension. as 
1 	constraint satisfaction 
an illustration we can generalize the earlier coloring example. 1-cliques  triangles  cannot be colored with 1 colors. for every 1-clique we can factor out a subproblem where 1 variables have the same 1 colors and the rest of the variables have all possible colors. another simple illustration: in the n-queens problem  a subproblem that includes a 1-queens problem cannot be solved. for example  consider a subproblem in which the first three rows are restricted to take values from the first three columns. 
　alternatively  we can extract subproblems that we suspect have solutions. this will enable us to focus on such subproblems early. if we know that a subset  subdomain subproblem s does have a solution  we can extract its extension. we are not guaranteed the extension has a solution  but it might be a good place to look. if we know an actual solution for s we can extract a subjiroblem where the values for the s variables are the solution and all values are available again for variables not in s. again  this might be a good place to look. this idea can be extended if we have the cartesian product representation of a set of solutions  hubbe and freuder 1   where each tuple in the cartesian product is a solution. again we can extract a subproblem where the values for the s variables are the values in the cartesian product set  and all values are available for variables not in s. to some degree here we can again try to  extract  a tight constraint  or several of them   but this time by considering a subproblem where the only values remaining for the variables involved form a cartesian product set of solutions. since all pairs are possible in the subproblem the constraint is effectively eliminated. 
　the extraction method could also be useful if we have other reasons to want to work on s first  or last   earlier  or later   in the search for a solution. we might even wish to factor out a subproblem that contains possibilities that are not considered to be of interest at the moment  or alternatively embody current preferences. another way to get good subproblems to try first might be to extract a subproblem where all the values were loosely constraining and/or constrained. ideally this would mean loose within the subproblem  as opposed to within the problem as a whole  but that seems harder to achieve. for example we might collect the values that relatively speaking are most consistent with other values in each domain and extract that subproblem to work on first. notice that this is related to  but not equivalent to  the idea of value ordering for  succeed first . our scheme allows us to try all the  easiest  combinations before we involve any of the less likely values  and still know exactly what we left out  in case we do not find a solution with these values  or want to look for more solutions. alternatively we could try to recognize good subproblems to put off examining  by extracting subproblems where all the values were tightly constraining and/or constrained. or we could try to be more sophisticated yet about identifying subproblems with characteristics that strongly suggest the existence or absence of solutions. 
　work on  really hard  problems has provided considerable insight in this regard  cheeseman et al. 1  williams and hogg 1   which has been tested on various types of csps. likelihood of solution has been related to the tightness and density of constraints. likelihood of solution has also been related to problem difficulty. really hard problems have often been found on a  ridge  in  tightness/density space  between a region where problems are very likely to have solutions and a region where they are very likely to be unsolvable. we could try to extract in such a way so as to move pieces of the problem outside these really hard problem parameters  by trying to raise or lower constraint tightness. starting with an overall hard problem we might try to extract subproblems that were not hard or isolate the hard part of a problem in a smaller subproblem. this strategy might be particularly useful for inhomogeneous problems. 
　constraint density could also be adjusted. earlier we considered extracting subproblems in which all pairs of values between some variables were consistent. if the domains of x and y in the subproblem consist only of mutually consistent values then there is effectively no constraint left between x and y in the subproblem and it can be deleted. following up on this notion of  eliminating constraints  in subproblems  we can try to extract out subproblems with desirable structure. by deleting enough constraints in this way we can try to reduce to a tree or 1-tree  for example  freuder 1 . 
　note that the factoring out decomposition can be applied recursively. for example  we might try 1coloring hard graphs  factoring out 1-cliques  and doing some  forward checking -type local consistency processing whenever a domain is reduced to 1 element in a subproblem  nadel 1 . we have seen how the factoring out process can be applied repeatedly during search. extraction may prove particularly useful for dynamic csps  or  families  of related csps  where we have information about subproblems left over from previous experience  and for inhomogeneous csps  where we can extract hard or easy pieces .  we need to consider overhead of course  but some required information may come  free . the fof decomposition uses only the constraint checks normally performed by forward checking.  
　our extraction mechanism provides an opportunity for practitioners to utilize domain-specific knowledge about subproblems. this knowledge may be available a priori  such as the simple  theorem  about uncolorable subproblems used in section 1  or it may be acquired knowledge. the acquired knowledge may be obtained and used while solving a single problem  as is the knowledge used by fof  or it might be acquired while solving an initial set of problems and then applied to enhance future performance in the same domain. 
1 	conclusion 
we have introduced a general method for disjunctively decomposing a constraint satisfaction problem so that one of the resulting subproblems will be any specified subdomain subproblem. we suggested a number of uses for this decomposition and implemented and tested one that factors out unsolvable subproblems discovered during search. 
acknowledgements 
we wish to thank gerard verfaillie for pointing out to us that idc decomposition generalizes to subdomain subproblem extraction and for further helpful discussion. 
