 
planning in any realistic setting requires the management of an enormous amount of information. this information is generally temporal in nature; prediction  plan choice  and debugging all involve reasoning about time. the assertions manipulated by traditional predicate-calculus data base systems  such as prolog  are timelessly true. in the temporal data base system described in this paper  the classical data base assertion is replaced with the notion of a tunc token. for any given event or fact type  the data base will typically contain a large number of time tokens of that type. these tokens correspond to different occasions when an event of that type occurred or a fact of that type was made true and remained so for some period of time. this profusion of time tokens of the same type presents a problem for systems supporting temporal deductions of the sort needed in planning. many routine planning operations must search through the data base for tokens satisfying certain temporal constraints. to expedite these operations  this paper describes a computational framework in which common-sense strategies for organizing temporal facts are exploited to speed search. 
	i. 	introduction 
representing and reasoning about time play a critical role in many facets of everyday problem solving. we continually have to make reference to what has happened  is happening  and might possibly happen. to make matters more difficult  we have to cope with the fact that the world is constantly changing around us. to plan for the future we must be able to predict change  propose and commit to actions on the basis of these predictions  and notice when certain predictions are no longer warranted. all of this requires handling an enormous amount of complexly interdependent information. 
     in the past  the problems of efficient deduction and reason maintenance in predicate-calculus data bases used for planning have largely been ignored as researchers have grappled with the basic issues of reasoning about deadlines  iteration  conditionals  backtracking  and the like. 

1
¡¡¡¡ this work was supported in part by the national science foundation under grant iri-1 and by an ibm faculty development award. 
1 	reasoning 
as our representations have become more sophisticated and our ambitions to tackle more realistic domains have grown  the problems inherent in managing large temporal data bases have become a major factor limiting growth  . what is needed is a computational framework in which common-sense strategies for organizing temporal facts can be easily exploited to expedite the search needed to support basic temporal inference procedures. as a simple example  suppose that you are planning a business trip and you are trying to remember if the travel agent has already confirmed your airline reservation. it shouldn't be necessary to recall  i e.  search through  all of the events past  present  and future that involve your communicating with a travel agent. only the most recent are likely to be of interest. restricting attention to a particular time frame requires that facts that change over time are indexed temporally  that is to say  stored in such a way that facts and events common to a given time frame are easily accessible from one another. you would also like to avoid expending energy on facts that have nothing to do with travel agents or airline reservations. this requires that facts be xndexed syntactically. 
     temporal indices  unlike their syntactic counterparts  are subject to frequent revision. quite often the span of time associated with an event or fact is shifted  compressed  or expanded to suit a change of plans or reflect new information. suppose that you decide to leave on your business trip a week earlier than previously planned. from this change  it should be apparent that certain prerequisite tasks  e.g.  ordering plane tickets  must also occur earlier. often  such a change will not require a major revision of plans. where modifications are needed  however  the data base should clearly indicate just what parts of a plan have to re-thought and why  e.g.  you might have to arrange for transportation to the airport in the event that the earlier departure will upset plans to share a ride with a friend . 
     to make accurate predictions about the future it is important to have access to information about the past. information about the past helps us to understand what went wrong in situations where our actions fail to achieve their intended effect or our predictions fail to coincide with observation. information about the past also helps us to resume interrupted or suspended tasks. even if we were to reconcile ourselves to throwing away all record of the more distant past  we still have to consider that there are 
thousands of facts that we know are currently true and that are likely to figure in guiding our behavior. in addition  we generally keep track of a great many facts corresponding to long and short term goals and to predictions about the near and distant future. in any reasonably complex domain  the number of temporally dependent facts necessary for minimal competence is large. this paper describes how to go about organizing and maintaining these facts. 
	ii. 	thesis 
 describes an approach to reasoning about time and a temporal data base management system that supports temporal reasoning of the sort necessary for planning. the temporal data base is called a time map   and the data base system is called a time map manager or tmm. in the tmm  the classical data base assertion is replaced by the notion of time token corresponding to a particular interval of time during which a general type of occurrence  a fact or event  is said to be true. for any given fact or event type  the time map will typically include many tokens of that type. the most expensive operation distinguishing temporal data base manipulations from those performed by static data base systems  e.g.  prolog  involves finding tokens that satisfy certain temporal constraints1. this operation  which we will call token retrieval  is the temporal analog of fetching assertions in the data base that match a given pattern. token retrieval requires the system to search through the data base for time tokens whose type matches a given pattern and whose associated interval spans a specified reference interval. to support backward chaining efficiently  token retrieval should be demand driven: a call to the token retrieval routine should return one token  if possible  and a suspended procedure call  or continuation  which  if resumed  will supply additional tokens by taking advantage of the effort expended in finding previous tokens. to maintain integrity  all operations on the data base  including token retrieval  should supply succinct descriptions of the reasons why they are to be believed. this allows that all deductions performed by the system can be made contingent upon continued belief in the reasons supporting their component steps. 
     the primary claim of this paper is that  in many common situations  temporal data bases can be organized so that the cost of token retrieval is comparable with assertion retrieval in static data bases. restricting our attention to the problem of retrieving a single token of ground type1 p spanning the interval /: 
1. in situations in which the set of tokens can be partitioned into temporally distinct periods  e.g.  months  years  factory work shifts   the cost of token retrieval is proportional to the sum of: 
 a  the number of periods separating the period porigin containing the beginning of / and the first period pdestination preceding the beginning of i containing a token of type p  and 
 b  the cost of determining if any token of type p in pdestination spans i. 
1. in situations in which the data base can be organized according to hierarchies of constraints  e.g.  constraints between a task and its sub tasks   and these constraints are metric and reasonably tight1  then the cost of determining if any token of type p in a given period spans /is proportional to the number of tokens of type p within that period. 
     as with any scheme for speeding up retrieval  there is a cost associated with organizing the data to support these fast retrieval routines. fortunately  much of the work required for organizing time tokens is already being handled by other routines in the time map. whenever a new constraint is added or removed  the system has to determine how the changes affect the current set of beliefs. this is the basic function of temporal reason maintenance  and it is essential for almost any interesting application of temporal reasoning . the process of detecting changes in the set of beliefs is performed by propagating constraints through the constraint network. essentially the system tries to compute new estimates  tighter bounds  on the distance in time separating selected pairs of points in the time map. these new estimates  referred to as derived constraints  can be used to either license new deductions or undermine old ones. the  derivation  complexity of a derived constraint is proportional to the length of the shortest path through the constraint network from which the associated distance estimate can be computed. if we assume that there is some bound  call it maxderivedlength  on the complexity of derived constraints necessary to catch all crucial changes in the set of beliefs  then we can perform constraint propagation in 1 m1pt log mpt   where pt is one of the points being constrained and mpt is the number of points reachable from pt by paths of length   maxderivedlength. in practice  mpt is generally quite reasonable. 
     the organizational schemes described in this paper are integral with the basic functionality of the tmm; they instigate reorganization correctly in response to the addition of new information or the deletion of old  and they serve to expedite the basic operations used in temporal reason maintenance. the techniques involve methods for partitioning the set of time tokens both temporally and syntactically and for caching selected derived constraints and noticing when certain distance estimates become licensed by the current set of constraints or cease to be so. if we accept that the basic operation of temporal reason maintenance is essential  then the additional overhead in time and space necessary to implement these organizational techniques is just a small constant factor times the number of tokens stored in the data base. 
     complexity measures are generally misleading indicators of expected performance. the basic problem of representing temporal information using linear constraints has been studied by a number of researchers    ; most of the proposed techniques involve some sort of fairly straightforward polynomial-time algorithm. unfortunately  even n1 is out of the question for large n  where n corresponds to the total number of points or intervals being considered  and our experience  indicates that n will typically be on the order of several thousand. as soon as you add disjunction  the problem becomes npcomplete   but  again  asymptotic complexity doesn't 
tell the whole story. in the absence of a thorough analysis of the sorts of inference supported by a temporal reasoning system  evaluating such a system is impossible. in   we provide a detailed account of the inferential capabilities of the tmm  including special purpose techniques for handling disjunctions  default rules  and both antecedent and consequent reasoning. in this paper  we are primarily interested in the techniques required to expedite retrieval in large temporal data bases. we take as given that token retrieval cannot be a function of the size of the entire data base. 
	iii. 	temporal data base management 
the tmm consists of: 
1. a data base  called a time map  that captures what is known about events and their effects over time. in particular  time maps are used to record information about the truth of propositions that change over time. 
1. a query language that enables application programs to construct and explore hypothetical situations. this language supports simple queries of the form   is p true at time r    as well as more complicated queries of the form   find an interval satisfying some initial constraints such that the conjunction  and p1 ... p   is true throughout that interval.  
1. a set of techniques for extending the information in the data base. these techniques allow for predictions on the basis of temporal antecedent conditions. predictions added to the data base in this way are made to depend upon the antecedent conditions in a meaningful way. 
1. a mechanism for monitoring the continued validity of conditional predictions. this mechanism extends the functionality of reason maintenance systems  to temporal domains. 
     a time map is a graph. its vertices refer to points  or instants  of time corresponding to the beginning and ending of events. one point is related to another using constraints where a constraint is represented as a directed 
edge linking two points. each edge is labeled with an upper and lower bound on the distance separating the two points 
1 	reasoning 
in time. these bounds allow us to represent incomplete information concerning the duration and time of occurrence of events  e.g.  unloading the truck will take between 1 and 1 minutes . any two points can be related by finding a path from one point to the other  where a path from pt1 to ptn is just a sequence ptoclpt1 ... cnpin such that pto through ptn are points and c  is a constraint relating pt-  to pti. new constraints are added to the time map by making assertions of the form  elt  distance pt  pt1  low high  where this is meant to indicate that the quantity corresponding to  distance pt  pi1  is an element of   it  the closed interval low to high. 
     an interval is just a pair of points. a type is denoted by a formula like  location obj 1 locl1  or  move obj1 locl1 locl1 . a time token  or simply token in situations where it should cause no confusion  is an interval together with a type   begin tok  and  end tok  denote the begin and end points the time token tok. predications of the form  occurs type token-name  are used to create new time tokens and refer to existing time tokens of a given type. the token-name gives us a handle so we can speak about the interval associated with a particular time token. 
	iv. 	indexing time tokens 
in this section  we will be concerned primarily with strategies for discriminating on data in order to expedite retrieval. a discrimination corresponds to a question  or deduction  concerning the form or content of the data. on the basis of the answer to such a question  the data is usually partitioned into disjoint sets so that if a program attempts to retrieve an item whose content depends on the answer to this question  then the system will know exactly where to look. this process of discriminating on data  asking questions and then partitioning according to the answer  can be thought of as caching the results of deductions that are likely to be frequently needed. to be useful  discriminations should substantially reduce search with a minimum overhead. not all discriminations can be depended upon to remain valid as the data changes over time. where the data is subject to change  there is an additional expense involved in keeping track of valid deductions. 
     the information content of a time token corresponds to the syntactic form of the token's type and the temporal extent  or scope  of the associated interval of time. deductions corresponding to syntactic discriminations on the type of time tokens are never invalidated  though their utility may be undermined as tokens are removed from the database . all time tokens are indexed through what is called a discrimination tree or dtree . each nonterminal node in a dtree corresponds to a discrimination: a question whose answer determines which subtree various data items are stored in. each terminal node in a dtree corresponds to a set  or bucket  of data items determined by the discriminations on the path leading from the root of the dtree to the terminal node. 
     discrimination is demand based in the tmm. if the size of a bucket exceeds some fixed threshold  the tmm will attempt to subdivide the bucket by adding an additional discrimination node and some number of terminal nodes as dictated by the chosen partitioning scheme. if it is possible  partitioning a bucket of tokens is based upon a syntactic discrimination according to the types of the tokens stored in the bucket. if further syntactic discrimination is either impossible or undesirable  the system attempts to discriminate on the basis of the temporal scope of tokens. temporal discrimination in the tmm involves choosing a temporal partitioning scheme and subdividing the overly large bucket of tokens according to this scheme. in the tmm  the application program is required to supply a set of hierarchically arranged temporal partitions of a time line  i.e.  the real numbers  such that all of the partitions can be related via a single global frame of reference  i.e.  1 . attempts to derive an adequate partitioning scheme solely on the basis of the current contents of a bucket have proven difficult . the partitioning scheme chosen must essentially anticipate the sort of questions that will frequently be asked during token retrieval. in the factory domain  the partitions supplied by the fohbin planner  correspond to weeks  days  eight-hour work shifts  and one-hour intervals. the system discriminates as demand dictates  starting with the coarsest partitions and refining only as required. 
     the partitioning scheme described above corresponds to a set of successively finer partitions of time with respect to a single clock. this is enormously useful as a coarse grained filter. unfortunately  many events cannot be tied to a precise time  though they can be related precisely to one another. for instance  in reasoning about a chemical process  you may not know exactly when a catalyst was added to a reactor vessel  but you do know that within 1 to 1 minutes following the addition of the catalyst the reaction was complete. the tmm provides a mechanism for specifying hierarchies of event relations that can serve to guide search in determining temporal orderings among points that are not precisely known with respect to the global frame of reference of the partitioning scheme. the most common strategy involves the use of the event/subevent hierarchy. if e1 is specified as a subevent of e1  then the tmm can guarantee  within certain limitations  that there exists an edge in the time map connecting the beginning of e1 and the beginning of e1 labeled with the best bounds on the distance in time separating the two points. 
     these edges constitute cached deductions and are handled by the same mechanisms used in  to ensure correct behavior with regard to the addition and removal of information. the token retrieval machinery takes advantage of these cached deductions to speed search in determining the relative ordering of tokens which are not distinguished in the dtree by temporal discriminations. these combined searching and caching techniques guarantee that under cer-
tain conditions1 the machinery for determining the bounds on the distance between two points will always return the best bounds and will do so in time proportional to the depth of the hierarchy. it is also possible to show that the system never reports false bounds and that the behavior of the system degrades gracefully as the information becomes less precise and the maxderivedlength increases beyond the fixed threshold. for most problems the depth of the hierarchy is seldom greater than 1 and the alternative exhaustive search would cost on the order of n1 where n is the total number of tokens in the partition  often on the order of several hundred . 
     token retrieval routines use the dtree to provide a set of candidate tokens and then determine the relative ordering of the beginning of these tokens using the search methods described in the previous paragraph. determining the duration of a fact token relative to a reference interval is also accomplished using search methods that exploit the hierarchical partitions and cached distance estimates. all the search routines return the information requisite for setting up appropriate data dependencies. searches corresponding to different token retrieval requests can be coroutined to support efficient backtracking during backward chaining. 
	v. 	algorithmic details 
 the tmm employs a heuristic graph traversal routine to compute bounds on the distance separating pairs of points in the time map. these bounds are used to determine relations between pairs of points and intervals. estimates of the distance between pairs of points are computed by finding paths through the network of constraints. recall that a path in the time map is a sequence of points and directed edges corresponding to constraints. each edge c is labeled with an upper and lower bound  denoted low c  and high c  respectively. for each path high c  . in 
computing the best bounds  the heuristic graph traverser tries to find the paths with the greatest lower and least upper bounds. the details of the tmm's graph traverser are described in   and won't be repeated here. for a discussion and analysis of existing constraint propagation techniques for applications in artificial intelligence see . 
     in addition to asserting constraints between pairs of points corresponding to the begin and end of time tokens  it is also possible to constrain the begin or end of a time token with respect to the global frame of reference  mentioned in the previous section . figure 1 shows a time map and the time line corresponding to a fixed global frame of reference; five tokens  notated i and their corresponding beginning points  pt  ... pt1  are labeled for easy reference. constraints are depicted as curved lines  e.g.  those labeled  the points in the shaded area 


figure 1: time map with privileged frame of reference 
 e.g.  pt1  can be related to points outside the shaded area  e.g.  pt1 or pr1  only through the constraints c  and c1 and the point in time corresponding to the frame of reference of the global time line. given that one or both of t1 and t1 are of type p  paths through the global frame of reference will be important in determining if p is true throughout the interval associated with 1. for relating pairs of points isolated in the same cluster of tokens  e.g.  pt1 and pt1   the global frame of reference may not help in establishing the necessary distance estimate  but  as we will see  the global frame of reference provides an invaluable service by allowing the search routines to ignore large portions of the time map. 
     in order to expedite token retrieval  the tmm precomputes and stores  i.e.  caches  certain point-to-point distance estimates. caching is performed as part of the constraint propagation routines used to implement tern poral reason maintenance. the tmm can maintain an estimate  guaranteed exact under certain assumptions  see    of the distance in time separating selected pairs of points. obviously  it would be wasteful to cache distance estimates for all pairs of points. selective caching  on the other hand  can provide real benefits. in section vi.  we will see how caching estimates of the distance between each point and the global frame of reference forms the basis for an effective temporal discrimination scheme. in   we demonstrate how a strategy for caching distance estimates between points corresponding to related tokens can expedite search within portions of the time map that are not highly constrained with respect to the global frame of reference. 
v i . 	hierarchical partitioning schemes 

figure 1: portion of a strict hierarchical time-line partition 
be more restrictive than  with respect to / just in case  and there is a subset s of p  such that 
and 1 partitions j. a hierarchical partitioning scheme is strict if it is the case that for any i such that  if  is more restrictive than p  with respect 
to /. all the partitioning schemes we will be looking at in this paper are strict. 
     a time-line partition is just a partition of r such that 1 is identified with a particular frame of reference  e.g.  midnight on april 1  1  and each real number x corresponds to an offset in time from this global frame of reference as measured by a particular clock. the global frame of reference simplifies internal bookkeeping and provides a basis for using dates in specifying constraints. figure 1 shows part of a strict hierarchical time-line partition in which the partitions consist of weeks  shifts  eight hour periods   half-shifts  four hour periods   and one hour periods offset from a fixed zero point. in figure 1  c is contained in a and b  and is partitioned by {d e f g}. 
     specifying constraints with respect to the global frame of reference is made particularly easy using dates. a date is just an offset from the global frame of refer-
ence specified in terms of the current partitioning scheme. for instance  in the scheme mentioned in the previous paragraph  the date   veeks 1   days 1   shifts 1   hours 1   minutes 1   1 is easily converted into an offset in minutes from the current global frame of reference. often  it is convenient to specify a default date  e.g.  noon today  and then specify offsets  called reldates  relative to the default. dates and reldates can appear anywhere in a formula that a point can. as an example  if the default date is noon today  asserting  eit  distinct  begin task1   reldate  hours 1   minutes 1    -1  determines that task1 begins between 1 and 1 this afternoon. the tmm employs simple rewrite rules to translate between various in-

ternally used partitioning schemes and computationally cumbersome  but familiar  methods of dating based upon the modern calendar. 
     every point is identified with a tuple  low high   called its relative offset  indicating the best  lower and 
 partition divisions not mentioned in a date default to 1. 
1 	reasoning 

upper  bounds over all paths through the network of constraints on the distance in time separating the point from the global frame of reference. relative offsets are updated during constraint propagation. the constraint propagation routines ensure that  if additions or deletions to the set of constraints require that the relative offset of a point be updated  i.e.  the bounds made either more or less restrictive   then the system can easily detect this and respond appropriately. the relative offset determines how tokens are indexed in the tmm discrimination tree. to simplify the discussion of temporal indexing  we will assume that all tokens are syntactically indexed down to atoms according to type  and consider only those nodes in the discrimination tree corresponding to temporal indices. a temporal index is implemented as a data structure called a tbucket consisting of a set  possibly empty  of tokens that can't be further discriminated upon  a partition interval  a b   and a set of subindices  i.e.  tbuckets  sorted by their associated partition intervals. for efficiency reasons  subindexing is usually postponed until the set of tokens stored at an index exceeds some fixed threshold. figure 1  shows a simple hierarchical partitioning scheme and a table indicating the relative offsets for five tokens of the same type. figure 1 shows a portion of a discrimination tree.  recall that  is the singleton set corresponding to the entire time line.  note that  could be further discriminated  but is not in this case since the tbucket containing it would otherwise be empty. if the relative offset of the beginning of  was changed from  to 
 1 1   then t1 would be further discriminated ending 

figure 1: search in hierarchical partitions 
up in the same bucket with t1. if  on the other hand  the relative offset was changed to  1 1  or  1  +oo   then t1 would end up  respectively  in the same bucket with t1 or in the top-most index corresponding to po . 
     now  it is straightforward to describe the algorithm used in the tmm for token fetching during temporal backward chaining. the algorithm makes use of two sorts of indexing in performing the requisite search: direct-path indexing and indexing relative to the global frame of reference. the steps in the algorithm are: 
1. determine a partition v and an interval i belonging to v such that the fetch interval is constrained to 
 necessarily  begin during i and cannot be shown to  necessarily  begin during any interval belonging to a partition more restrictive than v. 
1. using the heuristic graph searching routines  try each matching token whose associated partition interval either is contained in i or contains j. in figure 1  the shaded areas correspond to the partition intervals searched during this step. if a token is found that either begins before or can be constrained to begin before the fetch interval  then an attempt is made to determine whether or not that token persists throughout the fetch interval. relative offsets and the heuristic search routines are used to determine the relative ordering of the end point of the found token and the end point of the fetch interval. 
1. if the previous step fails to find an appropriate token or set of tokens then we search through the remaining tokens of the desired type as follows: 
 a  set the variable early-termination to false. 
 b  determine a partition interval  call it n  preceding i such that there is no other unexamined partition interval preceding / which is either later than n or in a less restrictive partition. 
 c  for each matching token in at  use the relative offset from the global frame of reference to determine if the token persists long enough. 
 d  mark n as examined. 
 e  if a matching token is found that fails to persist throughout the fetch interval because it has been 
	dean 	1 

clipped by a contradictory token  then set earlytermination to true  
 f  if early-termination is true and n is an element of the most restrictive partition  then stop  otherwise return to step 1b. 
     figure 1 illustrates the order in which partition intervals are examined during step 1 of the fetch. the starred partition interval is meant to indicate where the variable early-termination was set to true. 
     the above indexing scheme relies heavily upon their being a single global frame of reference and a rather simple and restrictive hierarchy of partitions. the fact that the partitions are strictly nested  i.e.  for each then there is a set of intervals in p +j that partitions i  can result in certain inefficiencies at the boundaries of partition intervals. points that are slightly unordered with respect to a major time break will be assigned a fairly unrestrictive partition interval even though their relative offset is known with considerable precision. for instance  an event constrained to begin at midnight january 1 give or take a minute will end up in the bucket corresponding to the decade partition  assuming a partition according to decades  years  months  weeks  days  and hours. such events cause the system to do a bit more work  but since they are relatively rare  the overall effect is negligible. tokens whose begin points are known with some precision  i.e.  the difference between the lower and upper bound of the relative offset is small   but which  nevertheless  are unordered with respect to major time breaks might be handled by considering pairs of partition intervals adjacent to the time break and indexing the token in more than one interval  but no attempt has been made to implement such a strategy in the current system. similarly  the beginning of fetch intervals can also span major time breaks and for this reason it is often useful to split the fetch and perform the requisite search on a case-by-case basis. 
	vii. 	related work 
the techniques described in this paper have profited from many sources.  describes a discrimination network capable of indexing spatial objects on the basis of metric information. methods for organizing intervals hierarchically have appeared quite frequently in the literature  . there have also been a number of strategies suggested for guiding search in reasoning about time  . a careful reading of the discussion of constraint propagation techniques in  convinced us that  while the general problem of reasoning in large constraint networks is hopeless  in most practical situations involving time  the requisite search can be directed with amazing precision. 
	viii. 	conclusion 
the tmm provides a wide range of functionality  backward and forward temporal inference  dependency directed default reasoning  temporal reason maintenance  in a simpleto-use system  predicate-calculus syntax and prolog com-
1 	reasoning 
patibility  in which routine temporal reasoning is optimized using sophisticated caching and search techniques to speed inference. straightforward partitioning schemes supplied by an application program are used to fragment a temporal data base into non-overlapping periods. under situations that arise frequently in everyday reasoning  token retrieval  the basic operation common to most forms of temporal inference  can be performed in time proportional to the sum of  a  the number of periods separating the period containing the beginning of the reference interval and the first previous period that contains a token of the desired type  and  b  the number of tokens of that type beginning in that previous period. the result is that the performance of the temporal inference engine corresponds roughly to our expectations given the distribution of tokens of the underlying fact types being manipulated. the discussion of techniques in this paper is necessarily cursory. a technical report  provides additional detail concerning both the algorithms and their expected performance. 
