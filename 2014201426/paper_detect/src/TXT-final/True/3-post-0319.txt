 
	ricardo m. araujo＞	and luis c. lamb
institute of informatics  federal university of rio grande do sul
　　　　1  porto alegre  brazil email: rmaraujo inf.ufrgs.br  luislamb acm.org
　
1 introduction
this work presents a study on the effects of different memory sizes in the minority game  mg  market model  zhang  1 . we analyse the effects on an agent's performance when this agent is endowed with a different memory size with respect to other agents in the game. our aim is to identify in which situations a large or small memory might be advantageous in the game. from the obtained results we argue that there exist convergence to and evolutionary stability around certain memory sizes and we consider an evolutionary setup which confirms our hypothesis.
　the mg is defined as an odd number of agents  n  which must choose at each turn of the game whether they will be in one of two possible groups. after all agents have made their choices  agents in the minority group are rewarded. in order to make a decision  agents use an inductive learning algorithm in order to try to exploit patterns from previous outcomes. the number of past turns that agents consider to decide defines their memory size  m .
　one of the main properties in a mg is its efficiency  which is evaluated by the statistical variance  σ1  of the number of agents in a group  moro  1 . efficiency is known to be a function of a control parameter   manuca et al.  1 . for n fixed  m becomes the main control parameter of the game. figure 1 shows σ1 for a range of memory sizes. three regions are observed:  i  for small m variance is very high  above the expected for the random case game  which we will call inefficient region ;  ii  for high values of m  variance is exactly the one expected for the random case game  random case region ;  iii  for intermediate m the system presents small variances  efficient region .
1 memory size and performance
firstly  we tackle the design aspects of agents by asking what is the best memory size for an agent immersed in a mg where agents have a different memory size. we do so by modifying the typical mg setup  defining two basic elements:  i  the environment  a classical mg with an even number  n  of agents;  ii  a target-agent endowed with a possibly different memory size  m  from that in the environment. we have simulated different target-agents within environments in all 1 efficiency regions  taking n as a large value  so that the target-agent plays no role in the determination of the game's

figure 1: variance versus memory size
efficiency. we are interested in the target-agent's success rate  number of correct decisions over total number of rounds .
　figure 1 shows the success rate for different values of m for the environment in the inefficient region  m = 1 . it is shown that the target-agent with larger memoriesperforms better than average. this agrees with the common belief that having access to more information is beneficial. figure 1 shows that this is not always the case. for m = 1 the environment is now in the efficient region and increases in the target-agent's memory size does not translate into better performance. actually  having access to more information is shown to be harmful.
　finally  we have simulated our target-agent in the random case region. we have observed that the target-agent showed no gains or losses of performance throughout all values of m. this is mainly due to the fact that if agents are acting as if they were choosing randomly  as is the case in this region  then there will be no pattern in the outcome time series to be exploited.
　figure 1 depicts the gain    of a target-agent with one more bit of memory than the environment for a range of m  i.e. m = m +1 for all m . we notice that gains are reduced as the system approaches the efficient region and indeed become smaller than unit for every m in the efficient region  reapproching unit when closer to the random case region. it is only in the inefficient region that gains become larger than unit and agents with more memory are able to
figure 1: target-agent's success rate in the inefficient region. dashed line is the average success rate of the environment.

figure 1: target-agent's success rate in the efficient region.
exploit the environment. interestingly  the same experiment repeated having m = m 1 shows that smaller memories makes no difference to the target-agent's performance  it becomes the same as the environment's average.
1 evolution of memory size
now we apply our results in an evolutionary setup  in a mg where mutant agents - endowed with slightly different memory sizes - may appear and replace badly performing agents. let the system be initially in an inefficient phase. our experiments have shown that an agent with a larger memory would be able to exploit the environment in this region while smaller memories provide no benefits. thus  agents with larger memories would be able to survive and average memory size would increase until the point where further increases lead to no gains. this happens when the system enters the efficient region. m1 denotes the memory size in which the system first becomes efficient. the system cannot move beyond m1  as larger memories would cause harm to the mutant's performance. in addition  the system cannot move towards lower memory sizes: whenever a sufficient number of agents convert to smaller memories  the system would again be in an inefficient region and the push towards larger memories would reappear.
　a similar reasoning might be applied to a mg starting at any point m in the efficient region  above or equal to m1.
figure 1: gain of a target-agent with m=m+1.
a mutant agent with larger memory would be penalized  but one with smaller memory would perform just as well and we could expect that memory sizes would end up being distributed throughout values below m. however  this would result in a reduction of the average memory size  causing the system to move towards the inefficient region and making larger memories harmful  thus creating a new upper limit in memory size. the process continues until m1 is reached.
　we may say that m1 is not only evolutionary stable  as no mutant agent can do better than the agents already in the environment at this point  but it is also an attractor of memory sizes in the efficient and inefficient regions. by starting in any point in these regions  the system is expected to converge to m1. this reasoning was indeed confirmed by simulations. however we did not have a convergence towards m1 =1 as it would be expected by analysing the traditional mg  see fig. 1 . we have observed an initial  arms race  of memory sizes up to m = 1 and then a fall towards a final equilibrium at m =1. this may be explained if we consider that two adaptations happen  adaptation of memory size and adaptation of strategies  for when an agent modifies her memory size she also modifies her strategies. since the strategies space is much larger than the memory sizes space  adaptation of the latter is faster. thus  at the beginning of the simulation we indeed have m1 = 1 and  as strategies adapt  a different dynamics emerge  araujo and lamb  1＞    with a different value of m1.
　acknowledgments: this work was partly supported by capes  cnpq and fapergs  brazil.
