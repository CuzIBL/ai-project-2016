 
this paper is about rule-based problem solving. it has a motto: the less a rule can do  the better the effects of using it can be predicted. 
this motto suggests the key to obtaining coherent behavior by avoiding exponential behavior  combinatorial explosion  in a problem solving program for the domain of synthesizing numerical computer programs from their non-procedural specifications 
introduction 
　　　　to construct an automatic programming system  several separate but related sub-domains and their corresponding experts are required. these include program understanding  specification acquisition  program verification  and program synthesis. the last is the domain of expertise of the system described by this paper. 
　　　　a program synthesis system takes a specification  for example  a predicate calculus description of the relationships between the inputs and the outputs  of a program and produces a program meets those specifications. diagram  bernoulli example  shows an input specification  in mathematical notation  and the lisp code 
produced by the system. 
　　　　the fundamental difficulty in constructing problem-solving systems is the so-called  combinatorial explosion  problem. that is to say  given a reasonable measure of how hard the problem at hand should be  a  combinatorially explosive  problem solver takes an amount of time proportional to an exponential  or worse!  function of that measure in general  the size of program  and therefore certainly the time needed to write it  is an unbounded function of the size of its specification for purposes of measuring the coherence of a problem solver  a measure of  how hard a problem is  could be the number of rule applications that were needed  as opposed to the larger number actually performed  in the solution found. 
the result of this research is a collection of techniques 
 embedded in a synthesis system  that synthesize numerical computer 
key principles 

programs. these techniques do not depend strongly on the fact that the task is synthesizing programs  so there is hope that they could be adapted to problem-solving in other domains. i conjecture that these techniques in fact run in time proportional to a polynomial function of the number of rules required to solve a synthesis problem a working program was obtained that  on a relatively unloaded pdp-io  can solve several examples faster than an expert human programmer. 
this report describes research done at the mit ai laboratory. 
support for the laboratory's artificial intelligence research is provided in part by the advanced research projects agency of the department of defense under office of naval research contract n1-c-1. 
1 
there are three basic principles on which this research is based: 
i. synthesis by analysis of constraints. since the use of  networks of constraints  in statlman and sussman's el system  s1   this formalism has become increasingly popular this research uses the idea of propagating code fragments  in addition to the usual arithmetic and algebraic expressions  through a network of constraints in order to write code. stated another way  the  data base  of constraints connected together to form a network is interpreted many different ways by the synthesis system. 
1 diagrams provide insight. in terms of the actual implementation  synthesis problems are of course presented as text strings. internally  the problem is represented as a  network of constraints.  what might be termed 

 topological configurations  in this internal representation seem to be the real key in deciding what deductions to perform next. by extensively using the diagrams of networks of constraints  much insight into the recognition and use of these  topological configurations  was obtained 
1. combinatorial blow-up should be the central concern people can perform the task of writing code given specifications what ever this task actually involves  people do not suffer from combinatorial blow-up. this observation should be the starting point for an attempt to get computers to perform program synthesis: if a combinatorial amount of work is required to perform the task  then the task must be reformulated. 
in developing a program synthesis system it is crucial to avoid trying to solve problems whose solution time is exponential  or worse  in the length of the solution derivation  solution time and length refer to writing the code  not running the code  notice this is not the usual time-complexity measure. 
　　　　how can  combinatorial blowup  be avoided  to see  consider a detailed example proving a statement in the propositional calculus is not a tautology  this is known to be np-complete . the statement  a or b implies a  is not a tautology because it is false if a is  false  and b is  true . a proof is an assignment of  true  and 
 false  to the variables in the statement such that the entire statement is false. the best  known  algorithm for finding proofs like this is exponential in the number of variables  the length of the  solution   
　　　　one way to limit a deductive system  hoping to make it run in polynomial time  is to limit the expressive power of the  rule language.  the term  expressive power  refers not to  an absence of  syntactic sugaring  but to the existence of predicate calculus formulae that cannot  in any way  be translated into rules. as an example of such a restriction  consider a modification to the task of proving the non-tautologic status of certain propositional calculus expressions suppose the form of the expression is limited to implications of conjunctions of variables - expressions without the  or  connective and without the  not  operator  these are horn clauses   h1  . such an expression  like  a and b and c implies a and d   can easily be shown not a tautology by finding a letter variable  d in this case  on the right hand side of the implication that is not among the conjuncts to the left of the implication. if this letter variable is assigned  false  and all others assigned  true   the entire expression evaluates to  false   expressions of this restricted form can be proven non-tautologies in linear time  actually o n*log n   because of the time taken to look up variable names  
example of the system's capabilities 
 naturally one must be concerned that a program synthesis system with a restrictive rule language  and specification language  nonetheless still be able to solve  interesting  problems and write  interesting  code the synthesis system has solved examples concerning the general topics: 
i. systems of equations 
1 inverses and zeros of functions 
1 changing recursive control flow to iterative  and vice versa 
1. algorithms involving data structures 
the  example space  of numerical computer programming problems is very large  the list of example problems above is very small. nonetheless  this small list forces the system to demonstrate 
1 

one can interpret these expressions as declaring that certain relations hold among the variables x  y  a  and b  or as a rule to compute a or b from x and y. 
　　　　the rule language for the program synthesis system is based on a representation combining both the declarative and computational interpretations specifically  a *c device would be defined as having three terminals  arguments  named summandi  summand1. and sum. some constraint rules would be written telling that some terminals can be computed on the basis of others  and giving details about how that computation is effected: 
t e r m i n a l 
sum 
summandi 
summand1 computed 	terminals needed 
summan1.summandi 
sum.summand1 
sum.summandi expression 
 + summandi summand 1  
 - sum summand1  
 - 	sum summand1  the expressions can be thought of as lisp code  although the system actually uses a separate constraint rule language and interpreter system so that expressions can be evaluated not only to form code  but to provide time-costs  upper and lower bounds  and symbolic expressions 
form of local deduction  would examine the constraint rules of the two *c devices  named dl and d1  to see if any other nodes can be assigned values in this example no new values can be obtained  so some other  more global  deductions apparently must be made 
　　　　the diagram for this example contains a configuration called a circuit this circuit starts at node x  goes to device dl  direction is not important   then to node y  then to device d1  and finally back to node x. none of the nodes in the circuit have been given values  which is to say that the system doesn't know how to compute them yet . a key observation is that if a problem can be solved  but is not solved by propagation  then the output node must be in a circuit  or can be determined by propagation from nodes that are in a circuit  
　　　　a second key observation is that to solve a problem one wants  somehow  for circuits to get smaller  see  two linear equations solution   the system uses a library of transformation rules  or simply transforms  to make this happen. 
transformation rules 
1 

an important observation to make concerning this diagram is that if 
s is known and node r is not  then the left side of the diagram contains a circuit  with only one node r  but the right side does not the  doubling  transformation apparently  removes  a circuit when s is known and r is not. 
another transform  expressing the mathematical fact that 
 q +r *s= q *s *r 
is shown in diagram  funny associativity.  there is nothing special about this transformation  other than it happens to help solve the 
current problem written as constraints  this transform is 
　　　  +c r qd   +c d s e  =    +c s q f   +c r f e . the left part of this diagram  the pattern  matches the problem in the diagram  two linear equations  with the correspondence q-  y  r    b  s -  y  e -  a the result of applying the  funny associativity  transform is to add a copy of the instantiation network to the problem network  of course. this may trigger other rules -- combinatorial explosion is a danger the system controls  see below and  b1i  . a new node must be created for the node f because it does not appear in the left-hand pattern part of the transform. the solution is found in the data base after applying  funny associativity  followed by  doubling   diagram  two linear equations solution   
　　　　an important observation to make concerning the  funny associativity  diagram is that  in the left  pattern  part  there are exactly two circumstances in which using this transform will result in shrinking a circuit. these are first if nodes  q  and  s  are known  and nodes  r  and  e  are unknown  and second if nodes  r  and  e  are known and nodes  q   and  s  are unknown. the second situation is extant in the initial problem. 
limiting expressive power and combinatorial blowup 
the mathematical facts contained in the  funny associativity  and 
 doubling  are of the form that the left side  a conjunction  implies the right side  another conjunction . all transforms have this simple form disjunctions  use of or  and negations  use of not  are not allowed the expressive power of the transformation rule language has been limited furthermore  it should be remembered that propositional statements of this restricted form  implications of conjunctions  are precisely those for which the non-tautology proof was easy  i.e. horn clauses  
　　　　to see that the expressive power has been limited  notice that even though implication can be rewritten as shown  it cannot be used to encode disjunction 
　　　　　　　　a =  b is equivalent to ~a or b trying to encode a disjunction using negation doesn't work because negations are not allowed; 
　　　　　　　　 ~a  -  b  equivalent to a or b . direct use of negation is not allowed. but maybe indirectly one could encode negation 
c -   false   equivalent to ~c or  false   equivalent to ~c  
but consider what such a transformation rule would have to say: 
from c one can deduce that some constraint holds among some nodes when in fact it can never be satisfied! this violates the imposed condition that all constraints in a network are satisfiable so a disjunction cannot be constructed in this  or any other  way. 
　　　　by using this restricted form of transformation rule two separate kinds of combinatorial explosion  exponential behavior  have been avoided. the first concerns the general problem of proof by contradiction and data base splits. the second concerns legitimate but useless deductions. 
　　　　if a problem-solver is faced with a fact like  x implies y or z  a common reaction is  if x is true in the current data base  to assume y and see if a contradiction results. if one does  then  one way or another  the deductive system  backs up  and decides that  not y  and  z  are both the case. since in effect the problem solver is thus searching a tree  there is a potential for exponential behavior - an exponential number of data bases must be searched  in general  as a function of the depth of the tree. 
　　　　in sharp contrast  the transformation rule language has been restricted so that it is impossible to state a rule that might give rise to a tree of data bases reasoning by contradiction cannot be used by the deductive system because the rules are not expressive enough to say what should be done if a contradiction is found. 
　　　　one could worry that the number of rules might increase exponentially. even worse  that knowledge required might be unexpressible but this turns out not to happen. for example  looking at barstow's  ba1  set of rules  almost all are implications of conjunctions  and the exceptions can be restated in this form without increasing the number of rules 
　　　　another kind of exponential behavior arises in the problem of finding the value of a variable vf given the values of vi through 
vn when the following is true: 
　　　　　　　 vf   vi *   v 1 * ... * vf   ...    -1. suppose that the system can interchange two variables without changing the nesting. by applying the interchange operation  all sequences of vf and vi through vn can be generated  and there are 1* n*1 ' such sequences if the system can solve the problem after getting the two vf variables next to each other  then in fact no more than n interchanges actually need to be performed. since the factorial function grows faster than an exponential  there is a kind of 
exponential behavior to avoid see diagram  interchange problem  for a demonstration of how knowledge of circuits solves this combinatorial problem. 
　　　　because the form of transformation rule is so simple  one can determine in advance the effect of applying a transformation with respect to shrinking or removing circuits. the doubling transformation  for example  removes the circuit containing the node r. before an applicable transformation is actually used  the system performs a few simple tests to see if the transformation will shrink or remove a circuit of current interest  these tests involve examining nodes to see if they are known or unknown . if it won't  then the transform's use is postponed. in this way  the form of exponential behavior described above is avoided. // disjunctions were allowed on the right of implications  this style of analysis would be impossible. 
the network-matching problem is known to be 
np-complete  and indeed  finding a pattern of n nodes in a graph of m nodes takes time on the order of m . therefore  matching transformations against the  problem network  is exponential in the sue of the transformation pattern; but this sue is bounded a priori because the size n of the longest rule the user wrote is independent of the particular problem  size m  being solved conversely  the conjectured bound on the synthesis system's run time is a polynomial  in the number of transformations needed  of order  the longest transformation pattern  
specifications involving iteration 
 one would like to be able to reason directly about looping control structures  but the undecidability of almost any question concerning looping procedures makes the situation hopeless. since one cannot in general reason about looping control structures  the approach taken has been to  bury  looping in device rules. 
1 


1 

for example  there is a sigma device  see diagram  sigma 
device   that contains prepackaged looping control structures for computing sum from numbers c  n  and a function f: sum - sum m from c to n of f m  the device rule for the sigma device contains a  blank  to be filled in with  code  for the function called f to be found by the synthesis system when the system finds how to compute f  that knowledge is packaged into a structure called a macro-device 
　　　　the key to being able to  bury  all iteration and recursion into prepackaged control structures is being able to tell the synthesis system how to find macro-devices this is always done by telling the system that the macro-device has certain nodes as inputs  and others as outputs. in the case of sigma  terminal f-in is macro-device f's input  and terminal f-out is f's output. 
　　　　diagram  introduction to macro devices  summarizes how the system responds to a device rule in sigma asking for a macro-device f essentially  three steps are involved  first find the portion of the network that can perform the required computation  then package this portion of the network into a macro-device  and finally use the macro-device in device rule interpretation. 
　　　　by using the macro-device mechanisms  devices for various kinds of iteration and recursion  as well as devices like sigma  devices for doing bisection searches  and devices for power series manipulation have been written. transformation rules involving these devices can ignore the fact that the devices' definitions involve looping control structures. 
