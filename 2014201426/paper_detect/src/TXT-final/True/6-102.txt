 
multi-robot learning faces all of the challenges of robot learning with all of the challenges of multiagent learning. there has been a great deal of recent research on multiagent reinforcement learning in stochastic games  which is the intuitive extension of mdps to multiple agents. this recent work  although general  has only been applied to small games with at most hundreds of states. on the other hand robot tasks have continuous  and often complex  state and action spaces. robot learning tasks demand approximation and generalization techniques  which have only received extensive attention in single-agent learning. in this paper we introduce grawolf  a general-purpose  scalable  multiagent learning algorithm. it combines gradient-based policy learning techniques with the wolf   win or learn fast   variable learning rate. we apply this algorithm to an adversarial multirobot task with simultaneous learning. we show results of learning both in simulation and on the real robots. these results demonstrate that grawolf can learn successful policies  overcoming the many challenges in multi-robot learning. 
1 introduction 
multi-robot learning is the challenge of learning to act in an environment containing other robots. these other robots  though  have their own goals and may be learning as well. other adapting robots make the environment no longer stationary  violating the markov property that traditional singleagent behavior learning relies upon. multi-robot learning combines all of these multiagent learning challenges with the problems of learning in robots  such as continuous state and action spaces and minimal training data. 
　a great deal of recent work on multiagent learning has looked at the problem of learning in stochastic games  littman  1; singh et a/.  1; bowling and veloso  1a; greenwald and hall  1 . stochastic games are a natural extension of markov decision processes  mdps  to multiple agents and have been well studied in the field of game theory. the traditional solution concept for the problem of simultaneously finding optimal policies is that of nash 
multiagent systems 
equilibria. an equilibrium is simply a policy for all of the players where each is playing optimally with respect to the others. this concept is a powerful solution for these games even in a learning context  since no agent could learn a better policy when all the agents are playing an equilibrium. 
　multiagent learning in stochastic games  thus far  has only been applied to small games with enumerable state and action spaces. robot learning tasks though have continuous state and action spaces  and typically with more than just a couple dimensions. discretizations of this space into an enumerable state set also do not typically perform well. in addition  data is considerably more costly to gather  and millions of training runs to learn policies is not feasible. typical solutions to this robot learning problem is to use approximation to make the learning tractable and generalization for more efficient use of training experience. 
　in this paper we introduce a new algorithm  grawolf1  that combines approximation and generalization techniques with the wolf multiagent learning technique. we show empirical results of applying this algorithm to a problem of simultaneous learning in an adversarial robot task. in section 1  we give a brief overview of key concepts from multiagent learning along with the the formal model of stochastic games. in section 1  we describe a particular adversarial robot task and its challenges for learning. in section 1  we present the main components of grawolf: policy gradient ascent and the wolf variable learning rate. in section 1  we present experimental results of applying this algorithm to our adversarial robot task  and then conclude. 
1 multiagent learning 
multiagent learning has focused on the game theoretic framework of stochastic games. a stochastic game is a tuple  n  s  a1...ni t  r1...n   where n is the number of agents  s is a set of states  a1 is the set of actions available to agent i  and a is the joint action space a1 x ... x  1n   t is a transition function s x axs -+  1   and r1 is a reward function for the ith agent s x a -r. this looks very similar to the mdp framework except we have multiple agents selecting actions and the next state and rewards depend on the joint action of the agents. another important difference is that each agent 
     1  grawolf is short for  gradient-based win or learn fast   and the v has the same sound as in  gradient . 
has its own separate reward function. the goal for each agent is to select actions in order to maximize its discounted future rewards with discount factor  
　stochastic games are a very natural extension of mdps to multiple agents. they are also an extension of matrix games to multiple states. each state in a stochastic game can be viewed as a matrix game with the payoffs for each joint action determined by the matrices rt{s  a . after playing the matrix game and receiving their payoffs the players are transitioned to another state  or matrix game  determined by their joint action. we can see that stochastic games then contain both mdps and matrix games as subsets of the framework. 
stochastic policies. unlike in single-agent settings  deterministic policies in multiagent settings can often be exploited by the other agents. consider the children's game rock-paperscissors. if one player were to play any action deterministically  the other player could win every time by selecting the action that defeats it. this fact requires the consideration of mixed strategies and stochastic policies. a stochastic policy   is a function that maps states to mixed strategies  which are probability distributions over the player's actions. we later show that stochastic policies are also useful for gradient-based learning techniques. 
nash equilibria. even with the concept of stochastic policies there are still no optimal policies that are independent of the other players' policies. we can  though  define a notion of best-response. a policy is a best-response to the other players' policies if it is optimal given their policies. the major advancement that has driven much of the development of matrix games  game theory  and even stochastic games is the notion of a best-response equilibrium  or nash equilibrium  nash  jr.  1 . 
　a nash equilibrium is a collection of strategies for each of the players such that each player's strategy is a best-response to the other players' strategies. so  no player can do better by changing strategies given that the other players also don't change strategies. what makes the notion of equilibrium compelling is that all matrix games and stochastic games have such an equilibrium  possibly having multiple equilibria. zero-sum  two-player games  like the adversarial task explored in this paper  have a single nash equilibrium.1 
learning in stochastic games. stochastic games have been the focus of recent research in the area of reinforcement learning. there are two different approaches being explored. the first is that of algorithms that explicitly learn equilibria through experience  independent of the other players' policy  e.g.   littman  1; greenwald and hall  1 . these algorithms iteratively estimate value functions  and use them to compute an equilibrium for the game. a second approach is that of best-response learners  e.g.   claus and boutilier  1; singh et ai  1; bowling and veloso  1a . these learners explicitly optimize their reward with respect to the 

figure 1: an adversarial robot task. the top robot is trying to get inside the circle while the bottom robot is trying to stop it. the lines show the state and action representation  which is described in section 1. 
other players' possibly changing policies. this approach  too  has a strong connection to equilibria. if these algorithms converge when playing each other  then they must do so to an equilibrium  bowling and veloso  1a . 
　neither of these approaches  though  have been scaled beyond games with a few hundred states. games with a very large number of states  or games with continuous state spaces  make state enumeration intractable. since previous algorithms in their stated form require the enumeration of states either for policies or value functions  this is a major limitation. in this paper we examine learning in an adversarial robot task  which can be thought of as a continuous state stochastic game. specifically  we build on the idea of best-response learners using gradient techniques  singh et ai  1; bowling and veloso  1a . we first describe our robot task and then describe our algorithm and results. 
1 	an adversarial robot task 
consider the adversarial robot task shown in figure 1. the robot at the top of the figure  the attacker  is trying to reach the circle in the center of the field  while the robot closer to the circle  the defender  is trying to prevent this from happening. if the attacker reaches the circle  it receives a reward of one and the defender receives a reward of negative one. these are the only rewards in the task. when the attacker reaches the circle or ten seconds elapses  the trial is over and the robots reset to their initial positions  where the attacker is a meter from the circle and the defender half-way between. the robots simultaneously learn in this environment each seeking to maximize its own discounted future reward. for all of our experiments the discount factor used was 1 for each full second of delay. 
1 	multiagent systems 　the robots themselves are part of the cmdragons robot soccer team  which competes in the robocup small-size league. there are two details about the robot platform that are relevant to this learning task. first  the learning algorithm itself is situated within a large and complex architecture of existing capability. the team employs a global vision system mounted over the field. this input is processed by an elaborate tracking module that provides accurate positions and velocities of the robots. these positions comprise the input state for the learning algorithm. the team also consists of robust modules for obstacle avoidance and motion control. the actions for the learning algorithm then involve providing target points for the obstacle avoidance module. situating the learning within the context of this larger architecture focuses the learning. rather than having the robot learn to solve well understood problems like path planning or object tracking  the learning is directed at the heart of the problem  the multirobot interaction. 
　second  the system control loop that is partially described above has inherent  though small  latency. specifically  after an observed change in the world 1ms will elapse before the robot's response is executed. this latency is overcome for each robot's own position and velocity by predicting through this latency using knowledge of the past  but not yet executed  actions. since the actions of the opponent robots are not known  this prediction is not possible for other robots. latency effectively adds an element of partial observability to the problem  since the agents do not have the complete state of the world  and in fact have separate views of this state. notice  that this also adds a tactical element to successful policies. a robot can  fake  the opponent robot by changing its direction suddenly  knowing the other robot will not be able to respond to this change for a full latency period. 
　this task involves numerous challenges for existing multiagent learning techniques. these challenges include continuous state and action spaces  elements of partial observability due to system latency  and violation of the markov assumption since many of the system components have memory  e.g.  the tracking and the obstacle avoidance modules. in fact  these limitations may even make equilibria cease to exist altogether  bowling and vcloso  1b . this is a further reason for exploring best-response learning techniques  which don't directly seek to learn an equilibrium. we now present grawolf  a best-response learning algorithm that can handle the challenges of multi-robot learning. 
1 	grawolf 
grawolf  or gradient-based wolf  combines two key ideas from reinforcement learning: policy gradient learning and the wolf variable learning rate. policy gradient learning is a technique to handle intractable or continuous state spaces. wolf is a multiagent learning technique that encourages best-response learning algorithms to converge in situations of simultaneous learning. we briefly describe these techniques and how they are combined. 
1 policy g radicnt ascent 
we use the policy gradient technique presented by sutton and colleagues . specifically  we define a policy as a gibbs distribution over a linear combination of features of a candidate state and action pair. let 1 be a vector of the policy's parameters and let  j1.sa be an identically sized feature vector for state 1' and action a  then the gibbs distribution defines a 
multiagent systems 
stochastic policy according to  

sutton and colleagues' main result was a convergence proof for the following policy iteration rule that updates a policy's parameters in a gibbs distribution according to  
is an independent approximation of q ＜* s a  
with parameters w  which is the expected value of taking action a from state a and then following the policy 1r fc. for the gibbs distribution  sutton and colleagues showed that for convergence this approximation should have the following form  
		 1  
as they point out  this amounts to /w being an approximation of the advantage function   
where  is the value of following policy -k from state .s. it is this advantage function that we estimate and use for gradient ascent. 
　using this basic formulation we derive an on-line version of the learning rule  where the policy's weights are updated with each state visited. the summation over states can be removed by updating proportionately to that state's contribution to the policy's overall value. since we are visiting states onpolicy then we only need to weight later states by the discount factor to account for their smaller contribution. iff time has passed since the trial start  this turns equation 1 into  

a 
　since the whole algorithm is on-line  we do the policy improvement step  updating   simultaneously with the value estimation step  updating w . we do value estimation using gradient-descent sarsa i lsutton and barto  1  over the same feature space as the policy. this requires maintaining an eligibility trace vector  e. the update rule is then  if at time k the system is in state s and takes action a transitioning to state s' in time t and then taking action a'  we update the trace and the weight vector using  
 1  
 1  
where * is the sarsa parameter and cayed learning rate. the addition of raising t allows for actions to take differing amounts of time to execute  as in semi-markov decision processes  sutton and barto  1 . 
　the policy improvement step then uses equation 1 where s is the state of the system at time k and the action-value estimates from sarsa    are used to compute the advantage term. we then get  

this forms the crux of grawolf. what remains is the selection of the learning rate   this is where the wolf variable learning rate is used. 
1 	win or learn fast 
wolf   win or learn fast   is a method by bowling and 
veloso  1a  for changing the learning rate to encourage convergence in a multiagent reinforcement learning scenario. notice that the policy gradient ascent algorithm above does not account for a non-stationary environment that arises with simultaneous learning in stochastic games. all of the other agents actions are simply assumed to be part of the environment and unchanging. wolf provides a simple way to account for other agents through adjusting how quickly or slowly the agent changes its policy. 
　since only the rate of learning is changed  algorithms that are guaranteed to find  locally  optimal policies in nonstationary environments retain this property even when using wolf. in stochastic games with simultaneous learning  wolf has both theoretical evidence  limited to two-player  two-action matrix games   and empirical evidence  experiments in games with enumerated state spaces  that it encourages convergence in algorithms that don't otherwise converge. the intuition for this technique is that a learner should adapt quickly when it is doing more poorly than expected. when it is doing better than expected  it should be cautious  since the other players are likely to change their policy. this implicitly accounts for other players that are learning  rather than other techniques that try to explicitly reason about the others' action choices. 
　the wolf principle naturally lends itself to policy gradient techniques where there is a well-defined learning rate  with wolf we replace the original learning rate with two learning rates ;o be used when winning or losing  respectively. one determination of winning and losing that has been successful is to compare the value of the current policv v  s  to the value of the average policy over time then the algorithm is consid-
ered winning  otherwise it is losing. with the policy gradient technique above  we cannot easily compute the average policy. instead we examine the approximate value  using qw  of the current weight vector 1 with the average weight vector over time 1. specifically  wc are  winning  if and only if  

when winning in a particular state  we update the parameters for that state using otherwise  
1 	our task 
returning to the robot task shown in figure 1  in order to apply grawolf we need to select a policy parameterization. specifically we need to find a mapping from the continuous space of states and actions to a useful feature vector  i.e.  to define for a given s and a. the filtered positions and velocities of the two robots form the available state information  and the available actions are points on the field for navigation. by observing that the radial angle of the attacker with respect to the circle is not relevant to the task we arrive at a seven dimensional input space. these seven dimensions are shown by the white overlaid lines in figure 1. 
　we chose to use tile coding  sutton and barto  1   also known as cmacs  to construct our feature vector. tile coding uses a number of large offset tilings to allow for both a fine discretization and large amount of generalization. we use 1 tiles whose size was 1mm in distance dimensions and 1mm/s in velocity dimensions. we simplify the action space by requiring the attacker to select its navigation point along a perpendicular line through the circle's center. this is shown by the dark overlaid line in figure 1. this line  whose length is three times the distance of the attacker to the circle  is then discretized into seven points evenly distributed. the defender uses the same points for its actions but then navigates to the point that bisects the line from the attacker to the selected action point. the robot's action is also included in the tiling as an eighth dimension with a tile size for that dimension of the entire line  so actions are distinguishable but there is also generalization. agents select actions every tenth of a second  or after every three frames  unless the feature vector has not changed over that time. this keeps the robots from oscillating too much during the initial parts of learning. 
1 	results 
before presenting results on applying grawolf to this problem we first consider some issues related to evaluation. 
1 	evaluation 
evaluation of multi-robot learning algorithms present a number of challenges. the first is the problem of data gathering on a real robot platform. learning often requires far more trials than is practical to execute in the physical world. we believe and demonstrate that the grawolf technique is practical for the time constraints of real robots. yet  from a research standpoint  we want thorough and statistically significant evaluation  which requires far more trials than just those used for learning. we solve this problem by using both a simulator of our robot team as well as the robots themselves to show that the approach is both practical for robots while still providing an extensive analysis of the results. 
　the second challenge is the problem of evaluating the success of simultaneous learning. the traditional single-agent evaluation that shows performance over time converging to some optimal value is not applicable. multiagent domains have no single optimal value independent of the other agents' behavior. the optimal value is changing over time as the other agents also learn. this is especially true of self-play experiments where both agents are using an identical learning algorithm  and any performance success by one agent is necessarily a performance failure for the other. 
　on the other hand we would still want learning robots  even in self-play  to improve their policy over time. in this paper  our main evaluation compares the performance of the learned policy with the the performance of the initial policy before learning. the initial policy is a random selection of the available actions  and by design of the available actions is actually a fairly capable policy for both agents. we also use the evaluation technique of challengers  which was first examined by 

1 	multiagent systems 


figure 1: the value of learned policies compared to a random opponent in simulation. lines to the right of the bars show standard deviations. 
littman   this technique trains a worst-case opponent  or challenger  for a particular policy to see the generality of the learned policy. in this paper we present challenger results demonstrating that the learned policies are indeed robust  and that the wolf variable learning rate plays a critical role in keeping the learning away from easily exploited policies. 
1 	experiments 
in all of the performance graphs in this section  the yaxis shows the attacker's expected discounted reward  which roughly corresponds to the expected time it takes for the attacker to reach the circle. on the right of the graph the range is shown in seconds. all measurements of expected discounted reward are gathered empirically over the course of 1 trials. all training occurred over 1 trails  and takes approximately 1 hours of training time on the robot platform or in simulation. unless otherwise noted  the training was always done with a simultaneously learning opponent  both using gra wolf. the experiments in simulation were repeated nine times and the averages are shown in the graphs. we first examine the performance of the policies learned in simulation  and then examine the performance of learning on the robot. 
　figure 1 shows the results of various learned policies when playing against an opponent following the random policy  which was the starting point for learning. the middle bar   r v. r   corresponds to the expected value of both players following the random policy.  l v. r  corresponds to the value of the attacker following the policy learned in self-play against a random defender.  r v. l  corresponds to the value of a random attacker against the defender policy learned in self-play. 
　notice that  as was desired  learning does improve performance over the starting policy. the learned attacker policy against random does far better than the random attacker policy against the learned defender. these experiments demonstrate that grawolf improves considerably on its starting policy. the next experiment explores how robust the learned policy is and the effect of the wolf component. 
　figure 1 shows results of challenger experiments. policies are trained using simultaneous learning. the policy is then fixed and a challenger policy is trained for 1 trails  
multiagent systems 

figure 1: the value of learned policies playing their challengers in simulation. lines to the right of the bars show standard deviations. 
to specifically measure the policy's worst-case performance. the better the worst-case performance  the less exploitable and more robust the policy is to unknown opponents. we trained a challenger against the policies learned after 1  1  and 1 trials  averaging together the results. we used this experiment to investigate the robustness of the learned policy and the affect of the wolf variable learning rate on the grawolf algorithm. the left side of the graph shows the worst-case performance of learned defender policies  while the right side shows the worst-case performance for attacker policies.  wolf  corresponds to the described grawolf algorithm   slow  does not use a variable learning rate but rather always uses the wolf's winning rate  while  fast  always uses its losing rate. 
　first  notice that the distance between the worst-case performance of the defender and the worst-case performance of the attacker  the third and fourth column of table 1  respectively  is quite small. this demonstrates that the learned policies are quite close to the equilibrium policy  if one exists. this also means that the learned policies are robust and difficult to exploit. 
　second  notice that the wolf learned defender policy performs better against its challenger  i.e.  keeps its challenger to a lower reward  than either of the two learning rates it switches between. for the attacker  the learned policy performs better against its challenger than the fast learning rate  but is not significantly different than the slow learning rate. there are a couple of possible reasons for this. one explanation is due to the initialization of values. since all values were intialized to zero for both sides  this amounts to an optimistic initialization for the defender  and a pessimistic one for the attacker  as all rewards are non-negative for the attacker.  this may mean the attacker considers itself winning far more often than the defender  causing the slower learning rate to be employed most of the time. there is evidence to this effect when examining the percentage of updates that use 1w versus si. during training  the attacker used the slower  winning rate for 1% of its updates  while the defender used the winning rate for only 1%. the effect of value initialization on grawolf is an interesting top to be explored further. overall  although the results are certainly not as dramatic as the unapproximated results  bowling and veloso  1a   wolf still 


figure 1: the value of learned policies playing the random policy on the real robots. training was done with the first half of the trials in simulation and the last half on the robots. 
seems to be providing a converging effect. 
　finally  we examine results of using grawolf on the real robots. we took policies that were trained for 1 trials in simulation and then did an additional 1 trials of training on the robots. we evaluated the resulting policies against the random policy as we did with the simulator results in figure 1. these results are shown in figure 1 
　in the real robots the attacker is nearly impossible to keep from reaching the circle. the defender can at best only slow down its progress  and this is even true for a random attacker policy. this can be seen in figure 1 by the much higher rewards as compared to the simulator results. despite this  these results are qualitatively identical to the results produced in simulation. simultaneous learning improves the performance for both the attacker and defender over their initial policies. 
　finally  as an adversarial environment we would expect good policies to be stochastic in nature. this is true of all of the learned policies in both simulation and on the robots. for example  the most probable action in any given state has probability on average around 1% in the attacker's learned policy  and 1% in the defender's. in some states  though  the learned policies are nearly deterministic. so  the learned policies are indeed stochastic to avoid exploitation  while still learning optimal actions in states that don't depend on the other agent's behavior. 
1 conclusion 
we have introduced  grawolf  a general-purpose multiagent learning algorithm capable of learning robot tasks in multirobot  and even adversarial  environments. we showed results of this algorithm learning in one particular adversarial robot task  both in simulation and from actual robot experience. these results both demonstrated the effectiveness of the policy gradient learner  and the importance of the wolf variable learning rate. it should be noted that the use of sutton and colleagues' particular policy gradient formulation is not critical. it would be interesting to combine wolf with other policy gradient techniques  such as  williams  1; baxter and bartlett  1 . 
1 
acknowledgements 
this research was sponsored by grants f1-1 and dabt1-1. the content of this publication does not necessarily reflect the position of the funding agencies and no official endorsement should be inferred. the authors also thank brett browning and james bruce for the development of the cmdragons'1 robots used in this work. 
