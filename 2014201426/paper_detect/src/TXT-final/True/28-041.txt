 
the objective of this work is to interpret inductive results obtained by the unsupervised learning method osham. we briefly introduce the learning process of osham  that extracts concept hierarchies from unlabelled data  based on a representation combining the classical  prototype and exemplar views on concepts. the interpretive process is considered as an intrinsic part in osham and is carried out by a combination of case-based reasoning with matching approaches in inductive learning. an experimental comparative study of some learning methods in terms of knowledge description and prediction is given. 
1 	introduction 
though the interpretation of induction results has a significant role in machine learning applications  there have been little work in inductive learning  particularly in unsupervised learning  associated with interpretation procedures  e.g.   bergadano et a/.  1    wu  1 . 
there are three broad classes of logical  threshold  and competitive interpreters for intensional concept descriptions  langley  1 . naturally  three types of outcomes occur when matching logically an unknown instance with learned concepts: no-match  single-match  and multiplematch. most work dealing with the cases of no-match and multiple-match employ a probabilistic estimation  e.g.   michalski et a/.  1 . however  it is not always possible to obtain such an estimation in unsupervised learning when it requires using the class information. moreover  the logical match of the concept intent does not always provide a prediction with enough satisfaction  particularly in boundary regions of concepts. 
*this work is supported by kokusai electric co.  ltd 
 japan  to jaist  japan advanced institute of science and technology   and by the national research programme on information technology kc1-rd1  vietnam . 
1 	case-based reasoning 
one of two styles of case-based reasoning  cbr  is interpretive by which new situations are evaluated in the context of old situations  kolodner  1 . rather than classifying new cases using the intensional concept descriptions  cbr typically does classification by using the nearest neighbor methods which have been demonstrated to be able to work often as well as other inductive learning techniques  aha et al.  1 . however  one limitation of the cbr is that it does not provide the concept description which is the main advantage of inductive learning about the knowledge understandability. 
this paper highlights the intrinsic role of the interpretive process in unsupervised inductive learning and proposes a procedure that combines cbr with matching approaches in inductive learning to interpret the concepts learned by method osham  ho  1    ho  1 . the reason for this combination lies in the fact that the use of results in unsupervised learning  obtained by a nonexhaustive searching for regularities  can be suported by the nearest neighbor rule. the paper is organized as follows. section 1 briefly resumes the learning phase in osham consisting of an extended representation of concepts in the galois lattice  and the essential ideas of the learning algorithm for extracting concept hierarchies from unsupervised data. section 1 presents the interpretation phase of osham to classify unknown cases using learned knowledge. section 1 presents an experimental comparative study of four learning methods and the discussion. section 1 is a short conclusion. 
1 learning concept hierarchies 
1 	concept representation and extraction 
among views on concepts in cognitive science and machine learning  the classical  prototype and exemplar ones are widely known and used. main strengths and limitations of these views on concepts have been widely recognized  e.g.   van mechelen et al.  1    wrobel  1j. moreover  without the class information  unsupervised learning systems often compose solutions by employing one or more of three main categorization con-


	ho & luong 	1 

the quality of splitting a concept ck into subconcepts in the next level  denoted by is measured by 
 1  
the basis of learning in osham is a generate-and-test procedure to split a concept c into subconcepts at a higher level of  starting from the root concept of the galois lattice with the whole set of training instances  it extracts the concept hierarchy recursively in a topdown direction. this algorithm is originally designed for discrete attributes with unordered nominal values. in the current version  continuous attributes are discretized before learning process by k-means clustering  hartigan  1 . in fact  for each continuous attribute the k-means algorithm is applied to cluster its values into k groups 
 k = 1 ... k . a criterion similar to  1  with the euclidean distance is used to choose a value of k that corresponds to the best partition according to this criterion. the basic idea of learning algorithm in osham  described fully in  ho  1   is resumed in table 1. 
1 	a b o u t learned concept hierarchies 
by using different constraints for l. a  in the learning algorithm  osham is able to extract both overlapping or disjoint concepts depending the user's interest  ho  1 . osham has been implemented in the x window on a sparcstation with the direct manipulation style of interaction which allows the user to participate actively in the learning process. the user can initialize parameters to cluster data  visualize the concept hierarchy gradually  observe the results and the quality estimation  manually modify the parameters when necessary before the system continues to go further to cluster subsequent data or backtrack to regrow branches of the concept hierarchy with respect to the categorization scheme. figure 1 shows a main screen of the interactive osham with a hierarchy of overlapping concepts learned from the wisconsin breast cancer dataset. a full description of concept 1 in this figure is given below 
concept 1 
level = 1 
super-concepts = {1}  sub-concepts = {1  1} features =  uniformity of cell size  1    bare nuclei  1  
  bland chromatin  1   uniformity of cell shape  1  
locaunstances/coverec instances = 1 
localinstances = {1  1  1  1  1  1} 
concept probabilrty = 1 
locaunatance.conditionalprobability = 1 
concept dispersion = 1 localinstancejdispersion = 1 
subconcept-part'rtion-quality = 1 
there is a considerable distinction in the concept description of osham in contrast to those of other meth-
1 	case-based reasoning 
ods such as the supervised learning system c1  quintan  1   the unsupervised learning systems cobweb 
 fisher  1  and autoclass  cheeseman and stutz  1 . c1 induces decision trees in which concepts are represented by their intent associated with a predicted error rate  and it has not to maintain intermediate concepts. cobweb represents each concept  as a set of attributes a  associated with a set of their possible values vij  the occurrence probability  and the conditional probability associated with each value vij .a classification in autoclass is defined as a set of classes  the probability of each class  and two additional probabilities for each hypothesized model: the model probability p h  and the conditional parameter probability distribution p p | h . 

figure 1: a screen of the interactive osham 
we share the opinion in  langley  1  that the interpretive process is a central issue in learning. an intensional representation has no meaning  e.g.  no extension  without some associated interpreters and different interpreters can yield different meaning for the same representation. next section describes the second phase in osham - its interpretive process. 
1 interpreting concept hierarchies 
1 	interpretive c b r 
interpretive case-based learning is a process of evaluating situations in the context of previous experience. one way a case-based classifier works is to ask whether the unknown case is enough like another one known. it does classification by trying to find the closest matching case in its case base to the new case rather than using intensional concept descriptions. many studies have pointed out the strong points of cbr  e.g.  simplicity  relatively 

robust  often excellent performance  etc.  and its weak points  e.g.  {aha et al.  1    kolodner  1 . 
in inductive learning  the logical interpretation approach carries out an  all or none  matching process depending on whether the unknown instance satisfies the concept intent. the threshold approach carries out a partial matching process and employs some threshold to determine an acceptable degree of match. the competitive approach also carries out a partial matching process and selects the best competitor based on estimated degrees of match  langley  1 . the interpretation of inductive learning results is commonly understood as the process of comparing an unknown case to the learned concepts. in osham  as the generality decreases along branches of the concept hierarchy  we say that a concept ck matches the unknown instance e if ck is the most specific concept in a branch that matches e intensionally  though all superconcepts of ck match e . naturally  there are three types of outcomes when matching 
logically an unknown instance e with the learned concepts: only one concept that matches e  single-match   many concepts that match e  multiple-match   and no concept that matches e  no-match . we believe in the alternative roles of cbr and generalization in inductive learning. as noted in  kolodner  1   rules could be used when they matched cases exactly  while cases would be used when rules were not immediately applicable. in fact  the nearest neighbor of e in the training set and the learned concept to which it belongs  denoted by nn e  and c nn e    provide useful information which could be used in all cases of single-match  multiple-match and no-match. 
1 	i n t e r p r e t a t i o n of i n d u c t i o n results 
we develop an interpretation procedure for concept hierarchies that uses the concept intent  the hierarchical structure information  the probabilistic estimations and 
the nearest neighbors of unknown instances. this interpretation procedure consists of two stages:  1  find all concepts on the concept hierarchy that match e intensionally  and  1  decide among these concepts which matches e best. this procedure shares the same scheme of the system poseidon  bergadano et a/.  1   but functions differently. in the second stage  it determines the best matched concept of e with some satisficing degree of prediction. 
consider the multiple-match case when we have to decide among the competitors the best matched concept. to do it we need to determine and compare the degree of match of competitors. from various experiment casestudies we note that a logically matched concept ck will match e well  with low error rate  if it satisfies a ma-
jority of the following conditions: is high  c is a leaf concept  is high  is low  
is low  and generally none of these factors has a clearly 
	ho & luong 	1 

where are positive weights for the importance of the level  leaf concept  local instance conditional probability  concept dispersion  and local instance dispersion  all with value 1 by default . 
   denote by c e  the best matched concept that e is finally decided to belong to  and by  the satisficing degree of prediction. table 1 presents the interpreta-
tion procedure in osham based on the function  and the nearest-neighbor principle. essentially  this procedure makes final decision by comparing the best concept matching c with the concept c nn e   containing the nearest neighbor nn e  of e regarding the function in this interpretation procedure  different symbolic values are assigned to the satisficing degree of prediction 

   values of  indicate the decreasing rank of prediction satisfaction. for example  we may consider s1 as  best prediction   m1 as  strong prediction  while n1 as  weakly accepted prediction  and n1 as  no prediction . the interpretation for different values of depends on the judgment of the user or domain experts. 
1 evaluation 
1 	experimental results 
a way to evaluate unsupervised learning system is to employ supervised data but hide the class information in the whole learning and interpreting phases and use the class information only to estimate the predictive accuracy. we employ this way to evaluate unsupervised learning systems where the predicted name of each learned concept  is determined by the most frequently occurring name of instances in  with this predicted name of learned concepts  the error rate of an unsupervised learning system can be estimated as the ratio of the number of testing instances correctly predicted regarding the predicted name over the total number of testing instances. it is worth mentioning that multiple train-and-test experiments are much more computationally expensive but give more reliable evaluation than a single train-and-test experiment. 
   experiments are carried out on ten datasets from the uci repository of machine learning databases  including the wisconsin breast cancer  breast-w   congressional voting  vote   mushroom  mushroom   tic-tactoe  tictactoe   glass identification  glass   ionosphere  ionosphere   waveform  waveform   pima diabetes  diabetes   thyroid  new  disease  thyroid   and heart disease cleveland  heart-c . the numbers of attributes  discrete and continuous   instances and  natural  classes of these datasets are given in columns 1 of table 1. all experiments on these datasets are carried out with 1fold cross validation by four programs c1  quinlan  
1   cart-like  breiman et al.  1   autoclass 
1 	case-based reasoning 
and osham in the same condition  i.e.  the same randomly divided datasets into subsets. for autoclass  we use the public version autoclass-c implemented in c and run three steps of search  report and predict with the default parameters. the predicted name and predictive accuracy of autoclass and osham are obtained as mentioned above. columns 1 report the predictive accuracies of c1  cart-like and autoclass  respectively. 
   for osham we carried out experiments for two interpretation procedures: osham-nn fusing only the nearest neighbors  and   using the nearest neighbors and matching regarding the function  by the procedure described in table 1 . experimental results are reported in columns 1  respectively. in order to avoid a biased evaluation of osham  although with each dataset parameters can be adjusted to obtain the most suitable concept hierarchy  we fixed values of the size of the training set  of the number of attributes  and the beam size commonly to all datasets. two last columns in table 1 give the average size of concept hierarchies  number of concepts  and cpu time  in second  of osham learned from these datasets. 
1 	discussion 
the predicted name obtained in osham and autoclass by the majority of occurring name of instances in concepts is different from the concept name obtained in supervised learning  e.g.  c1  using the pruning threshold based on the class information. an unsupervised concept in the worse case may contain nearly equal numbers of instances belonging to different natural classes  and an unsupervised classification may be failed in distinguishing very similar instances. it explains that while the predictive accuracies between these supervised and unsupervised methods look not so different  they are slightly different in nature. note that the recent release 1 of c1  quinlan  1  treats the continuous attribute better than the release we used in this work. 
   the predictive accuracies of osham and autoclass in these experiments are only slightly different. 
in these first trials  each system is better in several datasets and these two systems can be considered having comparable performance. one advantage of osham is its concept hierarchies can be easily understood by its extended classical view on concepts and the graphical support. 
   empirical results with osham-nn and oshamillustrate that these strategies are both good for 
interpreting unsupervised induction results. we believe that in general cbr can also be used to interpret results of unsupervised learning and this topic is worth for a further investigation. 

table 1: datasets  predictive accuracies of methods  average size and cpu time of osham 


1 conclusion 
in this paper we first briefly resumed the main ideas of the unsupervised learning method o s h a m in terms of description and extraction of concepts. we then described how the nearest neighbor rule is combined with domain knowledge to interpret the induction results. careful experiments with different datasets have demonstrated that this combination provides a good solution to the use of unsupervised learned knowledge in prediction. the main conclusions can be drawn from this research are  1  the interpretive process needs to be a part of a unsupervised learning method  and  1  the cbr can be used to interpret results obtained from the non-exhaustive search in unsupervised inductive learning. our near future research concerns further investigations on the effects of k-nearest neighbors and the discretization of continuous attributes to the interpretation of unsupervised induction results  based on experiments with a larger number of datasets. 
