 
the 'strong ai' claim that suitably programmed computers can manipulate symbols that they understand is defended  and conditions for understanding discussed. even computers without ai programs exhibit a significant subset of characteristics of human understanding. to argue about whether machines can really understand is to argue about mere definitional matters. but there is a residual ethical question. 
topi c area and keywords 
philosophical foundations  machines  language  meaning  understanding  reference  strong ai. 
introduction 
filing cabinets contain information but understand nothing. computers are more active than cabinets  but so are copiers and card-sorters  which understand nothing. is there a real distinction between understanding and mere manipulation  unlike cabinets and copiers  suitably programmed computers appear to understand. they respond to commands by performing tasks; they print out answers to questions; they paraphrase stories or answer questions about them. does this show they attach meanings to symbols  or are the meanings 'derivative' on our understanding them  as claimed by searle  c1   is real understanding missing from simulated understanding just as real wetness is missing from a simulated tornado  or is a mental process like calculation: if simulated in detail  it is replicated  
i argue that there is no clear boundary between things that do and things that do not understand symbols. our ordinary concept of 'understanding' denotes a complex cluster of capabilities  and different subsets of these may be exhibited in different people  animals or machines. to ask 'which are necessary for real understanding ' is to attribute spurious precision to a concept of ordinary language. 
instead of answering either 'yes' or 'no' to the question whether suitably programmed computers can understand  we note that within the space of possible 'behaving systems'  including animals  there are infinitely many cases  some sharing more features with human minds  some fewer. the important task is to analyse the nature and the implications of these similarities and differences  
without 
assuming existing english words can label the cases adequately. 
dennett  thinks we can justifiably take the 'intentional' stance towards any machine or organism whose behaviour thereby becomes easier to 
predict or explain. searle c1  c1 retorts that behaviour is not enough  alleging that a suitable program could make a system appear to understand chinese when it doesn't really  e.g. if searle is inside executing the programs. in  i show that he actually attacks an extreme and implausible thesis  namely that any 'instantiation' of a suitable program would understand. but he is right in suggesting that actual behaviour is not what mental concepts refer to. how the behaviour is produced is relevant. there are significantly different ways in which the same behaviour might be generated. for instance a huge lookup table  prepared by an extraordinarily foresightful programmer who anticipated all our questions  could pass a collection of behavioural tests. but it might produce nasty surprises later  because no finite set of actual tests can establish the powers required for passing a wider range of possible tests. since there are indefinitely many counterfactual conditional state-
ments that are true of us  but which would not be true of such a machine  we would be unwise to rely on it in future simply because it has worked so far  without knowing the basis for success. 
attributions of mentality imply coherent behaviour and reliability  as friends  enemies  colleagues  or goal achievers. there are different kinds of unreliability. one kind would exist in a machine whose computations depended on co-operation of a  speeded up  human interpreter performing millions of steps  as in searle's experiment. tiredness  boredom  cussedness  and mere slips could easily interfere. this supports searle's claim that men-
tality presupposes machinery with the right causal powers  though not his other conclusions . 
the lookup table is unreliable in a deeper way: we cannot rely on it to deal with the unanticipated. the same applies  to a lesser degree  to less rigid programs: human-like performance in any finite set of tests does not justify the assumption that the behaviour would be convincing in other possible situations. this is painfully evident in ai programs to date. 
so  taking the intentional stance on purely behavioural grounds  turing's test   is potentially risky. we must adopt what dennett calls the 'design 1 a. sloman 
stance1 for a better justification of our ascriptions of intentionality  understanding  etc. a machine must not merely produce appropriate behaviour  but must satisfy the design requirements for understanding. could a machine do this  
the main features of human understanding are sketched below. we'll find important aspects of our ordinary concept of 'understanding' in simple computers  even without ai programs. requirements for richer human-like capacities are also described. there are no reasons for doubting that machines can satisfy them. 
the semantic linkage problem 
a central issue is the 'semantic linkage problem': how can a person  or machine  take one thing as referring to or describing another  ai work on language and image understanding often relies on 
translation into some internal representation. but if the machine itself does not understand the internal representation  we have not progressed much beyond filing cabinets. if all understanding requires translation we risk an infinite regress. ultimately something must be interpreted as meaningful in its own right. how  it is implausible that existing ai story 'understanders' really can think about parties  political events  or passionate murders  despite printing out sentences about them after reading stories. if a symbol-user u uses a symbol s to refer to some object 1  then it seems that u must have some other way of relating to 1  attending to 1  thinking of 1  etc.  besides using s. this 'semantic linkage' problem pervades recent analytical philosophy  e.g. see     . it is ignored in work on formal semantics  and both linguistics and psychology seem to have little to say about it. it is complicated by the fact that 1 can be remote from u  or even long dead  or imaginary  which rules out direct causal connections between u  s and 1  as necessary. we shall see that when 1 is part of u  e.g. a location in u's memory  an internal action u can 
perform  an internal pattern u can test for   the link may be a comparatively simple causal relationship. my conjecture is that more sophisticated types of meaning and reference are possible only on the basis of this 'internal' semantics. 
what is understanding a language  
i use the word 'language' loosely as equivalent to 'notation'  'representational scheme'  'symbol system' etc. very roughly  a language l is a system of symbols used by some agent u in relation to a world w. a full analysis would distinguish different kinds of:  a  symbol media   b  symbol systems   c  mechanisms for manipulating symbols   d  symbol users   e  worlds  and  f  purposes for which symbols might be used. this paper discusses only a subset of this rich array of possibilities. 
symbols are structures that can be stored  compared with other structures  searched for  etc. they may be physical structures  like the marks on a piece of paper  or virtual symbols  i.e. abstract structures in a virtual machine  like 1-d arrays in a 
computer  see c1 . they may be internal or external. they need not be separable physical objects or events  since a single travelling wave may 'carry' different signals simultaneously  and a network of active nodes may have several patterns superimposed in its current state. symbols include maps  descriptions  representations  of all kinds  including computer programs  and non-denoting sym-
bols  like parentheses and other syntactic devices. 
 in fact  anything at all can be used as a symbol.  
a language l contains symbols used by u to represent or refer to entities  properties  relations  events  processes  or actions in some world w. the word 'used' may suggest that u has goals or purposes. however  this is not a necessary condition  since a plant  uses  water in photosynthesis without having any explicit goal  or purpose. we can tell that u uses a symbol s to refer to object 1  by discovering that some significant subset of the conditions listed below are satisfied. we shall see that in the more elaborate cases goals are involved. 
the symbols need not be used for external communication. meaning and understanding are often assumed  e.g.   to be essentially concerned with communication between language users. as argued in c1  
this is a mistake  since understanding of an external language is secondary to the use of an internal symbolism for storing information  reasoning  making plans  forming percepts and motives  etc. this is prior in  a  evolutionary terms   b  in relation to individual learning  and  c  insofar as the use of an external language requires internal computations. in short: 
'representation is prior to communication'. 
objects in the world w may be concrete  e.g. physical objects  or abstract  e.g. numbers  grammatical rules . they may be external  or internal to u. like symbols  the objects may exist in a virtual world  embodied in a lower level world  like a virtual machine implemented in a lower level computer. many programming languages refer to objects in a virtual world  such as lists  arrays  procedures  etc. similarly social systems form a virtual world embedded in a psychological and physical world. 
the structure of the concept 'understanding' 
instead of fruitlessly trying to identify a set of defining conditions for u to use symbols with understanding  i offer a prototypical set of conditions for saying that u uses some collection of symbols as a language l referring to objects in a 
world w. different combinations of these conditions define different concepts of 'language'  'meaning'  'understanding'  etc. asking which is the 'right' concept is pointless. 
for each condition i comment on how it might be satisfied by a machine  ignoring  for brevity  the difference between internal and external representations of computer languages. the discussion will appear to be question-begging  as fragments of evidence will be presented as if the case had been made. the fact that so many fragments can be presented this way  is what makes the case! it shows that events and processes in a machine can 

constitute a model for a significant subset of the 'axioms' implicitly defining mentalistic concepts. unlike simulations of  e.g.  tornadoes/ people outside the model can relate to the model as to the real thing  though some may find this distasteful . a robot may obey commands  answer questions  teach you things. but a simulated tornado will not make you wet or cold. anyone who objects that this is not enough can be challenged to describe precisely what is missing. appeals to mystery  or to unanalysable kinds of mental or spiritual stuff are 
undiscussable. 
we'll see that computers can manipulate internal structures and use them as symbols associated with a world w consisting of both entities within the machine and more abstract entities like numbers and symbol-patterns. later  the discussion addresses reference to an 'external' world. 
prototypical conditions for u to use l  to refer to w  
* l is a set containing simple and complex symbols  the latter being composed of the former  in a principled fashion  according to syntactic rules. 
this condition is satisfied by most computer languages  though machine codes generally have very simple syntactic rules and structures. rules may be implicit in procedures. 
* u associates some symbols of l with objects in w  and other symbols with properties  relations  or actions in w. 
a computer can associate 'addresses' with a world w containing locations in its memory  or in a virtual machine  and their contents and relationships. the symbols cause processes to be directed to or influenced by specific parts of the system. some of the symbols specify which processes - i.e. they name actions. 
various sorts of properties and relations may be symbolised in a machine language  e.g. equality of content  neighbourhood in the machine  arithmetic relations  having a bit set  etc. symbols indicating tests that produce a boolean result  name properties and relationships. 
so  if u is a simple computer  the basic semantic relation is causal: 
's refers to 1 for u' = 
	's makes u's activities relate to 	or 	involve 
where 1 may be an object  property  relation or type of action. 
instructions have imperative meanings because 	they systematically cause actions to occur. roughly  
's denotes action a to u' = 's makes u do a' 
depending on how rich the language is  s and a may have 	independently 	variable 	components  	e.g. object  instrument  manner  location  time  etc. 
a. sloman 1 
in computers imperative meaning is basic: even denoting expressions are often instructions to compute a value. this low level meaning depends on direct causal connections within the machine. later we discuss non-imperative denotation. 
* some of the objects referred to in world w are abstract  like numbers. 
computers can use certain symbols to denote numbers because they are manipulated by arithmetical procedures and used as loop counters  address increments  array subscripts etc. thus the machine can count its own operations  or the elements of a list that satisfy some test. the way a machine does this is typically very close to the core of a young child's understanding of number words - they are just a memorised sequence used in certain counting activities. so: 
's refers to a number  for u' = 
's belongs to a class of symbols which u manipulates in a manner characteristic of counting  adding  etc.' 
* what a complex symbol s expresses for u depends on its structure  its more primitive components and some set of interpretation rules related to the syntactic rules u uses for l.    
this is true of many computer languages. e.g. what is denoted by a complex arithmetical expression  or 
a complex instruction  depends on what the parts denote  and how they are put together according to the syntactic rules of the language. 
* u can treat the symbols of l as 'objects'  i.e. can examine them  compare them  change them  etc.  though not necessarily consciously. 
this applies to computers. symbolic patterns used to refer can also be referred to  compared  transformed  copied  etc. e.g. two patterns may be tested for equality  or overlap  or set inclusion. an address can be incremented to get the next location. it is not clear whether other animals can or need to treat their internal symbols as objects. this may be a pre-requisite for some kinds of learning. 
* certain symbols in l express conditionality. 
this is the key to much creative thinking or planning  and to flexibility of action. we can distinguish  a  ' i f used in conditional imperatives   b    i f used as the standard boolean  truthfunctional  operator and  c  ' i f used in conditional assertions   c  is not found in the simplest computer languages. 
conditional imperatives are found in machines since ' i f  or some equivalent  when combined with evaluable expressions permits or suppresses actions  depending on the evaluation. 
* by examining w  u can distinguish formulas in l that assert something true from those asserting something false. 
computers typically use symbols to 	denote 	'truth1 a. sloman 
values'  'true' and 'false' or '1' and '1' . boolean operations e.g. 'or'  'and'  'not' are also represented  by symbols that trigger actions transforming inputs to outputs consistently with truth-tables. the 'result' is taken as a truthvalue partly because of its role in conditional imperatives. the sense in which computers can examine their internal states to assign a truth-value is fairly clear  though how they check arithmetical 
statements requires deeper analysis. 
if u assigns truth-values to symbols in a manner that depends on the state of world w  the symbols can be thought of as representing factual propositions  that so and so is the case in w. more generally  
'for u  s means p is the case' = 'in certain contexts the expression s causes u to do certain things only if p is the case  otherwise not' 
we have yet to see how a machine can treat 'true' and 'false' as more than just formal duals. 
* u can detect that stored symbols contain errors and take corrective action  e.g. noting that two descriptions are inconsistent and finding out which to reject. 
something like this occurs in programs that attempt to eliminate wrong inferences derived from noisy data  e.g. in vision  and in plan-executors that check whether the assumptions underlying the current plan are s t i l l true. here we find support for a richer conception of a truth-value than just a pair of arbitrarily chosen symbols  if 'true' connotes surviving tests  and 'false' rejection. more on this later. 
* a complex symbol s with a boolean value may be used for different purposes by u  for instance: questioning  specifying information to be found by lookup  computation  or external sensing   instructing  specifying actions   asserting  storing information for future use . 
we have seen how  in a computer  s can function as a primitive question  in a conditional instruction where action depends on the answer to the question. in low level machine languages there is not usually the possibility of using the same symbol to express the content of an imperative as in  make s true . i.e. machine codes do not have 'indirect imperatives' with embedded propositions. however  ai planning systems have shown how in principle this can be done  at least in simple cases  assuming the initial availability of direct imperatives. 
apart from a few exceptions like planner  conniver and prolog  most computer languages include requests and instructions  but not assertions: factual statements assimilated to some store of beliefs. however  it is easy to allow programs to record results of computations or externally sensed data  or even results of self-monitoring. recomputable information may be stored simply for easy access  as people store multiplication tables. 
whether u uses s as a question  an assertion  or an instruction  will depend on context. s may specify the content of an assertion in one context  'store s '   a question in another  'if s then...' or 'lookup s '   and an instruction in a third  'achieve s ' . i.e. role is determined by use rather than form or content. 
* u can make inferences by deriving new symbols in l from old ones  in order to determine some semantic relation  e.g. proofs preserve truth  refutations demonstrate falsity . 
work in ai has demonstrated mechanisms for doing this  albeit in a restricted and mostly uncreative fashion so far. human forms of inference require some of the functional architecture discussed below in connection with motives  and also require use of 
a much wider range of representations than ai has so far addressed   1 . 
* l need not be a fixed  static  system: it should be extendable  to cope with expanding requirements. 
one source of language change in people is communication with others using different dialects. a deeper source is situations that prove hard to describe. 
many computer languages are extendable. adaptive dialogue systems are beginning to show how a machine may extend its own language according to need. but deep concept formation is s t i l l some way off. it is not clear which animals can and which cannot extend their internal languages. without this  certain other forms of learning may be impossible.  more on language change below.  
* u may use symbols of l to formulate goals  pur-poses  or intentions; or to represent hypothetical possibilities for purposes of planning or 
prediction. 
simple versions of this sort of thing are found in existing ai planning systems. 
without a functional architecture supporting distinctions between beliefs  desires  plans  suppositions  etc.  a machine cannot assign meanings in the way that we do. merely storing information  and deriving consequences  or executing instructions  leaves out a major component of human understanding  i.e. that what we understand matters to us. for information to matter to a machine it would have to have its own desires  preferences  likes  dislikes  etc. this presupposes that there are modules whose function is to create or modify goals - motive generators. full flexibility requires motive-generator generators. deciding and planning require motive comparators and motive-comparatorgenerators. this is a complex story  spelled out in a little more detail in . when desires  intentions  plans  p