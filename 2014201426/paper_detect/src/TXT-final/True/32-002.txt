 
keyphrases are an important means of document summarization  clustering  and topic search. only a small minority of documents have author-assigned keyphrases  and manually assigning keyphrases to existing documents is very laborious. therefore it is highly desirable to automate the keyphrase extraction process. this paper shows that a simple procedure for keyphrase extraction based on the naive bayes learning scheme performs comparably to the state of the art. it goes on to explain how this procedure's performance can be boosted by automatically tailoring the extraction process to the particular document collection at hand. results on a large collection of technical reports in computer science show that the quality of the extracted keyphrases improves significantly when domain-specific information is exploited. 
1 	introduction 
keyphrases give a high-level description of a document's contents that is intended to make it easy for prospective readers to decide whether or not it is relevant for them. but they have other applications too. because keyphrases summarize documents very concisely  they can be used as a low-cost measure of similarity between documents  making it possible to cluster documents into groups by measuring overlap between the keyphrases they are assigned. a related application is topic search: upon entering a keyphrase into a search engine  all documents with this particular keyphrase attached are returned to the user. in summary  keyphrases provide a powerful means for sifting through large numbers of documents by focusing on those that are likely to be relevant. 
　unfortunately  only a small fraction of documents have keyphrases assigned to than-mostly because authors only provide keyphrases when they are explicitly instructed to do so-and manually attaching keyphrases 
1 	machine learning 
craig g. nevill-manning 
department of computer science 
rutgers university 
piscataway  new jersey  usa 
to existing documents is a very laborious task. therefore  ways of automating this process using artificial intelligence-more specifically  machine learning techniques-are of interest. there are two different ways of approaching the problem: keyphrase assignment and keyphrase extraction. in keyphrase assignment  also known as text categorization  dumais et al.  1   it is assumed that all potential kephrases appear in a predefined controlled vocabulary-the categories. the learning problem is to find a mapping from documents to categories using a set of training documents  which can be accomplished by training a classifier for each category  using documents that belong to it. as positive examples and the rest as negative ones. a new document is then processed by each of the classifiers and assigned to those categories whose classifiers identify it as a positive example. the second approach  keyphrase extraction  which we pursue in this paper  does not restrict the set of possible keyphrases to a selected vocabulary. on the contrary  any phrase in a new document can be identified- extracted-as a keyphrase. using a set of training documents  machine learning is used to determine which properties distinguish phrases that are keyphrases from ones that are not. 
　turney  describes a system for keyphrase extraction  genex  based on a set of parametrized heuristic rules that are fine-tuned using a genetic algorithm. the genetic algorithm optimizes the number of correctly identified keyphrases in the training documents by adjusting the rules' parameters. turney compares genex to the straightforward application of a standard madiine learning technique-bagged decision trees  breiman  1 -and concludes that it gives superior performance. he also shows that genex generalizes well across collections: when trained on a collection of journal articles it successfully extracts keyphrases from web pages on a different topic. this is an important feature because training genex on a new collection is computationally very expensive. 
　this paper briefly summarizes the kea keyphrase extraction algorithm  and goes cm to show that it generalizes as well as genex across collections. in con-

trast to genex  however  it does not employ a specialpurpose genetic algorithm for training and keyphrase extraction: it is based on the well-known naive bayes machine learning technique. training is therefore much quicker. the main finding of this paper is that performance can be boosted significantly if kea is trained on documents that are from the same domain as those from which keyphrases are to be extracted. this allows us to capitalize on speedy training  because deriving domainspecific models would be less practical with the original lengthy genetic algorithm approach. 
　section 1 summarizes the kea algorithm for keyphrase extraction  and shows that it performs comparably to genex if used in the same domain-independent setting. section 1 explains a simple enhancement that enables kea to exploit collection-specific information about keyphrases  and shows how this addition boosts performance on a large collection of computer science technical reports. the main findings of this paper are summarized in section 1. 
1 keyphrase extraction using naive bayes 
keyphrase extraction is a classification task: each phrase in a document is either a keyphrase or not  and the problem is to correctly classify a phrase into one of these two categories. machine learning provides oif-the-shelf tools for this kind of situation. in machine learning terminology  the phrases in a document are  examples  and the learning problem is to find a mapping from the examples to the two classes  keyphrase  and  not-keyphrase . machine learning techniques can automatically generate this mapping if they are provided with a set of training examples  that is  examples with class labels assigned to them. in our context  these are simply phrases which have been identified as either being keyphrases or not. once the learning method has generated the mapping given the training data  it can be applied to unlabeled data  in other words  it can be used to extract keyphrases from new documents. 
1 	generating candidate phrases 
not all phrases in a document are equally likely to be keyphrases a priori in order to facilitate the learning process  most phrases that appear can be eliminated from the set of examples that are presented to the learning scheme. 
　first  the input text is split up according to phrase boundaries  punctuation marks  dashes  brackets  and numbers . non-alphanumeric characters  apart from internal periods  and all numbers are deleted. kea takes all subsequences of these initial phrases up to length three as candidate phrases. it than eliminates those phrases that begin  or end  with a stopword- it also deletes phrases that consist merely of a proper noun. in the next step  all words are case-folded and stemmed using the iterated lovins stemmer  lovins  1   and stemmed phrases that occur only once in the document are removed. 
1 	building the model 
so far we have shown how candidate phrases are generated. however  in conventional machine learning terms  phrases by themselves are useless-it is their properties  or  attributes   that are important. several plausible attributes immediately spring to mind: the number of words in a phrase  the number of characters  the position of the phrase in the document  etc. however  in our experiments  only two attributes turned out to be useful in discriminating between keyphrases and non-keyphrases: the score of a phrase  and the distance into the document of the phrase's first appearance. in the following we explain how these attributes are computed and how a naive bayes model  domingos and pazzani  1  is built from them. 
　the  score of a phrase is a standard metric in information retrieval. it is designed to measure how specific a phrase p is to a given document dz 

the first probability in this equation is estimated by counting the number of times the phrase p occurs in the document d  and the second one by counting the number of documents in the training corpus that contain 
p  excluding d .1 
　the distance of a phrase from the beginning of a document is calculated as the number of words that precede its first appearance  divided by the number of words in the document. the resulting feature is a number between 1 and 1 that represents the proportion of the document preceding the phrase's first appearance. 
	both these attributes are real numbers. 	the naive 
bayes learning method can process numeric attributes by assuming  for example  that they are normally distributed. however  we obtained better results by discretizing the attributes prior to applying the learning scheme  domingos and pazzani  1 . this indicates that the normal distribution is not appropriate in this application. discretization quantizes a numeric attribute into ranges so that the resulting new attribute can be treated as a nominal one: each value represents a range of values of the original numeric attribute. kea uses fayyad and irani's  discretization scheme  which is based on the minimum description length principle. it recursively splits the attribute into intervals  at each stage minimizing the entropy of the class distribution. it stops splitting when the total cost for encoding both the discretization and the class distribution cannot be reduced further. 
　the naive bayes learning scheme is a simple application of bayes' formula. it assumes that the attributes- in this case tfxidf and distance-are independent 
　　1 the counters are initialized to one to avoid taking the logarithm of zero. 
	frank et al 	1 

given the class. making this assumption  the probability that a phrase is a key phrase given that it has discretized tfxidf value t and discretized distance d is: 

where pr t/key  is the probability that a keyphrase has tfxidf score t  pr d key  the probability that it has distance d  pr key  the probability that a phrase 
is a keyphrase  and pr t d  a normalization factor that makes pr key/ t  d  lie between zero and one. all these probabilities can be estimated reliably by counting the number of times the corresponding event occurs in the training data.1 
　it has been shown that naive bayes can be a very accurate classification method even if the independence assumption is not correct  domingos and pazzani  1 . however  it can be argued that the two attributes we use  tfxidf and distance  are close to being independent given the class. this implies that naive bayes is close to being the optimum classification method for this application  and might be the reason why it performs better than all other learning methods that we have investigated.  in particular it performs better than bagged decision trees  as we show in section 1.  
1 	extracting keyphrases 
kea uses the procedure described above to generate a naive bayes model from a set of training documents for which keyphrases are known  typically because the author provided them . the resulting model can then be applied to a new document from which keyphrases are to be extracted. 
　first  kea computes tfxidf scores and distance values for all phrases in the new document using the procedure described above  taking the discretization obtained from the training documents.  both attributes can be computed without knowing whether a phrase is a keyphrase or not.  the naive bayes model is then applied to each phrase  computing the estimated probability of it being a keyphrase. the result is a list of phrases ranked according to their associated probabilities. assuming that the user wants to extract r keyphrases  kea then outputs the r highest ranked phrases. 
　there are two special cases that have to be addressed in order to achieve optimum performance. first  if two phrases have equal probability-which is quite likely to happen due to the discretization-they are ranked according to their tfxidf score  in its pre-discretized form . second  if a phrase is a subphrase of another phrase  it is only accepted as a keyphrase if it is ranked higher; otherwise it is deleted from the list before the r top-ranking phrases are output. 
1 	experimental results 
we have evaluated kea on several different document collections with author-assigned keyphrases. our cri-
　　1 the naive bayes implementation used by kea initializes all counts to one. 
1 	machine learning 
terion for success is the extent to which kea produces the same stemmed phrases as authors do.1 because this method of evaluation is the same as used by turney   we can directly compare kea's performance to his results. 
comparison to genex 
we compared kea and genex using two experimental settings from turney's paper-1 the first one involves training and testing on journal articles. in this setting  1 articles are used for training  1 from the journal of the international academy of hospitality research  1 from 
the neuroscientist  1 from the journal of computeraided molecular design  and 1 from behavioral and brain sciences   and 1 for testing  all from psycoloquy . 
in the second setting  the same documents are used for training but 1 fips web pages ate used for testing. 
　table 1 shows the number of correctly identified author-provided keyphrases among the five and fifteen top-ranking phrases output by the extraction algorithms. four extraction algorithms are represented: genex  fifty bagged c1 decision trees  quinlan  1  as used by turney  kea  and kea using fifty bagged c1 trees instead of the naive bayes learning scheme. results for the first two methods are from turney's paper.1 the third scheme is the standard kea algorithm that we have described. in the fourth  bagged c1 trees were used instead of discretization and naive bayes  with all the standard pre- and post-processing done by kea. this variation of kea is computationally much more expensive  by a factor of at least fifty . 
　turney found bagged c1 trees to perform universally worse than genex  but in only one of the four experimental settings from table 1  journal/fips with cutoff of five  was the difference statistically significant. kea sometimes performs worse than genex and sometimes better; the differences are not statistically significant  at the 1% level  according to a t-test . moreover  kea-c1 performs much better than turney's c1 in the case where the latter does significantly worse than genex. we conclude that genex and kea perform at about the same level  kea-c1 seems slightly worse but the difference is not statistically significant on these datasets. the only statistically significant result is the poor performance that turney observed in one case with c1. 
　the difference between turney's findings for bagged c1 trees and ours deserves some explanation. turney uses many more attributes-among them distancebut he does not use tfxidf. moreover  he performs no post-processing for c1-although he does for genex- 
     1  author-assigned keyphrases are  of course  deleted from the documents before they are given to kea. 
1
　　we could not compare kea on the other document collections used by turney because we did not have access to his corpus of email messages  which contains confidential information. 
     1 to get the number of correctly identified keyphrases  turney's  precision  figures were multiplied by the cutoff employed  five or fifteen . 


table 1: experimental results for different extraction algorithms 

whereas we remove subphrases if they do not perform better than their superphrases. these appear to be the main differences between his way of applying c1 and ours. 
changing the amount of training data 
an interesting question is how kea's performance scales with the amount of training data available. in order to investigate this  we performed experiments with a large collection of computer science technical reports  cstr  from the new zealand digital library  www.nzdl.org . the documents in cstr are fairly noisy  partly because the source files have been extracted automatically from postscript. also  they contain on average fewer keyphrases than the other collections. this makes keyphrase extraction in this domain more difficult than in the other corpuses. 
　there are two potential ways in which the corpus of documents that is available can influence kea's performance on fresh data. first  training documents are used when computing both the discretization of the attributes  and the corresponding counts for the naive bayes model. it is essential that these documents have keyphrases assigned to them because the learning method needs labeled examples. second  the document corpus supports the learning process when each phrase's  document frequency  is calculated-this is used for deriving its tfxidf score. in this case the documents need not be labeled. our experiments showed that no further performance improvement was gained by increasing the number of documents used to compute the document frequencies beyond 1. 
　to illustrate the effect of training set size  figure 1 shows kea's performance on an independent set of 1 test documents. it plots the number of  correct  keyphrases  for both five and fifteen phrases extracted  against the number of documents used for training  from 1 through 1 files. the error bars give 1% confidence intervals derived by training kea on ten different training sets of the same size. we used the same independent 1 documents for calculating the document frequencies throughout this particular experiment. it can be seen from figure 1 that if more than twenty documents are used for training  little is gained by increasing the number further. with 1 documents  there is no further performance improvement. 
　these results show that kea's performance is close to optimum if about 1 training documents are used; in other words  1 labeled documents are sufficient to push performance to the limit. however  section 1 demonstrates that this is not the case if domain-specific information is exploited in the learning and extraction process. in that case  much larger amounts of labeled training documents prove beneficial. 
subject area of training documents now we investigate the extent to which models formed by kea transfer from one subject domain to another. to this end we use the collection of journal articles described above  and two collections of web pages also used by turney  1   aliweb  and nasa  all of which have keyphrases assigned. the basic procedure was to train on one of the collections and test on another  producing nine combinations. for each collection we chose 1 training documents at random and used the rest for testing  1 for the journal articles  1 for aliweb  and 1 for nasa. the training documents were used to compute the document frequencies; thus the entire keyphrase assignment model was based on the training documents alone. for the journal articles  as well as the randomlychosen test set  we ran experiments with the same training/testing division that turney  used  the test set comprising 1 articles in the journal psycoloquy. 
　figure 1 shows the average number of correct keyphrases returned when five keyphrases are retrieved  for twelve cases. the first nine represent every combination of training and testing sets drawn from one of the three collections  and the last represents the psycoloquy test set with the same three training sets  except 

figure 1: performance on cstr corpus for different numbers of training files  error bars show 1% confidence intervals  
frank et al 1  
