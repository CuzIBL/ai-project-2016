 
     in a recent paper  etherington & reiter formalized a simple version of semantic networks with exceptions in terms of reiter's default logic. with this approach they were able to formally characterize the correctness of an inference algorithm in terms of default logic  and exhibited an algorithm that was correct in this sense. finally  they concluded that massively parallel architectures for semantic networks  such as netl apparently cannot implement this algorithm. in this paper  we present a different massively parallel architecture for the simplified semantic networks outlined in their paper which appears to avoid the objections to netl. we also present some results of simulations in this framework of the examples presented in etherington and reiter. 
introduction 
     in a recent paper  etherington & reiter  1   hereafter e&r  formalized the inheritance hierarchy subset of semantic networks with exceptions in terms of reiter's  1  default logic. with this approach they were able to formally characterize the correctness of an inference algorithm in terms of default logic  and exhibited an algorithm that was correct in this sense. finally  they concluded that massively parallel architectures for semantic networks  such as netl  fahlman  1   apparently cannot implement this algorithm. in this paper  we present a different massively parallel architecture for the simplified semantic networks outlined in their paper which appears to avoid the objections to netl. we also present some results of simulations in this framework of the examples presented in e&r. 
the problem 
     semantic networks have been found to be an efficient and useful representation of knowledge by ai researchers for many years. one principal advantage is the ability to store information about objects at appropriate levels of abstraction in the is-a hierarchy  so that the fact that dogs  elephants  and people nurse their young  for example  can be stored once at the mammal node. retrieving all of the properties associated with an instance of some class is done by an inference procedure that is particularly simple in these systems  known as inheritance. 
　　as hayes  1  points out. there is an obvious correspondence between is-a hierarchies and simple collections of fopc formulas. for example   clyde is an instance of an elephant  corresponds to the assertion elephant clyde . statements about classes  such as 
 elephants are gray   correspond to first-order formulae  in this case   x .elephant x = gray x . inheritance can then be seen as a repeated application of modus ponens. one nice property of inheritance hierarchies is that  since they are acyclic  modus ponens can only be applied a finite number of times  no more than the depth of the hierarchy. also  as pointed out by e&r  the node labels in such hierarchies are unary predicates  e.g. mammal x . finally  no exceptions are permitted to inheritance. a dog is a mammal  no matter what. 
　　unfortunately  the real world is not as simple as a taxonomic hierarchy. often it is useful to abandon the tree structure in favor of multiple inheritance hierarchies  and to allow exceptions to inheritance relations. this introduces non-monotonicity into the representation  as well as ambiguity. an common example of a nonmonotonic rule is:  assume a particular elephant is gray unless proven otherwise.  this is often known as default reasoning and has been formalized by reiter  1 . when combined with multiple inheritance  default reasoning can lead to ambiguity. a well-known example is: 
1  nixon is a quaker. 
 1  nixon is a republican. 
 1  republicans are normally non-pacifists. 
 1  quakers are normally pacifists. 
　　reiter's formalization of the above facts would be  assuming  for convenience  that nixon is a type : 
 1   x .nixon x = quaker x  
 1   x .nixon x = republican x  
republican x f pacifist x  
 1  
~pacifist x  
quaker x :pacifist x  
 1  
pacifist x  

　　 1  and  1  are just the first order rules corresponding to  1  and  1  above.  1  is an example of a default rule. the formula to the left of the colon is called the prerequisite of the default. if this is known  and the part to the right of the colon   the justification  can be consistently assumed  i.e.  its negation isn't provable from what we know   then we can infer ~pacifist x   the consequent. often  the justification contains all of the exceptions to the rule we know about. in this case  we might add  nramember x   to the justification of  1 . 
　　is an individual b for which nixon b  holds a pacifist or not  in reiter's terminology  there are two extensions' consistent with our knowledge. an extension contains the first order facts and is closed under the default rules as well as first order theorem-hood. one contains 
pacifist b   the other -pacifist b . in general  the problem we want to solve is: given an individual b  and a predicate p known to be true of b  we want to compute 
p1b          pn b  such that the pi's all he within a single extension. as noted by e&r  we can ignore the unary predicate argument  and the default theory is purely propositional. fortunately  then  non-provability is computable. 
etherington and reiter's algorithm 
　　we briefly review e&r's inference algorithm in intuitive terms. those interested in the formal details may refer to their paper. the purpose of the algorithm is to  derive conclusions all of which lie within a single extension of the underlying default theory.  when faced with multiple extensions  the algorithm randomly chooses one. the algorithm operates by successive approximations to an extension. starting with the first order facts as a first approximation to an extension  it successively chooses  randomly  default rules which are not blocked by the current approximation or the previous approximation  and adds their consequents to the current approximation  until all of them are used. the constraints derived in previous approximations thus propagate to the current approximation. it iterates on this  starting with the first order facts ag. n  until two successive approximations are the same  convergence . etherington  1  has proved that this algorithm will always converge on an extension. the randomness is essential to the algorithm's ability to derive any possible extension  if it is run  enough  times. an important point about the algorithm as given is that it can be viewed as a relaxation-style constraint propagation technique. 
　　unfortunately  netl is unable to capture such algorithms due to the  one-shot  nature of markerpassing. markers are propagated through the network to find properties. cancellation links can block this propagation to implement exceptions to inheritance. the very existence of cancellation links in the version of netl discussed in e&r  discarded in later versions; see 
	g.cottrell 	1 
touretzky  1  defeats marker passing because a link can be crossed before it is cancelled from a longer path. see figures 1 a  and 1 b   reproduced from far. in 
figure 1 a   f must be reached before b in order to generate the extension properly  and vice-versa in 1 b . it is clear from this that the problem with n etl is not that it is a parallel machine. rather  the problem is that is a single pass marker passing machine. 
an alternate parallel approach 
　　an obvious answer to these objections is to relax the  one-shot  nature of the parallel network. connectionist networks  feldman & ballard  1   being iterative  have no such restriction. connectionist models consist of simple processing units connected by links. a unit or node is a computational entity comprised of: 
p: a continuous value in  -1   called the potential v: an output  in the range  1.1  in discrete jumps of .1 i:a vector of inputs  
and functions for updating these: 

　　we will term an application of these functions an update of the unit. note that there is no interpreter for a 

	 a  	 b  
figure 1. networks which defeat the shortest path heuristic. 
connectionist network; all updates are done locally by each unit in parallel. there are no constraints on the functions that can be used  though they are usually kept simple. it is an important research topic at the moment to discover what constraints on the functions can be reasonably assumed without losing computational ability. in the following model  we show that even with simple updating functions  we can still get fairly powerful results. finally  note that there is no mention of time in the definition. that is  in simulating such networks  the units could be scheduled for updating in various ways: they could be kept in lock step  synchronous  or they could be updated in random order  with some units perhaps being 

1 	g.cottrell 
updated several times before another gets a chance to be updated  simulating asynchrony . we use an asynchronous version in the following model. 
　　a connection  or link   is an identification of an element of a units input vector with the another unit's output  along with a weight  a value between -1 and 1. any value transmitted on the link is multiplied by the weight before it is passed to the unit. in the following model  we use only use weights of -1 and 1. links with negative weights are called inhibitory links. these are drawn with a small circle at their head in the figures. a special kind of link  modifier links  are node-link connections that have the effect that when the unit at their tail has positive output  they block activation from crossing the link at their head. these are also drawn with a small circle at their head  but since they are always incident on other links  there is no confusion between them and inhibitory links. 
　　for convenience  the potential function is then often broken down into two stages: an evidence function  which is applied to the inputs  and an activation function  which computes the actual potential given the result of the evidence function and the current potential. the activation function usually employs a decay parameter so that if the evidence goes to 1  so does the activation. a conjunctive connection is used to refer to two links that must both have non-zero input for the evidence function to pass a non-zero result to the activation function. we will use an output function that thresholds the potential  we use a threshold of 1 in this model  so negative activation is not spread  and rounds it to the nearest tenth. a unit that has non-zero output is called firing  or simply   on  . 
	in 	the so-called 	localist connectionist models  
 feldman & ballard  1  we represent an object in the domain as a unit or small set of units1. the basic idea is that a unit stands for a value of a parameter  the unit/value principle  and collects inputs from other units which represent evidence for that value  positive or negative. for example  in vision   see ballard  1  a unit could represent the presence of an edge at a certain angle at a particular  x y  coordinate on the retina. the unit's output represents its confidence that there is an edge at the point in the visual field that this unit refers to. thus  at run time  the unit's output represents a 
confidence level in a hypothesis about the parameter it refers to. an output of 1 after convergence represents certainty about the parameter value represented by the unit. the links between the units are weighted  reflecting the importance to the receiving unit of the evidence from that link. much of the information encoded in the 
lsee  hinton. 1  for a distributed connectionist approach to 
semantic networks. 
network is contained in the connections between units  hence the name  connectionism  . 
　　computation is performed by designating some of the units as input units. these are units that may be  clamped on   that is  their output is fixed by the experimenter. activation is allowed to spread from these  and if the network is well-designed  it converges to a fixed point where no unit's output changes from one iteration to the next at this point the result is read out from the network by the experimenter. unfortunately  no theory exists at the moment that guarantees convergence of these networks. analysis is difficult when arbitrary functions are allowed on different units. this is one principle advantage of some connectionist models  hinton & sejnowski  1  that use uniform functions on all units  where analysis is possible. 
　　connectionist networks are a natural architecture for solving relaxation style problems. their  activation passing  is iterative  and constraints between hypotheses can be easily encoded in the networks as positive or negative links between mutually compatible or incompatible hypotheses  represented as processing units . the typical way to go about building connectionist models is to first decide on which elements of the domain we want to model  choose a way to encode those as units  and then to wire the units together in such a way as to encode constraints between the elements. finally  we must choose an appropriate function for combining the evidence. in the following  we present a connectionist model of semantic networks of the kind discussed in e&r. it should be kept in mind that these have a particularly simple form. properties are not distinguished from type nodes  and there are no two place predicates. for a different formulation of semantic networks in connectionist terms which overcomes these objections  see  shastn & feldman  1 . 
a connectionist inheritance model 
　　in e&r  a correspondence was made between the five link types of a semantic network  strict is-a and isnt-a  default is-a and isnt-a  and exception links  and formulae in default logic. since our purpose here is to show that a connectionist network can mimic their inference algorithm  we start with formulae from default logic that correspond to inheritance axioms and display the corresponding bits of network. the first step  however  is to choose a representation of the predicates. following the unit/value principle  we will start with two units for every predicate p  called +p and ~p  representing the two different possible assignments of truth values to those predicates. when computing an extension  a node that is firing  after convergence  represents that it is part of the extension. there is an immediate consistency constraint between these two nodes  i.e.  they should not both be on in any stable state. 
thus we should make them mutually inhibitory. however  a unit that has evidence should be allowed to 

	g.cottrell 	1 
propagate that evidence before being inhibited. this is 

essential if we are to consider all possibilities in parallel. thus we introduce a third unit  #p   to use touret/kys notation  if not his semantics   which represents  inconsistency . see figure 1. this node inhibits +p and ~p if both of them are firing  by using a conjunctive connection . it outputs the maximum of the two  inhibiting both +p and -p1. thus this introduces a delay in the inhibition between +p and -p. 
　　the important point about this design is that this subnetwork of three units representing a predicate has only three stable states: 
 1  they are all off. 
 1  + pis on. 
 1  ~p is on. 

inhibitory connection 
conjunctive connection 
figure 1. the spock representation of the predicate p. 
any other configuration of activation will cause changes in some unit's activation. if only #p is on  it will go off. if only one of the others are on as well  then #p will also go off. if both are on  then #p will inhibit them until one goes off. if #p is off and both of the others are on  #p will come on. thus if the network is at a fixed point  the predicate p is either assigned the values true   + p is on   false  -p is on   or don't care  neither is on . 
　　we make the following correspondence between the four formulae defined in e&r  and networks in the 
connectionist framework. all links shown have weight 1. 

our encoding of an inference rule is thus just to put a positive link from the antecedent to the consequent. this is the first link in the encodings of  1  and  1   and it guarantees that if + a comes on  eventually +b  or ~b  as appropriate  will come on as well. since these correspond to first order facts  we don't allow any exceptions to these  i.e.  modifier links - see below . in an attempt at some semblance of completeness  we encode the contrapositive as well  remember we have no interpreter to deduce these consequences . this is the motivation for the second link in the encodings of  1  and 
 1 . we are explicitly adding the rule -b =  ~a  or b =    a  in  1  . in e&r's formulation  extensions are closed under first order inference. this has some odd consequences  which we will see in the cephalopod example. 
in the encodings of  1  and  1   there are no 
  backwards  links like these  because the contrapositive doesn't hold for default inferences. however   b in  1   as well as any of the -t-cj's  for example  should block the inference of + b if it is on  so we use a modifier link from ~b to the link between +a and +b. this blocks activation from crossing the link if  b has any output at all. thus the modifier link seems a good choice for encoding the semantics1. 
　　now  suppose we have encoded the rules  1  or  1  as above. the inference of b from a corresponds to + a being active  which activates + b. in the case of  1   this 

1 	g.cottrell 
inference may be retracted later if  b is inferred by some other  necessarily first order  rule. a default rule will not be able to infer ~b  since this will be blocked by the modifier link from +b. the retraction is accomplished by using activation functions that cause a unit's potential to go to 1 if its evidence goes to 1. in the case of  1   the inference may be retracted if the original inference chain that lead to 1-a included a default inference that was retracted. 
　　an extension of a predicate p is computed by treating +p as an input unit  that is  clamping it on  and then letting the network converge. an extension of p the corresponds to all of the predicate units that are on in a stable state of the network  with the following caveat: some first order facts may not be relevant to the computation of the extension. these will not be activated at all. also  all facts derivable from the extension will not be included  e.g.  a =   a or b . so the actual extension is the logical closure of the union of w and the active predicates in the stable state. 
　　finally  we specify what the units compute. an evidence function which appears reasonable for our purposes is to take the maximum of all positive input  from subtypes  and add the minimum of all inhibitory inputs. the motivation for this rule is that we will use one  source  for the network's activation  namely the predicate whose extension we are seeking  and so it does not seem appropriate to use more evidence than the maximum from that source. however  there are arguments against this. in a non-demonstration system  
one would want an alternate way to combine evidence  for example dempster-shafer rules  see ginsberg  1   for some extensions of dempster-shafer rules for semantic networks . this is especially important for default rules. if someone is a republican and an nra member and a veteran then we would be more inclined to assume they are not a pacifist  even if they are a quaker. see shastri and feldman  1  for a formal theory of evidence that incorporates these considerations. anyway  for the examples we will be discussing  the max and min rule appears sufficient. 
　　the result of the evidence function is passed to the activation function. we have implemented two activation functions in this system. one uses table lookup and looks almost like iterative marker passing  it only uses three values   and the other is more continuous valued. the first function appears in table 1. the basic idea is to move towards the value of your input. this version of the system we call spock because it doesn't allow intermediate interpretations. a unit is on  or it isn't. the second uses the activation function in table 1  due to mcclelland & rumelhart  1 . in this function  e is the result of the evidence function  p is the potential and d is a decay constant  we used .1 in the simulations . we call this version dr. spock because it is more permissive  allowing intermediate values. if one had no decay  then 
table 1. the spock activation function 
evidence 
1 
1 
1 
1 
1 
1 
-1 -1 
-1 current pot. 
1 
-1 
1 
-1 
1 
-1 new pot. 
1 
1 
1 
i 
1 
1 
-1 
1 
-1 table 1: the dr. spock activation function 

starting the system with a 1 on the assumed unit  and only using weights of 1 or -1 on links  then the result is that units only take on the values 1 or 1. the problem with this is that if a unit is at 1  then it will stay there if it is not inhibited  causing false inferences to stay around. the desire to try to have a system with no decay led to the table lookup function above. 
　　we should state at this point that there is a major difference between f&r's algorithm and the following implementation1. we claim here  without proof  that if the network only encodes a consistent set of first order  inheritance  rules  we can allow all first order rules to fire in parallel and the network converges. an interesting question is whether we can allow all inferences to proceed in parallel  including the default inferences  while relying on our predicate networks to guarantee consistency. while at this stage we have no proof of convergence or correctness  experimental results with the system support the conjecture. e&rs algorithm stipulates introducing one default rule at a time  generating all inferences  and then trying another  so we depart from their algorithm in this. the difference is that we don't wait for first order consequences to propagate before we try another default. we use a random update order  simulating asynchrony  to allow one default to run before another  as in e&r's algorithm. if they are  competing  defaults  as in the nixon example   this ensures one will block the other  so this is basically a tiebreaking strategy. another method would be to use noise in a synchronous network to break ties. 
simulation results 
　　we present the results of simulating several of the networks from e&r. in the following  the results are from the spock version except where noted. this is because in almost all cases  the results from the two systems were similar. as noted above  we use a random update order. that is  units are all equally likely to be 

	g.cottrell 	1 
updated at any point. an  iteration  consists of doing as 

many updates as there are units in the network. note that this doesn't mean that all units have been updated; some may have been updated more than once  others not at all. 
　　we begin with the cephalopod example from e&r  shown in figure 1. the source of this example is 
 fahlman et al  1 . in english  it's: 
molluscs are normally shell-bearers. 
cephalopods must be molluscs but normally are no! shell-bearers. 
nautili must be cephalopods and must be shellbearers. 
　　the default theory corresponding to this  as given in e&r  is: 

the connectionist implementation of these rules is given in figure 1. note the  downward  link from -s~b to -n  
encoding the contrapositive. 

figure 1. e&rs network representation of cephalopod facts. 
for this example we spread activation from the 
+ cephalopod node in order to find the extension of cephalopod. to shorten the tables  only iterations where one of the units in the table changed are shown. this is a  typical  execution  although of course they are rarely the same. we simulated this network about ten times  and it never took longer than 1 iterations to converge. as the results in table 1 show  activation spreads from + cephalopod and activates  shell-bearer  which blocks the is-a link from + mollusc to + shell-bearer. 
　　an interesting result here is that  nautilus is inferred! since e&r require that  an extension ... is closed under the defaults of d as well as first order theoremhood   then they will have to live with this. the inference came about because -nautilus was consistent  allowing us to infer  shell-bearer from +cephalopod. 

figure 1. the spock implementation of the knowledge in figure 1. 
however  since nautilus =  shell-bearer  first order  then   shell-bearer =  -nautilus  and we get the result that we prove  nautilus from assuming it to be consistent. this is not unintuitive  since if cephalopods are usually not shell-bearers  then they are usually not nautili. this is just not what we expect from an inheritance hierarchy. not including such  downward  inferences would eliminate completeness  but would also eliminate a problem with the final example. 
table 1. trace of unit outputs from example 1 
	iteration 	1 	1 activating cephalopod
1 1  
1 activating nautilus
1 	1 	+ nautilus 1 	1 
	 nautilus 1 	1 
. 	+ 	cephalopod 	1 
	+ mollusc 1 	1 
+ shell- bearero 	1 
- 	shell- 	bearer 	1 1 
1 1 
1 
1 1 
1 1 
1 
1 
1 
1 
1 1
1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 	1 　　if we activate + nautilus instead  activation spreads to + cephalopod and + shell-bearer  and + nautilus cancels the is-a from + cephalopod to  shell-bearer  resulting in the correct extension. 
　　if we use the default theory relating to the netl version of this hierarchy1 given in e&r  see figure 1   we get an ambiguity with respect to whether a given cephalopod has a shell or not  since the default is-a from + mollusc to + shell-bearer is not cancelled. also  in this version all inferences are defaults  so  nautilus is never inferred. our network shows a marked preference for shortest paths in this case. in 1 runs  we got  shell-

1 g. cottrell 

figure 1. the netl version of the cephalopod example. all inferences are defaults. 
bearer 1 times and + shell-bearer only twice. this is not surprising  given that the activation is most likely to follow the shorter path first.  in simulations of the nixon example described earlier  where the paths are of equal length  the the results were 1 between the two extensions.  
　　in example 1 we show that we overcome the fixed radius problems of netl. the networks in figure 1 defeat any shortest path algorithm. as table 1 shows  left hand side   since activation is computed continually  the effects of node + f are felt when it gets activated  and the is-a from +b to x is cancelled  allowing +c to become active. note that  because this is all default inferences  +c can't be inferred until c goes off  because -c blocks the inference of +g table 1 shows that the network of figure 1 b  works as well. in this case  it is practically impossible for x to come on  since + b is almost always inferred early  blocking the inference of x. if it did the network would still converge as it did for the network of figure 1 a . in this case  the dr. spock version was a little slower  because x took several iterations to decay to 1. the modifier links are strict  so that any output from x stops default inference of +c. 
     in all of the previous examples  the results from the two activation functions were essentially the same. the final example shows that this is not always the case. the example is given in figure 1. it is nearly identical to the example in figure 1 a   except that the inferences are all first order in the left hand chain. this example looks harmless because it has a unique extension. however  where the dr. spock activation function takes only slightly more than 1 iterations to converge on the extension  the spock function required over 1 iterations! to see why  recall that first order is-a's allow downward inference of -p's.  both a =  b and a =   b result in contrapositives with negative consequents . secondly  the delay in inhibition between a   +   node table 1. trace of unit outputs from example 1 
figure 1 a  
iteration 1 1 1 1 figure 1 b  
1 1 1 + a 
+ b + c ~c 
+ d 
+ e 
+ f 1 1 1 	1 
1 	1 1 	1 
1 1 1 	1 
1 1 	1 1 
1 	1 1 1 	1 
1 1 	1 1 	1 
1 1 	1 	1 	1 1 1 1 
	1 	1 1 
1 1 1 
1 1 1 
	1 	1 1 
1 1 1 
1 1 1 1 
1 
1 
1 
1 
1 and a  *   node induced by the   #   node allows inference chains to  pass  each other. what happens in this case is that the default inference of x starts a downward chain of  not  inferences. this meets an upward chain of positive inferences. they pass each other  then the consistency constraints begin turning off both 1- and predicate nodes in the middle of the chain. the inference chains then meet rather incorrigible resistance at both ends. +g has a hard time blocking the inference of x because by the time it gets there  its support is falling apart behind it. if it fails  x is inferred again straightaway  since + a is clamped on  and + p follows from that. then the process starts over again. dr. spock appears to be able to break out of this trap because the units are less  all or none . because +g decays slowly  it is more likely to be on when +p tries to infer x  since any nonzero output from +g blocks the inference. on the other hand  spock's activity resembles a search for a stable pattern of assignments o   1 or 1 to the units  which takes a lot longer. 

figure 1. this one causes spock some trouble  but not the dr. . 
　　this behavior of the spock function results from the fact that that the model does not wait for the results of first order knowledge to propagate before applying a default rule  and it took a while getting to the point of disagreement. in all of the other examples  the ambiguity was very localized. local ambiguities are not hard for connectionist networks to handle. it is ones that depend on global properties of the network that are hard to deal with and still maintain a small radius o  communication  an unspoken assumption in most connectionist models . there are several ways this problem could be avoided. 
　　a change to the model that may speed convergence on such examples is to make default inferences literally less strong than first order ones. if a default inference link is weighted by .1 instead of 1  then a first order inference could more easily overcome it. in this case  we need an activation function that converges to its evidence  to propagate the .1 value  unfortunately  this is not a property the mcclelland & rumelhart function  called dr. spock here   enjoys . the function: 

achieves this1. experiments with this scheme have been encouraging. the table lookup function could also be altered to reflect this other value. first order inferences which had a default at their source would then reflect that fact in their potential. in this way information that was non-local could be encoded in the signal. this still doesn't avoid the problem altogether. 1 here is no reason why a default inference could not be at the base of each chain. 
　　a second way to avoid this problem in the spock version which avoids this pitfall is to disallow downward inferences altogether  by going the way of netl  and assuming that everything is a default inference.  or by foregoing our attempt at completeness; hence not encoding the contrapositive.  this does avoid the problem of two default inferences being at the bottom of competing chains. the competition at the top is a local one  since either outcome is consistent. adding weights reflecting belief strengths  as advocated by rich  1   might make such a system a possible cognitive model. 
conclusions 
     we have seen how a connectionist model of inheritance mimics e&r's inference algorithm  avoiding the problems of netl. so  a massively parallel inheritance scheme apparently can work. two caveats should be mentioned. first  this is within the context of a very simple characterization of semantic networks. second  the examples are only an informal  engineers  argument for correctness. what is missing from this presentation is a formal proof of correctness. a first cut would be to show that if the network is in a stable state  then the predicate units that are firing along with the 
g. cottrell 1 
original w represent the  seed  of an extension  i.e.  their closure is the extension   leaving the problem of convergence for another time. 
　　a second point is that the choice of activation function can make a big difference in the speed of 
convergence of the network. the current results appear to favor smoother activation functions over the relatively discrete spock version. also  an interesting avenue for exploration now is using weights on the links to encode default strengths. this could also have a speed-up effect on convergence  and could possibly be an interesting cognitive model. finally  incorporating an evidence function that would better reflect the contribution of multiple sources of evidence is left for future research. 
acknowledgements 
i would like to thank james allen and jerry feldman for many helpful discussions on this paper  and mark fanty for prompt addition of the asynchronous facility to the c version of iscon. this document greatly increased in clarity through the comments of one patrick j. hayes  who nevertheless may be dismayed at this association with  connectionism . this work was supported by grants ist-1 and mcs-1 from the national science foundation. 
bibliography 
ballard  	d.h. 	 parameter 	networks.  	artificial 
intelligence  1  1  1. 
cottrell  g.w.  re: on inheritance hierarchies with exceptions   in proceedings of the workshop on nonmonotonic reasoning  new paltz  n.y.  october 1. 
etherington  d.  formalizing non-monotonic reasoning systems . tr 1  department of computer science. university of british columbia  1. 
etherington d. and r. reiter  on inheritance hierarchies with exceptions   in proceedings of the national conference on artificial intelligence  washington  d.c.  august 1. 
fahlman  s. e.  netl: a system for representing and 
using real-world knowledge . mit press  cambridge  mass  1. 
fahlman  s.e.  touretzky  d.s.  and w. van roggen. 
 cancellation 	in 	a 	parallel 	semantic 	network.'tn proceedings of the seventh international joint conference on artificial intelligence  vancouver  b.c.  1. 
feldman  jerome a. and dana ballard. connectionist models and their properties  cognitive science  1  1-1. 
ginsberg  	matthew 	w. 	 non-monotonic 	reasoning 
using dempster's rule.  in proceedings of the national 
conference on artificial intelligence  austin  texas  august  1. 

1 g. cottrell 
hayes  p. j. in defense of logic. in proceedings of the fifth annual international joint conference on artificial intelligence  cambridge  mass.  1. 
hinton  g.e. implementing semantic networks in parallel hardware. in g. e. hinton  & j. a. anderson  eds.   parallel models of associative memory  hillsdale  n.j.: lawrence erlbaum associates  1. 
hinton  g.e. and t. sejnowski. optimal perceptual inference. proceedings of the ieee computer society conference on computer vision and pattern recognition. washington  d.c 1. 
mcclelland  j.l. on the time relations of mental processes: an examination of systems of processes in cascade. psych. review. 1  
mcclelland  j.l. and d.e. rumelhart. an interactive activation model of the effect of context in perception: part i  an account of basic findings. psych. review  1  
reiter  ray  a logic for default reasoning   artificial intelligence 1  1  pp 1. 
rich  e.  default reasoning as likelihood reasoning.  in proceedings of the national conference on artificial intelligence  washington  d.c  august 1. 
shastri  l. and feldman  j.a. semantic networks and neural nets. t.r. 1  dept. of computer science  university of rochester  may 1. 
shastri l. and j.a. feldman. evidential reasoning in semantic networks: a formal theory. in this volume. 1. 
small  s. l.  shastri l.  brucks m.  kaufman s.  addanki  
s. and cottrell  g.w. iscon: an interactive simulator 
for connectionist networks  technical report 1  
department 	of 	computer 	science  	university 	of 
rochester  dec. 1. 
touretzky  david s.  the mathematics of inheritance systems.  ph.d. thesis  carnegie mellon university 1. available as t.r. cmu-cs-1. 
         1 or  the maximum possible output after decay if we use a thresholded potential for the output  and the activation function employs decay  a unit's maximum output is reduced by the decay factor. 
         1 an alternate formulation would have been to use two of these units so that the stronger unit would not receive its own inhibition  but the opposing units. we intend to explore this option in the future. 
         1 modifier links have not been implemented in the iscon  small et al. 1  simulator. the actual implementation at least doubles convergence times. see  cottrcll. 1  for details. 
         1 for a sketch of how to mimic their algorithm exactly  see  cottrell 1 . 
         1 note that in netl  as of 1   all inferences arc defaults and only exceptions to isnt-a's are allowed. 
         1 it turns out  unbeknownst to us when we derived it  that this is the function used by mcclelland in his  1  cascade model. 
   ---------------

   ------------------------------------------------------------

---------------

------------------------------------------------------------

