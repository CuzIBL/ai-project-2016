
arguably  analogy is one of the most important aspects of intelligent reasoning. it has been hypothesized that  given suitable background knowledge  analogy can be viewed as a logical inference process. this study follows another school of thought that argues that similarity can provide a probabilistic basis for inference and analogy. most similarity measures  which are frequently viewed as being conceptually equivalent to distance measures  are restricted to either nominal or ordinal attributes  and some are confined to classification tasks. this paper proposes a flexible similarity measure that is task-independent and applies to both nominal and ordinal data in a conceptually uniform way. the proposed similarity measure is derived from a probability function and corresponds to the intuition that if we consider all neighborhoods around a data point  the data points closer to this point should be included in more of these neighborhoods than more distant points. experiments we have conducted to demonstrate the usefulness of this measure indicate that it fares very competitively with commonly used similarity measures.
1 introduction
natural and artificial intelligent agents rely on different inference and reasoning strategies to achieve their goals and maintain their intended behavior. arguably  two of the more successful strategies are based on the notions of analogy 1

      also lita  universit└ de metz  ile du saulcy  1 metz cedex  france.
     1here we consider the subject of analogy in the following  relatively narrow  sense: objects are represented by feature vectors rather than the typical graphs; we do not consider transfer of knowledge from one object to another; we do not consider cross-domain knowledge but assume all objects share common set of attributes.
and similarity. this is evidenced by the large volume of ai research in the areas of analogical reasoning and case-based reasoning  vosniadou and ortony  1; kolodner  1 . in the context of available background knowledge  analogy can be viewed as a logical inference process  davies and russell  1 . another way of looking at analogy holds that similarity can provide a probabilistic basis for inference  and that a quantitative framework can be developed for the probability that an analogy is correct as a function of the degree of similarity measured or observed  russell  1 . this study is concerned with the latter view of modeling analogy.
　distance measures  or  equivalently  similarity measures  are central to many areas related to ai  including reasoning under uncertainty  knowledge-based systems  machine learning  pattern recognition  data mining  analogical  case-based  instance-based reasoning  and to other fields such as statistics  operations research and decision theory. a large number of distance metrics and measures are available and a particular choice may have considerable implications for the success of the problem that is to be addressed.
　distance functions are broadly categorized into those that can handle ordinal input data  nominal input data  and heterogeneous input data  consisting of both ordinal and nominal data 1. ordinal distance functions make use of the intrinsic total ordering relation in the underlying attribute values  which are either continuous  e.g. weight of an object  or discrete  e.g. number of obstacles . a nominal or symbolic attribute is a discrete attribute whose values do not necessarily exhibit a total order relation. for example  an attribute representing the role of crew member may have the values scientist  mission specialist and commander. using an ordinal distance measurement on such values is meaningless.
　because the notion of 'distance' is intrinsically numerical  most available distance measures are defined for data with ordinal attributes. however  distance measures that can

     1here we consider two scales of measurement: ordinal and nominal. in an ordinal scale  values  discrete or continuous  are ordered whereas in a nominal scale   discrete  values are unordered.
process nominal attributes are required by many modern ai algorithms. some measures that handle nominal attributes do exist. the most well known measure of this kind is the value difference metric  vdm   stanfill and waltz  1 . it is defined in terms of attribute values that are conditioned by posterior probabilities of a class. hence  like some other distance measures  the vdm is restricted to classification tasks.
　to cope with heterogeneous data  containing both nominal and ordinal attributes  two approaches are commonly taken:  1  the data is transformed into one of the two data types that complies with the distance measure used  and  1  two types of distance measures are combined and handle the data separately in accordance with the data type  see  for example   wilson and martinez  1  . however  both approaches are problematic when it comes to the interpretation of the results.
　the novel distance measure proposed in this study is called neighborhood counting metric  ncm . it is derived from a probability function and it can handle both nominal and ordinal attributes in a conceptually uniform way. intuitively  it can be understood or interpreted as follows  see figure 1 : if we consider all neighborhoods  n  around a data point  t  then those data points closer to t should be included in more of these neighborhoods  n  than points that are not close to t. usually neighborhoods are interpreted in terms of distance. however  if we adopted this interpretation  we would define one distance function in terms of another. to avoid this dilemma  we interpret neighborhood without distance through the concept of hypertuples  wang et al.  1  for both nominal and ordinal attributes. as a result  our distance measure applies to both ordinal and nominal attributes in a uniform way.
　the new ncm is conceptually simple  it is straightforward to implement  and it has the added property that it is independent of the underlying analytical or reasoning task  e.g. classification . the measure's clear and unambiguous meaning is defined by the number of neighborhoods of a query that include or cover a given data point.
　paper outline: section 1 presents a short review of distance measures relevant to this study; it is followed by sections 1 and 1  which present mathematical details of the new similarity measure. in section 1 the empirical evaluation of the method is presented and its results discussed. the paper concludes with a summary and a brief discussion of future work.
1 a brief review of important distance functions
two of the most common distance functions are euclidean distance and the hamming distance. the former is restricted to ordinal and the latter is usually used for nominal attributes. the heterogeneous euclidean-overlap metric  heom   wilson and martinez  1  combines the ham-

figure 1: an illustrative example. each of the 1 concentric circles represents a neighborhood of data point t defined by some distance measure. data point a is covered by 1 neighborhoods whereas b by only 1. geometrically  a is clearly closer  more similar  to t than b.
ming and the normalized euclidean distance functions and can therefore be used on heterogeneous data.
　the value difference metric  vdm   stanfill and waltz  1  is designed to handle nominal attributes in classification tasks only. it uses pre-computed statistical properties from available training data and can be interpreted in terms of probabilities. the heterogeneous value difference metric  hvdm   wilson and martinez  1  combines the euclidean distance and the vdm. it is therefore able to handle heterogeneous data  but because of its vdm heritage it is confined to classification tasks.
　the interpolated value difference metric  ivdm   wilson and martinez  1  extends the vdm to scenarios involving ordinal attributes. in the learning phase  it employs a discretization step to collect statistics and determine the probability for the discretized values  pa x c in the vdm formula   but then retains the continuous values in the training data for later use in the application or testing phase. the ivdm requires a non-parametric probability density estimation to determine the probability values for each class. the discretized value difference metric  dvdm  is the same as the ivdm except that it avoids the retention of the original continuous values but uses only the discretized values.
　 blanzieri and ricci  1  introduced the minimal risk metric  mrm   a probability-based distance measure for classification and case-based reasoning. it minimizes the risk of misclassification and depends on probability estimation techniques. as a result  the mrm exhibits a high computational complexity and is task-dependent.
1 a probability function
let v be a  non-empty  universe of discourse. a Σ-field f on v is a collection of subsets of v  such that:
1. v （ f ;
1. if a （ f then a1 （ f   where a1 is the complement of a;
1. if a b ，，， （ f   then a“b“，，， （ f .
　a probability function p over v is a mapping from f to  1  satisfying the three axioms of probability  ash  1 . if p is restricted to v  it is called a probability mass function  discrete  or probability density function  continuous .
suppose we have a probability function p as defined above.
for x （f let f x  be a non-negative measure of x satisfying f x1 “x1  = f x1 + f x1  if x1 ”x1 = 1/. as an example 
we can take f x  for the cardinality of x.
consider a function g : f ★  1  such that  for x （ f  
g x  = ‘ p e f x ”e /k
                         e（f where k = ‘e（f p e f e . it can be easily shown that g is a probability function.
　g x  is calculated from all those e （ f that overlap with x  i.e.  f x ”e  1= 1 . these e's are relevant to x and serve as the contexts in which g x  is induced. each such e is called a neighborhood of x. in other words a neighborhood of x is an element e of f   i.e.  a subset of v  such that e overlaps x.
　in practice p is usually not known  but a sample of data drawn from the data space according to p is commonly available. p can be estimated from the sample by either parametric or non-parametric techniques. in some tasks  e.g.  classification  the point-wise probability is needed. when there is insufficient data  which is not uncommon  the point-wise probability may be difficult to estimate using non-parametric methods. however  it is likely that probability can be estimated for some regions  contexts  in the data space based on the sample. using the g probability function  we can estimate the g probability for any single data point from knowledge of the p probabilities of various regions or contexts. this provides us with a formal way of inference about probability under incomplete  hence uncertain  situations.
　to calculate the g probability for x （ f in practice  we need to
1. find the set of all neighborhoods appropriate to the problem at hand 
1. estimate the p probability for every neighborhood 
1. finally  estimate  calculate  the g probability for x according to its definition.
in classification tasks  we can further calculate the conditional
g probability of a class given a single data point in a similar fashion. while this may appear to be computationally expensive  it is often possible to derive a simple formula for g.
　depending on what f is and how neighborhoods are defined  we can use the g probability for different tasks. in section 1 we take f to be the set of all regions in a multidimensional space definable as hypertuples and our similarity measure is derived from this interpretation.
1 estimation of g
this section describes how g is estimated for a data point from data samples. let d be a sample of data drawn from v according to an unknown probability distribution p. our aim is to calculate g  t|d  for any t （v.
　to calculate g we need p  which can be estimated from data assuming the principle of indifference as follows: for
any e （ f  
p  e|d  = |e|/n
where |e| is the number of elements in e and n is the number of elements in d. additionally we assume that f x  = |x| for x （ f .
we then have 
p  e|d 〜 f e ”t 
g  t|d =definition
=specification of f
=assumption that t is in e so e ”t = t
=estimation of p by principle of indifference
     = nk x‘（d expansion and then re-organisation where k is a normalisation factor  and cov t x  is the number of such e （ f that covers both t and x. we call this number the cover of x with respect to t.
　to obtain cov t x   a straightforward approach would be to iterate over all e （ f and check if e covers both t and x. clearly  such a process is undesirable because of its exponential complexity. in section 1 we will present an efficient way of calculating cov t x .
1 measuring distance through counting
this section considers an interpretation of neighborhood and derives a formula for calculating cov t x   which is then taken as a measure of similarity or distance.
1 neighborhood
pursuing a non-distance-based conceptualization of neighborhood  we interpret a neighborhood as a hypertuple  wang et al.  1   and a neighbor as a data point  or tuple  covered by some neighborhood. so a neighborhood of a tuple is a hypertuple that covers the tuple.
hypertuples
let r = {a1 a1 ，，，  an} be a set of attributes  and dom a  be the domain of attribute a （ r. furthermore let v def= ’ni=1dom ai  and l def= ’ni=1dom ai . v is the data space defined by r  and l an extended data space. a  given  data set is d  v - a sample of v.
　the attributes can be either ordinal or nominal. for simplicity  we assume the domain of any attribute is finite  but the results are not limited to finite domains.
　if we write an element t （ v by hv1 v1 ，，，  vni then vi （ dom ai . if we write h （ l by hs1 s1 ，，，  sni then si （ 1dom ai  or si   dom ai .
an element of l is called a hypertuple  and an element of
v a simple tuple. the difference between the two is that a field within a simple tuple is a value while a field within a hypertuple is a set. if we interpret vi （ dom ai  as a singleton set {vi}  then a simple tuple is a special hypertuple. thus we can embed v into l  so v   l.
　consider two hypertuples h1 and h1  where h1 = hs1 s1 ，，，  s1ni and h1 = hs1 s1 ，，，  s1ni. we say h1 is covered by h1  or h1 covers h1   written h1 ＋ h1  if for
i （ {1 ，，，  n} 
 
 x （ s i min s1i  ＋ x ＋ max s1i  if ai is ordinal i	if ai is nominal
furthermore the sum of h1 and h1  written by h1 + h1 def= hs1 s1 ，，，  sni  is: for each i （ {1 ，，，  n} 
 
	  {x （ dom ai  :	min s1i “s1i  ＋ x ＋ max s1i “s1i } 
si =    if ai is ordinal s1i “s1i  if ai is nominal
the product operation   can be similarly defined. it turns out that hl ＋ +  i is a lattice.
　for a simple tuple t  t ai  represents the projection of t onto attribute ai. for a hypertuple h  h ai  is similarly defined.
　a hypertuple can be generated by taking one subset from each attribute. let ai be an attribute  i = 1 ，，，  n; si be the set of all subsets of the domain of ai; and n. then a hypertuple h is an element of ’i si. therefore the number of all hypertuples is.
　if ai is nominal  then si = 1dom ai  and so ni1 = 1mi  where mi = |dom ai |. if ai is ordinal  then an element s （ si corresponds to an interval  min s  max s  . as a result  some elements of si may correspond to the same interval  and hence become equivalent. in general we have the following number of distinctive intervals for an ordinal attribute:
ni1 .
neighborhoods as hypertuples
for any simple tuple t  a neighborhood of t is taken to be a
hypertuple h such that t ＋ h; and a neighbor of t with respect to h is x （ v such that x ＋ h. by this definition any simple tuple has a neighborhood. at least the maximal hypertuple in the extended data space is a neighborhood of any simple tuple since the maximal hypertuple covers all simple tuples.
　for a query t （ v  not all hypertuples in ’i si are neighborhoods of t. for a hypertuple h to be a neighborhood of t we must have t ai  （ h ai  for all i. therefore  to generate a neighborhood of t  we can take an si （ si such that t ai  （ si for all i  resulting in a hypertuple hs1 s1 ，，，  sni. if ai is nominal  the number of si （ si such that t ai  （ si is
ni1 mi 1 since si is any subset of dom ai  that is the super set of t ai . if ai is ordinal  this number is ni1 =  max ai   t ai  + 1  〜  t ai    min ai  + 1  since  max ai  t ai +1  is the number of ordinal values above t ai   and  t ai  min ai +1  is such a number below t ai . any pair of values from the two parts respectively forms an interval.
　to summarize  the number of neighborhoods of t is ’i ni1  where
 1 
 
	  1mi 1 	if ai is nominal
ni1 =    max ai  t ai +1 〜 t ai  min ai +1  
if ai is ordinal.
cover of simple tuples
under the above interpretation of neighborhood  we know exactly the number of all neighborhoods of a given simple tuple t. here we present an efficient way of calculating cov t x   the number of neighborhoods of t that cover x  which is needed in section 1.
　consider two simple tuples t = ht1 t1 ，，，  tni and x = hx1 x1 ，，，  xni. a neighborhood h of t covers t by definition  i.e.  t ＋ h. what we need to do is to check if h covers x as well. in other words  we want to find all hypertuples that cover both t and x.
　eq.1 specifies the number of all simple tuples that cover t only. we take a similar approach here by looking at each attribute and explore the number of subsets that can be used to generate a hypertuple covering both t and x. multiplying these numbers across all attributes gives rise to the number we require.
　consider attribute ai. if ai is ordinal  then the number of intervals that can be used to generate a hypertuple covering both xi and ti is as follows:
ni = max ai  max {xi ti} +1 〜 min {xi ti}  min ai +1 .
if ai is nominal  the number of subsets for the same purpose
is:	 
1mi 1 if xi = ti
	ni =	1mi 1 otherwise
recall that mi = |dom ai |.
　to summarize  the number of neighborhoods of t covering x is cov t x  = ’i ni  where
 1 
 
     max ai  max {xi ti} +1 〜 min {xi ti}  min ai +1  
niif ai is ordinal
 	if ai is nominal and xi = ti if ai is nominal and xi 1= ti
1 use of cover as similarity measure
we use cov t x  as a measure of similarity between t and x  which we call the neighborhood counting metric or simply ncm. that is  for any two tuples x and y  the ncm between them is
n
 1 	ncm x y  = cov x y  = ’ni
i=1
where n is the number of attributes and ni is given by eq.1. it is clear that ncm x y  − 1  ncm x x  − ncm x y  and ncm x y  = ncm y x . therefore the ncm is reflexive and symmetric  the properties generally required for a similarity measure  osborne and bridge  1 .
　this measure can be interpreted intuitively as follows. if we consider all neighborhoods around a data tuple  those tuples closer to this tuple should be included in more neighborhoods and those farther away should be included in fewer neighborhoods  see figure 1 . as a result  closer tuples should be assigned higher cover values.
　in contrast to its usual interpretation in terms of distance  we interpret the notion of a neighborhood without distance through the concept of hypertuples for both nominal and ordinal attributes. as a consequence  our novel distance measure applies to both ordinal and nominal attributes in a conceptually uniform way.
　incidentally  the ncm intrinsically handles missing values in a fashion consistent with other measures. recall  that the ncm is a product of all ni's  where i is attribute index. for two data tuples  t and x  if there is a missing value in t or x for attribute i  then ni is set to 1. as a result  this attribute does not contribute towards the ncm value.
1 evaluation
the new neighborhood counting metric is task-independent and can therefore be used for classification  clustering and other analytical tasks involving distances or similarities. we empirically evaluated the ncm in the context of a classification task  using the k-nearest neighbor  k-nn  classification algorithm with and without distance-based  neighbor  weighting  baily and jain  1 . the purpose of the evaluation is to compare the new measure with some of the commonly used distance measures in a setup involving heterogeneous data. the evaluation uses public benchmark data sets from uc irvine machine learning repository  which were selected with respect to their balance of ordinal and nominal attributes.
　we implemented a k-nn algorithm with the novel ncm as well as the measures heom  hvdm  ivdm  and dvdm. the computational runtimes of the mrm turned out too excessive  so it was excluded from the study. in the experiments k was set to 1 1 1 1 and to 'maxk'  i.e.  k = the number of data tuples in the training data . we adopted a 1-fold cross-validation procedure  and for each measure and each k value we ran the cross-validation 1 times and recorded the results for subsequent analysis.
　due to space limitations  we report only some of the results in detail. table 1 shows the results when weighting was used and k = maxk.
　a statistical significance analysis was carried out using the student t-test  two samples  assuming unequal variances  with α = 1  at a 1% confidence level . for each data set  each measure and each k value  we ran a 1-fold crossvalidation using k-nn 1 times with random partitioning of data  resulting in a sample of 1 values. for a pair of samples  by two different measures  we have a total of 1 values. so the critical value is 1. we then calculate the 't' values. if t − 1 or t ＋ 1 the two samples are significantly different  the classification rate of one sample is significantly higher or lower than that of the other . notice that every value in table 1 is the average of a sample of 1 values.
　from table 1 we can see that based on our experimental design the ncm achieved 1 out of 1 'significantly' higher classification success rates compared to the other methods  last row in table . looking at the subtotals for 'nominal'  'ordinal' and 'mixture' categories  reflected by subtotals from top to bottom   the ncm achieves the largest margin over its competitors for the 'nominal' data sets  followed by 'mixture' and then 'ordinal'. this suggests that  when all data tuples are taken into consideration  the ncm is clearly superior under all circumstances.
　the details of the results involving other values for k are not shown. in terms of k-changes  we observe that without weighting there was no significant difference between the used measures. when weighting was used  there was still little difference for small k values. the general trend for all measures was: as k got larger the classification success rate increased slightly but soon started to decline. the difference then showed up as the ncm displayed a much slower rate of decline and  after k   1  the ncm consistently outperformed the other four measures  see k = maxk as discussed above . we can conclude that the ncm produces less variable or more robust results than the other four measures.
1 conclusion
starting from a probability function  we have developed a novel similarity or distance measure  the neighborhood counting metric  which can be used with ordinal  nominal and heterogeneous attributes in a conceptually uniform way. this measure is defined without reference to any particular analytical task  e.g.  classification . this means that the measure is potentially useful for many reasoning  inferential and reasoning tasks and systems modeling analogy or similarity. because the ncm is based on a simple  easy-to-implement mathematical formulation and has a computational complexity in the same order as the euclidean distance measure  it is a prime candidate for practical ai methods and tools.
　our empirical evaluation demonstrates that in a k-nnbased classification task  the measure significantly outperforms its competitors when distance-based neighbor weighting is used and when k is not too small  in particular when all data tuples are taken into consideration. as k gets larger  the ncm displayed a consistently superior performance over the reference methods. the difference is most significant when all data tuples in a training data set are considered. this implies that the ncm is less sensitive to k than the other measures considered in this study. given an application based on the k-nn algorithm  if the optimal value for k is not known  we can simply consider all or a relatively large set of data tuples without a significantly compromising performance. therefore  the performance of this measure is more predictable than the other methods investigated in this study.
　in our experiments  all attributes were assumed to have equal weights. future work will investigate how to determine the best attribute weights to achieve improved performance  as well as an application of the ncm to clustering tasks.
