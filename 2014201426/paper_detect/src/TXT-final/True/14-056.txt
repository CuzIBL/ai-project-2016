: many knowledge-based expert systems employ numerical schemes to represent evidence  rate competing hypoth eses  and guide search through the domains problem space. this paper has two objectives: first  to introduce one such scheme developed by arthur dempster and glen shafer  to a wider audience; second  to present results that can reduce the compu tationtime complexity from exponential to linear allowing this scheme to be implemented in many more systems. in order to enjoy this reduction  some assumptions about the structure of the type of evidence represented and combined must be made the assumption made here is that each piece of the evidence either confirms or denies a single proposition rather than a disjunction for any domain in which the assumption is justified the savings are available 
1. introduction 
how should knowledge-based expert systems reason  clearly when domain-specific idiosyncratic knowledge is available it should be formalized and used to guide the inference process problems occur either when the supply of easy-to-formalize knowledge is exhausted before our systems pass the  sufficiency  test or when the complexity of representing and applying the knowledge is beyond the state of our system building technology unfortunately  with the current state of expert system technology  this is the normal  not the exceptional case 
at this point  a fallback position must be selected  and if our luck holds  the resulting system exhibits behavior interesting enough to qualify as a success. typically a fallback position takes the form of a uniformity assumption allowing the utilization of a non domain-specific reasoning mechanism for example  the numer ical evaluation procedures employed in mycin  and internist  1  the simplified statistical approach described in 
  and a multivalued logic in  1 . the hearsay-ii speech understanding system  provides another example of a numer teat evaluation and control mechanism-however  it is highly domain-specific 
section 1 describes-another scheme of plausible inference  one that addresses both the problem of representing numerical weights of evidence and the problem of combining evidence the scheme was developed by arthur dempster  1  1  1  1  1  1  then formulated by his student  glen shafer  1 . in a form that is more amenable to reasoning in finite discrete domains such as those encountered by knowledge-based systems the theory 
 this research is supported by the defense advanced research projects agency under contract no dahc1 c1 views and conclusions contained in this report are the author' s and should not be interpreted as representing the official opinion or policy of darpa the us government or any person or agency connected with them reduces to standard bayesian reasoning when our knowledge is accurate but is more flexible in representing and dealing with ignorance and uncertainty section 1 is a review and introduction other work in this area is described in . 
section 1 notes that direct translation of this theory into an implementation is not feasible because the time complexity is exponential however if the type of evidence gathered has a useful structure  then the time complexity issue disappears section 1 proposes a particular structure that yields linear time complexity in this structure  the problem space is partitioned in several independent ways and the evidence is gathered within the partitions the methodology also applies to any domain in which the individual experiments  separate components of the evidence  support either a single proposition or its negation 
section 1 and 1 develop the necessary machinery to realize linear time computations it is also shown that the results of experiments may vary over time  therefore the evidence need not be mono tonic section 1 summarizes the results and notes directions for future work in this area 
1. the dempster-shafer theory 
a theory of evidence and plausible reasoning is described in this section it is a theory of evidence because it deals with weights of evidence and numerical degrees of support based upon evidence further  it contains a viewpoint on the representation of uncer tainty and ignorance it is also a theory of plausible reasoning because it focuses on the fundamental operation of plausible reasoning  namely the combination of evidence the presentation and notation used here closely parallels that found in  
after the formal description of how the theory represents evidence is presented in section 1. an intuitive interpretation is given in section 1. then a comparison is made in section 1. to the standard bayesian model and similarities and differences noted the rule for combining evidence  dempster s orthogonal sum is introduced in section 1 and compared to the bayesians' method of conditioning in section 1. finally  section 1 defines the simple and separable support functions these functions are the theory's natural representation of actual evidence 
1. formulation of the representation of evidence let 1 be a set of propositions about the exclusive and exhaustive possibilities in a domain for example  if we are rolling a die  g contains the six propositions of the form 'the number showing is i' where 1  i 1. 1 is called the frame of discernment and 1¡ã is the set of all subsets of 1 elements of 1 e   i.e.. subsets of 1  are the class of general propositions in the domain; for example  the proposition the number showing is even' corresponds to the set of the three elements of g that assert the die shows either a 1  1  or 1. 
the theory deals with refimngs  coarsenings  and enlargements of 

1 


1 


1 

1. algorithms and computations 
the goal is to calculate quantities associated with m= e 1...$e n   where n ¡ö |1| and the e  are the simple evidence functions defined in the previous section. all computations are achieved in o n  time measured in arithmetic operations. 
figure 1 is a schematic of information flow in a mythical system. the ¦Ì and ¦Ìx. may be viewed as sensors  where a sensor is an instance of a knowledge source that transforms observations into internally represented evidence  i.e.. belief functions. each is initially v. the vacuous belief function as time passes and events occur in the observed world  these sensors can update their state by increasing or decreasing their degree of support. the simple 
*l have not proved this however  if the formulae introduced in section 1 are evidence function  ei. recomputes its state  a  and f . and changes 
directly implemented  then the statement stands 
1 


1 

1 


1 

evidence narrow down the possibilities first  then apply some ad hoc method afterward 
another problem  not peculiar to this theory  is the issue of independence. the mathematical model assumes that belief functions combined by dempsters rule are based upon inde pendent evidence  hence the name orthogonal sum when this is not so  the method loses its feeling of inevitability. also  the elements of the frame of discernment. 1  are assumed to be exclusive propositions. however  this is not always an easy constraint to obey. for example  in the mycin application it seems natural to make the frame the set of possible infections but the patient can have multiple infections. enlarging the frame to handle all subsets of the set of infections increases the difficulty in obtaining rules and in their application; the cardinality of the frame grows from |1| to 1 e|. 
one more problem that deserves attention is computational efficiency. above it is shown that  with a certain set of assumptions  it is possible to calculate efficiently. however  these assumptions are not valid in all or even most domains a thorough investigation into more generous assumptions seems indicated so that more systems can employ a principled reasoning mechanism 
the computational theory as presented here has been imple mented in simula. listings are available by wnttmg directly to the author. 

1. 