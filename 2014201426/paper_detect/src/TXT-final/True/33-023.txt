 
　　this paper describes a system that learns the rules of pronunciation inductively. it begins with a set of 1 rules for single-letter pronunciation. individual words are presented to i t   and the system uses its rule set to hypothesise a pronunciation. this is compared with a dictionary pronunciation  and if any part of the pronunciation is incorrect new rules are created to handle the word as an exception condition. 
   these rules are checked for similarity with others already produced  and where suitable a  general  rule is produced to deal with two or more created rules. the effect is to produce rules that are more and more general  and these approach the general pronunciation rule sets that have been 
produced manually by other workers. 
i introduction 
　　conversion of unrestricted english text into i t s phonetic equivalent is usually performed using context dependent rules  which dictate how a character string should be pronounced when in a particular context. the rules are arranged so that one set exists for each letter of the alphabet. the rules within these sets appear in the order in which they should be applied i.e. more specific rules appear before more general rules. each set ends with a context independent rule 
 default rule  which is used if no rule matches the context. 
   systems using such rules operate on one word at a time  which is scanned from left to right. the rules are scanned sequentially until one that matches is found. the character s  handled by that rule are discarded  and the process repeated until the whole word has been processed. the result is a set of phonemes showing the system's pronunciation of that word. 
     the f i r s t such system of any significance was written by ainaworth  1   who produced a speech system that used a set of 1 rules for which he  sponsored by the science research council 
claimed an error rate of about 1% a second more recent system produced by an american group  1  is rather more successful. they changed and enhanced ainsworth's rules to produce their own set of 1 rules  and claimed a success rate of 1% of words correctly translated  and 1% of phonemes correctly translated  in an average text sample. 
   the system described in this paper automates the production of these rules by deducing them from samples of the input and required output. it is written in lisp  and uses the following rule format:-
   ch.string  l.context  r.context  phoneme-set    
 two other fields are contained at the end of each rule  but these w i l l be described later.  the character string being searched for is at the beginning of the rule; then come the l e f t and right context character strings; followed by the latin equivalent of the international phonetic alphabet phonemes  see appendix 1 . 
   as an example of rule format and the translation operation  suppose the rule set for the letter a was:-
	 a 	1 	 c1ns1 e delim  	 ey   	1 
	 a 	   	   	 ae   	1 
 c1ns1 and delim are group identifiers - see appendix 1.  
   the f i r s t rule states that an a preceded by anything and followed by a single consonant  an e  and the end of the word receives the pronunciation ey. the second rule is the default  and w i l l match any other a  giving it the pronunciation ae. 
   if the a of the word pale was being processed then this would f i t the f i r s t rule and receive the pronunciation ey* on the other hand  a in the word pat does not f i t this rule so the system tries the next rule  which does match  giving it the 
pronunciation ae. 
　　the american group  1  produced their rule set by following an iterative procedure which involves looking at erroneously pronounced words and trying to produce rules to overcome the errors. this procedure when used to translate a large dictionary involves a great deal of work in checking the output  f i r s t l y to see if the words have indeed 

1 

been pronounced correctly  then in formulating new rules to correct the errors. this process needs repeating several times to produce a good set of rules and for each iteration the individual checking of each word must take place. this process of detecting the errors in pronunciation could be carried out automatically if some  correct'* pronunciation  ie a standard dictionary pronunciation  was held alongside each word in the dictionary. 
　　this led on to automatic rule generation. a system with the above information included has a l l the data available to enable it to develop i t s own rules. the main problem with such a system is deciding how to implement new rules from an examination of the pronunciation errors produced. 
   basically the approach taken is to use a rule set to hypothesise a pronunciation: check this against a standard pronunciation: if the pronunciation is in error  create a special purpose rule to correct the error s : then find similar special purpose rules and combine them to produce slightly more-general rules  which may in turn be combined together to produce more-general rules. 
ii role production 
   this is the process of taking a word in normal ihglish  plus i t s pronunciation  ip a code equivalent   and hypothesising i t s pronunciation by using the current rule set. in a similar fashion to rule-based systems  the rules are searched from the top until one that matches both contexts is found. this requires rules to be ordered with the more-specific preceding the more-general.  the rule set is initialised to the single-letter pronunciation rules given in appendix 1  and this is augmented by rules produced by the system.  
   the hypothesised phonetic spelling and the dictionary's phonetic spelling obtained from the input can then be compared. if no differences are found then the rules in the data base require no amendment. however  if differences occur  then it is necessary to produce new rules to augment the database. for example:-
i n i t i a l l y the word pale gets the pronunciation p / ae / l / eh 
this is compared with the correct phonetic spelling 
             p / ey / l and specific rules are produced which say that 'a' preceded by *p' and followed by 'ie' has the long vowel pronunciation  and that the final e is silent 
i.e.  a  delim p   l e delim   ey   and   e  delim p a l    delim       
 the delim's mark both ends of each word.  these two rules would be added to the rule sets of a and e respectively. 
　　the comparing routines  it was found  work best by looking at the contexts for  exact matches . 
the system ie always able to find at least two exact matches  these being the delim at both ends of the word. after a l l of the  exact matches  within the word are found then what's left in the computer produced pronunciation must translate to what's l e f t in the actual pronunciation. 
   a match is deemed to be  exact  if the dictionary phoneme matches only one machine produced phoneme in the locality of the one being considered. checking one machine produced phoneme either side was found to be adequate. example:-
	d	u	c	k 	 word  
delim 	d 	ah 	k 	k 	delim 	 computer  
delim 	d 	ah 	k 	delim 	 actual  
　　the f i r s t k in the actual pronunciation is not an  exact  match with the k in the computer pronunciation as there is another k in the locality. the next exact match is found with the 
final delim's  so the rule produced i s : -
	  ck 	 delim d u  	 delim  	 k   
	one 	other 	factor 	in 	the 	process 	is 	the 
look-ahead-count. this global variable determines how far ahead the system should look to find an exact match. if no match is found within the limit then the system moves on to consider the next machine phoneme. the count is i n i t i a l l y set to 1  but is dynamically increased by the system if the delims at the end of each word are not encountered together. this ensures that long words w i l l be matched correctly. example:-
	s	e	a	t 	 word  
delim s eh ae t delim  computer  delim s iy t delim  actual  
exact matches are found up to the s  but the iy and 
eh do not match. no iy is found on the look ahead so t and ae are compared next. these also do not match  but within the look ahead limit the exact match t t is found. this means the following rule is produced:-
	  	ea 	 delim s  	 t delim  	 iy   
   the above system w i l l create rules to correctly translate a l l the words it has met  but as several rules are usually needed to translate each word this  as it stands  is not very useful. a method of creating rules which w i l l translate words the system has not met is required. 
	iii 	rule induction 
a. generalisation 
   the purpose of this part of the system is to produce general rules from consideration of the very specific rules produced by the f i r s t part of the system. this process is performed every time a new basic rule enters the system. rules are only considered for generalisation  if they translate the same character string to the same phoneme  

no 

t the l e f t and right contexts of the rules under consideration are examined and a pairing off of the context elements is undertaken. 
　　the three different ways of combining context elements can be shown by considering the following example pairing from l e f t to right:-
contextl  p c1ns1 	p 	p 	vowel vowel  context1  p c1ns1 c1ns1 	t 	c1ns1  giving  p c1ns1 c1ns1 c1ns1  type a 	a 	b 	c 	d 	d type a is an exact match between elements; type 
b occurs if one is a member of the other; type c if both are of the same group; and type d is a mis-match i.e. vowel versus c1ns1 and any context elements that remain when the pairing can go no further. 
   it can be seen that the type c combination is more of a generalisation than types a and b. once a group such as c1ns1 has been inserted into the resultant rule then it w i l l allow any consonant in that position  even though it was created using just two consonants. 
　　a weighting system was used to reflect these differences in type. types a and b are given a value of +1; type c a value of 1; and type d a value of - 1 . 
   left and right contexts are considered separately' and the total weight is calculated for each. the number of letters in the character string of the rule  the f i r s t field  is added to both of these  these letter s  are counted as exact matches  type a  . if the resultant values from left and right contexts are both positive  then the two rules are combined to produce a 
   general rule. 
   for example  if the rules under consideration are:-
 ck  delim d u   delim   k    ck  delim l u   delim   k   
the left context total weight is 1  the right is 1 so combination is allowed  and the final general rule appears as:-
	 ck 	 delim c1ns1 u  	 delim  	 k   
　　this rule when added to the database w i l l correctly translate the ck part of the words creating the rule as well as other words such as muck  suck  etc. 
but the combination of the rules:-
	 a 	 delim p r i v  	 t e delim  	 ax   
　　 a  delim w 1 m   n delim   ax   would not be allowed. the left context weight gives - 1   as does the right. 
   there are two other fields in the rules. the f i r s t of these is the l i s t of rules used to create a general rule. in the case of specific rules  basic rules  this field is null  but in a l l others it holds the rules which were picked out and combined to form the general rule. these sub-rules may themselves contain the rules they were created from. thus a general rule contains a complete history of how it was formed. 
   the second field is a rule usage count. this number indicates how many times a rule has been successfully used. on creation of a new rule it takes the value 1 and on creation of a general rule it takes on the sum of the counts of i t s composing rules. each time a rule successfully translates a letter sequence  i t s count is incremented by 1. effectively  then  the count shows how many words the rule has translated correctly. for example:-
1.  ck  delim c1ns1 vowel   delim   k   1 
1.  ck 	 delim 	b 	a  	 delim   k   1 
1.  ck 	 delim c1ns1 	u  	 delim   k   1 
	k. 	 ck 	 delim 	d 	u  	 delim   k   1 
	1. 	 ck 	 delim 	l 	u  	 delim   k   1 
rules 	1 	and 1 were combined to give rule 1  which was combined with  basic  rule 1 to 	give 	rule 	1. the usage count appears following the rule. 	this format shows pictorially the way a 	rule 	has 	been derived  	which 	is 	very useful when debugging and investigating other areas of the system. 
b. rule conflict 
　　rule conflict occurs when the l e f t and right context of two rules for the same letter are the same with the rules producing different phonemes. when this condition is detected the system evaluates the importance of the offending general rules by seeing which satisfies the most words  using the rule usage count . this one is kept  and the other discarded and replaced by i t s composing rules one level down. rules may be made and broken several times  depending on the order of the words accepted into the system  but usually a clear  winner  emerges. 
   unfortunately  there are other  more-complex  types of conflict. given two rules 
a.  stringl 	 a-left  	 a-right  	 phonemel   
b.  string1 	 b-left  	 b-right  	 phoneme1   
where phonemel does not equal phoneme1  then the different combinations of l e f t and right contexts may be represented using the following table:-
table 1: types of conflict 

1 


   priority ambiguity can be illustrated as follows:-
	 	a	 
 delim vowel  1 
which is the more general   1  is not restricted to having a delimiter before it and  1  is not restricted to just the vowel  a . action taken on finding conflict is outlined below. 
1. no conflict  type 1  
   when a new rule enters the system it is compared with each rule in i t s letter l i s t . this comparison starts at the end of the l i s t  with the default rule  and finishes at the top. if no conflict condition occurs then the new rule is inserted at the top of the l i s t . 
1. proper conflict  type 1  
　　　if during the above process conflict is discovered then the count of the new rule is compared with that of the conflicting rule. the rule with the highest count is retained  and the other rule is broken into i t s constituents and input to the system without trying to combine them with other rules in the l i s t . 
1. partial conflict  type 1 or 1  
   if a type 1 conflict occurs as the comparison operation is taking place  then the incoming rule is inserted into the rule l i s t immediately following the rule it conflicts with  but see part iv . if however it is a type 1 conflict the comparison passes over  and therefore above  the conflicting rule and continues on until conflict occurs again  or the top of the l i s t is found and the rule inserted there. this partial conflict handling therefore causes the rules to be ordered with the more general following the less general. 
1. ambiguity 	 type 1  
   while this type is reported by the system  no special action is taken. no adverse effects seem to occur by ignoring it in this way  as stabilisation  see part iv  corrects any resultant rule misplacement. 
　　occasionally  rules produced towards the end of reading a batch of words can negate the effect of rules produced earlier. to overcome such errors and check the rules more generally  the batch of words are again input to the system. all the words bhould be pronounced correctly but if this is not the case then the rule set must be incorrect. the following example shows this. 
	consider 	the 	two 	following ordered  s  rules 
 where the indented rules listed below them are the rules that were combined to form the general rule :-

when  was  is reinput  it is translated by rule 1 rather than rule 1  since 1 precedes 1 in the rule set   giving the wrong phoneme for the  s . this requires the following  specific  rule to be created:-
	 s 	 delim w a  	 delim  	 z   
and this is checked against the current rule set in the usual way to look for possible generalisations. it of course finds rule 1  and attempts to combine the two. 
　　　at this point  the system checks to see if the rule being combined with rule 1 has already been combined with other rules to produce rule 1. finding that it has  as 1a   rule 1 is broken into i t s  immediate  lower-level components  and these are re-entered individually into the system  as if they had just been created . 
　　this in i t s e l f is insufficient  in that if no other action was taken these components would merely be recombined to give the original  over-general  rule. to stop this  during recombination the system w i l l only combine those rules which have exactly the same pattern. 
   using the above example  then  rule 1 is broken to give the four separate rules 1a to 1d. with exact matches now needed for generalisation  the  was  and  has  rules can be combined to give:-

iv rule stabilisation 
     the 	system 	so far described reads in a set of words  creates basic rules to translate these words and attempts to form general rules from examination of these basic rules. 

     there are occasions when this is insufficient and the same rule is s t i l l formed. this 
1 

situation is handled by breaking the other rule  ie the one that interfered with the rule that should have been used - rule 1 in the example . 
     if 	this 	also recombined to form the same rule  even under this stricter control  then 	the 	least used 	rule 	is broken and i t s components re-entered into the rule set without allowing any combination. 
   this process of re-inputting the words is repeated until a l l the words are translated correctly and the system is stable. this usually takes 1 or 1 iterations. the system is then complete in that it w i l l correctly translate a l l the words given to i t . 
v results 
   the system has three modes of operation. the f i r s t is the normal reading of words and creation of rules; the second is the stabilisation procedure; and the third is a rule evaluation mode. this third mode enables the system to read words  show their pronunciation using i t s rule set and compare this with the actual pronunciation to enable statistics to be produced. these show the number of words the system has pronounced correctly and the number of phonemes correctly produced. during this process the rule set is not changed in any way. 
     if a l l the words that created a set of rules were run in this way  then the statistics would always show 1* correctness of pronunciation. to obtain more meaningful results a l l the basic rules are deleted before evaluation. the resultant statistics give an indication of how good the general rules produced are. it may also be the case that some of the basic rules may not be needed anyway as the general rules may have developed far enough to make them redundant. 
   the usage counts of a l l the rules are zeroised before an evaluation run and incremented during i t   so the figures for usage are correct at the end of a run. any unused rules  usage count of zero  can be deleted. 
   the result is a set of figures showing the proportion of phonemes and words correctly translated by the rule set. in addition  the l i s t of rules used is given together with an indication  via the usage count  of the usefulness of individual rules. 
　　evaluation  runs have been carried out on the rules produced from reading the frequency based words from the ladybird key word books  1   and the dictionary entries  1  for the f i r s t two letters of the alphabet  run separately . to act as a comparison  the same data have been run through a manually produced set of rules. this was obtained by anglicising the rules produced by the american group  1 . the results are shown in the following table:-
   the original american rule set was claimed to have a success rate of 1* words correct and 1* phonemes correct  but it should be stressed that these figures were derived from listening tests  not comparison with dictionary definitions. their figures are also frequency weighted i.e. correct translation of a frequent word was more highly rated than an infrequent word. 
     this is shown in the table  where the best results were obtained from the ladybird words  the most frequent 1 words of juvenile reading . here the percentage correct produced by the manual system approaches the figures quoted. the deficiency can be accounted for by the different method of defining correctness and also because the anglicising may have led to deterioration of the rules. 
　　it can be seen that there is a large discrepancy between the results for the a's and b's for both the automatic and the manual systems. this is caused by the difficulty of finding a rule to define the pronunciation of i n i t i a l a's. the automatic system spent nearly 1 minutes trying to come up with a suitable rule to define which word should start with ae and which with ax. the problem is basically one of stress and  it is interesting that no set of rules appears to exist to handle this situation. 
　　the  b  run is a better test as it is not so affected by the stress problem. the automatic system took only 1 minutes to come up with a set of rules that perform much better than the  a  set. 
   in both the a's and the b's  the automatic and manual system are close in their results and only in the ladybird set do they differ significantly. frequent words often have unusual pronunciation   i . e . they do not follow set rules  so in most systems they are usually handled by a small exceptions dictionary. this is similar to the system's basic rules  so by deleting them the system's performance has been seriously affected for frequency based words input to i t . this is also reflected in the rules used  where the manual system has used three times as many as the automatic system. 
1 

　however  the system does successfully learn rules governing the pronunciation of words from examples. a number of rules produced are exactly the same as some in the manually produced set  while others show considerable similarity. and the rule set produced from a section of dictionary performs approximately as well as the manually produced set. 
	appendix i: 	constants used in the system 
delim : word delimiter  space  f u l l stop  etc.  
vowel : a  x  i  1  u 
c1ns1 : b c d f q h j k l m n p q r s t v v x y z 
initial rule set: 

appendix i i : phonetics used in the system 

