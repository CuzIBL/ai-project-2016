 
this paper presents a new measure of semantic relatedness between concepts that is based on the number of shared words  overlaps  in their definitions  glosses . this measure is unique in that it extends the glosses of the concepts under consideration to include the glosses of other concepts to which they are related according to a given concept hierarchy. we show that this new measure reasonably correlates to human judgments. we introduce a new method of word sense disambiguation based on extended gloss overlaps  and demonstrate that it fares well on the senseval-1 lexical sample data. 
1 introduction 
human beings have an innate ability to determine if two concepts are related. for example  most would agree that the automotive senses of car and tire are related while car and tree are not. however  assigning a value that quantifies the degree to which two concepts are related proves to be more difficult  miller and charles  1 j. in part  this is because relatedness is a very broad notion. for example  two concepts can be related because one is a more general instance of the other  e.g.  a car is a kind of vehicle  or because one is a part of another  e.g.  a tire is a part of a car . 
　this paper introduces extended gloss overlaps  a measure of semantic relatedness that is based on information from a machine readable dictionary. in particular  this measure takes advantage of hierarchies or taxonomies of concepts as found in resources such as the lexical database wordnet  fellbaum  1 . 
　concepts are commonly represented in dictionaries by word senses  each of which has a definition or gloss that briefly describes its meaning. our measure determines how related two concepts are by counting the number of shared words  overlaps  in the word senses of the concepts  as well as in the glosses of words that are related to those concepts according to the dictionary. these related concepts are explicitly encoded in wordnet as relations  but can be found in any dictionary via synonyms  antonyms  or also-see references provided for a word sense. to our knowledge  this work represents the first attempt to define a quantitative measure of 
natural language 
relatedness between two concepts based on their dictionary definitions. 
　this paper begins with a brief description of wordnet  which was used in developing our measure. then we introduce the extended gloss overlap measure  and present two distinct evaluations. first  we conduct a comparison to previous human studies of relatedness and find that our measure has a correlation of at least 1 with human judgments. second  we introduce a word sense disambiguation algorithm that assigns the most appropriate sense to a target word in a given context based on the degree of relatedness between the target and its neighbors. we find that this technique is more accurate than all but one system that participated in the senseval-1 comparative word sense disambiguation exercise. finally we present an extended analysis of our results and close with a brief discussion of related work. 
1 wordnet 
wordnet is a lexical database where each unique meaning of a word is represented by a synonym set or synset. each synset has a gloss that defines the concept that it represents. for example the words car  auto  automobile  and motorcar constitute a single synset that has the following gloss: four wheel motor vehicle  usually propelled by an internal combustion engine. many glosses have examples of usages associated with them  such as  he needs a car to get to work  
　synsets are connected to each other through explicit semantic relations that are defined in wordnet. these relations only connect word senses that are used in the same part of speech. noun synsets are connected to each other through hypemym  hyponym  meronym  and holonym relations. 
　if a noun synset a is connected to another noun synset b through the is-a-kind-of relation then b is said to be a hy-
pernym of synset b and b a hyponym of a. for example the synset containing car is a hypernym of the synset containing hatchback and hatchback is a hyponym of car. if a noun synset a is connected to another noun synset b through the is-a-part-of relation then a is said to be a meronym of b and b a holonym of a. for example the synset containing accelerator is a meronym of car and car is a holonym of accelerator. noun synset a is related to adjective synset b 
through the attribute relation when b is a value of a. for example the adjective synset standard is a value of the noun synset measure. 
1 
taxonomic or is-a relations also exist for verb synsets. 
verb synset a is a hypernym of verb synset b if to b is one way to a. synset b is called a troponym of a. for example the verb synset containing the word operate is a hypernym of drive since to drive is one way to operate. conversely drive is a troponym of operate. the troponym relation for verbs is analogous to the hyponym relation for nouns  and henceforth we shall use the term hyponym instead of the term troponym. adjective synsets are related to each other through the similar to relation. for example the synset containing the adjective last is said to be similar to the synset containing the adjective dying. verb and adjective synsets are also related to each other through cross-reference also-see links. for example  the adjectives accessible and convenient are related through also-see links. 
　while there are other relations in wordnet  those described above make up more than 1% of the total number of links in wordnet. these are the measures we have employed in the extended gloss overlap measure. 
1 the extended gloss overlap measure 
gloss overlaps were introduced by i lesk  1  to perform word sense disambiguation. the lesk algorithm assigns a sense to a target word in a given context by comparing the glosses of its various senses with those of the other words in the context. that sense of the target word whose gloss has the most words in common with the glosses of the neighboring words is chosen as its most appropriate sense. 
　for example  consider the glosses of car and tire: four wheel motor vehicle usually propelled by an internal combustion engine and hoop that covers a wheel  usually made of rubber and filled with compressed air. the relationship between these concepts is shown in that their glosses share the content word wheel. however  they share no content words with the gloss of tree: a tall perennial woody plant having a 
main trunk and branches forming a distinct elevated crown. 
　the original lesk algorithm only considers overlaps among the glosses of the target word and those that surround it in the given context. this is a significant limitation in that dictionary glosses tend to be fairly short and do not provide sufficient vocabulary to make line grained distinctions in relatcdness. as an example  the average length of a gloss in wordnet is just seven words. the extended gloss overlap measure expands the glosses of the words being compared to include glosses of concepts that are known to be related to the concepts being compared. 
　our measure takes as input two concepts  represented by two wordnet synsets  and outputs a numeric value that quantifies their degree of semantic relatedness. in the sections that follow  we describe the foundations of the measure and how it is computed. 
1 using glosses of related senses 
there are two fundamental premises to the original lesk algorithm. first  words that appear together in a sentence will be used in related senses. second  and most relevant to our measure  the degree to which senses are related can be identified by the number of overlaps in their glosses. in other 
1 
words  the more related two senses are  the more words their glosses will share. 
　wordnet provides explicit semantic relations between synsets  such as through the is-a or has-part links. however such links do not cover all possible relations between synsets. for example  wordnet encodes no direct link between the synsets car and tire  although they arc clearly related. we observe however that the glosses of these two synsets have words in common. similar to lesk s premise  we assert that such overlaps provide evidence that there is an implicit relation between those synsets. given such a relation  we further conclude that synsets explicitly related to car are thereby also related to synsets explicitly related to tire. for example  we conclude that the synset vehicle  which is the hypernym synset of car  is related to the synset hoop  which is the hypernym synset of tire . thus  our measure combines the advantages of gloss overlaps with the structure of a concept hierarchy to create an extended view of relatedness between synsets. 
　we base our measure on the idea of an extended set of comparisons. when measuring the relatedness between two input synsets  we not only look for overlaps between the glosses of those synsets  but also between the glosses of the hypernym  hyponym  meronym  holonym and troponym synsets of the input synsets  as well as between synsets related to the input synsets through the relations of attribute  similar-to and alsosee. not all of these relations are equally helpful  and the optimum choice of relations to use for comparisons is possibly dependent on the application in which the overlaps-measure is being employed. section 1 compares the relative efficacy of these relations when our measure of relatedness is applied to the task of word sense disambiguation. 
1 scoring mechanism 
we introduce a novel way of finding and scoring the overlaps between two glosses. the original lesk algorithm compares the glosses of a pair of concepts and computes a score by counting the number of words that are shared between them. this scoring mechanism does not differentiate between single word and phrasal overlaps and effectively treats each gloss as a  bag of words . for example  it assigns a score of 1 to the concepts drawing paper and decal  which have the glosses paper thai is specially prepared for use in drafting and the art of transferring designs from specially prepared paper to a wood or glass or metal surface. there are three words that overlap  paper and the two-word phrase specially prepared. 
　there is a zipfian relationship  zipf  1j between the lengths of phrases and their frequencies in a large corpus of text. the longer the phrase  the less likely it is to occur multiple times in a given corpus. a phrasal n-word overlap is a much rarer occurrence than an single word overlap. therefore  we assign an n word overlap the score of n1. this gives an n-word overlap a score that is greater than the sum of the scores assigned to those n words if they had occurred in two or more phrases  each less than n words long. 
　for the above gloss pair  we assign the overlap paper a 
　score of 1 and specially prepared a score of 1  leading to a total score of 1. note that if the overlap was the 1-word phrase specially prepared paper  then the score would have been 1. 
natural language 
　
　thus  our overlap detection and scoring mechanism can be formally defined as follows: when comparing two glosses  we define an overlap between them to be the longest sequence of one or more consecutive words that occurs in both glosses such that neither the first nor the last word is a function word  that is a pronoun  preposition  article or conjunction. if two or more such overlaps have the same longest length  then the overlap that occurs earliest in the first string being compared is reported. given two strings  the longest overlap between them is detected  removed and in its place a unique marker is placed in each of the two input strings. the two strings thus obtained are then again checked for overlaps  and this process continues until there are no longer any overlaps between them. the sizes of the overlaps thus found are squared and added together to arrive at the score for the given pair of glosses. 
1 computing relatedness 
the extended gloss overlap measure computes the relatedness between two input synsets a and b by comparing the glosses of synsets that are related to a and b through explicit relations provided in wordnet. 
　we define rels as a  non-empty  set of relations that consists of one or more of the relations described in section 1. that is  rels is a relation defined in wordnet}. 
suppose each relation has a function of the same name that accepts a synset as input and returns the gloss of the synset  or synsets  related to the input synset by the designated relation. 
　for example  assume r represents the hypernym relation. then r a  returns the gloss of the hypernym synset of a. r can also represent the gloss  relation  such that r a  returns the gloss of synset a  and the example  relation  such that r a  returns the example string associated with synset a. if more than one synset is related to the input synset through the same relation  their glosses are concatenated and returned. we perform this concatenation because we do not wish to differentiate between the different synsets that are all related to the input synset through a particular relation  but instead are only interested in all their definitional glosses. if no synset is related to the input synset by the given relation then the null string is returned. 
　next  form a non-empty set of pairs of relations from the set of relations above. the only constraint in forming such pairs is that if the pair  r1 r1  is chosen  rels   then the pair  r 1  r 1   must also be chosen so that the relatedness measure is reflexive. that is  rclatedness a  b  - relatedness b a . thus  we define the set relpa1rs as follows: 

　finally  assume that score{  is a function that accepts as input two glosses  finds the phrases that overlap between them and returns a score as described in the previous section. given all of the above  the relatedness score between the input synsets a and b is computed as follows: 

natural language 
　our relatedness measure is based on the set of all possible pairs of relations from the list of relations described in section 1. for purposes of illustration  assume that our set of relations rels =   where hype 
and hypo arc contractions of hypernym and hyponym respectively . further assume that our set of relation pairs relpairs = { gloss  gloss    hype  hype    hypo  hypo    hype  gloss    gloss  hype }. then the relatedness between synsets a and b is computed as follows: 

observe that due to our pair selection constraint as described above  relatedness a  b  is indeed the same as relatedness b  a . 
1 comparison to human judgements 
our comparison to human judgments is based on three previous studies. irubenstein and goodenough  1  presented human subjects with 1 noun pairs and asked them how similar they were on a scale from 1 to 1.  miller and charles  1l  took a 1 pair subset of this data and repeated this experiment  and found results that were highly correlated  .1  to the previous study. the results from the 1 pair set common to both studies were used again by  budanitsky and hirst  1  in an evaluation of five automatic measures of semantic relatedness that will be mentioned in section 1. they report that all of the measures fared relatively well  with the lowest correlation being .1 and the highest .1. when comparing our measure to these 1 words  we find that it has a correlation of .1 to the miller and charles human study  and one of .1 to the rubenstein and goodenough experiment. 
　we do not find it discouraging that the correlation of extended gloss overlaps is lower than those reported by budanitsky and hirst for other measures. in fact  given the com-
plexity of the task  it is noteworthy that it demonstrates some correlation with human judgement. the fact that the test set contains only 1 word pairs is a drawback of human evaluation  where rigourous studies are by necessity limited to a small number of words. automatic measures can be evaluated relative to very large numbers of words  and we believe such an evaluation is an important next step in order to establish where differences lie among such measures. as a final point of concern  concepts can be related in many ways  and it is possible that a human and an automatic measure could rely on different yet equally well motivated criteria to arrive at diverging judgements. 
1 application to wsd 
we have developed an approach to word sense disambiguation based on the use of the extended gloss overlap measure. 
　in our approach  a window of context around the target word is selected  and a set of candidate senses is identified for each content word in the window. assume that the window of context consists of 1n + 1 words denoted by wi   where the target word is w1. further let  wi  denote the number of candidate senses of word wi  and let these senses be denoted by  
1 
　
　next we assign to each possible sense k of the target word a sensescorek computed by adding together the relatedness scores obtained by comparing the sense of the target word in question with every sense of every non-target word in the window of context. the sensescore for sense is computed as follows: 

　that sense with the highest sensescore is judged to be the most appropriate sense for the target word. if there are on average a senses per word and the window of context is n words long  there are  pairs of sets of synsets to be compared  which increases linearly with n. 
1 experimental data 
our evaluation data is taken from the english lexical sample task of senseval-1 ledmonds and cotton  1 . this was a comparative evaluation of word sense disambiguation systems that resulted in a large set of results and data that are now freely available to the research community. 
　this data consists of 1 instances each of which contains a sentence with a single target word to be disambiguated  and one or two surrounding sentences that provide additional context. a human judge has labeled each target word with the most appropriate wordnet sense for that context. a word sense disambiguation system is given these same instances  minus the human assigned senses  and must output what it believes to be the most appropriate senses for each of the target words. there are 1 distinct target words: 1 nouns  1 verbs  and 1 adjectives  and the part of speech of the target words is known to the systems. 
1 experimental results 
for every instance  function words are removed and then a window of words is defined such that the target word is at the center  if possible . next  for every word in the window  candidate senses are picked by including the synsets in wordnet that the word belongs to  as well as those that an uninfected form of the word belong to  if any . given these candidate senses  the algorithm described above finds the most appropriate sense of the target word. 
　it is possible that there be a tie among multiple senses for the highest score for a word. in this case  all those senses are reported as answers and partial credit is given if one of them prove to be correct. this would be appropriate if a word were truly ambiguous in a context  or if the meanings were very closely related and it was not possible to distinguish between them. it is also possible that no sense gets more than a score of 1 - in this case  no answer is reported since there is no evidence to choose one sense over another. 
　given the answers generated by the algorithm  we compare them with the human decided answers and compute precision  the number of correct answers divided by the number of answers reported  and recall  the number of correct answers divided by the number of instances . these two values can be summarized by the f-measure  which is the harmonic mean 
1 
table 1: wsd evaluation results 

of the precision and recall: 

　table 1 lists the precision  recall and f-measure for all the senseval-1 words when disambiguated using a window size of 1. the overall results for our approach are shown as overall*  and these are also broken down based on the part of speech  pos  of the target word. this table also displays results from other baseline or representative systems. the original lesk results are based on utilizing the glosses of only the input synsets and nothing else. while this does not exactly replicate the original lesk algorithm it is quite similar. the random results reflect the accuracies obtained by simply selecting randomly from the candidate senses. 
　the sval-first  sval-second  and sval-third results are from the top three most accurate fully automatic unsupervised systems in the senseval-1 exercise. this is the class of systems most directly comparable to our own  since they require no human intervention and do not use any manually created training examples. these results show that our approach was considerably more accurate than all but one of the participating systems. 
　these results are significant because they are based on a very simple algorithm that relies on assigning relatedness scores to the senses of a target word and the senses of its immediately adjacent neighbors. while the disambiguation results could be improved via the combination of various techniques  our focus is on developing the extended gloss overlap measure of relatedness as a general tool for natural language processing and artificial intelligence. 
1 	discussion 
table 1 shows that the disambiguation results obtained using the extended gloss overlap measure of semantic relatedness are significantly better than both the random and original lesk baselines. in the original lesk algorithm  relatedness between two synsets is measured by considering overlaps between the glosses of the candidate senses of the target word 
natural language 
and its neighbors. by adding the glosses of related synsets  the results improve by 1% relative  1% absolute . this shows that overlaps between glosses of synsets explicitly related to the input synsets provide almost as much evidence about the implicit relation between the input synsets as do overlaps between the glosses of the input synsets themselves. 
　table 1 also breaks down the precision  recall and fmeasure according to the part of speech of the target word. observe that the noun target words are the easiest to disambiguate  followed by the adjective target words. the verb target words prove to be the hardest to disambiguate. we attribute this to the fact that the number of senses per target word is much smaller for the nouns and adjectives than it is for the verbs. nouns and adjective target words have less than 1 candidate senses each on average  whereas verbs have close to 1. thus  when disambiguating verbs there are more choices to be made and more chances of errors. 
　the results in table 1 arc based on a 1 word window of context. in other experiments we used window sizes of 1  1  1 and 1. although this increase in window size provides more data to the disambiguation algorithm  our experiments show that this does not significantly improve disambiguation results. this suggests that words that are in the immediate vicinity of the target word are most useful for disambiguation  and that using larger context windows is either adding noise or redundant data. the fact that small windows are best corresponds with earlier studies on human subjects that showed that humans often only require a window of one or two surrounding words to disambiguate a target word  choueka and lusignan  1. 
　we also tried to normalize the overlap scores by the maximum score that two glosses can generate  but that did not help performance. we believe that the difference between the sizes of various glosses in terms of number of words is small enough to render normalization unnecessary. 
1 	evaluating individual relation pairs 
our measure of relatcdness utilizes pairs of relations picked from the list of relations in section 1. in this section we attempt to quantify the relative effectiveness of these individual relation pairs. specifically  given a set of relations rels  we create all possible minimal relation pair sets  where a minimal relation pair set is defined as the set that contains either exactly one relation pair or exactly two relation pairs    where~ . for example 
{ gloss  gloss } and { hype  gloss    gloss  hype } are both minimal relation pair sets. 
　we evaluate each of these minimal relation pair sets by performing disambiguation using only the given minimal relation pair set and computing the resulting precision  recall and f-measure. the higher the f-measure  the  better  the quality of the evidence provided by gloss overlaps from that minimal relation pair set. in effect we are decomposing the extended gloss overlap measure into its individual pieces and assessing how each of those pieces perform individually. 
　recall that each part of speech has a different set of relations associated with it. the difference in the numbers and types of relations available for the three parts of speech leads us to expect that the optimal minimal relation pair sets will 
natural language 
table 1: best relation pair sets 
nouns 

adjectives 

verbs 

differ with the part of speech of the input synsets. i able 1 lists the top 1 minimal relation pair sets for target words belonging to the three parts of speech  where relation pair sets are ranked on the f-measure achieved by using them in disambiguation. note that in this table  hypo  mero  also  attr  and hype stand for the relations hyponym  meronym  alsosee  attribute  and hypernym respectively. also in the table the relation pair ri-r1 refers to the minimal relation pair set and 	otherwise. 
　perhaps one of the most interesting observations is that no single minimal relation pair set achieves f-measure even close to that achieved using all the relation pairs  1  1  and 1 for nouns  verbs  and adjectives respectively   suggesting that there is no single relation pair that generates a lot of evidence for the relatedness of two synsets. this finding also implies that the richer the set of explicit relations between synsets in wordnet  the more accurate the overlap based measure of semantic relatedness will be. this fact is borne out by the comparatively high accuracy attained by nouns which is the best developed portion of wordnet. 
　for nouns  table 1 shows that comparisons between the glosses of the hyponyms and meronyms of the input synsets and also between the glosses of the input synsets are most informative about the relatedness of the synsets. interestingly  although both hyponyms and hypernyms make up the is-a hierarchy  the hypernym relation does not provide an equivalent amount of information. in wordnet  a noun synset usually has a single hypernym  parent  but many hyponyms  children   which implies that the hyponym relation provides more definitional glosses to the algorithm than the hypernym re1 
　
lation. this assymetry also exists in the holonym-meronym pair of relations. most noun synsets have less holonym  is-apart-of  relations than meronyms  has-part  resulting in more glosses from the meronym relation. these further confirm that the accuracy of the relatedness measure depends at least partly on the number of glosses that we can access for a given pair of synsets. 
　this finding also applies to adjectives. the two most frequent relations  the also-see relation and the attribute relation  rank highest among the useful relations for adjectives. similarly for verbs  the hyponym relation again appears to be extremely useful. interestingly  for all three parts of speech  the example  relation   which simply returns the example string associated with the input synset  seems to provide useful information. this is in keeping with the senseval-1 results where the addition of example strings to a lesk-like baseline system improves recall from 1% to 1%. 
1 	related work 
a number of measures of semantic relatedness have been proposed in recent years. most of them rely on the noun taxonomy of the lexical database wordnet. iresnik  1  augments each synset in wordnet with an information content value derived from a large corpus of text. the measure of relatedness between two concepts is taken to be the information content value of the most specific concept that the two concepts have in common.  jiang and conrath  1  and  lin  1 extend resnik's measure by scaling the common information content values by those of the individual concepts. our method of extended gloss overlaps is distinct in that it takes advantage of the information found in the glosses. the other measures rely on the structure of wordnet and corpus statistics. in addition  the measures above are all limited to relations between noun concepts  while extended gloss overlaps can find relations between adjectives and verbs as well. 
1 	conclusions 
we have presented a new measure of semantic relatedness based on gloss overlaps. a pair of concepts is assigned a value of relatedness based on the number of overlapping words in their respective glosses  as well as the overlaps found in the glosses of concepts they are related to in a given concept hierarchy. we have evaluated this measure relative to human judgements and found it to be reasonably correlated. we have carried out a word sense disambiguation experiment with the senseval-1 lexical sample data. we find that disambiguation accuracy based on extended gloss overlaps is more accurate than all but one of the participating senseval-1 systems. 
acknowledgements 
thanks to jason rennie for his wordnet: :querydata module  and to siddharth patwardhan for useful discussions  experimental help  and for integrating the extended gloss overlap measure into his wordnet::similarity module. both of these modules are freely available from the comprehensive perl archive network  search.cpan.org . 
this work has been supported by a national science 
foundation faculty early career development award  #1  and nsf grant no. rec-1. any opinions  findings  conclusions  or recommendations expressed in this publications are those of the authors and do not necessarily reflect the views of the nsf or the official policies  either expressed or implied  of the sponsors or of the united states government. 
