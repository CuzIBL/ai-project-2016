 
we describe a supervised learning algorithm  eodg that uses mutual information to build an oblivious decision tree the tree is then converted to an oblivious read-onre decision graph  oodg  b  merging nodes at the same level of the tree for domains that art appropriate for both decision trees and oodgs  per formance is approximately the same as that of c1    but the number of nodes in the oodg is much smalle r the merging phase that converts the oblivious decision tree to an oodg pro-
vides a new way of dealing with the replication problem and a new pruning mechanism that works lop down starting from tin root the pruning mechanism is well suited for finding symmetries and aids in recovering from splits on irrelevant features that mav happen during 
the tree conslrm tion 
1 	introduction 
dmsion trees provide a hypothesis space for supervised mat hine learning algorithms that is well suited for many data ets encountered in the real world  breiman  friedman olshen & stone 1 quinlan 1  the tree structure  used to represent concepts suffers from some well-known problems  most notably the replication problem disjunctive concepts such as are inefficiently represented  pagallo & haussler 1  a. related problem  the fragmentation problem sometimes called over-branching or over-partitioning  fayyad 1   is specific to the top-down recursive partitioning that is used to build the trees in the common top-down decision tree construction algorithms including cart  breiman et al 1   id1  and c1  quinlan 1   the training set is partitioned according to a test at each node  usually a test on a single feature  after a few splits  the number of instances at the node diminishes to a point where distinguishing between the actual pat' rn  signal  and random events  noise  becomes difficult the fragmentation problem is especially apparent when there are mulli-valued features that are used as tests or specific intervals that need to be identified in continuous features  the multi-way splits quickly partition the dataset into small sets of instances while there have been attempts by quinlan  1  and fayyad  1  to atlack the problem directly there is no agreedupon method and   1 s default parameters do not even default to such splits 
　the smallest decision trees for most symmetric functions  such as majority  'in of n   and parity  require 
exponentially-sized trees while parity is unlikely  to occur in practice m of n ' is more common  spademan 1  induction algorithms that search for small trees tend to generalize poorly on these functions 
　kohavi  1a  introduced oblivious read-once decision graphs  oodgs  as an alternative representation structure for supervised classification learning oodgs retain most of the advantages of decision trees  while overcoming the above problems oodgs are similar to ordered binary decision diagrams  bryant 1  which have been used in the eugineenng community to represent state-graph modeis of systems allowing verification of finite-state systems with up lo 1 states much of the work on obdds carries over to oodga we refer the reader to kohavi  1a  for a discussion of related work 
the hoodg algorithm for bottom-up induction of 
oodgs was shown to be effective for nominal features 
 kohavi 1  but the algorithm could not cope with continuous features and lacked a global measure of improvement that could help cope with irrele ant features 
　in this paper  we introduce eodg  entropv-based oblivious decision graphs  a top-down induction algorithm for inducing oblivious read-once decision graphs. the eodg algorithm uses the mutual information of a single split across the whole level to determine the appropriate tests for the interior nodes of the tree ah instances are involved at every choice point in the tree construction and  therefore the algorithm does not suffer from fragmentation of the data as much as recursive partitioning algorithms do after an oblivious decision tree is built  it is pruned and converted into an oblivious 
	kohavi andli 	1 
decision graph by merging nodes in a top-down sweep 
　the paper is organized as follows in section 1 we briefly review the oodg structure and its properties section 1 describes the eodg algorithm section 1 describes the experimental results section 1 describes fu-
ture work  and section 1 concludes with a summary 
1 	oblivious read-once decision graphs 
in this section  we formally define decision graphs and then specialize them to oblivious read-once decision graphs 
	given n discrete fealures  attributes  	  
 respectively  the in-
stance space   is the cross-product of the domains 
 a k-classification function is a function / mapping each instance in the instance space lo one of k classes  i e   
　a decision graph for a k-classification function over features is a directed acyclic graph  d1g  with the following properties 
1 there are exactly k nodes called category nodes  that are labelled 1    k- 1  and have outdegree zero 
1 non-category nodes are called branching nodes each branching node is labelled by some feature and has  d   outgoing edges  each labelled by a distinct value from 
1 there it. one distinguished node-the root-that is the only node with indegret zero 
　the class assigned by a decision graph to a given feature assignment  an instance   is determined by tracing the unique path from the root to a category node  branching according to the labels on the edges 
　in a read-once decision graph  each test occurs at most once along any path from the root to a leaf in a levelled decision graph the nodes are partitioned into a sequence of pairwise disjoint sets  the levels such that outgoing edges from each level terminate at the next level an oblivious decision graph is a levelled graph such that all nodes at a given level are labelled by the same feature 
　previous work on oodgs  kohavi 1a  kohavi 1  defined read-once as allowing a feature to be tested only once for continuous features  this restriction is is inappropriate  thus we allow continuous features to be tested at different levels as long as the thresholds are 
different 
an o o d g is an oblivious read-once decision graph 
a constant node is a node such that all edges emanating from it terminate at the same node of the subsequent level figure 1 shows an oodg derived by the eodg algorithm for the mushroom problem  a post-processing 
1 	learning 

figure 1 the oodg  constant nodes removed  induced by eodg for the mushroom concept 
algorithm removes constant nodes to make the graphs more readable  
　oodgs have many interesting properties these include an oodg for nominal features is canonical for any total function if an ordering on the features is given any symmetric boolean function  c g parity  majority and m of n  can be represented by an oodg of size and the width of levels in oodgs is bounded for boolean inputs we refer the 
reader to kohavi  1a  for a more detailed description of these properties 
1 	the eodg algorithm 
the eodg algorithm has three phases growing an oblivious decision tree  odt  pruning complete levels from the bottom and merging nodes from the top down we now describe the phases in detail 
1 t o p - d o w n construction using m u t u a l i n f o r m a t i o n 
the top-down construction of the odt is performed using mutual information in a manner similar to c1  quinlan 1  the main difference is that instead of doing recursive partitioning  i e finding a split and constructing the subtree recursively  an iterative process is used 
　the general mechanism for growing odts is simple decide on a test with k outcomes that will be done at all nodes in the current level of the tree partition the instances at each node p according to the test and create a new level containing k nodes for each node p in the current level the k nodes are connected from the parent node p to the k children by edges labelled with the k possible outcomes figure 1 shows this algorithm 
　the find split routine determines which split to conduct at all the nodes at the current level and decides when to terminate while the test can be of any type  our implementation uses multi-waysplits on nominal features  the test outcome is the feature value  and threshold splits on continuous feature to determine which 

input a set t of libelled instances 
output an oblivious decision tree for classifying instances each set created by this algorithm has a corresponding node in the graph 

1 lgure 1 the algorithm for growing the oblivious decision trees 
feature to split on  and what threshold to pick for con urinous features  we use mutual information our notalion follows that of cover & thomas  1  the conditional entropy of the label 
v given features of information about y after we have seen the values of 

diven a new feature a   wt define the mutual information as the difference in the conditional entropies 

if t is zero  the mutual information is defined as 

because the mutual information is biased in favor of testa with many outcomes  we define the adjusted mutual information as the mutual information divided by log k  where k is the number of outcomes for x given s  a set of sets of instances  each set of instances is a node in the graph   find split checks all the possible features and determines the one that has the highest adjusted mutual information for continuous features  we try all possible thresholds between two adjacent values of a feature computing the best threshold split after the values for a given feature have been sorted can be done in linear time in the number of instances by shifting instances from the left child node to the right child node as the threshold is increased the technique is exactly the same as that used in breiman et al  1  and quinlan  1  exrept that we determine one split for all the nodes at a level 
and shift instances from the left child to the right child 
for all nodes at the current level 
   when building a decision tree  it is clear that one should not split on a nominal feature twice  since no information can be gained however  a surprising phenomenon that occurs in oodgs  which has no equiva lent in decision tree indurtion  is that it may be useful to split pure nodes i t   nodes that contain instances of only one class  because the merge process may benefit for example  if we are left with one instance per node then all nodes are trivially pure if we split on a relevant feature some instances will go left and some instances will go right these nodes will then be merged and the correct target concept might be identified 
1 	merging nodes 
the main advantage of building odts over regular decision trees is that nodes at the same level can be merged because they contain the same test we begin by describing how to convert an odt into an oodg we then describe how this merging can be used to conduct top-down pruning  a technique that we believe is more robust to noise than bottom-up pruning and also aids in overcoming splits on irrelevant features by merging the children to a single node as in figure 1 
　two subtrees in a decision tree are isomorphic if they art both leaves labelled with the same class  or if the root nodes contain the same test and the corresponding children  reached by following the edges with the same labels on both subtrees  are the roots of isomorphic subtrees two isomorphic subtrees may be merged without 
changing the classification behavior of the decision structure while subtree isomorphism may be performed for 
decision trees too it is unlikely that subtrees will be isomorphic without the leveling restriction 
　the merging process is a post-processing step that is executed after the odt has been grown for each level we check all pairs of nodes for isomorphism and merge those that are isomorphic the complexity of the merge process is dominant in the induction of oodgs atea  h level with fe nodes  1{k1  isomorphic tests have to be conducted however  because of the kite theorem  kohavi 1a   there is a limit on the number of nodes that may be at lower levels  and many merges must succeed at higher levels where the number of nodes is much less than the number of leaves of the odt 
   one advantage of merging nodes comes from the abil ity to assign classes to nodes with no instances if a split is done and no instances fall to one of the children the child is labelled with the majority class of the parent 
	kohaviandli 	1 
figure 1 the merged subtree of the compatible subtrees above 
when budding the odj we leave the node labelled as unknown  until the merge phase is done because  he merge phase may assign it a class intuitively we would like an unknown node to match anything when we check whether two subtrees can be merged two subtrees; are defined as compatible if either at least one root is labelled unknown'  in which case it is a leaf  or if the root nodes make the same test and the corresponding children are the roots of compatible subtrees figures 1 shows two compatible subtrees and figure 1 shows the merged subtree 
　the merging of compatible subtrees changes the bias of the algorithm bv assuming that a child node that is marked unknown' is likely to behave the same as another child if they belong to compatible subtrees in order to strengthen the usefulness of the merging process we split all the nodes at a level that are not leaves labelled  unknown   even those that are pure node 1 
　in figure 1 is pure but it has no instances that have the color green if the node is merged with node 1  it will correctly classfy all instances in the training set  but will classify instances that reach the node and have the color green the same way that node 1 classifies them 
　in noisy situations it may be that two subtrees are almost compatible and should be merged  but the above definition of compatibility will not do so two subtrees are k-compatible if when we merge them  the number of newly misclassified instances  i e instances correctly classified by the subtrees but not by the merged tree  is 
1 	learning 
less than h compatible subtree are 1-compatible when two nodes are considered for merging we allow them to misclassify k instances where k is defined similarly to the pruning method used in c1  quinlan 1  the number of misclassifications of each subtree is increased to the high-value of a confidence interval and the merge takes place only if a similar increase in the merged sub-
graph results in fewer misclassifications than the sum of the adjusted misclassifications of the children we have 
used a 1c   onfidence interval 
1 	bottom-up pruning 
intuit ively  the test for merging nodes checks whether the reduced error-rate due to the nodes being separate is significant statistically the merging of k-compatible nodes works well in noisy situations only if the misclassification rates at the leaves are non-zero if the tree overfits the data and has a resubstitution error rate near zero  any merging of subtrees at the top will seem to introduce many misclassifications that are unlikely to happen in a random sample with a large number of instances 
　in order to get a more 'honest  estimate of the misclassification error at the leaves  we use cross-validation to decide how many levels of the odt to prune we fix the order of tests  and thresholds  at the levels  and cross validate every level to get an accuracy estimate building an odt given an ordering is an extremely fast process  so cross-validating the tree at different levels is not an expensive process note however that the estimate is likely to be slightly optimistic because the split tests were determined using all the data figure 1 shows the bottomup pruning algorithm and the top-down merge/prune phase 




table 1 comparison of the sizes of the induced structures the last column indicates the time in cpu seconds on a sparc-1 for eodg to train and  est once  equivalent to one fold in ten-fold cv  

1 	e x p e r i m e n t s 
to estimate the performance accuracy of eodg  we chose a few artificial datasels and real-world datasets from the uc irvine repository  murphy & aha 1  the artificial datasets are standard benchmark datasets such as the monk problems  thrun etal 1   from the real-world domains  we chose ones that had at least 1 instances because we have not implemented classification of instances with unknown values we removed all such instances from the datasets tested table 1 shows the characteristics of the different domains used for the artificial domains and for shuttle  we ran a train/test-set sequence for the other domains  we did 1-fold crossvalidation 
table 1 shows the accuracies of c1  c1-rules  
eodg and eodt  the first two stages of eodg  the last column is the p-va ue for a paired t-test on the crossvalidation folds of c1 and eodg  for the rune on a single test-set  we did an unpaired t-test using the sum of the estimated standard deviations from the test set   for the single runs  we did an unpaired t-test values greater than 1 indicate that eodg is better and values less than 1 indicate that c1 is better on many datasets  the behavior of c1 and eodg is similar  there are datasets for which eodg's accuracy is worse then c1  soybean-large  vehicle  waveform   and there 
1 	learning 
are datasets for which the accuracy is better  balancescale cleve tic-tac-toe  monkl  monk1-!ocal  it is interesting to note that on some problems eodt performs better  indicating that the symmetry bias assumed in the merge phase of eodg is not appropriate but the oblivious restriction doe* not hurt much 
　one of the main advantages of eodg is the small graphs it produces compared to c1 table 1 shows the average sizes tor thf graphs created for each dataeet eodg clearly creates smaller graphs and when it creates significantly larger graphs  soybean  vehicle  it is also a worse performer the graphs created by eodt are huge 
1 	future work 
the bottom-up pruning phase of eodg prunes the odt  which may be inappropriate for example  parity is badly represented as a tree  and thus the pruning stage reduces the number of features too much initial experiments that involve pruning after merging showed that it may be better  e g   parity1 gives 1% accuracy   but the algorithm becomes too expensive 
　the eodg algorithm can be improved along a few directions we have not implemented handling of unknown values  but we believe that an approach similar to that used in decision trees  quinlan 1  should work well we are currently conducting tests on a single fea-

ture at a node  other tests  such as oblique splits are possible because eodg suffers less from lack of instances at lower levels  using such tests might not suffer from the same problems that have occurred in decision trees the disadvantage of such an extension is the loss of comprehensibility that is one of the most important characteristics of decision trees and graphs over other hvpothesis spaces 
　we noticed that for continuous features  many splits art made on different thresholds since we only merge nodes at the same level  we might consider multi-way splits on real features  as suggested in fayyad & irani  1  
　a different approach to feature selection and ordering  which is more feasible in oodgs then in decision trees is to try different ordenngs on the features an odt on a dataset is defined by a set of tests one per level and hence the space of possibilities is much smaller than that 
of derision trees  although still ver  large  
1 	s u m m a r y 
we reviewed the characteristics of oblivions read-once decision graphs as the underlvmg hypothesis spa* t for in dution algorithms we introduced eodg  a top-down algorthm that constructs oblivious decision trees using information gain as a splitting criteria and then merges nodes to form oblivious read-once decision graphs 'the merging of nodes from the top provides a new wa  of pruning nodes in some observed  ases  this corrects a split on an irrelevant feature by merging the children 
　oodgs have a differenl bias from that of decision tress  and thus some concepts that are hard to represent is trees are easy to represent as oodgs and vice-versa 
 independent of the specific induction algorithm used  
wf have shown that for some real-world dalasets at uc irvine the bias of eodg is appropriate and for the problems where performance is approximately the same as that of c1  eodg produces much smaller graphs 
acknowledgments 
acknowledgments 	the work in this paper was done using the  library parti} funded by onr grant 
n1 1 and nsf grants iri-1 and ir1 we thank scott benson and the anonymous reviewers for comments on the paper we thank yael kleefeld and ya-huei wang for their support 
