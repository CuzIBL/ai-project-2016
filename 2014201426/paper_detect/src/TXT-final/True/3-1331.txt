
although natural language processing  nlp  for requests for information has been well-studied  there has been little prior work on understanding requests to update information. in this paper  we propose an intelligent system that can process natural language website update requests semi-automatically. in particular  this system can analyze requests  posted via email  to update the factual content of individual tuples in a databasebacked website. users' messages are processed using a scheme decomposing their requests into a sequence of entity recognition and text classification tasks. using a corpus generated by human-subject experiments  we experimentally evaluate the performance of this system  as well as its robustness in handling request types not seen in training  or user-specific language styles not seen in training.
1 introduction
in this paper  we present a natural language system that helps a webmaster maintain the web site for an organization. specifically  we describe a system for understanding certain natural-language requests to change the factual content on a website. we will assume that the website is based on a database  and focus on requests to update specific facts in this database.
　to motivate this  we note that although nlp for requests to deliver information i.e.  question-answering  has been wellstudied  there has been little prior work on nlp for requests to update information. however  nlp for update requests is an attractive research problem  in part because a user can more easily detect an imperfectly-processed utterance.
　as a concrete example of update requests  we consider here requests for web-site updates. such a system would be practically useful  as many organizations maintain a single large database-backed web site that includes information that can be contributed or corrected by many individuals. since individual users  each of whom may only contribute a few database changes a year  may be reluctant to learn how to interface with the database to make their occasional updates  in many orginazations users submit update requests via email in natural language to a human webmaster. frequently  there is
add the following contact to the staff list.
arthur scott ascott ardra.com rm 1 1on the events page  delete row  december 1 assembly for automotive engineers conference room a1 on the people page under tommy lee delete 1please delete kevin smith's phone number - thanx  marthachange mike roberts to michael roberts.figure 1: example update requests  edited slightly for space and readability 
a waiting period before the human webmaster can incorporate corrections  leading to long processing times  and a web site that is not up to date.
　in this paper  we describe an intelligent system that can process website update requests semi-automatically. first  natural language processing is used to analyze an incoming request. based on the analysis  the system then constructs an executable version of the proposed change  which is represented as a pre-filled instance of a form. by examining the form  the end user can efficiently determine whether the analysis step was correctly accomplished  and  if necessary  override the results of the agent's analysis by changing values in the form. prior experiments with human subjects have shown that this process is an effective means of reducing human effort  even if the initial analysis step is imperfect .
　this paper focuses on the natural-language processing part of this system. as is typical of informal text like email  users' messages are often ungrammatical use capitalization patterns inconsistently  use many abbreviations and include typos  as illustrated in figure 1 . as a consequence  standard shallownlp tools such as part-of-speech tagging and noun-phrase chunking  which are preliminary steps for text parsing  are quite unreliable. we therefore suggest here a learning approach  where rather than parse the text into a framework of pre-modeled domain knowledge  we decompose the general task into a sequence of entity extractionand classification sub-tasks. all of these sub-tasks can be learned from incoming messages  improving system performance over time.
　we will first describe a scheme for decomposing requestunderstanding into a sequence of learning tasks. next  we describe the corpus of requests that is used for performance evaluation. we then describe each of the learning sub-tasks in detail  along with experimental results. we also present experimental results on the robustness of the system - in particular  how the system will perform on request types not seen in training  or on user-specific language usage not seen in training. finally  we evaluate the end-to-end system's performance  to determine what fraction of messages can be processed completely without errors. we conclude with a review of related work and our conclusions.
1 understanding update requests
1 analysis procedure
figure 1 gives some example web site update requests that are addressed by the given analysis procedure. general requests that are not for factual update  e.g.   the animated gif in the logo doesn't flash properly when i view it from my home pc   will simply be flagged and forwarded to the real human webmaster.
the analysis procedure contains the following steps.
   request type classification. an informal preliminary analysis of real webmaster request logs suggested that factual-update requests are in one of the following forms: add a new tuple to the database; delete an existing tuple; delete a value from an existing tuple; or alter  add or replace  a value of an existing tuple. one step of the analysis is thus determining the type of request. this is a text classification task: each request will be mapped to one of the categories addtuple  deletetuple  deletevalue  altervalue. if it is not in one of these categories  it will be mapped to otherrequest.
　named entity recognition  ner . another step of the analysis is to identify all entity names in a request. figure 1 shows the result of correctly recognizingperson names  email addresses  phone numbers  room numbers  and event titles in some sample requests. the subscript after an entity indicates its type  for instance   person  or  room number  .
　role-based entity classification. we distinguish between four different roles for an entity in an update request.  a  an entity is a keyentity if it serves to identify the database tuple which is to be modified. in the figure  key entities are marked with a superscript k. an example is the entity  freddy smith  in the sentence  please delete freddy smith's phone number .  b  an entity is a newentity  marked with a superscript n  if it is a value to be stored in the database.  c  an entity is an oldentity  superscript o  if it is a value currently in the database which the user expects to be replaced with a newentity.  d  entities unrelatedto the executionof the request are considered to be noiseentities. in the figure  they have no superscript marking. role-based entity classification is an entity classification task  in which entities produced by the earlier ner step are given an additional classification.
　target relation classification. the second column of figure 1 shows the relation associated with each request. for any fixed database schema  there is a fixed set of possible relations  so this is a text classification operation.
　target attribute classification. given entities  the roles of entities  the target relation  and the request type  the semantics of the many tuple-based commands will be often completely determined. one type of request that may still be underspecified is the deletevalue request. as an example consider request 1 in the figure: the previous analysis tells us we should delete some attribute value from the tuple of the  person  relation with the key value of  tommy lee   but does not specify the value to be deleted. hence  to complete the analysis for deletevalue requests  it is necessary to determine the attribute that needs to be deleted. this is again a text classification task: given a database schema  only a fixed number of attributes need to be considered as possible targets.
　for pedagogical reasons  we have described these steps as if they are taken separately. however  the steps are not independent-i.e.  information from each step of analysis may affect other steps. in section 1 we describe and evaluate a particular sequence  where outputs of some steps are propagated as inputs to the next steps.
1 the experimental corpus
in orderto collect an appropriatecorpus  a series of controlled human-subject experiments were performed  in which participants were given a series of tasks in pictorial form and asked that they compose and send an appropriate e-mail messages to a webmaster agent. in response to the user's request  the agent returned a preview of the updated page  and also prefilled form that contained a structured representation of the user's request. the user could correct errors by editing text in various slots of the form  or by choosing from pull-down menus.
　overall  the human-generated corpus contains a total of only 1 example requests  involving approximately 1 subjects  and about 1 different tasks.
　note that the same pictorial task descriptions were presented to multiple users. this sort of duplication can lead to undesirable behavior for a learning system: if a certain pictorial task  demonstrating addition of a phone number to a person named greg johnson for example  is represented by multiple similar examples in the data  then the system might learn a correlation between the phrase  greg johnson  and the task of adding a phone number. to address this problem  we manually replaced duplicate entity names with alternative values throughout the corpus  preserving surface features such as capitalization patterns and misspellings.
　the requests in the corpus are largely factual updates concerning a single tuple in the database  so we will focus our attention on such requests. also  the relations in the underlying database schema of the corpus do not contain two attributes or more of the same type  where  type  is defined by the output of the entity recognizer. for instance  personal details might include a home phone number and an office phone number  but our corpus has no such duplications. duplications of this sort would require an additional entity classifier.
　as mentioned  the text itself is often ungrammatical and noisy. we pre-processed the text  annotating it with a version of brill's part-of-speech tagger  brill  1  and a handcoded noun-phrase chunker which was tuned for email  using a different corpus . in learning  however  we rely mainly on alternative features that exploit syntactic properties of the messages. these features prove to be informative for the noisy text in our corpus.
requestrequesttargettargettyperelationattribute1add the following contact to the staff list.  arthur scott nperson
 ascott ardra.com nemail rm nroom  1 1 nphoneaddtuplepeople 1on the events page  delete row   december 1 	 assembly for
automotive engineers conference keventtitle room  a1 kroom deletetupleevents 1on the people page under  tommy lee kperson delete  1
1 ophonedeletevaluepeoplephonenum1please delete  freddy smith's kperson's phone number - thanx 
 martha persondeletevaluepeoplephonenum1change  mike roberts kperson to  michael roberts nperson on the people page.altervaluepeoplepersonname1please add  greg johnson kperson's phone number-  1
1 nphonealtervaluepeoplephonenumfigure 1: analyzed update requests.1 learning
below we describe each of the individual learning tasks. relevant experimental results are given for every component.
1 entity recognition
named entity recognition  ner   or the identification of the substrings of a request that correspond to entity names  is a well-studied yet non-trivial natural-language processing task. we evaluated ner performance for seven linguistic types: time  date  amount  email addresses  phone numbers  room numbers  and personal names. the data includes some mentions of additional entity types  e.g.  job titles and organization names  but not in sufficient quantity for learning.
　we experimented with two approaches to entity extraction: a rule-based approach  in which hand-coded rules are used to recognize entities; and learning-based extraction. the rule language we used is based on cascaded finite state machines. the learning algorithm we use here is vphmm  a method for discriminatively training hidden markov models using a voted-perceptron algorithm  collins  1 .
　we found that manually constructed rules are best suited for entities such as e-mail addresses and temporal expressions. these types are based on limited vocabularies and fairly regular patterns  and are therefore relatively easy to model manually. email addresses are an extreme example of this: a simple regular expression matches most email addresses.
　table 1 a  shows the results of extraction using hand-coded rules for email and temporal expressions. we evaluated the rules on the main corpus  which was used for generating the rules  and also on a 1-message  validation set   containing messages which were collected in a second  later series of human-subject experiments  unfortunately  no time expressions were present in this additional set.  as shown in the table  the entity f1 performance is above 1% for all cases that could be evaluated.
in table 1 b  we show results for learning on the full set of
test settypefull corpus	validationtime	1	n/adate	1	1email	1	1 a  rules
base f.tuned f.tuned featurestype1cv1cv1cvusr1cvreqtime1111date1111amount1111phone1111room#1111person1111 b  learning
table 1: entity recognition results: f1 measures
entity types  applying the vphmm algorithm. here ner is reduced to the problem of sequentially classifying each token as either  inside  or  outside  the entity type to be extracted. performance is evaluated by the f1-measure1  where entities are only counted as correct if both start and end boundaries are correct  i.e.  partially correct entity boundaries are given no partial credit.  the left-hand columns in the table  titled  1cv   show f1-measure performance on unseen examples  as estimated using 1-fold cross validation. the right-hand columns will be discussed later.
　performance is shown for two sets of features. the base feature set corresponds to words and capitalization templates over a window including the word to be classified  and the three adjacent words to each side. the second set of features  labeled tuned features in the table  is comprised of the base features plus some additional  entity-type specific features  which are constructed using the same rule language used to build the hand-coded extractors. for example  in extracting dates we added an indicator as to whether a word is a number in the range 1; for personal names  we added an indicator for words that are in certain dictionaries of first and last names.
　overall  the level of performance for extraction - better than 1% for every entity type  using the tuned features - is very encouraging  especially considering the irregularity of the text and the relatively small amount of training data available. we found that users tend to use the terminology and formats of the website  resulting in reduced variability.
1 role-based entity classification
once an entity span has been identified  we must determine its functionalrole-i.e.  whether it acts as a keyentity  newentity  oldentity  or noiseentity  as outlined in section 1 . we approach this problem as a classification task  where the extracted entities are transformed into instances to be further classified by a learner.
　the features used for the learner are as follows.  a  the closest preceding  action verb . an action verb is one of a few dozen words generally used to denote an update  such as  add    delete   etc.  b  the closest preceding preposition.  c  the presence or absence of a possessive marker after the entity.  d  an indication whether the entity is part of a determined np.
　the experimental results for the important classes are shown in table 1  in the column marked  1cv . we used here an svm learner with a linear kernel  joachims  1 . we show results for each class separately  and in addition to f1 performance for each category  we also show error rate. the  default error  is the error obtained by always guessing the most frequent class.
entityf1/errordefaultrole1cv1cvusr1cvreqerrorkeyentity1/11/11/11newentity1/ 11/11/11oldentity1/ 11 /11/ 11table 1: role-based entity classification results
　the results for the role determination are almostsurprisingly good  considering the difficult  linguistic nature of this role assignment task. the set of features suggested here is small and simple  and yet very informative  supporting effective learning of roles even for semi-ungrammatical texts.
1 target relation classification
to determine the target relation  we used the same svm learner. the input features to the classifier are a  bag-ofwords  representation of a request  as well as the entity types included in the request  for example  presence of a  phone number  entity in a request indicates a  people  relation  in our database schema. . results are shown in table 1 in the  1cv  column. as shown by these results  the task of relation determination is relatively straight-forward  provided sufficient training data.
targetf1/errordef.relation1cv1cvusr1cvreqerrorpeople1 / 11 / 11/ 11budget1 / 11 / 11 / 11events1 / 11 / 11 / 11sponsors1 / 11 / 11 / 11table 1: target relation classification results
1 request type classification
in many cases the type of a request can be determined from the roles of the entities in the request. for instance  an addtuple request has no keyentities but may have multiple newentities; conversely a deletetuple request has keyentities  but no newentities; and only an altervalue request can have both keyentities and newentities. this means that most request types can be determined algorithmically from the set of entity roles found in a request.
　the primary need for a request-type classifier is to distinguish between deletevalue and deletetuple requests. these types of requests are often syntactically quite similar. consider for instance the requests  delete the extension for dan smith  and  delete the entry for dan smith . the first is a deletevalue for a phone number  and the second is a deletetuple request. the action verb   delete   and the included entities  however  are identical. to distinguish the two requesttypes  it is necessary to determine the direct object of the verb  delete -which is difficult  since shallow parsing is inaccurate on this very noisy corpus-or else to construct features that are correlated with the direct object of the verb.
　thus  we used the following as features.  a  the counts of keyentities  oldentities  and newentities in a request.  b  the action verbs appearing in a request.  c  the nouns that appear in an np immediately following an action verb  or that appear in nps before an action verb in passive form.  d  nouns from the previous step that also appear in a dictionary of 1 common attribute names  e.g.   phone    extension    room    office   etc .
　the results are shown in table 1. with these features  one can distinguish between these request types quite accurately.
requestf1/errordef.type1cv1cvusr1cvreqerrordeletetuple1 / 11 / 11 / 11deletevalue1 / 11 / 11 / 11table 1: request type classification results
1 target attribute classification
the classification of requests by target attributes is very similar to request type classification  except that rather than determining if a delete request concerns an attribute  one must determine which attribute the request concerns. given our assumptions  this step need only be performed for deletevalue requests that do not specify an oldentity value.
　here in fact we learn a vocabulary for attributes names. a simple bag-of-words feature works quite well for this task  as is shown by the results in table 1 in the  1cv  column. the vocabulary used in the corpus to describe each attribute is fairly small: e.g.  phone is usually described as  phone    line  or  extension . perhaps this is because users tend to use the terminology of the website  or because the relevant vocabularies are limited by nature.
requestf1/errordef.type1cv1cvusr1cvreqerrorpersonal name1 / 11 / 11 / 11phone#1 / 11 / 11 / 11room#1 / 11 / 11 / 11publication1 / 11 / 11 / 11photo1 / 11 / 11 / 11cv1 / 11 / 1- / -1amount1 / 11 / 11 / 11table 1: attribute classification results
1 robustness issues
one practically important question is how robust this automated webmaster is to changes in the distribution of users and/or requests. to investigate such questions  one can use a different sampling strategy in performing cross-validation. for instance  to determine how robust the system is to queries from new users  we grouped all the examples generated by each subject into a single set  and then performed a crossvalidation constrained so that no set was split between training and test. in other words  in every test fold  all of the example requests were from subjects that had not contributed to the training set. this split thus estimates performance of a system that is used for a very large pool of users. cross validation by user results are given in the results tables  in the columns marked as  usr .
　in the corpus  users usually have some personal stylistic quirks-for instance  a user might consistently give dates  names etc. in a particular format. thus one would expect that performance with this sort of split will be worse than performance with the default uniform splits. as can be seen from the results  the f1 for most ner task drops only slightly  and is above 1 for all entity types. slight drops in performance are also seen on two of the three entity-role tasks  and noticible drops are seen on two of the seven attribute-classification tasks  person name and photo . overall  performance seems to be affected only slightly in this setting.
　similarly  to determine how robust the system is to future requests that are quite different from requests encountered during training  we grouped together examples for the
same request type  including all requests generated from a particular pictorial task   and then again performed a crossvalidation constrained so that no set was split between training and test. in this scenario  all of the example requests in every test fold are for tasks that were not encountered in the training set. the results of this split are given in the columns titled as  req .
　to summarize the results  the loss in performance for ner problems is moderate  but larger than that seen when splitting by users. entity-role classification drops off only slightly  and performancefor target-relation classification also remains excellent for most relations. however  performance for requesttype classification does drop off noticibly. this drop is almost certainly due to lack of appropriate training data: there are only a handful of tasks updatingthe  budget relation  and also only a relatively small number of tasks requiring requesttype classification. similarly  the task of classification by attribute name is practically infeasible for some attribute types in this settings  due to the small number of attribute names mentions in the corpus. however  provided that the system is given sufficient training data for the relevant relation and attribute  it should perform well on different requests.
1 overall evaluation
in this section  we complement the component-level evaluations with an evaluation of the entire end-to-end process. we executed the tasks in the following order: ner is run for each entity type; then  roles of the extracted entities are assigned; finally  relation and request types are assigned. note that the noisy predicted entities  i.e.  entities extracted by a ner model  were used as input to the entity-role classifier  as well as to the relation to the request-type classifiers. here we used vphmms and hand-coded rules for extraction  and a non-sequential multi-class voted perceptron  freund and schapire  1  for classification.1
　from the user's perspective  it is interesting to note what percentage of the requests can be successfully processed at a message level  at different levels of automation. in the experiments  1% of the messages got both their relation and request type classified correctly. in these cases the user would have received the correct form  with some entries filled out incorrectly. in more than half of the cases  1%   the user would have received the correct form  with all entities correctly extracted  but with some entity roles mislabeled. in 1% of the messages  the automatic processingencountered no errors at all.
　note that in the end-to-end scenario errors from the entity recognition phase are propagated to role classification task. also  in order for a message to be considered fully correct  assignments must be accurate for each one of the multiple entities included in this message. that is  many correct decisions must be made for perfect performance per request. overall  we find these results to be very promising  considering the limited size of our corpus.
1 related work
lockerd et. al  propose an automated webmaster called  mr. web  which has a similar email-based interface.
they manually analyzed 1 update requests to assess their linguistic regularity  but do not describe any algorithm for processing the requests.
　our system addresses a fairly general natural-language processing task: learning to understand database updates. as such it might be compared to other systems that use learning in nlp. previous nlp systems have generally either performed deep semantic analysis using hand-coded grammars in a restricted domain  or else a shallower analysis in a broader domain. while learning has been an important tool for developing broad-coverage nlp components such as pos taggers  parsers  and named entity recognition systems  there have surprisingly few attempts to use learning to perform a complete semantic analysis. notable exceptions are the chill system  zelle and mooney  1   which learns to parse database queries into a meaning representation language  and the work by miller et. al  on using a combination of generative models to extract facts from text. work in learning such  semantic parsers  is surveyedand motivated elsewhere  mooney  1 .
　there are several important differences between the work described in this paper and prior efforts. one difference is that we consider understanding update requests  rather than understanding queries  like zelle & mooney  or declaratively stated facts  like miller et al . one advantage of the updaterequest task is that a partially correct analysis is still useful  and furthermore  is likely to elicit user feedback which can be used for training. in contrast  it is unclear how useful it is to answer an imperfectly analyzed database query  or what could be learned from such an episode. a second difference is that our learning method uses primarily data which can plausibly collected from user feedback. in contrast  zelle & mooney's system learns from sentence/query pairs  and miller et. al. use a variety of sources for training data including pos-tagged text  parsed sentences  and semantically annotated text. on the other hand  we limit ourselves to conceptually simple database updates  while zelle & mooney consider complex structured queries. there are also numerous smaller differences stemming from the nature of the task and corpus.
　although the purpose and scope of our research is different  the entity role classification step we consider above is broadly similar to recent work on semantic role analysis  fillmore et al.  1; gildea and jurafsky  1   and earlier work on case-role assignment  e.g.   miikkulainen and dyer  1  .
1 conclusions
we have described and experimentally evaluated a scheme for processing email requests for certain website facutal updates using a sequence of entity recognition and classification tasks. we showed that the noisy informal email text can be successfully processed applying a learning approach  using relatively small sets of syntactic features. experimental results show that also with limited amount of data  the system reaches a promising rate of 1% of messages processed perfectly. we expect this rate to improve as the examples set grows. further  human-subject experiments have also shown that partially correct results are useful in settings described here  tomasic et al.  1 . thus the work in this paper is a realistic evaluation of components of an efficient  adaptive  automatic webmaster assistant.
　open questions that remain to be resolved by future research include relaxing the restriction that each request concerns the update of a single tuple per email and evaluating more complex entity types for the entity recognition component. improving on entity recognition will both enable expansion of system coverage  as well as boost its overall performance.
acknowledgements
this material is based upon work supported by the defense advanced research projects agency  darpa  under contract no. nbchd1. any opinions  findings and conclusions or recommendations expressed in this material are those of the author s  and do not necessarily reflect the views of the defense advanced research projects agency  darpa   or the department of interior-national business center  doi-nbc .
