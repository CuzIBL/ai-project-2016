 
simple recurrent networks  srns  have been widely used in natural language tasks. sardsrn extends the srn by explicitly representing the input sequence in a sardnet self-organizing map. the distributed srn component leads to good generalization and robust cognitive properties  whereas the sardnet map provides exact representations of the sentence constituents. this combination allows sardsrn to learn to parse sentences with more complicated structure than can the srn alone  and suggests that the approach could scale up to realistic natural language. 
1 introduction 
the subsymbolic approach  i.e. neural networks with distributed representations  to processing language is attractive for several reasons. first  it is inherently robust: the distributed representations display graceful degradation of performance in the presence of noise  damage  and incomplete or conflicting input  miikkulainen  1; st john and mcclelland  1 . second  because computation in these networks is constraint-based  the subsymbolic approach naturally combines syntactic  semantic  and thematic constraints on the interpretation of linguistic data  mcclelland and kawamoto  1 . third  subsymbolic systems can be lesioned in various ways and the resulting behavior is often strikingly similar to human impairments  miikkulainen  1;. 1; plant  1 these properties of subsymbolic systems have attracted many researchers in the hope of accounting for interesting cognitive phenomena  such as role-binding and lexical errors resulting from memory interference and overloading  aphasic and dyslexic impairments resulting from physical damage  and biases  defaults and expectations emerging from training history  miikkulainen  1; 1; 1; plaut and shallice  1 . 
　since its introduction in 1  the simple recurrent network  srn   elman  1  has become a mainstay in connectionist natural language processing tasks such as lexical disambiguation  prepositional phrase attachment  active-passive transformation  anaphora resolution  and translation  allen  1; chalmers  1; munro et al. 1; touretzky  1 . this paper describes an extension to the standard srn  which 
1 	machine learning 
utilizes sardnet  james and miikkulainen  1   a selforganizing map algorithm designed to represent sequences. sardnet permits the sequence information to remain ex-
plicit  yet distributed in the sense that similar sequences result in similar patterns on the map. sardsrn  the combination of the srn and sardnet  effectively solves the fundamental memory accuracy limitations of the srn  and allows the processing of sentences of realistic length. 
　this paper shows how sardsrn improves upon the performance of the srn in a nontrivial syntactic shift-reduce parsing task. the results show that sardsrn outperforms the srn in this task by providing an effective solution to the memory problem. sardsrn therefore forms a solid foundation for building a subsymbolic parser of realistic language. 
1 the task: shift-reduce parsing 
the task taken up in this study  shift-reduce  sr  parsing  is one of the simplest approaches to sentence processing that nevertheless has the potential to handle a substantial subset of english  tomita  1 . its basic formulation is based on the pushdown automata for parsing context-free grammars  but it can be extended to context-sensitive grammars as well. 
　the parser consists of two data structures: the input buffer stores die sequence of words remaining to be read  and the partial parse results are kept on the stack  figure 1 . initially the stack is empty and the entire sentence is in the input buffer. at each step  the parser has to decide whether to shift a word from the buffer to the stack  or to reduce one or more of the top elements of the stack into a new element representing their combination. for example  if the top two elements are currendy np and vpt the parser reduces diem into 1  corresponding to the grammar rule s  np vp  step 1 in figure 1 . the process stops when the elements in the stack have been reduced to s  and no more words remain in the input  the reduce actions performed by the parser in this process constitute the parse result  such as the syntactic parse tree  line 1 in figure 1 . 
　the sequential scanning process and incremental forming of partial representations is a plausible cognitive model for language understanding. sr parsing is also very efficient  and lends itself to many extensions. for example  the parse rules can be made more context sensitive by taking more of the stack and the input buffer into account also  the partial parse results may consist of syntactic or semantic structures. 


figure 1: shift-reduce parang a sentence. each step in the parse is represented by a line fro the current stack is at left  the input buffer in the middle  and the parsing decision in the current situation at right. at each step  the parser either shifts a word onto the stack  or reduces the top elements of the stack into a higher-level representation  such as the boy np the boy   step 1 .  phrase labels such as  np  and  rc are only used in this figure to make the process clear.  

　the general sr model can be implemented in many ways. a set of symbolic shift-reduce rules can be written by hand or learned from input examples  hermjacob and mooney  1; simmons and yu  1; zelle and mooney  1 . it is also possible to train a neural network to make parsing decisions based on the current stack and the input buffer. if trained properly  the neural network can generalize well to new sentences  simmons and yu  1 . whatever correlations there exist between the word representations and the appropriate shiftz/reduce decisions  the network will learn to utilize them. 
　another important extension is to implement the stack as a neural network. this way the parser can have access to the entire stack at once  and interesting cognitive phenomena in processing complex sentences can be modeled. the spec system  miikkulainen  1  was a first step in this direction. the stack was represented as a compressed distributed representation  formed by a raam  recursive auto-associative memory  auto-encoding network  pollack  1 . the resulting system was able to parse complex relative clause structures. when the stack representation was artificially lesioned by adding noise  the parser exhibited very plausible cognitive performance. shallow center embeddings were easier to process  as were sentences with strong semantic constraints in the role bindings. when the parser made errors  it usually switched the roles of two words in the sentence  which is what people also do in similar situations. a symbolic representation of the stack would make modeling such behavior very difficult. 
　the spec architecture  however  was not a complete implementation of sr parsing; it was designed specifically for embedded relative clauses. for general parsing  the stack needs to be encoded with neural networks to make it possible to parse more varied linguistic structures. we believe that the generalization and robustness of subsymbolic neural networks will result in powerful  cognitively valid performance. however  the main problem of limited memory accuracy of the sen parsing network must first be solved. an architecture that will do that  sardsrn  will be described next 

figure 1: the sardsrn network. this snapshot shows the network during step 1 of figure 1. the representation for the current input word  chased  is shown at top left. each word is input to the sardnet map  which builds a representation for the sequence word by word. at each step  the previous activation of the hidden layer is copied  as indicated by the dotted line  to the previous hidden layer assembly. this activation  together with the current input word and the current sardnet pattern  is propagated to the hidden layer of the srn network. as output  the network generates the compressed raam representation of the top element in the shift-reduce stack at this state of the parse  in this case  line 1 in figure 1 . sardnet is a map of word representations  and is trained through the self-organizing map  som  algorithm  kohonen  1; 1 . all other connections are trained through backpropagation. 
1 the sardsrn parser architecture 
1 	simple recurrent network 
the starting point for sardsrn  figure 1  is the simple recurrent network. the network reads a sequence of input word representations into output patterns representing the parse results  such as syntactic or case-role assignments for the words. at each time step  a copy of the hidden layer is saved and used as input during the next step  together with the next word. in this way each new word is interpreted in the context of the entire sequence so far  and die parse result is gradually formed at the output. 
	mayberry iii and miikkulainen 	1 

　the srn architecture can be used to implement a shiftreduce parser in the following way: the network is trained to step through the parse  such as that in figure l   generating a compressed distributed representation of the top element of the stack at each step  formed by a raam network: section 1 . the network reads the sequence of words one word at a time  and each time either shifts the word onto the stack  by passing it through the network  e.g. step 1   or performs one or more reduce operations  by generating a sequence of compressed representations corresponding to the top element of the stack: e.g. steps 1 . after the whole sequence is input  the final stack representation is decoded into a parse result such as a parse tree. such an architecture is powerful for two reasons:  1  during the parse  the network does not have to guess what is coming up later in the sentence  as it would if it always had to shoot for the final parse result; its only task is to build a representation of the current stack in its hidden layer and the top element in its output.  1  instead of having to generate a large number of different stack states at the output  it only needs to output representations for a relatively small number of common substructures. both of these features make learning and generalization easier. 
　a well-known problem with the srn model is its low memory accuracy. it is difficult for it to remember items that occurred several steps earlier in the input sequence  especially if the network is not required to produce mem in the output layer during the intervening steps  stolcke  1; miikkulainen  1 . the intervening items are superimposed in the hidden layer  obscuring the traces of earlier items. nor has simply increasing the size of the hidden layer or lowering the learning rate been found to offer much advantage. as a result  parsing with an srn has been limited to relatively simple sentences with shallow structure. 
1 	sardnet 
the solution described in this paper is to use an explicit representation of the input sequence as additional input to the hidden layer. this representation provides more accurate information about the sequence  such as the relative ordering of the incoming words  and it can be combined with the hidden layer representation to generate accurate output that retains all the advantages of distributed representations. the sequence representation must be explicit enough to allow such cleanup  but it must also be compact and generalize well to new sequences. 
　the sardnet  sequential activation retention and decay network   james and miikkulainen  1  selforganizing map for sequences has exactly these properties. s ardnet is based cm the self-organizing map neural network  kohonen  1; 1   and organized to represent die space of all possible word representations. as in a conventional self-organizing map network  each input word is 
mapped onto a particular map node called the maximallyresponding unit  or winner. the weights of the winning unit and all the nodes in its neighborhood are updated according to the standard adaptation rule to better approximate the current input the size of the neighborhood is set at the beginning of the training and reduced as the map becomes more organized. in s ardnet  the sentence is represented as a distributed 
1 	machine -learning 

figure 1: grammar. this phrase structure grammar generates sentences with subject- and object-extracted relative clauses. the rule schemata with noun and verb restrictions ensure agreement between subject and object depending on the verb in the clause. lexicon items are given in bold face  
activation pattern on the map  figure 1 . for each word  the maximally responding unit is activated to a maximum value of 1  and the activations of units representing previous words are decayed according to a specified decay rate  e.g. 1 . once a unit is activated  it is removed from competition and cannot represent later words in the sequence. each unit may then represent different words depending on the context  which allows for an efficient representation of sequences  and also generalizes well to new sequences. 
　in the sardsrn architecture  sardnet is used to directly handle the memory limitation of the srn. a sardnet representation of the input sentence is formed at the same time as the srn hidden layer representation  and used together with the previous hidden layer representation and the next word as input to the hidden layer  figure 1 . this architecture allows the srn to perform its task with significantly less memory degradation. the sequence information remains accessible in sardnet  and the srn is able to focus on capturing correlations relating to sentence constituent structure during parsing. 
1 experiments 
1 	input data  training  and system parameters 
the data used to train and test the srn and sardsrn networks were generated from the phrase structure grammar in figure 1  adapted from a grammar that has become common in the literature  elman  1; miikkulainen  1 . since our focus was on shift-reduce parsing  and not processing relative clauses per se  sentence structure was limited to one relative clause per sentence. from this grammar training targets corresponding to each step in the parsing process were obtained. for shifts  the target is simply the current input. in these cases  the network is trained to auto-associate  which these networks are good at. for reductions  however  the targets consist of representations of the partial parse trees that result from applying a grammatical rule. for example  the reduction of the sentence fragment who liked the girl would produce the partial parse result  who  liked  the iri   . two issues arise: how should the parse trees be represented  and how should reductions be processed during sentence parsing  

the 
whom 1 
1 |who 
 1 | 1 boy girl 1 
1 dog cat 1  chased   liked 1 
1 |saw 
 1 b i t 1 figure 1: lexicon  each word representation is put together from a part~of-speech identifier  first four components  and a unique id tag oast four . this encoding is then repeated eight times to form a 1-unit word representation. such redundancy makes it easier to identify the word. 
　the approach taken in this paper is the same as in spec  section 1   as well as in other connectionist parsing systems  miikkulainen  1; berg  1; sharkey and sharkey  1 . compressed representations of all the partial syntactic parse trees using raam are built up through auto-association of die constituents. this training is performed beforehand separately from the parsing task. once formed  the compressed representations can be decoded into their constituents using just the decoder portion of the r aam architecture. 
　in shift-reduce parsing  the input buffer after each  reduce  action is unchanged; rather  the reduction occurs on the stack. therefore  if we want to perform the reductions one step at a time  the current word must be maintained in the input buffer until the next  shift  action. accordingly  the input to the network consists of the sequence of words that make up the sentence with the input word repeated for each reduce action  and the target consists of representations of the top element of the stack  as shown in figure 1 . 
　word representations were hand-coded to provide basic part-of-speech information together with a unique id tag that identified the word within the syntactic category  figure 1 . the basic encoding of eight units was repeated eight times to fill out a 1-unit representation  the 1-unit representation length was needed to encode all of the partial parse results formed by raam  and redundancy in the lexical items facilitate learning. 
　four data sets of 1%  1%  1%  and 1% of the 1 sentences generated by the grammar were randomly selected to train both parsers  and each parser was trained on each dataset four times. training on all thirty-two runs was stopped when die error on a 1-sentence  1%  validation set began to level off. the same validation set was used for all the simulations and was randomly drawn from a pool of sentences that did not appear in any of the training sets. testing was then performed on the remaining sentences that were neither in the training set nor in the validation set. 
　the srn network architecture consisted of a 1-unit input layer  1-unit hidden and context layers  and 1-unit output and target layers. sardsrn added a 1-unit feature map  sardnet  to the srn setup. a learning rate of 1 was used to train both networks  while the learning and decay rates for the s ardnet feature map input in sardsrn were set to 1 and 1  respectively. the neighborhood was set at 1 initially and gradually reduced to 1. these parameters were found experimentally to result in the best general performance for both parsers. 

figure 1: results. averages over four simulation runs using the stricter average mismatches per sentence measure on the test data. the srn's performance in all 1 runs bottomed out at a much higher error than sardsrn  while still unable to parse all of the training sentences. sardsrn  on the other hand  did learn to parse the training sentences  and showed very good generalization to the test sentences. these differences are statistically significant with p   1. 
1 	results 
the average mismatches performance measure reports the average number of leaf representations per sentence that are not correctly identified from the lexicon by nearest match in euclidean distance. as an example  if the target is 
 step 1 of figure 1   but the outthen a mismatch would occur 
at the leaf labelled saw once the raam representation was decoded. average mismatches provide a measure of the correctness of the information in the raam representation. it is a much stricter measure of the utility of the network than the standard mean squared error and was  therefore  used in our experiments. 
　training took about four days on a 1 mhz pentium pro workstation  with sardsrn taking about 1 times as long per epoch as the srn alone. the validation error in the srn runs quickly leveled off  and continued training did nothing to improve it on the other hand  the sardsrn simulation runs ware still showing slight improvements when they were cut off. figure 1 plots these performance measures averaged over the four simulation runs against the test sentences. 
　by all measures  sardsrn performed significantly-even qualitatively-better than th e standard srn. on the training datasets  there was roughly an order of magnitude difference in both the epoch errors and the average number of mismatches per sentence between sardsrn and srn. these results suggest that the srn could not even learn the training data to any useful extent  whereas sardsrn does not appear to be nearing its limit. on the test sets  the epoch error for the srn never fell below 1  and there were nearly 1 mismatches per sentence on average. even in the most difficult case for the sardsrn  on the 1% test dataset  in which the networks were trained on just 1 sentences  and tested on 1   these errors never reached half that level. these results show tiiat sardsrn forms a promising starting point for parsing sentences of realistic length and complexity. 
	mayberry iii and miikkulainen 	1 

1 example parse 
adding sardnet to the srn architecture made it possible for the network to learn the parsing task. this can be shown clearly by contrasting the performances of sardsrn and the srn on a typical sentence  such as the one in figure 1. neither s ardsrn nor srn had any trouble with the shift targets. not surprisingly  early in training the networks would master all the shift targets in the sentence before they would get any of the reductions correct the first reduction   the boy  in our example  also poses no problem for either network. nor  in general  does the second   the girl   because the constituent information is still fresh in memory. however  the ability of the srn to generate the later reductions accurately degrades rapidly because the information about earlier constituents is smothered by the later steps of the parse. interestingly  the structural information survives much longer. for example  instead of the srn might produce 
the structure of this representation 
is correct; what is lost are the particular instantiations of the parse tree. this is where sardnet makes a difference. the lost constituent information remains accessible in the feature map. as a result  sardsrn is able to capture each constituent even through the final reductions. 
1 discussion 
these results demonstrate a practicable solution to the memory degradation problem of simple recurrent networks. the srn does not have to maintain specific information about the sequence constituents  and can instead focus on what it is best at: capturing structure. the explicit and concise representation of the entire sequence on sardnet also enables sardsrn to handle long-term dependencies better than a movingwindow architecture such as narx  mayberry and miikkulainen  in press . although the sentences used in these experiments are still relatively uncomplicated  they do exhibit enough structure to suggest that much more complex sentences could be tackled with sardsrn. 
　the operation of sardsrn on the shift-reduce parsing task is a nice demonstration of holistic computation. the network is able to learn how to generate each raam parse representation during the course of sentence processing without ever having to decompose and recompose the constituent representations. partial parse results can be built up incrementally into increasingly complicated structures  which suggests that training could be performed incrementally. such a training scheme is especially attractive given that training in general is still relatively cosdy. 
　an extension of die sardsrn idea  currently being investigated by our group  is an architecture where sardnet is combined with a raam network. raam  although having many desirable properties for a purely connectionist approach to parsing  has long been a bottleneck during training. its operation is very similar to the srn  and it suffers from die same memory accuracy problem: with deep structures the superimposition of higher-level representations gradually obscure the traces of low-level items  and the decoding becomes inaccurate. this degradation makes it difficult to use raam to encode/decode parse results of realistic language. 
 1 	machine learning 
preliminary results indicate that the explicit representation of a compressed structure formed on a sardnet feature map  coupled with the distributed representations of the raam  yields an architecture able to encode richer linguistic structure.. this approach should readily lend itself to encoding the feature-value matrices used in the lexicalist  constraint-based grammar formalisms of contemporary linguistics theory  such as hpsg  pollard and sag  1   needed to handle realistic natural language. 
　the sardsrn idea is not just a way to improve the performance of subsymbolic networks; it is an explicit implementation of the idea that humans can keep track of identities of elements  not just their statistical properties  miikkulainen  1 . the subsymbolic networks are very good with statistical associations  but cannot distinguish between representations that have similar statistical properties. people can; whether they use a map-like representation is an open question  but we believe the sardnet representation suggests a way to capture a lot of the resulting behavior. it is useful for building powerful subsymbolic language understanding systems  but it is also a plausible cognitive approach. 
1 conclusion 
we have described an extension of the srn called sardsrn that combines the subsymbolic distributed properties of the srn with the localtst properties of sardnet. the distributed component leads to good generalization and robust cognitive properties  whereas the map provides exact representations of the sentence constituents. the results in this paper demonstrate a practicable solution to the memory degradation problem of srns. with sardnet keeping track of the sequence constituents  the srn is able to learn the structure representation necessary to perform shift-reduce parsing. this combination allows sardsrn to learn to parse longer and more complex sentences than the srn alone. the representative properties of sardnet also promise to allow raam to encode die more complicated structures used in linguistics theory. 
acknowledgments 
this research was supported in part by the texas higher education coordinating board under grant arp-1. sardsrn demo: http://www.cs.utexas.edu/users/nn/pages/research/nlp.html. 
