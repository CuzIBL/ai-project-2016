 
combinatorial auctions  i.e. auctions where bidders can bid on combinations of items  tend to lead to more efficient allocations than traditional auctions in multi-item auctions where the agents' valuations of the items are not additive. however  determining the winners so as to maximize revenue is complete. we present a search algorithm for optimal winner determination. experiments are shown on several bid distributions. the algorithm allows combinatorial auctions to scale up to significantly 
larger numbers of items and bids than prior approaches to optimal winner determination by capitalizing on the fact that the space of bids is necessarily sparsely populated in practice. we do this via provably sufficient selective generation of children in the search and by using a method for fast child generation  heuristics that are accurate and optimized for speed  and four methods for preprocessing the search space. 
1 	introduction 
auctions are popular  efficient  and autonomy preserving ways of allocating items among agents. this paper focuses on auctions with multiple items to be allocated. 
　in a sequential auction  the items are auctioned one at a time. if a bidder has preferences over bundles  i.e. combinations of items  as is often the case e.g. in electricity markets  equities trading  bandwidth auctions  mcafee and mcmillan  1   and transportation exchanges  sandholm  1    bidding in such auctions is difficult. to determine her valuation for an item  the bidder needs to guess what items she will receive in later auctions. this requires speculation on what the others will bid in the future because that affects what items she will receive. furthermore  what the others bid in the future depends on what they believe others will bid  etc. this counterspeculation introduces computational cost and other wasteful overhead. moreover  in auctions with a reasonable number of items  such lookahead in the game tree is intractable  and then there is no known way to bid rationally. bidding rationally would involve optimally trading off the cost of lookahead against the gains it provides  but that would again depend on how others strike that tradeoff. furthermore  even if lookahead were computationally manageable  usually uncertainty remains about the others' bids because agents do not have exact information about each other. this often 
 patent pending since 1/1. 
1 	distributed al leads to inefficient allocations where bidders fail to get the combinations they want and get ones they do not. 
　in a parallel auction the items cire open for auction simultaneously and bidders may place their bids during a certain time period. this has the ad vantage that the others' bids partially signal to the bidder what the others1 bids will end up being so the uncertainty and the need for lookahead is not as drastic as in a sequential auction. however  the same problems prevail as in sequential auctions  albeit in a mitigated form. 
   combinatorial auctions can be used to overcome the need for lookahead and the inefficiencies that stem from the uncertainties  rassenti et al  1  sandholm  1 . in a combinatorial auction bidders may place bids on combinations of items. this allows the bidders to express complementarities between items instead of having to speculate into an item's valuation the impact of possibly getting other  complementary items. for example  the federal communications commission saw the desirability of combinatorial bidding in their bandwidth auctions  but it was not allowed due to perceived intractability of winner determination. this paper focuses on winner determination in combinatorial auctions where each bidder can bid on bundles of indivisible items  and any number of her bids can be accepted. 
1 	winner determination 
let m be the set of items to be auctioned  and let. m = 
|a/|. then any agent  /'  could place any bid for any combination 
let n be the number of these bids. winner determination is the following problem  where the goal is to maximize the auctioneer's revenue: 

where  is a valid outcome  i.e. an outcome where each item is allocated to only one bidder:  
 for every  
　if each combination s has received at least one bid of positive price  the search space will look like fig. 1. 
proposition 1 	the number of allocations is and  
the proof is long  and is presented in  sandholm  1 . 
　the graph can be searched more efficiently than exhaustive enumeration by dynamic programming  which takes  and  steps  rothkopf et al.  1 . 
this is still too complex to scale up above about 1 items. also  dynamic programming executes the same 


figure 1: space of allocations in a 1-item example. each node represents one possible allocation x. 
algorithmic steps regardless of which bids have actually been submitted. 
　some combinations of items may not have received any bids  so some of the allocations in the graph need not be considered. unfortunately no algorithm can find the optimal allocation in polynomial time in  the number of bids submitted  unless  
proposition 1 winner determination is  complete. 
proof. 	winner determination is weighted set packing  and set packing is   karp  1 .  
even approximate winner determination is hard: 
proposition 1 no polytime algorithm can guarantee an allocation within a bound  from optimum for any  1  unless  equals probabilistic polytime . 
the proof is based on  hastad  1   and is presented in the full length version of this paper  sandholm  1 . 
　if the bids exhibit special structure  better approximations can be achieved in polynomial time  chandra and halldorsson  1  halldorsson  1  hochbaum  1  
halldorsson and lau  1   but even these guarantees are so far from optimum that they are irrelevant for auctions in practice  sandholm  1 . 
　polynomial time winner determination can be achieved by restricting the combinations on which the agents are allowed to bid  rothkopf et al.  1 . however  because the agents may then not be able to bid on the combinations they want  similar economic inefficiencies prevail as in the non-combinatorial auctions. 
1 	our optimal search algorithm 
the goals of our approach to winner determination are: 
  allow bidding on all combinations. 
  strive for the optimal allocation. 
  completely avoid loops and redundant generation of vertices when searching the allocation graph  fig. 1. 
  capitalize heavily on the sparseness of bids. in practice the space of bids is necessarily extremely sparsely populated. for example  if there are 1 items  there are  combinations  and it would take longer than the life of the universe to bid on all of them even if every person in the world submitted a bid per second. sparseness of bids implies sparseness of the allocations  that need to be checked. 
our algorithm constructively checks each allocation x that has positive value exactly once  and does not construct the other allocations. therefore  unlike dynamic programming  the algorithm only generates those parts of the search space which are actually populated by bids. the disadvantage then is that the run time depends on the bids received. 
　to achieve these goals  we use a search algorithm that generates a tree  fig. 1. each path in the tree consists of a sequence of disjoint bids  i.e. bids that do not share items. as a bid is added to the path  the bid price is added to the  g-function. a path terminates when all items have been used on that path. at that point the path corresponds to a feasible allocation  and the revenue from that allocation  i.e. the  /-value  can be compared to the best one found so far to determine whether the allocation is the best one so far. the best so far is stored  and once the search completes  that allocation is optimal. 

figure 1: a search tree generated by our algorithm. 
　the naive method of constructing the search tree would include all bids  that do not include items that are already on the path  as the children of each node. instead  the following proposition enables a significant reduction of the branching factor by capitalizing on the fact that the order of the bids on a path does not matter. 
proposition 1 every allocation will be explored exactly once in the tree if the children of a node are those bids that 
  include the item with the smallest index among the items that are not on the path yet  and 
  do not include items that are already on the path. 
proof. we first prove that each allocation is generated at most once. the first bullet leads to the fact that an allocation can only be generated in one order of bids on the path. so  for there to exist more than one path for a given allocation  some bid would have to occur multiple times as a child of some node. however  the algorithm uses each bid as a child for a given node only once. 
　what remains to be proven is that each allocation is generated. assume for contradiction that some allocation is not. then  at some point  there has to be a bid in that allocation such that it is the bid with the item with the smallest index among those not on the path  but that bid is not inserted to the path. contradiction d 
　our search algorithm restricts the children according to the proposition  fig. 1. this can be seen for example at the first level because all the bids considered at the 
	sandholm 	1 

first level include item 1. the minimal index does not coincide with the depth of the search tree in general. 
　the auctioneer's revenue can increase if he can keep items. that can be profitable if some item has received no bids on its own. for example  if there is no bid for item 1  a $1 bid for item 1  and a $1 bid for the combination of 1 and 1  it is more profitable for the auctioneer to keep 1  and to allocate 1 alone. such optimization can be implemented by placing dummy bids of price zero on those individual items that received no bids alone  fig. 1. for example  if item 1 had no bids on it alone and dummies were not used  the tree under 1 would not be explored and optimality could be lost. when dummy bids are used  the resulting search generates each allocation that has positive revenue exactly once  and searches through no other allocations . this guarantees that the algorithm finds the optimal solution. throughout the rest of the paper  we use this dummy bid technique. 
1 	o p t i m i z e d g e n e r a t i o n o f c h i l d r e n 
the main search algorithm uses a secondary depth-firstsearch  dfs  to quickly determine the children of a node. the secondary search occurs in a data structure which we call the bidtree. it is a binary tree in which the bids are inserted up front as the leaves. only those parts of the tree are generated for which bids are received  fig. 1. what makes the data structure special is the use of a 
stopmatk 

figure 1: the bidtree data structure. 
stopmask. the stopmask is a vector with one variable for each auctioned item. if the variable corresponding to an item has the value blocked  those parts of the bidtree are pruned instantaneously  and in place  that contain bids containing that item. in other words  search in the bidtree will never progress left at that level. if the item's variable has the value must  all other parts of the bidtree are pruned instantaneously and in place  i.e. search cannot progress right at that level. the value any corresponds to no pruning based on that item: the search may go left or right. 
　to start  the first item has value must in the stopmask  and the others have any. the first child of any given node in the main search is determined by a dfs from the top of the bidtree. the siblings of that child are determined by backtracking in the bidtree after the main search has explored the tree under the first child. as a bid is appended to the path of the main search  blocked is inserted in the stopmask for each item of that bid. that implements the branching reduction of the main search based on the second bullet of prop. 1. 
1 	distributed al 
must is inserted at the unallocated item with the smallest index. that implements the branching reduction of the main search based on the first bullet of prop. 1. these must and blocked values are changed back to any when backtracking a bid from the path of the main search  and must is reallocated to the place where it was before that bid was appended to the path. 
　the secondary search can be implemented to execute in place  i.e. without memory allocation during search. that is accomplished via the observation that recursion or an open list is not required because in dfs  to decide where to go next  it suffices to know where the search focus is now  and from where it most recently came. 
1 	a n y t i m e 	w i n n e r 	d e t e r m i n a t i o n 	v i a depth-first-search 	  d f s   
we first implemented the main search as dfs which executes in linear space. the depth-first strategy causes feasible allocations to be found quickly  the first one is generated in linear time when the first search path ends   and the solution improves monotonically since the algorithm keeps track of the best solution found so far. this implements the anytime feature: if the algorithm does not complete in the desired amount of time  it can be terminated prematurely  and it guarantees a feasible solution that improves monotonically over time. when testing the anytime feature  it turned out that in practice most of the revenue was generated early on as desired  and there were diminishing returns to computation. 
1 	preprocessing 
our algorithm preprocesses the bids in four ways to make the main search faster without compromising optimality. the next subsections present the preprocessors in the order in which they are executed. 
p r e 1 : keep only the highest bid for a combination 
as a bid arrives  it is inserted into the bidtree. if a bid for the same s already exists in the bidtree  only the bid with the higher price is kept  and the other bid is discarded. we break ties in favor of the earlier bid. 
pre1: remove provably noncompetitive bids 
this preprocessor removes bids that are provably noncompetitive. a bid  prunee  is noncompetitive if there is some disjoint collection of subsets of that bid such that the sum of the bid prices of the subsets exceeds or equals the price of the prunee bid. for example  a $1 bid for items 1  1  1  and 1 would be pruned by a $1 bid for items 1 and 1  and a $1 bid for items 1 and 1. 
　to determine this we search  for each bid  potential prunee   through all combinations of its disjoint subset bids. this is the same dfs as the main search except that it restricts the search to those bids that only include items that the prunee includes  fig. 1 : blocked is kept in the stopmask for other items. 
　especially with bids that contain a large number of items  pre1 can take more time than it saves in the main search. in the extreme  if some bid contains all items  the preprocessing search with that bid as the prunee is the same as the main search  except for one main search 


figure 1: 	a search tree generated for one prunee in 
pre1. the dotted paths are not generated because pruning occurs before they are reached. 
path that contains that bid only . to save preprocessing time  pre1 is carried out partially. some of the noncompetitive bids are left unpruned  but that will not affect optimality of the main search-although it can make it slower. we implemented two ways of restricting pre1: 
1. a cap  t  on the number of pruner bids that can be combined to try to prune a particular prunee bid. this limits the depth of the search in pre1 to i  
1. a cap   on the number of items in a prunee bid. longer bids would then not be targets of pruning. 
this entails a cap   on tree depth. it also tends to exclude wide trees because long prunees usually lead to trees with large branching factors. 
with either method pre1 takes  time  which is polynomial for a constant cap  there are n prunees  the tree for each is  and finding a child in the bidtree is o m  . the latter method is usually preferable. it does not waste computation on long prunees which take a lot of preprocessing time and do not significantly increase the main search time. this is because the main search is shallow along the branches that include long bids: each item can only occur once on a path and a long bid uses up many items. second  if the bid prices are close to additive  the former method does not lead to pruning when a path is cut prematurely based on the cap. 
pre1: decompose bids into connected sets 
the bids are partitioned into sets such that no item is shared by bids from different sets. pre1 and the main search are then done in each set of bids independently  and using only items included in the bids of the set. the sets are determined as follows. we define a graph where bids are vertices  and two vertices share an edge if the bids share items. we generate an adjacency list representation of the graph in  time. we use dfs to generate a depth-first forest of the graph in time. each tree is then a set with the desired property. 
p r e 1 : m a r k noncompetitive tuples of bids 
noncompetitive tuples of disjoint bids are marked so that they need not be considered on the same path in the main search. for example  the pair of bids $1 for items 1 and 1  and $1 for items 1 and 1 is noncompetitive if there is a bid of $1 for items 1 and 1  and a bid of $1 for items 1 and 1. noncompetitive tuples are determined as in pre1 except that now each prunee is a virtual bid that contains the items of the bids in the tuple  and the prunee price is the sum of the prices of those bids. 
　for computational speed  we only mark 1-tuples  i.e. pairs of bids. a pair of bids is excluded also if the bids share items. pre1 is used as a partial preprocessor like 
pre1  with caps t or instead of t or  pre1 runs in time. handling 1-tuples would increase this to   etc. handling large tuples also slows the main search because it needs to ensure that noncompetitive tuples do not exist on the path. 
　as a bid is appended to the path  it excludes from the rest of the path those other bids that constitute a noncompetitive pair with it. our algorithm determines this quickly as follows. for each bid  a list of bids to exclude is determined in pre1. in the main search  an exclusion count is kept for each bid  starting at 1. as a bid is appended to the path  the exclusion counts of those bids that it excludes are incremented. as a bid is backtracked from the path  those exclusion counts are decremented. then  when searching for bids to append to the main search path from the bidtree  only bids with exclusion count 1 are accepted.1 
1 	i d a * a n d heuristics 
we sped up the main search by using an iterative deepening a*  ida*  search strategy  korf  1  instead of dfs. the search tree  use of the bidtree  and the preprocessors stay the same. at each iteration of ida*-except the last-the ida* threshold gives an upper bound on solution quality. it can be used  for example  to communicate search progress to the auctioneer. 
　since winner determination is a maximization problem  the heuristic function h should never underestimate the revenue from the items that are not yet allocated in bids on the path because that could lose optimality. we designed two heuristics that never underestimate: ... 

　1. as above  but accuracy is increased by recomputing c i  every time a bid is appended to the path since some combinations s are excluded: some of their items are on the path  or they constitute a noncompetitive pair with some bid on the path. we use  1  with several methods for speeding it up. a tally of h is kept  and only some of the c i  values in h need to be updated when a bid is appended to the path. in pre1 we precompute for each bid the list of items that must be updated: items included in the bid and in bids that are on the bid's exclude list. to make the update even faster  we keep a list for each item of the bids in which it belongs. the c i  value is computed by traversing that list and choosing the highest  among the bids that have exclusion count 1. so  recomputing h 
l pre1 and pre1 could be converted into anytime preproces-
sors without compromising optimality by starting with a small cap  conducting the searches  increasing the cap  reconducting the searches  etc. preprocessing would stop when it is complete  cap = n   the user decides to stop it  or some other stopping criterion is met. pre1 and pre1 could also be converted into approximate preprocessors by allowing pruning when the sum of the primers' prices exceeds a fixed fraction of the prunee's price. this would allow more bids to be pruned which can make the main search faster  but it can compromise optimality. 
	sandholm 	1 

takes o nin  time  where m is the number of items that need to be updated  and n is the  average or greatest  number of bids in which those items belong.1 
　on the last ida* iteration  the ida* threshold is always incremented to equal the revenue of the best solution found so far in order to avoid futile search. in other words  once the first solution is found  the algorithm converts to branch-and-bound with the same heuristic. 
1 	experimental setup 
not surprisingly  the worst case complexity of the main search is exponential in the number of bids. however  unlike dynamic programming  this is complexity in the number of bids actually received  not in the number of allowable bids. to determine how the algorithm does in practice  we ran experiments on a regular uniprocessor workstation  1mhz sun ultra 1 with 1 mram  in c-f-f with four different bid distributions: 
  random: for each bid  pick the number of items randomly from l 1 ... m. randomly choose that many items without replacement. pick the price randomly from  1 . 
  weighted random: as above  but pick the price between 1 and the number of items in the bid. 
  u n i f o r m : draw the same number of randomly cho-sen items for each bid. pick the prices from  1 . 
  decay: give the bid one random item. then re-peatedly add a new random item with probability a until an item is not added or the bid includes all m items. pick the price between 1 and the number of items in the bid. 
if the same bid was generated twice  the new version was deleted and regenerated. so if the generator was asked to produce e.g. 1 bids  it produced 1 different bids. 
　we let all the bids have the same bidder. this conservative method causes pre1 to prune no bids. in practice  the chance that two agents bid on the same combination of items is often small anyway because the number of combinations is large  however  in some cases pre1 is very effective. for example  it prunes all of the bids except one if all bids are placed on the same combination by different bidders. 
1 	experimental results 
we focus on ida* because it was two orders of magnitude faster than dfs. we lower the ida* threshold between iterations to 1% of the previous threshold or to the highest / = g + h that subceeded the previous threshold  whichever is smaller. experimentally  this tended to be a good rate of decreasing the threshold. 
if it is decreased too fast  the overall number of search nodes increases because the last iteration becomes large. if it is decreased too slowly  the number of search nodes increases because new iterations repeat a large portion of the search from previous iterations. 
　for pre1  the cap  = 1 gave a good compromise between preprocessing time and main search time. for pre1  = 1 led to a good compromise. these values are used in the rest of the experiments. with these caps  the hard problem instances with short bids get preprocessed completely  and pre1 and pre1 take negligible time compared to the main search because the trees under such short prunees are small. the caps only take effect in the easy cases with long bids. in the uniform distribution all bids are the same length  so pre1 does not prune any bids because no bid is a subset of another. 
　pre1 saved significant time on the uniform and decay distributions by partitioning the bids into sets when the number of bids was small compared to the number of items  and the bids were short. in almost all experiments with random and weighted random  all bids fell in the same set because the bids were long. in real world combinatorial auctions it is likely that the number of bids will significantly exceed the number of items which would suggest that pre1 does not help. however  most bids will usually be short  and the bidders' interests often have special structure which leads to some items being independent of each other  and pre1 capitalizes on that. 
　the main search generated 1 nodes per second when the number of items was small  e.g. 1  and the bids were short. this rate decreased slightly with the number of bids  but significantly with the number of items and bid size. with the random distribution with 1 items and 1 bids  the search generated only 1 nodes per second. however  the algorithm solved these cases easily because the search paths were short and the heuristic focused the search well. long bids make the heuristic and exclusion checking slower but the search tree shallower which makes them easier for our algorithm than short bids overall. this observation is further supported by the results below. each point in each graph represents an average over 1 problem instances. the search times presented include all preprocessing times. 
　the random distribution was easy  fig. 1  since the search was shallow because the bids were long. the weighted random distribution was even easier. the curves become closer together on the logarithmic value axis as the number of items increases  which means that search time is polynomial in items. in the weighted random case  the curves are sublinear meaning that search time is polynomial in bids as well  while in the unweighted case they are roughly linear meaning that search time is exponential in bids. 
　the uniform distribution was harder  fig. 1 left. the bids were shorter so the search was deeper. the curves are roughly linear so complexity is exponential in bids. the spacing of the curves does not decrease significantly indicating that complexity is exponential in items also. fig. 1 right shows complexity decrease as bids get longer. 


figure 1: search time for the random and weighted random distributions. in the random distribution  the point with 1 bids and 1 items is unusually high due to one hard outlier among the 1 problem instances. 

figure 1: search time for the uniform distribution. 
the decay distribution was also hard  fig. 1 left. 
however  the curves get closer together as the number of items increases: complexity is polynomial in items. complexity first increases in  and then decreases  
fig. 1 right. left of the maximum  pre1 decomposes the problem leading to small  fast searches. the hardness peak moves left as the number of bids grows because the decomposition becomes less successful. right of the maximum  all bids are in the same set. the complexity then decreases with a because longer bids lead to shallower search. 

figure 1: search time for the decay distribution. 
1 	conclusions and generalizations 
we presented a search algorithm for optimal winner determination in combinatorial auctions. it allows combinatorial auctions to scale up to significantly larger numbers of items and bids than previous approaches to optimal winner determination. the ida* search can also be distributed across multiple computers for additional speed. we believe that our algorithm will make the difference between being able to use a combinatorial auction design in many practical markets and not. 
　the algorithm can also be used to solve weighted set packing  independent set  and maximum clique problems because they are in fact the same problem. so is coalition structure generation in characteristic function games. 
　the methods discussed so far are based on the common assumption that bids are superadditive:  
 but what happens if agent 1 bids b1  {1}  = 
$1  1 {1}  = $1  and b1 {1}  = $1  and there are no other bidders  the auctioneer could allocate items 1 and 1 to agent 1 separately  and that agent's bid for the combination would value at $1 + $1 = $1 instead of $1. so  the current techniques focus on capturing synergies 
 positive complementarities  among items. in practice  local subadditivities can occur as well. for example  when bidding for a landing slot for a plane  the bidder is willing to take any one of a host of slots  but does not want more than one. to address this we developed a protocol where the bidders can submit xor-bids in our auction server  i.e. bids on combinations such that only one of the combinations can get accepted. this allows the bidders to express general preferences with both positive and negative complementarities  see also  rassenti et al.  1 . the winner determination algorithm of this 
paper can be easily generalized to xor-bids by marking  as in pre1  noncompetitive those pairs of bids that are mutually exclusive. these extra constraints cause the algorithm to run faster for xor-bids than for the same number of nonexclusive bids. 
　our server also allows there to be multiple units of each item. the winner determination algorithm then needs to keep track of the sum of the units consumed for each item separately on the main search path. for the multi-unit setting  the h-function can be improved to differentiate between the potential future contributions of units of different items. 
　currently we are developing winner determination algorithms for combinatorial double auctions which include multiple buyers and multiple sellers. 
