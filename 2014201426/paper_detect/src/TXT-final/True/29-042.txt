 
this paper proposes a new connectionist approach to numeric law discovery; i.e.  neural networks  law-candidates  are trained by using a newly invented second-order learning algorithm based on a quasi-newton method  called bpq  and the minimum description length criterion selects the most suitable from lawcandidates. the main advantage of our method over previous work of symbolic or connectionist approach is that it can efficiently discover numeric laws whose power values are not restricted to integers. experiments showed that the proposed method works well in discovering such laws even from data containing irrelevant variables or a small amount of noise. 
1 	introduction 
the discovery of a numeric law from a set of data is the central part of scientific discovery systems. such systems  for example  can detect a relationship between the distance r to the sun and the revolution period t of five planets known to kepler's third law t - kr1  k is a constant . 
after the pioneering work of the bacon systems 
 langley  1; langley et a/.  1   several methods  langley and zytkow  1; falkenhainer and michalski  1; langley and zytkow  1; schaffer  1; sutton and matheus  1  have been proposed. the basic search strategy employed by these methods is much the same: two variables are recursively combined into a new variable by using multiplication  division  or some predefined prototype functions. bacon and fahrenheit  langley and zytkow  1  use trend detectors to combine variables and employ a heuristic form of depth-first search. abacus  falkenhainer and michalski  1  creates a proportional graph and performs a modified beam search. in ids  langley and zytkow  
1   correlation analysis is applied  and a beam search 
1 	neural networks 
is performed. the e* algorithm  schaffer  1  considers only bivariate functions. also  sutton-matheus' algorithm  sutton and matheus  1  performs a regression  and the correlation between the squared error and the square of the variable's value  sanger  1  is used to combine variables. 
　these existing methods suffer from the following problems: first  because combining two variables into a new one must be done in order  a combinatorial explosion may occur when complex laws are sought for data consisting of a large number of variables  or the desired laws will be missed when some heuristic search parameters are inappropriate. second  when some powers appearing in a law are not restricted to integers  the law may remain unknown unless some appropriate prototype functions such as r1 are prepared in advance. however  a priori information is rarely available. third  these methods are often criticized for their lack of robustness; noise tolerance is definitely required since real observed data contain noise  langley and zytkow  1; schaffer  1 . 
　we believe a connectionist approach has great potential to solve the above problems. in order to directly learn a generalized polynomial term whose power values are not restricted to integers  a computational unit called a product unit has been proposed  durbin and rumelhart  1 ; instead of calculating a weighted sum of input values  this unit calculates a weighted product  where each input value is raised to a power determined by a variable weight. however  serious difficulties have been reported when using standard bp  rumel-
hart et a/.  1  to train networks containing these units  leerink et a/.  1 . although some heuristic strategies such as multiple learning algorithms have been proposed  leerink et a/.  1   their improvements over bp have been less than remarkable. moreover  these earlier studies dealt only with binary data and did not specifically address the problem of numeric law discovery. 
　in this paper  we propose a connectionist approach called rf1 for discovering numeric laws. section 1 ex-

plains how neural networks are used to discover a class of numeric laws. section 1 describes a second-order learning algorithm called bpq which trains the neural networks described in section 1. section 1 explains a criterion for selecting the most suitable candidate out of the trained ones. section 1 evaluates the proposed method rf1 by doing experiments using artificial data  real data  and time series data. 
	saito & nakano 	1 


	mdl 	= 	1m log mse  + 1nlog m    1  	or the total processing time exceeded 1 seconds. 
1 	neural networks 

table 1: learning statistics  artificial data  
hidden 
unit      mse value best 	avg. 	s.d. md
best l value avg. s.d. iteration avg. 	s.d.    time avg. 	s.d. original problem h = l h = 1 h = 1 1 
1 
1 1 
1 
1 1 
1 
1 -1 
-1 
-1 -1 
-1 -1 1 
1 
1 1 
1 1 
1 
1 1 
1 
1 1 
1 
1 modified problem h = 1 h = 1 h = 1 1 
1 
1 1 
1 
1 1 
1 
1 1 
-1 
-1 1 
-1 
-1 1 
1 
1 1 
1 
1 1 
1 
1 1 
1 
1 1 
1 
1 table 1: learning statistics  noisy artificial data  
hidden 
unit      mse value best 	avg. 	s.d. md
best l value avg. s.d.  iterati avg. on 
s.d.    tim avg. e 
s.d. original problem h = 1 h = 1 h = 1 1 
1 
1 1 
1 
1 1 
1 
1 -1 
-1 
-1 -1 
-1 
-1 1 
1 
1 1 
1 
1 1 
1 
1 1 
1 
1 1 
1 
1 modified problem h = 1 h = 1 h = 1 1 
1 
1 1 
1 
1 1 1 
1 1 
-1 
-1 1 
-1 -1 1 
1 
1 1 
1 1 
1 
1 1 
1 
1 1 
1 
1 
　in the experiments  we changed the number of hidden units from 1 to 1  h = 1 1  and performed 1 trials for each of them. table 1 shows the basic statistics of mse values  mdl values  iterations  and processing times  sec. .1 this table shows that the correct number of hidden units  h = 1  was successfully found because the best mdl value was minimized. when h = 1  all 1 trials converged to the global minimum for the original problem; however  a few of the 1 trials converged to undesirable local minima for the modified problem. the original and modified laws discovered by rf1 were 

where the weight values were rounded off to the second decimal place. namely  the laws discovered by rf1 were almost perfect to eq.  1  or  1 . note that without preparing some appropriate prototype functions  existing numeric discovery methods cannot find such laws as described in eq.  1 . this point is an important advantage of rf1 over existing methods. 
　to evaluate rf1's noise tolerance  we corrupted each value of y calculated from eq.  1  or  1  by adding noise generated according to a normal distribution with a mean of 1 and a standard deviation of 1. the other experimental conditions were exactly the same as before. table 1 shows the results. the best mse values were minimized when h = 1  while the best mdl values 
1
our experiments were done on hp/1 computer. 

although some weight values were slightly different  laws almost equivalent to the true ones were found. this shows that rf1 is robust and noise tolerant to some degree. 
1 	real data 
for real data  we used three data sets supporting hagenrubens' law  kepler's third law  and boyle's law.1 in this experiment  since the number of examples for each data set was small  the number of hidden units was fixed at 1. note that since we consider a constant term c1  the problem cannot be reduced to a simple regression 
1
　　we obtained the sets of kepler's data and boyle's data from the uci repository of machine learning databases. 
	saito & nakano 	1 


1 	neural networks 


figure 1: prediction by discovered law 
