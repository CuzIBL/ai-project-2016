 
in order to rank the performance of machine learning algorithms  many researchers conduct experiments on benchmark data sets. since most learning algorithms have domain-specific parameters  it is a popular custom to adapt these parameters to obtain a minimal error rate on the test set. the same rate is then used to rank the algorithm  which causes an optimistic bias. we quantify this bias  showing  in particular  that an algorithm with more parameters will probably be ranked higher than an equally good algorithm with fewer parameters. we demonstrate this result  showing the number of parameters and trials required in order to pretend to outperform c1 or foil  respectively  for various benchmark problems. we then describe out how unbiased ranking experiments should be conducted. 
1 	introduction 
estimating the accuracy of a classifier is a topic that has experienced much attention in the ml community. one of the main results is that tv-fold cross validation provides a bias-free  sto1  though not variance-free  zha1; koh1   estimate of the true accuracy  n-fold cross validation means that n classifiers are learned from   n - l /n ths of the available data  and tested on the remaining  l/n th of the training set. the averaged accuracies are a bias-free estimate of the accuracy of a classifier that is learned by the same algorithm on the complete data set. if the data set is too large  the accuracy is usually estimated on a test set that was not used for learning  one-shot training and test   which causes a slight pessimistic bias. for model selection purposes  a .1 bootstrap  efr1  may be preferable. bootstrap experiments are conducted by re-sampling a number of training sets of size n from an original data set of size n by randomly drawing samples with replacement. on the 
1 	learning 
average  .1m distinct samples will appear in the training set  and the averaged accuracies on the remaining test sets provide an optimistically biased estimate. the variance is claimed to be lower in many cases than the variance of cross validation  efr1   which is important when choosing an optimal model. 
　many papers propose new or modified machine learning algorithms  with claims such as  my new algorithm b is way better than algorithm a   or  extension x improves algorithm a a lot  typically supported by ranking experiments on a well known set of benchmark problems. there is also a book  mst1   resulting from the european statlog project  that is dedicated to the comparison of learning algorithms for benchmark problems. 
　virtually any learning algorithm possesses a number of parameters  e.g.  learning rates  number of learning steps  pruning thresholds  etc. . selecting values for these parameters is the model selection task  which has to be considered a part of the training process. unfortunately  it has become custom to adjust these parameters such that the error on either the test set  or  in case of n-fold cross validation  the averaged error on the n test sets  is minimized. since the error on the test set is used as the quality criterion for the model selection task  the test set influences the training process. hence  the assumption that the test sets are not used for learning  which is essential to the result that 1-fold cross validation is bias-free  is violated. 
　many authors are aware of that problem and properly separate model selection from accuracy estimation  e.g.   kj1   but a majority of authors seem to consider the resulting distortion of the results negligible. one of many examples is the statlog project  mst1   where it has not been taken into account by all contributing partners. a slight bias would not be dramatic  if all learning algorithms would be effected equally  such that ranking results would still remain valid. but we argue that the parameters form a communication channel from the test set to the learning algorithm  see figure 1   and that the resulting bias depends on the capacity of this chan-


figure 1: if model selection and accuracy estimation are mixed  the parameters for a communication channel  delivering information about the test set to the learning algorithm 
nel  i.e.  the number of parameters  and the number of different parameter settings which are tested. the main reason why many authors conduct their experiments in the naive way is that unbiased experiments imply a lot of additional computational effort  especially when the parameters are manually adjusted. our main contribution is to quantify this parameter-bias  identifying some cases where it actually is negligible and showing others where it makes ranking results invalid. 
　in the following sections  we consider different experimental settings: section 1 quantifies the bias of one-shot training and test  section 1 quantifies the bias of n-fold cross validation when different parameters are used for each of the n trials  and section 1 is dedicated to cross validation with equal parameter settings for each of the n trials. section 1 finally recalls the proper way of conducting ranking experiments. 
1 	one-shot training and test 
in this section  we assume the following setting. a learning algorithm l accepts a set of parameters and there are 1p distinct parameter settings  i.e.  the algorithm possesses logb 1p  parameters with b possible values each . this set of parameters can be viewed as a communication channel from the parameter optimizer to the learner with a capacity of p bits. 
1 	communication 
as figure 1 illustrates  when the learner is presented a set of parameters and a training set  it generates a hypothesis h which is used to determine the accuracy of h on the test set of size m. the parameter optimizer is told this accuracy and responds with a new set of parameters  which are again used for training. this cycle is repeated t times  and the best observed accuracy on the test set is then submitted for publication. based on the accuracy measured on the test set  the parameter optimizer can send p bits of information on the test set to the learning algorithm. 
　if h is the entropy of the test set then the capacity of the parameter channel allows to transmit information about the class labels of  test samples  because h bits are required to encode the class label of one sample   e.g.  if the test set contains 1 uniformly-distributed class labels  the entropy will be 1  allowing a parameter channel of width 1  1 distinct settings  to transmit the class labels of 1 = 1 test objects. this leaves two questions to be answered: how does the parameter optimizer get to know the class labels of the first c test instances  and how much will this knowledge improve the result  
1 	parameter adjustment 
the parameter optimizer guesses parameters and obtains the accuracy on the test set in return. we assume that the learning algorithm passes these parameters to the hypothesis  which uses the information to classify the first c test objects that it encounters in some particular way  i.e. the parameters encode class labels for the first c test objects. then the parameter optimizer can use the following strategy: 
1. for all objects i  1 through c 
 a  for all labels /  1 through  classes  
- tell the learner to classify the ith object as class / and determine the accuracy acci 
 b  keep the label of i to the class value i* that resulted in the highest accuracy accu 
　this algorithm tries the assignment of every possible class label to each of c test samples  resulting in a complexity of c    classes  learning trials  and finds the parameter setting that encodes correct class labels for c 
　samples. we can view this algorithm as greedy search for optimal learning parameters  but since the correct class label of sample i is independent of the assigned class label of any other sample  the greedy algorithm will find an optimal assignment after c    classes  trials.1 
1 	increase in observed accuracy 
now  after being told the class labels of the first c test objects  how much can we hope to improve the accuracy on the test set  assume that we have an initial 
1
　　note that this is not a lower bound for the number of trials  as we have found an algorithm that needs only  trials in the worst case  but while the 
algorithm given above essentially performs a gradient search in parameter space  the faster algorithm behaves unlike we would expect a parameter optimizer to behave  so the first result should be somewhat closer to the behavior of a  real  learning algorithm. 
	scheffer & herbrich 	1 

classifier h  learned by c1  qui1  say  which classifies ph objects of the test set  of size m  correctly. for the first c test instances we repeat the class labels that we were told by the parameter optimizer  rather than using h  we classify the remaining m - c instances using h. hence the number of hits is  where x is the number of hits that h would have obtained on the first c test instances  which are now classified correctly  . we now determine the probability that this procedure increases the number of hits by z. drawing c samples from a total of m  we know that ph of the m are  hits   classified correctly by h . hence  the number of hits within the c drawn samples follows a hyper-geometric distribution  note that m is finite . we then replace the c drawn samples with c  hits   because the parameter optimizer  tells  us their class labels via the parameter channel . 
c 
 1  
 1  
 1  
 1  
this leads us to the bias and computational effort  explained in section 1   required to achieve this bias for one-shot training and test  t is the expected number of learning experiments that need to be conducted in order to obtain ph + z hits on the m test samples with probability  when ph is the true hit rate of h  provided a parameter channel with p bits of capacity is used. 
 1  
 1  
1 	affected benchmark problems 
in this section  as well as in sections 1 and 1  we will quantify the bias on concrete data sets. we assume that we use a real learning algorithm  c1 in most cases  which we  tune  with additional parameters in order to pretend to outperform the learning algorithm one rank higher than the initial learner. we will answer two questions: how many parameters do we need to succeed with probability   1% after performing sufficiently many trials  note that this is an exact rather than an empirical result  and how many trials do we need in the average 
1 	learning 
to obtain this result  provided the parameter optimizer performs gradient search in parameter space  which may be an inexact result since it depends on the actual optimizer as well as how strong the parameters influence the result. 
　land-sat satellite images: this data set contains 1 training and 1 test instances. the default error rate is .1. based on  mst1   c1 is ranked 1th 
 error .1  i.e.  1 hits on the test set . to be ranked 1th it would have to outperform bay-tree  error .1  for which c1 needs only z = 1 extra hits on the test set. if we get class labels of c = 1 instances  then 
 hence we need t = 1   1 = 1 
trials with different parameter settings and since h = 1 we need a parameter channel of 1 bits. although an automatic parameter adjustment system may very well run 1 trials  a parameter channel of 1 bits is fairly uncommon  e.g.  achieved by 1 parameters with 1 possible values each . using c = 1 samples  parameter channel of 1 bits  and 1 experiments  we still have a 1% chance of being over-ranked. 
　d n a : this data set  also described in  mst1   possesses 1 training and 1 test instances. c1 is ranked 1th  1 hits . to be ranked 1th it would have to outperform indcart  requiring z = 1 extra hits. we need to be told c = 1 class labels to achieve this with 
1%:  since we have 1 classes  we need t = 1   1 = 1 trials. since h = 1  we need 1 bit of parameters  e.g.  1 parameters with 1 possible values each. 
　for both data sets we conclude that only a very eager scientist may obtain a modest over-ranking of his algorithm by using an incremental algorithm and an automatic parameter-adjustment procedure. 
1 n-fold cross validation with parameter adjustment 
in this section we study the bias caused by parameter adaptation when n-fold cross validation is conducted with parameter values chosen differently for the n runs of the learning algorithm. as an example  the number of learning steps is crucial to the performance of back propagation. often  the optimal number of learning steps is determined by observing the error on the test set and selecting the point at which the error rate starts increasing again. the minimum errors that occured in the n learning curves are then averaged and published  i.e. the number of learning steps may not be fixed to one value within the n folds. also  this setting is often used when the splits of the training set are explicitly stated  e.g.  mesh or vehicle silhouettes . 
　to achieve an average of z extra hits per fold this way  n   z has to equal at least   where x  is the number of hits lost by not using h on c samples of fold 
the total number of nz additional hits on n folds is the 1 	affected benchmark problems 
f e m mesh design:  dm1   a relational problem popular in inductive logic programming. it is explicitly split into five learning problems. there are 1 samples and 1 classes and he entropy is h = 1. foil  qui1  achieves an accuracy of 1%  1 hits . to achieve 1% accuracy with a probability of 1%  foil needs c = 1 class labels  while to achieve 1% with a probability of 1% foil would need c = 1 class labels. to achieve difference between the number of extra hits as explained in the last paragraph and the number of lost hits by not using the hypothesis h calculated in the last section: 
		 1  
　　1  without this assumption the probability would be hypergeometrically distributed  but the number of parameters needed to determine the class labels of a number of samples that gets close to the size of the test set would be outrageous. 
	scheffer & herbrich 	1 

　in this section we want to review how unbiased ranking experiments are supposed to be conducted. what has to be kept in mind is that model selection must not be confused with accuracy estimation. the impact of this well known result on accuracy estimation of parameterized learning algorithms has to be realized. as figure 1 illustrates  parameter adaptation needs to be performed without considering the test set. in order to get a reliable estimate of the optimal parameter settings  m-fold cross validation or .1 bootstrap should be conducted. the 

　note that in this situation the probability of being ranked too high depends on the hit rate of a default classifier and the hit rate of the initial hypothesis: if the default hit rate is high and the initial classifier performs poorly  the probability of being over-ranked is high. 
1 	affected b e n c h m a r k p r o b l e m s 
diabetes: in this setting  we need c = 1 parameters to achieve .1 extra hits per fold with a probability of 1%. increasing the number of parameters further decreases the probability  since the expected number of samples with equal class labels at some position in n folds  divided by the number of folds  is small compared to the hit rate of the initial hypothesis. in this experimental setting  diabetes is  safe . 
heart disease: in this data set  mst1  there are 
1 samples  1 classes  h = .1. since c1 performs poorly for this problem we only need c = 1 extra hits  a parameter channel of 1 bits and 1 trials are sufficient 
1 	learning 
best parameter setting is then used to learn a classifier on the whole training set  and the hypothesis is assessed on the test set. in order to get an unbiased estimate  n-fold cross validation has to be conducted making n   m learning trials in total. averaged over sufficiently many learning problems  this procedure clearly grants an unbiased estimate of the average performance of a learning algorithm  but it may be painful  since n   m trials need to be conducted for each problem. 
1 	discussion 
our calculation clearly shows that adapting the parameters such that the accuracy on the test set is optimized causes an optimistic bias that depends on the number of parameters and the number of trials. we quantified the bias that will be observed when sufficiently many trials are conducted and the learner makes optimal use of the available parameters  and we calculated the expected number of trials needed  assuming the parameter 

optimizer follows a gradient descent-like search. 
　it shows that it is hard to be over-ranked in the oneshot training and test situation if the test set is large. in the n-fold cross validation situation  the probability of being over-ranked is high if the difference between default hit rate and true accuracy is low. if in the n-fold cross validation setting different parameter values are used  i.e. the parameters are optimized locally  a highly over-ranked result can be achieved  so experiments conducted this way do not yield valid results. 
　our considerations do not prove that any learning algorithm actually is much worse than claimed  but they do show that such claims are not validly supported by experiments in the naive setting. our results are constructive in some sense: using the provided equations it can easily be proven that in some situations - depending on the properties of the data set - the naive but inexpensive experimental setting yields perfectly valid results. 
   based on our calculations  the validation of performance evaluations for new learning algorithms seems to be difficult: many authors do not document their experimental settings to a sufficient degree  and empirical results that are based on the naive setting are likely to be distorted and cannot be compared to those obtained with unbiased experiments. heuristic modifications that add new parameters may easily be over-estimated. even if the modification does not improve the true accuracy of the hypothesis  a new parameter may improve the ranking results. 
　an important question is which additional assumptions were made in our calculation. to summarize them: we assumed that the test sets are in random order  in contrast to ordered by their class labels   but that the order is fixed  as experiments with different parameter settings are conducted. we also assumed that the parameter optimizer conducts a gradient descent for optimal parameters and showed that it will find optimal parameters if the parameters encode class labels for the first c test samples. hill climbing algorithms for parameter adaptation are fairly common. also  different parameter settings will result in different classification of some of the test samples  although not every distinct parameter setting may yield a different classifier. however  the correlation between parameters and class labels of test set samples is usually less direct than assumed  hence the number of experiments required to gain z extra hits may be higher  and-depending on the learner-the bias may be smaller in real world situations than in our calculation. 
　parameters of learning algorithms are undesirable since the more parameters an algorithm has the less robust the algorithm is  and the harder it is to obtain a satisfactory result for a new problem. buchanan  paraphrased by  cat1l   called this the china syndrome: 
some learning algorithms have so many parameters that the only person who can make the program run is with high probability currently in china. here we have illustrated the following extension of this syndrome  which might be considered murphy's law on machine learning algorithms: the algorithm that is ranked most highly  and is therefore considered most suitable for a given problem  is the one that is least likely to run and grant a satisfactory result because it has so many parameters that the only person who claims to be able to make it run is  with high probability  currently unavailable. 
a c k n o w l e d g m e n t 
this work was supported by a grant of the dfg. we wish to thank prof. alberto segre who did a great job of helping us to edit this paper  and prof. wysotzki for the interesting discussion. 
