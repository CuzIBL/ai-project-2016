 
       most interesting machine tasks  e.g.  visual pattern recognition  require large complex systems. conventional machine designs are not well suited to the reliability demands of large complex systems. many designs  conceived and tested on a small scale  and theoretically extendable to large scale systems  are still awkward and impractical as a large scale system because reliability appears to vary inversely with the number of components. animal systems seem to have solved this problem and offer some hope for understanding the problem of how to build large complex systems with high reliablity. 
       in particular animal systems seem to tolerate a great deal of component variation and noise. it is suggested that animal systems represent designs that actually take advantage of noise  unlike conventional machines  and that some of these animal systems may provide techniques which are applicable to the problem of constructing reliable machines from unreliable components. 
design of an eye 
1. introduction 
　　　at present there is an effort underway to study the problems of building an eye. the long 
term goal is to build an animal. some of the details of this effort have been described elsewhere l ; the purpose of this paper is to explain why this effort is considered worthwhile and to focus attention on some of the more interesting philosophical issues. 
       recent work in frog's vision is used as a basis for forming general principles of organization that may be applicable to broad areas of the nervous system 1 . this is not an attempt to develop a full-blown theory of the vertebrate nervous system. it is rather an attempt to capture something of the style of the animal system in a way that may prove helpful in solving engineering problems. much of that  style1 is presented in a series of observations about variability in nature and the relation between structure and function in animal systems. 
       certainly one of the more interesting and intriguing aspects of anything in nature is the overwhelming variety. it is often said of things in nature that no two individuals are ever exactly alike; no two snowflakes are ever identical  and no two oak trees ever have quite the same configuration of branches. as c. s. peirce put it   the endless variety in the world has not been created by law. it is not the nature of uniformity to originate variation  nor of law to beget circumstance. when we gaze upon the multifariousness of nature we are looking straight into the face of living spontaneity  1 . we find the processes of nature inexplicable and sometimes aweinspiring partly because this wealth of individual variation does not fit into the familiar context of clockwork-like mechanisms which have become the paradigm for explanations. it is not immediately obvious how the outward behavior of two natural mechanisms can be so much alike when their individual components are allowed to vary over such a wide range. it is hoped that this study will help provide a context in which we may begin to find answers to some of these problems. the central problem here is to understand and explain processes of nature in terms of mechanisms whose composition is perhaps not capable of exact duplication even in principle. 
1. why imitate animal systems  
       in many areas it is becoming increasingly clear that we must somehow learn how to construct systems in which decisions about future events need not involve a total committment before all the facts are in. we certainly cannot send to mars  for example  a conventionally designed vehicle with all responses preprogrammed and all its hardware committed to specifically preconceived tasks. we simply do not know enough about the environment to know what responses would be appropriate in all cases. the same may be said of many terrestial problems as well. even an office manager must often order new equipment a year or two in advance without being very clear in detail about what he will be doing in one or two years. being forced to make a decision prematurely can have disasterous consequences. in many areas then we see that we must somehow master the technique of building systems with uncommitted or partially committted components  which may later be refined or modified as more data become available. these problems of reliability and selfmaintainance  of learning  and the constraints of real-time all represent exactly the kind of problems the animal nervous system must face on a moment by moment basis. 
　　　in the past engineers have faced these problems separately. for example  one thinks of a game like chess as the model of a learning situation  in which the principles of selforganization can be studied in isolation. here one 
can almost ignore real-time constraints; and one 
-1-

neednot bother at all with reliability problems. in this way the logical designer can devote his full resources to advancing the art of selforganizing systems  and let hardware people concern themselves with building fast  reliable components. however  when it comes to the practical task of building an integrated system satisfying a number of different requirements  our technology begins to fail us. the integration of different functions  originally treated separately  may introduce interface problems that overburden the system and detract from its effectiveness. a systems designer who has faced this problem of ever growing complexity in conventional hardware systems can appreciate nature's methods of integration in animal systems. this concern with the eye then has this ulterior purpose: it is hoped that a mastery of the principles of an animal visual system will help shed light on  and promote an understanding of  the functional organization of large complex control systems. 
1. the reliability problem 
       there are at least three distinct senses in which 	components 	may 	be 	described 	as 
 unreliable : 1. each component may be unstable or unpredictable and vary over time; 1. individual components may be stable  but the manufacturing process may introduce differences among these individuals; 1. all components may be identical and uniformly bad. for the purposes of this study we can ignore this last sense. although it is possible for components to be unreliable in this third sense  it is unlikely that we would find them easy to come by. this kind of unreliability assumes a kind of perverted mastery of quality control  and it is just that quality control that we find most difficult to achieve. 
       a realistic solution to the general problem of reliability must solve both the problem of the unstable component and the problem of variation in several components designed for the same task. the past attempts to solve the general problem may have been hindered by a failure to keep both of these aspects of the problem in mind. most of the conventional reliability measures  like the use of hamming codes and redundancy bits  are directed at the so-called problem of the  noisy channel . this is just one aspect of the more general problem of the noisy component. our experience in this area has already made it clear that even if one solves the component problem and succeeds in making fairly reliable components  one is still faced with the problem of the overall reliability of the larger system. all other things being equal  the larger the system  the less reliable it is. the conventional strategy here is to concentrate first on the problem of component reliability and when we run out of ideas  we turn in desperation to ad hoc systems reliability measures which for the most part are awkward and are hard to integrate into the system to which we have already painfully committed ourselves. we have known for some time that this strategy is only marginally successful  but we have not abandoned it because there has been no clear alternative. 
　　　in view of this one is tempted to say that the problem should be approached from the other end. unfortunately  that is easier said than done. it is not immediately obvious what  or where  the 
 other end  is. in a general way  of course  the suggested strategy here is to solve the systemsreliability problem first  and then design the individual components that turn out to be necessary for the job. perhaps for large systems at least the hardware and the system design have to go hand in hand. to put it more generally; the reliability requirements of large systems forces 
an integration of structure and function. 
       ordinarily it has not been easy  nor even desirable  to design a machine in which the structure and function are intimately connected. often for economic reasons  designs have been encouraged which could be implemented without a committment to a particular kind of hardware; in this way the manufacturer could take advantage of hardware improvements without having to modify radically the logical design. this is a reasonable strategy as long as the systems are small and simple. however  it is becoming increasingly clear that for large systems the separation of the hardware reliability problem from the rest of the system pays off in diminishing returns with the increase in system size. in the current literature there is a growing number of jeremiahs forecasting doom if we do not repent our practice of disintegrated design. for example. steel and kircher write in the crisis we face. 1  
to sum up the crisis in automation  we are pursuing a course that leads to a severe overcomplexity. the nature of this complexity arises not from the basic requirements of automatic control  but entirely from our disintegrated approach to invention  development  and production of military and commercial automatic control devices and business computers. 
　　　the suspicion that animal systems and perhaps even human social structures might provide an insight into the design of reliable systems with unreliable components has prompted control system designers to take interest in bionics and cybernetics. this should give rise to a reformulation of the reliability problem in more practical terms. instead of asking how to keep a machine error-free  one asks the weaker question: how to design machines in which errors simply go hand in hand with hardware failures; that is  how to prevent structural losses bringing about a disproportionate loss of function. 
       the work of lettvin  maturana  mcculloch  and pitts is enormously important in this regard  because it is one of the first serious attempts to explain the function of the frog's retina in terms of the shape and structure of retinal ganglion cells. if it can be shown that structure and function are intimately related  that a specific function depends on a specific shape  then one can begin to see in 
-1-

a general way at least how it might be that a small deterioration in structure brings about only a proportionally small deterioration of function. 
1. the problem of design 
       in all vertebrates the primary processing of visual information takes place in the three cellular layers of the retina: i.e.  the photoreceptors  the bipolar cells  and the ganglion cells. the optical stalk consists of axons  or output fibers  from the ganglionic layer. the problem of the design of the eye reduces then largely to the problem of assigning a plausible function to each of these three layers of the retina. the key word here is 'plausible1. the design not only must work  it must fit into the context of the nervous system. this leads one initially into a consideration of the kinds of constraints within which a nervous system must operate and the theoretical basis for supposing that some designs are better than others. 
       the development of the theoretical context in which one may explain and interpret the vertebrate eye as a mechanism will not be a simple achievement; certainly it will not be a simple extension of existing theory. even a casual look at the anatomy of the vertebrate eye reveals features that cannot be explained in conventional terms. and a closer look gives rise to the suspicion that virtually everything runs contrary to what is now accepted as good engineering practice. the striking thing here is that the layer of photoreceptors  the transducers in the system  is farthest from the light source. the image is erected on the back side of the retina  after having been filtered through literally a maze of blood vessels and cell bodies. there is no evidence that the light interacts chemically with anything in the bipolar or ganglion layers  before it strikes the photoreceptor layer. one can almost imagine a malevolent deity who turned things around and put these layers in front of the photoreceptors just to deteriorate the image  scatter the light  and confound our attempt to understand how it works. even the lens system is substandard by conventional standards; in the words of helmholz   the monochromatic aberrations in the optical system of the eye are not  like the spherical aberration of glass lenses  symmetrical about an axis. they are much more unsymmetrical and of a kind that is not permissible in well constructed optical instruments  1 . moreover  one glance at the ways neurons are connected to one another literally a jungle of interconnections - firmly impresses one with the impossibility of classical circuit analysis. 
1. design strategy 
       one approach to the problem of designing a reliable machine is to face the main issue directly. noisy components and imperfect quality control are the facts of life. if natural mechanisms take advantage of everything at hand  as they seem to  perhaps they even take advantage of noise. accordingly it seems to make sense to consider a machine which works well only when the components are in some sense faulty  or stamped out of an imperfect and flexible mold. the philosopher  feeling the frustration of dealing with the practical man  rationalizes:  it takes all kinds to make a world . here we are talking about an analogous situation in which it may be said that  it takes all kinds to make a reliable machine . the importance of this passing reference to social models can be appreciated more fully when we begin to look at some possible designs. in our development of these designs we have taken the social model seriously  and have more or less consciously tried to picture a mechanism in terms of  societies  of individual computers  voting mechanisms  societies of peers  the judgment of experts  and so on. social models are instructive because they seem to be examples of mechanisms which somehow rely on individual variation  and which have the same sort of loose coupling between individuals and the same kind of many-to-many connections that one finds in the nervous system. 
　　　this approach to machine reliability is of course in marked contrast to the conventional approaches. in fact  it may even be said that we are introducing a new concept of machine'. the traditional concept of a machine is linked with the notion of a mechanism with precisely describable components; all the gear teeth have to be precisely matched to be able to mesh. there is no room for individuality of parts. the traditional approach to machine manufacture is modeled after the 
!
 clockmaker'. the clockmaker-engineer first lays out his plan on paper; when he makes a part  he knows exactly what he wants. if he fails to  he throws it away  tries again until he succeeds in making something that satisfies the plan exactly. nature on the other hand rarely rejects any system component  even though many of the components appear to be  stamped out from a very inexact and flexible mold . it is as though nature first made the components and then later looked around to see how to use them. if one accepts the challenge of nature and attempts to design and build a reliable machine from unreliable components  one is doing violence to the traditional concept of a machine  especially if factors like noise and unreliability become explicit components of design and appear  as it were  on the blueprints. 
       this study is certainly not the first to violate the traditional concept of a machine  although not as many have covered this ground as one might think. nearly every philosopher since descartes has considered the possibility of mechanizing thought processes and building automata  but this typically has amounted to a reduction of thought processes to machine processes. in other words  this has been a redefinition of 'thought'  rather than a redefinition of 'machine1. the first time the traditional concept of a machine was violated was when someone thought it would be nice if machines could detect and correct their own errors. the use of hamming codes in machine design has made it possible for components to be  inexact  and noisy. mcculloch and pitts made a real advance when they described a network of threshold elements which compute the same function under 
-1-
different thresholds. reliability techniques like these help modify our concept of a machine because they make it possible to describe a mechanism without specifying the components in exact detail. the exact nature of the m i c r o - s t r u c t u r e becomes less important if it can be shown to have little effect on the m a c r o - s t r u c t u r e . and this in turn points the way to designs in which the quality control of individual components may be relaxed without compromising the overall performance. 
       in general  the mcculloch- pitts nets represent an important step forward because they show that machines are possible in which  in some aspects at least  the function depends upon ordering relations without depending upon a particular m e t r i c . that such designs are possible should not be too surprising; one should be able to see this much simply by gazing  with peirce  at  the multifariousness of nature . if there can be so many detail differences among individuals of a given species  then these differences  and the m e t r i c s associated with them  can have little to do with the basic mechanism. mechanisms which somehow depend on orderings without m e t r i c s are not only easier to build  in the sense that the component quality control may be relaxed  but also such mechanisms may be m o r e reliable  in the sense that they are less disturbed by noise  especially if it can be shown that the noise in the system affects only the metrics and not the orderings. 
       another important milestone in the breakdown of the traditional concept of a machine is the 1-called  perceptron  1 . in its simplest f o r m a perceptron consists of a number of random threshold elements tied together in a random network. inputs and outputs are connected to randomly distributed junctions in the network. the process of  learning  consists in representing patterns of inputs and 'rewarding  the network for issuing the desired output. the   r e w a r d   results in some f a i r l y simple internal modifications of thresholds. f o r example  the threshold of all elements that fired in the case to be rewarded s lowered  or perhaps the threshold of all those that did not f i r e is increased. although early expectations that the perceptron could be a practical device have not been realized  it is nevertheless an important theoretical contribution. since a perceptron is an almost structureless machine  it can be viewed as an answer to the question: what is the most function realizable f r o m the least structure  although the perceptron is far f r o m a practical device  the fact that it can do anything at all is simply astounding f r o m the point of view of traditional machines. it is important then because it gives one the confidence to face the otherwise unsettling question of the traditionalist: how is function possible at a l l in the absence of structure. 
       by exploring the middle ground between highly structured conventional machines and almost structureless perceptrons  we may learn how to take advantage of both. we may learn how to avoid highly structured machines whose complexity produces an undesirable sensitivity to noise and component failure. 
1. the concept of l a y e r e d computation 
       one of the f i r s t things that strikes the student of biology is the layered structure of animal tissue; this is especially striking in the  higher   m o r e organized species. seen through a microscope  almost any tissue f r o m these higher f o r m s is easily resolved into cell layers. this layered structure is apparent even in the nervous system. the one exception is the reticular formation  which is only one layer deep. here the p r i m a r y flow of information is in the horizontal dimension. 
       the picture that emerges is one of many units within a layer  all operating in parallel. the units  or neurons  accept inputs f r o m corresponding units in the previous layer  and issue outputs to the corresponding units in the subsequent layer. 
       one of the features of this picture is the wealth of possible feedback. while most of the cells in a given layer are designed to pass information in only one direction  a few cells are able to send information back to the previous layers  f r o m which the layer in question gets its inputs. in this scheme each layer could do a small amount of computation  then pass the results on to the next layer and at the same time issue a few feedback control commands to the previous layer to modify thresholds and generally tailor the computation to the demands of the input. in fact  it may just be this feedback potentiality which is the point of the layered structure of the nervous system. this may be the answer to the question: how does the animal control system act in realt i m e with such slow components. 
       talking about the nervous system in t e r m s of layers and computations with possible feedback is a way of emphasizing its role as a control computer and guidance system. in this sense it is to be contrasted with the conventional dataprocessing computer which is typically a problem solver without r e a l - t i m e constraints. it answers questions like how much is 1 plus 1  or how many biscuits do we have in the warehouse  or what are the odds maine w i l l vote democratic. the converiiiu lal computer solves peoblem - which can be structured in this simple question-andanswer fashion. faced with the decision  the designer of conventional data-processing computers w i l l always sacrifice speed for accuracy  and this is why conventional computers make poor control computers. 
       it is possible to approach the problem of object recognition using the method of conventional data processing; in fact  most of the current object-recognition schemes do just that. in this method  for example  we might scan an area for an  object   i.e.  closed edge  then  having discovered one  inquire after its properties and look for a match in a list of properties to see if this object is of interest. the difficulty with this method is that in any r e a l - w o r l d application  edges are r a r e l y closed and the object w i l l move around; by the t i m e one has recognized an object as an object  it may have moved. in fact  in animals it is just this motion that makes an object attractive. this suggests that an object-recognition device 

-1-

can only recognize moving objects if it can somehow servo its reference axes with the moving object; this is probably part of the mechanism of paying attention. the aspect of motion here introduces real-time constraints which the conventional object-recognition schemes are i l l suited to handle. 
       the need for rapid feedback capabilities arises in almost any control system requiring responses in real-time. a control system must be able to function within a  tight  feedback loop to achieve accuracy and fine grain control. conventional data-processing computers achieve fast computation times in a variety of ways; most of these involve large immobile pieces of hardware with large power requirements. control computers  on the other hand  are often intended for applications where size and mobility are crucial  and so are required to find answers with simpler hardware. here we find specialization  in which the hardware design and the logical design are closely intertwined with a specific application. word lengths are kept short to reduce carry propagation times in arithmetic units. in general  the techniques rely on relatively shallow computation.  the depth of computation here can be measured roughly by the number of significant parentheses in the expression of the function to be computed.  deep computation which may give more accurate results takes more time and slows down response. 
       it is not unreasonable to suppose that the presence of shallow layers found in the animal nervous system reflects a committment to shallow computation. by reducing a fairly shallow complicated or deep computation to a number of successive shallow computations  each one of 
which may yield information for feedback  as well as information to be passed on the next  layer  of computation  one begins to see how the layered structure of the nervous system might explain how fantastic response times can be achieved with relatively slow components. 
       there is some temptation to explain the superiority of the animal's response over conventional computer systems in terms of the number of components available for the task. one is tempted to say that the difference here is simply the difference between serial and parallel operation. here one might say that conventional systems must operate within the constraints of serial processing  and that is why they are slower. but this is surely a misleading description. it is not in general obvious that all problems  which are now solved in a serial fashion  could be solved faster in a parallel fashion.  it would be something like expecting two ships to cross the atlantic faster than one.  for one thing  any problem which can be solved in a parallel way must be representable as a function whose terms are commutative; that is  the order in which the terms are computed is not important. for example  one may indeed hasten the process of adding a column of numbers by separating that column into two smaller columns; this makes it possible to add the two columns in parallel  and then finally add the two sub-totals to get the final total. this technique is clearly not possible for all functions. in many computations some terms have to be computed before others. for example  it is well known that in computations involving both multiplication and addition   e.g.  ab＼c  the order of computation is important. however  the problem appears in many other areas as well. many pattern recognition problems involve the recognition of entities which are highly context dependent  as for example in 
the translation of natural languages. here it is important to establish the context before deciding on the meaning of particular words ! in this sense some computations are essentially serial. in general there will always be some computations which cannot be reduced to corresponding parallel computations. 
　　　the order in which the layers of the nervous system are arranged is undoubtedly related to the essentially serial aspects of the computations that the nervous sytems is designed to perform. part of our task is to explore this fast-feedback aspect of layered computation as a possible method of achieving fine control. 
　　　if it is true that the layered structure of the nervous system has something to do with the distribution of feedback  then one might expect that the kinds of computation performed in a given layer would all be similar  so that information which was fed back to a given layer would be appropriate to anything that might be going on in that layer. it would also satisfy our sense of economy if it turned out that neurons within a given layer were all simple variations on a single theme; it would explain how variations could be rich with a relatively simple genetic code. there is some reason to believe this may be the case. 
       lettvin's work on vision in frogs suggests that there are roughly five different functions performed by retinal ganglion cells  and that these correspond roughly to five different kinds of anatomically distinguishable cells. these five types  however  represent a convenient way of characterizing a population that has many intermediate types. it is this problem of the intermediate type that makes simple precise analytic models of each of the types of neurons an almost pointless endeavor. a good model of the frog's retina needs to show something like a family relationship between the various types and to exhibit the ways in which these intermediates are something like a variation on a simple theme. four of the functions performed in the frog!s retina are normally characterized as edge detection  moving convex edge detection  i.e.  bugs   event detection  and dimming; there is some question about the function performed by the fifth kind  because they are so rare that only a few have been studied. on the face of it  it may not seem as though edge detection and event detection are variations on a single theme  or even have anything at all in common. to understand the family relationship among these apparently diverse functions one needs to consider detectors of this sort from the stand point of the logical function they compute. from this point of view one sees that to detect an edge visually one needs to compute a difference in light intensity over some special area. in the simplest case  this is achieved with a boolean 'exclusive or' element with two 
-1-

 input sin different parts of the visual field. if one input is on and the other off  we know there is a visual edge or gradient between the two inputs. an event on the other hand involves a difference over time rather than space. in this case an exclusive-or element with two inputs coming from the same area of the visual field  but a delay introduced in one of the inputs  would compute change over time. this shows that edge detection and event detection are related in the sense that both involve a difference detector. now consider what would happen if we made a number of exclusive-or elements and allowed differences of delay in the two inputs. those elements with large delays in one input would respond to slow changes in the visual field; and those with short delays would detect relatively rapid changes. suppose further that we allowed the position of the two inputs to vary  so that some pairs of inputs came from widely different areas in the visual field and some came from relatively close areas. relatively sharp edges  steep gradients in light intensity  would be detected by elements whose inputs were close together  and less distinct edges would be detected by inputs that were farther apart. elements whose inputs were close together but with slightly different delays would detect either very sharp edges or slight movement. in this way  it becomes clear that one can represent all varieties of edge and motion detection in terms of two-dimensional abstract space whose coordinates range over the special relation between inputs and the difference in delays in inputs. 
       elsewhere l  the author has presented a detailed model of the f r og' s edge and bug dectector s which suggests that edge and moving convex edge detection are also variations on a simple theme  in which the size of the input area deterimines the sensitivity to convex edges. there it is suggested that a convex edge detector is one in which the two inputs to an exclusive or are concentric fields  one inside the other. as the size of the fields is diminished  the section of a given convex edge within the fields becomes less distinguishable from a straight line. the controlling assumption here is that the inferior lens that helmholz speaks of  and the fact that light has to penetrate two layers of neurons to reach the photoreceptors  transforms a stright line into a kind of uneven  or  wiggly  edge whose small convexities are detectable by a small convex edge detector; so that what we call an edge detector is actually a small convex edge detector. a larger cell has more room for delays between the concentric input areas  and so is more likely to detect motion as well  as in the moving bug detector. this is an example of the way in which structure and function may be intimately related in the animal nervous system. 
1. models of horizontal association 
　　　in considering designs which tolerate a wide variation of properties in the individual components  it is useful to consider mechanisms which seem to thrive on variation. human societies  as we have already suggested  are obvious examples of such mechanisms. however  we can find rudimentary examples of similar mechanisms in other areas. in high-fidelity sound reproduction  for example  it is well-known that a number of inexpensive and even poorly-made speakers  connected together in a series-parallel network  will often produce a satisfactory system. furthermore  it is often pointed out that the system is ever so much better when the speakers come from different manufacturers; the point here  of course  is that we do not want all the speakers to have the same peak in their response curves  say at 1 cps. it is obvious that the wider the range of individual differences  the better the the overall frequency response  although to be sure the transient response and efficiency may suffer without some further refinements. 
　　　a slightly more sophisticated use of randomness was suggested by albert novikoff 1 . using the techniques of integral geometry normally associated with the  fbuffon needle problem'   novikoff shows how patterns might be 
 recognized   that is transformed to a normal form  in a way that is independent of the effects of translation and rotation. specifically  each pattern is uniquely identified with a certain probability distribution. a pattern is projected onto a field of randomly distributed line segments  or  needles   of randomly varying lengths; for each pattern one may tabulate the points of intersection between the pattern and show how they are distributed among the various lengths of line segment; each unique pattern will have its own characteristic distribution  independent of translation and rotation within the field of line segments. what is particularly interesting about this scheme is that it may provide a way of making sense out of the apparent random jungle of neurons and dendrites that one finds  for example  in the visual cortex of higher vertebrates. this example  and the one above  at any rate show that the concept of a system which is in some way dependent on  or even thrives on  the random individuality of its components is not wholly unheard of. 
　　　however  there is no question that the social model provides the most fertile source for inspiration in this area. it is just that we mist be cautious about the way we use the social model. it can show things in a new light  but it can never act as an explanation of a mechanism  because we understand less about social mechanisms than the control systems we are trying to explain and build. this is so mainly because we lack a theory of large partially structured domains. 
-1-       in our limited and simple-minded experiments with social models  we have fallen onto two processes which we think are of fundamental significance for any device which uses voting mechanisms. these two processes are: 1. the formation of a peer group  and 1. the recognition of experts within that peer group. stated briefly  the function of the peer group is to link together similar units as part of a voting mechanism and to isolate extremely deviant units  and the function of an expert is to allow units of known reliability to settle differences of opinion or resolve close decisions during the voting process. this process can be compared to that of forming a crossover network in the seriesparallel network of speakers mentioned above to overcome the defects in transient response and inefficiency. 
　　　the peer group is the vehicle of the expert's influence. for example  expert lawyers influence only other lawyers  not plumbers or doctors. the expert plays a crucial role in any voting mechanism. he helps swing the bias the right way in close elections  because he can influence peers without being influenced by them. although we state these principles in distinctly social terms  our claim is that that we can describe a simple mechanism in which each of these social terms makes sense in a relevant and non-trivial way. 
　　　consider the device in figure 1. this device consists of four sub-systems: inputs  logic elements  outputs and association elements. the logic elements are active in the sense that they have gain and switching properties. the association elements are inactive in the sense that they are simply conductors; as conductors they have the properties of a resistor or a diode. 
　　　now let us consider a network of these units in which several different boolean functions are computed. some will have outputs only when both inputs are active; some will have outputs when either input is active. initially every element is connected to every other element through chain of association elements whose initial resistance is zero ohms. every output is a positive voltage. let the inputs overlap one another so that neighboring elements are presented with roughly the same inputs. if two neighbors fire together  no current flows along their common association element; however  if they do not fire together  a resulting difference in voltage allows a current flow across the association element. let us suppose that the association elements have this additional property: the resistance goes up a little each time a current flows through the associative element. now let us present a random pattern of inputs to our network. when two neighboring units fire differently. the connection between them will deteriorate slightly. only when both have a plus voltage together is there no current flow from one to the other. after a period of time  clearly most of the logic elements which compute a boolean sum will be tightly coupled 


together and loosely coupled  or not at all  to elements which compute other functions. 
similarly  in the case of those units that compute a product  only those that fire together will be linked together. this process we call the  formation of peergroup . if there are a few randomly scattered units that always issue an output  and some that never do  these will probably not be linked with anything. figure 1 represents the conditions before and after this process. 
　　　the formation of peergroups in the above sense is really only half the battle. the intercon-
nections between the various members of a peergroup constitute the channel of communication between the members. the whole point of that channel is that the more reliable components can be allowed to  communicate  with  and somehow offset the effects of  the less reliable components. we now have to say something about the way in which the more reliable components can be given additional weight. 
       it is natural to expect that one's quality control measures will always leave something to be desired. within each peergroup  some logic units will naturally be better than others. this raises two important problems: 1. what does it mean to be  better   or to be an  expert   and 1. how does the mechanism recognize one  the first is easier to answer than the second. in fact 
-1-
i do not think the second can be answered at all generally; there is an implied criterion for truth in any general answer  and there is just no adequate general criterion for truth. 
       an expert is one who influences his peers  but is not influenced by them. from the standpoint of our model  we call a unit an  expert1 if the horizontal association elements around it form some non-linear element  like a diode. in terms of our model  this is what it means to be an  expert . the problem of the criteria by which the mechanism is to decide which elements are to become surrounded by diodes is fairly complicated. the specific criteria for  expertise ' will probably vary radically with the specific task. in any case the specific criteria are less important  at this point at least  if we can say something about the manner in which they are applied; and this we can do in a general way. 
       the first thoughts on how  experts  might be recognized arose out of a consideration of the difficulties encountered in a series of attempts  by various members of the biology department at m.i.t.  to make reliable recordings of neurons firing in the retina of various vertebrates. much of this work is unpublished because the results were negative or inconclusive. it was found that it was virtually impossible to estimate the reliability or accuracy of the experiments because the results were not in general repeatable. it appears that much of the difficulty was due to feedback from other parts of the nervous system and to influences from other systems in the organism which at various times are more or less loosely coupled to the visual system. 
　　　　this sort of problem is hardly new or unexpected. for theneurophysiologist   feedback from other areas...   has become one of the facts of life  an occupational hazard to be endured without complaint. largely because of the work of hernandez-peon 1   it is now well known that there are control centers in the brain  i.e.  the reticular formation  in vertebrates which can regulate the output rates of afferent  or sensory  neurons. it is estimated that in the frog about 1% of the fibers in the optical stalk are channels for feedback to control the firing rates of ganglion cells in well. it is very likely that this feedback is used to control the amount of information delivered to the brain. it is not surprising that the brain cannot attend to or process all the inputs that are presented to it. at any one time many inputs represent merely background noise and can be ignored without any harm. it naturally occurred to us that the mechanism for recognizing background noise and eliminating it could also be used to recognize faulty neurons. furthermore  the mechanism whereby the decision is made to attend more closely to an object could also be used to recognize those neurons which seem more reliable indicators of objects worth attending to. the mechanism of attention-control appears to involve feedback commands which can select a small set of neurons whose output is to be enhanced or inhibited. it is as though a central control system could determine which neurons to amplify and which to turn off. if we knew how the animal system did this  we might have a scheme for recognizing 	the 	reliable 	and 	unreliable components. 
　　　one clue was suggested in the following statement  made by dr. mcculloch explaining some of the difficulties in interpreting the outputs of electrodes implanted in the visual system: 
　　　 .. . the mouse  which does not turn its eyes and keeps them open  is another nice animal to work on. his retina is the same all over  and whether you get a response from a particular ganglion cell or from a particular axon depends upon whether the mouse is hungry or whether it has smelled its cheese. if it has  then it bothers to look  but it will not look the rest of the time. the mouse shows very little response to any visual stimulus. the situation is far too complicated 
to be solved with a set of electrodes  1 . 
the striking thing here is that one sense modality  e.g.  olfaction  may regulate another  e.g.  vision . it suggests that neurons reporting the presence of an object have their output rates increased or decreased according to whether or not the object is reported by more than one sense. that is  neurons identified with objects that are both seen and heard  or seen and smelled  tend to be those that are in some more  real   and so there are the ones that have their output rates enhanced. 
       the requirement that two or more sense modalities agree may be related to the fact that noise on one sensory channel is not likely to have any interesting relation to noise on the other sensory channels. two sense modalities  considered as communication channels  are not likely to be susceptible to the same kind of noise; things which distort visual inputs probably do not distort auditory ones. 
       it seems then that in a certain sense we are linking our method of expert-recognition with a kind of  coherence theory of truth . when two different channels agree on the presence of an object  the gain on those neurons agreeing is turned up. if in the act of modifying the gain on those few neurons  a diode  or some similar non-linear device  is formed around those neurons  our mechanism is complete. this is a general account of the method we propose to pursue. it still leaves a lot unanswered. for example  the way in which it is decided whether two sense modalities  agree  is not at all a trivial matter. one can easily see why the criteria for the coherence of inputs has to be handled in terms of the specific inputs. it is one thing to see that the results of two different methods of computation agree  and quite another to see that two different forms of input somehow match. it could be a little like trying to match two person's automobiles and wardrobes on the basis of theories about underlying personalities. 
　　　a specific set of criteria for agreement in the case involving the retina will be considered later when we begin to apply some of these principles to the problem of the design of an eye. at this point we will assume that somehow  the mechanism has correctly identified the  experts   or the more reliable components. figure 1 shows how a peergroup might be expected to improve the system response with only a small number of  experts . clearly any voting device  which sums the outputs to determine the majority decision  will be correct more often if it can give this kind of weight to the components which are more likely to be correct. 

       although we have proposed a model of peergroup functions as a solution to the reliability problem  it is fairly :l - tr  i*t it also could pass as  a theory of learning and self-organization in general. in this regard a comparison with some of the other efforts in this area is instructive. the most publicised effort in the field is the  perceptron  approach.. the perceptron is structurally the simplest of all the learning devices proposed thus far. the unfortunate thing about the perceptron is that it is not immediately obvious that it works. we need a proof to convince us that this process of threshold modification in a randomly connected network actually converges on anything. the present suspicion is that it does not  at least in the interesting cases. 
       the peer group model proposed here is too structured a mechanism to count as a perceptron  at least in the ordinary sense. while the perceptron is an attempt to answer the question  how much function is possible with how little structure  the peer group mechanism is an attempt to face the problem of how to find a middle ground between a conventional highly structed but unreliable machine and a relatively structureless machine which finesses the quality control problem. it is 
clear that we know how to achieve good quality control in some things  and it is important to take advantage of that asset when we can. on the other hand  it is clear that sometimes we cannot achieve the quality control that we might like  and here it is important to learn how to take advantage of the other side of the coin as well. 
　　　unlike the perceptron  we do not need a proof to convince us that the peer group process is convergent.  in some sense  it is clearly not convergent since the process passes through  but does not necessarily stop at  the desired point.  rather we need a demonstration that the learning process does not rapidly deteriorate into an aging process. modifications in the network are made only by breaking connections  not by making new ones. this breaking of connections is the basic mechanism for dividing groups of similar logic units into peer groups. it is easy to see that after a long period of time the connections between two units  which are very similar  e.g.  they may compute the same logical function but have different thresholds   and which should be in the same peer group  finally will be broken. if no two units are ever exactly alike  even two that are very similar will eventually have fired differently enough times so that the association elements between them will have deteriorated. this shows that the process as we have outlined it has an inherent aging problem. after the initial formation of the peer groups  the groups will continue to divide and get smaller. when the groups get small enough to reduce the probability of there being an expert within each group  then it is obvious that the system will begin to fail. 
　　　there are several obvious ways in which this aging problem could be overcome  or at least postponed. for example  if new horizontal association elements could be made to grow and replace those that had deteriorated  it is easy to see how peer groups would become more stable. in fact the animal may do just this. if we identify these association elements with horizontal or glial cells in the nervous system  such a regrowth could be explained. in the nervous system  neurons are not regenerated; after birth they only deteriorate. any theory of learning which is attributable to animal systems must take this into account. however  glial cells are not neurons; they are structural cells which help hold a layer of neurons together. glial cells are in fact known to be regenerated. in the past it was hard to fit these glial cells into a convincing theory of the nervous system because  as glial cells  they are incapable of performing any of the interesting tasks which we can attribute to neurons. as passive elements  they can have only the properties of materials like copper wire and resistors; they cannot be active elements like transistors. 
       although the regeneration of these association elements is an obvious way to improve the system  in this study we have avoided relying on this expedient because we want to exhibit a design which is capable of being built within the framework of current technology. we are attempting to formulate a design whose manufacture consists in dumping a number of micro-elements 

-1-

made with very poor quality control  to insure variety  into a container  shaking the container to level out the pile into a layer  and then pouring a glue-like material in to fix it. the glue presumably has the properties of our association elements. it seems unlikely that we could come 
up with a  regenerative  glue  in the required sense  and so we have directed our efforts at other expedients which help stablize peer groups and result in a long and useful life span before they deteriorate. 
       some computer studies of the peer group mechanism have been carried out with a small robot designed to learn to solve a simple maze problem. the results indicate that this sort of mechanism  built with inferior quality control by ordinary standards  can actually begin to learn something about an unkown environment by determining which of its many and various coponents best best correllate with one another in that environment. this principle of relaxing quality control and  covering all bets  in an unkown environment sometimes pays off in surprizing ways. what was particularly interesting in these experiments was that many so-called 
imperfections  such as loose connections  actually turned out to play an important role in the processing of sensory inputs. a loose connection for example was shown often to be a very good wall sensor when it happened that the  noise spike  it generated systematically correlated with other sensory inputs explicitly designed to report impact with obstacles. a full report on these robot experiments is expected to be published in the near future. 
-1-

notes and 