 
stacked generalization is a general method of using a high-level model to combine lowerlevel models to achieve greater predictive accuracy. in this paper we address two crucial issues which have been considered to be a 'black art' in classification tasks ever since the introduction of stacked generalization in 1 by wolpert: the type of generalizer that is suitable to derive the higher-level model  and the kind of attributes that should be used as its input. we demonstrate the effectiveness of stacked generalization for combining three different types of learning algorithms. 
1 	introduction 
stacked generalization is a way of combining multiple models that have been learned for a classification task  wolpert  1 . the first step is to collect the output of each model into a new set of data. for each instance in the original training set  this data set represents every model's prediction of that instance's class  along with its true classification. during this step  care is taken to ensure that the models are formed from a batch of training data that does not include the instance in question  in just the same way as ordinary cross-validation. the new data is treated as the data for another learning problem  and in the second step a learning algorithm is employed to solve this problem. in wolpert's terminology  the original data and the models constructed for it in the first step are referred to as level-1 data and level-1 models  respectively  while the set of cross-validated data and the second-stage learning algorithm are referred to as level-1 data and the level-1 generalizer. 
　in this paper  we show how to make stacked generalization work for classification tasks by addressing two crucial issues which wolpert  originally described as 'black art' and have not been resolved since. the two issues are  i  the type of attributes that should be used to form level-1 data  and  ii  the type of level-1 generalizer in order to get improved accuracy using the stacked generalization method. 
   breiman  1a  demonstrated the success of stacked generalization in the setting of ordinary regression. the level-1 models are regression trees of different sizes or linear regressions using different number of variables. but instead of selecting the single model that works best as judged by  for example  cross-validation  breiman used the different level-1 regressors' output values for each member of the training set to form level-1 data. then he used least-squares linear regression  under the constraint that all regression coefficients be non-negative  as the level-1 generalizer. the non-negativity constraint turned out to be crucial to guarantee that the predictive accuracy would be better than that achieved by selecting the single best predictor. 
　here we show how stacked generalization can be made to work reliably in classification tasks. we do this by using the output class probabilities generated by level-1 models to form level-1 data. then  for the level-1 generalizer we use a version of least squares linear regression adapted for classification tasks. we find the use of class probabilities to be crucial for the successful application of stacked generalization in classification tasks. however  the non-negativity constraints found necessary by breiman in regression are irrelevant to improved predictive accuracy in our classification situation. 
　in section 1  we formally introduce the technique of stacked generalization and describe pertinent details of each learning algorithm used in our experiments. section 1 describes the results of stacking three different types of learning algorithms. the following section describes related work  and is followed by a summary of our conclusions and future work. 
1 	stacked generalization 
given a data set ♀ = { y n   x n    n = 1   . . .   tv}  where yn is the class value and xn represents the attribute values of 


	ting & witten 	1 

datasets # samples # classes # attr & type  led1 
waveform 
horse 
credit 
vowel 
euthyroid 
splice abalone 
nettalk s  
coding 1 
1 
1 
1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 
1 
1 
1 1
1c 
1b+1n+1c 
1b+1n+1c 
1c
1b+1c 
1n
1n+1c 
1n
1ntable 1: details of the datasets used in the experiment. 
n 
  
 
 
n-nominal; b-binary; c: continuous. 
seed. the algorithms used for the experiments are then tested on a separate dataset of 1 instances. results are expressed as the average error rate of ten repetitions of this entire procedure. 
　for the real-world datasets  w-fold cross-validation is performed. in each fold of this cross-validation  the training dataset is used as l  and the models derived are evaluated on the test dataset. the result is expressed as the average error rate of the w-fold cross-validation. note that this cross-validation is used for evaluation of the entire procedure  and is quite different from the jfold cross-validations employed as part of the stacked generalization operation. however  both w and j are set to 1 in the experiments. 
table 1 shows the average error rates of c1  nb and 
ib1  and bestcv  which is the best of the three  selected using j-fold cross-validation. as expected  bestcv is almost always the classifier with the lowest error rate.1 
　table 1 shows the result of stacked generalization using the level-1 model m  for which the level-1 data comprises the classifications generated by the level-1 models  and m'  for which the level-1 data comprises the probabilities generated by the level-1 models. results are shown for the best two level-1 generalizers in each case 
 full results see ting and witten  1a    along with bestcv. the lowest error rate for each dataset is given in bold. 
　table 1 summarizes the results in table 1 in terms of a comparison of each level-1 model with bestcv totaled over all datasets. clearly  the best level-1 model is m' derived using mlr. it performs better than bestcv in nine datasets and equally well in the tenth. the best performing m is derived from nb  which performs better than bestcv in seven datasets but significantly worse in two. 
the datasets are shown in the order of increasing size. 
mlr performs significantly better than bestcv in the 
1
　　note that bestcv does not always select the same classifier in all w folds. that is why its error rate is not always equal to the lowest error rate among the three classifiers. 

table 1: ave. error rates of c1  nb and ib1  and the best among them selected using j-fold cross-validation. 
datasets level-1 generalizers 
c1 	nb 	ib1 bestcv led1 
waveform 
horse 
credit 
vowel 
euthyroid 
splice 
abalone 
nettalk s  
coding 1 
1 
1 1 
1 
1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 1
1
1
1
1
1 
1
1
1
1table 1: ave  error rates for stacking c1  nb and ibl. 
	datasets 	m 	m' 
	bestcv 	nb 	mlr 	ibl 	mlr 
	led1 	1 	1 	1 	1 	1 
	waveform 	1 	1 	1 	1 	1 
	horse 	1 	1 	1 	1 	1 
	credit 	1 	1 	1 	1 	1 
	vowel 	1 	1 	1 	1 	1 
	euthyroid 	1 	1 	1 	1 	1 
splice 1 1 1 1 1 abalone 1 1 1 1 1 
	nettalk s  	1 	1 	1 	1 	1 
	coding 	1 	1 	1 	1 	1 
table 1: summary of table 1-comparison of bestcv with m and m'. 
m 
nb 	mlr m' 
ibl 	mlr #win vs. #loss 1 	1 1 	1 four largest datasets.1 this indicates that stacked generalization is more likely to give significant improvements in predictive accuracy if the volume of data is large- a direct consequence of more accurate estimation using cross-validation. 
　mlr has an advantage over the other three level-1 generalizers in that its model can easily be interpreted. examples of the combination weights it derives  for the probability-based model m'  appear in table 1 for the splice dataset. the weights indicate the relative importance of the level-1 generalizers for each prediction class. in this dataset  nb is the dominant generalizer for predicting class 1  nb and ibl are both good at predicting class 1  and all three generalizers make a worthwhile contribution to the prediction of class 1. 
table 1: weights generated by mlr  model m'  for the splice dataset. the first column indicates the class. 

table 1: average error rates of three versions of mlr. 
nc - no constraints; ni - no intercept. 
datasets 	mlr with 	!nc ni non-negativity led1 
waveform 
horse 
credit 
vowel 
euthyroid splice 
abalone 
nettalk s  
coding 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 1 
1 
1 !
1 
1 
1 
1 
1 
1 
1 1 	a r e non-negativity constraints necessary  
both breiman  1a  and leblanc and tibshirani  use the stacked generalization method in a regression setting and report that it is necessary to constrain the regression coefficients to be non-negative in order to guarantee that stacked regression improves predictive accuracy. here we investigate this finding in the domain of classification tasks. 
　to assess the effect of the non-negativity constraint on performance  three versions of mlr are employed to derive the level-1 model m' 
i each linear regression in mlr is calculated with an intercept constant  that is  / 1- 1 weights for the / classes  but without any constraints; ii each linear regression is derived with neither an intercept constant  j weights for i classes  nor constraints; 
iii each linear regression is derived without an intercept constant  but with non-negativity constraints  i non-negative weights for i classes . 
　the third version is the one used for the results presented earlier. table 1 shows the results of all three versions. they all have almost indistinguishable error rates. we conclude that in classification tasks  non-negativity constraints are not necessary to guarantee that stacked generalization improves predictive accuracy. 
　however  there is another reason why it is a good idea to employ non-negativity constraints. table 1 shows an example of the weights derived by these three versions 
	ting & witten 	1 

table 1: weights for the euthyroid dataset with three versions of mlr:  i  no constraints   ii  no intercept  and  iii  non-negativity constraints. 

table 1: ave. error rates of bestcv  majority vote and mlr  model m'   along with the standard error  #se  between bestcv and the worst level-1 generalizes. 
	  dataset 	#se 	bestcv 	majority 	mlr 
	horse 	1 	1 	1 	1 
	splice 	1 	1 	1 	1 
	abalone 	1 	1 	1 	1 
	led1 	1 	1 	1 	1 j 
	credit 	1 	1 	1 	1 
	nettalk s  	1 	1 	1 	1 
	coding 	1 	1 	1 	1 
	waveform 	1 	1 	1 	1 
	euthyroid 	1 	1 	1 	1 
	vowel 	1 	1 	1 	1 
of mlr on the euthyroid dataset. the third version  shown in row  iii   supports a more perspicuous interpretation of each level-1 generalized contribution to the class predictions than do the other two. in this dataset c1 is the dominant generalizer  as evidenced by its high weights. however  the negative weights render the interpretation of the other two versions much less clear. 
1 h o w does stacked generalization c o m p a r e t o m a j o r i t y vote  
let us now compare the error rate of m'  derived from mlr  to that of majority vote  a simple decision combination method which requires neither cross-validation nor level-1 learning. table 1 shows the average error rates of bestcv  majority vote and mlr. in order to see whether the relative performances of level-1 generalizes have any effect on these methods  the number of standard errors  #se  between the error rates of the worst performing level-1 generalizer and bestcv is given  and the datasets are re-ordered according to this measure. since bestcv almost always selects the best performing level-1 generalizer  small values of #se indicate that the level-1 generalizers perform comparably to one another  and vice versa. 
　mlr compares favorably to majority vote  with seven wins versus three losses. out of the seven wins  six have significant differences  the only exception is for the splice dataset ; whereas all three losses have insignificant differences. thus the extra computation for cross-validation and level-1 learning seems to have paid off. 
　it is interesting to note that the performance of majority vote is related to the size of #se. majority vote compares favorably to bestcv in the first seven datasets  where the values of #se are small. in the last three  where #se is large  majority vote performs worse. this indicates that if the level-1 generalizers perform comparably  it is not worth using cross-validation to determine the best one  because the result of majority vote-which is far cheaper-is not significantly different. the same applies when majority vote is compared with mlr. mlr performs significantly better in the five datasets that have large #se values  but only one in the other cases. 
summary 
  none of the four learning algorithms used to obtain model m perform satisfactorily. 
  mlr is the best of the four learning algorithms to use as the level-1 generalizer for obtaining model m'. 
  when obtained using mlr  m' has lower predictive error rate than the best model selected by j-fold cross-validation  for almost all datasets used in the experiments. 
  another advantage of mlr over the other three level-1 generalizers is its interpretability. the weights aki indicates the different contributions that each level-1 model makes to the prediction classes. 
  model m' can be derived by mlr with or without non-negativity constraints. such constraints make little difference to the model's predictive accuracy. 
  the use of non-negativity constraints in mlr has the advantage of interpretability. non-negative weights  support easier interpretation of the extent to 
which each model contributes to each class. 
  when derived using mlr  model m' compares favorably with majority vote. 
1 	related work 
our analysis of stacked generalization was motivated by that of breiman  1a   discussed earlier  and leblanc and tibshirani . leblanc and tibshirani  examine the stacking of a linear discriminant and a nearest neighbor classifier and show that  for one artificial dataset  a method such as mlr performs better with non-negativity constraints than without. our results show that these constraints are irrelevant to mlr's predictive accuracy in the classification situation. 
　the limitations of mlr are well-known  duda and hart  1 . for a /-class problem  it divides the description space into / convex decision regions. every region is 

singly connected  and the decision boundaries are linear hyperplanes. this means that mlr is most suitable for problems with unimodal probability densities. despite these limitations  mlr still performs better as a level-1 generalizer than i b l   its nearest competitor in deriving m ' . these limitations may hold the key for a fuller understanding of the behavior of stacked generalization. jacobs  reviews linear combination methods like that used in mlr. 
　previous work on stacked generalization  especially as applied to classification tasks  has been limited in several ways. they have a different focus and evaluate the results on just a few datasets  leblanc and tibshirani  
1; chan and stolfo  1; kim and bartlett  1; fan et al  1 . 
1 	conclusions and future work 
we have addressed two crucial issues for the successful implementation of stacked generalization in classification tasks. first  class probabilities should be used instead of the single predicted class as input attributes for higherlevel learning. second  the multi-response least squares linear regression technique should be employed as the high-level generalizer. 
　when combining three different types of learning algorithms  this implementation of stacked generalization was found to achieve better predictive accuracy than both model selection based on cross-validation and ma-
jority vote. unlike stacked regression  non-negativity constraints in the least-squares regression are not necessary to guarantee improved predictive accuracy in classification tasks. however  these constraints are still preferred because they increase the interpretability of the level-1 model. 
　this paper concentrates on finding conditions under which stacked generalization works. a better understanding of why it works in this particular configuration may open up other possibilities for further improvement of stacked generalization. 
　the implication of our successful implementation of stacked generalization is that earlier model combination methods which employs  weighted  majority vote  averaging  or other computations that do not make use of level-1 learning  can now apply this learning to improve their predictive accuracy. this includes methods which combine models derived from a single learning algorithm such as bagging and arcing  breiman  1b; 1c . the full version of this paper  ting and witten  1a  and a subsequent investigation  ting and witten  1b  include studies of this kind of model combination. 
acknowledgments 
the authors are grateful to the new zealand marsden fund for financial support for this research. 
