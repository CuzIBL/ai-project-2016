 
   this paper is concerned with state space problem solvers that achieve generality by learning strong heuristics through experience in a particular domain. we specifically consider two ways of learning by analysing past solutions that can improve future problem solving: creating macros and the chunks. a method of learning search heuristics is specified which is related to 'chunking' but which complements the use of macros within a goal directed system. an example of the creation and combined use of macros and chunks  taken from an implemented system  is described. 
	i 	introduction 
integrating ideas and techniques devoloped in 
machine learning  with those of problem solving  has attracted substantial recent research effort  e.g. laird et al 1  korf 1  langley 1  mitchell et al 1 . an important aspect is the revival of the 'general' problem solver. its demise was due in part to the failure of its weak heuristics to tackle problems of complexity in some given application domain; now it returns equipped with not just weak problem solving heuristics but with weak heuristics for learning strong  i.e.domain dependent  heuristics. the latter may take the form of useful shifts in the problem space representation  a simple example is the learning of macro operators  or improving search through a particular space by the acquisition of search control heuristics. thus  while its generality is maintained  learning may improve the problem solver's efficiency during the application to a particular domain. this is the approach we have taken in the construstion of a 'heuristic learning problem solver shell' called fm; it can acquire strong heuristics from problem solving experience when it is applied to specific domains. a complementary approach is to acquire or discover them during a preprocessing stage as in  iba 1    korf 1   and  dawson & siklossy 1 . 
   fm's application domains can have variable initial and goal states. applications are interchangeable by specifying domain environments  states and goals as expressions in first order logic  and operators in terms of structured add  delete and precondition predicates. control stategies may be interchanged  e.g. forward best-first or goal reduction  as can weak learning methods such as macro and chunk creation. 
 this constitutes a more general approach to recent work on heuristic learning in problem solvers  e.g.  mitchell et al 1    korf 1    where systems typically improve in domains with a fixed goal  employ a more specialised representation scheme  and a forward state space search strategy. this paper will outline fm's goal directed search and describe how macros and chunks are created and used as complementary heuristics during that search. 
ii goal node search in fm 
   the backward search of fm proceeds in a goal reduction manner  starting with the initial goal  through a space of goal nodes  similar to those in  dawson & siklossy 1  . each goal node can be modelled as a 1-tuple: 
 identifier  goal  initial state  ancestors  purpose  trace  the trace records attempts to solve the goal  whereas the purpose records why the goal node was created  typically to solve the unsatisfied preconditions of an operator . goals  expressed as conjunctions of ptedicates  are initially assumed to be decomposable: when a goal node is activated  operator instantiations which add goal predicates have their unsatisfied preconditions form another goal node  unless they are already satisfied in which case those operators are applied to the initial state and the result recorded in the trace. 
   when the trace of a goal node eventually contains a state satisfying its goal  via an operator sequence os   we say that the goal node is solved  and all nodes which are ancestors of it are removed from the search. if it was activated to solve an operator o's preconditions  then the sequence os + 1 is applied to the goal node's parent's initial state and the result recorded in the parent's trace. 
   a goal node's initial state may be the state inherited from a parent node  or may be an advanced state partially satisfying the parent's goal. the latter is the case when goals cannot be solved by simple decompositon; fm examines the trace and forms new goal nodes whose goal predicates are inherited but whose initial states are selected from intermediate states taken from the parent's trace. 
	mccluskey 	1 

   the kind of representation of goal nodes outined above aids both the formation and use of strong heuristics. the trace is available for analysis and criticism after the solution of each goal node  allowing 'within-trial transfer of learning'  see  laird et al 1   to take place. in our implementation of fm we have experimented with the formation of closed macros  'b-chunks' and also subgoal ordering heuristics at this stage  but we shall limit our discussion to the first two. 
ill closed macro creation 
　　we consider a closed macro operator to be an operator sequence that has been compiled and generalised into a form similar to that of a primitive operator  in contrast to the 'open' macrops of  fikes et al 1  . this sequence forms part of a past solution  in the case of learning by experience  which includes fully instantiated operators and intermediate states. here the compilation involves finding the sequence's weakest precondition through the intermediate states and using it as the macro's precondition. within this certain constants can then be selectively generalised using a technique similar to the explanation-based learning of  mitchell et al 1 . 
   systems that learn closed macros   minton 1    iba 1   seem to demonstrate significant improvement in problem solving within robot and puzzle worlds but there are pitfalls in using this technique as the sole learning component: 
　　-search trees do shorten but unfortunately grow bushy since distinct instantiations of macros proliferate.  this is reminisent of the effect of paramodulation  a 'macro inference rule' in theorem proving  which combines resolution with the axioms of equality  but when used in search changes long thin trees to short bushy ones! . 
   -solutions which comprise of closed macros are prone to produce non-optimal paths even after checks for redundant primitive operator sequences have been made. 
   we claim that such problems may be overcome by the learning of strong heuristics such as chunks to complement the use of macros. 
　　macros are created and stored in fm when goal nodes are solved  and then are immediately available for use in problem solving. each are compiled from a successful operator sequence into a primitive operator format. the major part of this compilation process is in building up the precondition m.p  a conjunction of predicates  of a macro m. this is accomplished by a procedure modelled on goal regression equations: 
m.p = pn where po = g and 
pi m  pi-1 - 1 n+1-i .a  u 1 n+1-i .p   i = 1 to n 
where 'u' and '--' mean set union and difference  1 i .p  1 i .a stand for the precondition and add predicates of operator i respectively  and g the goal predicates for the solution sequence. 
1 	knowledge acquisition 
   constants that appeared as arbitrary members of some particular type in the solution's operator sequence are carefully generalised to a variable with that type restriction  following  kodratoff 1  . generalisation is justified since no operator in the solution sequence referred to the constant specifically but only to its type. identical constants are generalised 
to the same variable throughout the macro  but equality binding restrictions are added where variables of the same type are generalised from distinct constants  so that they may not be instantiated to the same constant when in use. macros are then incorporated into future problem solving as primitive operators  although some may later be deleted if rarely used. 
	iv 	b-chunk creation 
   the chunks created by fm improve the system's subsequent problem solving behaviour by providing search control knowledge. they are formed during the goal directed search and advise on the search through partial solutions. the absence of such a learning component in strips with macrops is pointed out in 
 porter and kibler 1  and minton's morris system  minton 1  apparently combines only weak search heuristics with the use of macros. 
　　consider 1 i   1 i n  taken from an operator sequence 1 1 n  which achieves a goal node  with goal predicate s  g  from a initial state i within a domain environment e  e is a set of facts and rules constituting background knowledge for a 
particular application . a b-chunk  1 i '; g'; p'  is built for each 1 i  to the following specification: consider a function 'sim': 
sim : cp x cp x cp x nato --  cp 
where cp is the space of conjunctions  or sets  of predicates and 
sim x y e o  = {p in y: p logically follows from x&e} sim x y e n  = sim x y e n-1  union 
{y el. of y  e subset of e : y is related to an x in x by an association chain e of length n} 
then 
　p= sim m i  m 1  e k  where m j = the macro precondition  see section iii  of sequence 1 j  1 j+1   
... o n ;k =1  and finally 
 1 i '; g'; p'  = the careful generalisation of  1 i ; g; p . 
　when k = 1 then 1 i 's chunk's third component may be roughly described as those predicates which were present in the goal node's initial state and that were also involved in the achievement of g after 1 i-1 . this includes environment information  which is assumed to be a part of every state  that has been used in the satisfaction of the operator's preconditions. fm initially forms p with k=1 and then checks to see if the resulting chunk would be discriminatory if used to solve 
the same goal node again. if it is not the case then k is incremented and p is augmented with predicates using an 'association chain' technique similar to that described in fvere 1. 
　b-chunks are then used during subsequent search when fm finds multiple operators  or operator instantiations  are available to achieve a goal predicate gp  but none of their preconditions are completely satisfied. a b-chunk  1; g1; p  will favour an operator instantiation o applied to a goal node if p logically follows from l&e under the variable bindings obtained by the successful matching of 1 to o  and g1 to either gp or one of gp's ancestors. the instantiation s  favoured by the most chunks is then chosen to form a new goal node. 
v combined use of learnt heuristics 
   to clarify the combined use of closed macros and b-chunks we use a simple example. we applied fm to a robot world using a similar operator set to  fikes et al 1 . after box moving tasks it forms macros such as: 
  name: macro1 rm1  dr1 rm1 box dr1 rm1   preconditions: in room box rm1 &nexmo robot box  &connect rm1 rm1 dr1 &connect rm1 rm1 dr1  .... add: in room box rm1   side effects: in room robot rm1   ...  . 
macro1 is equivalent to the primitive sequence: {pushto box dr1 rm1   pushthru box dr1 rm1   pushto box dr1 rm1   pushthru box dr1 rm1 }. 
   in solving the goal 'in room boxa  room1 ' from the situation in figure 1  macro1 constitutes the part of the solution shown by an arrow. one b-chunk  where k=1 in section iv  created to advise on its use is  note: we leave out some details; capital letters denote variables : 
   macro1 rm1  dr1 rm1 box dr1 rm1  ; in room box rm1  ; in room box rm1 &connect rm1 rm1 dr1 & connect rm1 rm1 dr1 &connect rm1 rm1 dr1 & ...  in a future problem  this chunk will support the inclusion of instantiations of macro1 in partial solutions which conform to its constraints. for instance  consider task in room boxb room1 . it can be seen by the description of chunk use in section iv that instance macro1 room1 door1 room1 boxb door1 room1  is favoured by the chunk shown above to form the first part of a solution  resulting in a filtering out of any other undersirable instantiations. note that this chunk suggests the initial position of the robot is irrelevant. 

vi conclusions 
　we have described a goal directed search which allows the use of weak methods for learning. given a particular domain  these weak methods create strong heuristics  in the form of macros and b-chunks  through the experience of successful problem solving. the chunks record for each operator and generalised goal pair  the adviseable instantiations for operator variables. they do this by storing important similarities among the environment  initial state and goal in a form usable for future goal directed search. the number of possible instantiations of macros in the backward search tends to be much higher than primitives  and so the need for this heuristic pruning is greater. 
   we have used fm in several applications in which it builds up strong domain dependent heuristics by experience. of particular note is the b-chunks' high degree of accross-task transfer of learning. this is because they record quite general similarities between the components of a problem space such that when these similarities are encountered again the choice of  macro  operator instantiation can be determined. 
