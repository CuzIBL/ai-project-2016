 
probabilistic networks which provide compact descriptions of complex stochastic relationships among several random variables are rapidly becoming the tool of choice for uncertain reasoning in artificial intelligence we show that networks with fixed structure containing hidden variables can be learned automatically from data using a gradient-descent mechanism similar to that used in neural networks we alio extend the method to networks with intensionally represented distributions  including networks with continuous variables and dynamic probabilistic networks because probabilistic networks provide explicit representations of causal structure human experts can easily contribute pnor knowledge to the training process  thereby significantly improving the learning rate adaptive probabilistic networks  apns  may soon compete directly with neural networks as models in computational neuroscience as well as in industrial and financial applications 
1 	introduction 
intelligent systems whether biological or artificial require the ability to make decisions under uncertainty using the available evidence several computational models exhibit some of the required functionality for example  neural networks which represent complex input/output relations using combinations of simple nonlinear processing elements are a familiar tool in ai and computational neuroscience probabilistic networks  also called belief networks or bayesian networks  are a more explicit representation of the joint probability distribution characterizing a problem domain providing a topological description of the causal relationships among variables 
   computational models in ai are judged by two main criteria ease of creation and effectiveness in decision making  cognitive science and neuroscience add the criterion of biological plausibility   some computational models are associated with learning algorithms that construct specific models 
'this research was supported by nsf gram 1ri 1  py1  
     1  daphne roller was supported by a university of california pres ident s postdoctoral fellowship and an nsf postdoctoral associate ship in experimental science 
1 	learning 
automatically from data  adapting to reality rather than to an expert s conception thereof neural networks for example use a localized gradient-descent scheme to learn the model from the data the resulting ease of construction and the bio logical plausibility of this approach have contributed signifi cantly to the popularity of neural networks the drawbacks ol current learning schemes include the need for large amounts of training data and the incomprehensibility of the resulting models furthermore many of the computational models that are associated with a learning algorithm are not the most et fective models for decision making probabilistic networks on the other hand  perform well in complex decision-making domains such as medical diagnosis but have usually required a good deal of construction effort 
   in this paper we present a new learning algorithm for probabilistic networks that is effective even when some of the variables are hidden-that is their values are not observable this makes probabilistic network;  competitive with neural networks in terms of ease of creation in fact because prob abilistic networks have a precise local semantics it is quite possible for human experts or other computational systems to provide prior knowledge to the learning process thereby reducing the need for training data moreover the output of the learning process is comprehensible to humans 
   the paper begins with a basic introduction to probabilistic models in ai we then present the following results 
  derivation of a gradient-descent learning algorithm for probabilistic networks with hidden variables  where the gradient can be computed locally by each node using information that is available in the normal course of prob abilistic network calculations 
  extensions of the algorithm to handle mtensionally rep resented distributions  such as noisy-or nodes  con tinuous variables and dynamic probabilistic networks representing temporal processes 
  experimental demonstration of the algorithm on small and large networks showing a dramatic improvement in learning rate resulting from inclusion of hidden variables 
  experimental demonstration of the extended algorithm applied to a dynamic probabilistic network 
we conclude that adaptive probabilistic networks  apns  may provide an excellent tool for scientists and engineers in building complex models from noisy data our results also mo tivate the use of a much broader class of models satisfying 

the basic requirements of computational neuroscience than is commonly considered 
1 	probabilistic networks 
systems based on probability theory now dominate the fields of expert systems and speech recognition  and are making rapid progress in language understanding and computer vi won here we provide only a brief introduction for a thorough treatment see pearl  
　probability theory views the world as a set of random variables x1 x  each of which has a domain of possible values for example in describing cancer patients the variables lungcancer and smoker can each take on one of the values true and false the key concept in probability theory is the point probability distribution  which specifies a probability tor each possible combination of values for all the random variables given this distribution one can compute any desired probability given any combination ot evidence for example given observations and test results one can compute the probability that the patient has lung cancer 
　unfortunately  an explicit description of the joint distri bution requires a number of parameters that is exponential in n the number of variables probabilistic networks derive their power from the ability to represent conditional independences among variables  which allows them to take advantage of the locality of causal influences intuitively a variable is independent of its indirect causal influences given its direct causal influences in figure 1 for example the outcome ot the x-ray does not depend on whether the patient is a smoker given that we know that the patient has lung cancer if each variable has at most k other variables that directly influence it then the total number of required parameters is linear in n and exponential in k this enables the representation ot quite large problems for example the cpcs network  pradhan et al 1 contains 1 variables and compares well with the world s leading diagnosticians in internal medicine 

figure 1  a  a simple probabilistic network showing a proposed causal model  b  a node with associated conitional probability table the table gives the conditional probability of each possible value of the variable emphysema  given each possible combination of values of the parent nodes smoker and coalminer 
　formally a probabilistic network is defined by a directed acyclic graph together with a conditional probability table  cpt  associated with each node  see figure 1 ' each node 
l
   we have described the simplest form of network networks can include continuous as well as discrete variables provided the representation of the conditional density function is finite cpts represents a random variable the cpt associated with variable x specifies the conditional distribution p x | parents x   the arcs encode probabilistic dependence in the sense that each variable must be conditionally independent of its nondescendants in the graph given its parents this constraint implies that the network provides a complete representation of the joint distribution through the following equation n 
	p x1 xn = iip x  i 	parents x    	 1  
i=i 
where p x1 xn  is the probability of a particular combina tion of values for x  xn 
　once a network has been constructed inference algorithms operate on it to calculate probabilities for query variables given values for evidence variables it is important to note that the distinction between evidence and query variables is entirely flexible-any variable can be set and any variable can be queried the best exact inference algorithms typically use a transformation to markov random fields  launtzen and spiegelhaller 1  stochastic approximation algorithms using monte carlo simulation have also been developed  pearl 1  although the general inference problem is likely to be of exponential complexity in the worst case large networks are often solvable in practice massive parallelism can easily be applied particularly with simulation algorithms 
1 	learning probabilistic networks 
how can probabilistic networks be learned from data  there are several variants of this question the structure of the network can be known or unknown and the variables in the network can be observable or hidden in all or some of the data points  the latter distinction is also described by the terms complete data and incomplete data   
　the case of known structure and fully observable variables is the easiest in this case we need only learn the cpt entries since every variable is observable each data case can be pigeonholed into the cptentries corresponding to the values of the parent variables at each node simple bayesian updating then computes posterior values for the conditional probabilities based on dirichlet priors folesen et al 1 spiegelhalterer al 1 
　the case of unknown structure and fully observable variables has also received some attention in this case the problem is to reconstruct the topology of the network-a discrete optimization problem usually solved by a greedy search in the space of structures  cooper and herskovits 1 heckerman et al 1  for any given proposed structure the cpts can be reconstructed as described above the resulting algorithm-  are capable of recovering fairly large networks from large data sets with a high degree ot accuracy 
　in this paper we are mostly concerned with problems in which the structure is fixed but some variables are hidden 1 can be represented implicitly by paramelenzed lunctions instead ot 
explicit tables 
   'we also note that an algorithm for learning cpts on a fixed structure with hidden variables can be applied to the general case of hidden vanables and unknown structure by wrapping a structural search algorithm around it. however the structural search algonlhm must be more powerful than lhose described above for the fully observable case since it may need to introduce new hidden variables 
	russell  ft al 	1 

this case often occurs 1 n practice since causal structure is a lot easier to elicit from experts than numbers  whereas data cases are unlikely to contain values for all the relevant variables for example the causal connections between diseases and their symptoms are often known and medical records can easily provide a large number of data cases but the medical records are not typically complete data points the actual disease is often not observed directly we rarely have results for all possible clinical tests and so on furthermore causal models often contain variables that are sometimes inferred but never observed directly such as syndromes in medicine 
   the fixed-structure  hidden-variable case has been stud led by several researchers the earliest work of which we are aware is thai by golmard and mallet  who describe an algonthm for learning in tree-structured net works the general case of directed acyclic graphs was addressed by launtzen 1   see also the discussions in ispiegelhalter et al 1 olesen et al 1 spiegelhalter and cowell  1   these papers describe the application of the em  expectation maximization  algorithm  dempster et al 1  to probabilistic networks em like gradient descent  finds local maxima on the likelihood surface defined by the network parameters launtzen notes some difficulties with the use of em for this problem and suggests gradient descent as a possible alternative thiesson is currently undertaking direct comparison of the performance of the two approaches a third possible approach is to use gibbs sampling  heckerman personal communication  buntine 1  in the course of a general mathematical analysis of structured learning problems also suggests that one could use generalized network differentiation for learning probabilistic networks with hidden variables 
   as mentioned above the gradient-descent approach for belief network learning is closely related to neural network learning an analogy observed by neal  1 neal derives an expression for the likelihood gradient in sigmoid networks using stochastic simulation and uses it to show that the boltz mann machine  a variety of neural network  is a special cast  of a probabilistic network the helmholtz machine '  dayan et al in press  is a hybnd of neural network and probabilistic network ideas it restricts the kinds of probability distributions that can be represented in an attempt to retain the linear-time execution property of neural networks 
   one might ask why the known-structure hidden vanable problem cannot be reduced to the fully observable case by eliminating the hidden variables using marginalization   averaging out   there are two reasons for this first it is not necessarily the case that any particular vanable is hidden in all the observed cases  although we do not rule this out  second networks with hidden variables can be more compact than the corresponding fully observable network  see figure 1  in general if the underlying domain has significant local structure  then with hidden vanables it is possible to take advantage of that structure to find a more concise representation for the 
joint distribution on the observable vanables this in turn makes it possible to learn from fewer examples 
   before describing the details of our solution we will explain the task in more detail the algorithm is provided with a network structure and initial  randomly generated  values for thecpts it is presented with a set d of data cases d1 dm 
we assume that the cases are generated independently from some underlying distribution in each data case values are 
1 	learning 

figure 1  a  a probabilistic network with a hidden vanable la belled h  h is two valued and the other vanables are three valued   the network requires 1 independent parameters  b  the corre sponding fully observable network which requires 1 parameters 
given for some subset of the vanables this subset may differ from case to case the object is to find the cpt parameters w that best model the data we adopt a bayesian notion of best more specifically we assume that each possible setting of w is equally likely a prion so that the maximum likelihood model is appropriate this means that the arm is to maximize pw d  the probability assigned by the network to the observed data when the cpt parameters are set t o w 1 
1 	gradient-descent algorithms 
our approach is based on viewing the probability pw d  as a function of the cpt entnes w this reduces the problem to one of finding the maximum of a multivanate nonlinear function algonthms for solving this problem typically take small steps on the surface whose coordinates are the parameters and whose height is the value ot the function trying to get to the highest point on the surface in fact it turns out to be easier to maximize the log-likelihood function in pm d  since the two functions are monotonically related maximizing one is equivalent to maximizing the other 
　the simplest variant of this approach and the one we use is gradient descent  also known as  hill-climbing   at each pointw itcomputesvw the gradient vector of partial denva lives with respect to the cpt entries the algorithm then takes a small step in the direction of the gradient naively this would be to the point w + o v w where a is the step-size parameter however we need to be more careful we ac tually want to maximize pw d  subject to the constraint that w consists of conditional probability values which must be between 1 and 1 furthermore in any cpt the entnes corre sponding to a particular conditioning case  an assignment of values to the parents  must sum to 1 standard results show that taking a step in the direction vw and then renormalizing to the constrained surface achieves the same effect in par ticular when an edge of the parameter space is reached this algorithm will have the effect of following it the algonthm terminates when a local maximum is reached that is when the renormalized gradient is zero 
1
　　compare this to the neural network task minimize ew d  the sum of squared differences between observed and predicted data val ues when the network weights arc set to w it has been pointed out that both maximum likelihood and neural network methods some umes find local maxima at extreme values of their parameters which can cause problems it is possible that such problems can be avoided by carrying through the analysis for nonuniform pnors 

　by moving in the direction of the gradient this simple algorithm executes a greedy hill-climbing procedure a variety of techniques can be used to speed up this process  such as polak-ribiere conjugate gradient methods variants of this basic technique are the standard approach for training the pa rameters  weights  of a neural network our results along with the results of buntine and neal demonstrate a very close connection between neural networks and probabilistic net 
works our results are a significant extension of neal s result 
since they apply to any probabilistic network 
1 	local computation of the gradient 
the usefulness of gradient descent depends on our ability to compute the gradient efficiently this is one of the main keys to the success of gradient descent in neural networks there back-propagation is used to compute the gradient of the function encoded by the neural network with respect to the network parameters  i e   the weights on the links  the 
existence of a simple local algorithm for training the network allows one to use the same network and algorithms for both training and inference furthermore the similarity to real 
biological processes lends a certain plausibility to the entire neural-network paradigm 
　we now show that a similar phenomenon occurs in probabilistic networks in fact for probabilistic networks  no back-propagation is needed the gradient can be computed locally by each node using information that is available in the normal course of probabilistic network calculations in our derivation we will use the standard notation wrjk to denote a 
specific cpt entry the probability that variable x  takes on its jlh possible value assignment given that its parents u  take on 
their kth possible value assignment this last equation allows us to piggyback the computation ot the gradient on the calculations of posterior probabilities done in the normal course of probabilistic network operation essentially any standard probabilistic network algo-
rithm when executed with the evidence d1 will compute the term pw  xrl urk | d1  as a by-product we are therefore able to use a standard commercial package  hugin  for the required inference calculations 
　the gradient vector can now be obtained as follows we run an inference algorithm on each data case d1 separately computing pw xsl ulk |d|  for each tjk in the process we then sum these expressions over the different data cases / and 
divide by wrjk this is then used as outlined above section 1 describes results obtained from our implementation 
	1 	extensions for generalized parameters 
our analysis above applies only to networks where there is no relation between the different parameters  cpt entries  in the network clearly this is not always the case if we do a particular clinical test twice the parameters associated with these two nodes in the network should probably be the same  even though the results can differ  in many situations the causal influences on a given node are related so that more compact representations than an explicit cpt are called tor viewing a cpt as a function from the parent values uik and the child value xrj to the number p  x1 -xt  | ur =urk   it is often reasonable to describe this function intensionally using a small number of parameters for example  we may choose to describe this function as a neural network in other contexts we might have more information about the structure of this 
	function 	a noisy or model for example encodes our belief 
that a number of diseases all have an independent chance of causing a certain symptom we then have a parameter a  describing the probability that disease i in isolation causes the sympiom the probability ot the symptom appearing given a combination of diseases is fully determined by these parameters it the symptom node has k parents the cpt for the node can de described using k rather than 1k parameters  assuming that all nodes are binary-valued  using noisyor nodes can make an otherwise intractably large network practical for example the cpcs network mentioned above has only 1 parameters but would require 1 1 parameters if the cpts were defined by explicit tables 
   given that we want our network to be defined using parameters that are different from the cpt entries themselves  we would like to learn these parameters from the data our basic algorithm remains unchanged rather than doing gradient ascent over the surface whose coordinates are the cpt entries 
	russell  et al 	1 

we do gradient ascent over the surface whose coordinates are these new parameters the only issue we need to address is the computation of the gradient with respect to these parameters as we now show our analysis can easily be extended to this more general case using a simple application of the chain rule for derivatives technically assume that the network is defined using some vector of parameters a whose values we are trying to adjust each cpt entry wyk can be viewed as a function wyk x  assuming these functions are differentiable we obtain the following 

our analysis above shows how the first term in each product can be easily computed as a by-product of any standard probabilistic network algorithm the second term requires only a simple function application 
   the ability to learn intensionally represented probabilistic networks confers many advantages first as we argued certain networks are simply impractical unless we reduce the size of their representation in this way this is even more important when learning such networks since learning each cpt entry separately would almost certainly require an unreasonable amount of training data this is another instance where our algorithm is able to utilize prior knowledge in the right way to speed up the learning process but even more importantly this ability allows us to learn networks that otherwise would not fit into this framework for example as we mentioned above  probabilistic networks can also contain continuous-valued nodes the 'cpt  for such nodes must be intensionally defined  for example as a gaussian distribution with parameters for the mean and the variance  launtzen and wermuth 1  equation 1 gives us the fundamental tool needed for learning such networks 
   perhaps the most important application of equation 1 is in learning dynamic probabilistic networks  dpns  i e   networks that represent a temporal stochastic process such networks are typically divided into time slices where the nodes at each slice encode the state at the corresponding time figure 1 shows the coarse structure of a generic dpn the cpts for a dpn include a state evolution model which describes the transition probabilities between states and a sensor model  which describes the observations that can result from a given state typically one assumes that the cpts in each slice do not vary over time the same parameters therefore will be duplicated in every time slice in the network in this case we can show that equation 1 simplifies out to the sum of the gradients corresponding to the different instances of the parameter  section 1 demonstrates the application of this algorithm to a simple example   as a way of modelling a partially observable process dpns compete directly with hidden markov models  hmms  dpns allow the decomposition of the hidden state into several variables potentially revealing additional structure in the process being modelled and improving inductive performance intuitively a dpn represents n bus of state information using o n  state variables whereas an h m m uses 1n  states if the state evolution model can be described compactly in terms of the the cpts for the state variables we would expect dpns to outperform hmms on problems with large state spaces 

figure 1 generic structure of a dynamic probabilistic network in an actual network there may be many state and sensor variables in each lime slice 
1 	experimental results 
we report on three experiments the first shows the importance of prestructuring the probabilistic network using hidden variables the second shows the effectiveness of the algo nthm on a large network with many hidden variables the third demonstrates the capability for learning a model of a temporal process with hidden variables 
   the basic tools we need are a probabilistic inference engine and a gradient-descent algorithm for the former  we use hugin in the hrst two experiments and our own stochastic simulation system for the third experiment for the latter we have adapted the conjugate gradient algorithm  price 1  to keep the probabilistic variables in the legal  1    range as described above this uses repeated line minimizations  with direction chosen by the polak-ribiere method  and a heuristic termination condition to signal a maximum in each exper iment training cases are generated by stochastic sampling from the distribution defined by each network for the observ 
able variables  new training cases are added incrementally into the existing training set 
   the performance of the algorithm is measured as a function of the number of training cases  x-axis  among the possible choices for the performance metric  y-axis  the most obvious would be the probability p d'  assigned by the learned network to a set of test data d' generated from the original net work  or possibly the kullback-liebler distance from lhe true distribution if available however in order to facilitate com parison with traditional algorithms which have fixed inputs and outputs we designate certain observable nodes as out puts ' and measure the ability of lhe learned network to predict the output values given values for lhe remaining observable nodes more precisely we measure the mean square error for each output node probability value where the mean is taken over the distribution of input values  in large networks  this is approximated by sampling   
   the first experiment uses data generated from the 1-1 network in figure 1 a  we ran three algorithms on this data an apn with the 1' structure shown in figure 1 b  a backpropagation neural network with a maximum-likelihood energy function and an apn with theonginal 1-1  struc ture the neural network had three nodes for each of the three-valued nodes in the probabilistic network  and used lo cal coding the number of nodes in the hidden layer was optimized using 1-fold cross-validation the resulls  shown in figure 1  demonstrate the advantage of using the network structure that includes the hidden node 
   the second experiment uses data generated from a network for car insurance risk estimation  figure 1  the network has 

1o 	learning 

1 nodes of which only 1 are observable and over 1 parameters three of the observable nodes are designated as 


figure 1 the output prediction accuracy as a function of the number of cases observed for data generated from the network shown in figure 1 a  the three curves are for the apn algorithm using the network structure in figure 1 b  a back propagation neural network using 1-fold cross validation and the apn algorithm using the correct network structure 

figure 1 a network for estimating the expected claim costs for a car insurance policyholder hidden nodes are shaded and output nodes arc shown with heavy lines 

figure 1 the prediction accuracy as a function of the number of cases observed for data generated from the network shown in figure 1 the two curves are for the apn algorithm using a 1 network and the apn algorithm using the correct structure 
 outputs ' we ran an apn with the correct structure  and an apn with a 1 structure analogous to the 1 network in figure 1 b  the correctly structured apn learns essentially the correct distribution from around 1 cases whereas the 1 a p n requires many thousands of cases to reach the same level  figure 1  1 
   the third expenment uses data generated from the dynamic probabilistic network shown in figure 1 applying the apn algorithm with the correct network structure and using the chain rule extension from section 1 we obtain the learning curve shown in figure 1 for this experiment we used a stochastic simulation algorithm based on likelihood weight-
ing  shachter and peot 1  because this algorithm pro vides anytime estimates of the required probabilities it suits our purposes very well early in the gradient descent process we need only very rough estimates of the gradient and these can be generated very quickly 

figure 1 a simple dynamic probabilistic network modelling a partially observable markov process with reinforcement hidden nodes arc shaded and output nodes are shown with he ivy lines 
1 	conclusions 
we have demonstrated a gradient-descent learning algorithm for probabilistic networks with hidden variables that uses localized gradient computations piggybacked on the standard network inference calculations although a detailed comparison between neural and probabilistic networks  requires more extensive analysis than is possible in this paper one is struck by the fact that the motivations tor the widespread adoption of neural networks as cognitive and neural models-localized learning massive parallelism and robust handling of noisy data-are also satisfied by probabilistic networks furthermore the precise local semantics of probabilistic networks 
allows humans or other systems to provide prior knowledge to constrain the learning process we have demonstrated the dramatic improvements that can be achieved by pre structuring 
     1  for comparison we applied a two layer neural network learning algonthm to the same data using cross validation to find the best size of hidden layer the neural net s behavior is interesting for all sample sizes it converges to predict just the output proportions observed in the training data independent of the particular inputs the same behavior was found for decision trees with pruning and for k nearest-neighbor learning this suggests that the complex patterns found by the structured apn are not discernible by the knowledge free algorithms one might imagine that a neural network structured similarly to the structured apn would be able to overcome this problem there arc two reasons to doubt this first  structure in a neural network represents deterministic functional dependencies rather than the probabilistic dependencies represented by the apn structure second training sparse neural networks with more than a few layers seems to be very difficult although we are currently trying to make it work 
	russell etal 	1 

figure 1 the prediction accuracy as a function of the number of cases observed for data generated from the network shown in figure 1 the curve is for an apn algorithm using the correct structure 
the network especially using hidden variables theoretical analysis of the sample complexity of learning probabilistic networks is an obvious next step we would also like to investigate the use of apns for classification rather than den-
sity estimation this can be done by altering the optimization goal to minimize the error on specified variables  spiegelhalter and cowell 1  detailed empirical comparisons with em gibbs and neural network methods are urgently needed 
   the existence of localized gradient descent algorithms for both adaptive probabilistic networks and back-propagation neural networks is no accident in other work we have established general conditions under which any distributed computational system is amenable to local learning  see also  buntine  1   such results suggest that the class of ab stract models considered in computational neuroscience can be broadened considerably 
