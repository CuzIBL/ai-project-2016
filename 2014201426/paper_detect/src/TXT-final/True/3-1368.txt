
by relaxing the hard-goal constraints from classical planning and associating them with reward values  over-subscription planning allows users to concentrate on presenting what they want and leaves the task of deciding the best goals to achieve to the planner. in this paper  we extend the over-subscription planning problem and its limited goal specification to allow numeric goals with continuous utility values and goals with mixed hard and soft constraints. together they considerably extend the modeling power of goal specification and allow the user to express goal constraints that were not possible before. to handle these new goal constraints  we extend the sapaps planner's planning graph based techniques to help it choose the best beneficial subset of goals that can include both hard or soft logical and numeric goals. we also provide empirical results in several benchmark domains to demonstrate that our technique helps return quality plans.
1 introduction
in classical planning  a given set of conjunctive goals must all be achieved together for the plan to be a success. as planning problems get more complicated with temporal and resource constraints  it is harder to predict which goals are achievable. recently  motivated by the requirements of different nasa planning problems such as planning for airborne  telescopes such as hubble and sirtf  and planning for a mars rover mission  smith  1; 1  introduced the over-subscription planning problem. in this type of problem  the planner is not required to achieve all the goals but instead achieve the best subset of goals given the resource limitations.
　while the ability to represent goals as soft constraints with  fixed  utility values makes it much easier to model goals in many applications  the restriction to only support soft logical goals has limitations in metric temporal planning domains. in this paper  we extend the framework to handle numeric goals having variable utility values and mixed goal sets containing

 
　　this research is supported in part by the nsf grant iis-1 and ibm faculty award.both hard and soft goal constraints. we first present a motivating example before discussing our techniques to handle the new extensions.
mars rover: a rover is sent to mars to collect soil/rock samples and take pictures at different locations. the rover should first land safely  roll off the lander and then go to different locations to gather scientific data. while the objective is to collect as much data as possible given energy and time limitations  only a subset of the data can be gathered. the quality of the plan will be measured by the amount of samples collected at different locations and the amount of picture data transferred back to earth. obviously  more samples and picture data give better values and thus it is more natural to model the goals of collecting them as numeric goals with the utility given according to the amount actually collected.
　the example above brings up two issues:  1  there are certain types of goals that are better represented as numeric goals with a range of utility values  e.g. amount of sample/data ;  1  there are goals that are critical to the success of the plans  e.g. landing safely  and there are goals that are not critical but beneficial  e.g. collecting samples . besides the rover domain  those types of goals are also common in many other real-world planning problems such as transportation or travel planning. we extend the goal structure by allowing numeric goals  e.g. constraints on some continuous quantities  with continuous utility values that depend on the actual value achieved by the final plan. we also strike a middle ground between classical planning and over-subscription goal constraints by supporting both hard  critical  and soft  beneficial  goals. those additional types of goal constraints complement traditional logical goals and allow the user more freedom in expressing what they desire from the final plan.
　while extending the current framework of hard and soft goals to support a mixture of them is not overly challenging  effective handling of the numeric goals with continuous utility values does pose several challenges. in particular  we need to reason about the degree of satisfaction of goals. to illustrate  if the goal is sample − 1 grams  it can be satisfied by sample = 1 as well as sample = 1 at different degrees. we need techniques to:
1. assign utilities to different degrees of satisfaction.
1. track costs of achieving goals of different degrees of satisfaction.
1. use the achievement costs and utilities in combination
to estimate the final plan that maximizes the overall net benefit achievable from a given state.
　we present sapamps an extension of sapaps  do & kambhampati  1; van den briel et al.  1  to support both numeric goals and mixed soft/hard goal sets. sapamps significantly extends the relaxed plan extraction procedure used in sapaps to support numeric goals.
we first briefly discuss the search framework used in
sapaps in section 1. we then show how to extend the cost propagation and relaxed plan extraction routines to handle metric quantities and the combination of soft and hard goals in section 1. we present empirical results on extended versions of some well-known planning benchmark domains in section 1 to show that the new techniques help find larger and better quality plans. we conclude the paper with the related work and discussion.
1 background
we start with the formal definition of the over-subscription  aka partial-satisfaction  planning problem. we then proceed by describing the sapaps  do & kambhampati  1; van den briel et al.  1  planner and its framework that heuristically searches for good quality plans for the problem.
1 over-subscription planning  osp 
to formally define the over-subscription planning problems  smith  1; van den briel et al.  1   the following notations will be used: f is a finite set of fluents and a is a finite set of actions  where each action consists of a list of preconditions and a list of add and delete effects. i   f is the set of fluents describing the initial state and g   f is the set of goals. hence we define a planning problem as a tuple p =  f a i g . the osp problem is defined as follows:
definition osp net benefit: given a planning problem p =  f a i g  and  for each action a （ a a  cost  ca − 1 and  for each goal specification g （ g a  utility  ug − 1: find a finite sequence of actions   = ha1 ... ani starting from ipthat leads to a statep 1 s maximizing the net benefit value g（sg ug   a（  ca  where sg is the set of goals satisfied
in s.
　thus  in contrast to the classical planning problems  in osp no goal achievement is needed to qualify the plan as valid  any sequence of actions is a valid plan . we thus put emphasis on finding good quality plans where the objective function is to maximize the tradeoff between total achieved goal utility and total incurred action cost  both additive .
1 sapaps : heuristic search for osp
sapaps adapts the heuristic  progression  search framework to solve osp problems. the quality of each search node s visited by the a* search algorithm in a forward planner is ranked using the valuep p f s  = g s  + h s  with: g s  = g（sg ug   a（  ca. starting from the initial state i with the net benefit value g i   sapaps keeps track of all the visited states s that have better g s  values than the best

figure 1: rover example and the planning graph
state visited before s. thus  instead of finding a single plan  the algorithm keeps searching for incrementally better plans in terms of the achieved net benefit  g s   value. the heuristic value h s  is calculated by extending the cost propagation over the planning graph routine. it is followed by the relaxed plan extraction process in sapa do & kambhampati  1 . the search stops when the first node in the queue has value h s  = 1  i.e. f s  = g s  . for the rest of this section  we will discuss the three steps in estimating the h s  value. example: figure 1 shows an example in the mars rover domain along with the complete planning graph structure for this example1. the rover initially resides at location l1 and needs to collect samples at l1  and to take a picture of l1. the rover can:  1  move between two locations  mlx ly ;
 1  calibrate the equipment before taking sample/picture  c ;  1  collect a sample at location l  sal ; and  1  take pictures  pil . action execution costs are depicted next to each action in the complete planning graph for the example shown in figure 1. the first action layer of the graph contains three actions with their respective execution costs: cm1 = 1  cm1 = 1  and cc = 1.
cost-propagation over the planning graph: the planning graph propagates the achievement cost of each predicate and execution cost of each action forward starting at the initial level/time-point until fix-point. to simplify the discussion for this section  we will assume that all actions have unit duration.
　intuitively  the heuristic needs to realize that to be able to take the picture at l1 we first need to have the camera calibrated and be at l1. for each action a  its execution cost ca is static and different from the cost to enable its execution cost a   which is the cost to achieve all of its preconditions. thus  cost pil1  = cost ced +cost l1  = cc+cm1 =
1 + 1. we also want the heuristic to capture the fact that the cost to have a picture involves the cost to enable execution of pil1 and the cost to actually carry out that action. thus: cost pl1  = cost pil1 +cpil1 = 1. the propagation rules are:
1. initially  at t = 1:  1   p （ i : cost p  = 1;  1   p /（ i : cost p  = ±;  1   a （ a : cost a  = ±.
1. atp level l:  1   a （ a : cost al  = p（precond a  cost pl 1 ;  1   p （ f : cost pl  =

figure 1: the relaxed plan
minp（effect a cost al 1 
　our heuristic is inadmissible due to the use of sum propagation in calculating the cost to execute each action. as we grow the graph  new actions and facts are introduced and the cost to achieve facts  cost p   and execute actions  cost a   decreases due to new ways to achieve and support them. in figure 1  we highlight the new facts and actions introduced at each level and the new achievement costs for each fact. for example  l1 first appears at level 1 with cost l1  = 1  achievable by m1 . the value of cost l1  decreases to 1 at level 1 due to the new action m1 in level 1. while the set of achieved facts and supporting actions in level 1 and level 1 are the same  we did not stop growing the graph at level 1. this is because cost s1  decreases at level 1 due to the reduction in cost l1  at level 1  which leads to the decrease in cost sa1  .
extracting the relaxed plan: after terminating the costpropagation  the cost values can be used to extract the relaxed plan rp  starting from all achieved goals g  as follows:
1. starting from the last level   g （ g at level l select action a at the action level l 1 that supports g with the lowest cost.
1. when action a is selected at level l  then all preconditions p of a at the previous level will be added to g.
1. stop when: g   i.
　all the collected actions a and the causal links between them make up the relaxed plan rp.
refining the relaxed plan: for each goal g  we build the goal supporting set gs for each proposition p and action a by going backward using the extracted relaxed plan as follows:
   g （ g : gs g  = {g} s
  gs a  =	gs p  : p （ effect a  s
  gs p  =	gs a  : p （ precond a 
　intuitively  for each action a  gs a  is the set of goals that a supports. thus  the achievement of any goal in gs a  depends on the inclusion of a in the relaxed plan while for any goal g /（ gs a   g will still be achievable without a. in figure 1  we show the goal supporting sets for all actions and related propositions  e.g. gs c  = {s1 s1}  and the corresponding utility values  e.g. util s1  = 1  of the three goals in the relaxed plan. for each set sg   g  let asg = a : gs a    sg be the set of actions supporting only goals in sg. we will remove sg along with asg from the relaxed plan if Σa（asgcost a    Σg（sgutil g   i.e. cost   utility .
in our ongoing example  only the goal set s = {s1} can be removed because cost as1  = cost m1  + cost sa1  = 1 + 1   util s1  = 1. figure 1 shows the relaxed plan before and after refinement.
　the net benefit of the final relaxed plan is: util {s1 p1}    cost {m1 c sa1 pi1}  =  1 + 1     1 + 1 + 1 + 1  = 1. this is used as the heuristic value h s  to guide the a* search algorithm. in general  we define rp s  to be the relaxed plan found for state s and u apply rp s  s   to be the utility achieved by applying all actions in the relaxed plan to state s. the heuristic value is then calculated as: h s  =  u apply rp s  s     u s     cost rp s   where rp s  is the final relaxed plan.
1 handling numeric goals with utility
sapaps only supports logical achievement goals of the form g = true  e.g. haspicture l1  . however  if we have the goal to collect at least m grams of a mars soil sample at a given location l  i.e. sample l    m   we can more naturally represent it as a numeric goal.
1 numeric goal representation
unlike logical goals that only have true/false values  there are an arbitrarily large number of values that can satisfy a given numeric goal. we assign a range of continuous utility values for numeric goals to represent a degree of satisfaction. specifically  numeric goals and their utility values are set up as follows:
definition numeric goal: a numeric goal is a relation f （ d in which f is a mathematical formula involving an arbitrary number of numeric variables and d =  l u  is an interval open or closed at either end and is bounded by two real values l ＋ u  l u can be infinity .
　for example  the goal of keeping travel cost between $1 and $1 can be represented as: hotel + airticket （
 1 .
　for each numeric goal g （ d  the utility value u g  is specified by a linear function. for example  if the goal is to collect at least 1 grams of mars sample  i.e. f = sample   1   then the utility of this goal can be u f  = 1  sample
 i.e. it is worth 1 million dollars for each gram of mars' soil if we have at least 1 grams  but 1 dollars otherwise .
1 cost propagation with numeric goals
to incorporate numeric goals into our current heuristic framework  we first have to be able to estimate the cost to achieve them. unlike logical goals  there are multiple degrees of satisfaction for a numeric goal g. therefore  the procedure that tracks the achievement cost for g is necessarily more complicated. specifically:
  for logical values we track the time point tp at which a proposition p can first be achieved  p = true  and the achievement cost for p at time points t − tp. for numeric values we need to track a range of possible values  for a numeric variable vi at each jth update to the lower bound value and kth update to the upper bound value.

figure 1: the rtpg for our example. our actions are defined above it.
  in tracking costs to achieve logical literals  actions are only re-applied  e.g. sa1 at action level 1 in figure 1  when the cost to execute  i.e. cost to achieve their preconditions  them decreases. however  actions having numeric effects on vi need to be applied whenever possible  e.g. sa1 in figure 1  because their effects continue to change the upper    or lower    bound values of the quantity vi.
example: to illustrate the techniques to track achievement costs for numeric goals  we will use a variation of our ongoing mars rover example. we solely concentrate on metric quantities in this example  illustrated in figure 1 . there are two sample-collecting actions: sample1  sa1  collects a single gram of soil sample; sample1  sa1  collects 1 grams of soil sample. the effects of these actions occur at the end of execution. the third action  communicate  com   communicates the sample information to a lander at the start of execution. we use two continuous variables; v1 to track the weight of the collected soil sample in the rover's store and v1 to track the total amount of communicated sample. the goal g is to achieve v1   1 and the goal utility formula is u g  = v1   1  i.e. if v1 ＋ 1 we get a utility of zero  otherwise the utility is found using u g  .
　while the connection between time and numeric goals is not obvious in osp problems  one important component of action cost is the amount of time consumed by each action. goal utilities also normally depend on the time the goals are achieved. like sapaps   the sapamps planner handles actions with different durations and thus we do not make the assumption that all actions have uniform duration  as in previous section .
tracking the upper and lower bounds: the first step in estimating the achievement costs for numeric goals is to track the possible values for numeric variables at different time points. previous work in tracking upper/lower bounds for numeric variables using the planning graph was done for non-temporal planning  koehler  1; hoffman  1; sanchez & mali  1 . there  actions are either always executed serially or marked mutually exclusive of one another
if their numeric effects give varying results when ordered differently. in sapamps   the semantics of the planner disallows interacting actions to be concurrent  whereas the relaxed temporal planning graph  rtpg  allows this. in our context  this means that an action in the rtpg can be applied concurrently with itself an arbitrary number of times  causing the number of time points in the graph to increase significantly. to avoid this problem  we disallow such concurrency as a practical compromise.
　figure 1 shows a rtpg for our example. we re-apply the numeric effects of actions directly after their duration completes. at time t = 1  action sa1 completes and we add the upper bound value u to indicate that the collected weight of the soil sample has increased by 1. also at this time point  the precondition of com can be satisfied by the bound u  so we apply the numeric effects of the action  adding the upper bound u on the weight of the soil sample. at t = 1  sa1 completes and we add the upper bound value u increasing the previous upper bound of v1 the second time by 1 according to the numeric effect of sa1. this continues until we reach our numeric goal v1   1  when   . in figure 1  we show the upper and lower bound values for v1 and v1 as we grow the graph. because we do not have actions that decrease the values of v1 or v1 in this simple example  the lower bound values of those two variables remain unchanged.
　the rtpg handles numeric expressions in effects and preconditions by applying the formulas to each bound. for instance  if the bounds of three variables v1 v1 v1 are v1 :  1   v1 :  1  and v1 :   1  and we want to calculate f = v1 + v1   v1  we first find v1   v1 =  min 1   1   1   1   1   max 1   1   1   1   1   =  1  then f = v1 +  1  =  1 +  1 + 1  =   1 .
tracking achievement costs: we let refer the jth upper or lower bound of vi in the rtpg. the rtpg associates for each bound value  a propagated cost . the cost value estimates how costly it is to achieve a certain numeric value. the idea is that for each value n that satisfies some numeric goal  the tradeoff between the cost of achieving n and the utility that n incurs will be used as heuristic guidance.
　cost propagation is not trivial in the presence of numeric expressions. before turning to this case  let us concentrate our discussion on simple numeric effects  i.e. effects using only constant values . numeric updates  e.g. increases and decreases  will generate a new bound value with respect to the previous one. because of this  we base the cost of each bound on the previous bound's cost . specifically  when an action a adds an upper or lower bound for a variable vi using an increase  +=  or decrease  -=  numeric effect  the propagated cost of the bound is.
this lets us track the cost of executing several actions that may be required to reach a numeric goal or precondition. bounds found using an assign  :=  numeric effect only depend upon the action itself. so  the propagated cost is
. in our example shown in figure 1  the cost of the new bound found by the numeric effect of sa1 at
. when
the numeric effect of sa1 is re-applied at t = 1 and causes the third update on the value of v1 we have
.
　in our ongoing example  each numeric effect and precondition involves only two or fewer variables. however  in a more general scenario  numeric goals  action preconditions and action effects can be a formula involving an arbitrary number of numeric variables. these inter-dependencies between variables further complicates cost propagation on bounds. that is  when we calculate new bound values from expressions  we need to find the cost of the new bound based upon the costs of the values involved in the expression. to do this  we define for each variable vi  a set bvi of all bound values involved in computing a new bound for variable vi. for example  we have an expression f = v1 + v1   v1 and define an effect v1 += f. when applying this effect  we track each of the bounds used to generate the minimum and maximum values from this formula  in this case  -1  1  . we apply the effect using the resulting bounds. for the new upper bound of v1  we have the set bv1 = {uv1 = 1 uv1 = 1 uv1 = 1} to indicate the dependencies between the upper bound of v1 and the particular bound values of v1  v1  and v1 used to achieve this new upper bound.
　during cost propagation  bounds used to satisfy a precondition are included in the cost of an action. for each variable used in expressions  we allow only a single bound of that variable to be included when calculating costs of new bounds  i.e. avoid including more than one bound for each variable when a variable is used both to support a precondition and as part of an expression in a numeric effect . to do this  we let pa be the set of all bound values used to satisfy the numeric preconditions of an action a. in our example  pcom = {uv1 = 1} starting at t = 1. we also define
t
pa bvi to be a set operation over the variables represented by the bounds in pa and bvi  where the result gives us only the bounds in pa that are not equal to the bounds in bvi. more formally .
for instance  if we have the sets
t
andthen pa bvi =
　　　　　. this result provides the means for removing the costs of bounds in pa that are already present in an action  when we also use them in bvi to calculate bound cost. thus  the cost of a bound value of variable vi that is changed by an increase  +=  or decrease  -=  effect of an actionp vi  = cost a  + cost bvj i 1  + pd（bvi pa cost da   is cost bj
　　e（patbvi cost e . it follows that the cost of a bound found by an assign effect  p :=  on actionp a is cost bvji  = cost a  + d（bvi pa cost d    e（patbvi cost e .
　as shown in figure 1  at t = 1 both actions sa1 and sa1 are added to the rtpg. the delayed numeric effect of sa1 increases v1 by 1 at t = 1 and incurs this time point  the precondition v1 − 1 of com is satisfiable and we have. the is included to find cost com . so  when com is put in the graph at t = 1  we have cost com  = 1. its instantaneous effect v1 := v1 leads to a new bound for v1 with
1  = cost com  = 1. at t = 1  sa1's delayed numeric effect is activated and increases v1 by 1 to a new bound with . as we increase the time to t = 1 and t = 1  we keep updating the upper bound of v1 and v1 to values as shown in figure 1. when activating com at t = 1  we have the set representing the bounds used for the expression v1 := v1. we get a new bound that satisfies the numeric goal with
costthe update of com +sacost1 completes  giving u1 = 1  cost ucost1  = 1  = 1uv1 = 1  = 1. at t = 1and
1
at t = 1  the numeric effect of sa1 gives
1.
　notice that even after numeric goals f （  l u  are satisfied by the bound values on l u t lf uf  1=   at t   we allow the rtpg to continue tof at a given time point t  i.e.
expand until fix-point.
relaxed plan extraction with numeric goals: after doing cost-propagation over the rtpg  the cost information can be used to extract a relaxed plan using an approach similar to that discussed in section 1. the challenge here is in deciding for each numeric goal f （  l u   how to select the most beneficial value vf of f that is achievable through the planning graph and extract the action that supports that vf value. when selecting an action a we add all of its logical and numeric preconditions into the goal set g. also  we ensure that the cost of numeric bounds used to satisfy the numeric goal constraints are included with the relaxed plan. this is so we can accurately determine the achievement cost for each bound.
　to handle relaxed plan extraction for numeric goals  we choose the bound values that provide the best tradeoff between goal utility value1 with the achievement cost. thus  for each achievable value  that satisfies the goal constraint on g  we select the one that gives the greatest u vg  cost vg  value. the action a that supports vg is then selected and added to the relaxed plan. if given a goal interval vi （  l u  we never find a value v （  l u  while expanding the rtpg  but do find two values vl   l and vu   u  we say the goal is subsumed. in this case  we allow the first subsuming value to support the goal. however  since we cannot estimate the utility on this bound  we let its utility be 1.
heuristic estimation: for each numeric goal supporting bound that we select  we include the cost to support it and its utility value in the net benefit calculation. in our ongoing example the net benefit of the relaxed plan would be u {u v1 = 1 }     c sa1  + c com  + c uv1 = 1    c uv1 = 1   =  1   1     1 + 1 + 1   1  = 1.
1 combining hard & soft goals
in our work  we support hard and soft goals for both traditional logical goals and the numeric goals discussed in the previous section. in the case of numeric goals  a single goal may involve both hard and soft constraints. for example the goal of having the rover collect between 1 to 1 grams of mars soil  sc = soilcollected （  1    can be modeled so that sc   1 is a hard constraint  i.e. should collect at least 1 grams  and sc   1 is a soft constraint  i.e. 1 is enough but more than that is not harmful .
　to support both hard and soft goals in the best first search framework for over-subscription problem  we need to change both the search-termination criterion and the heuristic estimation routine in the search framework discussed in section 1. specifically:
search node evaluation: when all goals are soft  any visited node encodes a legal plan. in the presence of hard goals  only nodes that satisfy all hard goals can be represented a valid plan.
heuristic estimation:
  for a given node  if we cannot achieve some hard goal when building the planning graph until fix-point for cost propagation  then the node is a deadend.   when refining the relaxed plan  we only remove soft goals and actions  solely  supporting them.
　notice that with the mixed soft/hard goal combination  it's possible to return plans that have negative net benefit  total action cost is higher than the total achieved goal utility . those plans with negative net benefit will not be found if all the goals are soft constraints because an empty plan with zero net benefit is a valid plan in that case.
1 empirical evaluation
we have implemented the support for numeric goals on top of the sapaps planner and also extended its best-first-search framework to support the combination of hard and soft goals. we call the new planner sapamps . to test the implementation  we have created a test suite by extending the problem sets of the mars rover and logistic domains. both were used in previous planning competitions  ipc1 and ipc1 . the experiments aim at testing whether or not the cost-propagation techniques for numeric goals can help sapamps find reasonable size plans with good quality for the extended osp problems.
1 configuration of testing domains
all test suites involve durative actions consuming metric resources. action costs are added to the action representation and appear as a function of the time and resources consumed by that action  e.g. cost travel  = hotelrate   duration travel  + airticket . utilities of goals are randomly generated within some reasonable bounds. logical goals have fixed utilities while utilities of numeric goals are represented using linear functions. the goals have the same probability to be either hard or soft  applied to both logical and numeric goals . specific types of goals in the domains are created with various probabilities. in general the potential number of numeric goals increases with the problem number  with a minimum of at least one numeric goal.
　the rovers domain is extended and includes the weight of collected soil and rock samples. the goals are to send analyzed data about samples at different locations back to the lander. utility of a sample is proportional to the weight collected for the sample. rover's store has a fixed weight capacity and each time a sample action is executed  a single gram is stored. if the rover keeps collecting sample  then the cost of sampling outweighs the overall utility of the collected sample at some certain time.
　in the logistics domain  we add numeric goals to deliver a certain number of packages of the same type to its destination using trucks and airplanes. the initial state is extended

figure 1: comparison of utilities for our rovers domain
to represent numeric quantities for the number of packages at each location and the maximum capacity of each vehicle. goals are then intervals of the number of packages we want at a particular location. for example  we may start with 1 packages at location l1 and want to deliver between 1 and 1 packages to l1 and at least 1 packages to l1. delivering a package to a certain location may be more beneficial than to the other  e.g. delivering 1 packages to l1 gives more utility than delivering the same number of packages to l1 . the cost of delivering packages varies depending upon the distance between locations.
1 results
all results were collected using a 1ghz p1 machine with  1
gb of ram and 1 mb allocated to the planner. given that
sapamps employs an anytime search algorithm and continues to look for better quality plans as it is given more time  we set the time limit of 1 seconds for all the tests1. we compare the final plan quality  i.e. the cumulative utility of goals achieved minus the cumulative action cost  on two heuristic approaches:  1  based on the cost-propagation as described in this paper;  1  only propagate the upper/lower bounds on numeric variables to check for numeric goal satisfaction  but do not propagate the goal achievement cost. the plan qualities are measured by the total achieved goal utility minus the total action cost. we were unable to make comparison with external planners because we are not aware of any other planner that is capable of solving the same problems.
　figure 1 shows the comparison results between two approaches listed above for the rovers domain. the results clearly show that the cost-propagation helps sapamps return better quality solutions in all but 1 problems  with one of lower quality . the average improvement on the solution quality is 1 times better. the plans found are of reasonable size with a average/longest plan length in terms of number of actions is 1/1  with cost propagation  and 1/1  without cost propagation .
　figure 1 shows the results for the logistics domain. within the time limit  sapamps solves 1 of 1 problems with costpropagation while only 1  all with lower quality  without cost-propagation. among the 1 problems that both approaches can solve  the average improvement in plan qual-

1 1 1 1 1 1 1 1 1 1
problems
figure 1: comparison of utilities for our logistics domain
rovers utility  problem 1

figure 1: plan net benefit as a function of time.
ity is 1 times. the average/longest plan length in terms of number of actions is 1/1  with cost propagation  and 1/1  without cost propagation  in this domain.
　the results in these two domains confirm that the costpropagation technique is indeed useful in helping sapamps find better quality and larger plans  compared to only doing bound propagation.
　figure 1 shows how quickly the utility of a typical problem is found  in this case  problem instance 1 of rovers . since we are using an anytime a* search framework  the utility increases gradually during search. the cost propagation heuristic over numeric bounds enables the planner to find greater utility plans.
1 related work
our primary focus in this paper was to develop effective heuristics for handling numeric goals in the context of oversubscription planning. as we mentioned in the introduction  the recent interest in over-subscription planning has lead to the development of several algorithms  van den briel et al.  1; smith  1 . none of these approaches can handle numeric goals  and thus do not address the challenges involved in deciding the degree of satisfaction of a goal. earlier work on the pyrrhus planning system  williamson & hanks  1  did consider the issues of handling goals that allow different degrees of satisfaction in terms of achievement time. however  pyrrhus did not have effective heuristics for directing its search.
　as discussed in the paper  our work is also related to existing efforts on cost propagation over planning graphs  c.f.  do & kambhampati  1   and propagating reachability information over numeric goals  c.f.  sanchez & mali  1; hoffman  1  . our method of propagating the upper and lower bounds of numeric variables over the rtpg has its roots in work done by koehler in the metric-ipp planner
.
1 conclusion
many real-world planning scenarios are over-subscribed  and require that the planner carefully balance the utility of the achieved goals and the cost of actions in the plan. in this paper we focused on extending over-subscription planning to handle numeric goals as well as a mix of hard and soft goals. our primary technical contributions involved effective approaches for reasoning about the expected net benefit of a partial plan in the presence of numeric goals that allow different degrees of satisfaction  with noncomitant utilities . we have empirically demonstrated the effectiveness of our heuristics in leading the planner to solutions with higher net benefit. for the future  we are investigating extensions to our heuristic to handle delayed satisfaction of goals  i.e. goals whose utility depend upon achievement time .
