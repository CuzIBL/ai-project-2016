 
we propose a formal framework for modelling case-based inference  cbi   which is a crucial part of the case-based reasoning  cbr  methodology. as a representation of the similarity structure of a system  the concept of a similarity profile is introduced. this concept makes it possible to formalize the cbr hypothesis that  similar problems have similar solutions  and to realize c b i in the form of constraint-based inference. in order to exploit the similarity structure more efficiently  a probabilistic generalization of the constraintbased view is developed. this formalization allows for realizing c b i in the context of probabilistic reasoning and statistical inference and  
hence  makes a powerful methodological framework accessible to cbr. within the generalized setting  a  formalized  cbr hypothesis corresponds to the assumption of a certain stochastic model  and a memory of cases can be seen as statistical data underlying the inference process. as a particular result we establish an approximate probabilistic reasoning scheme which generalizes the constraint-based approach. 
1 introduction 
the approach of case-based reasoning  cbr  relies on the hypothesis that  similar problems have similar solutions.  this assumption  which we subsequently refer to as the   c b r hypothesis   is the guiding principle underlying most cbr systems. until recently  however  there have been only few attempts at formalizing this hypothesis  dubois et a/.  1; esteva et a/.  1; plaza et a/.  1  and  thus  at making an important step toward a theoretical foundation of cbr. 
　in this paper  we develop a formalization in which we proceed from a constraint-based interpretation of the 
c b r hypothesis  according to which the similarity of 
   *this work has been supported by a tmr research grant funded by the european commission. 
1 	case-based 	reasoning 
problems imposes a constraint on the similarity of associated solutions in form of a lower bound  dubois et a/.  1 . we then propose a generalization of this ap-
proach  according to which the similarity of problems allows for deriving conclusions about the  conditional  probability distribution of the similarity of associated solutions. a probabilistic formalization of the c b r hypothesis according to which  similar problems do probably have similar solutions  seems appropriate since it 
emphasizes the heuristic character of c b r and is suitable for modelling the  exception to the rule.  
　the formal model we are going to propose is not related to the complete methodological framework of casebased reasoning in the sense of the so-called   c b r cycle.  rather  we focus on c b r as case-based inference  cbi   which essentially corresponds to the reuse process within the  informal  r1 model of the c b r cycle  aamodt and plaza  1 . more precisely  we emphasize the idea of case-based reasoning as a prediction method  dubois et a/.  1; faltings  1   which is close to the idea of instance-based reasoning  aha et a/.  1 . according to this point of view  the main task of c b i is to exploit past experience in the form of observed cases in order to predict or characterize the solution of a new problem. in this narrow sense  c b r may not even cover the complete process of problem solving  i.e.  the process of ultimately finding the solution  but may only constitute the first part thereof. typically  the characterization of the solution provided via c b i will be utilized by methods applied in subsequent stages of the overall problem solving procedure. indeed  these subsequent stages  which roughly correspond to the revise part of the r1 model  are often not directly  case-based  but make use of domain-specific knowledge or user input. 
　according to the point of view represented above  c b i has important aspects in common with statistical methods  and  more generally  with approaches to machine learning. an important difference  however  concerns structural assumptions about an underlying data generating process  which machine learning algorithms are based on  and which are generally represented in form of a hypothesis space h. as a simple example consider the case where h is given as a class of functions 
. each of these functions corresponds to a 

certain hypothesis which is considered for explaining observed data . as in this example  
the hypotheses h typically relate properties  attributes  
of the instances . the c b r hypothesis  however  concerns the  supplementary  concept of similarity  which can be seen as a derived property related to pairs of instances. thus  c b r makes structural assumptions about the data generating process not directly at the system or instance level but at the  say  similarity level. seen from this perspective  the process of c b i should mainly take place in some kind of similarity space instead of instance space  such as  e.g.  instancebased reasoning.  consequently  a formalization of c b i   whether probabilistic or not  should proceed from this level. 
　the remaining part of the paper is organized as follows: section 1 introduces the basic framework of casebased inference. c b i is realized in the form of constraintbased inference in section 1. section 1 is devoted to the probabilistic generalization of this approach. the paper is concluded with a summary in section 1. 
1 the basic framework 
within our framework  the primitive concept of a case is defined as a tuple consisting of a situation and a result or outcome associated with the situation.1 definition 1   c b i set-up . a c b i set-up is defined as a 1-tuple 

where *s is a countable set of situations  r is a set of results  and   assigns results to situations. the functions 
define similarity measures over the set of situations and the set of results  respectively. m is a finite memory 

of cases 	
　we do not make particular assumptions concerning the characterization of situations or results  which will generally be marked using an attribute-value representation. as far as similarity measures are concerned we suppose them to be reflexive  symmetric and normalized in the sense that degrees of similarity are elements of the unit interval  1   where a value of 1 corresponds to perfect similarity. observe that card  implies the same to be true for the sets 
  
of actually attained similarity degrees. 
　clearly  the assumption that a situation determines the associated outcome does not imply that the latter is known as soon as the situation is characterized. for example  let situations correspond to instances of a class of 
     1 we prefer these slightly more general expressions to the terms  problem  and  solution.  
combinatorial optimization problems. moreover  define the result associated with a situation as the  unique  optimal solution of the respective problem. deriving this solution from the description of the problem might involve a computationally complex process. in this connection  we refer to case-based inference as a method supporting the overall process of problem solving by predicting the result associated with a certain situation. to this end  c b i performs according to the c b r principle: it exploits experience in form of precedent cases to which it 
 applies  background knowledge in form of the heuristic c b r hypothesis. 
definition 1   c b i problem . a c b i problem is a tuple consisting of a c b i set-up ♀ and a new situation the task is to exploit the similarity structure1 of in conjunction with observed cases in order to predict resp. characterize the result associated with s1. 
1 cbi as constraint propagation 
the hypothesis of ''similar situations having similar results  is interpreted in this section from a constraintbased point of view  according to which the similarity of situations constrains the similarity of the associated results  at a minimum level.  
definition 1  similarity profile . for a c b i set-up the function 
　the similarity profile  is the  fingerprint  of the system  at the similarity level and  partly  represents the similarity structure of the set-up  it can be seen as a condensed representation of knowledge concerning the system structure . indeed  the domain and the range of  are one-dimensional  whereas s and r will generally be of higher dimension. consequently  a hypothesis related to the similarity structure of a system will generally be less constraining than a hypothesis related to  directly. on the other hand  a similarity profile has a relatively simple structure which facilitates the formulation  derivation  and adaptation of hypotheses. 
definition 1  similarity hypothesis . a similarity hypothesis is identified by a function h :  1  -   1   and similarity measures i the intended meaning of the hypothesis h  or  more precisely  the hypothesis  is that 
		 1  
holds true for all  a hypothesis h is called stronger than a hypothesis h! if h!   h and we say that a c b i set-up satisfies the hypothesis h if 

　a similarity hypothesis h is thought of as an approximation of the similarity profile h and can be interpreted 
	＊ 	  . 
1 this expression will be defined formally in section 1. 
	hullermeier 	1 

as a quantification of the c b r hypothesis for the set-up 
 this hypothesis is often formulated as  the more similar two situations are  the more similar are the corresponding results.  returning to the concept of a similarity profile such as introduced above  it becomes obvious that this formulation implicitely makes a stronger assumption than the formulation used at the beginning of this section. namely  it suggests the function  associated with a set-up  to be increasing  or at least non-decreasing. we  therefore  call h a strict hypothesis if it is a non-decreasing function. 
   now  consider a cbi problem   and suppose that satisfies the hypothesis h. if the memory m contains a 
case  s  r  such that s = so  the outcome  = r can simply be retrieved. otherwise  we can derive the following restriction: 
 1  
where the -neighborhood of a result r r is defined as the set of all outcomes which are at least a-similar to r:   1  
thus  in connection with the constraint-based view  the task of c b i can be seen as one of deriving and representing the set ch in  1   or an approximation thereof. this may become difficult if  e.g.  the definition of the similarity r and  hence  the derivation of a neighborhood is complicated. the sets  1  may also become large  in which case they cannot be represented by simply enumerating their elements. 
　　in the context of c b i it must generally be assumed that the similarity profile  of a c b i set-up is unknown. consequently  we cannot guarantee that satisfies a certain hypothesis h. nevertheless  taking for granted that h is indeed a  more or less  good approximation of   it seems reasonable to utilize it for deriving a set ch according to  1  as an approximation of 
 while keeping the hypothetical character of h in mind.  
this situation  which reflects the heuristic character of 
c b i as a problem solving method  is closely related to the aspect of learning. within our framework  one reasonable way of realizing case-based learning is that of finding a good approximation of a similarity profile an obvious approach  for instance  is to start from a hypothesis space h and to look for the most favorable  e.g.  strongest  among those hypotheses  which are consistent with the memory m in the sense that  1  is satisfied for all 
remark 1. according to  1   the set ch depends only on the relation of degrees of the  linearly ordered  similarity scales ds and dr  as specified by the hypothesis h. hence  the measures  and  can be considered as ordinal concepts. 	
　the overall c b i process  as introduced in this section  is illustrated by figure 1: 
  in a first step  the problem  is characterized at the similarity level by means of its similarity structure  consisting of the similarity profile h resp. a cor-
responding hypothesis h and the similarity structure 
1 	case-based reasoning 

zs = sstr .m so  of the  extended  memory  m so . 
the latter can be thought of as the set of values 
. 	in fact  	  resp. zs can be 
seen as the  image  of the system resp. the  extended  memory under the transformation defined by the similarity measures  and 
  the main step of the c b i process is then to utilize the similarity structure of the problem for constraining the unknown outcome  at the similarity level. the corresponding constraints c are implicit in the sense that they refer to the derived  bilateral  property of similarity  not to the result itself. 
  by applying the function which is inversely related to   to the 
observed outcomes  k = 1   . . .   n   the similarity constraints c are transformed into constraints on outcomes  which are combined via  1  to a constraint  resp. ch at the system level. 
remark 1. two characteristics of c b i   as introduced above  are worth mentioning. firstly  c b i is indirect in the sense that constraints on outcomes are obtained via constraints on similarity degrees . secondly  
c b i is local in the sense that the rules  1  associated with a hypothesis h derive evidence concerning the value r1 from single cases only. these pieces of evidence still have to be combined in order to obtain the constraint implied by the complete memory m. within the deterministic framework of this section the combination of evidence is simply accomplished by  1 . 
   of course  the more  convenient  the similarity structure of a set-up  is  the more successful c b i will be. within our framework  we have quantified this convenience  i.e.  the degree to which the c b r hypothesis holds true for the set-up   by means of the similarity profile . this quantification  however  may appear rather restrictive. the existence of some  exceptional  pairs of cases  for instance  might call for small values  of the similarity profile  in order to guaran-
tee the validity of  1 . then  the predictions  1  which reflect the success of the c b i process might become imprecise even though the similarity structure of  is otherwise strongly developed. this motivates the probabilistic generalization of the constraint-based approach as proposed in the next section. 

1 cbi as probabilistic reasoning 
1 basic probabilistic concepts 
we now extend the definition of a c b i set-up such that the set s of situations is endowed with a probability measure   defined on 1s.  this measure models the occurence of situations. thus  we assume that the situations resp. the associated cases which constitute the memory m are chosen repeatedly  and independently  according to  the assumption of this kind of proba-
bilistic setting is a typical one for  e.g.  machine learning  and can also be found in recent  more formally oriented  approaches to c b r  bergmann and wilke  1 . it is less restrictive than it might appear and should  therefore  not be overrated. in connection with probabilistic reasoning schemes  for instance  it will generally not be necessary to make explicite assumptions about us. 
　　now  consider a c b i problem  with  1  the memory of the set-up according to our assumption  the sequence of involved situations can be seen as the realization of a corresponding random sequence characterized by the measure 
		 1  
this measure defines the  discrete  probability space  underlying the c b i problem. since c b r is particularly concerned with modelling the  similarity  relation between two cases let us consider a random tuple  of situations. the random variable z =  x y   where x is the similarity  of the situations and y denotes the similarity of the associated outcomes  is then defined on the probability space as the mapping 
r
x~  x  and  to denote corresponding conditional probabilities. moreover  we make use of intuitive notations such as 
definition 	1 	 probabilistic 	similarity 	profile . 
consider a c b i set-up  denote the 
class of probability measures over  1  l .1 the function 

is called the probabilistic similarity profile of 	  
　　the probabilistic similarity profile h provides a much more precise picture of the similarity structure of a c b i set-up  than a  deterministic  profile does. for each degree of similarity ds it specifies the probability distribution of the similarity of results  i.e.  of the random variable y  given 
1
　　this means that m is actually a sequence of not necessarily different cases. 
1
that is  the class of measures on with b the 
borel  1-field over  1 . 
  
that the similarity of two situations is x. compared to this  the function specifies only the lower bound inf 
definition 1  stochastic dominance . for a probability measure   define the decumulative distribution function :  1   1  by 
. we write 	and say that the measure 
dominates the measure stochastically if definition 1  probabilistic hypothesis . a probabilistic!  similarity  hypothesis is identified by a function h : . l e t b e a c b i set-up with 
probabilistic similarity profile satisfies the hypothesis h if dominates h x  stochastically for all x ds  h is called strict if for all 
hypothesis h' is called stronger than for all 	and 
for at least one 
　the over p  1   is a natural generalization of the -relation over  1 . again  a c b i set-up  satisfies a hypothesis // if the latter is  pessimistic1' 
enough in the sense that it never over-estimates the probability that  for any   the degree of similarity of the outcomes associated with two situations is equal to or larger than  the strict version of the c b r hypothesis now means that  the more similar two situations are  the larger is the probability that the associated results are at least -similar.'' 
　let us finally introduce the concept of a generalized hypothesis which turns out to be useful in connection with  approximate  probabilistic inference  cf. section 1 . definition 1  generalized hypothesis . a gener-
alized  similarity  hypothesis is identified by a function  denotes the class of 
normalized uncertainty measures  fuzzy measures  over 
 1   i.e.  the class of measures r; such that 	- 1  
　in section 1  a generalized hypothesis g will be used for modelling upper bounds of probability measures. thus  it is associated with rules of the form 

whereas a probabilistic hypothesis h defines rules 
		 1  
1 	p r o b a b i l i s t i c inference 
it has already been mentioned that c b i is not based directly on the information provided at the system level. rather  the concept of similarity  quantified in form of similarity functions  and   is exploited in order to transform this information and to represent it at the similarity level. the following definition specifies this kind of transformation. 

	hullermeier 	1 

. we generally assume  to be given and simply call sstr .m so  the similarity structure of 
. in order to take information about observed results into account  we define the outcome structure as ostr 
　　the values ' and xj define realizations of corresponding random variables and xj. 
we combine these variables into one vector zs- hence  the similarity structure of a c b i problem can be thought of as a random variable zs defined on the probability space . in the same way  a random vari-
able zo is associated with the outcome structure of a c b i problem. 
　the structure zs can be seen as statistical data at the similarity level. the measure  1  determines the probability of the occurence of such structures completely. 
thus  given the  observation    it is principally possible to derive  conditional  probabilities such as  e.g.  
		 1  
or the likelihood  of results  in connection with the unknown outcome  which is now treated as a random variable  the indirect and local character of c b i  cf. remark 1 and figure 1   however  has important consequences for probabilistic reasoning. firstly  results such as  1  will not be derived directly. rather  evidence concernig .  is obtained indirectly via evidence concerning similarity vec-
tors   where 
secondly  the local inference rules  1  do not allow for the direct specification of probabilities such as p v =   which condition on a complete similarity structure as an event. this leads to the problem of combining probabilistic evidence derived from different cases. due to the stochastic dependency of the random variables which constitute a similarity structure  this problem is more involved than the combination of evidence in the deterministic approach of section 1. thirdly  the transformation of a measure over  in accordance with  does not necessarily yield 
a  unique  probability measure over r. 
　as a consequence of the characteristics just mentioned  probabilistic c b i will often be approximative. we can 
establish  for instance  the following result.1 
proposition 1. consider a c b i problem  and 
suppose the generalized hypothesis g to satisfy g x    
	  where 	denotes 
the upper envelope of the measures 	moreover  
 1  
is an upper approximation of the likelihood function 

     1  the proof of this result can be found in the full version of this paper  hiillermeier  1 . 
1 	case-based reasoning 
figure 1: illustration of the probabilistic c b i process. 

　this proposition provides a natural generalization of the constraint-based inference scheme  1 . indeed  we recover  1  as a special case of  1  with 
expression  1  also suggests to represent a generalized hypothesis g as a parametrized class of fuzzy measures such as  e.g.  possibility measures and  hence  to realize probabilistic c b i by means of fuzzy set-based approximate reasoning techniques. 
　figure 1 provides an overview of the probabilistic approach to c b i . essentially  this approach realizes a process of probabilistic reasoning in similarity space. the similarity structure  or the outcome structure  of a c b i problem plays the role of the statistical data. a hypothesis h defines the stochastic model which explains the occurence of such structures and which underlies the reasoning process. the task of case-based learning  understood as the estimation of the similarity profile h‘  thus corresponds to statistical inference. 
1 summary 
the main concern of this paper was to establish a framework for modelling c b i   rather than to derive formal results related to particular inference schemes. the following points deserve mentioning: 
  we have introduced a formal framework in which the task of case-based inference has been defined as one of predicting resp. characterizing the outcome ro associated with a new situation s1. the distinction between reasoning at the instance level and reasoning at the similarity level has been emphasized. 
  we have adopted a constraint-based view of c b i   according to which the c b r hypothesis imposes constraints on the relation between the similarity of situations and the similarity of corresponding outcomes. 
  the concept of a similarity profile has been introduced  which establishes a connection between the instance level and the similarity level  and represents the similarity structure of a c b i set-up. a similarity hypothesis is thought of as an approximation of the similarity profile and  hence  defines a quantification of the c b r hypothesis. it allows for realizing a constraint-based inference procedure. 
  a probabilistic generalization of the constraint-based 

approach has been proposed. the similarity structure is now represented by means of a probabilistic similarity profile. it allows for replacing the constraint-based inference scheme by more general probabilistic reasoning procedures based on  e.g.  the derivation of conditional probabilities or likelihood functions. an exemplary result concerning the approximation of a certain likelihood function has been established. 
　finally  we would like to point out some additional aspects and make some remarks concerning related and future work. 
  it should be noted that the probabilistic formalization developed here does not rely on very specific assumptions but emerges quite naturally as a generalization of the constraint-based approach in connection with the probabilistic modelling of the occurence of situations. it should also be stressed that this formalization does not correspond to a particular inference scheme. rather  it provides the basic concepts for  translating  a c b i problem into one of probabilistic reasoning and case-based learning into statistical inference. this way  it makes the powerful methodology of statistics accessible to cbr. the investigation of particular inference schemes such as  e.g.  those emerging from proposition 1  is an important aspect of further research. 
  the probabilistic point of view and the idea of  proba-bilistic  constraint-based inference guarantee for a clear semantics underlying our approach to cbi  in which similarity should be seen as an essential but at the same time auxiliary concept. indeed  the inference procedure prin-
cipally works with all pairs of similarity functions  and 
  each of which defines a certain similarity structure. of course  the more suitably these functions are chosen  the better the inference results will be. the interpretation as an auxiliary concept contrasts with other formalizations of c b i  dubois et al  1; esteva et al  1; plaza et al  1   in which similarity is awarded a considerable semantical meaning. 
  probabilistic models  particularly bayesian networks  have been used in connection with case-based reasoning by several authors  see  e.g.   aha and chang  1  . as main differences between most of these approaches and our work let us emphasize two points. firstly  the concept of similarity is often derived from that of probability or vice versa. in  rodriguez et al  1   for instance  similarity is interpreted as a certain probability related to a classification task. instead  probability and similarity are treated as different concepts in our model. secondly  a probabilistic model is often related to the features  attributes  of cases  at the system level  directly  whereas our formalization proceeds from the similarity level. for example  the problem considered in  rodriguez et al  1  is to estimate the  conditional  probability  for a case c =  to belong to category 
cj as a function of the features fk. seen from this perspective  such approaches are rather case-based  whereas our approach is similarity-based: inference results are derived from similarity structures at the similarity level instead of cases at the system level. 
  the question of how to define suitable hypotheses for concrete applications is an important topic of future work. closely related with this question is the realization of case-based learning as the adaptation of hypotheses. furthermore  it seems interesting to explore the relation between probabilistic inference and  fuzzy set-based  approximate reasoning mentioned in section 1.   in this paper  c b i utilizes the complete memory m. often  however  one will only take the most similar case s  into account. the approach should therefore be generalized in this direction. 
  the concepts developed in this paper have already been applied successfully to  repititive  combinatorial optimization problems. the thorough investigation of such applications is a further topic of future work. 
