 
word sense disambiguation  wsd  is the process of distinguishing between different senses of a word. in general  the disambiguation rules differ for different words. for this reason  the automatic construction of disambiguation rules is highly desirable. one way to achieve this aim is by applying machine learning techniques to training data containing the various senses of the ambiguous words. in the work presented here  the decision tree learning algorithm c1 is applied on a corpus of financial news articles. instead of concentrating on a small set of ambiguous words  as done in most of the related previous work  all content words of the examined corpus are disambiguated. furthermore  the effectiveness of word sense disambiguation for different parts of speech  nouns and verbs  is examined empirically. 
1 introduction 
the meaning of a word may vary significantly according to the context in which it is used. for instance the word  bank  will have a completely different meaning in financial text than in geological text. this is a case of a clearly identifiable sense distinction  but there are cases where different senses of a word may be harder to distinguish  e.g.  bank  as a financial institution and as a 
building. both senses are likely to appear in the same context and one needs to take into account the details of their use  in order to distinguish between them. the process of distinguishing between different senses of a word is called word-sense disambiguation  wsd . word-sense disambiguation is necessary for a number of tasks in natural language processing  nlp   such as machine translation  query-based information retrieval and information extraction. 
　in general  the rules for distinguishing between the senses of different words differ. for instance  a valid disambiguation rule for the senses of the word  bank  would examine the occurrence of the words  river    fi-
1 	machine learning 
nancial   etc. in the context of the ambiguous word. this evidence would be completely irrelevant for most other words. thus the disambiguation rules are in general word-specific. furthermore  it is difficult to construct such rules manually  especially when the difference between the senses is not great  e.g.  bank  the institution and the building. for this reason  the automatic construction of disambiguation rules is highly desirable. one way to achieve this aim is by applying machine learning techniques to training data containing the various senses of the ambiguous words. 
　the machine learning method used here belongs in the class of symbolic supervised machine learning  requiring that the training texts are hand-tagged with the correct senses for ambiguous words. an important aspect of the work presented here  as compared to similar previous work  is that all content words  rather than a handful of them  in the training texts are subject to disambiguation. this step towards large-vocabulary disambiguation is necessary if wsd systems are to be used in practice. however  the automatic construction of large-vocabulary disambiguators is hard  due to the sparseness of the training data for each individual word. one of the issues examined in this context is the construction of simple general rules that apply to all words  capturing regularities in less frequent words in the data. 
　another important issue that we examine is the effectiveness of word sense disambiguation for different parts of speech  nouns and verbs  and the ability to learn disambiguators for each of those two word-types. the learning algorithm is applied separately to verbs and nouns and the results are compared. 
　section 1 presents related work in wsd. the wsd task  as this is realised in our approach  is presented in section 1. our experiments  i.e.  experimental setup and results  are presented in section 1. finally  in section 1  we summarise the work and present our future plans. 
1 	related work 
early efforts in automating the sense disambiguation task made use of machine-readable dictionaries  mrds  and thesauri  which associate different senses of a word with 

short definitions  examples  synonyms  hypernyms  hyponyms  etc. a simple approach of this type is to compare the dictionary definitions of words appearing in the surrounding text of an ambiguous word with the text in the definition of each sense of the ambiguous word in the dictionary. clearly  the higher the overlap between the dictionary definitions of the surrounding words and the definition of a particular sense of the ambiguous word  the more likely it is that this is the correct sense for the word. some of the methods that are based on mrds and thesauri are presented in  lesk  1; wilks  et al  1; cowie  et al.  1 . the resources that are commonly used in these studies are: the wordnet  longman's dictionary of contemporary english  ldoce   roget's thesaurus and collins english dictionary  cde . 
a more thorough account of this work can be found in  ide and veronis  1 . 
　despite the useful information that they contain  mrds and thesauri are often inadequate for wsd  e.g. mrd sense definitions are often non-representative of the context in which the sense is met. as a result  the focus of wsd research has recently turned to corpusbased methods. according to this approach  a corpus of text is used as training data for the construction of disambiguation rules for different words. the construction of these disambiguation rules is achieved by a variety of machine learning methods. 
　an important distinguishing feature for machine learning methods is the extent of supervision provided for training. supervision is provided in the form of handlabelling the examples that are used for learning. in the case of wsd  a fully supervised method requires that all occurrences of an ambiguous word in the training text be labelled with the correct sense. the sense labels are typically taken from a dictionary. given this information  a supervised learning algorithm constructs rules that achieve high discrimination between occurrences of different word-senses. examples of supervised learning methods for wsd appear in  black  1; gale et a!.  
1; leacock et al  1; yarowsky  1; towell and voorhees  1 . the learning methods used in those studies are general-purpose  including: decision-tree induction  decision-list induction  feed-forward neural networks with backpropagation and naive bayesian learning. their results are very encouraging  exceeding 1% correct sense labelling in some cases. 
　however  this high disambiguation rate is achieved at the expense of disambiguating only a small number of words. in all of the above-mentioned studies only a handful of words are included in the evaluation experiments and for each of these words a sufficient number of examples are provided  covering all senses of the word. this is an unrealistic scenario  when aiming to construct a system to be used in practice. the results presented here are on a much larger scale  considering all content words of a corpus. a similar approach has been adopted by the system that won the senseval competition1 and is presented in forthcoming work  hawkins and nettleton  1 . despite the fact that the senseval competition did not involve large-scale disambiguation  the system presented in  hawkins and nettleton  1  is designed to deal with a large number of words  each represented by a small number of examples. for this purpose it has been evaluated on the semcor corpus  which contains about 1 content words  achieving 1% accuracy on low-level wordnet senses. the low accuracy figure  in conjunction with the fact that the same system won the senseval competition  illustrates the difficulty of largevocabulary disambiguation. 
　in addition to the supervised approaches to learning wsd systems  unsupervised learning has been used for the same purpose  which does not require hand-tagging of the training data  e.g.  yarowsky  1; leacock et al.  1; schutze  1 . as expected  the performance of the unsupervised learning approaches is lower than that of their supervised counterparts. however  performance evaluation of unsupervised learning methods is not straightforward  as there are no correct tags against which to compare the results of the disambiguation. 
　a compromise solution between supervised and unsupervised learning is the use of a small number of tagged examples  together with a large set of untagged data. such partially supervised learning methods are presented in  yarowsky  1; towell and vorhees  1   using rule-learning and neural networks respectively. 
　an important issue for any wsd learning algorithm is what features will be used to construct the disambiguation rules  i.e.  what evidence is relevant for wsd. since syntactic information is not considered useful for hard wsd tasks  the evidence commonly used consists of words that can be found in the neighbourhood of the ambiguous word. the question that arises then is how large this neighbourhood ought to be  i.e.  how broad a context is needed for disambiguation. according to this criterion  the wsd methods in the literature can be divided into two large groups: local and topical wsd. in local wsd only the close neighbourhood of the word   1 words on each side  is used. topical methods on the other hand use a larger context window    1 words on each side . none of the fairly recent approaches presented above uses purely local information. yarowsky  and schutze  present purely topical methods  but in both papers the value of local information is noted. most of the recent approaches  e.g.  yarowsky  1; towell and voorhees  1   combine local and topical information  in order to improve their performance. another interesting claim is that different sizes of context window are effective for different parts of speech. noun senses seem to be dependent on topical information  while verbs and adjectives are better disambiguated using local information  yarowsky  1 . 
1  senseval was the first competition for wsd systems. for more information see  kilgariff  1 . 
paliouras  karkaletsis  and 	spyropoulos 	1 

rank   rather than building complex disambiguation rules  using the collocates. this combination of general and word-specific disambiguation is desirable for largevocabulary wsd. 
1 results on nouns and verbs separately 
another issue examined here is the different behaviour of disambiguators for words of different part of speech  verbs and nouns . out of the 1 examples in the complete dataset  1 are verb-cases  and 1 are noun-cases. the remaining 1 examples correspond to adjectives and adverbs. 
　the 1 verb-cases represent 1 occurrences of 1 different verbs. thus  ldoce polysemy in this subset of the data is 1=1 and average word repetition 1=1. the base case performance of choosing the most frequent sense is shown in table 1. 

table 1: the base case for verbs only. 
the base case results in this case are better than those in the complete dataset  suggesting an easier disambiguation problem. this is in accordance to the lower polysemy value. however  average word repetition is considerably lower than before  making teaming more difficult. 
　figure 1 shows the performance of c1 on this reduced problem. in comparison to the results in figure 1  recall has improved slightly  while precision has decreased considerably. overall  there is little improvement over the base case for all three measures. 

figure 1. performance of c1 on verbs only. 
the 1 noun-cases represent 1 occurrences of 
1 different nouns in the text. thus the polysemy in the dataset is 1/1.1 and the average word repetition is 1=1. both values are close to those in the complete dataset  since the noun-cases correspond to a large proportion of the dataset. the polysemy is larger than for verbs  suggesting a difficult disambiguation task. however  word repetition is also higher than for verbs  suggesting that learning can do better in this problem. 
the base case for the naive most-frequent-sense rule is shown in table 1. 

table 1: the base case for nouns only. 
according to all measures  this problem seems harder than the disambiguation of verbs. the results for the base case are in accordance with the higher polysemy. 
　figure 1 presents the performance of c1 for noun disambiguation. as expected  the results in this experiment are similar to these for the whole dataset. the main difference is the level of recall  which is considerably lower. this can be explained by the removal of adjectives and adverbs from the dataset  for which almost 1% recall is achieved. compared to the results for verb disambiguation  recall is lower  but precision is higher. thus  it is difficult to draw a conclusion about whether verbs or nouns are disambiguated better. however  in terms of learning the results are much better for nouns than for verbs  since there is an improvement over the base-case results. 

figure 1. performance of c1 on nouns only. 
1 	concluding remarks and further work 
machine learning algorithms are a promising approach to the automatic construction of word sense disambiguators. we examined a symbolic supervised learning technique  c1  which requires that the training texts are handtagged with the correct senses for ambiguous words. the learning algorithm was evaluated on financial news articles from the semcor corpus. the textual data were translated into feature-vector examples  as needed by the learning algorithm. 1-fold cross-validation was used to gain an unbiased estimate of the performance of the algorithm. two experiments were carried out: one using all content words and one examining verbs and nouns separately. 
an important difference of the work presented here 

from previous work on this subject is the size of the vo-
1 	machine learning 

cabulary being disambiguated. rather than restricting the attention of the system to a handful of words  all content words in the data were considered for disambiguation. this is a more realistic scenario  introducing the problem of sparseness of the training data. the reaction of the learning algorithm to this was to combine a simple general disambiguation filter for the words that appear less frequently in text  with word-specific disambiguation rules for the remaining words. this combination of wordspecific and general disambiguation rules is an interesting outcome of our experiments that deserves further study. the overall disambiguation results were comparable to those presented in  hawkins and nettleton  1   where large-vocabulary disambiguation is also examined. however  the results of the two studies are not directly comparable  due to the use of a different set of senses  i.e.  ldoce instead of wordnet. 
　another interesting issue was generated by the second experiment that looked at the disambiguation of different parts of speech. the behaviour of the learning algorithm was different for nouns than for verbs  but no conclusion could be reached as to whether local information favours verbs or nouns. however  the interesting observation is the difference between the difficulty of the disambiguation problem and the learning task. the verb disambiguation problem examined here seems easier than the noun disambiguation one. however  the task of learning a good disambiguator for verbs was harder than that of learning to disambiguate nouns. 
　another issue that we want to examine in the future is the appropriate representation of training examples. the representation that was used here separates word instances into different senses  which are then treated as individual examples. alternative representations that would allow the grouping of all senses related to a single word  should also be examined. 
　finally  an important issue in wsd is the extent of the context used for disambiguation. only local context was taken into account here. topical evidence has also been shown to help in wsd and should be examined. 
