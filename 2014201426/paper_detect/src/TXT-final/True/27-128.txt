 
this panel explores issues of systematic and stochastic control in the context of constraint satisfaction. 
1 	i n t r o d u c t i o n : eugene c. freuder 
constraint satisfaction problems  csps  involve finding values for problem variables that satisfy restrictions on which combinations of values are allowed  freuder and mackworth  1; tsang  1 . they have many applications  including planning and scheduling  design and configuration  vision and language  temporal and spatial reasoning. the map coloring problem is a simple example  where the problem variables correspond to countries  the values to colors  and the constraints specify that neighboring countries cannot have the same color. 
　in constraint satisfaction  as in many areas of ai  there is competition between systematic methods for problem solving  which will  if necessary  explore the entire problem space  and stochastic methods  which can involve an element of random choice as to which portions of the problem space will be examined. systematic csp aproaches include search and inference techniques. stochastic approaches include those influenced 
　 this material is based on work supported by the national science foundation under grant no. iri-1. 
   *this work was partially supported by nsf grant iri1  by the electrical power research institute  epri   and by grants from toshiba of america  xerox northrop and rockwell. 
   *this work has been supported by the air force office of scientific research under contract 1 and by arpa/rome labs under contracts f1-c-1 and f1-c-1. 
edward tsang 
department of computer science university of essex 
essex c1sq  united kingdom edward essex.ac.uk 
by physical metaphors  hill climbing  simulated annealing  and those influenced by biological metaphors  neural networks  genetic algorithms . each of our panelists is prepared to speak for one of these four approaches  but will not be restricted to that limited advocacy role . 
　though our primary focus is on systematic versus stochastic  there are also interesting issues of search versus inference on the systematic side  and physical versus biological on the stochastic side. on the other hand  there are intriguing possibilities for conjoining the systematic and the stochastic. 
　stochastic methods are generally incomplete: they cannot guarantee that they will find a solution  or prove that a solution does not exist. systematic methods can. however  stochastic methods are sometimes very fast. 
arc stochastic methods inherently faster  or are there some types of problems for which stochastic methods are better suited  and some types of problems for which systematic methods are better suited  
　if and when stochastic methods are faster  why are they faster  there are actually at least three factors that these methods generally have in common - randomness  locality  incompleteness. which are really operative factors  
　is the random element important  why  there is often a local nature to stochastic search  decision making based only on local features of the problem space: how can making less informed decisions be helpful  is there actually a positive advantage to randomness or locality; or is it simply unproductive to spend more time to make informed or global choices  on the surface it seems plausible that one could gain something - speed - by giving up something - completeness: but can we explain how 
this tradeoff works  
do the successes of stochastic methods really depend 
	freuder etal 	1 

on their being non-systematic  or can they be incorporated into systematic methods  for example  an early success with constraint satisfaction using neural networks  led to a hill climbing technique  that led to ordering heuristics for systematic backtrack search. the systematic version did at least as well as the hill climbing in at least one of the tested problem domains  minton et a/.  1 . 
　stochastic methods often begin by transforming a problem into a more  atomistic  representation  e.g. in terms of sat propositional variables or neural network nodes. these representations would appear to lose  or at least deemphasize  higher level organizational information; are they succeeding in spite of that  or because of it  are these methods succeeding because they emphasize the lower level  microstructure1' of problems  the consistency patterns as opposed to the higher level patterns of constraints or subproblems  freuder  1   
　some additional provocative questions for the stochastic side: 
  time versus space. are stochastic methods buying speed with unreasonable space demands  to what extent do these methods only appear to solve large problems because they have a large representation of a small problem  
  learning versus cheating. are stochastic methods being fairly tuned for a given problem domain  or unfairly biased  
  consistency computation versus control computa-tion. how would stochastic methods fare on problems where there is a high cost associated with determining consistency of individual values  enhancing the effectiveness of inference that avoids consistency checking  
　some additional provocative questions for the systematic side: 
  completeness versus responsiveness. to what ex-ent is completeness a red herring: if we cannot wait for the complete answer  is not the systematic method de facto incomplete  
  intelligence versus brute force. does systematic reasoning pay off or is intelligence just overhead  
  optimal versus satisficing. will repeated applica-tion of stochastic methods find a  good enough  solution faster than a systematic search for the best solution  
　ultimately the most provocative question for both sides may be: 
  rather than competing  can we cooperate  
　can the two communities learn from each other  which differences are important and which superficial  or even misleading  can methods usefully combine ideas from both communities  there has been exciting recent progress in that direction   ginsberg and mcallester  1; verfaillie and schiex  1; yokoo  1  . 
1 	panels 
1 	systematic vs stochastic greedy algorithms: rina dechter 
systematic algorithms have two properties  pearl  1 :  1  do not leave any stone unturned  completeness   and  1  do not turn any stone more then once  efficiency . most classical research in heuristic search focused on such algorithms  some examples of which are a* and backtracking. recently  greater attention is shifting towards a class of greedy nonsystematic algorithms that we will call stochastic greedy  sg . these algorithms may leave  many stones unturned  and may also  turn the same stone multiple times   as do many complete algorithms  . 
　here are some facts: first  it can be demonstrated that many large problem instances that are practically unsolvable by systematic algorithms are solved efficiently using stochastic-greedy methods  minton et ai  1; selman et al  1 . secondly  it was also observed that for many problem instances sg methods are less effective than systematic search  even when the problem is satisfiable  konolige  1; kask & dechter  1 . third  sg methods are highly ineffective for inconsistent problems. finally  constraint satisfaction problems are npcomplete and therefore finding one  best performing  algorithm on every problem instance is unlikely. 
　the important problem is not finding which paradigm is the winner  but rather how to exploit identified classsuperior algorithms within one meta framework. for ar-
gument's sake  let us call an algorithm  class-superior  if there exists a reason able-size class of problem instances over which the algorithm is proven superior  be it experimentally or theoretically. let us also assume that we have identified a collection of classes and their corresponding class-superior algorithms and that no algorithm dominates other on all classes. i make two claims:  1  that this hypothetical picture is where we stand today  and  1  that it makes no sense given such a picture to talk about a winner. instead it makes more sense to talk about  integration  of algorithms among those identified as class-superior. 
　sg algorithms should surely be added to the arsenal of class-superior algorithms  as should incomplete and polynomial consistency enforcing  if we consider the immense class of unsatisfiable problems   mackworth &; freuder  1 . 
　a brute-force approach of combining class-superior algorithms would be by parallel integration. namely  to run all algorithms in parallel and stop with the first one to finish  hogg & williams  1   sequential integration is another brute-force option where algorithms are run in sequence  each being stopped after a specified amount of time . a second approach would be to understand what problem features make a given algorithm run more effectively  e.g.  identify tractable classes  and to exploit those features in the combining rule yielding a case-based integration. lastly  algorithms that address different and orthogonal aspects of the search  e.g.  lookback vs look-ahead enhancements to backtracking  can be integrated into one umbrella algorithm  unconditional integration  which may improve on each individual participating algorithm over the union of their preferred instances. such an integrated algorithm can replace its individual components in the arsenal of class-superior algorithms for further case-based or parallel-based integration  e.g  backjumping integrated with dynamic variable ordering  dvo   outperforms both   frost and dechter  1 . another proposed integration is dynamic backtracking that integrates backjumping with some lookahead ideas  ginsberg  1 . 
　research in constraint networks in the last decade has lead to substantial understanding resulting in a list of class-superior algorithms  including backtrackingstyle algorithms like backjumping  backmarking  constraint learning  forward-checking  dynamic variable ordering; structure exploiting algorithms like adaptiveconsistency  tree-clustering  and cycle-cutset decomposition; as well as consistency enforcing algorithms like arc and path consistency algorithms  dechter 1 . 
　to be able to exploit sg algorithms within an integrated algorithm  similar understanding should be acquired. here are some sample questions: are there polynomially recognizable conditions under which sg methods are guaranteed to provide a solution and in polynomial time  can some form of preprocessing help sg methods  can some form of learning during search help  is there a set of enhancement schemes which are unanimously desirable  
　in our efforts to answer some of these questions we focused on trying to improve sg performance on problems with sparse structure using two approaches: by using consistency enforcing algorithms prior to sg  kask & dechter  1  and by adapting the sg algorithm to be sensitive to the problem's structure  pinkas and dechter  1 . 
　a note on  local vs global  search. sg methods are often called local while systematic methods are associated with  global  search. the notion of  locality  is dependent on the selected search space definition with its associated neighborhood relation. what is local in one space is global in the other. backtracking is a local search procedure that can be viewed as selecting the next state with the maximum gradient increase of some look-ahead function. 
1 systematic and nonsystematic search: matthew l  ginsberg 
there seems to be a great deal of confusion here regarding the question of whether or not systematic search methods can be applied to problems of interesting size. tsang  for example  says: 
　　complete algorithms alone have little chance of solving realistic csps  constraint satisfaction problems . for example  the problem of assigning 1 jobs to 1 machines  satisfying certain constraints  has a search space of 1 nodes. 
　so what  neither a systematic nor a nonsystematic approach needs to search the entire space in order to solve a satisfiable problem. perhaps what tsang is trying to say is simply that there is no point in trying to search enormous search spaces in their entirety. assuming that p 』 np  it's hard to argue with that. 
　the upshot of this observation seems to be that if a large problem is unsatisfiable  finding a proof of unsatisfiability is likely to be impossible for any approach. one can conclude from this that the principal claimed advantage of systematic methods  that they can eventually report a problem as unsolvable  is illusory. 
　of course  that's simply not true. there are many large problems that are clearly unsatisfiable. can united airlines schedule all of its daily flights using only 1 aircraft  surely not - and we don't need to examine the entire space of possible schedules in order to prove it. so we can probably say the following: 
observation 1 some - but not all - large unsatisfiable problems are unlikely to be solvable using systematic search methods. all unsatisfiable problems of any size are unsolvable using nonsystematic methods. 
　contrary to the nonsystematic community's propaganda  unsatisfiable problems are important  and can sometimes - not always - be solved using systematic methods. 
　the claimed advantage of the nonsystematic methods comes from their superior performance on satisfiable problems. there has been a great deal written and said about this  langley  1; minton et al.  1; selman et a/.  1   and i won't repeat it here. a reasonable summary appears to be that the symbolic approaches  min-confiicts  gsat and so on  uniformly outperform nonsymbolic methods  simulated annealing  neural nets  etc.  and that: 
observation 1 when solving a satisfiable problem  it is important to be able to follow local gradients. 
　all of the claimed performance advantage of the stochastic or other nonsystematic methods appears to be based in their ability to move locally through the search space; systematic methods typically lack this property.1 but nowhere is it suggested that nonsystematicity in and of itself is an attractive property; how could it be  
　in spite of their inability to follow local gradients  systematic approaches are competitive with nonsystematic ones on a wide variety of problems; job-shop scheduling is a typical example  harvey and ginsberg  1; smith and cheng  1; smith  1 . one explanation for the performance of the systematic approach is that some measure of systematicity is needed to escape the local minima that arise in realistic problems. 
　when solving a mix of problems including both satisfiable and unsatisfiable instances  it is clear that systematicity and the ability to follow local gradients are both important. given the job-shop scheduling results  it appears that both can be important even if we restrict our attention to satisfiable problems alone. 
1
　　dechter's suggestion in this panel that  backtracking is a local search procedure that can be viewed as selecting the ... maximum gradient increase of some function  misses the point. we want to follow local gradients of a function that measures solution quality  not one that happens to mimic our particular search procedure. 
	freuderetal 	1 
　one member of the panel  dechter   reaching the same conclusion  suggests that the algorithms run in parallel or that we strive to  understand what problem features make a given algorithm run more effectively . . yielding a case-based integration.  she suggests that algorithm development should perhaps be regarded as less important than this integration  saying that   the important problem is ... how to exploit identified class-superior algorithms within one meta-framework.  
　this seems to me to miss the mark completely. far more sensible would be for the constraint-satisfaction community to focus at least some of its effort on the development of new systematic techniques that follow local gradients. both dynamic backtracking  ginsberg  1   misdescribed by dechter as an integration of backjumping and lookahead  and partial-order dynamic backtracking  ginsberg and mcallester  1  are preliminary attempts in this direction. the work is much more difficult than simple and continued experimentation on everlarger randomly generated problems  but seems far more likely to lead to results of practical importance. 
1 stochastic search for m o d e l f i n d i n g : b a r t s e l m a n 
recent work on stochastic search for solving constraint satisfaction problems has shown that that such methods can be surprisingly good at finding globally optimal solutions. for example  in our work on boolean satisfiability testing we have found that  when given a set of propositional clauses  a greedy local search method can often find a truth assignment that satisfies all clauses in the input set  selman et al.  1 . this came as a surprise to us  since we assumed that local search methods would tend to get stuck in local optima  and thus might find good near-optimal solutions  but not completely satisfying assignments. in dealing with combinatorial optimization problems  finding good near-optimal solutions is quite useful  but in many ai applications  near-satisfying assignments are of little value. for example  in satisfiability encodings of planning problems  a near-satisfying assignment corresponds to a plan with a  magic  step  i.e.  a physically infeasible operation. so  the fact that stochastic search methods often find completely satisfying solutions significantly widens the range of potential applications of such search methods. 
 see also  minton et al.  1 .  
   one obvious drawback is that stochastic search cannot be used to prove the inconsistency of a set of constraints. this is a problem in applications based on theorem proving. one way to show that a formula a follows from a theory e  is to show that ♀ with the negation of a is inconsistent. unfortunately  since stochastic search methods are incomplete  they cannot be used to prove inconsistency  simply because these methods do not systematically explore all possible models. therefore  in order to take full advantage of stochastic search methods  it is necessary to formulate tasks not in terms of theorem proving  but rather in terms of model finding. 
　it has recently been shown that systematic methods can be improved by taking advantage of principles behind stochastic  or local  search  this has led some peo-
1 	panels 
ple to suggest that perhaps in the end we will be able to develop systematic methods that will be as good as stochastic search methods at showing the consistency of a set of constraints  but  in addition  can also discover inconsistencies. as a consequence  there may not be much practical difference between model-finding and theorem proving. i will argue that this is unlikely. more specifically  i will discuss the fundamental asymmetry between the task of showing a set of constraints to be consistent versus showing them to be inconsistent. of course  this issue is closely related to the traditional distinction between np and co-np. i will argue that this is not just an interesting theoretical distinction but that there are good reasons to believe that this distinction has concrete practical consequences. the difference has been ignored in the past because of the traditional emphasis on systematic search methods  for which the distinction between np and co-np has no practical consequences. the recent work on stochastic search  however  suggests that there may in fact be a real difference in practical terms. consider  for example  randomly generated satisfiability problems. stochastic search methods can be used to find satisfying assignments of hard random 1sat formulas with up to 1 variables. however  the current best systematic methods can show the inconsistency of formulas with only up to about 1 variables  crawford and auton  1 . moreover  a theoretical analysis of resolution proofs has shown that the shortest resolution proofs of most hard inconsistent instances are of exponential size  chvatal and szemeredi  1 . since backtrack-style search can often be viewed as a variant of resolution  the exponential lower-bound on resolution places a hard limit on what one can expect  from most  backtrack-style procedures. what is needed to tackle larger formulas are more powerful proof systems  such as extended resolution  where one allows new variables to be introduced in the proof. such systems might give us shorter proofs of the inconsistency of certain large instances; unfortunately  we do not yet know whether shorter proofs actually exist for  e.g.  the hard random problems  nor do we know how to to search automatically for such shorter proofs. 
　in summary  the recent work on stochastic search methods has revealed an interesting asymmetry: it is 
generally easier to find a satisfying assignment of a consistent set of constraints  than to show a set of constraints to be inconsistent. this asymmetry suggests that it is preferable to formulate problems in terms of model-finding instead of as theorem proving tasks. a tantalizing possibility is that in order to handle certain larger inconsistent sets of constraints  we may have to develop stochastic methods that search for short proofs expressed in more powerful proof systems. 
1 	case for stochastic search: edward tsang 
this debate is important because research direction is often more important than the development of techniques. in the past the majority of research in constraint satisfaction has focussed on complete search algorithms and heuristics to be employed by them  tsang  1 . given 

lems  csps   should complete algorithms be given so much attention and incomplete search algorithms so little  what should their roles in constraint satisfaction applications be  
　the advantage of tackling problems with complete algorithms is obvious. however  it is also important to realize their limitations. the fact is  complete algorithms alone have little chance of solving realistic csps. for example  the problem of assigning 1 jobs to 1 machines  satisfying certain constraints  has a search space of 1 nodes. even if we generously assume that a very  very efficient complete search algorithm only needs to look at one in every 1 nodes in the search space  it still needs to examine 1 nodes. if a machine can examine 1 nodes per second  which would be incredible  and require the compatibility checks to be very inexpensive computationally   then given that roughly 1 x 1 seconds are available in a year  this innocent looking 1-variables1-values-each problem would require about 1 years to solve! developing interactive systems to solve such problems by complete methods is out of the question. 
　complete algorithms have other limitations. in some applications  one prefers to find optimal or near optimal solutions. complete algorithms  such as branch and bound  need to explore the whole search space to establish that the solution found is optimal  and this is limited by the combinatorial explosion problem  explained above . moreover  when a problem has no solution  most complete algorithms developed so far can only report that no solution exists. what one may need is a nearsolution to guide one through constraint relaxation. 
　all this does not mean that complete algorithms are completely useless  but that  they are very limited in their applications. they could be used for small problems. when a problem is decomposable  they could be used to solve some of the subproblems. in any case  one should always evaluate the possibility of complete algorithms being able to solve the given problem within the time available before looking at other methods. 
　stochastic methods that have been looked at for constraint satisfaction include hill climbing  hc   minion et a/.  1   connectionist methods  davenport et a/.  
1   genetic algorithms  gas   warwick and tsang  
1  and simulated annealing  sa   ghedira  1; davis  1 . 
　no one should believe that the networks that people develop in connectionist methods realistically resemble the brain. neither does any ga model evolution closely. but nature often gives us invaluable inspiration  and these methods have already demonstrated their effectiveness in various domains  including constraint satisfaction. 
　there are many reasons why the above mentioned stochastic methods  which sacrifice completeness for efficiency  are practical answers to real life csps. one important advantage of these methods is that they can be designed to terminate within a specified time period. besides  domain knowledge can be incorporated into these methods-for example  knowledge can be used in the formulation of a connectionist network and in the repreanother important issue is that the above mentioned stochastic methods have great potential to be extended to tackle constraint optimization problems and partial constraint satisfaction problems. for example  penalties for violating individual constraints and costs for assigning certain values to individual variables can be incorporated into the weights of the connections and the fitness functions of a ga. in contrast  the role of problem reduction techniques in constraint optimization problems is far from clear  and the usefulness of most of the search strategies  including lookahead and learning  are doubtful in optimization problems. 
　one important criteria for evaluating how promising a technique is lies in the current development of hardware.  arguably  advances in hardware has been much more significant than advances in software in the past decade or two.  parallel architectures have become more and more readily available. previous research and the above example have shown that a polynomial number of processors is not going to be sufficient to contain the combinatorial explosion problem in complete search methods. on the other hand  connectionism by nature can easily take advantage of the availability of parallel architectures. gas have also been shown to be able to exploit parallelism effectively   e.g. see  muhlenbein et a/.  1  . 
　reliability is often the greatest concern when using stochastic methods. indeed  some stochastic methods can easily be trapped in local optima  but connectionism and gas can be very reliable. for example   davenport et a/.  1  showed that genet has a remarkably high success rate in solving csps fin fact  it has never missed a solution in binary csps .  pinkos and dechter  1 to appear  shows that connectionist methods can be complete for certain types of problems. quite a lot of success has been found by gas in finding near-optimal solutions. 
　real life constraint satisfaction applications often involve large numbers of variables and large domains. one is often given limited time to solve such problems; e.g. a schedule may need to be produced every night. some applications demand interactive systems to be produced. limited by the combinatorial explosion problem  complete algorithms are unlikely to be able to meet the requirements of these applications. 
　some applications require that optimal or nearoptimal solutions be found. in other applications  when the problem is insoluble  near-solutions may be preferred to a report suggesting that the problem is insoluble. complete algorithms have little to offer to these requirements. 
	stochastic methods  	especially connectionism and 
gas  have had a lot of success in real life applications  so they should receive much more attention by constraint satisfaction researchers than they have so far. the questions that should interest us are: which stochastic method to use when  how reliable is a particular stochastic method  how many times does a particular stochastic method miss solutions when they exist  how successful is a particular stochastic method in finding 
	freuderetal 	1 

near-optimal solutions  how close are they to the optimal   these  rather than complete search algorithms  
should be the focus of future constraint satisfaction research. 
　to summarize  complete search algorithms suffer from the combinatorial explosion problem in general and therefore cannot be expected to solve most real life problems. in order to bring constraint satisfaction research out of the laboratory and put it into applications  attention should be shifted to stochastic search in constraint satisfaction in the future. 
