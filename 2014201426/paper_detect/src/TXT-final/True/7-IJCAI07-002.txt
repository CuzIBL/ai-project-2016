
one goal of artificial intelligence is to enable the creation of robust  fully autonomousagents that can coexist with us in the real world. such agents will need to be able to learn  both in order to correct and circumvent their inevitable imperfections  and to keep up with a dynamically changing world. they will also need to be able to interact with one another  whether they share common goals  they pursue independent goals  or their goals are in direct conflict. this paper presents current research directions in machine learning  multiagent reasoning  and robotics  and advocates their unification within concrete application domains. ideally  new theoretical results in each separate area will inform practical implementations while innovations from concrete multiagent applications will drive new theoretical pursuits  and together these synergistic research approaches will lead us towards the goal of fully autonomous agents.
1 introduction
much of the past research in artificial intelligence has focused either on high-level reasoning from abstract  ungrounded representations or on interpreting raw sensor data towards building grounded representations. however  neither of these foci taken alone is sufficient for deploying practical  real-world ai systems. in recent years  an active contingent within the field has focused on creating complete autonomous agents: those that sense their environment  engage in highlevel cognitive decision-making  and then execute their actions in the environment.
　as the field progresses in this direction  individual autonomous agents  either in software or physically embodied  are becoming more and more capable and prevalent. multiagent systems consisting of homogeneous or similar agents are also becoming better understood. however  to successfully interact in the real world  agents must be able to reason about their interactions with heterogeneous agents of widely varying properties and capabilities.
　some of these other agents will have similar or identical goals  in which case there may be some advantage to explicitly coordinating their activities. on the other hand  some of the other agents will have opposing or uncorrelated goals. in both cases  when dealing with other agents  it is beneficial from the points of view of robustness and flexibility for an agent to be able to learn or adapt how it interacts with them.
　as such  learning  interaction  and their combination are key necessary capabilities on our path towards creating robust autonomous agents. to enable these capabilities  we need to combine 1  basic  algorithmic research on machine learning and multiagent systems with 1  application-oriented research threads aimed at studying complete agents in specific  complex environments  with the ultimate goal of drawing general lessons from the specific implementations.
　this paper illustrates this research methodology  drawing heavily on concrete examples from my own research  and suggests research directions for the future aimed at creating fully autonomous agents  including robots  capable of learning and interacting. it represents my own perception of the important and interesting research topics in the field as of 1. it is not intended to be comprehensive - there are certainly other ai areas that are active  interesting  and likely to be fruitful. my ambition for the paper is that it will inspire some of my current and future colleagues to take an interest in the problemspresented here  and perhapsthat it will inspire others to formulate opposing arguments that entirely different topics are more worthy of pursuit than those advocated here.
　the remainder of this paper is organized as follows. first  in sections 1 and 1  learning and multiagent reasoning are considered separately. next  in section 1  related issues on the path towards robust physical agents  robots  that can learn and interact are addressed.
　although sections 1 treat agent components the essence of creating complete autonomous agents is putting all the pieces together. indeed  daphne koller organized her 1 computers and thought lecture around the notion of three conceptual bridges connecting representation  reasoning  and learning as a way of mitigating her observation that:
in ai  as in many communities  we have the tendency to divide a problem into well-defined pieces  and make progress on each one. but as we make progress  the problems tend to move away from each other.  koller  1 
in section 1  i argue that creating fully functional agents in complex application environments is another excellent way to combat such fragmentation. finally  section 1 concludes.
1 learning
machine learning has been an active area within ai for many years. indeed tom mitchell's computers and thought paper more than two decades ago refers to  a resurgence of interest in machine learning  due to advances in general problem solving and knowledge-based expert systems  mitchell  1 . since then  supervised learning methods  for both classification and regression  have matured to the point of enabling a general purpose toolkit that can be used productively by experts and novices alike  witten and frank  1 . similarly  unsupervised learning algorithms for data clustering have advanced nicely. but from the point of view of autonomous agents  it is the relatively recent development of reinforcement learning  rl  algorithms  designed to learn action selection from delayed reward in sequential decision making problems  that is most significant.1
　unlike classical supervised and unsupervised learning where the learner must be supplied with training data so that it can learn a functionfromfeatures to the targetset  the premise of rl matches the agent paradigm exactly: the learner gathers its own training data by interacting with the environment so that it can learn a policy mapping states to actions. an rl agent repeatedly takes actions that both move it to a new state in its environment and lead to some immediate reward signal. the learner must explicitly tradeoff between exploration and exploitation in an effort to maximize the long-term reward that it will receive.
　one common approach to reinforcement learning relies on the concept of value functions  which indicate  for a particular policy  the long-termvalue of a given state or state-action pair. temporal difference  td  methods  sutton  1   which combine principles of dynamic programming with statistical sampling  use the immediate rewards received by the agent to incrementally improve both the agent's policy and the estimated value function for that policy. hence  td methods enable an agent to learn during its  lifetime  from its individual experience interacting with the environment.1
　for small problems  the value function can be represented as a table. however  the large  probabilistic domains which arise in the real world usually require coupling td methods with a function approximator  which represents the mapping from state-action pairs to values via a more concise  parameterized function and uses supervised learning methods to set its parameters. many different methods of function approximation have been used successfully  including cmacs  radial basis functions  and neural networks  sutton and barto  1 . however  using function approximators requires making crucial representational decisions  e.g. the number of hidden units and initial weights of a neural network . poor design choices can result in estimates that diverge from the optimal value function  baird  1  and agents that perform poorly. even for reinforcementlearning algorithms with guaranteedconvergence baird and moore  1;lagoudakis and parr  1   achieving high performance in practice requires finding an appropriate representation for the function approximator. as lagoudakis and parr observe 
the crucial factor for a successful approximate algorithm is the choice of the parametric approximation architecture s  and the choice of the projection  parameter adjustment  method.  lagoudakis and parr  1 
nonetheless  representational choices are typically made manually  based only on the designer's intuition.
　despite an early emphasis in the field on  tabula rasa  learning  it is becoming increasingly accepted that the process of designing agents for complex  real-world domains will always require some manual input of this sort. the key to success will be limiting the requirements for human knowledge to tasks and representations for which humans have good intuitions. as referred to above  the existence of a general purpose toolkit that can be used without expert knowledge of the underlying algorithms suggests that supervised learning methods are mature enough for use in robust  complex systems. that is not yet the case for rl. despite the elegant theory that accompanies td methods  most notably that q-learning converges to an optimal value function if every state is visited infinitely often  watkins  1   and despite a limited number of successes that have been reported in large-scale domains  tesauro  1; crites and barto  1; stone et al.  1   crucial  somewhat unintuitive decisions about representations need to be made based on a deep understanding of the underlying algorithms and application domain.
　the remainder of this section outlines directions for research that will help current reinforcementlearning  and other machine learning  methods scale up to the point that they can become core components of fully autonomous agents in realworld tasks. sections 1 and 1 deal with automatically adjusting knowledge representations used for learning. sections 1 and 1 address ways in which humans can provide intuitive knowledge to learning agents  either by providing a subtask decomposition or by suggesting related tasks for knowledge transfer.
1 adaptive function approximation
in order to address the observation of lagoudakis and parr above  whiteson and stone  automate the search for effective function approximators for rl agents by applying optimization techniques to the representation problem. in that research  we propose using evolutionary methods  goldberg  1  for optimizing the representation because of their demonstrated ability to discover effective representations  gruau et al.  1; stanley and miikkulainen  1 . synthesizing evolutionary and td methods results in a new approach called evolutionary function approximation  which automatically selects function approximator representations that enable efficient individual learning. thus  this method evolves individuals that are better able to learn. this biologically intuitive combination has been applied to computational systems in the past  hinton and nowlan  1; ackley and littman  1; boers et al.  1; french and messinger  1; gruau and whitley  1; nolfi et al.  1  but never  to our knowledge  to aid the discovery of good td function approximators.
　specifically  we use neuroevolution of augmenting topologies  neat   stanley and miikkulainen  1  to select neural network function approximators for qlearning  watkins  1   a popular td method. the resulting algorithm  called neat+q  uses neat to evolve topologies and initial weights of neural networks that are better able to learn  via backpropagation to represent the value estimates provided by q-learning.
　in experimentalevaluation  we test q-learning with a series of manually designed neural networks and compare the results to neat+q and regular neat  which learns direct representations of policies. the results demonstrate that evolutionary function approximation can significantly improve the performance of td methods  thus providing a much-needed practical approach to selecting td function approximators  automating a critical design step that is typically performed manually.
　though that research takes a step towards automating the choice of representations for learning  there is still much room for future work  including extending it to use different policy search methods such as pegasus  ng and russell  1  and policy gradient methods  sutton et al.  1 ; and perhaps more importantly  optimizing function approximators other than neural networks  such as cmacs and radial basis functions. research on feature selection to adapt the inputs to the representation  fawcett  1; whiteson et al.  1  is also in this space of adaptive representations.
1 learned abstractions
another approach to adjusting problem representation is state abstraction  which maps two distinct ground states to a single abstract state if an agent should treat the ground states in exactly the same way. the agent can still learn optimal behavior if the environment satisfies a particular condition  namely that each action has the same abstract outcome  next state and reward  for all primitive states that are mapped to the same abstract state: ground states that are grouped together must share the same local behaviorin the abstract state space  dean and givan  1; ravindran and barto  1 . however  this cited research only applies in a planning context  in which the domain model is given  or if the user manually determines that the condition holds and supplies the corresponding state abstraction to the rl algorithm.
　jong and stone  propose an alternative condition for state abstraction that is more conducive to automatic discovery. intuitively  if an agent can behave optimally while ignoring a certain variable of the state representation  then it might be able to learn successfully while ignoring that state variable. recognizing that discovering such qualitative structure tends to require more time than learning an optimal behavior policy  thrun and schwartz  1   this approach suggests a knowledge-transfer framework  in which we analyze learned policies in one environment to discover abstractions that might improve learning in similar environments.
　we introduce an efficient algorithm that discovers local regions of the state space in which the same action would be optimal regardless of the value of a given set of state variables. for example  you would take the same route to work in new york no matter what the weather is in london. this algorithm depends on determining what set of actions is optimal in each state; we give two statistical tests for this criterion that trade off computational and sample complexity. we then generalize this learned structure to a similar environment  where an agent can ignore each set of state variables in the corresponding learned region of the state space.
　we must take care when we apply our discovered abstractions  since the criteria we use in discovery are strictly weaker than those given in other work on safe state abstraction  li et al.  1 . transferring abstractions from one domain to another may also introduce generalization error. to preserve convergence to an optimal policy  we encapsulate our state abstractions in temporal abstractions  or options  which construe sequences of primitive actions as constituting a single abstract action  sutton et al.  1 . in contrast to previous work with temporal abstraction  we discover abstract actions intended just to simplify the state representation  not to achieve a certain goal state. rl agents equipped with these abstract actions thus learn when to apply state abstraction the same way they learn when to execute any other action.
　the fact that abstractions are the building blocks for hierarchical rl suggests the recursive application of our abstraction discovery technique to create hierarchies of temporal abstractions that explicitly facilitate state abstractions  as in maxq task decompositions  dietterich  1 . this possibility highlights the need for robust testing of optimal actions  since each application of our method adds new potentially optimal actions to the agent. learning maxq hierarchies based on our method is a natural direction for future research.
1 layered learning
hierarchies in general are powerful tools for decomposing complex control tasks into manageable subtasks. as a case in point  mammalian biology is a composition of hierarchically organized components  each able to perform specialized subtasks. these components span many levels of behavior ranging from individual cells to complex organs  culminating in the complete organism. even at the purely behavioral level  organisms have distinct subsystems  including reflexes  the visual system  etc. it is difficult to imagine a monolithic entity that would be capable of the range and complexity of behaviors that mammals exhibit.
　as covered in the previous section  initial steps have been made towards learning hierarchies automatically in relatively simple domains. however  in complex tasks  the hierarchies still need to be defined manually  brooks  1; gat  1 . layered learning  stone and veloso  1; 1a  is a hierarchical paradigm that similarly requires a given task decomposition  but that then relies on learning the various subtasks necessary for achieving the complete highlevel goal. layered learning is a bottom-up paradigm by which low-level behaviors  those closer to the environmental inputs  are trained prior to high-level behaviors.
　the layered learning approach is somewhat reminiscent of rodney brooks' subsumption architecture as summarized in his computers and thought paper  brooks  1 . the subsumption architecture layers control modules  allowing highlevel controllers to override lower-lever ones. each control level is capable of controlling a robot on its own up to a specified level of functionality. in order to focus on learning and to move quickly to high-level behaviors  layered learning abandons the commitment to have every layer be completely able to control a robot. instead  many situation-specific  but as general as possible  behaviors are learned which are then managed by higher-level behaviors. nevertheless  the idea of building higher levels of functionality on top of lower levels is retained.
　table 1 summarizes the principles of the layered learning paradigm which are described in detail in this section.

1. a mapping directly from inputs to outputs is nottractably learnable.
1. a hierarchical task decomposition is given.
1. machine learning exploits data to train and/or adapt.learning occurs separately at each level.
1. the output of learning in one layer feeds into the nextlayer.

table 1: the key principles of layered learning.
principle 1
layered learning is designed for domains that are too complex for learning a mapping directly from the input to the output representation. instead  the layered learning approach consists of breaking a problem down into several task layers. at each layer  a concept needs to be acquired. a machine learning  ml  algorithm abstracts and solves the local concept-learning task.
principle 1
layered learning uses a bottom-up incremental approach to hierarchical task decomposition. starting with low-level subtasks  the process of creating new ml subtasks continues until the high-level tasks  that deal with the full domain complexity  are reached. the appropriate learning granularity and subtasks to be learned are determined as a function of the specific domain. the task decomposition in layered learning is not automated. instead  the layers are defined by the ml opportunities in the domain.
principle 1
machine learning is used as a central part of layered learning to exploit data in order to train and/or adapt the overall system. ml is useful for training functions that are difficult to fine-tune manually. it is useful for adaptation when the task details are not completely known in advance or when they may change dynamically. like the task decomposition itself  the choice of machine learning method depends on the subtask.
principle 1
the key defining characteristicof layeredlearning is that each learned layer directly affects the learning at the next layer. a learned subtask can affect the subsequent layer by:
  constructing the set of training examples;   providing the features used for learning; and/or
  pruning the output set.
　layered learning was originally applied in a complex  multi-agent learning task  namely simulated robot soccer in the robocup soccer server  noda et al.  1 . an extension that allows for concurrent training of multiple layers was also implemented in simulation  whiteson and stone  1 . as described below in section 1  layered learning has recently been applied successfully on physical robots. in all these cases  the subtask decomposition is supplied manually and if relatively intuitive to construct. nonetheless  discovering ways to automate this decomposition  perhaps by leveraging the abstraction discovery work described in section 1  is an important future goal.
1 transfer learning
a particularly topical area of ai research in 1 is transfer learning: leveraging learned knowledge on a source task to improve learning on a related  but different  target task. transfer learning can pertain to classical learning  but it is particularly appropriate for learning agents that are meant to persist over time  changing flexibly among tasks and environments. rather than having to learn each new task from scratch  the goal is to enable an agent to take advantage of its past experience to speed up new learning.
　for a reinforcement learning agent  there are several ways in which the source and target may differ in a transfer problem. for example  source and target tasks with the following differences have been studied in the literature:
  transition function: effects of agents' actions differ  selfridge et al.  1 
  reward structure: agents have different goals  singh  1 
  state space: agents' environments differ  konidaris and barto  1; fernandez and veloso  1 
  initial state: agents start in different locations over time  asada et al.  1 
  actions: agents have different available actions  maclin et al.  1; taylor et al.  1; soni and singh  1 
  state variables: agents' state descriptions differ  maclin et al.  1; taylor et al.  1; soni and singh  1 
　in the most general case  the source and target can differ in all of these ways. for such cases  taylor et al.  introduce a method for transferring the learned value function in a source rl task to seed learning in the target. the key technical challenge is mapping a value function in one representation to a meaningful value function in another  typically larger  representation  despite the fact that state-action values are inherently task-specific.
　past research confirms that if two tasks are closely related the learned policy from onetask can be used to providea good initial policy for the second task. for example  selfridge et al.  showed that the 1-d pole balancing task could be made harder over time by shortening the length of the pole and increasing its mass; when the learner was first trained on a longer and lighter pole it could more quickly learn to succeed in the more difficult task with the modified transition function. in this way  the learner is able to refine an initial policy for a given task.
　we consider the more general case where tasks are related but distinct in that their state and/or action spaces differ. to use the source policy πs as the initial policy for a td learner in the target task  we must transform the value function so that it can be directly applied to the new state and action space. we introduce the notion of a behavior transfer functional ρ πs  = πt that will allow us to apply a policy in the target task. the policy transform functional  ρ  needs to modify the source policy and its associated value function so that it accepts the states in the target task as inputs and allows for the actions in the target task to be outputs  as depicted in figure 1.
　defining ρ to do this modification in such a way that πt is a good starting point for learning in the target is the key technical challenge to enable general behavior transfer. current results indicate that such ρ's do exist  at least for some tasks  taylor et al.  1 . however  automating the discovery of the inter-task mapping between the state variables and actions in the source and target tasks remains an open challenge  as does automatically selecting the source and target tasks themselves.
1 multiagent reasoning
in addition to learning  a second essential capability of robust  fully autonomous agents is the ability to interact with other agents: multiagent reasoning. as argued in the introduction  to successfully interact in the real world  agents must be able to reason about their interactions with heterogeneous agents of widely varying properties and capabilities. once we have a single complete agent that is able to operate autonomously for extended periods of time in the real world  it is inevitable that soon after we will have many. and they will need to interact with one another.
　though more recent to our field than machine learning  in the past decade multiagent systems  mas  has begun to come to the forefront. as with any methodology  two important questions about mas are:
  what advantages does it offer over the alternatives  and
  in what circumstances is it useful 
it would be foolish to claim that mas should be used when designing all complex systems. like any approach  there are some situations for which it is particularly appropriate  and others for which it is not. in a survey of the field  stone and veloso  1b summarizedthe circumstancesin which mas

figure 1: ρ is a functional which transforms a value function q from one task so that it is applicable in a second task that has different state and action spaces.
is appropriate.1 for example  some domains require mas  such as those in which agents represent people or organizations with different  possibly conflicting  goals and proprietary information; having multiple agents may speed up computation via parallelization; mas can provide robustness via redundancy; mas may be more scalable as a result of modularity; and they can be useful for their elucidation of fundamental problems in the social sciences and life sciences  cao et al.  1   including intelligence itself  decker  1 . regarding this last point  as wei   put it:  intelligence is deeply and inevitably coupled with interaction.  in fact  it has been proposed that the best way to develop intelligent machines might be to start by creating  social  machines  dautenhahn  1 . this theory is based on the socio-biological theory that primate intelligence first evolved because of the need to deal with social interactions  minsky  1 .
　multiagent systems differfrom single-agentsystems in that several agents exist that model each other's goals and actions. in the fully general multiagent scenario  there may be direct interaction among agents  communication . although this interaction could be viewed as environmental stimuli  we present inter-agent communication as being separate from the environment.
　from an individual agent's perspective  multiagent systems differ from single-agent systems most significantly in that the environment's dynamics can be affected by other agents. in addition to the uncertainty that may be inherentin the domain  other agents intentionally affect the environment in unpredictable ways. thus  all multiagent systems can be viewed as having dynamic environments. figure 1 illustrates the view that each agent is both part of the environment and modeled as a separate entity. there may be any number of agents  with different degrees of heterogeneity and with or without the ability to communicate directly.

figure 1: the fully general multiagent scenario. agents model each other's goals  actions  and domain knowledge  which may differ as indicated by the different fonts. they may also interact directly  communicate  as indicated by the arrows between the agents.
　most mas research assumes that the protocol for interaction among agents is fixed from the outset. perhaps they will interact via a well-defined communication language as represented by the arrows between the agents in figure 1  or perhaps they will interact just through observation of one another's actions as is often the case in adversarial domains. however robust agents should also be able to cope with flexible forms of interaction. in section 1  the focus was on research areas of machine learning that pertain to changing the representation of learned knowledge  e.g. function approximators  abstractions  and inter-task mappings for transfer  more so than the learning algorithms themselves. an analog in mas is considering that the patterns and very rules of interaction among agents may be able to change over time. this section considers cases in which the rules of interaction can change  sections 1 and 1  as well as cases in which the behaviors or capabilities of other agents change or are not known in advance so that adaptive agent modeling is useful or necessary  section 1 .
1 adaptive mechanism design
the first such case we consider is adaptive mechanism design  which lies in the space considered by tuomas sandholm's computers and thought lecture  namely agent-based electronic commerce  sandholm  1 . this area falls at the intersection of computer science and economics.
　recent years have seen the emergenceof numerous auction platforms that cater to a variety of markets such as businessto-business procurement and consumer-to-consumer transactions. many different types of auction mechanisms defining the rules of exchange may be used for such purposes. varying parameters of the auction mechanism  such as auctioneer fees  minimum bid increments  and reserve prices  can lead to widely differing results depending on factors such as bidder strategies and product types. mechanism design is the challenge of optimizing auction parameters so as to maximize an objective function  such as auctioneer revenue.
　mechanism design has traditionally been largely an analytic process. assumptions such as full rationality are made about bidders  and the resulting properties of the mechanism are analyzed in this context  parkes  1 . even in largescale real-world auction settings such as the fcc spectrum auctions  game theorists have convened prior to the auction to determine the best mechanism to satisfy a set of objectives. historically  this process has been incremental  requiring several live iterations to iron out wrinkles  and the results have been mixed  cramton  1; weber  1 . an important component of this incremental design process involves reevaluating the assumptions made about bidders in light of auction outcomes. these assumptions pertain to bidders' intrinsic properties and to the manner by which these properties are manifested in bidding strategies.
　pardoe et al.  address this problem by considering an adaptive mechanism that changes in response to observed bidder behavior through the use of a learning algorithm. our view of adaptive mechanism design is illustrated in figure 1. a parameterizedmechanismis defined such that the seller can use an adaptive algorithm to revise parameters in response to observed results of previous auctions  choosing the most promising parameters to be used in future auctions. upon execution  the parameterized mechanism clears one or more auctions involving a population of bidders with various  generally unknown  bidding strategies. the results of the auction are then taken as input to the adaptive algorithm as it revises the mechanism parameters in an effort to maximize an objective function such as seller revenue. any number of continuous or discrete auction parameters may be considered  such as reserve prices  auctioneer fees  minimum bid increments  and whether the close is hard or soft  wurman et al.  1 .
　the bidders in figure 1 may use a variety of different bidding strategies  including heuristic  analytic  and learningbased approaches. for the latter to make sense  the same bidders must interact repeatedly with the mechanism  leading to a potential co-evolutionary scenario in which the bidders and mechanism continue to adapt in response to each other  phelps et al.  1 . however  our approach does not depend on repeated interactions with the same bidders. the only required assumption about the bidders is that their behavior is somewhat consistent  e.g. bidders associated with a particular industry tend to bid similarly  for a sufficient period of time to allow for prediction of auction results as a function of the mechanism  at least in expectation. the main contribution of our work in this area is the presentation of a metalearning technique with which information about potential bidder behavior can be used to guide the selection of the method of adaptation and significantly improve auctioneer revenue.

figure 1: a high-level illustration of the concept of adaptive mechanisms. from the point of view of the seller  the bidder behaviors are unknown aspects of the environment.
　there are several directions in which this work could be extended. many auction parameters are available for tuning  ranging from bidding rules to clearing policies. the problem becomes more challenging in the face of multidimensional parameterizations. the choice of metalearning algorithm itself is a possible area for improvement as well. and perhaps most interestingly  the effects of including some adaptive bidders in the economiesthat are treated by adaptivemechanisms are currently unknown.
1 general game playing
as presented in the previous section  the idea behind adaptive mechanism design is that an agent may actively change the rules of market interaction for bidding agents as time goes on. for such an approach to be feasible in practice  1  there will need to be a way to specify the current rules of interaction to the participating agents  and 1  the agents must be able to figure out for themselves how to bid in any mechanism that is so specified. challenges similar to both of these have been recently addressed in the context of general game playing.
　creating programs that can play games such as chess  checkers  and backgammon  at a high level has long been a challenge and benchmark for ai. while several game-playing systems developed in the past  such as deep blue  campbell et al.  1   chinook  schaeffer et al.  1   and tdgammon tesauro  1 have demonstratedcompetitiveplay against human players  such systems are limited in that they play only one particular game and they typically must be supplied with game-specific knowledge. while their performance is impressive  it is difficult to determineif their success is due to the particular game-playing technique or due to the human game analysis.
　a general game playing agent  pell  1  must be able to take as input a description of a game's rules and proceed to play without any human input  despite having had no prior experience in that game. doing so requires the integration of several ai components  including theorem proving  feature discovery  heuristic search  and potentially learning.
　kuhlmann et al.  1a  present a complete and fully autonomous general game playing agent designed to participate in the first aaai general game playing  ggp  competition which was held at aaai 1  genesereth and love  1 . in that setting  a restricted class of games is considered  namely discrete state  deterministic  perfect information games. the games can be single or multi-player and they can be turn-taking or simultaneous decision.
　our main contribution in this setting is a novel method for automatically constructing effective search heuristics based on the formal game description. the agent analyzes the game description to automatically detect relevant features  such as the number of white pieces on a board  to encode within heuristic evaluation functions for use in iterative-deepening alpha-beta search. banerjee and stone  have also shown the applicability of rl transfer learning in this setting.
　research on ggp is still in its very early stages  with current agents performing far worse than agents with domainspecific evaluation functions. though perhaps ggp agents will never rival special-purpose game players  there are many open research directions pertaining to automatically generating heuristic evaluation functions that may help close the gap.
1 agent modeling
for fully autonomous agents in multiagent environments  the ability to predict the behavior of other agents in the environment can be crucial to one's own performance. specifically  knowing the likely actions of other agents can influence an agent's expected distribution over future world states  and thus inform its planning of future actions.
　in an adversarial environment  this predicted behavior of other agents is referred to as an opponent model. opponent models are particularly useful if they include some identification of potential patterns or weaknesses on the part of the opponent. for example  a chess grandmaster may study past games of a future opponent so as to determine how best to play away from that opponent's strengths.
　in multiagent adversarial settings  in which the adversary consists of a team of opponents  it can be useful to explicitly model the opponent as engaging in team activities. for example  tambe  presents a simulated air-combat scenario in which an individual's behavior can indicate the commencement of a team  pincer  maneuver that requires multiple participants  thus enabling the prediction of other opponents' future actions as well.
　one setting in which opponent modeling research has been conducted is the robocup simulation coach competition. robocup is an international research initiative that uses the game of soccer as a testbed to advance the state of the art in ai and robotics  kitano et al.  1a . in most robocup soccer leagues the goal is to create complete teams of agents that can succeed at the task of winning soccer games. though opponent modeling can play a part in this task  it is often the case that opponents cannot be observed prior to playing against them  at least not by the agents themselves . even when they can be observed  opponent modeling challenges are easily overshadowed by challenges such as vision  localization  locomotion  individual ball manipulation  and teamwork.
　in contrast  the goal of the simulation coach competition is to focus entirely on opponent modeling. this focus is accomplished by 1  providing entrants with recordings of the opponents' past play that is understandable by the coach agent; 1  providing each entrant with an identical team of fully competent player agents; and 1  restricting the actions available to advice regarding how the team should alter its playing style to fit a particular opponent.
　in this context  riley et al.  approach advice-giving as an action-prediction problem. both offensive and defensive models are generated using the c1  quinlan  1  decision tree learning algorithm. their work also stresses the importance of learned formation advice. subsequently  kuhlmann et al.  1; 1b  decompose the problem similarly  but using different model representations and advicegeneration procedures.
　in other work  riley and veloso  use bayesian modeling to predict opponent movement during set plays. the model is used to generate adaptive plans to counter the opponent's plays. in addition  riley and veloso  have tried to model high-level adversarial behavior by classifying opponent actions as belonging to one of a set of predefined behavioral classes. their system could classify fixed duration windows of behavior using a set of sequence-invariant action features.
　opponent team modeling has also been studied in militarylike scenarios. in addition to tambe's work mentioned above  tambe  1   sukthankar and sycara  use hmms to monitor and classify human team behavior in a mout  military operations in urban terrain  scenario  especially focusing on sequential team behaviors.
　as the number and variety of agents increases  methods for agent modeling will only become more central to enabling agent autonomy. whether through recursive modeling methods based on deep knowledge of the other agents' internal states  vidal and durfee  1 ; strictly via observation of the other agents' actions  huber and durfee  1 ; or somewhere in between  there will continue to be a need for methods that enable prediction of other agents' future actions.
1 robotics
the general topics of learning and multiagent reasoning are relevantto autonomousagents of all kinds  includingsoftware agents. but to the extent that one goal of ai is to enable the creation of intelligent physical agents  or robots  that can coexist with us in the real world  it is important to consider to what extent these topics must be tailored to facilitate learning  interacting robots.
　compared to other machine learning scenarios such as classification or action learning by an individual agent in simulation  multiagent learning on physical robots presents several formidable challenges  including the following.
sparse training data: it is often prohibitively difficult to generate large amounts of data due to the maintenance required on robots  such as battery changes  hardware repairs  and  usually  constant human supervision. thus learning methods designed for physical robots must be effective with small amounts of data.
dynamic environments: robots are inherently situated in a dynamically changing environmentwith unpredictable sensor and actuator noise  namely the real world. when acting in teams  they must adapt to each other's changing behaviors in addition to changes in the environment. thus learning algorithms designed for teams of physical robots must be able to adapt quickly and continually.
　due at least in part to these challenges  most research on learning robots to date has focused on individual tasks or relatively simple multi-robot tasks. however  recent successful applications of machine learning to complex dynamic environments with limited training examples  both with multiple agents in simulation and with individual robots in the real world  suggest that the time is right for a concerted effort towards further developing learning methods that can be deployed on teams of physical robots. as such  this section focuses on research pertaining to learning and multiagent reasoning on physical robots.
1 learned behaviors on physical robots
ideally  a robot should be able to respond to a change in its surroundings by adapting both its low-level skills  such as its walking style  and the higher-level behaviors that depend on them. because hand-coding is time-consuming and often leads to brittle solutions  one would like this adaptation to occur as autonomously as possible.
　unfortunately  current learning methods typically need a large amount of training data to be effective. if that data must be gathered by a robot in the real world  the amount of time required for learning could become prohibitively large. one possible way to alleviate this problem is to train behaviors first in simulation before implementing them in the real world  davidor  1; gat  1; porta and celaya  1 . however  especially when concerned with complex perception or manipulation tasks  we cannot assume an adequate simulator will always exist for a given robot. with no simulator  each trial requires interaction with the physical world in real time. in such cases  it is not possible to offset the costs of an inefficient learning algorithm with a faster processor. the learning algorithm must make efficient use of the information gained from each trial  i.e.  it must have low sample complexity .
　for this reason  until recently  most of the locomotion approaches for quadrupedalrobots have focused on hand-tuning a parameterized gait. this approach has been somewhat fruitful: since the introduction of the sony aibo robot  sony  1  in 1  the walking speed of the aibos has increased significantly via manual gait tuning. however  the process of hand-tuninga parameterizedgait both can be time-consuming and can require a good deal of human expertise. furthermore  a change of robot hardware and/or the surface on which it is to walk necessitates retuning.
　as an alternative to hand-tuning a parameterized gait  machine learning can be used to automate the search for good parameters. various machine learning techniques have proven to be useful in finding control policies for a wide variety of robots including helicopters  bagnell and schneider  1; ng et al.  1   biped robots  zhang and vadakkepat  1  and aibos  hornby et al.  1; 1; kim and uther  1 . kohl and stone  1a  present a policy gradient learning approach for generating a fast walk on legged robots. using this method  we have created a walk that is faster than hand-tuned gaits and was among the fastest learned gaits of its time.1 a key feature of our approach is that the robots time themselves walking across a known  fixed distance  thus eliminating the need for any human supervision.
　fidelman and stone  further demonstrate that it is possible to similarly learn a higher-level fine-motor skill  again with all learning occurring directly on the robot. in particular  the aibo is able to learn a ball-grasping skill with no human intervention other than battery changes. the learned skill significantly outperforms our best hand-tuned solution.
　as the learned grasping skill relies on a learned walk  we characterize our learning implementation within the framework of layered learning  as introduced in section 1. this research represents the first implementation of layered learning on a physical robot  with all training performed in the real world. extending this approach to other robots and additional hierarchical behavior layers is a promising direction for future research.
1 learned sensor and actuator models
one popular approach towards achieving the goal of robust autonomy in robots is to imbue the robot with a general reasoning capability that relies on 1  a model of the current state of the world; and 1  a model of the effects of the robot's actions on the world. given these models  the robot can then plan its actions so as to best achieve its goals given the current state of the world.
　for such a model-based approachto be effective  the sensor and actuator models must be accurate and well-calibrated  at least in relation to one another. for example  state-of-the-art robot localization algorithms such as particle filtering  dellaert et al.  1; kwok et al.  1  rely on calibrated sensor and actuator  odometry  models to fuse sensory and action history information into a unified probabilistic estimate of the robot's location.
　currently  these sensor and actuator models are typically calibrated manually: sensor readings are correlated with actual measured distances to objects  and robot actuator commands are measured with a stopwatch and a tape measure. however this type of approach has three significant drawbacks. first  it is labor intensive  requiring a human operator to take the necessary measurements. second  for sensors and actuators with many  perhaps infinitely many  possible readings or parameter settings  the measured model can only be made to coarsely approximate the complete model. third  and perhaps most importantly  the model is necessarily tuned to a specific environment and may not apply more generally.
this brittleness is a major motivation for the automatic model building advocated by stronger and stone .
　when considered in isolation  there is no choice but to build each individual sensor and action model manually. in practice  however  each of the robot's sensors as well as its action selection mechanism can be related through their reflection on the world state  as illustrated in figure 1.
sensor
model
action model
figure 1: the flow of information on an autonomous robot. the data from each sensor and the action selections are interpreted based on the robot's action and sensor models  represented by arrows here. the resulting extracted information can then be used to inform the robot's estimate of the state of the world.
　motivated by this relationship  stronger and stone  introduce asami  autonomous sensor and actuator model induction   a general methodology by which a robot can learn its action and sensor models from each other. because the world state can be estimated from independent  redundant sources of information  the robot can use this information to learn a model of any given individual source. it learns each model by comparing the input to that model to an estimate of the world state based on all of the other information sources. figure 1 shows  in a simplified setting  how the robot can use redundant information to learn its action and sensor models. for the sensor model  the world state estimate is first relayed back through arrow a to the  information about world state  from the sensor model. this tells us what the output of the sensor model should have been assuming the world state estimate is perfectly accurate. when this data is combined with the raw sensory input via arrow b  the result is training data for learning the sensor model. this data can be processed by supervised learning method based on the structure of the sensor model. similarly  the world state estimate can be relayed back to the  changes to world state  from the action model  arrow c . in this case  the world state  if accurate  indicates how the world actually changed as a result of previous actions. this information can be combined with the action selections  arrow d  to train the action model.
　the benefit of asami is that it enables a robot to induce models of its sensors and actions without any manually labeled training data. that is  the only inputs to the learning process are the data the robot would naturally have access to: its raw sensations and its knowledge of its own action se-

figure 1: dashed arrows a through d show how information can be propagated back from a redundantly informed world state estimate to calibrate the action and sensor models.
lections. this general methodology promises to increase the robustness and general applicability of autonomous robots. if a fully featured robot could automatically calibrate all of its processing modules  it would be able to navigate under a variety of environmental conditions without any human supervision. however  achieving this goal is not straightforward. standard techniques for model induction require accurately labeled training data; in the absence of manually labeled data  the robot must combine information from its actuators and sensors to bootstrap accurate estimates of both models.
　asami is instantiated in a broad class of settings  namely those in which an agent navigates through a one-dimensional state space with a sensor that provides information about the state of the world and actions that determine the world state's rate of change. examples of such settings include a robot on a track with a global positioning sensor and a velocity control; a temperature regulator; and a vehicle with a throttle whose settings correspond to accelerations. asami works by simultaneously learning an action model and a sensor model  each one based on the current estimate of the other one. this bootstrapping process enables the agent to learn accurate approximations to its true action and sensor models  starting with only a very simplistic action model estimate. furthermore  the learning process is completely autonomous and unsupervised  so that no human oversight or feedback is necessary.
　asami is implemented and validated in a robotic test-bed domain based on the sony aibo ers-1. in experimentaltests  the robot learns models from its action commands to the resultant velocities and from its visual sensor readings to the corresponding distances. the learning process takes only two and a half minutes of completely autonomous behavior.
　looking forward  we consider asami to be a first step towards enabling a robot to autonomously navigate through a very high-dimensional space while learning many functions with various numbers of input and output variables. extending asami in this direction raises two main challenges. first  asami currently allows a robot to learn two functions: one sensor model and one action model. with more than two models to learn  the robot will have multiple sources of information that can be used to train each model. this situation raises the question of how these sources can best be integrated with each other to provide effective training data for each model. second  asami currently assumes that the robot operates in a one-dimensional state space  thus restricting its learned models to being from one variable to one other variable. for many applications  it will be necessary or useful to learn functions with multiple input and output variables. in this case  straightforward polynomial regression  as used in the initial implementation  is insufficient. in principle  the asami methodology should extend to other generalpurpose function approximators such as neural networks and cmacs. however  additional research is needed to determine how these methods may need to be adapted for automated model learning.
1 learned vision
to operate in the real world  autonomous robots depend on their sensory information. visual input  in the form of color images from a camera  should be an excellent and rich source of such information. but color  and images in general  have been used sparingly on mobile robots  where people have mostly focused their attention on other sensors such as tactile sensors  sonar  and laser. there are three main reasons for this reliance on other relatively low-fidelity sensors. first  most sophisticated vision algorithms require substantial amounts of computational and/or memory resources  making them infeasible to use on mobile robotic systems that demand realtime processing. second  most vision algorithms assume a stationary or slowly moving camera and hence cannot account for the rapid non-linear camera motion that is characteristic of mobile robots. third  the variation of illumination over the operating environment causes a nonlinear shift in color distributions that is difficult to model; mobile robots  while moving around the world  often go into places with changing illumination.
　even in the face of these formidable challenges  one factor that can be leveraged is that many mobile robot applications involve a structured environment with objects of unique shape and color - information that can be exploited to help overcome the problems mentioned above. sridharan and stone focus on designing algorithms that can work within the robot's computational and environmental constraints while making the best use of the available information.
　first  we present a prototype vision system that works on-board an autonomous robot using vision as the primary source of information  sridharan and stone  1b . this baseline system includes solutions to two main vision challenges  color segmentation1 and object recognition  that can run within the robot's hardware constraints. the system is robust to the presence of jerky non-linear camera motion and noisy images. however  the baseline system relies on manually labeled training data and operates in constant  and reasonably uniform  illumination conditions.
　second  we use the structure inherent in the environment to eliminate the need for manual labeling. the robot uses the knowledge of the location of unique objects in its world to autonomously learn the desired colors required for color segmentation  thereby eliminating the time-consuming and brittle manual calibration process. this information is also used by the robot to navigate around its environment  sridharan and stone  1a .
　third  we enable the robot to perform efficiently even in the presence of illumination variations  a challenging vision problem because of the corresponding non-linear shift in color distributions: the very same pixel values corresponding to a color in one illumination may correspond to a completely different color in another illumination. as the robot navigates in its moderately structured world  it autonomously detects and adapts to the changes in illumination conditions  sridharan and stone  1c .
　fourth  we unify these results by enabling a robot 1  to recognize when the illumination has changed sufficiently to require a completely new color map rather than using one of the existing ones; and 1  to plan its own action sequence for learning the new color map on-line  sridharan and stone  1 .
　all of the algorithms above run in real-time on a physical aibo ers-1 robot enabling it to operate autonomously in an uncontrolled environmentwith changing illumination over an extended period of time. in the end  the robot is able to detect changes in illumination robustly and efficiently  without prior knowledge of the different illumination conditions. when the robot detects an illumination condition that it has already learned  it smoothly transitions to using the corresponding color map.
　one direction of future work is to have the robot adapt to minor illumination changes by suitably modifying specific color distributions. the ultimate research goal along this path is to develop efficient algorithms for a mobile robot to function autonomously under uncontrolled natural lighting conditions.
1 communication connectivity
many applications of distributed autonomousrobotic systems can benefit from  or even may require  the team of robots staying within communication connectivity. for example  consider the problem of multirobot surveillance  parker  1; ahmadi and stone  1b   in which a team of robots must collaboratively patrol a given area. if any two robots can directly communicate at all times  the robots can coordinate for efficient behavior. this condition holds trivially in environments that are smaller than the robots' communication ranges. however in larger environments  the robots must actively maintain physical locations such that any two robots can communicate  possibly through a series of other robots. otherwise  the robots may lose track of each other's activities and become miscoordinated. furthermore  since robots are relatively unreliable and may need to change tasks  for example if a robot is suddenly called by a human user to perform some other task   in a stable multirobot surveillance system  if one of the robots leaves or crashes  the rest should still be able to communicate. some examples of other tasks that could benefit from any pair of robots being able to communicate with each other  are space and underwater exploration  search and rescue  and cleaning robots.
　we say that robot r1 is connected to robot r1 if there is a series of robots  each within communication range of the previous  which can pass a message from r1 to r1. in order for the team to stay reliably connected  it must be the case that every robot is connected to each other robot either directly or via two distinct paths that do not share any robots in common. we call this property biconnectivity: the removal of any one robot from the system does not disconnect the remaining robots from each other.
　ahmadi and stone  1a  study this problem of enabling robots to remain connected in the face of robot failures. we address this problem by dividing it into three main steps: 1  checking whether a team of robots is currently biconnected  1  maintaining biconnectivity should a robot be removed from  or added to  the team  and 1  constructing a biconnected multi-robot structure from scratch. to be applicable for teams of autonomous robots  all algorithms must be fully distributed.
　our work to date addresses step 1 under the assumption that robots have constant and identical communication ranges. this assumption applies in the case of homogeneous robot teams  or at least teams with homogeneous transmitters  such that the range is not dependent on a robot's battery level. this assumption allows us to assume the connection graph among robots is undirected: if robot a can send a message to robot b  then the reverse is also true. the heterogeneous case  along with steps 1 and 1 are natural directions for future work in this area.
1 applications
sections 1 treated learning and multiagent reasoning as components of autonomous agents. but as pointed out in the introduction courtesy of koller's quote  doing so has the risk of fragmenting the field. her reaction to this risk was to provide conceptual bridges among three different ai topics. but another way to address the risk is to build applications that require the practical unification of the various topics into a complete agent  in our case one that learns  interacts  and perhaps acts in the real world.
　this notion of starting research from applications is complementaryto stuart russell's metaphorin his computersand thought paper. he wrote:
theoreticians can produce the ai equivalent of bricks  beams  and mortar with which ai architects can build the equivalent of cathedrals.  russell  1 
on the other hand architects may  by creating new artifacts that exhibit previously unseen properties  lead theoreticians to the possibility of  or even need for  new bricks and beams. in other words  one valuable way to advance the field is to study complete agents in specific  complex domains  with the ultimate goal of drawing general lessons from the specific implementations. from this point of view  theoreticians and architects share a complementary bi-directionaldependencyon one another. theory paves the way for practice and also vice versa: it is not a one-way road.
　consistent with this view  russell writes that  ai is a field defined by its problems  not its methods   russell  1 . in the remainder of this section  i briefly describe four such problems that have been useful to me in my pursuit of agents that can learn and interact with one another: robot soccer  autonomous bidding agents  autonomic computing  and autonomous vehicles. the first and last are physical domains with real robots  while the othertwo focus on software agents. the second and third are also interesting for the connections they provide between ai and other research areas  namely economics and computer systems respectively.1 most importantly  all four are well-suited for the integration of machine learning and multiagent reasoning  including opportunities for leveraging the research directions emphasized in sections 1 and 1 such as adaptive and hierarchical representations  layered learning  transfer learning  adaptive interaction protocols  and agent modeling.
1 robot soccer
the original motivating domain for my research on multiagent learning was robot soccer. robot soccer pits two teams of independently-controlled agents against each other. originated by mackworth   it has been gaining popularity over the past decade  with several international  robocup  competitions taking place  kitano  1; asada and kitano  1; veloso et al.  1; stone et al.  1a; birk et al.  1; kaminka et al.  1; polani et al.  1; nardi et al.  1; noda et al.  1 . it was the subject of an official ijcai-1 challenge  kitano et al.  1b  and poses a long-term grand challenge to the field  namely the creation of a team of humanoid robots that can beat the best human soccer team on a real soccer field by the year 1. robocup's explicit goal is to encourage research in the fields of ai and robotics  with a particular emphasis on collaborative and adversarial reasoning among autonomous agents in dynamic multiagent environments. one advantage of the domain is that robot soccer can be used to evaluate different ai techniques in a direct manner: teams implemented with different algorithms can play against each other.
　currently  robocup includes five robot soccer leagues  simulation  small-size  mid-size  four-legged  and humanoid   with each one emphasizing slightly different research issues depending on the physical properties of the robots. the commonalities across the leagues are that they are run in dynamic  real-time  distributed  multiagent environments with both teammates and adversaries. in general  there is hidden state  meaning that each agent has only a partial world view. the agents also have noisy sensors and actuators  meaning that they do not perceive the world exactly as it is  nor can they affect the world exactly as intended. in addition  the perception and action cycles are asynchronous  prohibiting the traditional ai paradigm of using perceptual input to trigger actions. communication opportunities are limited; and the agents must make their decisions in real-time. these italicized domain characteristics combine to make robotic soccer a realistic and challenging domain.
　much of the research described in sections 1 has been directly motivated by the challenges posed by the various robocup leagues.
1 autonomous bidding agents
in contrast to robot soccer  in which all the agents are either fully collaborative teammates or fully adversarial opponents  research on autonomous bidding agents presents an opportunity to consider agents whose goals are independent: an economic agent seeks to maximize its own profit regardless of the effects on the profits of other agents.
　like robocup  autonomous bidding has been the subject of annual competitions for several years. the first trading agent competition  tac  was held in 1  wellman et al.  1  with the goal of providing a benchmark problem in the complex and rapidly advancing domain of emarketplaces  eisenberg  1  and motivating researchers to apply unique approaches to a common task.
　one key feature of tac is that it requires autonomous bidding agents to buy and sell multiple interacting goods in auctions of different types. another key feature is that participating agents compete against each other in preliminary rounds consisting of many games leading up to the finals. thus  developers change strategies in response to each other's agents in a sort of escalating arms race. leading into the day of the finals  a wide variety of aggregate economic scenarios is generally possible. a successful agent needs to be able to perform well in any of these possible circumstances.
　current tac domains include a travel scenario in which agents procure flights  hotels  and entertainment tickets for clients with various travel preferences; and a supply chain management scenario in which agents manage a pc manufacturing process  purchasing componentsfrom suppliers  deciding what computers to produce  and bidding for customer orders. the next tac is planned to focus on adaptive mechanism design  as described in section 1.
　in these domains  the multiagent learning opportunities tend to pertain to modeling the aggregate effect of the other agents on the economy  either from extensive offline data  stone et al.  1   or in response to short-term changes in the other agents' strategies  stone et al.  1b; pardoe and stone  1 . note that even though it is a domain for strictly software agents  autonomous bidding is very much a real-world problem  with potential high-stakes economic impact.
1 autonomic computing
at the intersection of ai and computer systems research  autonomic computing  kephart and chess  1  has the potential to address shortcomings in today's systems and to enable future systems. autonomic computing refers to a broad set of strategies to reduce the amount of complexity exposed to human operators of computing systems. autonomic systems need to intelligently make complex decisions based on large amounts of uncertain  heterogeneous information. the area of machine learning has made significant progress in developing methods that automate the construction of such complex decision-making systems by inducing robust models directly from relevant empirical data.
　many recent papers have identified systems problems that can benefit from machine learning  wildstrom et al.  1; chen et al.  1; liblit et al.  1; 1; mesnier et al.  1; fern et al.  1; kolter and maloof  1; chang et al.  1; fox et al.  1; gomez et al.  1; walsh et al.  1; murray et al.  1; newsome et al.  1 . my own experience  and the experience of others  shows that machine learning cannot be integrated into systems as a simple black box. rather  to achieve the goals of autonomic computing we will need to achieve a much tighter coupling between systems and machine learning in which system designs are adapted to facilitate machine-learningbased control  and in which machine learning techniques are advanced to meet the demands of large-scale systems.
　autonomic computing may be a candidate for the next successful application of layered learning. for entire systems to be able to self-diagnose failures and repair themselves  there will need to be learning components at multiple levels  including the os  databases  and networking modules. thus a paradigm like layered learning may be essential to keeping the entire system operating smoothly.
　in addition to learning  autonomic commuting is also fundamentally a multiagent problem. in today's highly interconnected world  computers are generally networked together such that failures or upgrades on one system can have internet-wide effects  for instance with respect to packet routing or grid services.
　like autonomous bidding agents  autonomic computing is a software agents domain with high potential for real-world impact. i believe that autonomic computing will become an increasingly important domain for testing  deployment  and development of machine learning and multiagent reasoning advances in the years to come.
1 autonomous vehicles
enabling cars to drive autonomously in cities is currently technologically feasible  and will likely be economically feasible within the next 1 years. indeed  general motors has announced that it plans to release a nearly autonomous vehicle under its european  opel  brand. the 1 opel vectra will be able to drive itself at speeds up to 1 miles per hour  even in heavy traffic.
　such autonomous vehicles will change the way we think about transportation  for example enabling people to concentrate on other activities while  driving   and enabling minors and the elderly to be transported on their own. as a result  once there's a single autonomous vehicle  there will likely be many more  and every major automobile company will need to respond.
　the successful darpa grand challenge  darpa  1  has shown that current ai can produce autonomous  embodied agents capable of navigating the mojave desert. while certainly no small feat  traversing a barren desert devoid of pedestrians  narrow lanes  and multitudes of other fastmoving vehicles solves at best half the problem. as gary bradski  a researcher at intel corp. said following the successful completion of the 1 grand challenge by  stanley   a modified volkswagen touareg  montemerlo et al.  1    now we need to teach them how to drive in traffic   johnson  1 .
　while autonomous vehicles driving in traffic may seem to be a long way off  advances in ai  and more specifically  intelligent transportation systems  bishop  1   suggest that it may soon be a reality. cars can already be equipped with features of autonomy such as adaptive cruise control  gps-based route planning  rogers et al.  1; schonberg et al.  1   and autonomous steering  pormerleau  1; reynolds  1 . some current production vehicles even sport these features. in addition to the opel vectra mentioned above  daimlerbenz's mercedes-benz s-class has an adaptive cruise control system that can maintain a safe following distance from the car in front of it  and will apply extra braking power if it determines that the driver is not braking hard enough. both toyota and bmw are currently selling vehicles that can parallel park completely autonomously  even finding a space in which to park without driver input.
　once such autonomous vehicles are possible  it is only a matter of time before they become affordable  and then ubiquitous. a natural question that then arises is whether the current traffic control paradigms  which are designed for human drivers  are appropriate for such autonomous drivers. dresner and stone  have created and implemented a novel algorithm that enables cars and intersections to autonomously negotiate fine-grained reservations for the cars to pass through. we demonstrate that our approach can lead to more than a 1-fold decrease in delays at busy intersections when compared to standard approaches such as traffic lights. this foundational result opens the way to the investigation of multiagent learning and market-based methods for autonomous vehicle navigation  a direction of inquiry that promises several years' worth of fruitful research challenges in multiagent reasoning and machine learning.
1 conclusion
in summary  the most exciting research topics to me are those inspired by challenging real-world problems. furthermore  successful research results involve 1  fully implemented solutions; 1  general algorithms that transcend individual domains; and 1  theoretical explanations for  or bounds on  the effectiveness of these algorithms. from where we stand today  there is both the need and the foundation for such contributions in ai aimed at enabling fully autonomous agents  both in software and physically embodied  to learn and interact with one another.
　throughout this paper i have summarized what i see to be the most interesting and promising areas for current and future research pertaining to machine learning  multiagent reasoning  and robotics with the ultimate goal of enabling the creation of robust  fully autonomous agents that are able to learn and interact with one another. by leveraging new theoretical results to inform practical implementation  and by taking advantage of innovations from concrete multiagent applications to inspire new theory  i believe that the field can continue to make fast and exciting progress towards this goal.
acknowledgments
the research presented in this paper would not have been possible without the active collaboration of my colleagues and students  most notably my ph.d. advisor manuela veloso  and my students in the learning agents research group at the university of texas at austin including mazda ahmadi  kurt dresner  peggy fidelman  nicholas k. jong  nate kohl  gregory kuhlmann  david pardoe  mohan sridharan  daniel stronger  matthew e. taylor  shimon whiteson  and jonathan wildstrom. this research was supported in part by nsf career award iis-1  darpa grant hr1-1  darpa/afrl grant fa1-1  onr yip award n1-1  an ibm faculty award  and a sloan foundation fellowship.
