
automating the task of scoring short handwritten student essays is considered. the goal is to assign scores which are comparable to those of human scorers by coupling two ai technologies: optical handwriting recognition and automated essay scoring. the test-bed is that of essays written by children in reading comprehension tests. the process involves several image-level operations: removal of pre-printed matter  segmentation of handwritten text lines and extraction of words. recognition constraints are provided by the reading passage  the question and the answer rubric. scoring is based on using a vector space model and machine learning of parameters from a set of human-scored samples. system performanceis comparableto that of scoring based on perfect manual transcription.
1 introduction
handwritten essays are widely used for student performance evaluation in schools and colleges. since this approach to evaluation is efficient and reliable it is likely to remain a key componentof learning. assessing largenumbersof handwritten essays is a relatively time-consuming and monotonous task. in statewide examinations on reading comprehension in the u.s. there is an intense need to speed up and enhance the process of rating handwritten essays while maintaining cost effectiveness. the assessment can also be used as a source of timely  relatively inexpensive and responsible feedback about writing.
　writing done by hand is the primary means of testing students on state assessments of reading comprehension. consider as an example the new york state english language assessment  ela  administered statewide in grades 1 and 1. in the reading part of the test the student is asked to read a passage such as that given in fig 1 and answer several questions in writing.
　an example of a reading comprehension question based on the passage of fig. 1 is the following: how was martha washington's role as first lady different from that of eleanor roosevelt  use information from american first ladies in your answer.  the completed answer sheets of three different students to the question are given in fig. 1. the answers are

figure 1: from the new york english language arts assessment for grade 1  1 - two of three pages of the story  american first ladies  are shown.

figure 1: sample answer sheets of three students  a-c  based on the reading comprehension passage of fig. 1. the human assigned scores for these essays  on a scale of 1  were 1  1 and 1 respectively.
scored by human assessors on a seven-point scale of 1. a rubric for the scoring is given in table 1. this is referred to as a holistic rubric- which is in contrast to an analytic rubric that captures several writing traits.
　there is significant practical and pedagogical value in computer-assisted evaluation of such tests. the task of scoring and reporting the results of these assessments in a timely manner is difficult and relatively expensive. there is also an intense need to test later in the year for the purpose of capturing the most student growth and at the same time meet the requirement to report student scores before summer break. the biggest challenge is that of reading and scoring the handwritten portions of large-scale assessments.
111understandingunderstandinglogicalpartialreadablebriefof textroles of first ladiesand accurateunderstandingunderstanding oforganizedonly literaldrawingnot logicalrepetitivesimilarities andunderstandingconclusionsdifferencesof articleabout roles ofamong the rolesfirst ladiescharacteristicsnot thoroughlyorganizedsketchylimitedunderstoodof first ladieseleaborateunderstandingonly sectionscomplete tooweakaccurate and insightfulgeneralizedfocused facts withoutfluent and engagingsynchronizationtable 1: holistic rubric chart for  how was marth washington's role as first lady different from that of eleanor roosevelt  　the assessment problem is a well-constrained problem in artificial intelligence  ai  whose solution will push forward existing technologies of handwriting recognition and automatic essay scoring. the task is a first step in solving an inverse of a grand challenge of ai- that of a computer program to read a chapter of a freshman physics textbook and answer the questions at the end  reddy  1 .
1 component ai subsystems
the major component ai systems for solving the task are optical handwriting recognition  ohr  and automatic essay scoring  aes . both subsystems involve a learning phase.
1 handwriting recognition
ohr is concernedwith transformingan image of handwritten text into its textual form; a survey of ohr is  plamondon and srihari  1 . while computers have become indispensable tools for two of three r's  viz.  arithmetic and writing  their use in the third r of reading is still emerging. ohr involves several processing steps such as form  or rule line  removal  line/word segmentation and recognition of individual words. word recognition relies on a lexicon of words-which could be derived from the passage  question and rubric available in statewide tests.
　recognition of characters and words is performed in a two step process of feature extraction followed by classification. features can be at the character level  called analytic recognition  or at the word level  holistic recognition . word recognition  which is the task of assigning a word image to a member of a list of words  or lexicon   can be performed well for correctly segmented words with small lexicons. the process is error prone for mis-segmented text and large lexicons.
　when the lexicon is limited  a majority of the words are correctly recognized although there are substitution errors and missed words. these errors can be reduced by better word segmentation and by using linguistic context in the form of transitional probabilities between words  between partsof-speech tags  noun phrases  etc. it is possible that certain words  when considered in isolation  are illegible. local context can resolve such ambiguity. the reading comprehension passage and the rubric provide a rich source of contextual information that can be exploited to get high recognition rates. however  the task itself is one of correct interpretation rather than that of recognizingevery illegible word. it includes character recognition  ocr   word recognition  part-of-speech tagging  etc.
　prior to ohr  several image processing steps need to be performed on answer sheets  e.g.  detecting and eliminating extraneous information such printed instructions  questions  ruled lines and margin lines. within the handwritten text the ordering of the lines has to be determined and within each line the words need to be segmented. these operations are similar to those for analyzing unconstrained handwritten pages for forensic  or questioned document  analysis  srihari et al.  1  .
　handwriting interpretationis where the goal is not so much one of recognizing every character and word perfectly but to perform some overall task using recognition results. it involves using contextual information when there is uncertainty in the specific components. such approaches have found success when the domain is limited and contextual information is available  e.g.  in the postal domain the destination zip+1 code can be assigned even when individual components are poorly written  srihari and keubert  1 .
1 automatic essay scoring  aes 
automatic scoring of computer readable essays has been a topic of research for over four decades. a limitation of all past work is that the essays or examinations have to be in computer readable form. a survey of previous aes methods has been made by palmer  et. al  1 . project essay grade  peg   page  1 uses linguistic features fromwhich a multiple regression equation is developed. in the production automated essay grading system a grammar checker  a program to identify words and sentences  software dictionary  a part-of-speech tagger  and a parser are used to gather data. e-rater  burstein  1  uses a combination of statistical and nlp techniques to extract linguistic features. larkey  1  implemented an aes approach based on text categorization techniques  tct . one approach to aes is based on an information retrieval technique known as latent semantic indexing. its application to aes  known as latent semantic analysis  lsa   uncovers lexical semantic links between an essay and a gold standard. landauer  et. al.  1  developed the intelligent essay assessor using lsa. a matrix for the essay is built  and then transformed by the algebraic method of singular value decomposition  svd  to approximately reproduce the matrix using reduced dimensional matrices built for the topic domain. using svd new relationships between words and documents are uncovered  and existing relationships are modified to represent their significance. using lsa the similarity between two essays can be measured despite differences in individual lexical items. where a gold standard exists  lsa is a robust approach. it correlates as closely with human raters as human raters correlate with each other  landauer et al.  1 .
1 system integration
the overall task is that of handwriting interpretation for essay scoring. a first level of integration is to sequentially couple the ohr and aes systems by regarding ohr simply as a transcription system. both the ohr and aes components involve machine learning at several levels. in the case of ohr  lexicons are acquired from three sources: reading passage  question and rubric. learning of handwriting styles in the formation of letters and words is a classic pattern recognition problem. in the case of aes the learning process acquires a method associating content to score by learning from a set of human scored essays. a system to analyze and score the scanned answer sheet s is shown in fig. 1.
1 ohr
after performing image pre-processing steps  e.g.  foreground/background extraction  eliminating non-informative material  rule lines and printed markings   determining the presence of handwriting  etc.  the main tasks are:
　 1 word segmentation into lines and words in the presence of ambiguity. to determine whether a gap is a true gap or not by learning from the current document.
　 1  word recognition: when vocabularies are large contextual information needs to be exploited to dynamically limit word choices. contextual information is available in the form of the passage to be read and the answer rubric.
　after words are recognized the resulting word sequences are written to text files. these text files are then pre-processed for aes which include the following steps:  a . removing punctuations and special characters   b . converting upper case to lower case for generalization   c . stop word removal - removing common words such as a and the which occur very often and are not of significant importance   d . stemming - morphological variants of words have similar semantic interpretations and therefore a stemming algorithm is used to reduce the word to its stem or root form. the algorithm  porter  1   uses a technique called suffix stripping where an explicit suffix list is provided along with a condition on

figure 1: answer processor architecture.
which the suffix should be removed or replaced to form the stem of the word  which would be common among all variations. for example the word reading after suffix stripping is reduced to read.
1 aes
in the lsa approach  a good approximation of the computer score to a human score heavily depends on the optimal reduced dimensionality. this optimal dimension is related to the features that determine the term meaning from which we can derive the hidden correlations between terms and answer documents. reducing the dimensions is done by omitting inconsequential relations and retaining only significant ones. a factor analysis method such as singular value decomposition  svd  helps reduce the dimensionality to a desired approximation.
　the first step in lsa is to construct a t x n term-bydocument matrix m whose entries are frequencies. svd or two-mode factor analysis decomposes this rectangular matrix into three matrices  baeza-yates and ribeiro-neto  1 . the svd for a rectangular matrix m can be defined as
	 	 1 
where prime    indicates matrix transposition  m is the rectangular term by document matrix with t rows and n columns  t is the t x m matrix  which describes rows in the matrix m as left singular vectors of derived orthogonal factor values  d is the n x m matrix  which describes columns in the matrix m as right singular vectors of derived orthogonal factor values  s is the m x m diagonal matrix of singular values such that

when t  s and d are matrix multiplied mis reconstructed  and m is the rank of m = min t   n .
　to reduce dimensionality to a value k from the matrix s we have to delete m   k rows and columns starting from those which contain the smallest singular value to form the matrix

figure 1: projected locations of 1 answer documentsin two dimensional plane

s1. the corresponding columns in t and rows in d are also deleted to form matrices t1 and respectively. the matrix m1 is an approximation of matrix m with reduced dimensions as follows
	.	 1 
standard algorithms are available to perform svd. to illustrate  from the document-term matrix constructed from 1 essays from the american first ladies example shown in fig 1 and fig 1  the first two principal components are plotted in fig.1. the principal components are the two most significant dimensions of the term by document matrix shown in table 1 after applying svd. this is a representation of the documents in semantic space. the similarity of two documents in such a semantic space is measured as the cosine of the angle made by these documents at the origin.
　the testing set consists of a set of scored essays not used in the training and validation phases. the term-documentmatrix constructed in the training phase and the value of k determined from the validation phase are used to determine the scores of the test set.
1 performance evaluation
the corpus for experimentation consisted of 1 handwritten answer essays for the  american first ladies  task shown in fig. 1. of these essays 1 were by students and 1 were by teachers. each of the 1 answer essays were manually assigned a score  the  gold standard   by education researchers. the essays were scored manually using the holistic grading rubric shown in table 1. the essays were divided into 1 training samples  each of which also served as validation samples in the leave one out cross validation method employed  and 1 testing samples. the training set had a human-score distribution on the seven-point scale as follows: 1 1 1 1. the testing set had human-score distributions of 1 1 1 1. the answer sheets were scanned as gray scale images at a resolution of 1 pixels per inch.

figure 1: comparison of human scores and manual transcription - latent semantic analysis  mt-lsa  scores on 1 student responses to the american first ladies question: mtlsa scores  open circles  are within 1 of human scores
 stars .
　two different sets of 1 transcribed essays were created  the first by manual transcription  mt  and the second by the ohr system. the lexicon for the ohr system consisted of unique words from the passage to be read  which had a size of 1. separate training and validation phases were conducted for the mt and ohr essays. for the mt essays  the document-term matrix m had t = 1 and m = 1 and the optimal value of k was determined to be 1. for the ohr essays  the corresponding values were t = 1  m = 1 and k = 1. the smaller number of terms in the ohr case is explained by the fact that several words were not recognized. comparisons of the human-assigned scores  the goldstandard  with  i  automatically assigned scores based on mt is shown in fig. 1 and  ii  automatically assigned scores based on ohr is shown in 1. using mt the human-machine mean difference was 1  fig. 1 . using ohr the humanmachine difference was 1  fig. 1 . thus a 1 difference is observed between mt and ohr using lsa scoring. these preliminary results demonstrate the potential of the method for holistic scoring and robustness with ohr errors.
1 summary and discussion
automatically evaluating handwritten essays involves the integration of optical handwriting recognition and automatic essay scoring methodologies. handwriting recognition is assisted by constraints provided by the reading passage  question and rubric. scoring based on latent semantic analysis  lsa is robust with respect to recognition inadequacies. results on a small testing set show that with manually transcribed  mt  essays  lsa scoring has on an average less than a two-point difference from human scoring. with the same test set  ohr-lsa scoring has a minor difference from mt-

figure 1: comparison of human scores and ohr-lsa scores on 1 student responses to the american first ladies question: ohr-lsa scores  open circles  are within 1 of human scores  stars .
lsa scoring.
　the results point out that despite errors in word recognition the overall scoring performance is good enough to have practical value. this points out that when the evaluation of an ohr system is based not so much on word recognition rates but in terms of the overall application in which it is used  the performance can be quite acceptable. the same phenomenon has been observed in other ohr applications such as postal address reading where the goal is not so much as to read every word correctly but achieve a correct sortation.
　the lsa approach has the advantage that as a  bag of words  or holistic technique it is robust with respect to word recognition errors. however it ignores linguistic structures. the analytic approach to scoring is based on idea development  organization  cohesion  style  grammar  or usage conventions. the result of analytic scoring will be more useful to teachers and education researchers.
　essay scoring based on language features such as general vocabulary  passage related vocabulary  percentage of difficult words  percentage of passive sentences  rhetorical features and usage of conjunctions  pronouns  punctuations etc for connectedness could play a significant role in improving the performanceof this system. this approach is employed in the automated japanese essay scoring system: jess  ishioka and kameda  1  where the final weighted score is calculated by penalizing a perfect score based on features recognized in the essay.
