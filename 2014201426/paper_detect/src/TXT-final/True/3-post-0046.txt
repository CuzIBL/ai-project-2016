maintaining arc consistency using adaptive domain ordering1
chavalit likitvivatanavong
cork constraint computation centre
university college cork  irelandyuanlin zhang
department of computer science
texas tech university  texas  u.s.james bowen and eugene c. freuder
cork constraint computation centre  university college cork  ireland

1 introduction
solvinga constraint satisfaction problem csp by maintaining arc consistency  mac   sabin and freuder  1  has been one of the most widely-used methods. since arc consistency  ac  is enforced at every node in a search tree  its efficiency is critical to the whole algorithm. we propose a new mac algorithm based on ac-1  zhang and yap  1  in which the ac component is capable of starting from where it left off in its previous execution with low overhead. it has the following properties:  1  o ed1  worst-case time complexity in any node and any branch of the search tree;  1  o ed  space complexity;  1  the ability to avoid a type of redundant constraint check called a negative repeat;  1  no recomputation and maintenance of its internal data structures upon backtrack.
1 definitions
in this paper we consider csps with binary constraints and use dx  n  e  and d to denote the current domain of variable x  the number of variables  the number of constraints  and the maximumdomain size respectively. we also assume a domain to be totally-ordered and adopt a random-access doubly linked-list implementation. the terms head and tail denote the boundary of a domain. a constraint check between value a （ dx and value b （ dy is denoted by cxy  a b .
　a propagation-oriented backtrack search algorithm for csps is the standard depth-first backtrack search framework augmented by some process that handles all constraint propagation and the maintenance of the internal data structures involved. the search tree is defined by associating each node with a variable assignment of the algorithm. node complexity of the algorithm is the time complexity cost of the constraint propagation performed at each node. path complexity is the aggregate cost for any path in the search tree  summing the cost of every node in succession  starting from the root  to a leaf. during search some constraint checks may be repeated many times even though the constraint processing component is optimal in a single execution. we define the following type of redundant check called a negative repeat. negative repeats can be troublesome for hard problems that require a large amount of backtracking.
definition 1  negative repeat  a constraint check cxy  a b  performed at time t during search is called a
negative repeat with respect to y if and only if:
 1  cxy  a b  = false  and
 1  cxy  a b  has been performed at time s where s   t  and  1  b has been continuously present in the time interval  s t .
1 adaptive domain ordering
in this section we introduce a new mac algorithm called adaptive domain ordering  ado . the key feature is the dynamic rearrangement of variable domains after each backtrack. a pruned value is simply restored to the end of its domain  rather than its initial position. this technique makes the algorithm capable of avoiding all negative repeats as well as requiring no maintenance on its internal data structure. unlike ac-1  on which the constraint processing unit of ado is based  we associate the following two invariants for last x a y . first  the safety invariant: there exists no support of a in {c （ dy | c   last x a y }. second  the prospect invariant: there exists a support of a in {c （ dy | c − last x a y }. ado is a propagation-oriented backtrack search algorithm and behaves like mac-1 for the most part. the main differences lie in the routines for removing a value  remove   finding a support  and restoring a value  restore . when a value is removed  any last that points to it will be moved to the next value. when no support for a （ dx is found in dy   last x a y  will be made to point to tail of y . when a value is restored  all the last pointers that point to the tail will be made to point to the restored value instead. it can be proved that  given any path in a search tree  the worst-case aggregate complexity of remove is o ed1 . due to space restriction we are not able to give detailed proofs of the complexity cost and correctness of ado. an example trace of the algorithm is shown as follows.
　example consider dx = {a b c d}  dy = {1 1} and cxy ={ a 1   b 1   c 1   d 1 }  allowed tuples . the result after the initial ac processing1 is shown in figure  1 : last x a y =1  last x b y =1  last x c y =1  and last x d y =1. suppose a and d are removed due to some external cause; their last values remain unchanged  1 . next  suppose 1 is removed. the result after ac processing is shown in  1 . at the next search level suppose 1 is removed. according to the algorithm  any last that points to 1 - including last x a y  - will be shifted to 1  1 . figure  1  shows the result after the problem is made arc consistent. now consider the network after backtrack. since 1 is removed after 1  it must be restored before. figure  1  shows the result after 1 is restored. all the last pointers that pointed to tail are moved to 1. notice that b is restored as well because it was pruned at the same level as 1. figure  1  shows the result after 1 is restored. figure  1  shows the network after the search backtracks to the point where we started this example. note how the last pointers and the domain ordering differ from  1 . 1
　to make ado efficient  we need another data structure associated with each value to account for the last pointers that point to it. this is called buffer. its elements can be preallocated since a value has only one last pointer in any constraint. we then make a last pointer refer to a value's buffer instead of the value itself - i.e.  a x  （ buffer b y  iff last x a y  points to buffer b y . this allows a set of last pointers to be switched all at once just by rearranging related buffers. for example  consider figure  1   a more detailed view of  1 . when 1 is restored  we want to move all the pointers from tail to 1. this can be done simply by swapping the two buffers  1 . as a result restore takes o 1  time.
　we use a similar technique for remove. we compare the buffer of the value to be removed with that of the next value in the current domain  or tail if there is none  and swap both buffers if the first contains more elements. for an example  consider figure  1   which is the detailed view of figure  1 . when value 1 is removed  its buffer size is compared with the buffer size of value 1. since it has more elements  we move pointers from the buffer for value 1 into that for value 1 and swap both buffers  which results in  1 .
　given any path in a search tree  the worst-case aggregate complexity of remove using buffer with the above techniques is o edlgd . we can further reduce the cost by representing data in a buffer as a rooted tree and having the root of the smaller tree point to that of the larger one. in fact  this is the union-by-rank operation for the disjoint-set union problem. remove then takes o 1  time. however  locating the value of a last pointer no longer takes constant time. by using path compression  its worst-case aggregate complexity is o ed α ed1   where α is the inverse ackerman's function  which is almost constant  tarjan  1 .
　ado has o ed1  for node and path complexity since the cost related to buffer is subsumed by the standard cost of establishing arc consistency. correctness follows because:  1  the prospect invariant states that there is a support somewhere for each value  thereby enforcing arc consistency;  1  the safety invariant states that no support is skipped. note that because the problem is made arc consistent before the search starts  we can always expect a value to have a support. if a value is subsequently removed  it is only because all of its supports are pruned; these supports are still in the original domain. the existence of a support in the original domain is central in the correctness proofs  and without the ac preprocessing before the search starts ado would not be correct.
1 conclusions
we have designed ado to explore the theoretical limits of mac and  as far as we know  it achieves the best results and outperforms all other existing mac algorithms. specifically  ado has o ed1  worst-case time complexity in any node and any branch of the search tree while using only o ed  space. this resolves the trade-off between the two traditional implementations of mac-1. the first one records every change made to last so that after backtrack the algorithm can start from the exact same state. although both node and path complexity for this approach is o ed1   its space complexity is o edmin n d  . the second approach resets last in every search node to keep the space at o ed   but this comes at the expense of path complexity  which becomes o ned1 .
　re＞gin  also aims to create a maintenance-free mac algorithm that has the best features from the two implementations of mac-1. the algorithm in  re＞gin  1  is both node and path optimal while using o ed  space. however  the last structure needs to be recomputed and updated after each backtrack besides the normal restoration of pruned values. it cannot avoid negative repeats. by contrast  ado requires no recomputationand no updateof its internal structure and is able to avoid negative repeats.
