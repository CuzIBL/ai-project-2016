
a new algorithm  neighborhood minmax projections  nmmp   is proposed for supervised dimensionality reduction in this paper. the algorithm aims at learning a linear transformation  and focuses only on the pairwise points where the two points are neighbors of each other. after the transformation  the considered pairwise points within the same class are as close as possible  while those between different classes are as far as possible. we formulate this problem as a constrained optimization problem  in which the global optimum can be effectively and efficiently obtained. compared with the popular supervised method  linear discriminant analysis  lda   our method has three significant advantages. first  it is able to extract more discriminative features. second  it can deal with the case where the class distributions are morecomplex than gaussian. third  the singularity problem existing in lda does not occur naturally. the performance on several data sets demonstrates the effectiveness of the proposed method.
1 introduction
linear dimensionality reduction is an important method when facing with high-dimensional data. many algorithms have been proposed during the past years. among these algorithms  principal component analysis  pca   jolliffe  1  and linear discriminant analysis  lda   fukunaga  1  are two of the most widely used methods. pca is an unsupervised method  which does not take the class information into account. lda is one of the most popular supervised dimensionality reduction techniques for classification. however  there exist several drawbacks in it. one drawback is that it often suffers from the small sample size problem when dealing with high dimensional data. in this case  the withinclass scatter matrix sw may become singular  which makes lda difficult to be performed. many approaches have been proposed to address this problem  belhumeur et al.  1; chen et al.  1; yu and yang  1 . however  these variants of lda discard a subspace and thus some important discriminative information may be lost. another drawback in lda is its distribution assumption. lda is optimal in the case that the data distribution of each class is gaussian  which can not always be satisfied in real world applications. when the class distribution is more complex than gaussian  lda may fail to find the optimal discriminative directions. moreover  the number of available projection directions in lda is smaller than the class number  duda. et al.  1   but it may be insufficient for many complex problems  especially when the number of class is small.
¡¡for the distance metric based classification methods  such as the nearest neighbor classifier  learning an appropriate distance metric plays a vital role. recently  a number of methods have been proposed to learn a mahalanobis distance metric  xing et al.  1; goldberger et al.  1; weinberger et al.  1 . linear dimensionality reduction can be viewed as a special case of learning a mahalanobisdistance metric see section 1 . this viewpoint can give a reasonable interpretation for the fact that the performance of nearest neighbor classifier can always be improved after performing linear dimensionality reduction.
¡¡in this paper  we propose a new supervised linear dimensionality reduction method  neighborhood minmax projections  nmmp . the method is largely inspired by the classical supervised linear dimensionality reduction method  i.e.  lda  and the recent proposed distance metric learning method  large margin nearest neighbor  lmnn  classification  weinberger et al.  1 . in our method  we focus only on the pairwise points where the two points are neighbors of each other. after the transformation  we try to pull the considered pairwise points within the same class as close as possible  and take those between different classes apart. this goal can be achieved by formulating the task as a constrained optimization problem  in which the global optimum can be effectively and efficiently obtained. compared with lda  our method avoids the three drawbacks in lda discussed in above. compared with the lmnn method  our
method is computationally much more efficient. the performance on several data sets demonstrates the effectiveness of our method.

figure 1: in the left figure  point a and b belong to the same class i  and the two circles denote the within-class neighborhood of a and b respectively. a is b's within-class neighborhood and b is a's within-class neighborhood. after the transformation  we try to pull the two points as close as possible; in the right figure  point a belongs to class i  point b belongs to class j  and the two circles denote the between-class neighborhood of a and b respectively. a is b's between-class neighborhood and b is a's between-class neighborhood. after the transformation  we try to push the two points as far as possible.
1 problem formulation
given the data matrix x =  x1 x1 ... xn  xi ¡Ê rd  our goal is to learn a linear transformationw : rd ¡ú rm  where w ¡Ê rd¡Ám and wtw = i. i is m ¡Á m identity matrix.
then the original high-dimensionaldata x is transformed into a low-dimensional vector:
	y = wtx	 1 
¡¡let each data point of class i have two kinds of neighborhood: within-class neighborhood nw i  and between-class neighborhood nb i   where nw i  is the set of the data's kw i  nearest neighbors in the same class i and nb i  is the set of the data's kb i  nearest neighbors in the class other than i. obviously  1 ¡Ü kw i  ¡Ü ni   1  and 1 ¡Ü kb i  ¡Ü n   ni  where ni is the data number of class i.
¡¡here  we focus only on the pairwise points where the two points are neighbors of each other. after the transformation  we hope that the distance of the considered pairwise points within the same class will be minimized  while the distance of those between different classes will be maximized.  see figure 1 .
¡¡after the transformation w  the sum of the euclidean distances of the pairwise points within the same class can be formulated as:
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡sw = tr wt s ww   1  where tr ¡¤  denotes the trace operator of matrix  and
s w =
¡¡here  ci denote the class label of xi  and cj denote the class label of xj. obviously  ci = cj  and s w is positive semi-definite.
¡¡similarly  the sum of the euclidean distances of the pairwise points between different classes is:
	sb = tr wt s bw 	 1 
where
s b =
here   and s b is positive semi-definite too.
¡¡to achieve our goal  we should maximize sb while minimize sw. the following two function can be used as objective:
		 1 
		 1 
as it is difficult to determine a suitable weight ¦Ë for the former objective function  we select the latter as our objective to optimization. in fact  as we will see  the latter is just a special case of the former  where the weight ¦Ë is automatically determined.
¡¡therefore  we formulate the problem as a constrained optimization problem:
	w  = argwmaxtw=i tr wts ww 	 1 
fortunately  the globally optimal solution of this problem can be efficiently calculated. in the next section  we will describe the details for solving this constrained optimization problem.
1 the constrained optimization problem
we address the above optimization problem in a more general form which is described as follows:
¡¡the constrained optimization problem: given the real symmetric matrix a ¡Ê rd¡Ád and the positive semi-definite matrix b ¡Ê rd¡Ád  rank b  = r ¡Ü d. find a matrix
w ¡Ê rd¡Ám that maximize the following objective function with the constraint of wtw = i:
	 	tr wt aw 
	w = argwmaxtw=i tr wtbw 	 1 
¡¡at first  we propose lemma 1  which shows that when wtw = i and m   d   r  the value of tr wtbw  will not be equal to zero.
¡¡lemma1. suppose w ¡Ê rd¡Ám  wtw = i  b ¡Ê rd¡Ád is a positive semi-definite matrix  and rank b  = r ¡Ü d m   d   r  then it holds that tr wtbw    1.
¡¡proof. according to the result of rayleigh quotient  golub and van loan  1   min tr wt bw  = m   wtw=i
where ¦Â1 ¦Â1 ... ¦Âm are the first m smallest eigenvalues of b. as b is positive semi-definite  rank b  = r  and m   d r  then. therefore  with the constraint of wtw = i  tr wtbw  ¡Ý mintr wt bw    1. 
thus we discuss this optimization problem in two cases.
case 1: m   d   r 
lemma 1 ensures that the optimalvalue is finite in this case.
suppose the optimal value is ¦Ë   guo  has derived that tr wt a ¦Ë b w .
w
note that tr wt bw    1  so we can easy to see  max tr wt  a   ¦Ëb w    1   ¦Ë   ¦Ë   and
wtw=i
	tr wt a ¦Ëb w  	  ¦Ë   ¦Ë .
w
¡¡on the other hand  max tr wt a   ¦Ëb w  = ¦Ã  wtw=i where ¦Ã is the sum of the first m largest eigenvalues of a   ¦Ëb. given a value ¦Ë  if ¦Ã = 1 then ¦Ë is just the optimal value  otherwise ¦Ã   1 implies ¦Ë is smaller than the optimal value and vice versa. thus the global optimal value of the problem can be obtained by an iterative algorithm. subsequently  in order to give a suitable value ¦Ë  we need to determine the possible bound of the optimal value. theorem 1 is proposed to solve this problem.
¡¡theorem 1. given the real symmetric matrix a ¡Ê rd¡Ád and the positive semi-definite matrix b ¡Ê rd¡Ád  rank b  = r ¡Ü d. if w1 ¡Ê rd¡Ám1  w1 ¡Ê rd¡Ám1 and m1   m1  
	tr wt	 	
d   r  then w w1=	1	w1tmaxw	tr w1tbw1 .
	1=i
the proof of theorem 1 is based on the following lemma:
¡¡lemma 1. if  i ai ¡Ý 1 bi   1 and   then.
proof. let. so  i ai ¡Ý 1 bi   1  we have ai ¡Ü
qbi. therefore	
now we give the proof of theorem 1 in the following.
proof of theorem 1. suppose w1  =  w1 w1 ... wm1 
and w1  = arg	max	tr  ww1t	1 .	let  h 
wi
without loss of generality  we suppose	 1 	p 1  
¡¡¡¡ 1 	p 1  	tr wpt h awp h   ¡Ü ¡¤¡¤¡¤ ¡Ü	 	p h bwp h   where wp i 	¡Ê rd¡Ám1 is the i-th combination of w1 w1 ... wm1 with m1 elements note that m1   m1   so the number of combinations is h.
¡¡let  note that each of wj 1 ¡Ü j ¡Ü m1  occurs l times in {wp 1  wp 1  ... wp h }. according to lemma
1  we have
	aw1 	l¡¤tr w  taw   
	wt	1=	=	¡¤  w1  	=
1

tr wpt h awp h  
	p h 	p h 	w w1=	bw1 
1
¡¡according to theorem 1 we know  with the reduced dimension m increases  the optimal value is decreased monotonously. when m = d  the optimal value is equal to
tr a 	tr wtaw  . so	max	¡Ý	.
tr b 	wtw=i
¡¡on the other hand  max tr wtaw  =  and wtw=i min tr wt bw  =   where ¦Á1 ¦Á1 ... ¦Ám wtw=i
are the first m largest eigenvalues of a  and ¦Â1 ¦Â1 ... ¦Âm are the first m smallest eigenvalues of b. therefore  max tr wtaw  ¡Ü ¦Á1+¦Á1+¡¤¡¤¡¤+¦Ám.
wtw=i
as a result  the bound of the optimal value is given by
tr a 	tr wtaw 	1
¡¡now  we obtain an iterative algorithm for obtaining the optimal solution  which is described in table 1. from the algorithm we can see  only a few iterative steps are needed to obtain a precise solution. note that the algorithm need not calculate the inverse of b  and thus the singularity problem does not exist in it naturally.
case 1: m ¡Ü d   r 
¡¡in this case  when w lies in the null space of matrix b  then tr wtbw  = 1  the value of the objective function becomes infinite. therefore  we can reasonably replace the optimization problem with v  = arg max tr vt ztaz v   where v ¡Ê r d r ¡Ám  and
vtv=i
z =  z1 z1 ... zd r  are the eigenvectors corresponding to d   r zero eigenvalues of b.
¡¡we know that v  =  ¦Ì1 ¦Ì1 ... ¦Ìm   where ¦Ì1 ¦Ì1 ... ¦Ìm are the first m largest eigenvectors of
ztaz. so  in this case  the final solution is w  = z ¡¤ v 
input:
¡¡the real symmetric matrix a ¡Ê rd¡Ád and the positive semi-definite matrix b ¡Ê rd¡Ád  rank b  = r ¡Ü d. the error constant .
output:
¡¡projection matrix w   where w  ¡Ê rd¡Ám and w tw  = i.
in the case of : m   d   r.
¡¡1.	  where ¦Á1 ¦Á1 ... ¦Ám are the first m largest eigenvalues of a  ¦Â1 ¦Â1 ... ¦Âm are the first m smallest eigenvalues of b.
	1.while	  do
a  calculate ¦Ã  where ¦Ã is the sum of the first m
largest eigenvalues of a   ¦Ëb.
b  if ¦Ã   1  then ¦Ë1 ¡û ¦Ë  else ¦Ë1 ¡û ¦Ë.
c  ¦Ë ¡û ¦Ë1¦Ë1 .
end while.
¡¡w  =  ¦Í1 ¦Í1 ... ¦Ím   where ¦Í1 ¦Í1 ... ¦Ím are the first m largest eigenvectors of a   ¦Ëb. in the case of : m ¡Ü d   r.
¡¡w  = z ¡¤  ¦Ì1 ¦Ì1 ... ¦Ìm   where ¦Ì1 ¦Ì1 ... ¦Ìm are the first m largest eigenvectors of ztaz  and z =  z1 z1 ... zd r  are the eigenvectors corresponding to d   r zero eigenvalues of b.table 1: the algorithm for the optimization problem
1 neighborhood minmax projections
the method of neighborhood minmax projections nmmp  is described in table 1 . in order to speed up  pca can be used as a preprocessing step before performing nmmp.
¡¡denote the covariance matrix of data by st  and denote the null space of st by ¦Õ  the orthogonalcomplementof ¦Õ by ¦Õ¡Í.
1. preprocessing: eliminate the null space of the covariance matrix of data  and obtain new data x =  x1 x1 ... xn  ¡Ê rd¡Án  where rank x  = d
1. input:
x =  x1 x1 ... xn  ¡Ê rd¡Án  kw i   kb i   m
1. calculate s w and s b according to eq. 1  and eq. 1  1. calculate w using the algorithm described in table 1
1. output:
y = wtx  where w ¡Ê rd¡Ám and wtw = i.table 1: algorithm of nmmp
it is well known that the null space of st can be eliminated without lose of any information. in fact  it can be easy to prove that the null space of st comprises the null space of
s w and the null space of s b defined in section 1. suppose
w ¡Ê ¦Õ¡Í and ¦Î ¡Ê ¦Õ  then
	 w + ¦Î ts b w + ¦Î 	wts bw
	=	 1 
	 w + ¦Î ts w w + ¦Î 	wts ww
¡¡eq. 1  demonstrates that eliminating the null space of the covariance matrix of data will not affect the result of the proposed method. thus we use pca to eliminate the null space of the covariance matrix of data.
1 discussion
our method is closely connected with lda. both of them are supervised dimensionality reduction methods  and the goals are also similar. they both try to maximize the scatter between different classes  and minimize the scatter within the same class. the matrix s b defined in eq. 1  and s w defined in eq. 1  are parallel to the between-class scatter matrix sb and within-class scatter matrix sw in lda respectively. in fact  when the number of neighbors reaches the number of the total available neighbors kw i  = ni 1  and kb i  = n ni  where ni is the data number of class i  and n is the number of total data   we have s b + s w = n1st  which is similar to
sb + sw = st in lda.
¡¡however  in comparison with lda  we do not impose the faraway pairwise points within the same class to be close to each other  which makes us focus more on the improvement of the discriminability of local structure. this property is especially useful when the distribution of class data is more complex than gaussian. we give a toy example to illustrate it figure 1 .
¡¡the toy data set consists of three classes shown by different shapes . in the first two dimensions  the classes are distributed in concentric circles  while the other eight dimensions are all gaussian noise with large variance. figure 1 shows the two-dimensional subspace learned by pca  lda and nmmp  respectively. it illustrates that nmmp can find a low-dimensional transformation preserving manifold structure with more discriminability.
¡¡moreover  compared with lda  our method is able to extract more discriminative features and the singularity problem existing in lda will not occur naturally.

	 1	 1	 1	 1	1.1	1	1	 1	 1	 1	1.1	1
	 c  lda	 d  nmmp
figure 1:  a  is the first two dimensions of the original tendimensional data set;  b   c   d are the two-dimensionalsubspace found by pca  lda and nmmp  respectively. it illustrates that nmmp can find a low-dimensional transformation preserving manifold structure with more discriminability.
distance metric learning is an important problem for
 the distance based classification method. learning a mahalanobis distance metric is to learn a positive semidefinite matrix m  and using the mahalanobis distance metric  xi   xj tm xi   xj  to replace the euclidean distance metric  xi   xj t xi   xj   where m ¡Ê . note that m is positive semidefinite  with the eigen-decomposition  m = vvt  where v =  ¦Ò1 ¦Ò1 ... ¦Òdvd   ¦Ò and v are eigenvalues and eigenvectors of m. therefore  the mahalanobis distance metric can be formulated as  vtxi  vtxj t vtxi  vtxj . in this form  we can see that learning a mahalanobis distance metric is to learn a weightedorthogonallinear transformation. nmmp learns a linear transformation w with the constraint of wtw = i. so it can be viewed as a special case of learning a mahalanobis distance metric  where the weight value ¦Òi is either 1 or 1. note that directly learning the matrix m is a very difficult problem and it is usually formulated as a semidefinite programming  sdp  problem  where the computation burden is extremely heavy. however  if we learn the transformation v instead of learning the matrix m  the problem will become much easier to solve.
1 experimental results
we evaluated the proposed nmmp algorithm on several data sets  and compared it with lda and lmnn method. the data sets we used belong to different fields  a brief description of these data sets is list on table 1.
¡¡we use pca as the preprocessing step to eliminate the null space of data covariance matrix st. for lda  due to the singularity problem existing in it  we further reduce the dimension of data such that the within-class scatter matrix sw is nonsingular.
¡¡in each experiment  we randomly select several samples per class for training and the remaining samples for testing. the average results and standard deviations are reported over 1 random splits. the classification is based on k-nearest neighbor classifier k = 1 in these experiments .
irisbalfacesobjectsuspsnewsclass111training number111testing number111input dimensionality111dimensionality after pca111table 1: a brief description of the data sets.
data setmethodprojection numberaccuracy % std. dev. % training time per run irisbaseline1.1.1-lda1.1.1slmnn1.1.1.1snmmp1.1.1.1sbalbaseline1.1.1-lda1.1.1slmnn1.1.1.1snmmp1.1.1.1sfacesbaseline1.1.1-lda1.1.1.1slmnn1.1.1.1snmmp1.1.1.1sobjectsbaseline1.1.1-lda1.1.1.1slmnn1.1.1.1snmmp1.1.1.1suspsbaseline1.1.1-lda1.1.1.1slmnn1.1.1.1snmmp1.1.1.1snewsbaseline1.1.1-lda1.1.1.1slmnn1.1.1.1snmmp1.1.1.1stable 1: experimental results in each data set.¡¡it is worth noting that the parameters in our method are not sensitive. in fact  in each experiment  we simply set kb i  to 1  and set kw i  to ni/1 for each class i  where ni is the training number of class i.
¡¡the experimental results are reported in table 1. we use the recognition result directly performed after the preprocessing by pca as the baseline.
in the following we describethe details of each experiment. the uci data sets
¡¡in this experiment  we perform on two small data sets  iris and balance  taken from the uci machine learning repository1. as the class distributions of this two data sets are not very complex  lda works well  and our method also demonstrates the competitive performance.
face recognition
¡¡the at&t face database  formerly the orl database  includes 1 distinct individuals and each individual has 1 different images. some images were taken at different times  and have variations  samaria and harter  1  including expression and facial details. each image in the database is of size 1 ¡Á 1 and with 1 gray-levels.
¡¡in this experiment  no other preprocessings are performed except the pca preprocessing step. the result of our method is much better than those of lda and the baseline. lmnn have a good performance too  but the computation burden is extremely heavy.
¡¡we also perform the experiments on many other face databases  and obtain the similar results  say  our method demonstrates the much better performances uniformly.
object recognition
¡¡the coil-1 database  nene et al.  1  consists of images of 1 objects viewed from varying angles at the interval of five degrees  resulting in 1 images per object.
¡¡in this experiment  each image is down-sampled to the size of 1 ¡Á 1 for saving the computation time.
¡¡similar to the face recognition experiments  the results of our method and lmnn are much better than those of lda and the baseline. note that both face images and object images distribute on an underlying manifold  the experiments verify that nmmp can preserve manifold structure with more discriminability than lda.
digit recognition
¡¡in this experiment  we focus on the digit recognition task using the usps handwritten 1¡Á1 digits data set1. the digits 1 1 and 1 are used in this experiment as the four classes. there are 1  1  1 and 1 examples for each class  with a total of 1.
¡¡on this data set  the baseline already works well  and our method still makes a little improvement. lda fails in this case  which demonstrates that the available projection number of lda may be insufficient when the data distributions are more complex than gaussian.
text categorization
¡¡in this experiment  we investigated the task of text categorization using the 1-newsgroups data set1. the topic rec which contains autos  motorcycles  baseball  and hockey was chosen from the version 1-news-1. the articles were preprocessed with the same procedure as in  zhou et al.  1 . this results in 1 document vectors in a 1dimensional space. finally the documents were normalized into tfidf representation.
¡¡our method and lmnn both bring significant improvements comparing with the baseline on this data set. in comparison  the performance of lda is limited in that the available projection number of it is only 1  which is insufficient for this complex task.
1 conclusion
in this paper  we propose a new method  neighborhood minmax projections  nmmp   for supervised dimensionality reduction. nmmp focuses only on the pairwise points where the two points are neighbors of each other. after the dimensionality reduction  nmmp minimizes the distance of the considered pairwise points within the same class  and maximizes the distance of those between different classes. in comparison with lda  nmmp focuses more on the improvement of the discriminability of local structure. this property is especially useful when the distribution of class data is more complex than gaussian. toy example and real world experiments are presented to validate it. moreover  other disadvantages of lda i.e.  the singularity problem of sw and the limitation of the available number of dimension are also avoided in our method.
¡¡as a linear dimensionality reduction method  nmmp can be viewed as a special case of learning a mahalanobis distance metric. usually  the computation burden of learning a mahalanobis distance metric is extremely heavy. our method formulates the problem as a constrained optimization problem  and the global optimum can be effectively and efficiently obtained. experiments demonstrate that our method has a competitive performance compared with the recent proposed mahalanobis distance metric learning method  lmnn  but the computation cost is much lower.
