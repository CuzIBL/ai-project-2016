 
in this paper  we show how simple and parallel techniques can be efficiently combined to compute dense depth maps and preserve depth discontinuities in complex real world scenes. our algorithm relies on correlation followed by interpolation. during the correlation phase the two images play a symmetric role and we use a validity criterion for the matches that eliminates gross errors: at places where the images cannot be correlated reliably  due to lack of texture or occlusions for example  the algorithm does not produce wrong matches but a very sparse disparity map as opposed to a dense one when the correlation is successful. to generate dense depth map  the information is then propagated across the featureless areas but not across discontinuities by an interpolation scheme that takes image grey levels into account to preserve image features. 
we show that our algorithm performs very well on difficult images such as faces and cluttered ground level scenes. because all the techniques described here are parallel and very regular they could be implemented in hardware and lead to extremely fast stereo systems. 
1 	introduction 
over the years numerous algorithms for passive stereo have been proposed  they can roughly be classified in two main categories  barnard et al. 1 : 
1. feature based. those algorithms extract features of interest from the images  such as edge segments or contours  and match them in two or more views. 
   *this research was supported in part under the centre national d'etudes spatiales vap contract and in part under a defense advanced research projects agency contract. 
1 	vision 
these methods are fast because only a small subset of the image pixels are used  but may fail if the chosen primitives cannot be reliably found in the images; furthermore they usually only yield very sparse depth maps. 
1. area based. in these approaches  the system attempts to correlate the grey levels of image patches in the views being considered  assuming that they present some similarity. the resulting depth map can then be interpolated. the underlying assumption appears to be a valid one for relatively textured areas; however if may prove wrong at occlusion boundaries and within featureless regions. 
alternatively the map can be computed by directly fitting a smooth surface that accounts for the disparities between the two images. this is a more principled approach since the problem can be phrased as an optimization one; however the smoothness assumptions that are required may not always be satisfied. 
　all these techniques have their strengths and weaknesses and it is difficult to assess their compared merits since few researchers work on similar data sets. however  one can get a feel for the relative performances of these systems from the study by guelch  guelch 1 . in this work  the author has assembled a standardized data set and sent it to 1 research institutes across the world. it appears that the correlation based system developed at sri by hannah  hannah 1  has produced the best results both in terms of precision and reliability. unfortunately this system only matches a very small proportion  typically less than 1%  of the image points. 
　in this paper we propose a correlation algorithm that reliably produces far denser maps with very few false matches and can therefore be effectively interpolated. in the next section we describe our hypothesis generation mechanism that attempts to match every point in the image and uses a consistency criterion to reject invalid matches. this criterion is designed so that when the cor-

relation fails  instead of yielding an incorrect answer  the algorithm returns no answer. as a result  the density of the computed disparity map is a very good measure of its reliability. the interpolation technique described in the section that follows combines the depth map produced by correlation and the grey level information present in the image itself to introduce depth discontinuities and fit a piecewise smooth surface. these algorithms have proven very effective on real data. their parallel implementation on a connection machine t m 1 relies only on local operations and on nearest neighbor communication; they could be ported to a dedicated architecture  thereby making fast and cheap systems possible. 
1 	correlation 
most correlation based algorithms attempt to find interest points on which to perform the correlation. while this approach is justified when only limited computing resources are available  with modern hardware architectures and massively parallel computers it becomes possible to perform the correlation over the whole image and retain only results that appear to be  valid.  the hard problem is then to provide an effective definition of what we call validity and we will propose one below. 
   in our approach  we compute correlation scores for every point in the image by taking a fixed window in the first image and a shifting window in the second. the second window is moved in the second image by integer increments along the epipolar line and an array of correlation scores is generated. in this work we use correlation of grey level values and take the correlation score to be s = max 1 - c  where 
		 1  
i1 and i1 being the left and right image intensities  x the average value of x over the correlation window and dx dy the displacement along the epipolar line.1 the measured disparity can then be taken to be the one that provides the highest value of s. in fact  to compute the disparity with subpixel accuracy  we fit a second degree curve to the correlation scores in the neighborhood of the maximum and compute the optimal disparity by interpolation. 
1 	v a l i d i t y o f t h e d i s p a r i t y m e a s u r e 
   as shown by nishihara  nishiharaet al. 1   the probability of a mismatch goes down as the size of the correlation window and the amount of texture increase. however  using large windows leads to a loss of accuracy and 
1
t m c inc. 
1
　　 we remove the mean to offset transformations of the images that may result from slightly different settings of the cameras. 

figure 1: consistent vs inconsistent matches: the two rows represent pixels along two epipolar lines of i1 and i1 and thearrows go from a point in one of the images towards the point in the other image that maximizes the correlation score. the match on the left is consistent because correlating from l  to i1 and from i1 to i  yields the same match unlike the matches on the right that are inconsistent. 
the possible loss of important image features. for smaller windows  the simplest definition of validity would call for a threshold on the correlation score; unfortunately such a threshold would be rather arbitrary and  in practice  hard to choose. another approach is to build a correlation surface by computing disparity scores for a point in the neighborhood of a prospective match and checking that the surface is peaked enough  anandan 1 . it is more robust but also involves a set of relatively arbitrary thresholds. here we propose a definition of a valid disparity measure in which the two images play a 
symmetric role and that allows us to reliably use small windows. we perform the correlation twice by reversing the roles of the two images and consider as valid only those matches for which we measure the same depth at corresponding points when matching from i1 into i1 and i1 into i1. as shown in figure 1  this can be defined as follows. 
given a point p  in i1  let p1 be the point of 
i1 located on the epipolar line corresponding to 
p  such that the windows centered on p  and p1 yield the optimal correlation measure. the match is valid if and only if p  is also the point that maximizes the score when correlating the window centered on p1 with windows that shift along the epipolar line of i1 corresponding to 
p1 
　for example  the validity test is likely to fail in presence of an occlusion. let us assume that a portion of a scene is visible in i1 but not i1. the pixels in l  corresponding to the occluded area in i1 will be matched  more or less at random  to points of i1 that correspond to different points of i1 and are likely to be matched with them. the matches for the occluded points will therefore be declared invalid and rejected. we illustrate this behaviour using the portion of the tree scene of figure 1 outlined in figure 1 a . different parts of the ground be-


 a  	 b  	 c  

figure 1: 	 a  an outdoor scene with two trees and a stump different parts of the ground are occluded by the trees. 
tween the two trees and between the trees and the stump are occluded in figures 1  b  and  c . in figures 1  a  and  b   we show the computed disparities for this image window after correlation with the images shown in figures 1 b  and 1 c  respectively. the points for which no valid match can be found appear in white and the areas where their density becomes very high correspond very closely to the occluded areas for both pairs of images. these results have been obtained using 1 correlation windows; these small windows are sufficient in this case because the scene is very textured and gives our validity test enough discriminating power to avoid errors. 
   we use the face shown in figure 1 to demonstrate another case in which the validity test rejects false matches. the epipolar lines are horizontal and in figure 1 d  we show the resulting disparity image  using 1 windows  in which the invalid matches appear in black. in figure 1 e  we show another disparity image computed after having shifted one of the images vertically by two pixels  thereby degrading the calibration and the correlation. note that the disparity map becomes much sparser but that no gross errors are introduced. in practice  we take advantage of this behaviour for poorly calibrated images: we compute several disparity maps by shifting one of the images up or down and retaining the same epip olar lines 1 thereby replacing the line by a band  and retain the highest scoring valid matches.1 
   in the two examples described above  we have shown that when the correlation between the two images of a stereo pair is degraded our algorithm tends  instead of making mistakes  to yield sparse maps. generally 
1
assumed not to be exactly vertical 
1
　　the precision of the computed distance is then obviously degraded  but remains qualitatively correct for large enough 
baselines. 
1 	vision 
. the same scene seen from the left  b  and the right  c  so that l 
speaking  correlation based algorithms rely on the fact that the same texture can be found at corresponding points in the two images of a stereo pair and are known to fail when: 
  the areas to be correlated have little texture. 
  the disparities vary rapidly within the correlation window. 
  there is an occlusion. 
if we consider the local image texture as a signal to be found in both images  we can model these problems as noise that corrupts the signal. in a companion report  fua 1l   we use synthetic data to formalize this argument and show that as the noise to signal ratio increases  or equivalently  as the problems mentioned above become more acute  the performance of our correlation algorithm degrades gracefully in the following sense: 
as the signal is being degraded  the density of matches decreases accordingly but the ratio of correct to false matches remains high until this proportion has dropped very low. 
in other words  a relatively dense disparity map is a guarantee that the matches are correct  at least up to the pre-
cision allowed by the resolution being used. in the report  fua 1l   we also show the effectiveness of a very simple heuristic: if we reject not only invalid matches but also isolated valid matches we can increase even more the ratio correct/incorrect matches without losing a large number of the correct answers. 
   other stereo systems include a validity criterion similar to ours but use it as only one among many others. in our case  because we correlate over the whole image and not only at interest or contour points  we do not need the other criteria and can rely on density alone. however  our validity test depends on the fact that it is 


figure 1:  a  the result of matching 1 a  and 1 b  for window of 1 a  delimited by the white rectangle   b  the result of matching 1 a  and 1 c  for the same windows   c  the merger of four disparity maps computed using the image of figure 1  a  as a reference frame  the two other images of 1 and two additional images. invalid matches appear in white and become almost dense in occluded areas of  a  and  b . the closest areas are darker; note that they are few false matches although the 

correlation windows used in this case are very small  1 . 
improbable to make the same mistake twice when correlating in both directions and can potentially be fooled by repetitive patterns  which is a problem we have not addressed yet. 
1 	hierarchical approach and additional images 
to increase the density of our potentially sparse disparity map  we use windows of a fixed size to perform the matching at several levels of resolution 1 which is almost equivalent to matching at one level of resolution with windows of different sizes as suggested by kanade  kanade et al. 1  but computationally more efficient. more precisely  as shown by burt  burt et al. 1   it amounts to performing the correlation using several frequency bands of the image signal. 
　we then merge the disparity maps by selecting  for every pixel  the highest level of resolution for which a valid disparity has been found. in figure 1  c  we show the merger of the disparity maps for two levels of resolution that is dense and exhibits more of the fine details of the face than the map of figure 1  e  computed using only the coarsest level of resolution. the reliability of our validity test allows us to deal very simply with several resolutions without having to introduce  as in  kanade et al. 1  for example  a correction factor accounting for the fact that correlation scores for large windows tend to be inferior to those for small windows. 
　the computation proceeds independently at all levels of resolution and this is a departure from traditional hierarchical implementations that make use of the results generated at low resolution to guide the search at 
1
computed by subsampling gaussian smoothed images. higher resolutions. while this is a good method to reduce computation time  it assumes that the results generated at low resolution are more reliable  if less precise  than those generated at high resolution; this is a questionable assumption especially in the presence of occlusions. for example in the case of the trees of figure 1  it could lead to a computed distance for the area between the trunks that would be approximately the same as that of the trunks themselves  which would be wrong. furthermore  it can be shown  fua 1l  that  in the absence of repetitive patterns  the output of our algorithm is not appreciably degraded by using the large disparity ranges that our approach requires. 
　as suggested by several researchers  more than two images can and should be used whenever practical. when dealing with three images or more  we take the first one to be our reference frame  compute disparity maps for all pairs formed by this image and one of the others and merge these maps in the same way as those computed at different levels of resolution. in this way we can generate a dense disparity map  such as the one of figure 1  c : the three images of figure 1 belong to a series of five taken by an horizontally moving camera. taking the image of 1 a  as our reference frame  we merge the four resulting disparity maps  each of them relatively sparse  to produce a dense map with few errors. 
　in this section we have presented an hypothesis generation mechanism that produces depth maps that are correct where they are dense and unreliable only where they become very sparse. typically these sparse measurements occur in featureless areas that are usually smooth and at occlusion boundaries where one expects to find an image intensity edge. to fit a surface  one must therefore 


figure 1:  a   b  left and right 1 images of a face   c  the disparity map obtained by merging the results computed at two levels of resolution   d  disparity map computed at the highest resolution   e  disparity map computed at the highest resolution after shifting the right up by one pixel   f  disparity map computed at the lower resolution. 

interpolate those measures in such a way as to propagate the depth information in the featureless areas and preserve depth discontinuities. in the next section  we describe the model and algorithm we use to perform the interpolation. 
1 	interpolation 
we model the world as made of smooth surfaces separated by depth discontinuities. we also assume that these depth discontinuities produce gradients in grey level intensities due to changes in orientation and surface material. we first describe a simple interpolation model that is well suited to images with sharp contrasts and then propose a refinement of that scheme for lower contrast scenes. 
1 	simple interpolation model 
ideally  if we could measure with absolute reliability the depth  w1  at a number of locations in the image  we could compute a depth image w by minimizing the fol-
1 	vision 
lowing criterion: 
 1  
where cx and cy are two real numbers that control the amount of smoothing. 
　as discussed in the previous section  when a valid dis parity can be found it is reliable and can be used  along with the camera models  to estimate ; we then take .s to be the normalized correlation score of equation 1. assuming that changes in reflectance can be found at depth discontinuities  we replace the and of equation 1  by terms that are inversely proportional to the image gradients in the x and y directions. in fact  we have observed that the absolute magnitudes of the gradients are not as relevant to our analysis as their local relative mag nitudes: boundaries can be adequately characterized as 

the locus of the strongest local gradients  independent of 	interpolated image: 
the actual value of these gradients. we therefore write: 

		 1  
where fnorm is the piecewise linear function defined by 
  1  
and x1 and x1 are two constants. in all our examples  x1 is the median value of x in the image and x1 its maximum value . the result is quite insensitive to the value chosen for x1 as long as it does not become so large as to force the algorithm to ignore all edges. what really matters is the monotoniaty of fnorm that allows the depth information to propagate faster in the directions of least image gradient and gives to the algorithm a behaviour somewhat similar to that of adaptative diffusion schemes  e.g  perona et al. 1  . 
　to compute w  the vector of all values of w  we discretize the quadratic criterion c of equation 1; we then solve  using a conjugate gradient method  szeliski 1; 
terzopoulos 1   the linear equation 
		 1 
in figure 1  we show the depth map computed by interpolating the disparity map of figure 1 c . note that the main features of the face  nose  eyebrows and mouth have been correctly recovered. 
　this simple interpolation technique is appropriate for the face of figure 1 that presents few low-contrast depth discontinuities but produces a somewhat blurry result for the tree scene of figure 1  as can be seen in figure 1 a  to improve upon this situation  we propose a slightly more elaborate interpolation scheme that takes depth discontinuities explicitly into account. 
1 	introducing depth discontinuities 
the  coefficients defined by equation 1 in-
troduce  soft  discontinuities: when the contrast is low  some smoothing occurs across the discontinuity. the depth image  however  is less smoothed than in the complete absence of an edge resulting in a strong w gradient. we take advantage of this property of our  adaptative  smoothing by defining the following iterative scheme: 
1. interpolate using the 	and 	defined above. 
1. iterate the following procedure: 
 a  recompute  as functions of both the intensity gradient and the depth gradient of the 
 1  
where ples. 
 b  interpolate again the raw disparity map using the new  coefficients 
the algorithm converges after a few iterations resulting in a much sharper depth map such as the one of figure 1 h . this algorithm can be regarded as a continuation method on the depth discontinuities. we start without knowing their location  use the grey level information to hypothesize them and then propagate the results. 
1 	conclusion 
in this work we have described a correlation based algorithm that combines two simple and parallel techniques to yield reliable depth maps in the presence of depth discontinuities  occlusions and featureless areas: 
  the correlation is performed twice over the two images by reversing their roles and only matches that are consistent in both directions are retained  thereby guaranteeing a very low error rate 
  the disparity map is then interpolated using a tech-nique that  takes advantage of the grey level information present in the image to preserves depth discontinuities and propagate the information across featureless areas. 
the depth maps that we compute are qualitatively correct and the density of acceptable matches provides us with an excellent estimate of their reliability. because of the great regularity and simplicity of the techniques described here 1 we hope to be able to build dedicated hardware that would implement them and could  for example  be used by a mobile robot in an outdoor environment. 
