 
　　　the author's state-space learning system has effectively optimized the coefficients of linear evaluation functions. the incremental approach uses s t a t i s t i c a l performance measures from completed solutions to bootstrap the heuristic  which estimates probability of task usefulness. these s t a t i s t i c s are clustered in feature space  forming a mediating knowledge structure  region set  between the direct performance measures and the generalized evaluation function. the regions are data-determined  insensitive to noise  and allow management of interacting features through natural piecewise l i n e a r i t y . early experiment with nonlinearity indicates s t a b i l i t y   f l e x i b i l i t y and improved task performance. 
	1. 	introduction 
　　　the evaluation function has frequently been used as a heuristic in what is called b e s t - f i r s t search  1 1. a standard technique is to combine several more elementary functions or features. as argued in   1     forming a heuristic function from a set of features is t h e o r e t i cally as powerful as any other design. the problem though is to merge features usefully. often the linear combination h = b.f is imposed  where b is the coeff i c i e n t vector for the feature vector f  though this is generally i n s u f f i c i e n t   1   1   . even with this r e s t r i c t i v e formulation  b is d i f f i c u l t to optimize. 
　　　the optimization should be governed by some performance measure  such as number of states generated to reach the goal  but often no solution whatever can be found within resource constraints. despite this impediment  some approaches have been very e f f e c t i v e   e.g.   1   . in  the present author described a successful new basis for learning. the system implemented was able not only to solve the fifteen puzzle  but also to optimize feature coefficients for linear evaluation functions  a unique r e s u l t . since the scheme has good conceptual and experimental rendell 
and information science of guelph 
nig 1wl 	canada 
support  work is underway to improve i t . one extension is to increase accuracy   1   ; another is to accommodate feature i n t e r actions  to allow more general evaluation. the l a t t e r uses a natural piecewise linear method outlined in   1     and developed  
implemented and tested here. 
	1. 	knowledge structure 
　　　like other recent approaches   a penetrance learning system  pls  uses completed searches of training problems. unlike them  it computes s t a t i s t i c s measuring solution density in feature space  fig. 1 . although it is data driven  pls is insensitive to noise since it is stochastic. the raw s t a t i s t i c s   which depend on the problem instance set p attempted and heuristic h guiding the search  are called elementary penetrances p r  h  p    where r is a  rectangular  feature space volume. from these data  a normalized true penetrance estimate p r  is computed. this value is the estimated probability of a state a being in a breadth f i r s t solution of a random problem instance  given that a maps into r. derived from repeated observations and incremental computations  the evolving evaluation function h is designed to predict true penetrance. 
　　　to house the true penetrance estimate p of a feature space volume r  a region r is defined to be the quintuple 
 r  c  p  e  b  . the second element c is the centroid  a representative of r. the f i n a l two elements relate to p: e is the error  an inverse measure of the r e l i a b i l i t y of p  and b is a coefficient vector  explained l a t e r . a set of these regions  the cumulative region set c  is both the control structure used by the problem solver and the knowledge structure improved by the learning element  fig. 1 . this set accumulatates information over several i t e r a t i o n s   as i t s regions are incrementally resolved into smaller units just adequate to express known r e l a t i o n ships. the result is an effective economy  a refinement of samuel's  signature tables which did not alter data categories automatically. 
　　　
1 l. rendell 
　　　
	1. 	piecewise linearity 
　　　this method takes advantage of the natural partitioning of the feature space into regions   c . f .   1   p.1    1  p.1   and allows increasing departure from l i n e a r i t y as the refinement improves in later iterations  as the number of regions increases . in this extension of pls1  the clusterer remains unchanged while the regresser is altered. 
　　　instead of a single penetrance-feature surface f i t t e d over the whole space  there are now as many of these hyperplanes as regions in the cumulative set c. each region r =  r  c   p  e  b r   of c is viewed as the principal one for i t s own regression; the coefficient vector br is computed using an r-centered weighting of every contributing region q e c. as mentioned in the previous section  the regression is already weighted according to penetrance error e. in this new  piecewise linear design pls1a  the former weight is multiplied by an additional factor related to the distance between r and q  so that q plays a greater role if it is near r. in the determination of this distance  the feature space is deformed to capture the r e l a t i v e importance of the various features.  details of this and related aspects are provided in the appendix.  
　　　since each regression is s t i l l linear  the process is quite stable.  in contrast  permitting feature interaction by using higher order models requires many more coefficients and this 'uses up1 the data.  at the same time the piecewise linear scheme pls1a is f l e x i b l e   allowing a continuously variable amount of nonlinearity in order to suit the current power of the entire learning system. this v a r i a b i l i t y is mechanized by introducting a system parameter called the localization power l   1 as an exponent for the distance measure  again  refer to the appendix . 
　　　to test the u t i l i t y of t h i s more sophisticated learning element  the solver was altered so that two new modes of evaluation can be selected. the f i r s t   discrete piecewise linear procedure simply predicts the true penetrance of a state a according to the local heuristic function of the region into which a maps. the more complex evaluation mode  smooth piecewise linear  uses a l l regions in the cumulative set in every evaluation  employing a distance weighting l i k e the one above. 
　　　prelimarary program runs have been made to discover characteristics of the scheme: i t s u t i l i t y   cost  and s t a b i l i t y . the cumulative region set used was a four dimensional one for the f i f t e e n puzzle l rendell 	1 
  1   . since these four features were o r i g i n a l l y used in the s t r i c t l y linear system  they had been deliberately selected for low i n t e r a c t i o n . hence this is a mild test of pls1a. in the solver  the cost increase of discrete piecewise linear evaluation over the s t r i c t l y linear pls1 mode is n e g l i g i b l e   but edge effects  discontinuities  v i t i a t e the scheme; performance is very poor. on the other hand  smooth piecewise l i n e a r i t y seems promising; i t s cost is also low. results are shown in fig. 1  where the extent of nonlinearity is varied by choice of the localization power l. in this curve the optimum is attributed to two c o n f l i c t i n g factors: as l is increased some advantage occurs because the relationships are inherently nonlinear  and nearby regions now play a j u s t i f i a b l y bigger part in the determination of each local heuristic. however  distant regions  which formerly had a s t a b i l i z i n g r o l e   now have a diminished influence  so there is a general loss of support. the inaccuracy and graininess of individual regions gradually overpower the benefit of l o c a l i z a t i o n . 
　　　an important property of pls1a is i t s s t a b i l i t y . when pls1 was used with higher order models instead of piecewise l i n e a r i t y   performance was degraded. also in contrast  pls1a allows easy observation of the r e l a t i v e importance of features in any area of the space  since simple feature weighting is used. furthermore  relationships exemplified by fig. 1 are useful. the magnitude of the optimal localization power is a measure of region accuracy  and i n d i r e c t l y   of the u t i l i t y of the entire learning system. 

fig. 1* variation of performance with degree of nonllnearity. shown is average number of nodes developed d before solution in a random sample of 1 puzzles  vs. localization power l. 1% oonfldenoe intervals are indicated. 
　　　
1 	l rendell 
	1. 	conclusions 
　　　the piecewise linear scheme pls1a is natural  f l e x i b l e and stable. i t s low cost and performance improvement warrant further investigation. the next step is to attempt stronger feature i n t e r a c t i o n   with support from a scheme designed to improve the accuracy of true penetrance estimates   1   . the freedom to vary the l o c a l i z a t i o n power l w i l l f a c i l i t a t e experimentation in determining the general u t i l i t y of pls1a as a heuristic learning system. 
