 
this paper presents part of an on-going project to integrate perception  attention  drives  emotions  behavior arbitration  and expressive acts for a robot designed to interact socially with humans. we present the design of a visual attention system based on a model of human visual search behavior from wolfe  1 . the attention system integrates perceptions  motion detection  color saliency  and face popouts  with habituation effects and influences from the robot's motivational and behavioral state to create a context-dependent attention activation map. this activation map is used to direct eye movements and to satiate the drives of the motivational system. 
1 	introduction 
socially intelligent robots provide both a natural humanmachine interface and a mechanism for bootstrapping more complex behavior. however  social skills often require complex perceptual  motor  and cognitive abilities  brooks et al.  1 . our research has focused on a developmental approach to building socially intelligent robots that utilize natural human social cues to interact with and learn from human caretakers. 
　this paper discusses the construction of one necessary component of social intelligence: an attention system. to provide a basis for more complex social behaviors  an attention system must direct limited computational resources and select among potential behaviors by combining perceptions from a variety of modalities with the existing motivational and behavioral state of the robot. we present a robotic implementation of an attention system based upon models of human attention and visual search. we further outline the ways in which this model interacts with existing perceptual  motor  motivational  and behavioral systems. 
　our implementation is based upon wolfe's model of human visual attention and visual search  wolfe  1 . this model integrates evidence from treisman   
julesz   and others to construct a flexible model 
1 	robotics and perception of human visual search behavior. in wolfe's model  visual stimuli are filtered by broadly-tuned  categorical  channels  such as color and orientation  to produce feature maps with activation based upon both local regions  bottom-up  and task demands  top-down . the feature maps are combined by a weighted sum to produce an activation map. limited cognitive and motor resources are distributed in order of decreasing activation. this model has been tested in simulation  and yields results that are similar to those observed in human subjects  wolfe  1 . in this paper we do not attempt to match human performance  a task that is difficult with current component technology   but rather require only that the robotic system perform enough like a human that it is capable of maintaining a normal social interaction. our implementation is similar to other models based in part on wolfe's work  itti et al.  1; hashimoto  1; driscoll et al.  1   but additionally operates in conjunction with motivational and behavioral models  with moving cameras  and it differs in dealing with habituation issues. 
1 	robot hardware 
our robotic platform consists of a stereo active vision system augmented with facial features for emotive expression. the robot  called kismet and shown in figure 1  is able to show expressions  analogous to anger  fatigue  fear  disgust  excitement  happiness  interest  sadness  and surprise  which are easily interpreted by an untrained human observer. the platform has four degrees of freedom in the vision system; each eye has an independent vertical axis of rotation  pan   the eyes share a joint horizontal axis of rotation  tilt   and the entire head has a single vertical axis of rotation  pan  at the neck. kismet also has fifteen degrees of freedom in facial features  including eyebrows  ears  eyelids  lips  and a mouth. each eyeball has an embedded color ccd camera with a 1 mm focal length. 
　the active vision platform is attached to a parallel network of eight 1mhz digital signal processors  texas instruments tms1 . the dsp network serves as the sensory processing engine and implements the bulk of the robot's perception and attention systems. a pair of motorola 1-based microcontrollers are also connected 


figure 1: kismet  a robot designed to interact socially with humans. kismet has an active vision system and can display a variety of facial expressions. 
to the robot. one controller implements the motor system for driving the robot's facial motors. the other controller implements the motivational system  emotions and drives  and the behavior system. the microcontrollers communicate with the dsp network through a dual-ported r a m . 
1 	perceptual systems 
our current perceptual systems focus on the preattentive  massively parallel stage of human vision that processes information about basic visual features  color  motion  various depth cues  etc. . the implementation described here focuses on three such pre-attentive processes: color  motion  and face pop-outs. in terms of the model from wolfe   our implementation contains the bottom-up feature maps  which represent the inherent saliency of a specific image property for each point in the visual scene  and incorporates top-down influences from motivational and behavioral sources. 
　the video signal from each of kismet's cameras is digitized by one of the dsp nodes with specialized frame grabbing hardware. the image is then subsampled and averaged to an appropriate size. for these initial tests  we have used an image size of which allows us to complete all of the processing in near real-time. to minimize latency  each feature map is computed by a separate dsp processor  each of which also has additional computational task load . all of the feature detectors discussed here can operate at multiple scales. 
1 	color saliency feature maps 
one of the most basic and widely recognized visual feature is color. our models of color saliency are drawn from the complementary work on visual search and attention from itti  koch  and niebur . the incoming video stream contains three 1-bit color channels and  which are transformed into four color-opponency channels each input color channel is first normalized by the luminance   a weighted average of the three input color channels : 

these normalized color channels are then used to produce four opponent-color channels: 
 1  
 1  
 1  
 1  
the four opponent-color channels are clamped to 1-bit values by thresholding. while some research seems to indicate that each color channel should be considered individually  nothdurft  1   we choose to maintain all of the color information in a single feature map to simplify the processing requirements  as does wolfe  for more theoretical reasons . the maximum of the four opponent-color values is computed and then smoothed with a uniform 1 x 1 field to produce the output color saliency feature map. this smoothing serves both to eliminate pixel-level noise and to provide a neighborhood of influence to the output map  as proposed by wolfe . a single dsp node computes these computations and forwards the resulting feature map both to the attention process and a vga display processor at a rate of 1 hz. the processor produces a pseudo-color image by scaling the luminance of the original image by the output saliency while retaining the same relative chrominance  as shown in figure 1 . 
1 	motion saliency feature maps 
in parallel with the color saliency computations  a second processor receives input images from the frame grabber and computes temporal differences to detect motion. the incoming image is converted to grayscale and placed into a ring of frame buffers. a raw motion map is computed by passing the absolute difference between consecutive images through a threshold function 
 1  
this raw motion map is then smoothed with a uniform 1 x 1 field. while using a 1 x 1 field would have maintained consistency with both wolfe's model and the color saliency feature map  using a slightly larger field size allows us to use the output of the motion saliency map as a pre-filter to the face detection routine  which has optimized performance in prior tests by a factor of 1  scassellati  1 . the motion saliency feature map is computed at 1 hz by a single dsp processor node and forwarded both to the attention process and the vga display. 
1 	face pop-out feature maps 
while form and size are part of wolfe's original model  we have extended the concept to include other known 
	breazeal and scassellati 	1 


figure 1: overview of the attention system. a variety of visual feature detectors  color  motion  and face detectors  combine with a habituation function to produce an attention activation map. the attention process influences eye control and the robot's internal motivational and behavioral state  which in turn influence the weighted combination of the feature maps. displayed images were captured during a behavioral trial session. 

pop-out features that have social relevance  such as faces. our face detection techniques are designed to identify locations that are likely to contain a face  not to verify with certainty that a face is present in the image. the face detector is based on the ratio-template technique developed by sinha  1g   and has been previously reported  scassellati  1 . the ratio template algorithm was designed to detect frontal views of faces under varying lighting conditions  and is an extension of classical template approaches  sinha  1 . ratio templates also offer multiple levels of biological plausibility; templates can be either hand-coded or learned adaptively from qualitative image invariants  sinha  1 . 
　a ratio template is composed of regions and relations  as shown to the left of the face detector in figure 1. for each target location in the grayscale peripheral image  a template comparison is performed using a special set of comparison rules. the set of regions is convolved with a 1 x 1 image patch around a pixel location to give the average grayscale value for that region. relations are comparisons between region values  for example  between the  left forehead  region and the  left temple  region. the relation is satisfied if the ratio of the first region to the second region exceeds a constant value  in our case  1 . the number of satisfied relations serves as the match score for a particular location; the more relations that are satisfied the more likely that a face is located there. in figure 1  each arrow indicates a relation  with the head of the arrow denoting the second 
1 	robotics and perception 
region  the denominator of the ratio . 
　the ratio template algorithm has been shown to be reasonably invariant to changes in illumination and slight rotational changes  scassellati  1 . the ratio template algorithm processes video streams in real time using optimization and pre-filtering techniques  and the system has been tested on a variety of lighting conditions and subjects. the algorithm can operate on each level of an image pyramid in order to detect faces at multiple scales. in the current implementation  due to limited processing capability  we elected to process only a single scale for faces. applied to a 1 x 1 image from kismet's cameras  the 1 x 1 ratio template finds faces in a range of approximately 1 feet from the robot. this range was suitable for our current investigations of faceto-face social interactions  and could easily be expanded with additional processors. the implemented face detector operates at approximately 1 hz. 
1 	behaviors and motivations 
in previous work  breazeal and scassellati  presented how the design of kismet's motivation and behavior systems  modeled after theories oflorenz   enable it to socially interact with a human while regulating the intensity of the interaction via expressive displays. for the purposes of this paper  we present only those aspects of these systems which bias the robot's attention  see figure 1 . 
perceptual stimuli are classified into social stimuli  i.e. 


figure 1: schematic of motivations and behaviors relevant to attention. see text for details. 
people  which move and have faces  which satisfy a drive to be social and non-social stimuli  i.e. toys  which move and are colorful  which satisfy a drive to be stimulated by other things in the environment. 
　for each drive  there is a desired operation point  and an acceptable bounds of operation around that point  the homeoslatic regime . as long as a drive is within the homeostatic regime  that corresponding need is being adequately met. unattended  drives drift toward an under-stimulated regime. excessive stimulation  too many stimuli or stimuli moving too quickly  push a drive toward an over-stimulated regime. 
　the robot's drives influence behavior selection by preferentially passing activation to select behaviors. by doing so  the robot is more likely to activate behaviors that serve to restore its drives to their homeostatic regimes. the top level  level 1  of the behavior system consists of a single cross-exclusion group  ceg  containing two behaviors: satiate social and satiate stimulation. each behavior is viewed as a self-interested  goal-directed process. within a ceg  behaviors compete for activation in a winner-take-all scheme based upon perceptual factors  motivational factors  and its own behavioral persistence. competition between behaviors at the top level represents selection at the task level. by organizing the top level behaviors in.this fashion  the robot can only act to restore one drive at a time. this is reasonable since the satiating stimuli for each drive are mutually exclusive and require different behaviors. specifically  whenever the satiate social behavior wins  the robot's task is to do what it must to restore the social drive  and when the satiate stimulation behavior wins  the robot's task is to do what it must to restore the stimulation drive. 
　each behavior node of the top level ceg has a child ceg  level 1  associated with it. once a level 1 behavior wins the competition  it activates its child ceg at level 
1. subsequently  the behaviors within the active level 1 ceg compete for activation. competition between behaviors within the active level 1 ceg represents competition at the strategy level. each behavior has its own distinct conditions for becoming relevant and winning the competition. for instance  the avoid person behavior is the most relevant when the robot's social drive is in the overwhelmed regime and a person is stimulating the robot too vigorously. the goal of this behavior is to reduce the intensity of stimulation. if successful  the social drive will be restored to the homeostatic regime. similarly  the goal of the seek person behavior is to acquire a social stimulus of reasonable intensity. if successful  this will serve to restore the social drive from the under-stimulated regime. the engage person behavior is active by default  i.e. the social drive is already in the homeostatic regime and the robot is receiving a good quality stimulus . 
1 	attention system 
the attention system must combine the various effects of the perceptual input with the existing motivational and behavioral state of the robot both to direct limited computational resources and to select among potential behaviors. figure 1 shows an overview of the attention system. 
1 	combining perceptual inputs 
each of the feature maps contains an 1-bit value for each pixel location which represents the relative presence of that visual scene feature at that pixel. the attention process combines each of these feature maps using a weighted sum to produce an attention activation map  using the terminology of wolfe  . the gains for each feature map default to values of 1 for color  1 for motion  and 1 for face detection. the attention activation map is thresholded to remove noise values  and normalized by the sum of the gains. connected object regions are extracted using a grow-and-merge procedure with 1-connectivity. to further combine related regions  any regions whose bounding boxes have a significant overlap are also merged. 
　statistics on each region are collected  including the centroid  bounding box  area  average attention activation score  and average score for each of the feature maps in that region. the tagged regions that have an area in excess of 1 pixels are sorted based upon their average attention activation score. the attention process provides the top three regions to both the eye motor control system and the behavior and motivational systems. the eye motor control system uses the centroid of the most salient regions to determine where to look next. the top-down processes use the attention activation score and the individual feature map scores of the most salient region to determine which of the drives and behaviors will become activated. 
	breazeal and scassellati 	1 

1 	attention drives eye movement 
the eye motor control process acts on the data from the attention process to center the eyes on an object within the visual field. our current implementation uses a static linear mapping between image position and eye position  which has been sufficient for our initial investigations. we are currently in the process of converting to a self-calibrated system that learns the sensori-motor mapping for foveation similar to that described by scassellati . 
　each time that the eyes move  the eye motor process sends two signals. the first signal inhibits the motion detection system for approximately 1 msec  which prevents self-motion from appearing in the motion feature map. the second signal resets the habituation state  which is described below. 

figure 1: changes of the face  motion  and color gains from top-down motivational and behavioral influences 
 top . when the social drive is activated by face stimuli  middle   the face gain is influenced by the seek people and avoid people behaviors. when the stimulation drive is activated by color stimuli  bottom   the color gain is influenced by the seek toys and avoid toys behaviors. all plots show the same 1 minute period. 
1 	habituation 
for our robot  the current object under consideration is always the object that is in the center of the visual field.1 the habituation function can be viewed as a feature map that initially maintains eye fixation by increasing the saliency of the center of the field of view and slowly decays the saliency values of central objects until a salient off-center object causes the eyes to move. the habituation function is a gaussian field centered in the field of view with peak amplitude of 1  to remain consistent with the other 1-bit values  and = 1 pixels. it is combined linearly with the other feature maps using the weight 
 1  
where is the time since the last habituation reset  is a time constant  and w is the maximum habituation gain. whenever the eyes move  the habituation function is reset  forcing  to w and amplifying the saliency of central objects until a time  when  = 1 and there is no influence from the habituation map. as time progresses   decays to a minimum value of which suppresses the saliency of central objects. in the current implementation  we use a value of w = 1 and a time constant  = 1 seconds. 
　the entire attention process  with habituation  operates at 1 hz on a single dsp processor node. the speed varies with the number of attention activation pixels that pass threshold for region growing. while this code could be optimized further  rates above 1 hz are not necessary for our current purposes. 
1 motivations and behaviors influence feature map gains 
kismet's drives and behaviors bias the attentional gains based on the current internal context to preferentially attend to behavior ally relevant stimuli. behaviors that satiate the stimulation drive influence the color saliency gain because color is characteristic of toys. similarly  the face saliency gain is adjusted when the robot is tending to its social drive. active level 1 behaviors influence attentional gains in proportion to the intensity of the associated drive. 
　as shown in figure 1  the face gain is enhanced when the seek people behavior is active and is suppressed when the avoid people behavior is active. similarly  the color gain is enhanced when the seek toys behavior is active  and suppressed when the avoid toys behavior is active. whenever the engage people or engage toys behaviors are active  the face and color gains are restored to their default values  respectively. weight adjustments are constrained such that the total sum of the weights remains constant at all times. figure 1 illustrates how the face  motion  and color gains are adjusted as a function of drive intensity  the active level 1 behavior  and the nature and quality of the perceptual stimulus. 

1 	robotics and perception 

which have a second camera that captures a high resolution foveal image. 


figure 1: preferential looking based on habituation and top-down influences. when presented with two salient stimuli  a face and a brightly colored toy   the robot prefers to look at the stimulus that has behavioral relevance. habituation causes the robot to also spend time looking at the non-preferred stimulus. 

1 	results and evaluation 
top-down gain adjustments combine with bottom-up habituation effects to bias the robot's gaze preference  see figure 1 . when the seek people behavior is active  the face gain is enhanced and the robot prefers to look at a face over a colorful toy. the robot eventually habituates to the face stimulus and switches gaze briefly to the toy stimulus. once the robot has moved its gaze away from the face stimulus  the habituation is reset and the robot rapidly re-acquires the face. in one set of behavioral trials when seek people was active  the robot spent 1% of the time looking at the face. a similar affect can be seen when the seek toy behavior is active - the robot prefers to look at a toy over a face 1% of the time. 
　the opposite effect is apparent when the avoid people behavior is active. in this case  the face gain is suppressed so that faces become less salient and are more rapidly affected by habituation. because the toy is relatively more salient than the face  it takes longer for the robot to habituate. overall  the robot looks at faces only 1% of the time when in this behavioral context. a similar scenario holds when the robot's avoid toy behavior is active - the robot looks at toys only 1% of the time. 
1 	future work 
in this paper we have demonstrated an attentional system that combines bottom-up perceptions and habituation effects with top-down behavioral and motivational influences. this results in a system that directs eye gaze based on current task demands. in the future  we intend to construct a richer set of perceptual inputs  depth  orientation  and texture  and motor responses  smooth pursuit tracking  vergence  and vestibulo-ocular reflex . we are also currently combining this system with expressive behaviors to facilitate social interaction with a human. 
