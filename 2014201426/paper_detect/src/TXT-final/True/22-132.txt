 
fisher  1a b  introduced a performance task for conceptual clustering: flexible prediction of arbitrary attribute values  not simply the prediction of a single 'class' attribute. this paper extends earlier analysis by considering the effects of noise and other environmental factors. the degradation in flexible prediction accuracy that results from noise is mitigated by 'preferred' prediction points for individual attributes. methods that identify these prediction points are inspired by pruning in learning from examples. we extend these noise-
tolerant techniques to untutored learning. in addition  prediction point preferences shed light on relationships between conceptual clustering  case-based  and default reasoning. 
1 introduction 
machine concept learning has traditionally been concerned with learning from examples  e.g.  quinlan  1   which assumes that observations are identified as members of a priori known classes  e.g.  diseases ; the learner must characterize the observations of each class. in contrast  conceptual clustering methods  michalski 
& stepp  1; fisher  1b; cheeseman  kelly  self  stutz  taylor  k freeman  1  discover  as well as characterize meaningful classes. 
   in principle  learning should improve an organism's performance at some task s . in learning from examples a performance task is apparent: improve prediction of class membership  e.g.  diagnose the illness of a patient . on the otherhand  conceptual clustering is not traditionally associated with a performance task  michalski k stepp  1; cheeseman  kelly  self  stutz  taylor  k freeman  1 . fisher  1a b  proposes that a performance task for conceptual clustering is flexible prediction: the prediction of multiple attributes  not simply a single class 'attribute'. for example  a learning 
　　*this work was supported by a grant from the vanderbilt university research council. from examples system may attempt to optimize cor-
rect prediction of a congressperson's political party  i.e.  class  from information about their congressional voting record  i.e.  'attributes' like their vote on farm aid or the mx missile . in contrast  flexible prediction is concerned with simultaneously improving prediction along all dimensions  e.g.  party  farm aid  and mx missile . while the performance task of learning from examples has implications for expert system construction  quinlan  1; bareiss k porter  1   flexible prediction permeates common-sense reasoning. despite the importance of flexible prediction  few systems have been concerned with it  kolodner  1; lebowitz  1   much less systematically characterized with respect to it. 
　this paper explores the impact of noise on flexible prediction accuracy using fisher's  1b  cobweb system. weaknesses in the face of noise motivate two extensions to cobweb that are inspired by research on pruning in learning from examples. our extensions are applicable to other untutored learning systems and shed light on relationships between conceptual clustering  case-based  and default reasoning. 
1 c o b w e b and flexible prediction 
cobweb incrementally builds classification trees from objects that are described by nominal attribute - value pairs. for example  consider the tree of figure 1 over voting records of u.s. senators  fisher  1 . stored at each node are the value distributions of each attribute over the objects classified under the node. consider node n l   which classifies many senators voting 'yes' on budget cuts  p budget-cuts = yes  n   = 1 . more generally  probabilities at a node are conditioned on membership in the node's parent  so that the probabilities of nl are  trivially  conditioned on classification at the root  while probabilities at n1 assume classification at nl. each node is a. probabilistic concept  smith k medin  1 ; the classification tree is a probabilistic concept tree. 
each tree level contains sibling classes that collec-
	fisher 	1 

distributions are updated permanently. this process is recursively applied to the subtrees rooted at the selected child until a leaf is reached. a leaf is a singleton class that represents a previously observed object. while objects are predominantly incorporated with respect to existing classes  operators also exist for new node  class  creation  node combination  merging   and node division  splitting . a more complete description of cobweb can be found in fisher  1b . 
   object incorporation is easily adapted to allow object classification and flexible prediction: category utility guides an object along a path of nodes to a 'best1 matching leaf. if any value s  are missing from the new observation  they may be predicted from the known values of the leaf. while cobweb trees are reminiscent of decision trees  probabilistic concepts are polythetic in that multiple attributes guide classification. if an object has missing attribute values then category utility acts as a partial-matching function with summation limited to probabilities of known attributes. 
　in our experiments a classification tree is constructed from a training set of objects. each object of a distinct testing set is repeatedly classified with respect to the tree; in each repetition a different attribute is removed from the object description and must be predicted. average prediction accuracy is computed over the test set for each attribute. in addition  prediction accuracy is tested for different size training sets. the result is a 'learning curve' for each attribute. figure 1 shows the learning curve for three of the 1 attributes in a domain of soybean case histories  stepp  1 . as the curves illustrate  learning difficulty may vary considerably across attributes; some attributes  e.g.  condition  are quickly and effectively learned  i.e.  simply 
guessing the most frequent condition value would yield about 1% accuracy   some are invariant to training  e.g.  occurrence-of-hail   but most are somewhere in between  e.g.  damage-severity . cobweb learns to predict all attributes  with variable success  using a single probabilistic concept tree. in contrast  id1 and other learning from examples systems would have to be applied separately for each attribute. fisher  1a  compares the performance of one cobweb tree with multiple  special-purpose id1 trees. 
     this paper investigates the impact of two environmental factors on flexible prediction. foremost among these is noise: the incorrect reporting of an attribute's value. noise alters attribute correlations that all inductive systems require for effective learning. 	for example  if we randomly replace attribute values in the soybean domain with a probability of 1%  i.e.  artificially introduce 1% noise  then prediction of severity-of-damage degrades to roughly 1%. a second influence on prediction accuracy is the extent of training  which also effects perceived statistical relationships between attributes. our investigation motivates two strategies for noise-tolerant concep1 	machine learning 

tual clustering. 
1 chi-square preferences 
quinlan  1  has extensively investigated the detrimental impact of noise on prediction in 1. original versions of id1 decomposed the training set to the point where all observations classified under a decision tree node were members of the same class. however  quinlan  1  demonstrates that this strategy tends to 'overfit/ the data in noisy domains: classification to a leaf may be guided by spurious  unjustified rules that do not benefit and actually detract from prediction accuracy. several authors  quinlan  1  in press; breiman  friedman  olshen  & stone  1  have explored methods for pruning unjustified rules so as to mitigate the effects of noise. a well-known technique is chi-square pruning  quinlan  
1 . a decision tree node is decomposed only if it will lead to class distributions  at the new children  that are significantly different than the class distribution at the node to be divided. if the distributions do not differ significantly then the subtree is pruned; deeper classification will not benefit prediction. 
　the benefits of pruning in learning from examples are well-documented in noisy domains and motivate an exploration of the possible benefits to flexible prediction. cobweb was modified to use a chi-square test of statistical significance for individual attributes. in particular  the distribution of an attribute's values at each node  n  are compared to the distribution of the node's children. if the distributions do not differ significantly by a user-specified confidence threshold  e.g.  1%  then n is taken as the prediction point for the attribute. note that in flexible prediction we cannot simply prune a subtree based on the significance test for any one attribute  since this may not be an appropriate pruning point for other attributes. rather  a strategy of less finality is to identify preference points based on the chi-square test that are maintained for each attribute. attribute prediction requires that object classification proceed to an appropriate preference point  but no deeper. the most common attribute value at the preference point is the predicted value. 
　experiments with the chi-square heuristic were run in three natural domains: the soybean disease case histories  a domain of poisonous and edible mushroom descriptions over 1 attributes  and voting records of 1 u.s. representatives defined over 1 votes each. prediction accuracy over a separate test set was checked at regular training intervals  for varying noise levels  and for 1 different chi-square confidence thresholds. thresholds included 1% and 1%. in the of case 1% confidence any distribution difference is significant and using this threshold results in classification to a leaf  which is identical to previous cobweb implementations. in contrast  1% confidence can never be achieved and disallows classification beyond the root; the prediction is the most common value over the entire training set. 
　figure 1 plots the 'averaged' attribute learning curves of the mushroom domain with no noise  but with different confidence thresholds. low confidence appears to be the best strategy early in training since predictive patterns have been exposed  but have not reached statistical significance to the degree required by higher confidences. chi-square preferences at high thresholds converge on equivalent accuracy  i.e.  no significant differences in accuracy levels  later in learning  also see fisher & schlimmer  1 . 
　the graph of figure 1 examines the effect of noise. this graph shows averaged attribute accuracy levels after significant training at noise levels of 1%  1%  and 1% . in general  the optimal threshold tends to increase with noise. over all attributes there is a significant positive correlation between noise level and optimal confidence threshold  pearson coefficient = 1  sample = 1 x 1 = 1 . as noise increases  the deeper the classification  i.e.  beginning at 1% confidence  the greater the overfitting. in noisy domains  higher chi-square confidences significantly increase accuracy. 
　a general trend in the data is that optimal confidence thresholds increase with noise and training  asymptoting at very high confidence thresholds  e.g.  1% . early 
	fisher 	1 

in training and under noiseless conditions classification to a leaf is the optimal strategy. no chi-square confidence threshold will maximize performance in all  or most  learning scenarios. thus  unless we can make a priori assumptions about the amount of noise and available training data  inconsistent performance will be a weakness of any constanti-threshold method. section 1 introduces a simple  but effective means of preference identification that makes no assumptions about noise  training  or their interaction. 
1 past-performance preferences 
a straight-for ward heuristic is that prediction of a missing attribute should occur at a node that historically has facilitated the greatest number of correct predictions. as a new training object is recursively classified  each of its attribute values is compared against the corresponding attribute values of the node; if the object's value equals that of the node's most common value  then the attribute's value would have been correctly predicted at the node. for each attribute and node  a count is maintained of the number of times the attribute was correctly predicted at the node  i.e.  correct-at-node counts  during training. a count is also kept of the number of times that the attribute was correctly predicted at one of the node's descendents  i.e.  correct-at-descendent . this latter count is updated as the recursive classification procedure unwinds; the correct prediction at a node is remembered and used to update the counts of its ancestors. when an object is added as a leaf we assume that it correctly predicts its own attribute values and there are no descendants  that correctly predict them . by convention  correct-at-node counts are initialized to 1 and correct-at-descendent counts are initialized to 1. 
　to predict the value of a missing attribute  classification proceeds until a node is reached that has historically outperformed its descendents in terms of predicting the missing attribute. at this point  the most common attribute value is forwarded as the correct answer. this method is similar in intent to breiman. friedman  ol-
1 	machine learning 
shen  & stone's  1  cost-complexity pruning strategy and quinlan's  in press  reduced error pruning. in these latter methods  a decision tree is fully constructed with a training set. the tree is then used to classify a separate test set. as each test item is classified a determination is made as to whether it would be correctly classified at each node  by the most common class at the node  on the path to a leaf. the tree is pruned at those nodes that maximized prediction over the test set. a separate test set is required because the original decision tree was engineered to fit the training set; cross-validation is necessary. in contrast  past performance is applicable to flexible prediction. a separate test set is not required  because the tradeoffs required to simultaneously improve prediction along many attributes introduces tradeoffs similar to those of training/cross-validation in id1. 
   experiments identical to those with chi-square were run. figures 1 and 1 indicate that past-performance preferences roughly match the optimal chi-square thresholds averaged over various training and noise levels. importantly  the optimal chi-square threshold differs between training and noise levels. over each training/noise-level combination in the three  mushroom  soybean  congressional  domains  i.e.  a total of 1 situations   past performance's mean accuracy is greater in significantly  x 1   a = 1  more situations than any single chi-square confidence level  i.e.  ranging from 1 when compared to confidence level 1 to 1 for confidence level 1 . these averaged results hide fluctuations among attributes  but conveniently and accurately reflect past performance's advantage. 
　the simplicity and effectiveness of past-performance preferences underscores an important principle: overt performance is the best  and simplest  'model' of the complex interactions between noise levels  training  and other factors. past performance makes no a priori assumptions about individual attribute characteristics  noise levels  or extent of training. rather  attribute preferences are determined by the accumulated correct prediction counts for each node and its descendents. 

1 related and future research 
our studies of flexible prediction suggest further research in conceptual clustering and related areas. 
1 default reasoning 
default values are often used to support uncertain reasoning  but brachman  1  points out that the automated reasoning literature makes no mention of prescriptive means for assigning default values. not coincidentally  there has been little  if any  work on default value maintenance during learning. our research provides a partial prescription for default identification. 
   kolodner's  1  cyrus system offers a probabilistic interpretation of default values: normative values are true of a user-specified percentage  e.g.  1%  of class members. normative  default  values at a toplevel node  e.g.  'conservative' congressmen vote 'yes' on budget cuts  can be shown otherwise by classification to deeper levels  e.g.  'southern democrats' vote 'no' on budget cuts . unfortunately  normative values require a user-supplied parameter and only delimit when a value is likely to be true; they fail to capture the more subtle notion of when it is best to predict that a value is true. past-performance and chi-square preferences address both issues by demarcating locations where the most common value should be predicted. nonetheless  important issues remain unaddressed. 
　attribute preferences are absolute - based on the presumption that a certain amount  most  of 'evidence' is known. an important goal of future research is a process of default selection that is sensitive to the amount of available evidence  i.e.  observed features . one idea is to cease classification when the difference in category utility scores between the best node and alternatives becomes sufficiently small: evidence is not sufficient to distinguish classification paths. this value would be the default value unless  and until  further evidence could be obtained. this classification procedure is more in line with traditional efforts in automated and default reasoning. future work will seek to flesh out the relationship between incremental concept formation and prescriptive mechanisms for default identification and exploitation. 
1 case-based reasoning 
case-based reasoning has emerged as a subfield of automated reasoning and learning. the subfield is distinguished by its reliance on cases or object-level descriptions as a source of problem-solving information. the emergence of this subfield has the unfortunate effect of segregating research efforts that share fundamental specification and design principles  but differ  perhaps  only in implementation. initial versions of cobweb generated predictions from best matching leaves  previously observed objects . thus  cobweb is an efficient implementation of case-based reasoning  since generalized concepts help identify appropriate cases in logarithmic time versus linear time in the number of cases. 
　in addition to efficiency concerns  this paper illustrates the utility of case-based and abstraction-based inferencing. reasoning at the case-level is most productive when few training observations are available and noise is not present  ashley & rissland  1   but this strategy overfits the data as training and noise increase. past performance has the emergent effect of using cases very early in training - a case  observation  is initially added to a concept tree and is viewed as correctly predicting each of its attribute values. when very few observations have been seen these individual 'successes' will tend to out way the formative classes above. gradually  solidifying attribute correlations at higher-level nodes become more reliable classifiers. past performance's emergent strategy can be viewed as a probabilistic and conservative specific to general search for the optimal prediction level of each attribute. 
　an alternative to abstract ion-based reasoning is to retain carefully-selected cases only. presumably  selective retention overcomes both problems of efficiency and accuracy that might otherwise hamper a nonselective case-based reasoner. selective retention is employed by bareiss k porter  1  and aha k kibler  1  in their respective case-based learning from examples systems. nonetheless  appropriate retention is difficult without the global guidance that abstracted knowledge can provide: aha k kibler report difficulties in dealing with irrelevant attributes and bareiss k porter use implicit sources of abstracted knowledge. finally  while strict adherence to a case-based strategy is feasible in a learning from examples context  it is difficult to imagine an efficient implementation for flexible prediction  in which many prediction dimensions must be simultaneously coordinated. 
	fisher 	1 
1 concluding remarks 
experiments with cobweb uncovered data overfitting that was mitigated by adapting pruning techniques for flexible prediction. however  the use of a constantthreshold method like chi-square often leads to nonoptimal performance; we cannot predict a 'priori the amount of noise  training  or their interaction. 
   limitations of chi-square preferences appear to be reduced by past performance: classification ceases at a point that has historically outperformed its descendents. thus  we do not model  or impose a model on  noise  training  and other statistical interdependences beyond that which cobweb is constructing through clustering. rather  whatever these interdependencies  their effect is evident through prediction performance; nothing beyond past performance is used to guide classification. this principle is also implicit in cobweb's use of category utility  a measure of the expectation of correct prediction afforded by classes. class quality is tied directly to the task that will benefit from clustering. the explicit consideration of a performance task that improves with learning distinguishes cobweb from almost all other clustering work and frees the system from user-supplied parameters or distributional assumptions of any kind. 
acknowledgements 
i thank jungsoon yoo  hua yang  ray bareiss  and 
gautam biswas for informative discussions. comments on an earlier draft by dennis kibler and john gennari improved discussion and style considerably. reviewers helped improve the clarity and correctness of the paper. 
