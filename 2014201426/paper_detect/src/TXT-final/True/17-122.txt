 
   this paper presents a scheme for learning complex descriptions  such as logic formulas  from examples with errors. the basis for learning is provided by a selection criterion which minimizes a combined measure of discrepancy of a description with training data  and complexity of a description. learning rules for two types of descriptors are derived: one for finding descriptors with good average discrimination over a set of concepts  second for selecting the best descriptor for a specific concept. once these descriptors are found  an unknown instance can be identified by a search using the descriptors of the first type for a fast screening of candidate concepts  and the second for the final selection of the closest concept. 
1. introduction 
   while the majority of the ai work on learning concentrates in error free domains  there is an acknowledged need for learning techniques directed towards noisy data  dietterich and michalski  1    mitchell  1 . a problem of a major importance in learning from data with errors is the choice of the preference criterion for ranking competing descriptions. the criteria such as maximum likelihood  minimum error  or minimum estimated entropy which are generally used for inference from noisy data  suffice for inferring simple parametric models  but are not well suited to learning in rich spaces of symbolic descriptions used in ai. these criteria minimize the discrepancy between a description and the training observations. if the language used to form descriptions is sufficiently rich to express the training data  such criteria will rank a description that exactly matches the training observations as better or equal to any other description. for example  if the space of descriptions includes predicate calculus expressions  a concept a represented in the training set by three instances  whose parameter  length  assumes values 1  1  and 1  might generate a description:  length a  - 1 or length a    1 or length a  - 1 . any errors in the training data will be represented in such an overspecified description along with possible regularities. a version of this problem known as the  curse of dimensionality  appears even with simple vector models when the number of dimensions is not specified   kanal  1 . 
   one way of preventing the inference process from generating overspecified descriptions is to include some measure of description complexity in the preference criterion  to bias it towards simple descriptions. this idea is well known in philosophy of science  occam's razor   and various measures of complexity  or simplicity  were proposed in ai literature  michalski and stepp  1    michalski  1    mitchell  1    buchanan and mitchell  1 . a specific question is the trade-off between the complexity of a description and the discrepancy with data. a criterion objectively combining these two measures by relating both to kolmogorov'g complexity  kolmogorov  1   was introduced in  segen  1  and called minimal representation criterion. 
in this paper we apply the minimal representation criterion to derive general rules for learning concept descriptions from noisy training data. these rules can be used to learn symbolic descriptors  such as logic formulas  as well as parametric models. in section 1 of this paper we summarize the minimal representation criterion  in section 1 we apply it to derive selection rules for two types of descriptors: concept specific descriptors and globally useful system descriptors  and to decide which descriptors should be used with default values. in section 1 we show how to apply both types of descriptors to classify instances using bottom-up and top-down strategies. 
1. minimal representation criterion 
　　consider the problem of rinding a program for a turing machine  to generate a given finite sequence of observations. while there are infinitely many programs for any such sequence  it seems reasonable to chose the shortest program since it represents the least commitment and minimum redundancy. if we treat a program to be a randomly generated binary sequence with o's and l's having equal probability  then the shortest program is also the most probable one. the problem of selecting a probability model p y  from a sequence of observations  can be recast as a case of the above problem by establishing an isomorphism between the class of probability distributions and a subset of programs for a turing machine  segen  1 . selecting the shortest program in this subset corresponds to finding a probability distribution minimizing the expression 
		 1  
where s p y   is the number of bits needed to specify the probability distribution p y . all the logarithms used in this paper are in the base 1. the above criterion for estimating the probability distribution has been called the minimal representation criterion. its main difference from the maximum likelihood criterion  equivalent to 
seeking a minimum of  comes from the term 
which is a measure of a complexity of the specification of the probability distribution p y . including it in the criterion in effect penalizes more complex distributions. 
   properties of the minimal representation criterion were treated formally in  segen  1 . it has been applied to discover patterns in a continuous signal and in a symbol sequence  and to such problems as selecting the number of clusters. 
1. choosing concept descriptors 
　the problem of selecting a single descriptor for each concept can be stated as follows: given is a training set t  consisting of a set of instances and a concept assignment for each of the instances: also given is a space f of functions  which we call descriptor functions or descriptors  defined on the domain of instances. a descriptor can be any computable function with a probability distribution defined on its range of values. for each of the concepts  we want to select a descriptor  that is most 

	j. segen 	1 
helpful in deciding whether an instance with unknown concept 
assignment should be assigned to this concept. 
   we approach the descriptor selection indirectly  as a problem of estimating the conditional p r o b a b i l i t y w h i l e its form is restricted to: 
		 1  
descriptor selection is a part of the task of finding the estimate of this form  for which we will use the minimal representation criterion. if the instances and their concept assignments in the training set are independent  we can write the logarithm of the probability of the concept assignments given in the training set t as 
		 1  
where  are the instances assigned to concept c/. 
choice of the best descriptor for a given concept ci can be treated as a single concept problem. in this case: 

1 	j. segen 
the product. also  the order imposed by the above proportion will be preserved if we take a logarithm of the right hand side. we will call the result an evidence towards the concept ci based on the facts f  
	 1  
the evidence provides the same ordering of concepts as the probability but its much less expensive to compute  since evaluation of a descriptor changes the evidence only for a subset of all concepts. this subset is known a priori for each descriptor and it can be much smaller than the set of all concepts known to the system. therefore  we can provide links from each s-descriptor to the affected concepts and update only the evidence for these concepts after evaluating the descriptor. an expression similar to  1   but without the above feature  was presented in  charniak  1 . the decision for switching from evidence accumulation to evaluating s-descriptors can come in two ways: either evidence for some concept reaches a given threshold  or after evaluating some number of s-descriptors the concepts are sorted and tested in order of decreasing evidence. 
　if the range of a descriptor is a small set of discrete values we can set a weighted link from each outcome vml to each concept ci for which fm is informative  with the weight of the link equal to  a firing outcome simply adds link weights to the evidence of corresponding concepts. such an organization clearly resembles models of neural nets  and it can be carried out in a parallel architecture such as thistle  fahlman  hinton  and sejnowski  1 . 
1. concluding remarks 
   the methods proposed here for learning of descriptors apply to both parametric models and logic formulas. they are particularly simple for predicate descriptors since their probability can be estimated as frequency. while we have not discussed domain specific descriptor generators  many of the generation schemes presented in ai literature  dietterich and michalski  1   cohen and feigenbaum  1    michalski  1  are compatible with the methods of this paper. a side result that might become important for large systems is the automatic assignment of a default status to some descriptors. the most important direction for future work lies in developing incremental learning strategies  needed for both time and storage efficiency. 
