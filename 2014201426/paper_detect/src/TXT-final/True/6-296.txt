 
thomas sandholm 
carnegie mellon university 
computer science department 
1 forbes avenue pittsburgh  pa 1 
sandholm cs.cmu.edu 

abstract 
collective choice settings are the heart of society. game theory provides a basis for engineering the incentives into the interaction mechanism  e.g.  rules of an election or auction  so that a desirable system-wide outcome  e.g.  president  resource allocation  or task allocation  is chosen even though every agent acts based on self-interest. 
however  there are a host of computer science issues not traditionally addressed in game theory that have to be addressed in order to make mechanisms work in the real world. those computing  communication  and privacy issues are deeply intertwined with the economic incentive issues. for example  the fact that agents have limited computational capabilities to determine their own  and others'  preferences ruins the incentive properties of established auction mechanisms  and gives rise to new issues. on the positive side  computational complexity can be used as a barrier to strategic behavior in settings where economic mechanism design falls short. 
novel computational approaches also enable new economic institutions. for example  market clearing technology with specialized search algorithms is enabling a form of interaction that i call expressive competition. as another example  selective incremental preference elicitation can determine the optimal outcome while requiring the agents to determine and reveal only a small portion of their preferences. furthermore  automated mechanism design can yield better mechanisms than the best known to date. 
　*this material is based upon work supported by the national science foundation under career award iri-1  grant iis1  itr iis-1  and itriis-1. 
1 introduction 
collective choice settings are the heart of society. citizens voting determines a president  producers and consumers bidding determines a set of trades  and surfers hitting links in web browsers determines a bandwidth allocation. a key difficulty in collective choice is that the agents generally have conflicting preferences over the outcomes  e.g.  presidents  resource allocations  or task allocations . work in mechanism design  a subfield of game theory  provides a basis for engineering the incentives into the interaction mechanism  e.g.  rules of an election or auction  so that a desirable-according to some objective-outcome is chosen even though every party acts based on self-interest. 
　however  there are a host of computer science issues not traditionally addressed in game theory that have to be addressed in order to make mechanisms work in the real world. those computing  communication  and privacy issues have to be handled while simultaneously handling the economic incentive issues. this is a particularly exciting research area because those issues are intimately intertwined  as i hope to convey. for example  the fact that agents have limited computational capabilities can ruin the incentive properties of established auction mechanisms  and give rise to new gametheoretic issues. on the positive side  computational complexity can be used as a barrier to insincere strategic behavior in settings where economic incentive engineering is known to fall short. 
　novel computational approaches and algorithms can also enable new economic institutions. for example  sophisticated market clearing technology with specialized search algorithms enables a new form of interaction that i call ex-
pressive competition: empowering market participants with potent expressiveness akin to human-to-human negotiation while at the same time harnessing the forces of competition  the global scale of the internet  and the speed and accuracy of algorithmic market clearing with all relevant information in hand. furthermore  even the mechanism itself  such as the rules of an auction  can be designed automatically-in many cases yielding better mechanisms than the best known to date. 

computers and thought award paper 	1 

　this writeup begins from distributed peer-to-peer negotiation  section 1   and transitions to markets that have have a mediator such as an auction server  section 1 . in this context 1 will lay out the vision and technology for expressive competition. section 1 discusses multiagent preference elicitation in auctions and voting settings: can the mediator elicit the information needed to determine the optimal outcome without requiring the agents to determine and reveal their preferences about impertinent aspects of the problem  the issue of how carefully computationally constrained agents should determine their preferences is addressed in section 1. it turns out that the computational constraint undermines desirable incentive properties in established auction mechanisms  and gives rise to new game-theoretic issues  in particular a phenomenon which i call strategic computing: using one's limited computing to approximate others' preferences at the cost of approximating one's own. the reverse is shown in section 1: computational complexity can be used as a barrier to undesirable strategic behavior. i illustrate this in voting. 
　section 1 discusses a new idea which i call automated mechanism design: designing the interaction mechanism computationally for the specific setting at hand. section 1 shows how mechanism design can not only be used to lead to a desirable outcome in a multiagent system  but also to determine a way to execute the outcome. in particular  the writeup looks at safe exchange mechanisms for carrying out trades among anonymous parties on the internet. finally  conclusions  perspective  and promising avenues for future research are presented. 
1 a first tack: peer-to-peer negotiation 
work on automated negotiation began in peer-to-peer contexts where the negotiating agents  humans or software  make deals with each other. a key insight for analyzing such negotiation is to think about the negotiation process as an ai search algorithm where the outcome is characterized by decision variables to which the negotiation assigns values  sathi and fox  1; sandholm  1; corny et al.  1;sycara et a/.  1; sandholm  1 . for example  in task allocation negotiation there is a decision variable for each task  and the variable's value is the name of the agent that the task is allocated to. there are several high-level families of peer-to-peer negotiation search algorithms. for example  the agents might negotiate one variable at a time  committing to the assignment before moving on to the next variable. this is analogous to constructive search in ai. as another example  the search might start from a status quo assignment of values to variables  for example  the initial assignment of tasks to agents before any negotiation begins   and then the agents might iteratively change the variable assignments whenever the agents relevant to the variables in question agree  sandholm  1; 1 .  for example  the current holder of a task can reallocate the task to another agent if they both agree.  this is analogous to iterative refinement search in ai. 
1 contracting based on marginal cost calculations 
in the original contract net framework  smith  1   agents allocated tasks among themselves. however  the framework was for cooperative agents only: an agent was assumed to take on a task whenever that was feasible. for selfinterested agents  more sophisticated methods are needed. a key idea toward this direction was contracting based on marginal costs  sandholm  1; 1; 1 . when a contract is proposed to an agent  the agent evaluates the cost of taking on the contract obligations  e.g.  tasks  by solving its local planning problem once with the new obligations and once without. the difference in the costs of those two local plans is the marginal cost of the obligations. an agent using this scheme accepts the proposal if the proposer pays it more than its marginal cost. similarly  when proposing a contract  an agent computes its marginal value of unloading some obligations  and pays another agent up to that amount for the other agent to take on those obligations. a desirable engineering facet of this framework is that it separates the domain specific marginal cost calculator  planner  from the agent's domain independent negotiation module. 
　one practical consideration is that in many settings the local planning problems are intractable  so the marginal costs have to be approximated. for example  in the traconet system for automatically reallocating trucking tasks among dispatch centers  the local planning problems were j fpcomplete vehicle routing problems with several side constraints. the traconet work included heuristic methods for deciding how carefully to approximate marginal costs  sandholm  1; 1; 1; sandholm and lesser  1b . this issue is revisited more formally in section 1. 
1 iterative reallocation and combinatorial contracts 
another early idea in automated negotiation was to have the agents iteratively reallocate the items  tasks   sandholm  1; 1 . an agent that had accepted a task could later contract out that task to some other agent  who in turn could contract it out  and so on. this proved to be highly effective in the traconet system. marginal cost based contracting guarantees that every contract improves the utilities of the contract parties. therefore  every agent's utility increases monotonicaliy in the distributed negotiation that keeps reallocating tasks. it follows that the agents can enter and exit the negotiation dynamically without risking a loss. 
　the marginal cost based iterative reallocation negotiation can be viewed as a distributed hill-climbing search where the height on the hill is the sum of the agents' utilities  sandholm  1 . under this interpretation it is easy to show that contracting one item  task  at a time against a payment  aka. original  o  contract  does not generally lead to an optimal outcome: the search gets stuck in a local optimum. this can be addressed by combinatorial contracts that enlarge the neighborhood in the hill-climbing search: contracts where multiple items are exchanged for a payment  cluster  c  contracts  1 where items from one agent are exchanged against items from another-potentially with a side payment- swap 
 s  contracts   and where the contract can involve more than 
l
   the traconet system was the first to use combinatorial bidding to allocate trucking tasks-an approach widely used commercially for procuring trucking services today. 
two agents  multiagent  m  contracts . if all of these contract types are allowed in a single contract  ocsm contract   then agents that contract using the marginal cost principle will reach a global optimum  that is  a task allocation that maximizes social welfare  which is simply the sum of the agents' utilities  in a finite number of contracts-and no subset of those contract types suffices  sandholm  1; 1a . thus the agents can myopically make contracts in any order.' from a hill-climbing search perspective  the neighborhood is large enough that from any allocation of tasks  a profitable contract exists that takes the agents to any other task allocation. therein also lies a key weakness. especially in a distributed setting  it can be difficult to find a combinatorial contract  involving multiple items and multiple agents  that will improve the current solution. also  the sequence of hill-climbing contracts can be exponentially long in the worst case. therefore  in practice  the optimal outcome is not found. nevertheless  the combinatorial contracts help reach better outcomes than o-contracts  andersson and sandholm  1; 1 . 
　another approach for avoiding local optima in search is backtracking. it turns out that a backtracking instrument can be constructed for negotiation as well  as the next section shows. it can easily be applied to the reallocation negotiation discussed in this section  instead-or in conjunction with- the combinatorial contract types  andersson and sandholm  1 . with that instrument  a backtracking option can be added to each  or only some  of the contracts. however  the backtracking instrument applies to basically any negotiation setting  and i will discuss it in that broader context. 
1 leveled commitment contracts to enable backtracking 
negotiating agents usually have to act under uncertainty  yielding behavior that is suboptimal in hindsight. for the purposes of the ensuing discussion  i divide the uncertainties faced by agents in negotiation into two high-level classes: 
  domain uncertainty stems from an agent not knowing how its local situation will change. such changes affect the value  cost  and feasibility of the deals that the agent has made. for instance  which of an agent's resources will break  or become available  that affect the agent's costs-or even feasibility-of handling different combinations of its tasks 1 
  negotiation process uncertainty stems from an agent not knowing what future negotiation events will occur. consider  for instance  the following uncertainties that an agent may face. which of my pending bids will get accepted  which  parts  of my tasks will i be able to subcontract out in the future  and at what prices  what tasks will i be offered and at what prices  the answers to all of these questions affect the cost of taking on  or letting go of  other obligation due to complementarity 
　　1 an additional source of uncertainty arises through the other agents' non-negotiation actions. specifically  how will the others act regarding aspects that have not been contractually bound  this is pertinent in domains where those actions can affect the agent's utility by hindering or helping the agent   
and substitutability: the cost of taking on an obligation usually depends on what other obligations one has. 
　in automated negotiation systems for self-interested agents  contracts have traditionally been binding. they do not allow agents to accommodate future events that are uncertain due to domain uncertainty or negotiation process uncertainty. both of these classes of uncertainty may also include subjective uncertainty due to an agent's limited capability to process information-for example computationally limited capability to do mental lookahead in a  sequential  negotiation process. if  in a negotiation  an agent has made a commitment that turns out unprofitable in hindsight  the agent would like to backtrack that commitment. for example  more lucrative offers can arrive later  or handling a task can turn out more costly than anticipated. 
　backtracking in negotiation search can be enabled using an instrument called a leveled commitment contract  where each contract party can unilaterally decommit from the contract by paying a predetermined penalty  sandholm and lesser  1b; 1; 1 . this mitigates both domain uncertainty and negotiation process uncertainty  whether objective or sub-
jective.1 a concern with this is strategic breach: a rational self-interested agent is reluctant to decommit because there is a chance that the other party will decommit  in which case the former agent gets freed from the contract  does not have to pay a penalty  and collects a penalty from the breacher.  this is an example issue that arises due to self-interest  and renders inapplicable traditional backtracking techniques like distributed constraint satisfaction  e.g.   yokoo  1   which assume that all parties execute the distributed algorithm faithfully.  given the contract price  decommitting penalties  and the agents' prior distributions of the value of the contract  one can conduct a nash equilibrium analysis of the decommitting game  in other words  one can find decommitting strategies for the agents such that each agent's strategy is a best response to the other's . each agent's best-response strategy is defined by a threshold on the value of the contract for that agent. if the value is below that threshold  the agent will decommit. it turns out that strategic breach indeed occurs: an agent does not decommit when the contract's value drops below the point where paying the decommitting penalty is 
　　1 a practical type of subjective uncertainty stems from the fact that computing the value  cost  of taking on the obligations of a contract is often intractable-as discussed-so the agents have to resort to approximate marginal value calculation. leveled commitment allows an agent to bid based on a rough value calculation. if the agent wins the bid  the agent can invest a more thorough value calculation. if the contract no longer looks beneficial in light of this more refined calculation  the agent can decommit. the fact that only the winning bidders carry out a refined calculation can save computation system wide. also  the negotiations can be carried out faster because agents can bid based on less computation. 
　leveled commitment can be used to increase the speed of negotiation in an additional way as well. an agent can make  lowcommitment  offers to multiple recipients although those offers are mutually exclusive from the agent's perspective. in case more than one recipient accepts  the agent can backtrack from all but one. this allows the agent to address the recipients in parallel instead of addressing them one at a time and blocking to wait for an answer before addressing the next. 
1 

worth it; rather it needs to drop to a level further down before the agent decommits. however  despite such strategic breach  leveled commitment contracts improve the expected payoffs of all contract parties compared to any contract where backtracking is not an option  sandholm and lesser  1 . it follows that leveled commitment also enables contracts that would not be mutually beneficial without the backtracking option. 
　leveled commitment contracts differ based on whether agents have to declare their decommitting decisions sequentially or simultaneously  and whether or not agents have to pay the penalties if both decommit. these mechanisms lead to different nash equilibria. it is easy to see that in the sequential mechanisms the second mover never decommits if the first mover does; if the first mover does not  then the second mover will decommit if the value of the contract to him turns out to be so low that it is worth paying the penalty. in the simultaneous game where both pay the penalties if both decommit  as an agent's penalty approaches zero  the agent becomes truthful. on the contrary  in the simultaneous game where neither pays if both decommit  as an agent's penalty approaches zero  the agent does not become truthful but the other contract party does! despite the fact that the equilibria of these mechanisms differ  surprisingly  among risk-neutral agents each of the mechanisms leads to the same expected payoffs to the agents if the contract price and decommitting penalties are optimized for each mechanism separately  sandholm and zhou  1 . for agents with risk attitudes  the different mechanisms yield a different sum of utilities  and the relative ranking of the mechanisms varies based on the exact utility functions. 
　computing plays a key role in operationalizing the idea of leveled commitment. given the contract price  the decommitting penalties  and piecewise linear prior distributions on the contract's value for the different contract parties  the nash equilibrium decommitting thresholds for each mechanism can be computed in polynomial time in the number of pieces  sandholm et al  1 . furthermore  given the piecewise linear priors  it turns out that the contract price and decommitting penalties that maximize the sum of the contract parties' payoffs can be determined in polynomial time in the number of pieces for each of the leveled commitment mechanisms. the reader is invited to try a leveled commitment contract optimizer prototype  ecommitter  on the web  sandholm  1b . leveled commitment contracts and the algorithms also generalize to deals that involve more than two agents. 
　the theoretical results discussed above pertain to a single contract. in negotiation there is usually a web of contracts  and an agent's breach can cause the victim of the breach to want to breach on another contract  and so on. there is generally a tradeoff between allowing enough backtracking to sufficiently explore the space for a good outcome and not wasting time in deep cascades of decommits-or even infinite loops of decommitting and recommitting. that tradeoff can be controlled by carefully increasing the decommitting penalties over time  andersson and sandholm  1; 1 . 
1 a paradigm shift to mediated clearing 
the achilles heel of peer-to-peer negotiation is negotiation process uncertainty. agents make commitments without visibility into what is going to occur later on in the negotiation process  and what has already transpired in negotiation among other agents . leveled commitment contracts reduce  but do not eliminate  the negative impact of such uncertainty. this uncertainty also introduces strategic problems. for example  if an agent expects a better deal in the future  which it cannot profitably handle if it takes on the contract currently offered to it   the agent may want to pass on the current offer. so  the agent is not best off acting myopically-not even in marginal cost based contracting or leveled commitment contracts as the basic analysis discussed in the previous section assumes. rather  a rational agent would want to look ahead into the future  which in turn requires speculating what the other agents will do. acting rationally equates to solving for the agent's best strategy in a game tree whose depth is at least the number of contracts that can occur in the system. such lookahead is intractable in practice  although leveled commitment contracts have been studied in this context with a small number of tasks to allocate  andersson and sandholm  1  . even if an agent can conduct such lookahead  uncertainty about the other agents' private information  their preferences  tasks  resources  etc.  causes the agent to make commitments that are suboptimal-in light of later negotiation events-for the agent and for social welfare  andersson and sandholm  1 . as discussed  there is an additional problem in peer-to-peer negotiation: it can be prohibitively complex to find a contract that improves the current solution  especially when using combinatorial contracts involving multiple items and multiple agents. 
　in light of these problems  it is clear that in many settings  economically better solutions can be obtained by collecting the agents' information to a mediated clearing point such as an auction server  and conducting a search  aka. a clearing  there to determine the outcome  rather than conducting a distributed  negotiation  search. the reason is that the mediated clearing has all the information in hand while the distributed search makes decisions based on incomplete views. furthermore  the mediated clearing can be programmed to execute the search algorithm faithfully while in the distributed search the agents will act based on self-interest-potentially causing the search not to find an optimal outcome.1 the mediated approach can be structured so that it motivates the agents to reveal their information truthfully; this will be discussed in section 1. in short  the mediated approach removes the negotiation process uncertainty.  the domain uncertainty remains  and the leveled commitment methodology can be used to mitigate it.  in most electronic commerce applications the mediated approach also saves communication because there is many-to-one communication instead of many-to-many  and because each issue needs to be communicated only once- rather than repeatedly as is commonly the case in peer-to-peer 
　　1 in the distributed search it can even be difficult to identify when an optimal solution has been found  and special termination detection algorithms are usually needed  sandholm  1; 1; walsh and wellman  1 . 

negotiation. 
　the shift to mediated clearing introduces the need for technology for conducting the clearing. the following subsections describe the mediated approach in more detail  different forms of the clearing problem  and algorithms for solving it. 
1 	a canonical example: combinatorial auction 
consider a setting where multiple distinguishable items  e.g.  a right shoe and a left shoe  are auctioned sequentially  and a bidder's valuation for a bundle of items is not the sum of those items' valuations. most multi-item allocation settings exhibit such nonadditivity  for example  strategic sourcing  allocation of trucking lanes  electricity markets  as well as many task and resource allocation problems in computer science. to bid appropriately for the item that is auctioned first  right shoe   a bidder needs to guess which other items he will win in the later auction s . this requires lookahead in a game tree  which is intractable if numerous items are auctioned. even with exact lookahead  the bidder generally does not know deterministically what will transpire in the auction-due to incomplete information about the other bidders' valuations of the items. for example  some other bidder could be a collector of left shoes who also likes right shoes somewhat  but less than our bidder.  this can cause our bidder to win just the right shoe that has no value to him  that is  a bundle of items that is undesirable to the bidder given the prices. while undesirable to our bidder  the outcome  that is  allocation of the items to the agents  also does not maximize social welfare. it would have been better to give both shoes to the other bidder. in general  the outcome might not maximize social welfare even if each bidder wins a bundle that is worth more than the bidder had to pay for it: an even better allocation of the items to the bidders might exist. 
　these problems  that are due to the negotiation process uncertainty of a sequential auction  can be overcome via a combinatorial auction where bids can be submitted on combinations  bundles  of items  rassenti et al.  1 . for example  a bidder can say:  i am willing to pay up to $1 for items 1  and 1 together . this removes the need for lookahead and for speculation about others because the bidder can evaluate the value of item 1 in the known context where the bidder hypothetical  receives items 1 and 1 as well the bidder cannot get stuck with item 1 in an unprofitable way. this removal of the so called exposure risk makes bidding easier. it also causes the bidders to bid more aggressively because they do not have to factor in the potential downside of getting stuck with undesirable bundles; the aggressive bidding makes the seller better off. finally  social welfare is maximized because the goods are allocated to the bidders that value them the most. 
1 	substitutability and xor-constraints 
the model discussed above  and most other work on combinatorial auctions  see for example  rothkopf et al  1; demartini et al.  1    are based on a setting where each bidder can bid on bundles of items  and any number of a bidder's bids can be accepted  except  of course  that bids on overlapping bundles cannot be accepted . this works well when bids are superadditive: 
where bi is the bid of agent i  and s and s' are disjoint sets of items. in other words  the current techniques focus on capturing synergies  complementarities  among items. however  in many auctions in practice  some items can at least partially substitute for others  e.g.  when bidding for an umbrella and a raincoat . for instance when bidding for landing slots for a given airplane flight  the bidder is willing to take any one of a host of slots  but getting more than one adds only slight value because extra slots beyond the first one obtained only serve as backup. substitutability causes bids to be subadditive: this can lead to problems. 
for example  what happens if agent i bids 	= 
　　　= 	and 	= a n d there are no other bidders  the auctioneer could allocate items 1 and 1 to agent 1 separately  charging 	+ 	= 	instead of 
　this problem can be removed by using a bidding language where the bidders can submit xor-bids  that is  bids on bundles such that only one of the bids can get accepted  sandholm  1a   this allows the bidders to express general preferences with both complementarity and substitutability. 
in other words  the bidder can express any value function  where m is the number of items for sale in the auction. for example  a bidder in a 1-item auction may submit the following input to the auctioneer 

　while xor-bids are fully expressive  representing one's preferences in that language often leads to large numbers of bids that are all combined with xor. to maintain full expressiveness  but at the same time to make the representation more concise  one can use a bidding language called or-of-xors  sandholm  1b; 1b . in this language  a set of bids can be combined with xor  forming an xordisjunct. multiple xor-disjuncts can then be combined with non-exclusive ors to represent independence  much like a lack of an edge represents independence in a bayes net . for example  a bidder who wants to submit the same offer as in the example above  can do so by submitting the following more concise input to the auctioneer: 

the xor-bidding language is a special case of the orof-xors language. therefore  the shortest way to represent any particular value function in the or-of-xors language is never longer than in the simple xor-bidding language. the reader is invited to try out xor bidding and 
or-of-xors bidding in our internet auction server prototype  emediator  see http: / / www. cs. cmu. edu/   amem/emediator . later other logical bidding languages were proposed  namely the xor-of-ors language where xors combine or-disjuncts  and the or  language where xor-constraints can be submitted between arbitrary pairs of bids  nisan  1 . recently  recursive logical bidding lan-
guages have also been proposed  hoos and boutilier  1; boutilier  1 . 
1 expressive competition beyond combinatorial auctions 
one can view combinatorial auctions as an extremely special ample in a reverse auction  a buyer may want 1 nuts and 1 bolts. a bidder's bid may state:  i offer 1 nuts and 1 bolts for $1 . 
　finally  markets differ based on whether or not extra units can be thrown away for free. this is called free disposal most markets have free disposal. it makes the clearing problem easier because any solution where supply equals or ex-
ceeds demand is feasible. without free disposal  supply needs to equal demand on every item. 
side constraints on the clearing: a form of extremely concise expressiveness 
case of a broader approach that i call expressive competition. strengths and to avoid the exposure problem. bidding on bun-expressive bidding enables the bidders to express their 

the vision is to empower market participants with potent expressiveness akin to human-to-human negotiation while at the same time harnessing the forces of competition  rather than 1-to-l negotiation   the global scale of internet auctions  and the speed and accuracy of algorithmic market clearing with all relevant information in hand. 
market types 
there are three high-level market types for expressive competition  each of which can involve bidding on bundles  and expressions of substitutability  for example with xor constraints using the or-of-xors language . in a combinatorial auction  there is one seller  and multiple buyers who bid. the clearing problem  aka. winner determination problem  is that of determining which bids win and which lose so as to maximize the sum of the winning bids* prices- under the constraint that every item can be allocated to at most one bid. in a combinatorial reverse auction  there is one buyer with a set of items he wants to procure  and multiple sellers who bid  sandholm  1b; sandholm et al  1 . the clearing problem is that of determining which bids win and which lose so as to minimize the sum of the winning bids' prices-under the constraint that every item in the set gets procured. in a combinatorial exchange  aka. combinatorial double auctions  there are multiple buyers and multiple sellers  sandholm  1b; sandholm et al.  1; walsh et al.  1 . a bidder can also act both as a buyer and as a seller  even in one bid. for example  one of his bids may state:  i want to buy a car  sell a boat  buy a bike  and get paid $1 . there are two natural clearing objectives for exchanges  sandholm and suri  1 . in surplus maximization  the goal is to maximize the sum of the payments collected from winning bids that offer a payment  minus the sum of the payments paid to winning bids that require a payment. in liquidity maximization  the goal is to maximize the number or dollar volume of trades. in an exchange  the constraint on the clearing is that among the winning bids  supply meets demand on every item  if the exchange sells an item to someone  it also has to buy it from someone . 
　for each of these market types  there are two variants: the single-unit variant  described above   and the multi-unit variant. in the latter  there are multiple indistinguishable units of each distinguishable item in the market  sandholm  1b; sandholm and suri  1; leyton-brown et al  1b; 
gonen and lehmann  1; sandholm et al  1 . for exdles  potentially with a language such as or-of-xors to enable substitutability to be expressed  is a simple form of expressive bidding  but representing one's preferences in such a language may require an expression of exponential length in the number of items. more concise forms of expressiveness can be used instead-or better  in addition. a key approach for accomplishing this is to allow bidders to express side constraints on the clearing  sandholm and suri  1b; kalagnanam et al.  1 . for example in a reverse auction  a bidder could submit a number of bids  maybe on bundles  and potentially with xor constraints   but in addition he could state that his capacity to produce tomatoes is only 1 tons. this would render infeasible all clearing solutions in which he is allocated more than 1 tons of tomato production. this is an instance of a class of constraints that i call unit constraints. similarly  in an auction  a bidder could submit a 
number of bids  maybe on bundles  and potentially with xor constraints   but in addition he could state that he has a budget constraint of $1 1. this is an instance of a class of constraints that i call cost constraints. yet another form of concise expressiveness is to allow bidders to submit discount schedules  e.g.   if i get to supply at least $1 1 of tomatoes to you  i will give you a 1% discount. at $1 1  i will raise the discount to 1%.  or the bidder could submit a supply  demand  curve where the per-unit price of tomatoes is a function of the quantity  sandholm  1b; sandholm and suri  1a; 1 . 
　while work on combinatorial auctions has traditionally focused on increasing the expressiveness for the bidders  i view expressive competition as having two equally important parts  expressive bidding and expressive bid taking. a key approch toward expressive bid taking is to allow the bid taker to submit side constraints on the clearing. this allows him to model and honor legal constraints  for example the following cost constraint:  minority bidders have to win 1% of the auction . it also allows the bid taker to honor contractual obligation. for example  the buyer in a reverse auction for transportation services may have a contract that states that joe's trucking has to get at least $1 1 of business. this can be modeled as a side constraint in the clearing  and the clearing algorithm will decide exactly what services are procured from joe's trucking and what services from other providers. this approach allows one to reap the benefits of both longterm contracts and dynamic pricing-modes of trade traditionally considered mutually exclusive. finally  the bid taker 
can submit his business rules as side constraints. he may use counting constraints such as  i don't want to deal with more than 1 winning suppliers  and i do not have the capability to handle more than 1 at my plant in chicago . he may also use cost constraints like  i don't want any one supplier to win more than 1% of the business  so that my supply chain stays competitive for the long run  . 
　the use of side constraints allows one to find a market clearing solution that is implementable in the world because the real-world constraints are honored. by changing side constraint and running the clearing algorithm again  the bid taker can also conduct quantitative what-if analyses. for example   how much would my procurement cost in this reverse auction increase if i decrease my supply base down to 1 suppliers   or   how much more would i save in this reverse auction if i did not go with a sole-source contract for electrical supplies in my cincinnati plant  but rather allowed 1 suppliers   
　in the 1 markets with expressive competition that we have fielded to date  see  e.g.   combinenet  inc.  1e; 1c; 1b; 1d; 1a    we have seen hundreds of side constraint types. we abstracted them into seven general classes  the most prevalent of which are cost constraints  unit constraints  and counting constraints. this abstraction allows the clearing algorithm to be designed for a few classes rather than for hundreds of constraint types. 
non-price attributes in the clearing 
an additional form of expressive competition stems from the fact that in many markets there are non-price attributes that are pertinent to the clearing problem  such as color  width  delivery date  quality  insurance terms  and so on. there are at least two reasons for introducing multi-attribute techniques into the clearing problem. first  in a basic combinatorial auction  or reverse auction or exchange   each item has to be completely specified. in many settings  this is overly restrictive. it is more desirable to leave some of the item attributes unspecified  so that each bidder can propose in his bids the attribute vector that is most suitable to him. each bidder can also submit multiple bids with alternative attribute vectors  which is desirable because different attribute vectors are generally not equally valuable to the bid taker. second  a bid from one bidder can be more valuable than the same bid from another bidder  due to bidder attributes such as historical data on timeliness and quality   and this should be taken into account in the clearing. 
　multiattribute considerations can be integrated into combinatorial auctions and reverse auctions as follows  sandholm and suri  1b . let  be a vector of attributes. these can be item attributes and/or bidder attributes. some of the attributes can be specific to one bid  say j  while others might not  such as quality of a certain line of products . the vector can include attributes revealed by the bidder as well as attributes whose values the bid taker gets from other sources such as historical performance databases. the bid prices can 
be re-weighted based on the additional attributes. the new 
price of any bid where is the original price of that bid. the re-weighting function / is usually expressed by the bid taker  seller in an auction  buyer in a reverse auction -either before or after the auction to characterize his preferences.  for example  the buyer of sea freight services may give a 1% advantage to bids that include less than 1 interim ports on the route from source to destination.  the clearing problem is then solved using these revised prices. 
　unlike in auctions and reverse auctions where multiple attributes can be handled in a preprocessor to the clearing problem as shown above  in exchanges multiple attributes cannot be handled in a preprocessor. the reason is that in an exchange there are multiple bid takers  each buyer and each seller is a bid taker in this sense   and they may have different preference functions / over attribute vectors. multiple attributes can still be handled  but their handling has to be incorporated into the clearing problem itself. this can be accomplished as follows. treat items that have different values of the item attributes as different items. then use a separate decision variable not just for each such item  but for each   item  buyer  seller   tuple. this way each buyer  seller  can condition his bid price on the item attributes and on whom he is buying from  selling to . conditioning on whom he is buying from  selling to  is pertinent when bidder attributes have to be taken into account.1 
1 	complexity of the clearing problem 
expressive competition is a new way of conducting business  and has a host of advantages as discussed above. however  it requires solving the clearing problem-a combinatorial optimization problem. many variants of it are hard  and the variants span an intriguing spectrum of worst-case complexity when it comes to the complexity of finding a feasible solution  an approximately optimal solution  or an optimal solution: 
  as discussed  in the canonical combinatorial auction there is one unit of each item  bids can be submitted on bundles  and bids on overlapping bundles cannot both win   there is free disposal  and there are no xorconstraints or other side constraints. optimal clearing in this setting is np-complete  rothkopf et a/  1 . 
  that problem is also inapproximable: no polynomial-time algorithm can guarantee a solution that is better than a bound from optimal  where n is the number of bids  unless p=zpp   sandholm  1a . 
  optimal clearing in a combinatorial reverse auction and a combinatorial exchange is np-complete even if there is only one unit of each item  free disposal  and no xorconstraints or other side constraints  sandholm et al.  1 . 
  combinatorial reverse auctions are not as inapprox-imable as combinatorial auctions: a bound  1 + inm  can be obtained in polynomial time even in the multiunit case  where m is the largest number of units that 
　　1 there also exist auctions and reverse auctions where the multiattribute aspects cannot be handled in a preprocessor. this occurs if the bid taker's way of evaluating attributes depends on which bids win. in such cases  as in exchanges  the way in which the attributes are to be taken into account can be modeled in the clearing problem itself. 

any bid contains  sandholm et al.  1 .  this assumes the canonical setting where there is free disposal  and no xor-constraints or other side cosntraints.  
finding a feasible solution is trivial in combinatorial auctions  accept no bids  or any one bid  and in combinatorial reverse auctions  accept all the bids; if this does not cover the demand  then nothing will . without free disposal  even finding a feasible solution is mpcomplete in these variants  even in the single-unit case with no xor-constraints or other constraints  sandholm et al.  1   this implies inapproximability to any ratio bound . the same results apply to combinatorial exchanges  the hardness applies if the trivial  no bids accepted  solution is excluded . 
  xor-constraints do not change the approximability of canonical combinatorial auctions  sandholm et al.  1 . however  xor-constraints make finding a feasible solution np-complete in combinatorial reverse auctions  even in the single-unit case with free disposal   sandholm et al.  1 . in other words  combinatorial reverse auctions are more approximate than combinatorial auctions  but this ordering reverses when xor-constraints are introduced. 
  combinatorial exchanges inherit the inapproximability of both combinatorial auctions and combinatorial reverse auctions  sandholm et al.  1 . 
  cost constraints and unit constraints do not affect the complexity class of the clearing problem when bids can be submitted on bundles: the basic case where bids have to be accepted all or nothing remains np-complete  and the case where bids can be accepted fractionally can be solved in polynomial time using linear programming  sandholm and suri  1b . however  xorconstraints and other counting constraints make even the fractional case np-complete  sandholm and suri  1b . there exist severe side constraints that restrict the search space enough so that even the case where bids have to be accepted all or nothing is optimally clearable in polynomial time  sandholm and suri  1b . 
  if bids can be accepted fractionally  a combinatorial exchange  and auction and reverse auction  can be optimally cleared by accepting a very small number of bids fractionally: m + 1 if the objective is to maximize surplus  and m + 1 if the objective is to maximize liquidity  kothari et al  1 . here m is the number of distinguishable items in the market. this clearing can be found using linear programming. however  if xorconstraints are allowed  finding the surplus-maximizing clearing is np-complete even in the fractional case  even if there is just one item in the market with multiple units of it  kothari et al.  1 . 
in an auction where bids can be submitted on individual items only  not on bundles   winner determination is trivial: simply accept the highest bid on each item. under budget constraints  optimal winner determination becomes np-complete. curiously  if instead of a budget constraint each bidder submits a constraint on the number of items he wins  the auction can be optimally cleared in polynomial time using bmatching  tennenholtz  1 . if bids can be accepted fractionally  winners can be determined in polynomial time using linear programming even with budget constraints. under xor-constraints  or other counting constraints   clearing is np-complete even if bids can be accepted fractionally  sandholm and suri  1b .  if each bidder places an xor-constraint between every pair of his bids-so that at most one of his bids can be accepted-then the problem becomes the assignment problem  sandholm and suri  1b . the assignment problem can be solved in polynomial time  kuhn  
1 .  
  even if there is just one item in the market  multiple units of it   piecewise linear supply/demand curves are np-complete to clear optimally  in an exchange  auction  and reverse auction   but with linear supply/demand curves  optimal clearing can be done in polynomial time  sandholm and suri  1a; 1 . 
　in summary  there is a tradeoff between expressiveness  with its economic and usability advantages  and the computational complexity of clearing the market. while many variants of the clearing problem are hard in the worst case  in practice problems of real-world sizes can usually be solved.  this is most likely due to the co-evolution of clearing technology and expressive markets.  experiments show that combinatorial reverse auctions tend to be easier to clear optimally than combinatorial auctions  which in turn are easier than combinatorial exchanges  sandholm et al.  1 . 
1 	algorithms for the clearing problem 
while the idea of a basic canonical combinatorial auction is two decades old  combinatorial auctions have traditionally not been used. the main reason is that the clearing problem is tough. in the last few years  hardware and especially clearing algorithms  have reached a level of scalability that enables combinatorial auctions of real-world sizes to be cleared optimally. as a consequence  numerous combinatorial auctions have emerged in industry. 
　the rest of this section focuses solely on clearing algorithms that find an optimal solution. optimal clearing is important because real money is at stake  because approximate clearing yields solutions extremely far from optimal  at least in the worst case   because a large change in the winners can occur even if the solution changes slightly from optimum  and because suboptimal clearing ruins the incentive properties of the market  as will be discussed in the next subsection . 
　the first-generation special-purpose clearing algorithms for the canonical combinatorial auction used a search tree where branching was on items  figure 1 left   sandholm  1a; 1b; fujishima et al.  1 . the newer  and significantly faster  clearing algorithms use a search tree where branching is on bids instead  figure 1 right   sandholm and suri  1; sandholm et at   1 . it has the advantage that there is more flexibility in variable ordering: at each branch point  commitment occurs on only one bid rather than on all of the bids that include a specific item. another ben-
computers amd thought award paper 

efit is that  unlike the branch-on-items tree  the branch-onbids tree easily supports all of the market designs for expressive competition discussed above  sandholm and suri  1; sandholm et a/.  1 . 

figure 1: branching on items vs. branching on bids. 
　interestingly  although the problem is np-complete  both the branch-on-items tree  sandholm  1a  and the branchon-bids tree  sandholm and suri  1  are of polynomial size in the number of bids even in the worst case  and exponential in the number of items . this is desirable because the auctioneer usually controls the number of items for sale  but not the number of bids that happen to be submitted. 
　the techniques that make the branch-on-bids search fast in practice include bid ordering heuristics that dynamically choose the next bid to branch on  techniques for dynamically choosing the bid ordering heuristic itself based on context  upper bounding based on a linear programming relaxation of the remaining subproblem  and intelligent addition of cutting planes that reduce the size of the linear programming polytope  but do not cut out the optimal integer solution   lower bounding based on rounding techniques  decomposition of the problem when the bid graph  figure 1 right  splits into independent components  techniques for enhancing upper and lower bounding based on information from sibling components  and methods for identifying and solving potential polynomially-solvable special cases at each node. in the interest of flow  i will not go further into those techniques here. many of them are described in detail in specialized papers  sandholm and suri  1; sandholm et al  1 . 
　suffice it to say that the cabob algorithm  sandholm et al  1  is  by and large  the fastest algorithm for the canonical combinatorial auction clearing problem currently  based on the widely adopted approach of evaluating clearing algorithms on standard randomly generated benchmark distributions  sandholm  1a; fujishima et al.  1; leytonbrown et al  1a; andersson et al  1 . interestingly  economically motivated random distributions  specifically the ones from cats  leyton-brown et al  1a   tend to be very easy compared to totally random distributions  sandholm et al.  1 . similarly  real clearing problems are often easier than totally random ones. our fielded algorithms systematically solve real reverse auctions with up to tens of thousands of items  hundreds of thousands of bids  and hundreds of thousands of side constraints to optimum. 
1 	incentives to bid truthfully: the 
vickrey-clarke-groves  vcg  mechanism 
one concern is that bidders may bid insincerely. for example  in an auction where each winning bidder is charged the sum of the prices of his winning bids  the bidders are motivated to bid less than their true valuations of the items. 
　this issue can be overcome under the traditional assumption that each bidder i has a quasilinear utility function:  where s is the bundle that bidder i 
wins  and pt is the price that he has to pay. it turns out that under this assumption  a fully expressive bidding language combined with an optimal clearing algorithm is sufficient  sandholm  1b; 1b  and necessary  nisan and ronen  1  for being able to design auction mechanisms that yield a social welfare maximizing allocation and where every bidder's dominant strategy is to bid truthfully.1 with such technology  bidding truthfully can be made a dominant strategy by using the vickrey-clarke-groves  vcg  mechanism  vickrey  1; clarke  1; groves  1 . this means that each bidder is motivated to bid truthfully regardless of how others bid-thus rendering speculation about others futile.1 
　the vcg mechanism can be applied to markets with expressive bidding as follows. the optimal clearing outcome is first computed as usual. the amount that an agent needs to pay is the sum of the others' winning bids had the agent not participated  minus the sum of the others' winning bids in the actual optimal outcome. so  the clearing problem has to be solved once overall  and once per winning agent without any of that agent's bids or side constraints.1 
1 	preference elicitation in multiagent systems 
most  but not all  game-theoretic interaction mechanisms are so called direct-revelation mechanisms  where each participant reveals all of his private information  e.g.  bundle valuations in combinatorial auctions  completely up front. there 
   1 under extremely strong assumptions about the bidders' utility functions  truth-dominance can be accomplished with approximate clearing  see e.g.   lehmann et al.  1; mu'ajem and nisan  1    but because the clearing problem is inapproximable  the allocation is extremely far from optimal in the worst case. 
   1 the vcg mechanism has also been used in mediated planning among software agents  ephrati and rosenschein  1; 1; ephrati  1 . 1 in auctions  the mechanism is budget balanced: the seller collects the nonnegative amounts paid by the bidders. in exchanges  an external benefactor is usually needed. in fact  with private valuation information on both the buy side and the sell side of the market  there is no mechanism for general quasilinear utility functions-even with 
just one unit to trade and only one buyer and one seller-that motivates the parties to participate in the mechanism  yields the social welfare maximizing outcome  and is budget balanced  myerson and satterthwaite  1 . 

are fundamental results that show that  in the absence of computation/communication limitations  this restriction comes at no loss in any sense  c/ the revelation principle  mascolell etal  1  . however  in practice such mechanisms are problematic because the agents may need to determine their own preferences via costly deliberation  e.g.  computing  sandholm  1; 1; 1c; larson and sandholm  1b; 1c   or information gathering  and communicating complete preferences may be undesirable from the perspective of privacy or conserving bandwidth. 
　this issue can be addressed using a methodology where the mediator incrementally elicits the agents* private information on an as-needed basis in a manner directed to being able to determine the outcome  that would have come about had the agents revealed all of their private information to the mediator   conen and sandholm  1 . it turns out that often the outcome can be determined while eliciting only a small portion of the agents' private information. a key insight is that in multiagent systems  what information is needed from a party depends on what information the other parties have revealed. this is a central motivation for interleaved preference elicitation from multiple agents. the approach offers the advantages of incremental problem solving  as in peerto-peer negotiation  while reaping the benefits of mediated clearing discussed earlier. the rest of this section studies this approach as applied to combinatorial markets and voting. 
1 	preference elicitation in combinatorial auctions 
preference elicitation is crucial in combinatorial markets because each agent has an exponential number of bundles to evaluate  and again  each such evaluation problem can be hard  sandholm  1; sandholm and lesser  1b; sandholm  1c; larson and sandholm  1b; 1c  . each agent would like to focus on a small number of bundles in order to minimize evaluation effort  communication  and loss of privacy. on the other hand  the social welfare among the agents  the seller's revenue  and the agent's utility will usually suffer if the agent's evaluation of a bundle that he would win is not communicated to the auctioneer. in a usual combinatorial auction it is difficult for the agent to decide which bundles to bid on because others' bids determine what bundles the agent would be competitive on. 
　to address this problem  we developed a methodology where an elicitor software  residing at the auctioneer  incrementally builds a model of the agents' valuation functions vi  conen and sandholm  1 . the elicitor queries the agents about vi  and fully assimilates the answers into its model. the next query to be asked is always chosen based on the answers so far. the elicitor in a sense opens up the clearing algorithm and elicits the inputs needed for determining the optimal outcome  allocation of items to agents . the agents are never asked for information that the elicitor can already infer from the answers  or that is known to be impertinent for determining the optimal allocation. the elicitor terminates the process when it has found a provably optimal allocation. 
　the rest of this section will focus on combinatorial auctions under the usual free disposal assumption  which almost always holds in practice. it gives structure to the elicitation problem because for each agent  the value of a bundle is no greater than the value of any superbundle of that bundle 

the general case 
even with free disposal  the worst-case communication complexity for even approximately optimally clearing the market is exponential in the number of items  regardless of the query types or the elicitation policy  nisan and segal  1 . the discussion below will focus on natural value queries:  what is your valuation of bundle st  other query types can increase the efficiency of elicitation  conen and sandholm  1; hudson and sandholm  1; conen and sandholm  1b; 
1a .1  it turns out that in practice elicitation is very promising: only a vanishing fraction of all the queries are asked before the elicitor can clear the auction provably optimally  hudson and sandholm  1 ! the preference elicitation methodology is also very promising in combinatorial reverse auctions  hudson and sandholm  1a  and combinatorial exchanges  smith et al.  1 . 
　one may also ask whether there exists a universal revelation reducer for combinatorial auctions  that is  a general elicitor algorithm that saves some elicitation  finishes finding an optimal outcome and proving its optimality without asking all value queries  on all instances where some instance-specifc elicitor saves some elicitation  that is  where the shortest certificate for verifying that a proposed outcome is optimal is shorter than the number of agents times the number of bundles . it turns out that a deterministic universal revelation reducer cannot exist  but randomized ones are easy to construct  hudson and sandholm  1b . 
restricted preferences for which the worst-case number of queries is polynomial in items 
for restricted classes of vi functions  the worst-case number of value queries needed is polynomial in the number of items being sold. many of these classes are rich enough to exhibit both complementarity and substutitability. this subsection will present classes like that  which are also arguably natural for capturing valuation functions. 
　first  consider read-once formulas. a read-once formula is a function that can be represented as a tree  where the items for sale in the auction are at the leaves  together with the bidder's valuations for the individual items. the formula's output value is obtained by feeding in a bundle s of items to the leaves and reading the valuation vi s  from the root a leaf sends the item's valuation up the tree if the item is included in 1  otherwise the leaf sends 1. different types of gates can be used in the nodes of the tree. a sum node sums the values of its inputs; a max node takes the maximum value of its inputs; an all node sums its inputs unless one of the inputs is zero  in which case 
　　1 ascending combinatorial auctions  e.g.   bikhchandani et al.  1; parkes and ungar  1; wurman and wellman  1   can be viewed as a special case of the preference elicitation framework where the queries are of the form:  given these prices on items  and possibly also on bundles   which bundle would you prefer the most  . 
computers and thought award paper 

the output is 1. for example  a legal function on 1 inputs might which gives value 1 to the bundle {1}  1 to bundle {1}  and 1 to bundle {1}. read-once formulas of this type allow for many natural preferences. for example  suppose items are flights and hotel rooms in different locations  e.g.  input represents the ith flight to location j  and represents the ith hotel room in location j  and we want to take just one trip. then for each location j we could compute 
and then at the root of the tree we would 
take a max over the different destinations. more general gates are also possible. let max* output the sum of the k highest inputs  and atleast* output the sum of its inputs if there are at least k positive inputs  and 1 otherwise. finally  let generalk.l be a parameterized gate capable of representing all the above types of gates. for instance  imagine that on a vacation to the bahamas  alice wanted entertainment. if she got to go out on at least three nights  then the trip would be worthwhile. otherwise  she would rather stay home. each night  she takes the maximum valued entertainment option. then there is an atleast1 node combining all of the different nights. in a different situation  imagine that joe wants a more relaxing vacation in hawaii  where he does not want to go out more than three nights. in this case  a max1 gate will be useful. for each night  he chooses the best possible entertainment given to him. then  he takes the best three nights of entertainment. finally  imagine that maggie wants a moderately active vacation  and is interested in going to paris for a week  and wants at least three but no more than four nights of entertainment. then a general1 gate will describe her preferences. it turns out that if the user's valuation function vi can be expressed as a read-once formula with these gates  even if the user is not aware of that   then vi can be elicited with a number of value queries that is polynomial in the number of items in the auction even in the worst case  zinkevich et al.  1 .1 furthermore  if a bidder's valuation function vt is close to some read-once function with the gates discussed above  then a close model of vi can be constructed with a polynomial number of value queries. 
　second  consider the class of preferences that can be expressed as monotone polynomials. for example  
this class is called toolbox dnf 
because it captures settings where each agent has a set of tasks to accomplish  one per term in the polynomial   each task re-
　　an analogous issue arises with shopping agents. consider the following scenario. alice goes to her software agent and asks it to help her purchase a vacation. in order to act on her behalf  the agent first needs to find out alice's preferences  how much is a trip to hawaii worth compared to a trip to the bahamas  does it substantially increase the value to her if she can get some entertainment booked in advance  etc. . then  after scouring the internet  the agent needs to solve the computational problem of deciding on the best vacation package-the one that maximizes alice's valuation minus the cost of the trip. in this scenario  there is no auctioneer. rather  the elicitor is the buyer's helper. again  the amount of querying can be prohibitively large when the buyer has general  monotone  preferences  but with these types of read-once preferences  alice's valuation function can be determined in a polynomial number of value queries. 
quiring a specific set of tools  the variables in the term  and each having its own value  the coefficient on that term . for example  the tools may be medical patents  and producing each medicine requires a specific set of patents. the value of a set of items to the agent is the sum of the values of the tasks that the agent can accomplish with those items. it turns out that any toolbox dnf valuation function v{ can be elicited in a polynomial number of value queries  zinkevich et a/.  1 . 
the power of interleaved preference elicitation from multiple parties 
the above two example classes of valuation functions can be elicited efficiently even if there is only one party whose preferences are to be elicited. so  the efficiency does not derive from the fact that information from other agents restricts the amount of information needed. the next class  on the other hand  derives its ease of elicitation from this phenomenon. 
　consider a combinatorial auction with two buyers that have their valuation functions in the form of the xor bidding language discussed earlier  where each bidder can submit bids on bundles  and all of the bidder's bids are mutually exclusive. it turns out that if no bid includes more than log1 m items  where m is the number of items in the auction   then the provably optimal allocation of items to the bidders can be determined in a worst-case polynomial number of value queries  blum et al.  1 . 
　interestingly  there are classes of vi;  functions where learning the functions requires an exponential number of value queries  while the provably optimal allocation can be constructed in a polynomial number of value queries  blum et al.  1 . for other classes of vt  being able to elicit enough to find the provably optimal allocation in a polynomial number of value queries implies that the vi functions themselves can be learned in a polynomial number of value queries. so  sometimes there is an exponential benefit to interleaving the queries made to the different parties  while at other times the benefit between that and eliciting each agent's preferences separately is polynomially bounded. 
incentives to answer truthfully: ex post equilibrium in an incremental push-pull multiagent elicitation mechanism 
motivating the bidders to answer queries truthfully is another key issue  and is exacerbated by the fact that the elicitor's queries leak information to the bidder about the answers that other bidders have given. recently  a methodology was proposed by which elicitors can be made incentive compatible in the sense that every bidder  with a quasilinear utility function  answering the queries truthfully is an ex post equilibrium  conen and sandholm  1 . this means that bidding truthfully is each bidder's best strategy  for any prior probability distribution that he may hold about the other bidders  given that the other bidders bid truthfully. in other words  truthful bidding strategies form a  bayesian  nash equilibrium even in hindsight.  this does not mean that bidding truthfully is a dominant strategy; if some of the other agents bid insincerely and conditional on the elicitor's query stream to them  one may do better by bidding insincerely. in summary  implementation in ex post equilibrium is stronger than 

implementation in  bayesian  nash equilibrium  but weaker than implementation in dominant strategies.  
　the methodology is the following. the mechanism is structured so that if all the bidders answer truthfully  the final allocation and payments follow the vcg mechanism. the 
amount a bidder has to pay is the sum of the others' revealed valuations for the bundles they get had the bidder not been given any of the items  minus the sum of the others' revealed valuations for the bundles they get in the actual optimal allocation. the elicitor can determine these payments by asking enough queries to be able to determine the welfare elicited if the voter is queried at all  this can be avoided by making sure that the voter does not know how many voters have been queried before him. in fine elicitation  where a voter is asked pairwise preferences one pair of candidates at a time  this can be avoided by making sure that the voter does not know what/how many queries have been made to others  and by making sure that the order in which queries are made to this voter is fixed up front rather than dependent on others' answers. 
maximizing allocation overall  and by asking extra queries 
to determine the welfare maximizing allocation for the auc-as discussed in the previous section  preference elicitation tions where each agent is ignored in turn. the extra queries can significantly reduce the agents' effort of evaluating bun-1 hard valuation problems 

needed to determine the vcg payments are a negligible fraction of the queries needed to determine the optimal allocation in practice  hudson and sandholm  1a   and in some elicitation policies that information comes purely as a side effect with no extra queries at all  conen and sandholm  1b; 1a . 
　truthful answering is an ex post equilibrium even in elicitation mechanisms where the bidders are allowed to pass on some queries  as long as they answer enough queries to determine the optimal allocation and the vcg payments  and to answer queries that were never asked  conen and sandholm  1 . this yields a pull-push mechanism where the elicitor guides the preference revelation  but each bidder can also proactively reveal values on bundles on which it thinks it is competitive. 
1 preference elicitation in voting 
multiagent preference elicitation can also improve the efficiency of running elections  conitzer and sandholm  1d . to determine the optimal outcome for a given voting protocol  it is generally not necessary to elicit complete preferences from all voters  and some voters' preferences may not need to be elicited at all. selective preference elicitation increases privacy  and reduces the cost of voting  traveling to the voting site  spending time  etc. . again  what  if any  information should be elicited from an agent depends on what other agents have revealed about their preferences so far. 
　however  it turns out that effective vote elicitation gives rise to challenging computational problems. in the single transferable vote protocol  defined later   even knowing when enough has been elicited to determine the provably optimal outcome is np-complete  while this is easy for all other common voting protocols  defined later . even for these protocols  determining whose votes to elicit is np-complete  even with perfect suspicions about how the agents will vote.  the exception is the plurality protocol-the most common voting protocol. there  everyone votes for one candidate and the candidate with the largest number of votes wins. in that protocol  effective elicitation is easy.  if the elicitor's suspicions are imperfect  then effective elicitation can even be 
vspace-hard. 
　elicitation can also introduce additional opportunities for strategic manipulation by the voters because the elicitor's queries leak information among the voters. in coarse elicnation  where each voter's entire ranking of candidates is dles in combinatorial auctions. however  it cannot eliminate the need to evaluate at least some bundles to some extent. in this section i present a deeper look into an agent's evaluation problem with an explicit model of computation  and illustrate new strategic issues that stem from it. 
　in many markets  even computing one's valuation for a single bundle  or individual item  is complex. for example when bidding for trucking lanes  i.e.  tasks   this involves solving two np-complete local planning problems: the vehicle routing problem with the new lanes of the bundle and the problem without them  sandholm  1 . the difference in the costs of those two local plans is the cost  valuation  of taking on the new lanes. 
　however  in practice bidders  humans or their software agents  have limited computation and time  so they cannot exactly evaluate all  or even any  bundles-at least not without cost! 
　this leads to interesting incentive issues. for example  even in an auction where one object is being sold  should a bidder evaluate the object if there is a cost to doing so  according to traditional auction theory  truthful bidding is the dominant strategy in the celebrated vickrey auction where the object is given to the highest bidder at the price of the secondhighest bid  vickrey  1   this is the vcg mechanism  discussed earlier  applied to single-object auctions . however  it turns out that the vickrey auction loses its dominant-strategy property if the bidder has the option to evaluate the object or not  sandholm  1c . whether or not the bidder should pay the evaluation cost depends on the other bidders* valuations. 
　the issues run even deeper. if a bidder has the opportunity to approximate its valuation to different degrees  how much computing time should the bidder spend on refining its valuation  if there are multiple items for sale  how much computing time should the bidder allocate on different bundles  a bidder may even allocate some computing time to evaluate other bidders' valuations  e.g.  how much it would cost for a competing trucking company to take on a given set of lanes  so as to be able to bid more strategically; i call this strategic computing. 
　to answer these questions  we developed a deliberation control method called a performance profile tree for projecting how an anytime algorithm  a blackbox from the perspective of the deliberation controller  will change the valuation if additional computing is allocated toward refining  or improving  it  larson and sandholm  1c; 1b; 1a; 
computers and thought award paper 
1 . unlike earlier deliberation control methods for anytime algorithms  the performance profile tree is a fully normative model of bounded rationality: it takes into account all the information that an agent can use to make its deliberation control decisions.  this is necessary in the game-theoretic context; otherwise a self-interested agent could take into account some information that the model does not.  specifically  the projection of the anytime algorithm's performance is conditioned on the path of the run on the current problem instance  as well as static instance features. 
　using this deliberation control method  the auction can be modeled as a game  where computing actions are part of the game. at every point  each agent can decide on which bundle to allocate its next step of computing as a function of the agent's computing results so far  and in open-cry auction format also the others' bids observed so far . at every point  the agent can also decide to submit bids. one can then solve this model for the  bayesian  nash equilibrium  where each agent's  deliberation and bidding  strategy is a best-response to the others' strategies. i call this a deliberation equilibrium. table 1 shows in which settings strategic computing can and cannot occur in equilibrium. first  this depends on the auction mechanism.1 interestingly  this also depends on whether the agent has limited computing  such as a free desktop computer on which it can run until the auction's deadline   larson and sandholm  1b  or costly computing  such as being able to buy any amount of supercomputing time where each cycle comes at a cost   larson and sandholm  1c .1 
table 1: does strategic computing occur  the most interesting results are in bold as a benchmark from classical auction theory  the table also shows whether or not perfectly rational agents  that can determine their valuations instantly without cost  would benefit from considering each others' valuations when deciding how to bid 
1
the vickrey and vcg mechanisms were discussed earlier. 
the first-price auctions are sealed-bid auctions where the winning bidders pay their winning bid prices. the dutch auction is a descending-price auction where the first bidder gets the object at the current price. the english auction is an open-cry ascending auction where the highest bidder wins and pays the price of his bid. 
　　1 in these settings one can also determine how much such selfish computing hurts social welfare in the worst deliberation equilibrium  larson and sandholm  1 . 
1 using computational complexity as a barrier to strategic manipulation 
as the discussion of valuation computation in the previous section shows  agents' computational limitations can have adverse effects on the incentive properties of interaction mechanisms. this section demonstrates that the reverse can also be made to be true: one can use the fact that agents are computationally limited to achieve things that are not achievable via any mechanism among perfectly rational agents. in particular  i illustrate in a voting context that computational intractability can be used as a barrier to undesirable strategic behavior  thus circumventing a seminal economic impossibility result. 
　one key problem voting mechanisms are confronted with is that of manipulation by the voters. an agent is said to manipulate  vote strategically  when it does not rank the alternatives according to its true preferences  but rather so as to make the eventual outcome most favorable to itself. for example  if an agent prefers nader to gore to bush  but knows that nader has too few other supporters to win  while gore and bush are close to each other  the agent would be better off by declaring gore as its top candidate. manipulation is an undesirable phenomenon because collective choice schemes are tailored to aggregate preferences in a socially desirable way  and if the agents reveal their preferences insincerely  a socially undesirable candidate may be chosen. 
　the issue of strategic voting has been studied extensively. a seminal negative result  the gibbard-satterthwaite theorem  states that if there are three or more candidates  then in any nondictatorial voting scheme  there are candidate rankings of the other voters  and preferences of the agent under which the agent is better off voting strategically than sincerely  gibbard  1; satterthwaite  1 .  a voting scheme is called dictatorial if one of the voters dictates the outcome no matter how the others vote . so  a reasonable general nonmanipulable voting protocol does not exist! one approach around this impossibility is to construct desirable general nondictatorial voting protocols  under which manipulations exist by the impossibility theorem   but under which 
finding a beneficial manipulation is prohibitively hard computationally. 
　in order to discuss specific hardness results  i first review the most common protocols. in each protocol  each voter expresses his preferences as a linear order over candidates. the protocol then takes those expressions and imposes one of the candidates as the chosen outcome. in the protocols that are based on scores  the candidate with the highest score wins. in each of the listed protocols  even the ones that have multiple rounds   the voters submit their preferences up front. that is  the voters are not allowed to change their preference revelations during the execution of the protocol. 
  scoring protocols. let be a vector of integers such that for each voter  a candidate receives points if it is ranked first by the voter  if it is ranked second etc. the score of a candidate is the total number of points the candidate receives. the borda protocol is the scoring protocol with  the plurality pro-
tocol  aka. majority rule  is the scoring protocol with 
=  1 ..    1 . the veto protocol is the scoring protocol with =  1 ...  1 . 
  maximin  aka. simpson . for any two distinct candidates i and j  let n i  j  be the number of voters who prefer i to j. the maximin score of i is 
  single transferable vote  stv . the protocol proceeds through a series of c - 1 rounds. at each round  the candidate with the lowest plurality score  i.e.  the least number of voters ranking it first among the remaining candidates  is eliminated. the winner is the last remaining candidate. 
  plurality with run-off. in this protocol  a first round eliminates all candidates except the two with the highest plurality scores. then votes are transferred to these  as in the stv protocol . after that  a second round determines the winner among these two. 
  cup  sequential binary comparisons . the cup is defined by a balanced binary tree t with one leaf per candidate  and an assignment of candidates to leaves  each leaf gets one candidate . each non-leaf node is assigned the winner of the pairwise election of the node's children; the candidate assigned to the root wins. the cup protocol assumes that the assignment of candidates to leaves is known by the voters before they vote. in the randomized cup protocol  conitzer and sandholm  1b   the assignment of candidates to leaves is chosen uniformly at random after the voters have voted. 
　there are two natural alternative goals of manipulation. in constructive manipulation  the manipulator tries to find an order of candidates that he can reveal so that his favorite candidate wins. in destructive manipulation  the manipulator tries to find an order of candidates that he can reveal so that his hated candidate does not win. these are special cases of the utility-theoretic notion of improving one's utility  so the hardness results cany over to that setting. 
1 	complexity of manipulation when the number of voters and the number of candidates grows 
unfortunately  finding a constructive manipulation is in v for the plurality  boida  and maximin voting protocols  bartholdi et al  1   which are commonly used. the only voting protocol for which constructive manipulation is known to be np-hard is the stv protocol  bartholdi and orlin  1 .1 
　however  by slightly tweaking the voting protocols that are easy to manipulate  they can be changed into ones that are hard to manipulate. in particular  we revise them so 
l1
   it is np-hard also for the second order copeland protocol  bartholdi et al  1   but the baldness is driven solely by the tie-breaking rule. 
1 
that before the original protocol is executed  one pairwise elimination round is executed among the candidates  and only the winning candidates survive to the original protocol  so  about half of the candidates are eliminated in the preround . this makes the protocols afp-hard  #p-hard  or pspace-hard to manipulate constructively  depending on whether the schedule of the preround is determined before the votes are collected  after the votes are collected  or the scheduling and the vote collecting are carefully interleaved  respectively  conitzer and sandholm  1e . we proved general sufficient conditions on voting protocols for this tweak to introduce the hardness  and showed that the plurality  boida  maximin  and stv protocols satisfy those conditions. so  these commonly used voting protocols can be made hard to manipulate by simply using one elimination round. 
1 complexity of manipulation when the number of candidates is constant 
all of the hardness results discussed above rely on both the number of voters and the number of candidates growing. the number of candidates can be large in some domains  for example when voting over task or resource allocations  but in many elections-such as presidential elections-the number of candidates is small. if the number of candidates is a constant  both constructive and destructive manipulation are in v  regardless of the number of voters  conitzer and sandholm  1b . this holds even if the voters are weighted  or if a coalition of voters tries to manipulate  but not both. when a coalition of weighted voters tries to manipulate  complexity can arise even for a constant number of candidates  as summarized in tables 1 and 1  conitzer and sandholm  1b; conitzer et al  1 . one lesson is that randomizing over instantiations of the mechanisms  such as schedules of a cup  can be used to make manipulation hard. 

table 1: complexity of constructive weighted coalitional manipulation. 

table 1: complexity of destructive weighted coalitional manipulation. 
computers and thought award paper 
　all of the hardness results discussed above hold even if the manipulators know the nonmanipulators' votes exactly. under weak assumptions  if weighted coalitional manipulation with complete information about the others' votes is hard in some voting protocol  then individual and unweighted manipulation is hard when there is uncertainty about the others' votes  conitzer and sandholm  1b . 
　computation not only serves as a means to circumventing incentive problems as dicussed above  but it can also serve as the means for designing appropriate incentives as discussed in the next section. 
1 automated mechanism design 
the aggregation of conflicting preferences for choosing an outcome is a central problem in multiagent systems  be the agents humans or software. the key difficulty is that the agents may report their preferences insincerely  i.e.  manipulate  as we just discussed in a voting setting . mechanism design is the art of designing the rules of the game so that the agents are motivated to report their preferences truthfully and a desirable outcome is chosen.1 the desirability objective can be  for example  social welfare  seller's revenue  fairness  or some tradeoff among these. 
　mechanism design has traditionally been a manual endeavor. the designer uses experience and intuition to hypothesize that a certain rule set is desirable in some ways  and then tries to prove that this is the case. alternatively  the designer formulates the mechanism design problem mathematically and characterizes desirable mechanisms analytically in that framework. these approaches have yielded a small number of canonical mechanisms over the last 1 years  each of which is designed for a class of settings and a specific objective. for example  the vcg and dagva  d'aspremont and g1rard-varet  1; arrow  1  maximize social welfare among the agents in the class of settings where the agents have quasilinear utility functions. mechanism design research has also yielded impossibility results that state that no mechanism works across a class of settings  for varying definitions of  works  and varying classes . for example  the gibbard-satterthwaite theorem discussed in the previous section states that for the class of general preferences  no mechanism works in the sense that 1  the mechanism's outcome can be any one of at least three candidates  1  the mechanism is nondictatorial  and 1  every agent's dominant strategy is to reveal his preferences truthfully. 
　in sharp contrast to manual mechanism design  i envision a systematic approach where the mechanism is automatically created for the setting and objective at hand. this has several advantages. first  it can be used even in settings that do not satisfy the assumptions of the classical mechanisms. second  it may allow one to circumvent the impossibility results: when the mechanism is designed to the setting  instance  at hand  it does not matter that it would not work on preferences beyond those in that setting {e.g.  for a class of settings . 
   a central result in game theory  the revelation principle  allows the designer to restrict attention to such truthful mechanisms without loss in the objective  mas-coleli et al.  1 . 
even when the optimal mechanism-created automatically- does not circumvent the impossibility  it always minimizes the pain entailed by impossibility. third  it may yield better mechanisms  in terms of stronger nonmanipulability guarantees and/or better outcomes  than the canonical mechanisms because the mechanism capitalizes on the particulars of the setting  the probabilistic information that the mechanism designer has about the agents' preferences . given the vast amount of information that parties have about each other today  it is astonishing that the canonical mechanisms  such as first-price reverse auctions   which largely ignore that information  have prevailed thus far. i foresee an imminent revolution  where future mechanisms will be created automatically. for example  imagine a fortune 1 company automatically creating its procurement mechanism based on its statistical knowledge about its suppliers  and potentially also the public 
prices of the suppliers* inputs  etc. . i call this vision automated mechanism design  conitzer and sandholm  1c .1 
1 the computational problem 
as a first step toward fulfilling this vision  we modeled mechanism design as an optimization problem  and studied its complexity. in the model  each agent can have any one of a finite number of utility functions. an agent's utility function is private information. the mechanism designer has a prior probability distribution over each agent's possible utility functions. the first constraint to the problem  the incentive compatibility constraint  is that each agent has to be motivated to reveal its utility function truthfully regardless of what utility function the agent has. this comes in two variants. in the first  called dominant strategy implementation   the agent has to be no worse off by revealing his true utility function regardless of what utility functions the other agents reveal. in the second  the agent has to be no worse off  in expectation  by revealing his true utility function.  the expectation is taken as a weighted average over the possible truthful utility function revelations of the other agents . the second constraint to the problem  the participation constraint  is that each agent has to be no worse off by participating in the mechanism than not participating  otherwise a rational agent would not participate . this again comes in two variants. in the first  the agent has to be no worse off regardless of what utility functions the other agents reveal. in the second  the agent has to be no worse off in expectation. the input to the optimization also includes the designer's objective. the output is a mapping from utility function revelations to outcomes  or in the case of randomized mechanisms  to probability distributions over outcomes . 
　in settings without side payments  such as voting  designing an optimal  e.g.  expected social welfare maximizing  deterministic mechanism is np-complete  conitzer and sandholm  1c .1 if side payments are allowed  designing a 
1 note that automated mechanism design is completely different from so called algorithmic mechanism design  nisan and ronen  1 . in the latter  the mechanism is designed manually with the goal that executing the mechanism is computationally tractable. on the other hand  in automated mechanism design  the mechanism itself is designed automatically. 
1 this actually holds for any solution concept from noncoopera-

deterministic mechanism is easy if the designer's objective is social welfare  but afp-complete more generally  for example  if the objective is to maximize the expected revenue collected from the bidders-as is the objective in some auctions   conitzer and sandholm  1b . interestingly  if one allows randomized mechanisms  the mechanism design problem becomes solvable in polynomial time using linear programming.1 in other words  the designer can tackle the computational complexity  introduced by its uncertainty about the agents  by making the agents face additional uncertainty. this comes at no loss  and in some cases at a gain  in the mechanism designer's objective. 
　if the agents' utility functions are additively decomposable into independent issues  the input to automated mechanism design can be represented  potentially exponentially  more concisely. in that representation it is np-complete  even under strong restrictions  to design a mechanism that maximizes one of the following objectives: 1  expected social welfare when payments are not possible  1  a general objective function even when payments are possible  and 1  expected revenue collected from the agents  conitzer and sandholm  1c . again  a randomized mechanism can be designed in polynomial time. so  the complexity as a function of the input length is the same in the concise representation as it is in the flat representation. in other words  due to its potentially exponentially shorter input length  the structured representation allows potentially exponentially faster automated mechanism design. 
1 	applications 
in initial experiments  automated mechanism design produced the following highlights  conitzer and sandholm  1a : 
  it reinvented the celebrated myerson auction  myerson  1   which maximizes the seller's expected revenue in a 1-object auction. 
  it created expected revenue maximizing combinatorial auctions. this has been a long-standing recognized open research problem in  manual  mechanism design  avery and hendershott  1; vohra  1 . the general form for such an auction is still unknown  but automated mechanism design created prior-specific optimal mechanisms.  in the manual mechanism design literature  even the problem with only two objects for sale is open; only a case with very special form of complementarity and no substitutability has been solved  armstrong  1 .  
  it created optimal mechanisms for divorce settlements  both with a benevolent arbitrator that tries to maximize the sum of the divorcees' utilities  with and without side payments   and an arbitrator that tries to maximize revenues collected from the divorcees. 
  it created optimal mechanisms for a public good prob-lem  deciding whether or not to build a bridge . the 
tive game theory  conitzer and sandholm  1a   not just the ones discussed through the constraints above. all of the hardness results discussed in this section hold even with just 1 agents. 
   1 this holds for any mechanism design objective that is linear in the outcome probabilities. 
vcg mechanism could be used in this setting as long as each agent's utility function is quasilinear. however  in the vcg mechanism  nonnegative payments arc collected from the voters  intuitively  the payments are collected in order to avoid the free rider problem   and those payments have to be burned. according to a seminal impossibility result  this problem plagues any mechanism that applies to general quasilinear utility functions  yields a social welfare maximizing decision  and makes truthful reporting of utility functions a dominant strategy  green and laffont  1 . the automated mechanism design approach allowed us to incorporate money burning as a loss in the social welfare objective  and maximize that revised objective. we had automated mechanism design create an optimal mechanism for the bridge building scenario under each variant of the incentive compatibility constraint discussed above  with the deterministic participation constraint . in neither variant was money ever burned. in the probabilistic variant  the bridge was always built if and only if that was best for the agents.  in the deterministic variant this was not always the case.  for the probabilistic variant of incentive compatibility  the general-purpose dagva mechanism could be used to yield the social welfare maximizing choice without burning money  d'aspremont and gerard-varet  1; arrow  1 . however  a seminal economic impossibility result shows that no mechanism for general quasilinear utility functions yields the social welfare maximizing choice  maintains budget balance  and satisfies the participation constraint  even the probabilistic variant   myerson and satterthwaite  1 . as the experiment above showed  automated mechanism design can circumvent this impossibility. it constructed a mechanism that satisfies all these desiderata  and actually the deterministic  i.e.  stronger  variant of the participation constraint. 
  it created optimal mechanisms for public goods problems with multiple goods. this is the public goods analog of combinatorial auctions. 
1 safe exchange mechanism design 
mechanism design is not only needed for deciding on an outcome among agents. it is also key for executing the outcome. for example  if the outcome is a joint plan  how should it be executed so that no agent is motivated to deviate along the way  braynov  1; braynov and sandholm  1   from the perspective of markets  an important type of joint plan is the plan for exchanging items and payments between parties. nondelivery is a major problem in exchanges  especially in electronic commerce: the supplier might not deliver the goods or the demander might not pay. a recent study shows that 1% of consumers with on-line shopping experience reported products or services that were paid for  but never received  national consumers league  1 . 
1 deal chunking 
in some settings  mechanism design can be used to enable safe exchanges without legal enforcement or escrow companies. one such approach is our methodology where the exchange is split into chunks which the agents deliver in alternation  sandholm  1; 1 .1 the mechanism is practical when such splitting incurs little cost  as is the case with digital goods  computation time  many web services  and many investment instruments  for instance. eexchangehouse  a safe exchange planner prototype  automatically determines a safe exchange plan for the exchange setting at hand such that neither party has incentive to vanish before completing the exchange  sandholm and ferrandon  1 . only some ways of splitting the exchange into chunks and some sequences of delivering the chunks are safe in this sense  sandholm  1; sandholm and lesser  1a . the planner's algorithms for chunking and chunk sequencing provably find a safe exchange plan if one exists  and determine the shortest safe plan. the algorithms  as well as the amount of input that is solicited from the users  vary based on whether the exchanged items and units of each item are dependent or independent in terms of their value to the exchange parties. 
1 a general model for safe exchange 
in order to more broadly study the possibilities of using mechanism design to enable safe exchange  we developed a unified model of exchange mechanisms  sandholm and wang  1 . a key idea behind the model is that at any point of the exchange  each agent has a  potentially empty  set of items that he possesses  and a  potentially empty  allocation set that includes the items that he can reallocate-except not to himself-or destroy. the two sets need not be the same. an item can simultaneously be in one agent's possession set and in another agent's allocation set. other aspects of the model include transfer costs and defection costs  how much of a reputation loss or risk would an agent face if he defaulted before completing the exchange . 
　the model captures the disparate earlier safe exchange approaches such as cryptographic coin ripping  jakobsson  1   digital signatures  and our game-theoretic chunking mechanism discussed above. it also allows one to creatively and systematically think about  and analyze  novel exchange mechanisms. for example  our reputation locking mechanism stemmed from this model. it works as follows: 1  agent a allows agent b to encrypt as reputation in the public database 
 e.g.  ebay   1  agent b delivers to a  1  agent a delivers to b  and 1  b decrypts as reputation back into plaintext. in this mechanism  as reputation is used as a virtual item that is is temporarily transferred into b's allocation set if a does not deliver  b does not give back the reputation. 
　being an overarching framework  the model also allows one to study what is inherently possible and impossible in safe exchange  with and without a trusted third party  and with an offline third party that only gets involved if the exchange fails . 
1 conclusions and perspective 
collective choice settings are ubiquitous and important  whether the agents are humans or software. game theory 
1 similar protocols have recently been studied by others  
e.g.   matsubara and yokoo  1 . 
provides a basis for engineering the incentives into the interaction mechanism so that a desirable outcome is chosen even though every agent acts based on self-interest however  a host of computer science issues not traditionally addressed in game theory have to be addressed in order to make mechanisms work in the real world. those computing  communication  and privacy issues are deeply intertwined with the economic incentive issues  as this writeup has illustrated. 
　here  i would like to draw some high-level conclusions from the results presented above. peer-to-peer negotiation suffers from negotiation process uncertainties that can be eliminated by using a mediator  such as an auction server  that collects the agents' private information and runs a clearing algorithm on that data to determine the outcome.1 domain uncertainty remains even in this approach  and leveled commitment contracts can be used to mitigate it to the economic benefit of all contract parties 1 expressive competition is a new form of interaction that empowers market participants with potent expressiveness akin to human-to-human negotiation  this has economic advantages and makes bidding easier  while at the same time harnessing the forces of competition  the global scale of the internet  and the accuracy of market clearing with all relevant information in hand. the mediated approach with optimal clearing is feasible at a grand scale even with expressive competition  despite the fact that most variants of the clearing problem are hard and inapproximable in the worst case. the economic and computational efficiencies that mediated expressive competition offers suggest that in the future  marketplaces will merge into larger ones. to a certain extent this trend is already underway. for example  in the last couple of years large corporations have undergone a massive transition from plant-based procurement to global procurement. 
　the mediated approach can be made to require dramatically less information from the agents-especially in combinatorial markets-by using selective incremental multiagent preference elicitation. this decreases the agents' valuation determination costs  and other preference determination efforts  and communication costs. it also enhances privacy. it can be made into an incentive-compatible push-pull mechanism where the information revelation is guided by both the elicitor  auctioneer  and each agent. this makes sense be-
 1
   the clearing algorithm-usually a specialized tree search-can run on multiple machines. this is usually the case in large-scale applications  but the machines are usually co-located rather than at each agent's location. also  there is no notion of one machine representing one agent. so  the distribution that is motivated by computational efficiency does not correspond to the distribution of agents  that is  the distribution of self-interest and private information . 
   1 leveled commitment can also mitigate the negotiation process uncertainty that arises if an agent participates in negotiations mediated by different mediators  and the negotiations* outcomes are not independent from the perspective of the agent's valuation. in that sense  leveled commitment can serve as part of the  glue  needed between marketplaces. furthermore  leveled commitment can be used to mitigate the uncertainty that arises if one mediator runs multiple clearings over time  for example at fixed intervals or every time a new bid arrives.  for online algorithms for market clearing  see  blum et al  1 .  
cause each agent has private information that suggests what should be revealed by the agent  and the auctioneer accrues information about the other agents that affects what informa-
tion from the agent is pertinent. 
　a deeper look into an agent's evaluation problem shows that valuation determination costs ruin the incentives of classical auction mechanisms  and give rise to a new phenomenon i call strategic computing: using one's limited computing to approximate others' preferences at the cost of approximating one's own. whether strategic computing occurs depends on the auction mechanism and the type of computational constraint  costly computing vs. limited computing . 
　while computational constraints can cause strategic problems  the reverse can also be made to be the case. computational hardness can be used as the barrier to manipulation. this is especialy desirable in settings where economic mechanism design  incentive engineering  is known to fall short. 
　computing can also be used to automatically design the mechanism  for instance the rules of a divorce settlement  auction  or public goods problem. because the mechanism is designed for the specific setting at hand  objective and information about the agents   it often yields a better mechanism than the ones known to date. it can also circumvent seminal impossibility results. 
　carefully designed mechanisms are needed not only for choosing an outcome  but also for executing the outcome  such as a joint plan. i illustrated this in the context of designing safe exchange mechanisms for anonymous parties in internet commerce. 
1 future research directions 
this is a fertile and relatively new area where ai and computer science at large can have enormous theoretical and practical impact. work on expressive competition includes the ongoing quest for faster clearing algorithms and the pursuit of concise-perhaps application-specific-forms of expressive bidding that are easy to use in bidding and elicitation  and fast to process by a clearing algorithm. automated mechanism design is still in its infancy  and holds significant promise for future research. it should be tested on new classes of problems  and applied to real-world settings. it could also be applied to design safe exchange mechanisms  using the general safe exchange model that we introduced -an endeavor that has traditionally taken considerable creativity  yet has not delivered mechanisms  or impossibility results  for all settings. known game-theoretic results that characterize properties of mechanisms could be used to reduce the search space in automated mechanism design. furthermore  automated mechanism design could be used on a variety of settings  real or artificially generated  to see whether new canonical mechanisms and mechanism design principles can be inferred. 
　another significant research stream arises from the observation that mechanism design should take into account the agents' computational constraints. our performance profile tree based deliberation control method together with the idea of deliberation equilibrium provide a normative model of bounded rationality in multiagent systems  which is needed to determine how computationally constrained self-interested agents would behave in a given mechanism. this allows one to evaluate mechanisms for computationally constrained agents  and hopefully paves the way to designing such mechanisms. not only auction and voting mechanisms  but also multiagent preference elicitation mechanisms should be designed with this methodology. this methodology could also be used to design mechanisms that are computationally hard to manipulate  where hardness is measured not in terms of worst-case complexity  but informed by game-theoretic deliberation control.  other approaches to improving upon worstcase measures include designing mechanisms where manipulation is average-case hard  or even hard on every-carefully constructed-instance such as in factoring.  this methodology could even yield new mechanism design principles. as discussed  the central design principle in mechanism design  the revelation principle  ceases to meaningfully hold under computational or communication constraints. in such settings it can be better to use multi-stage mechanisms such as preference eliciation  unlike the principle suggests. also  in such settings it has been theoretically demonstrated that there is some benefit to allowing for mechanisms where insincere preference revelation occurs  conitzer and sandholm  1d   unlike the principle suggests. is there a significant practical benefit to be gained from such mechanisms  what would such mechanisms look like  are there principles for constructing them  
　finally  are there unforeseen novel ways-beyond enabling expressive competition  multiagent preference elicitation  and automated mechanism design-of using computing to enhance collective choice  
references 
 andersson and sandholm  1  martin andersson and tuomas sandholm. leveled commitment contracting among myopic individually rational agents. in proceedings of the third international conference on multi-agent systems  icmas   pages 1  paris  france  july 1. 
 andersson and sandholm  1  martin andersson and tuomas sandholm. time-quality tradeoffs in reallocative negotiation with combinatorial contract types. in proceedings of the national conference on artificial intelligence  aaai   pages 1  orlando  fl  1. 
 andersson and sandholm  1  martin andersson and tuomas sandholm. contract type sequencing for reallocative negotiation. in proceedings of the twentieth international conference on distributed computing systems  taipei  taiwan  april 1. 
 andersson and sandholm  1  martin andersson and tuomas sandholm. leveled commitment contracts with myopic and strategic agents. journal of economic dynamics and control  1-1  1. special issue on agent-based computational economics. early version: national conference on artificial intelligence  aaai   p. 1  madison  wi  1. 
　the fourth international conference on multi-agent sys- combinenet  inc.  1c  combinenet  inc. combinenet tems  icmas   pages 1  boston  ma  1. provides essential optimization to enhance sourcing for  armstrong  1  mark armstrong. optimal multi-object global food manufacturer  1. case study white paper auctions. review of economic studies  1-1. at www.combinenet.com/customers/casejiistories/ seafreight consumerpackagegoodsco.pdf.  arrow  1  kenneth arrow. the property rights doctrine and demand revelation under incomplete information. in  combinenet  inc.  1d  combinenet  inc. leading m boskin  editor  economics and human welfare. new auto manufacturer enhances sea freight allocations 
	york academic press  1. 	with combinenet  1. 	case study white paper 
　　　　　　　　　　　　　　　　　　　　　　　　　　at 	www.combinenet.com/customers/casejiistories/  aveiy and hendershott  1  christopher avery and terrence hendershott. bundling and optimal auctions of mul-	seafreight.big1automotiveco.pdf. 
tiple products. review of economic studies  1-1   combinenet  inc.  1e  combinenet  inc. major indus-
	1. 	trial manufacturer identifies best strategy for optimizing 
 bartholdi and orlin  1  john j. bartholdi  iii and sourcing with combinenet  1. case study white paper james b. orlin. single transferable vote resists strategic at www.combinenet.com/customers/casejiistories/ voting. social choice and welfare  1 :1  1. directmaterialsjndustrialmfgco.pdf . 
 bartholdi et al  1  john j. bartholdi  ui  craig a. 	 conen and sandholm  1  wolfram conen and tiiomas  andersson et al  1  arne andersson  mattias tenhunen  and fredrik ygge. integer programming for combinatorial auction winner determination. in proceedings of tovey  and michael a. trick. the computational difficulty of manipulating an election. social choice and welfare  1 :1 1. 
 bikhchandani et al.  1  sushil bikhchandani  sven de vries  james schummer  and rakesh v. vohra. linear programming and vickrey auctions  1. draft. 
 blum et al.  1  avrim blum  thomas sandholm  and sandholm. preference elicitation in combinatorial auctions: extended abstract. in proceedings of the acm con-
ference on electronic commerce  acm-ec   pages 1  tampa  fl  october 1. a more detailed description of the algorithmic aspects appeared in the ucai-
	martin zinkevitch. online algorithms for market clearing. 	 conen and sandholm  1a  wolfram conen and tuomas 
in annual acm-siam symposium on discrete algorithms sandholm. differential-revelation vcg mechanisms for 
	 soda   pages 1  san francisco  1. 	combinatorial auctions. in aamas-1 workshop on agent-
　　　　　　　　　　　　　　　　　　　　　　　　　　mediated electronic commerce  amec   bologna  italy   blum et al  1  avrim blum  jeffrey jackson  tuomas 1. sandholm  and martin zinkevich. preference elicitation 
	and query learning  1. 	 conen and sandholm  1b  wolfram conen and tuomas 
sandholm. partial-revelation vcg mechanism for com-
 boutilier  1  craig boutilier. solving concisely ex- binatorial auctions. in proceedings of the national conpressed combinatorial auction problems. in proceedings of ference on artificial intelligence  aaai   pages 1  the national conference on artificial intelligence  aaai   edmonton  canada  1. 1 workshop on economic agents  models  and mechanisms  pp. 1. 

pages 1  edmonton  canada  1. 
 braynov and sandholm  1  sviatoslav braynov and tuomas sandholm. power  dependence and stability in multiagent plans. in proceedings of the national con-
ference on artificial intelligence  aaai   pages 1  orlando  fl  july 1. 
 braynov  1  sviatoslav braynov. deviation-proof plans in open multiagent environments. in proceedings of the  conitzer and sandholm  1a  vincent conitzer and tuomas sandholm. automated mechanism design: complexity results stemming from the single-agent setting  1. draft. 
 conitzer and sandholm  1b  vincent conitzer and tuomas sandholm. complexity of manipulating elections with few candidates. in proceedings of the national con-

1th european conference on artificial intelligence  pages ference on artificial intelligence  aaai   pages 1  
	1  august 1. 	edmonton  canada  1. 
 clarke  1  e h clarke. multipart pricing of public 	 conitzer and sandholm  1c  vincent conitzer and tuogoods. public choice  1-1. 	mas sandholm. complexity of mechanism design. in proceedings of the 1th annual conference on uncertainty in  combinenet  inc.  1a  combinenet  inc. 	com-	artificial intelligence  uai-1   pages 1  edmonbinenet improves negotiations  streamlines sea freight 	ton  canada  1. allocations  	for international technology and re-
search corporation  1. case study white paper at  conitzer and sandholm  1d  vincent conitzer and 'riowww.combinenet.com/customers/casejiistories/ mas sandholm. vote elicitation; complexity and strategyseafreight fortunel1chemicalco.pdf. proofness. in proceedings of the national conference on 
artificial intelligence  aaai   pages 1  edmonton  
 combinenet  inc.  1b  combinenet  inc. combinenet 
　　　　　　　　　　　　　　　　　　　　　　　　　canada  1. interoperability enhances sourcing systems for international food manufacturer  1. case study white paper at 	 conitzer and sandholm  1a  vincent conitzer and tuowww.combinenet.com/customers/case-histories/ in-	mas sandholm. applications of automated mechanism determodaljconsumerpackagegoodsco.pdf. 	sign  1. 
 conitzer and sandholm  1b  vincent conitzer and tuo-  gonen and lehmann  1  rica gonen and daniel mas sandholm. automated mechanism design for a self- lehmann. optimal solutions for multi-unit combinatorial 
interested designer. in proceedings of the acm confer-	auctions: branch and bound heuristics. in proceedings of ence on electronic commerce  acm-ec   san diego  ca  the acm conference on electronic commerce  acm-ec   

1. poster paper. 
 conitzer and sandholm  1c  vincent conitzer and tuomas sandholm. automated mechanism design with a structured outcome space  1. 
 conitzer and sandholm  1d  vincent conitzer and thomas sandholm. computational criticisms of the revelation 
principle  1. draft. 
 conitzer and sandholm  1e  vincent conitzer and tuomas sandholm. universal voting protocol tweaks to make manipulation hard. in proceedings of the eighteenth inpages 1  minneapolis  mn  october 1. 
 green and laffont  1  j green and j-j laffont. incentives in public decision making. amsterdam: northholland  1. 
 groves  1  theodore groves. incentives in teams. econometrica  1-1. 
 hoos and boutilier  1  holger hoos and craig boutilier. bidding languages for combinatorial auctions. in proceedings of the seventeenth international joint conference on 
artificial intelligence  ijcai   pages 1  seattle  

ternational joint conference on artificial intelligence  ij- wa 1. cai   acapulco  mexico  1. 
 hudson and sandholm  1  benoit hudson and tuomas 
 conitzer et a/.  1  vincent conitzer  jerome lang  and thomas sandholm. how many candidates are needed to make elections hard to manipulate  in theoretical aspects of rationality and knowledge  tark ix   bloomington  indiana  usa  1. 
 conry et al.  1  susan e conry  k kuwabara  victor r lesser  and r a meyer. multistage negotiation for dis-sandholm. effectiveness of preference elicitation in combinatorial auctions. in aamas-1 workshop on agentmediated electronic commerce  amec   bologna  italy  1. extended version: carnegie mellon university  computer science department  cmu-cs-1  march. also: stanford institute for theoretical economics workshop  site-1 . tributed satisfaction. ieee transactions on systems  man   hudson and sandholm  1a  benoit hudson and tuomas 
and cybernetics  1 : 1 1. 
 d'aspremont and gerard-varet  1  c d'aspremont and l a gerard-varet. incentives and incomplete information. journal of public economics  1-1. 
 demartini et al.  1  c demaitini  a kwasnica  j ledyard  and d porter. a new and improved design for multiobject iterative auctions. technical report 1  california institute of technology  social science  september 1. 
 ephrati and rosenschein  1  eithan ephrati and jeffrey s rosenschein. the clarke tax as a consensus mech-sandholm. generalizing preference elicitation in combinatorial auctions. in international conference on autonomous agents and multi-agent systems  melbourne  
australia  1.  poster presentation. . 
 hudson and sandholm  1b  benoit hudson and tuomas sandholm. using value queries in combinatorial auctions. 
in proceedings of the acm conference on electronic commerce  acm-ec   san diego  ca  1.  poster presentation. . 
 jakobsson  1  m jakobsson. ripping coins for fair exchange. in eurocrypt  pages 1  seattle  wa  	anism among automated agents. in proceedings of the na-	1. 
	tional conference on artificial intelligence  aaai   pages  kalagnanam et al. 1  jayant kalagnanam  	andrew 
	1  anaheim  ca  1. 	davenport  and ho lee. 	computational aspects of 
 ephrati and rosenschein  1  eithan ephrati and jef- clearing continuous call double auctions with assignment frey s rosenschein. multi-agent planning as a dynamic constraints and indivisible demand. electronic commerce search for social consensus. in proceedings of the thir- research journal  1   1. teenth international joint conference on artificial intelli- kothariera/. 1  anshul kothari  tuomas sandholm  
gence  ijcai   pages 1  chambery  france  1. 
 ephrati  1  eithan ephrati. a non-manipulable meeting scheduling system. in proc. 1th international distributed artificial intelligence workshop  lake quinalt  washington  july 1. aaai press technical report ws-1. and subhash suri solving combinatorial exchanges: optimality via few partial bids. in proceedings of the 
acm conference on electronic commerce  acm-ec   san diego  ca  1. poster presentation. early version was accepted to the aaai-1 workshop on artificial intelligence for business.  fujishima et al.  1  yuzo fujishima  kevin leyton-
brown  and yoav shoham. tuning the computational  kuhn  1  harold w kuhn. the hungarian method for complexity of combinatorial auctions: optimal and ap- the assignment problem. naval research logistics quarproximate approaches. in proceedings of the sixteenth in- terly  1-1. 
ternational joint conference on artificial intelligence  ij- larson and sandholm  1a  kate larson and tuomas cai   pages 1  stockholm  sweden  august 1. sandholm. bargaining with limited computation: deliber-
 gibbard  1  a gibbard. manipulation of voting ation equilibrium. artificial intelligence  1 : 1  schemes. econometrica  1-1. 1. short early version appeared in the proceedings of 

the national conference on artificial intelligence  aaai    national consumers league  1  national consumers pp. 1  austin  tx  1. league. new ncl survey shows consumers are both 
 larson and sandholm  1b  kate larson and tuomas 	excited and confused about shopping online  1. 
sandholm. computationally limited agents in auctions. in www.natlconsumersleague.org/ beewisepr.html  oct. 1. agents-1 workshop of agents for b1b  pages 1  survey conducted by opinion research corporation. 
	montreal  canada  may 1. 	 nisan and ronen  1  noam nisan and amir ronen. 
 larson and sandholm  1c  kate larson and tuomas computationally feasible vcg mechanisms. in proceedsandholm. costly valuation computation in auctions. in ings of the acm conference on electronic commerce theoretical aspects of rationality and knowledge  tark  acm-ec   pages 1  minneapolis  mn  1. 
	viii   pages 1  sienna  italy  july 1. 	 nisan and ronen  1  noam nisan and amir ronen. al-
 larson and sandholm  1  kate larson and tuomas gorithmic mechanism design. games and economic besandholm. an alternating offers bargaining model for havior  1-1. early version in stoc-1. 
computationally limited agents. in international con-  nisan and segal  1  noam nisan and ilya segal. the ference on autonomous agents and multi-agent systems  communication requirements of efficient allocations and bologna  italy  july 1. supporting lindahl prices  1. working paper  version: 
 larson and sandholm  1  kate larson and tuomas 	march 1 . 
sandholm. miscomputing ratio: the social cost of selfish 
　　　　　　　　　　　　　　　　　　　　　　　　 nisan  1  noam nisan. bidding and allocation in comcomputing. in international conference on autonomous binatorial auctions. in proceedings of the acm confer-
agents and multi-agent systems  melbourne  australia  ence on electronic commerce  acm-ec   pages 1  
1. early version appeared in the aaai-1 workshop on game-theoretic and decision-theoretic agents  edmonton  canada. 
 lehmann et al. 1  daniel 	lehmann  	lidian 	ita o'callaghan  and yoav shoham. truth revelation in rapid  approximately efficient combinatorial auctions. journal of the acm  1 :1  1. 
 leyton-brown et al.  1a  kevin leyton-brown  mark pearson  and yoav shoham. towards a universal test suite for combinatorial auction algorithms. in proceedings of minneapolis  mn  1. 
 parkes and ungar  1  david c parkes and lyle ungar. iterative combinatorial auctions: theory and practice. in proceedings of the national conference on artificial intelligence  aaai   pages 1  austin  tx  august 1. 
 rassenti et al.  1  s j rassenti  v l smith  and r l bulfin. a combinatorial auction mechanism for airport time slot allocation. bell j. of economics  1-1  
1. the acm conference on electronic commerce  acm-ec    rothkopf et al  1  michael h rothkopf  aleksandar 
	pages 1  minneapolis  mn  1. 	pekec  and ronald m harstad. computationally man-
 leyton-brown et al  1b  kevin leyton-brown  moshe ageable combinatorial auctions. management science  tennenholtz  and yoav shoham. an algorithm for 1 :1 1. 
multi-unit combinatorial auctions. in proceedings of  sandholm and ferrandon  1  tuomas sandholm and the national conference on artificial intelligence  aaai   vincent ferrandon. safe exchange planner. in pro-
	austin  tx  august 1. 	ceedings of the fourth international conference on 
 mas-colell et al.  1  andreu mas-colell  michael multi-agent systems  icmas   pages 1  boston  whinston  and jerry r. green. microeconomic theory. ma  july 1. 
oxford university press  1. 
 matsubara and yokoo  1  shigeo matsubara and makoto yokoo. defection-free exchange mechanisms for information goods. in proceedings of the fourth inter-
national conference on multi-agent systems  icmas   pages 1  boston  ma  july 1. 
 mu'alem and nisan  1  ahuva mu'alem and noam nisan. truthful approximate mechanisms for restricted combinatorial auctions. in proceedings of the national conference on artificial intelligence  aaai   pages 1  edmonton  canada  july 1. 
 myerson and satterthwaite  1  roger myerson and mark satterthwaite. efficient mechanisms for bilateral exchange. journal of economic theory  1-1  1. 
 myerson  1  roger b myerson. optimal auction design. 
mathematics of operation research  1-1.  sandholm and lesser  1a  tuomas sandholm and victor r lesser. equilibrium analysis of the possibilities of 
unenforced exchange in multiagent systems. in proceedings of the fourteenth international joint conference on 
artificial intelligence  ijcai   pages 1  montreal  canada  august 1. 
 sandholm and lesser  1b  tuomas sandholm and victor r lesser. issues in automated negotiation and elec-
tronic commerce: extending the contract net framework. in proceedings of the first international conference on multi-agent systems  icmas   pages 1  san francisco  ca  june 1. reprinted in readings in agents  huhns and singh  eds.  pp. 1 1. 
 sandholm and lesser  1  tuomas sandholm and victor r lesser. leveled commitment contracts and strategic breach. games and economic behavior  1-1  1. special issue on al and economics. early versions 1 

appeared as advantages of a leveled commitment contracting protocol in the proceedings of the national conference on artificial intelligence  aaai   pp. 1  portland  or  1  and as a university of massachusetts at amherst  computer science department technical report 1. 
 sandholm and lesser  1  tuomas sandholm and victor lesser. leveled commitment contracting: a backtracking instrument for multiagent systems. ai magazine  1 :1 1. 
 sandholm and suri  1a  tuomas sandholm and subhash suri. market clearability. in proceedings of the seventeenth international joint conference on artificial intelligence  ijcai   pages 1  seattle  wa  1. 
 sandholm and suri  1b  tuomas sandholm and subhash suri. side constraints and non-price attributes in markets. in ijcai-1 workshop on distributed constraint reasoning  pages 1  seattle  wa  1. 
 sandholm and suri  1  tuomas sandholm and subhash suri. optimal clearing of supply/demand curves. in 1th annual international symposium on algorithms and computation  isaac   vancouver  canada  november 1. also appeared in the proceedings of the aaai-1 workshop on agent-based technologies for b1b electronic commerce  aaai technical report ws-1   pp. 1  edmonton  canada. 
 sandholm and suri  1  tuomas sandholm and subhash suri. bob: improved winner determination in combinatorial auctions and generalizations. artificial intelligence  1-1. early version: improved algorithms for optimal winner determination in combinatorial auctions and generalizations. national conference on artificial intelligence  aaai-1   pp. 1  austin  tx  july 1 august 1. 
sandholm and wang  1  tuomas sandholm and xiaofeng wang.  im possibility of safe exchange mechanism design. in proceedings of the national conference on artificial intelligence  aaai   pages 1  edmonton  canada  1. 
sandholm and zhou  1  tuomas sandholm and yunhong zhou. surplus equivalence of leveled commitment contracts. artificial intelligence  1-1  1. early versions appeared in 1cmas-1 and in the aaai-1 workshop on negotiation: settling conflicts and identifying opportunities. 
sandholm et al  1  tuomas sandholm  sandeep sikka  and samphel norden. algorithms for optimizing leveled commitment contracts. in proceedings of the sixteenth international joint conference on artificial intelligence  ijcai   pages 1  stockholm  sweden  1. extended version: washington university  department of computer science technical report wucs-1. 
sandholm et al.  1  tuomas sandholm  subhash suri  andrew gilpin  and david levine. cabob: a fast optimal algorithm for combinatorial auctions. in proceedings 
of the seventeenth international joint conference on arti-
ficial intelligence  ijcai   pages 1  seattle  wa  1. 
 sandholm et al.  1  tuomas sandholm  subhash suri  andrew gilpin  and david levine. winner determination in combinatorial auction generalizations. in international conference on autonomous agents and multi-agent systems  pages 1  bologna  italy  july 1. early version appeared at the agents-1 workshop on agentbased approaches to b1b  pp. 1  montreal  canada  may 1. 
 sandholm  1  tuomas sandholm. a strategy for decreasing the total transportation costs among areadistributed transportation centers. in nordic operations analysis in cooperation  noas : or in business  turku school of economics  finland  1. 
 sandholm  1  ttiomas sandholm. an implementation of the contract net protocol based on marginal cost calculations. in proceedings of the national conference on artificial intelligence  aaai   pages 1  washington  d.c.  july 1. 
 sandholm  1  tuomas sandholm. negotiation among self-interested computationally limited agents. phd thesis  university of massachusetts  amherst  1. available at http:// www.cs.cmu.edu/ sandholm/ dissertation.ps. 
 sandholm  1  tuomas sandholm. unenforced ecommerce transactions. ieee internet computing  l 1 :1  nov-dec 1. special issue on electronic commerce. 
 sandholm  1  ttiomas. sandholm. contract types for satisficing task allocation: i theoretical results. in aaai 
spring symposium series: satisficing models  pages 1  stanford university  ca  march 1. 
 sandholm  1a  tuomas sandholm. agents in electronic commerce: component technologies for automated negotiation and coalition formation. autonomous agents and multi-agent systems  1 l :1  1. special issue on best of icmas 1. 
 sandholm  1b  tuomas sandholm. approaches to winner determination in combinatorial auctions. decision support systems  1-1. 
 sandholm  1c  tuomas sandholm. issues in computational vickrey auctions. international journal of electronic commerce  1 : 1  1. special issue on applying intelligent agents for electronic commerce. a short  early version appeared at the second international conference on multi-agent systems  icmas   pages 1 1. 
 sandholm  1a  ttiomas sandholm. algorithm for optimal winner determination in combinatorial auctions. arti-
ficial intelligence  1-1  january 1. first appeared as an invited talk at the first international conference on information and computation economies  charleston  sc  

1 	computers and thought award paper 

oct 1 1. extended version appeared as washington univ.  dept of computer science  tech report wucs1  january 1th  1. conference version appeared at the international joint conference on artificial intelligence  ijcai   pp. 1  stockholm  sweden  1. 
 sandholm  1b  tuomas sandholm. emediator: a next generation electronic commerce server. computational intelligence  1 :1  1. special issue on agent technology for electronic commerce. early versions appeared in the conference on autonomous agents 
 agents-1   pp. 1  1; aaai-1 workshop on 
ai in electronic commerce  orlando  fl  pp. 1  july 
1; and as a washington university  st. louis  dept. of computer science technical report wu-cs-1  jan. 1. 
 sathi and fox  1  a sathi and mark fox. constraintdirected negotiation of resource reallocations. in electronic commerce  acm-ec   pages 1  minneapolis  mn  1. 
 wurman and wellman  1  peter r wurman and michael p wellman. akba: a progressive  anonymousprice combinatorial auction. in proceedings of the acm conference on electronic commerce  acm-ec   pages 1  minneapolis  mn  october 1. 
 yokoo  1  makoto yokoo. algorithms for distributed constraint satisfaction: a review. autonomous agents and multi-agent systems  1 : 1  1. 
 zinkevich et al  1  martin zinkevich  avrim blum  and tuomas sandholm. on polynomial-time preference elicitation with value queries. in proceedings of the acm con-
ference on electronic commerce  acm-ec   san diego  ca  1. 

michael n. huhns and les gasser  editors  distributed artificial intelligence  volume 1 of research notes in artificial intelligence  chapter 1  pages 1. pitman  1. 
 satterthwaite  1  m a satterthwaite. strategy-proofhess and arrow's conditions: existence and correspondence theorems for voting procedures and social welfare functions. journal of economic theory  1-1. 
 smithsal 1  trey smith  tuomas sandholm  and reid simmons. constructing and clearing combinatorial exchanges using preference elicitation. in aaai-1 workshop on p