 
this paper presents an algorithm for extracting propositions from trained neural networks. the algorithm is a decompositional approach which can be applied to any neural network whose output function is monotone such as sigmoid function. therefore  the algorithm can be applied to multi-layer neural networks  recurrent neural networks and so on. the algorithm does not depend on training methods. the algorithm is polynomial in computational complexity. the basic idea is that the units of neural networks are approximated by boolean functions. but the computational complexity of the approximation is exponential  so a polynomial algorithm is presented. the authors have applied the algorithm to several problems to extract understandable and accurate propositions. this paper shows the results for votes data and mushroom data. the algorithm is extended to the continuous domain  where extracted propositions are continuous boolean functions. roughly speaking  the representation by continuous boolean functions means the representation using conjunction  disjunction  direct proportion and reverse proportion. this paper shows the results for iris data. 
1 	introduction 
extracting rules or propositions from trained neural networks is important    although several algorithms have been proposed by shavlik  ishikawa and others    every algorithm is subject to problems in that it is applicable only to certain types of networks or to certain training methods. 
　this paper presents an algorithm for extracting propositions from trained neural networks. the algorithm is a decompositional approach which can be applied to any neural network whose output function is monotone such as sigmoid function. therefore  the algorithm can be applied to multi-layer neural networks  recurrent neural networks and so on. the algorithm does not depend on training methods  although some other methods  
1 	neural networks 
 do. the algorithm does not modify the training results  although some other methods  do. extracted propositions are boolean functions. the algorithm is polynomial in computational complexity. 
　the basic idea is that the units of neural networks are approximated by boolean functions. but the computational complexity of the approximation is exponential  so a polynomial algorithm is presented. the basic idea of reducing the computational complexity to a polynomial is that only low order terms are generated  that is  high order terms are neglected. because high order terms are not informative  the approximation by low order terms is accurate. 
　in order to obtain accurate propositions  when the hidden units of neural networks are approximated to boolean functions  the distances between the units and the functions are not measured in the whole domain  but in the domain of learning data. in order to obtain simple propositions  only the weight parameters whose absolute values are big are used. 
　the authors have applied the algorithm to several problems to extract understandable and accurate propositions. this paper shows the results for votes data and mushroom data. 
　the algorithm is extended to the continuous domain  where extracted propositions are continuous boolean functions. roughly speaking  the representation by continuous boolean functions means the representation using conjunction  disjunction  direct proportion and reverse proportion. this paper shows the results for iris data. 
　section 1 explains the basic method. section 1 presents a polynomial algorithm. section 1 describes the experiments. section 1 extends the algorithm to continuous domains and applies it to iris data. 
　the following notations are used. x y .. stand for variables. f g .. stand for functions. 
1 	the basic method 
there are two kinds of domains  that is  discrete domains and continuous domains. the discrete domains can be reduced to {1} domains by dummy variables. so only {1} domains have to be discussed. here  the domain is {1}. continuous domains will be discussed later. 


tsukimoto 

neural networks 

tsukimoto 

data x1 x1 x1 class i 
1 
1 1 
1 
1 1 
1 
1 1 
1 
1 1 
1 
1 in this case  the domain of the learning data is  1 1    1 1  and  1 1   and a unit of a neural network is 

the existence condition of x1 after the approximation is as follows: 

but  the checking range is limited to the domain of the learning data  so the checking range is limited to the domain of the data 1 and data 1 where x1 = 1. therefore the existence condition is as follows: 

of course  this modified condition is not applied to the output units in 1-layer networks  because the inputs of the output units come from the hidden units. therefore   1  is applied to the output units. 
1 	weight parameters 
let's sort weight parameters pi's as follows: 

when terms are generated  if all weight parameters are used  the propositions obtained are complicated. therefore unimportant parameters should be neglected. in this paper  pi's whose absolute value is small are regarded as the unimportant parameters. we will use pi's up to a certain number  that is  we neglect small pi's. 
　how many weight parameters are used is the next problem. here  the weight parameters p1 ... pk are used  where k is determined by a value based on fourier transform of logical functions. due to space limitations  the explanation is omitted  which will be presented in another paper. 
1 	computational complexity of the algorithm and error analysis 
the computational complexity of generating the mth order terms is a polynomial of ncm  that is  a polynomial of n. therefore  the computational complexity of generating dnf formulas from neural networks is a polynomial of n. usual generations will be terminated up to a low order  because understandable propositions are desired. therefore  the computational complexity is usually a polynomial of a low order. 
　in the case of the domain {1}  linial showed the following formula : 

where / is a boolean function  1 is a term   s  is the order of s  k is any integer  f s  denotes the fourier transform of / at s and m is the circuit's size of the function. the above formula shows the high order terms have little power; that is  low order terms are informative. therefore  a good approximation can be obtained by generating up to a certain order. 
1 	comparisons 
this subsection briefly compares the algorithm with other algorithms. algorithms may be evaluated in terms of five aspects: expression form  internal structure  network type and training method  quality  computational complexity l . here  comparisons focus on internal structure  training method and network type. 
internal structure : there are two techniques  namely decompositional and pedagogical. the decompositional algorithms obtain rules by the unit and aggregate them to a rule for the network. the pedagogical algorithms generate examples from the trained network and obtain rules from the examples. obviously  the decompositional algorithms are better than the pedagogical algorithms in understanding the internal structures. therefore  for example  the decompositional algorithms can be also used for the training control. 
training method : the algorithm does not depend on training methods  although some other methods do. for example   and  use special training methods and their algorithms cannot be applied to networks trained by the back-propagation method. the algorithm does not modify the training results  although some other methods  do. 
network type : the algorithm does not depend on network types  although some other methods do. for example   cannot be applied to recurrent neural networks. 
1 	experiments 
the training method is the back-propagation method. the repetition is stopped when the error is less than 
1. therefore  the error after the training is less than 1. the data used for the training are also used for the prediction. therefore  the accuracy of the trained neural networks is 1%. usual prediction experiments use data different from those used for the training. but  in this case  it is desired that the accuracy of neural networks be 1%  because we want to see how boolean functions can approximate the neural networks. generating terms of propositions are terminated up to second order  because simple propositions are desired. 
1 	votes d a t a 
this data consists of the voting records of the u.s. house 

neural networks 

of representatives in 1. there are 1 binary attributes. classes are democrat and republican: the number of samples used for the experiment is 1. the 

accuracies of propositions extracted from the trained neural networks are shown in the table below. in the table  i.w.p. stands for initial weight parameter and the numbers in the column of hidden layer mean the numbers of hidden units. 
	hidden 	layer i i.w.p.l | i.w.p.1 | i.w.p.1 
	1 	1 	1 	1 
1.1 	1 	1 
1.1 	1 	1 
in the case of 1 hidden units and i.w.p.l  the following propositions have been obtained: 
democrat  physician-fee-freeze:n  v  adoption-of-thebudget-resolutipnry   anti-satellite- test-ban:n   synfuelscorporation-cutbackry   
republican:  physician-fee-freeze: y    adoptionof-the-budget-resolution:n v  anti-satellite-test-ban:y v  synfuels-corporation-cut back: n  . 
in the other cases  similar results have been obtained. 
the results for votes data by c1  which is a typical algorithm for machine learning  are as follows: 
 physician-fee-freeze:n  v  adoption-of-thebudget-resolution:y   synfuels-corporation-cut back:y  -  democrat  
 adoption-of-the-budget-resolution:n  physician-feefreeze:y  v  physician-fee-freeze:y  synfuels-corporationcutback:n  -  republican. 
the accuracy of the result of c1 is 1%  so the propositions extracted from trained neural networks by the algorithm are a little better than the results of c1 in accuracy and almost the same as the results of c1 in understandability. 
1 	mushroom d a t a 
there are 1 discrete attributes concerning mushrooms such as the cap-shapes. classes are edible or poisonous. the number of samples is 1. the accuracies of propositions extracted from the trained neural networks are shown in the table below. 
hidden layer i.w.p.l i.w.p.1 i.w.p.1 1 
1 
1 1 
1 
1 1 
1 
1 1 
1 j 
1 in the case of 1 hidden units and i .w.p.l  the following propositions have been obtained: 
edible: gill-size:broad   odor:almond v odor:anise v 
 odonnone    poisonous: gill-size:narrow v-  odor:almond  - odor:anise  	- odor:none . 
in the other cases  similar results have been obtained. the results for mushroom data by c1 are as follows: 
 odonnone  v  odonalmond  v  odonanise  -  edible  
 odonfoul  	v 	 odonspicy  	v 	 odonfishy  	v 
 odor:pungent  v  odorxreosote  -  poisonous. 
the accuracy of the result of c1 is 1%  so the propositions extracted from trained neural networks by the algorithm are a little worse than the results of c1 in accuracy and almost the same as the results of c1 in understandability. 
1 	extension to the continuous domain 
1 	the basic idea 
in this section  the algorithm is extended to continuous domains. continuous domains can be normalized to  1  domains by some normalization method. so only  1  domains have to be discussed. first  we have to present a system of qualitative expressions corresponding to boolean functions  in the  1  domain. the author presents the expression system generated by direct proportion  reverse proportion  conjunction and disjunction. fig.1 shows the direct proportion and the inverse proportion. the inverse proportion  y = 1 - x is a little different from the conventional one  y = - x     because y = 1 - x is the natural extension of the negation in boolean functions. the conjunction and disjunction will be also obtained by a natural extension. the functions generated by direct proportion  reverse proportion  con-
junction and disjunction are called continuous boolean functions  because they satisfy the axioms of boolean algebra. 

figure 1: direct proportion and reverse proportion 
　since it is desired that a qualitative expression be obtained  some quantitative values should be ignored. for example  two functions  a  and  b  in fig. 1 are different from direct proportion x but the two functions are proportions. so the three functions should be identified as the same one in the qualitative expression. that is  in 
tsuk1moto 


neural networks 

the accuracy of the results of c1 is 1%  so the propositions extracted from trained neural networks are a little worse than the result of c1 in accuracy. 
	1 	conclusions 
this paper has presented an algorithm for extracting propositions from trained neural networks. the algorithm does not depend on structures of networks and training methods. the algorithm is polynomial in computational complexity. the algorithm has been extended to the continuous domain. this paper has shown the experimental results for votes data  mushroom data and iris data. the algorithm has been compared with c1  but when classes are continuous  decision tree learning algorithms such as c1 cannot work. therefore  extracting propositions from trained neural networks is an important technique. the algorithm has been implemented as a module nne neural networks with explanation  in the data mining system kino  knowledge inference by observation . 
