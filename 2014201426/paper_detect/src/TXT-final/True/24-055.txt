 
　　this paper describes a linguistic knowledge representation technique suitable for reducing analysis time and memory requirements in a parser for continuous speech. parsing speech  having to process a lattice of word hypotheses instead of a string of words  involves a tremendous amount of search and the generation of a high number of phrase hypotheses. the aim is  while using powerful and flexible formalisms for syntax and semantics  to generate  compact  phrase hypotheses  each one accounting for many syntactic rules simultaneously. the proposed method is able to cope with  and to take advantage from  the fact that short words are often missing from the lattice. a detailed example is given to clarify this method. finally experimental data arc presented and discussed  showing the effectiveness of the proposed technique. 
1 	introduction 
　　the linguistic processor of a speech understanding system has to deal with a lattice of lexical hypotheses  that is with a set of hundreds of overlapping words hypothesized by a recognition subsystem  each with a particular score denoting its acoustical likelihood. to parse a lattice means to extract from it the best-scored word sequence that is compatible with the system linguistic knowledge. this activity induces a tremendous amount of search for any nontoy application. the uncertainty of input  in fact  combined with the large size of the language model  involves the generation of a high number of partial phrase hypotheses during parsing  especially for languages in which the constituents order is relatively free  like italian. this difficulty may be coped with in two ways. one is to devise an intrinsically efficient parsing control strategy  as 
 this research has been partially supported by eec esprit project no. 1 sundial. discussed in  giachin and rullent  1   where a fast algorithm for parsing word lattices  suitable for parallel implementation  was proposed. the other is a way for parsimoniously representing and efficiently using linguistic knowledge; this is the problem tackled here. 
1 	linguistic knowledge 
　　it is generally acknowledged that different pieces of knowledge  like syntax and semantics  have to be used 
joindy during parsing  hayes et at.  1; niemann et al.   1; poesio and rullent  1 . for these reasons  two types of language representations are used in this research. one is a high-level  human-oriented representation; the other  automatically obtained from the first one through a compiler  is used by the parser and is able to cope with the problems mentioned above. the high level formalisms are a dependency grammar for syntax and caseframes for semantics. the dependency grammar is augmented with information expressing morphological constraints between constituents  in a way similar to the approach followed in unification grammars. dependency rules and caseframes are compiled into structures called knowledge sources  kss   which retain the basic structure of the dependency rules and add semantic constraints to their constituents. the compiler partitions the head words into word classes. each word class is associated to one or more kss. each ks defines all the possible connections between the words of its word class and other words. 
the process of compilation is described in more detail in 
 poesio and rullent  1 . the focus of this paper is on the output of the compiler  not on the compiler itself; the formalism used at the high level is not relevant for this discussion. 
　　parsing may be viewed as the process of linking word hypotheses together into phrase hypotheses according to a priority defined by their scores. but what kind of structures can we build for representing phrase hypotheses  suppose a  classical  grammar  like a context free grammar  is used and that we are trying to connect two words into a grammatical structure. in general  this can be done in several ways according to different grammar rules. since structures built with different rules may connect with different word 
baggia  et al. 
hypotheses  we are compelled to record a new memory object for every structure. this makes no serious problem if we are processing written language  but in the case of speech there are two undesirable consequences. first  a very large memory size is required  owing to the high number of word combinations allowed by word lattices. second  each of the structures will be separately selected and expanded  possibly with the same words  during the score-guided analysis  thus introducing redundant work. 
　　we can avoid these two drawbacks by approaching the extreme situation in which only one single  compact  structure is generated for every group of different words  condensing all the information necessary to account for the different ways this word group may be connected to other words. therefore  the compiler should generate a small number of  compact  kss  still keeping the maximum discrimination power. 
　　this is accomplished by the fusion technique. two types of fusion have been experimented and both are described in the following. the second type of fusion is a more powerful generalization of the first type  hence it will be described in greater detail. 
this approach would be useless if the constraints in the 
 compact  structures were hard to check and propagate  so efficient representation methods have been studied. 
1 	the parsing approach 
1 	parsing as linking words 
　　the parsing of a lattice may be seen as the activity of linking word hypotheses together on grounds of linguistic knowledge. each link established between two word hypotheses is a hypothesis itself and is called link hypothesis  lh . it is represented in fig.l.a. 

fig 1 - a  a link hypothesis from wh1 to wh1. 
b  a phrase hypothesis ph of two lhs. 
　　a set of word hypotheses connected by link hypotheses is called phrase group  pg  and the final parsing result is a pg whose word hypotheses cover the whole speech time interval1. a pg is obtained step by step: at each step a new link hypothesis between two words is established; each of the two words may be already connected to other words. 
　　a word in the dictionary is called head word if it can be  according to the system linguistic knowledge  head of a syntactic constituent  otherwise it is called terminal word. 
　　each pg is characterized by an active head: a head word that has been selected as the one in charge to be connected to other words; if the pg corresponds to a complete 
 sub constituent  then the active head should be the lexical head of the  sub constituenl 
1 	representing phrase groups 
　　at each parsing step the control strategy must select the best pg or word hypothesis to generate new pgs. a pg is based on the simultaneous presence of a certain number of lhs that  when departing from the same head word  are grouped together to form a compound set of links called 
phrase hypothesis  ph   see fig. l.b . 
　　fig 1 shows the creation of a new lh between wh1 and a certain pg having active word wh1. 

fig 1 - the phrase group pg1 includes the four whs. 
　　for a more efficient representation  the control strategy always selects  as the active head of a pg  the deepest head word not yet saturated  i.e. that has still the possibility of being connected to other words. for instance let us suppose it is wh1  see fig. 1 ; then we can represent the phrase group pg1 simply through ph1 augmented with a pointer to its context phi  se fig. 1 . in this way each time a new lh is added  only a new phrase hypothesis is created  having all the information needed by the control strategy. 

fig 1 - ph1 in the context of phei. 
1 	the control strategy 
　　each wh has a score assigned by the recognition stage; each pg has a score that is derived directly from the scores and time intervals of the involved whs using a density method  woods  1  that guarantees that wh scores and pg scores are comparable1. at each parsing step the best item  either a wh or a pg  is selected and the kss are used to try to create lhs that  starting from that element  connect it to other  linguistically and temporally acceptable  items: words in the lattice and already produced pgs. 
1 rule fusion 
　　the production of kss according to restricted rule fusion aims at compacting rules having the same constituent number and order. 
　　consider  as an example  two grammars rules  each with one head and one dependent. suppose the head is the same  but the dependent is different in terms of syntactic and/or semantic constraints. fusion produces one single ks  grouping the information of both rules. the ks accounts for the same general structure as the rules  i.e. it has a head and one dependent. the head is of the same type as in the original rules. the dependent  instead  has a generalized type. the ks contains a set of possible constraints  called conds  between the head and its dependent  derived from the original rules. when a head word for this ks connects itself to another word  the resulting ph will have all the conds active  except those that are incompatible with the features of the head word and of the dependent. when the ph is selected by the scheduler for expansion and new structures  containing more words  are created  these structures will contain all the remaining active conds  except those incompatible with the newly added words  and so on. 
　　this technique provides the advantage of sensibly reducing the number of structures generated during parsing. 
1 	generalized rule fusion 
　　the generalized rule fusion aims at compacting together rules with constituents in different order or even with different number of constituents. 
　　table 1 contains four head words together with their classes  while table 1 contains eight sentence fragments that can be composed together to generate a large number of italian sentences. 

table 1 
　　of course the admissible sequences do not depend only on the class of the fragments but also on their specific 
　　all whs of a given pg have the same time interval  then the score of the pg would be the mean value of these scores. 

table 1 
characteristics  like morphological features  grammatical relations  semantic contents  etc. here  to simplify the 

description  different sets of constraints for a sentence fragment are represented just by appending a number to the fragment class. 
　　tables 1  1  1  1 contain  for each head class  a  b  c  m   a sketchy representation of the rules involved. the positions of the constituents are also shown. the zero position indicates the head. 

1 	kss as rules 
let us suppose we have a wh of class c  spedito  
 sent  and we want to connect it to other words that can depend on it and that are adjacent to the head on the right; from table 1 constituents of both classes a and b arc involved. let us focus on the class a case. 
　　without any kind of fusion we would have a ks for each row of table 1. as we want to find class a constituents  on the right of the header  six kss are involved  corresponding to rows cl c1 c1 c1 c1 c1; the first two kss propagate constraints  summarized by a1  that 
baggia  et al. 
will be considered by the second ks of class a  table 1  row a1 . 
　　if such ks can find a wh like  roberto   it tries to link it to the preposition  da   by  that is supposed to be missing from the lattice; a place holder  a virtual wh  is created and linked. two couples of phs are generated  one for each ks  rows cl and c1 of table 1   to represent the two phrase groups that are supported by the same set of whs   spedito    roberto  and the place holder  
　　other two couples of phs are generated in a completely similar way by the kss of row c1 and c1  the only difference being that the place holder is generated by the fourth ks of class a  row a1  and will be for a wh like  a  
 to . 
   the kss of rows c1 and c1 require a sentence fragment of kind a1  i.e. a preposition is not required; two new phs will be generated. 1 
ph1 is characterized by the union of constraints a1 and 
a1. in its turn ph1 is characterized by constraints c l   active rows c1 and c1 and saturated rows cl and c1. 
　　active rows are used to decide if and how new links can be established; for instance  the link starting from the head wh1 of ph1  fig. 1.b  has to be of class b and constraints b1  position 1 for rows c1 and c1 in table 1 . 
　　the fusion techniques greatly reduce the number of intermediate items  phs  that have to be generated. however  this would be of no use if it were balanced by an increased activity for checking and propagating constraints. to avoid this problem each ks position  excluding the head position  is characterized by a list of admissible classes  a and b for position 1 of the ks of table 1  and for each of them by a bit position of the rows involving that class in that position. the active rows of a ph are represented by a bit position too  so both the activity of deciding if a ph is saturated and the decision if the word can be linked to a word of a given class  can be performed through quite simple and operations. bit positions are also used to represent morphological information  admissible grammatical relations and semantic information  i.e. what has been indicated here just as a subscript . 
1 	parsing a lattice: an example 
　　the parsing input is a lattice of whs  each one characterized by the word label  the beginning frame  the ending frame and the acoustical score. fig. 1 is an example of a lattice1 for the utterance  mcssaggio spedito da roberto . 
　　the best scored wh in the lattice  wh1  ieri  of class b and score 1  is selected. the kss could be used to try to connect wh1 either with whs depending on it {top-down strategy  or with head whs from which wh1 may depend from  bottom-up strategy . both directions could be tried  but it is better to find first a complete constituent before starting a bottom-up activity. 
　　the ks of class b  see table 1  has two admissible sequences of constituents; row bl requires the presence of a terminal word: the preposition  di   of   on the left of the head; for b1  the head alone is already a complete constituent therefore both the strategies are tried. 
　　following the top-down action a link towards a preposition is tried. as short words may be missing from 1not a realistic one: real lattices contain hundreds of whs. 
the lattice  it is possible to hypothesize the correct word without its presence in the lattice  giachin and rullent  1 . the result is the creation of a link from wh1 to the missing wh  labelled  di  creating an instance of a phrase hypothesis  phi  as in fig. 1.a. the phrase group pg1 
   di ieri   is represented by phi  having the same score of wh1  i.e. 1  and a time boundary that takes into account a default time interval for  di. 
   in the bottom-up action all the kss that may connect head words to the class b word  ieri  are triggered; in the example they are of class c and m  see tables 1 and 1 . exploiting the time constraints from wh1  the lattice is searched to find proper head whs for each of the two kss. only class m ks is successful by finding the wh1  messaggio  that is linked to wh1. a check is made to see if the link has determined a complete constituent; this is the case as the m1 row of the m ks  see table 1  requires just a class b dependent on the right of the head. the result of the successful connection is pg1   messaggio  di ieri   as in fig. 1.b. 
　　ph1 contains all the run-time informations pertaining to the links  i.e. the possible rows of the ks have been restricted for ph1 only to m1  m1  m1 and m1  and a new score  1  has been computed. 
　　since wh1  spedito  of class c has a beuer score  1  than pg1  1   it is now selected. for the ks of class c  see table 1  only a top-down activity is possible. trying to exploit time adjacency  position 1 dependents are taken into account; they must be of class a or b  in four different characterizations  a1  a1  a1 and b1 . 
　　only wh1  roberto  of class a  satisfies the time constrains given by wh1. a direct link between wh1 and wh1  justified by row c1 and c1  is not created because a1 imposes a strict time adjacency between wh1 and wh1  while the gap between the two whs is supposed to be larger than the fixed threshold between adjacent words. 
so class a ks must connect wh1 to the preposition 
 da   by  or  a   to  to generate constituents that satisfy either the a1 or a1 characterization  see table 1 . this activity is performed in the context determined by wh1 and the result is as presented in the previous section  see fig. 
1.b . 
when ph1 is created  ph1 rows are restricted to cl +c1 
 due to a1  and c1+c1  due to a1 . rows cl and c1 corresponds to complete constituents  saturated rows in fig. 1.b   while for rows c1 and c1 a dependent of class b is still required  but it is not present in the lattice. 
　　the resulting complete pg  pg1   spedito  da / a  roberto  with score 1   can start a bottom-up activity. 
the only ks that is able to connect wh1 is the class m ks  that at last can create pg1  see fig. 1.a  of score 1. 
this is a solution  s1  but has a score worse than that of 
wh1; so it is not accepted and has to wait until no other better items remain active. 
　　now the best score pertains to wh1 of class a  1  and the ks of class a  see table 1  is considered; there are four possible characterizations   a l   a1  a1 and a1 . characterization a l   a1 and a1 requires the presence of a short word on the left of the head  but only one ph   ph1 in fig. 1.b  is generated. 
　　from row a1 it is not possible to obtain any link with other whs in the lattice  as we have seen before   while from ph1 a link can be tried towards wh1 in ph1  see fig. 1.b   so that a merge of two pgs can be attempted. 
one of the active rows still alive in ph1  m1  allows the link with a dependent of class a  with characterization al  in position 1 and a cross restriction is performed on ph1  with the creation of a new ph1  which maintains only the admissible row m1. ph1 is copied to create ph1 and the ambiguity about the preposition is solved thanks to constraint a l   see fig. 1. 

baggia  et al. 
　　
the compound pg1   messaggio  di ieri  di roberto   represents a solution  si  with score 1  which covers the whole time interval. 
　　at this point  being the parser almost optimal and pg1 a solution  the parsing process could stop. in rare occasions  as in the example  the solution may be wrong. for this reason  the verification phase has been introduced. 
the parser does not stop at the first solution  but continues for a fixed time until a few solutions have been collected. 
　　in the example the solution s1   messaggio spedito  da/ a roberto   previously generated will be accepted by the control strategy  see fig. 1.a . 
　　at the end of the parsing activity a phase of acoustical verification of the solutions eliminates the ambiguity between multiple candidates for each short word and reorders the set of solutions. in the example the acoustical verification will identify the new best scored solution s1 as the correct one and will eliminate the ambiguity between the two prepositions  a and  da in s1 by selecting the correct one   da  . 
1. 	experimental results 
　　the validity of the approach has been assessed by three series of experiments  using the no fusion  the restricted rule fusion  and the generalized rule fusion approach respectively. in all cases the test data consist of 1 word lattices produced by a speaker independent recognition system from sentences uttered in a continuous fashion  i.e.  with no pauses between words  and with large linguistic freedom. the utterances  referring to a 1-word electronic mail management task  were recorded from a pabx and include 1 different speakers. 
the recognition system  described in more detail in 
 fissore et al.  1   is based on hidden markov model technology. 1 discrete models are used to describe subword speech units. the lattices are produced using a viterbi-likc decoding algorithm that finds the best match for each word in every possible position of the utterance. 

table 1 
　　table 1 reports the average number of phs generated and the cpu time  in seconds  for each sentence. the parser  written in c  runs on a sun sparkstation 1. the most dramatic improvement refers to the reduction of phs. this is not surprising:  compact  kss permit to spare the generation of redundant structures  but each structure requires more work to be build. the high reduction of generated structures  anyway  compensates this fact and still allows a more than fourfold cut of total timings with respect to the no fusion case. 
　　table 1 shows the effect of the verification phase on the percentage of correctly understood sentences. 

table 1 
1. 	conclusions 
　　one of the problems in parsing word lattices is the generation of a large number of partial phrases. this number may be reduced provided a compact representation of similar language constructs is adopted. the proposed approach consists in fusing several rules of a high-level  human oriented language representation into a small number of knowledge sources. each knowledge source is provided with a concise definition of syntactic and semantic constraints necessary to specify how words may be connected together. alongside  efficient methods of checking and propagating these constraints are provided. the approach has been validated through an extensive experimentation with real data on a non-toy application obtaining a great decrease of the number of generated phrases and of total analysis times. 
