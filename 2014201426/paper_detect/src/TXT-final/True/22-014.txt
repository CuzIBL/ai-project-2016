 
　　　rising interest in production systems has led to a number of research efforts aimed at increasing their execution speed. the bottleneck in current implementations is the matching required during each production cycle. cupid is a multiprocessor architecture designed to execute ops1 matching using drete  a distributed version of the rete matching algorithm. this paper describes cupid and the drete algorithm with emphasis on correctness and effectiveness in exploiting parallelism in the match operation. the monkey and bananas program was executed on a cupid simulator running the drete algorithm. the results show that on a technology-independent comparison basis  and in measured execution speed  that the cupid/drete combination is several times faster than a commercial uniprocessor  a vax 1  running compiled ops1. 
1. introduction 
with the increasing use of production systems  there is growing interest in improving their execution speed. a number of researchers are working toward this goal  some concentrating on customized uniprocessor designs quin1 lehr1  and others on multiprocessor architectures hill1 stol1 ramn1 gupt1 oshi1 . 
	the 	work 	reported 	here 	involves 	the 
cupid kell1  multiprocessor architecture running a distributed matching algorithm  drete  kell1b . drete and cupid were designed in tandem  each taking advantage of characteristics of the other to ensure optimum performance from the union of the two. drete is based on the rete forg1  algorithm  a 
many-pattern/many-object matching algorithm designed for the ops family of production languages running on a uniprocessor. the performance of cupid executing drete has been assessed with respect to expected performance and theoretical 
1 	tools 
limitation and using a test simulation. the simulated program is the monkey and bananas problem. it was chosen because it has been used in the past to grade the performance of several machines  although it does not contain a high potential for parallelism. the ops program used is shown as an example in the ops1 user's guide forg1 . as a basis for comparison  this program was also run on a vax 1 using a standard ops1 compiler. 
　　　the next section contains a brief review of production languages  the ops family in particular  and the rete matching algorithm. the following two sections describe the drete algorithm and the cupid architecture. sections 1 and 1 give details of the simulator and the simulation results and section 1 is a discussion of these results and their implications. 
1. ops production languages and rete matching 
a production system consists of three parts: 1  a set of rules  or productions  defining available operations on a problem state  1  a set of data elements or working memory  which describe the current problem state  and 1  a control mechanism which applies the rules to the data elements. each rule is made up of a condition portion  describing the situation it applies to  and an action portion  describing operations to be performed on the data. 
　　　it is the responsibility of the control mechanism to determine which rules apply at a particular time by matching the data elements to the rule conditions. from the result of the match - the conflict set - the most appropriate rule is selected - conflict resolution - and fired. firing a rule changes the data  which invalidates the match. the match must be recomputed before another rule can be selected and so execution proceeds in a cycle of match  conflict resolution and rule firing phases. 
　　　of the three phases described  the match phase is by far the most computationally expensive forg1 . the rete algorithm reduces the actual number of match operations done by taking advantage of similarities among rule conditions  and the fact that each rule 

firing generally changes only a small portion of the data set. 
       employing the rete algorithm means compiling the rule conditions into a network of interconnected condition and memory nodes. condition nodes may have one input for constant value tests  or two inputs to join data elements  subject to consistent variable bindings. tokens representing changes to the problem state caused by firing a rule are injected into the top of this network. memory nodes store partial match information for use in subsequent comparisons. 
       using the rete algorithm  the match phase becomes a large set of node activations. at the node level  activation results from the arrival of a token representing one or more data elements on an input arc. a one-input node may or may not pass a token to the next level of nodes depending on the result of the constant comparison. a memory node stores an incoming token and indicates its arrival to the twoinput node it is connected to on the next level in the network. a two-input node tests variable bindings between a newly arrived token and each of the tokens stored in the memory node above it on the opposite side. for each successful comparison  a new token will be sent further down into the network. tokens emerging from the bottom of the network represent fully enabled rules. 
1  	t h e drete m a t c h i n g a l g o r i t h m 
the drete matching algorithm is a distributed version of the rete algorithm. in drete the match phase is partitioned at the token-to-token comparison level. the motivation for this method of partitioning  and the expected performance of the resulting algorithm are derived from the data contained in  gupt1  and  gupt1b . this work is an analysis of a number of well known production systems of various sizes. the characteristics of these programs related to communication requirements  and the quantity of data flowing in the match phase are shown in table 1. the differences between average and maximum values in this table reflect the variability in execution characteristics typical of production systems. 

       drete is designed to meet the requirements of the average data with the flexibility to manage the maximums.  these data  and the form of drete  guided the design decisions in the development of the cupid architecture.  
       in drete  partitioning of one-input nodes is not necessary since their activations do not involve any stored data. the partitioning is done at the two-input nodes where approximately 1% of the match phase time is spent. this partitioning is carried out in two stages. the first step is to isolate the two-input nodes from each other by providing separate memory nodes for each input they are attached to  as shown in figure 1. 

figure 1: m e m o r y node replication 
       each two-input node with its two memory nodes is now an independent process with three ports. in the second partitioning step  the two-input nodes are replicated; the number of replicated copies is equal to the number of tokens stored at the two memory nodes connected to it. in this way  each copy of a two-input node is associated with only one data token - either a left or a right token. one node copy on each side is designated the generative copy; it is responsible for storing new tokens. an example of this is shown in figure 1. 

figure 1: comparison node replication 
       it is important to note that the size of the image of a two-input node is directly dependent on the amount of data associated with it. thus  the level of 
kelly and seviora 

parallelism associated with a node activation is determined by the amount of data it must process. 
　　　the node copies are numbered as consecutive pairs; the left side is even-numbered and the right side is odd-numbered. this numbering scheme makes it easy to activate both sides of a node from a single destination address  while allowing for a distinction of which side of the node the node copy represents and to which side the token is arriving. 
　　　the operation of the node copies for an and node is as follows: 
　　　1  a positive or negative token arriving at a node copy on the opposite side to its stored token performs a token comparison  if required  between the arriving and stored tokens. it passes a token of the same polarity further into the network if the comparison is successful. 
　　　1  a positive token arriving at a node copy on the same side as the stored token will cause the generation of a duplicate node copy with the arriving token attached  if the node copy activated is generative. the generative quality of the activated node copy is then passed to the new node copy. if the node copy activated is not generative  the arriving token is ignored. 
　　　1  a negative token arriving at a node copy on the same side as the stored token will cause the deletion of that node copy if it is not generative  or the deletion of only its stored token if the node copy is generative. the negative token will be retransmitted if the node copy had been activated  not necessarily successfully  by a positive token from the opposite side earlier in the same match phase. this second function is performed to insure the correct formation of conjugate pairs by the node as a whole. 
　　　for a not node  the activity is similar to that described above for the and node  which keeps node interpretation code to a minimum   but a different node formation is used; it is a two-level structure involving three different types of node copy is used. in the top level  the left node copies  the positive side of the node  store tokens and the right side stores nothing. in the bottom level  only the right node copies  which store tokens potentially blocking the passage of left-side tokens  exist. in a not node  the retransmission of negative tokens as described above is not used. instead  the bottom node copies are put in the path of tokens generated by the top portion of the node. the function of the bottom node copies is to transmit tokens cancelling positive ones sent by the top portion of the node  if data exists which makes the positive tokens invalid. 
　　　with two-input nodes partitioned as described  the storing of an arriving token and the comparison of this token with each opposite-side stored token can all 
1 	tools 
be done in parallel. 
　　　the level of parallelism available in the match phase as a whole using drete is related to the product of the number of nodes activated and the number of tokens at each node - with an adjustment for the serial nature of some operations. it is approximately 
1  without hashing of tokens  as discussed in  ke 1b . the effect of hashing is to reduce this number  but it provides an overall advantage in execution characteristics  as described in the next subsection. 
　　　an important consideration in distributing the match operation is how to balance the workload over a set of processing elements. in drete  node/token pairs are continually being produced and eliminated as new match state is computed. a dynamic load balancing algorithm kell1b  is used to move new node/token pairs away from the processing elements that created them. this increases the probability that node copies activated at the same time are processed in different places. 
　　　recall that new node copies  which are generated during a match phase  are completely independent of each other and their generators. load balancing  which is done while the host performs the conflict resolution and act phases of the production cycle  consists of transfering the new node copies to other  nearby  processing elements in the multiprocessing environment. since the generative attribute of a node copy is given to a new node copy before it is transfered to a new processing element  the source of new node copies is not fixed at one processing element. 
　　　the load balancing scheme described above was chosen for its effectiveness but also because it is very simple to execute. a more complex load balancing scheme could be too time consuming and so reduce the value of speeding the match phase itself. the simulations revealed that the amount of time spent in the load balancing phases of operation is never greater than 1% of the match time. since the load balancing phases occur between match phases  overlapping conflict resolution and act phases  they have no serious effect on overall execution speed. 
　　　the replication of node and token information and the demands of the load balancing algorithm make a concise encoding of this information very important. the size of a node copy is minimized by storing it as a template to be interpreted by a matching processor. the amount of token information stored is minimized by keeping only those values that are required for matching further down in the network.  for the two program simulated  the average node size is 1 1-bit words; the average token size is 1 1-bit words. in contrast  the compiled ops1 versions of this program uses about 1 bytes to encode each comparison node because it generates inline code instead of templates to increase execution speed.  
1. drete and hashing 
　　　recent research has shown that the number of comparisons between tokens that are necessary during a match phase can be reduced using hashing of tokens gupt1b . using this method of token partitioning  a node's  node copy's  responsibility during a match phase is reduced from a case where it compares all incoming tokens to all stored tokens to a case where one hash bucket of tokens is compared with a corresponding hash bucket of tokens on the opposite side.  the responsibility for storing tokens is similarly divided.  comparing an incoming token to only a fraction of the corresponding tokens on the opposite side of the node both reduces the comparisons required and  very importantly  reduces the number of comparisons involved in a single node copy activation. hashing  however  cannot be used in all cases of twoinput node types and cannot always be done in a way which results in equally sized buckets for all data cases. 
　　　using drete to parallelize the match operation reduces the number of token-token comparisons done during a single node copy activation to one. over an entire match phase  a node copy's responsibility is to compare all incoming tokens to one stored token. hashing does not increase a node copy's activation expense significantly  and can be added to drete at very little increase in storage expense since the size of a node's image is already dependent on the number of tokens with which it is associated. with a combination of drete and hashing  a node copy's responsibility during a match phase is reduced to matching one hash bucket of tokens to only one other token. overall  the level of parallelism in drete is reduced from the 1 previously mentioned  but the net effect is a beneficial one. 
　　　drete with token hashing is potentially much faster than either concept applied alone since it both reduces the total number of comparisons done  and exploits the greatest degree of parallelism available. the work done with drete to date does not include hashing  but preparations are being made to explore this very promising alternative. 
1. the cupid architecture 
cupid is a matching processor attached to a host. the host executes the conflict resolution phase of the production cycle and fires the chosen rules. in firing a rule  the host sends changes to the current data set to cupid. as the new match state is computed  cupid responds with corresponding changes to the conflict set. 
　　　the cupid architecture is comprised of a large set of small processing elements connected by two independent communication schemes. the number of processing elements in the set is in the order of hundreds to take advantage of the expected level of parallelism available in large programs. the processing elements are arranged as a single two dimensional array  with all node information distributed evenly among them. 
　　　one of cupid's communication systems consists of a pair of unidirectional trees with the processing elements at the leaf positions in both trees. one of these trees is used to broadcast match information to all processing elements over parallel paths. this high speed path ensures fast activation of all network nodes by new match information. the second tree collects the responses of the processing elements to incoming match information. the data path width in this tree widens from serial at the processing element to wordwidth at the root of the tree to accommodate the expected volumes of data at each level in the tree. responses which represent data movement within the network of comparison nodes are fed back into the processing elements via a path between the roots of the collection and broadcast trees. responses representing changes to the set of enabled rules are removed from this stream and sent to a host processor  which awaits such responses from the match phase. 
　　　the second communication system is a set of serial links connecting each processing element with its four nearest neighbours. these links support the dynamic load balancing algorithm  carrying node/token pairs from those processing elements that have produced them to less busy processing elements during the inter-match phase interval. local links are used for this purpose because they match the nature of the transfers: node copies need not travel further than a nearest neighour and have only one destination. the destination of a node copy is not predetermined; it can be seated and processed anywhere in the processing element array. 
　　　figure 1 is a diagram of the cupid architecture with a four-by-four array of processing elements. the broadcast tree is shown branching down to the top of each processing element  and the collection tree branching down from the bottom of each processing element. the local interconnect system is also shown  except - for clarity - the links joining opposing edges of the array. 
　　　a cupid processing element consists of a set of functional blocks which combine to satisfy all of the computational requirements of the match operation. each processing element contains a cpu and local program rom for interpreting node activations and performing both local and global memory management 
kelly and seviora 


1 . 1 . 	t h e c u p i d processing element 
functions  a local r a m area to store node/token pairs  and a c a m block with entries indicating which nodes from the original network are represented at this processing element. a processing element also contains a set of state machines for handling data transfers over the various communication ports. one state machine conditionally stores information reaching the processing element from the broadcast tree in ram. the decision to store or ignore arriving match information is based on the c a m contents. an output state machine is used by the cpu to transfer the results of node activations onto the collection tree. from there  data is distributed to either the host  the processing element array via the broadcast tree  or both. a set of five state machines handle incoming and outgoing node copy transfers over the local links. interactions between these state machines and the cpu are managed by an interrupt protocol. 
       a complete design of the cpu and rom portion of the processing element has been done using an ncr two micron cmos standard cell library. preliminary designs of the individual communication state machines have also been done. 
the processor bond 1  is a three stage pipeline 
harvard architecture design with a 1-bit wide data path. it is a risc processor  satisfying the relatively simple processing requirements of the match operation  and the need for a compact design. it supports two levels of interruption - required by the communication mechanisms - and two special instructions. one 
1 	tools 
of these instructions is a variable condition branch instruction used in place of a sequence of fixed condition branches. this instruction benefits pipeline operation by reducing the number of branches executed. the second special instruction is included to assist in traversing the linked list structure used in storing node/token pairs in ram. the increased complexity of the processor caused by including these instructions is offset by a reduction in code size  resulting in an overall reduction in the size of a processing element. the cpu design required 1 cells; the processing element minus memories is comparable in complexity to a commercial 1-bit microprocessor. 
       prefabrication simulations of the cpu show a peak execution rate of 1 mips. dynamic measurements of instruction frequency from simulations of cupid running drete were used to determine memory reference frequency and branching characteristics during matching. it is estimated that the cpu will execute the required software at 1 mips. 
1. 	v a x 1 m i p s 
       in order to compare cupid results with those obtained on a v a x 1  it is necessary to establish the relative power of their respective processing units. as stated above  the cupid cpu executes the drete code at 1 mips. to normalize this to the v a x cpu speed  measurements were taken of the vax running instructions corresponding to those in the instruction set of the cupid cpu. the speed of the vax executing each of these instructions is combined with the frequency of their occurrence in the cupid processing element code to establish a speed for a v a x 1 running cupid software. table 1 shows the execution speeds for four classes of instructions on the vax and their relative frequencies in cupid software. instructions were run as part of a long stream to ensure that the processor pipeline was full during their execution  except  of course  for the case of the successful branch.  the memory access instruction speed was calculated by multiplying the speed of a memory update on the v a x by the equivalent number of instructions for a cupid cpu performing the same function.  

       from this data it is calculated that the vax 1 would execute cupid software at 1 mips. this is higher than the 1 mips rating for the machine and results from the elimination of complex 

1. t h e simulator 
the simulator was written using n.1 endo1   a commercially available package. register transfer level simulations of cupid running drete have been done to establish a coherent software structure for a processing element. this includes both the match and load balancing algorithms as well as memory management for all state machines. the results of these simulations were important in terms of verifying the basic drete and cupid concepts  but are too time consuming to run all but the simplest of tests. the hierarchical nature of the simulator made it possible to move to a higher level of simulation using process times and communication timing information from the original simulator. this allows the simulation of larger test programs on arrays of 1 processing elements. the accuracy of the higher level simulator was verified by comparing its responses to those obtained from the lower level simulations. the higher level simulator was tuned to agree with the lower level simulator to within 1% between individual responses  and to within 1% over long  mixed  strings of process activations. 
       it is by incorporating timing values from the standard cell implementation of the processing element's components into the simulator that a direct comparison of cupid execution time to that obtained on a standard commercial machine  in this case a v a x 1 running compiled ops1  was possible. 
       in the simulations  only the match phase is simulated; a dummy process is used in place of the host processor to interact with the processing element array. changes to working memory caused by rule firings are presented to the processing element array in the order established by running the ops1 version of the same program. 
1. simulation results 
the monkey and bananas benchmark executes in 1 ms on a v a x 1 with all output disabled. estimating the match phase as 1% of this execution time forg1   it takes 1 ms. 
table 1 shows the match execution times for the 
monkey and bananas program on the vax and on cupid for array sizes of 1 processing elements. below these are the speedup factors of cupid over the vax. the next line in the table shows the estimated match time of cupid with a cpu normalized to the speed of a vax.  this normalization does not take into account that communication time becomes less of a factor in the total match time as processor speed decreases.  the next line in the table shows the speedup obtained over the vax on the normalized cupid. 
       because processor speed will continue to increase as technology advances  it is a matter of theoretical interest to calculate the potential speed of cupid as its cpu clock speed approaches infinity. 
speed is then determined by the effectiveness of the communication system and the quantity of data which must be exchanged between processing elements. the numbers for the speed of cupid in this limiting case are shown at the bottom of table 1; they were calculated assuming that the communication clock remains at 1 mhz  although this too will increase with time  but that serial paths are widened to word width. 
1. 	conclusions 
the cupid matching time with one processing element  from table 1  shows that the cupid processing element can execute the operations required by the match phase very effectively. this contrasts with other research efforts where a parallel algorithm substantially reduces match speed for the uniprocessor case. the speed of a single processing element is multiplied by the effect of using greater numbers of these processing elements in the cupid array. 
kelly and seviora 

　　　it should be noted that the monkey and bananas problem does not contain a high degree of parallelism and  in fact  is fairly sequential in nature. the diminishing returns from increasing the cupid array size to 1 processing elements is an indication that the limit of parallelism is being reached. further simulations are under way  using programs which contain higher potentials for parallel execution  to better show the ability of the drete/cupid combination to speed match execution. 
　　　an 1 fold increase in speed is observed in the eight-by-eight processing element array for the monkey and bananas program despite the low availability of parallelism. this verifies that a fairly simple processing element implemented in a modest technology can provide sufficient processing power for production system matching. at the same time  it verifies that the drete and load balancing algorithms do not introduce an inordinate amount of overhead into the match operation. 
　　　the increase in match execution speed provided by the drete/cupid combination results in a substantial increase in the overall production system execution speed. executing the match phase on a computational element separate from the one executing the conflict resolution phase  the host  has the added advantage that conflict resolution can be performed partially in parallel with the match phase. responses from the match phase can be incorporated into the conflict resolution calculation as they arrive throughout the match phase. the result of this is that the match phase still remains the dominant factor in production system execution time and so increasing its speed further merits additional effort. 
