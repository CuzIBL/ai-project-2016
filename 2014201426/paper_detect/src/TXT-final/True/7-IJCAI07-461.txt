
many methods  including supervised and unsupervised algorithms  have been developed for extractive document summarization. most supervised methods consider the summarization task as a twoclass classification problem and classify each sentence individually without leveraging the relationship among sentences. the unsupervised methods use heuristic rules to select the most informative sentences into a summary directly  which are hard to generalize. in this paper  we present a conditional random fields  crf  based framework to keep the merits of the above two kinds of approaches while avoiding their disadvantages. what is more  the proposed framework can take the outcomes of previous methods as features and seamlessly integrate them. the key idea of our approach is to treat the summarization task as a sequence labeling problem. in this view  each document is a sequence of sentences and the summarization procedure labels the sentences by 1 and 1. the label of a sentence depends on the assignment of labels of others. we compared our proposed approach with eight existing methods on an open benchmark data set. the results show that our approach can improve the performance by more than 1% and 1% over the best supervised baseline and unsupervised baseline respectively in terms of two popular metrics f1 and rouge-1. detailed analysis of the improvement is presented as well.
1 introduction
document summarization has attracted much attention since the original work by luhn  luhn  1   which has found wide-ranging applications especially with the explosion of documents on the internet. besides its main role of helping readers to catch the main points of a long document with less effort  it is also helpful as a preprocessing step for some text mining tasks such as document classification  shen et al.  1 .
　document summarization can be categorized along two different dimensions: abstract-based and extract-based. an extract-summary consists of sentences extracted from the document while an abstract-summary may employ words and phrases that do not appear in the original document  mani  1 . the summarization task can also be categorized as either generic or query-oriented. a query-oriented summary presents the information that is most relevant to the given queries  while a generic summary gives an overall sense of the document's content  goldstein et al.  1 . in addition to single document summarization  which has been first studied in this field for years  researchers have started to work on multi-document summarization whose goal is to generate a summary from multiple documents that cover similar information. in this paper  we focus on generic single-document sentence extraction which forms the basis for other summarization tasks and is still a hot research topic  yeh et al.  1; mihalcea  1 .
　in the past  extractive summarizers have been mostly based on scoring sentences in the source document based on a set of predefined features  mani and bloedorn  1 . these features include linguistic features and statistical features  such as location  rhetorical structure  marcu  1   presence or absence of certain syntactic features  pollock and zamora  1   presence of proper names  statistical measures of term prominence  luhn  1   similarity between sentences  and measures of prominence of certain semantic concepts and relationships  gong and liu  1 . two kinds of approaches have been designed to leverage the above features  supervised and unsupervised. in most supervised approaches  kupiec et al.  1; yeh et al.  1   summarization is seen as a twoclass classification problem and the sentences are treated individually. however  we observe that the individual treatment of the sentences cannot take full advantage of the relationship between the sentences. for example  intuitively  two neighboring sentences with similar contents should not be put into a summary together  but when treated individually  this information is lost. sequential learning systems such as hidden markov models have also been applied  but they cannot fully exploit the rich linguistic features mentioned above since they have to assume independence among the features for tractability  conroy and o'leary  1 . on the other hand  unsupervised approaches rely on heuristic rules that are difficult to generalize. what is ideal for us is to develop a machine learning method based on a training corpus of documents  which can take full advantage of the inter-sentence relationship and rich features which may be dependent.
　in this paper  we tackle the extractive summarization problem in a different manner from the above approaches. we take the summarization task as a sequence labeling problem instead of a simple classification problem on individual sentences. in our approach  each document is considered as a sequence of sentences and the objective of extractive summarization is to label the sentences in the sequence with 1 and 1  where a label of 1 indicates that the sentence is a summary sentence while 1 denotes a non-summary sentence. the label of one sentence is expected to impact on the labels of other sentences that are nearby. to accomplish this task  we apply conditional random field  crf   lafferty et al.  1  in this paper  which is a state-of-the-art sequence labeling method. with crf  we provide a framework for leveraging all the features even if they may be complex  overlapping and not independent. thus we can fully incorporate our knowledge and intuition of extractive summarization by introducing proper features more effectively. besides that  the framework can ensemble the outcomes of other summarization methods in a unified way by designing features for them. our crf-based approach carries out the summarization task in a discriminative manner  by conditioning the whole label sequence on the sentence sequence  which can maximize the likelihood of the global label sequence as well as maximize the consistency among the different labels in the sequence. as a result  this approach overcomes many of the disadvantages of the previous supervised and unsupervised approaches. the experimental results on an open benchmark data set from duc1  http://duc.nist.gov/  show that our proposed approach can improve the performance compared to the state-of-the-art summarization approaches.
1 related work
supervised extractive summarization approaches treat the summarization task as a two-class classification problem at the sentence level  where the summary sentences are positive samples while the non-summary sentences are negative samples. after representing each sentence by a vector of features  the classification function can be trained in two different manners. one is in a discriminative way with well-known algorithms such as support vector machines  svm   yeh et al.  1 . although such classifiers are effective  they assume that the sentences are independent and classify each sentence individually without leveraging the relation among the sentences. hidden markov model based methods attempt to break this assumption  conroy and o'leary  1 . in conroy et al's work  there are two kinds of states  where one kind corresponds to the summary states and the other corresponds to non-summary states. the observations are sentences that are represented by a vector of three features. given the training data  the state-transition probabilities and the state-specific observation probabilities can be estimated by the baum-welch algorithm or an em algorithm  rabiner  1 . given a new document  the probability that a sentence corresponds to a summary state can be calculated. finally  the trained model can be used to select the most likely summary sentences. although such approaches can handle the positional dependence and feature dependence when the feature space is small by taking some special assumptions  they present two open problems  mccallum et al.  1 . firstly  when the feature space is large and the features are not independent or are even overlapping in appearance  the training process will become intractable. therefore this approach cannot fully exploit the potential useful features that we have mentioned above for the summarization task due to the computational inefficiency. secondly  the above approaches set the hmm parameters to maximize the likelihood of the observation sequence. by doing so  the approach fails to predict the sequence labels given the observation sequences in many situations because they inappropriately use a generative jointmodel in order to solve a discriminative conditional problem when observations are given. our work in this paper is aimed at solving such problems by crf.
　many unsupervised methods have been developed for document summarization by exploiting different features and relationships of the sentences as we mentioned above  such as rhetorical structures  marcu  1   lexical chains  barzilay and elbadad  1   the hidden topics in the documents  gong and liu  1  and graphs based on the similarity of sentences  mihalcea  1 . the methods based on the last two features require less extra resources and efforts while still achieve better performances compared to other methods  as shown in  gong and liu  1  and  mihalcea  1 . therefore  we now review these two works in more detail and compare our approach with them in the experiments.
　in  gong and liu  1   the authors observed that hidden topics can be discovered in a document as well as the projection of each sentence on each topic through latent semantic analysis  deerwester et al.  1 . they selected the sentences which have the large projections on the salient topics to form the summary. in mihalcea's work  mihalcea  1   she constructed a graph in which each node is a sentence and the weight of the edge linking two nodes is the similarity between the corresponding sentences. the direction of the edges can be decided by the appearance order of the sentences. after constructing the graph  she employed some graph-based ranking algorithms like hits  kleinberg  1  and pagerank  brin and page  1  to decide the importance of a vertex  sentence  which can take into account the global information recursively computed from the entire graph.
　some previous work has also considered to reduce the redundancy in summary. a typical method is based on the criteria of maximal marginal relevance  mmr   carbonell et al.  1 . according to mmr  a sentence is chosen for inclusion in summary such that it is maximally similar to the document and dissimilar to the already-selected sentences. this approach works in an ad hoc manner and tends to select long sentences. however  in this paper  the redundancy is controlled by a probabilistic model which can be learned automatically.
1 a crf-based summarization approach
1 motivation
our intuition comes from our observations on how humans summarize a document by posing the problem as a sequence labeling problem. a document can be regarded as a sequence of sentences that can be partitioned into several segments where each segment is relatively coherent in content. in order to generate a summary with good coverage and low redundancy  we need to select a representative sentence from each segment. therefore  we have to read the document from the beginning to the end and judge the informativeness of each sentence while reading. if we encounter a sentence which is informative enough  we will put it into the summary. after reading more sentences and encountering better ones  the decision on a previous sentence may be changed. therefore  the procedure of summarization is kind of sequence labeling. the goal is to produce a label sequence corresponding to the sentence sequence with a label of 1 denoting the summary sentences and 1 denoting the non-summary sentences.
　however  the informativeness cannot be easily measured directly by machines. fortunately  the sentences can be characterized by some features such as their lengths  positions in the article and the terms that they contain. the judgment criteria can be learned from the ground-truth samples generated by people. in other words  given a sequence of sentences represented by certain features  our goal is to label the sentences so that the likelihood of the label sequence given the whole sentence sequence is maximized. in this paper  we use crf as a tool to model this sequence labeling problem.
1 conditional random fields
for a random variable over data sequences to be labeled x  and a random variable over corresponding label sequences y   conditional random fields  crf  provide a probabilistic framework for calculating the probability of y globally conditioned on x  lafferty et al.  1 . x and y may have a natural graph structure. in this paper  we use a common special-case structure  which is a linear chain suitable for sequence labeling. we further assume that there is a one-to-one correspondence between states and labels  two states/labels in our problem: summary sentence and non-summary sentence . given an observation sequence  sentence sequence here  x =  x1 ... xt  and the corresponding state sequence y =  y1 ... yt   the probability of y conditioned on x defined in crfs  p y |x   is as follows:

where zx is the normalization constant that makes the probability of all state sequences sum to one; fk yi 1 yi x  is an arbitrary feature function over the entire observation sequence and the states at positions i and i   1 while gl yi x  is a feature function of state at position i and the observation sequence; λk and μl are the weights learned for the feature functions fk and gl  reflecting the confidence of feature functions. the feature functions can describe any aspect of a transition from yi 1 to yi as well as yi and the global characteristics of x. for example  fk may have value 1 when yi 1 is a summary sentence while yi is not a summary sentence and the similarity between xi 1 and xi is larger than a threshold; gl has a value 1 when yi is a summary sentence and xi has upper-case words.
parameters estimation
let Λ = {λk  μl} be the set of weights in a crf model. Λ is usually estimated by a maximum likelihood procedure  that is  by maximizing the conditional loglikelihood of the labeled sequences in the training data Ψ = { x1 y1  ...  xn yn  }  which is defined as:
		 1 
to avoid overfitting  some regularization methods are employed  peng and mccallum  1 . a common method is to add a gaussian prior over the parameters:

where σk1 and σl1 are the variances of the gaussian priors.
　various methods can be used to optimize lΛ  including iterative scaling algorithms such as gis and iis  lafferty et al.  1 . it has been found that a quasi-newton method such as l-bfgs converges significantly faster  sha and pereira  1; malouf  1 . therefore  in this paper  we use lbfgs.
inference
given the conditional probability of the state sequence defined by a crf in  1  and the parameters Λ  the most probable labeling sequence can be obtained as
	y   = argmaxy pΛ y |x 	 1 
which can be efficiently calculated with the viterbi algorithm  rabiner  1 . the marginal probability of states at each position in the sequence can be computed by a dynamic programming inference procedure similar to the forwardbackward procedure for hmm  lafferty et al.  1 . we can define the  forward values  αi y|x  by setting α1 y|x  equal to the probability of starting with state y and then iterate as follows:

where  is defined by:
		 1 
 then zx equals to. the  backward values  βi y|x  can be defined similarly. after that  we calculate the marginal probability of each sentence being a summary sentence given the whole sentence sequence by:
		 1 
thus we can order the sentences based on p yi = 1|x  and select the top ones into the summary.
1 feature space
many features have been designed for document summarization and can be leveraged through crf models. in this paper  we use some common features which are widely used in the supervised summarization methods as well as several features induced from the unsupervised methods. the detailed study of other sophisticated features such as the rhetorical relations between sentences is left for future work.
basic features
the basic features are the commonly used features in previous summarization approaches  which can be extracted directly without complicated computation  yeh et al.  1 . given a sentence xi   the features are defined as follows. position: the position of xi along the sentence sequence of a document. if xi appears at the beginning of the document  the feature  pos  is set to be 1; if it is at the end of the document   pos  is 1; otherwise   pos  is set to be 1. length: the number of terms contained in xi after removing the words according to a stop-word list. log likelihood: the log likelihood of xi being generated by the document  logp xi|d . this is calculated by  where n wk xi  is the numwk xi p wk|d can be estimated
by	k	wj	j	.
thematic words: these are the most frequent words in the document after the stop words are removed. sentences containing more thematic words are more likely to be summary sentences. we use this feature to record the number of thematic words in xi .
indicator words: some words are indicators of summary sentences  such as  in summary  and  in conclusion . this feature is to denote whether xi contains such words. upper case words: some proper names are often important and presented through upper-case words  as well as some other words the authors want to emphasize. we use this feature to reflect whether xi contains the upper-case words. similarity to neighboring sentences: we define features to record the similarity between a sentence and its neighbors.  sim to pre n  and  sim to next n   n = 1  1  1  record the similarity of xi to the previous three sentences and next three sentences respectively. the similarity measurement we use in this work is the cosine similarity.
　there are some other popular features such as the number of words in the sentence which are also present in the title  and the position of the sentence in its paragraph. however  since the information about the title and the paragraph is not available in the dataset that we are working on  we do not consider such features in this paper.
complex features
lsa scores: by decomposing the word-sentence matrix through singular vector decomposition   we can obtain the hidden topics in a document as well as the projection of each sentence on each topic  gong and liu  1 . then we can use the projections as scores to rank sentences and select the top sentences into summary. in this paper  we can also treat such projections as features to reflect the importance of the sentences.
hits scores: as shown in the related work section  a document can be treated as a graph and after applying a graphbased ranking algorithm such as hits or pagerank  each sentence gets a score reflecting its importance. according to  mihalcea  1  and our own experimental results  the authority score of hits on the directed backward graph is more effective than other graph-based methods. therefore  we consider only these authority scores and take them as features.
1 experiments and results
in this section  we conduct experiments to test our crfbased summarization approach empirically. the data set is an open benchmark data set which contains 1 documentsummary pairs from document understanding conference  duc  1  http://duc.nist.gov/ . we use it because it is for generic single-document extraction task that we are interested in and it is well preprocessed. we denoted it by duc1.
　for the supervised summarization methods  we need to split the data set into training data set and test data set. in order to remove the uncertainty of a data split  a 1-fold cross validation procedure is applied in our experiments  where 1 folds are used for training and one fold for test. though we do not need to split the data set for unsupervised methods  we apply the unsupervised methods on the same test data as the supervised methods  for the convenience of comparison.
　we use two methods to evaluate the results. the first one is by precision  recall and f1 which are widely used in information retrieval  van rijsbergen  1 . for each document  the manually extracted sentences are considered as the reference summary  denoted by sref . this approach compares the candidate summary  denoted by scand  with the reference summary and computes the precision  recall and f1 values as shown in equation  1 . we report only f1 for simplicity  since we come to similar conclusions in our experiments in terms of any of the three measurements.

　a second evaluation method is by the rouge toolkit  which is based on n-gram statistics  lin and hovy  1 . this tool is adopted by duc for automatic summarization evaluation that was found to highly correlate with human evaluations. according to  lin and hovy  1   among the evaluation methods implemented in rouge  rougen  n=1  1  is relatively simple and works well in most cases. therefore  we employ only rouge-1 for simplicity.
1 baselines
we compare our proposed method with both supervised and unsupervised methods. among the supervised methods  we choose support vector machine  svm   naive bayes  nb   logistic regression  lr  and hidden markov model  hmm . svm is one of the state-of-the-art classifiers. hmm extends nb by considering the sequential information  while lr is a discriminative version of nb. at the same time  lr can be considered as a linear chain crf model of order zero and crf is a discriminative version of hmm. that is  crf combines the merits of hmm and lr. recent literature has claimed the advantages of the discriminative models in classification problems and the effectiveness of sequential information in sequence processing  sutton and mccallum  1 . therefore  a detailed comparison among these methods can make it clear whether crf really hold these advantages in our summarization problem.
　we also compare our approach with four unsupervised methods. the simplest being to select sentences randomly from the document is denoted as random. the approach selecting the lead sentences  which is taken as the baseline popularly on the duc1 dataset  is denoted as lead. a similar method is to select the lead sentence in each paragraph. since the information about the paragraphs is not available in duc1  we do not include this method as a baseline. two other unsupervised methods we compare include gong's algorithm based on lsa and mihalcea's algorithm based on graph analysis. among the several options of mihalcea's algorithm  the method based on the authority score of hits on the directed backward graph is the best. it is taken by us for comparison. these two unsupervised methods are denoted by lsa and hits respectively.
1 results and analysis
performance based on the basic features the first experiment compares our crf-based method with the eight baselines  only using the basic features. tables 1 and 1 show the results of all the methods in terms of rouge1 and f1. we can see that random is the worst method as expected  while crf is the best in terms of both evaluation metrics. hits beats all other baselines  which confirms the effectiveness of graph-based approaches for discovering the importance of sentences. lead  by simply selecting the lead sentences  achieves a similar performance to lsa. both hmm and lr improve the performance as compared to nb due to the advantages of leveraging sequential information and discriminative models. lr and svm achieve similar performance on the summarization problem. by combining the advantages of hmm and lr together  crf makes a further improvement by 1% and 1% over both hmm and lr in terms of rouge-1 and f1  respectively. in fact  crf is not just a discriminative version of hmm; it is a more powerful method in exploiting dependent features. due to the same reason  crf outperforms hits by 1% and 1% in terms of rouge-1 and f1  respectively.
randomleadlsahitsrouge-1.1.1.1.1f1.1.1.1.1table 1: results of unsupervised methods
nblrsvmhmmcrfrouge-1.1.1.1.1.1f1.1.1.1.1.1table 1: results of supervised methods with basic features
incorporation of the complex features
the second experiment is to test the effectiveness of the complex features as well as the capability of the supervised methods to incorporate the complex features. the results are shown in table 1. compared to the results only based on the basic features  as shown in table 1  we see that the performance of all the supervised methods are improved significantly. after incorporating the complex features  crf is still the best method  which improves the values of rouge-1 and f1 achieved by the best baselines by more than 1% and 1%. compared with the best unsupervised method hits  the crf based on both kinds of features improves the performance by 1% and 1% in terms of rouge-1 and f1  respectively. in fact  the complex features are the outcomes of the unsupervised methods lsa and hits. to leverage the complex features through the supervised methods can be thought as a way of combining the outcomes of different methods. in order to test the effectiveness of crf on combining the outcomes  we compared it to the linear combination method used to combine the results of lsa  hits and crf based only on the basic features. by tuning the weight of each method for combination  the best result we can obtain on duc1 is 1 and 1 in terms of rouge-1 and f1 respectively  where the improvement is not as significant as crf based on all the features. therefore  we can conclude that crf provides an effective way to combine the outcomes of different methods by treating the outcomes as features.
nblrsvmhmmcrfrouge-1.1.1.1.1.1f1.1.1.1.1.1table 1: results of supervised methods with all features
nblrsvmhmmcrfrouge-1.1.1.1.1.1f1.1.1.1.1.1table 1: results of supervised methods with less training data
effect of the size of the training data
in order to study the impact of the size of the training data on the supervised methods  we conduct a third experiment. we change the training data and test data in the 1-fold cross validation procedure  where one fold is for training and the other nine folds for test. table 1 shows the results based on both the basic features and the complex features. we can see that the performances of all the supervised methods shown in table 1 are not as good as those given in table 1  which is consistent with our intuition  that is we can obtain more precise parameters of the models with more training data. another observation is that the gap in the performance between crf-based methods and the other four supervised methods is clearly larger when the size of the training data is small. the reason is that crf performs better with less training data than hmm since it does not require the features to specify completely a state or observation  lafferty et al.  1 . on the other side  hmm  as a generative model  spends a lot of resources on modeling the generative models which are not particularly relevant to the task of inferring the class labels. the bad performance of nb  lr and svm may be due to the fact that they tend to be overfitting with a small amount of training data.
1 conclusion and future work
in this paper  we have proposed a novel crf based approach for document summarization  where the summarization task is treated as a sequence labeling problem. by applying the effective sequence labeling algorithm crf  we provided a framework to consider all available features that include the interactions between sentences. when comparing our crfbased approach with several existing summarization methods  including the supervised and unsupervised ones on an open data set  we found that our approach can improve the summarization results significantly. the experimental results also validated the capability of our proposed approach to integrate the outcomes of other summarization methods.
　in our future work  we plan to exploit more features  especially the linguistic features which are not covered in this paper  such as the rhetorical structures. we will also apply our approach to some more data sets with different genres to test its robustness.
1 acknowledgements
dou shen and qiang yang are supported by a grant from nec  neclc1.eg1 . we thank ms qionghua wang and the anonymous reviewers for their useful comments.
