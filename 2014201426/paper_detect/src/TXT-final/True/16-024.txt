 
　　recent attempts at modeling humans' abilities at processing natural language have centered around depth first parsing algorithms  and control strategies for making the best choices for disambiguation and attachment. this paper proposes a breadth-first algorithm as a model. the algorithm avoids some of the common pitfalls of depth-first approaches regarding ambiguity  and by using more pre-ccmputed information about the grammar  avoids same of the usual problems of parallel parsing algorithms as well. 
1. parsing models 
     in the study of computational models of human language processing  cognitive scientists seem to have given l i t t l e attention to all-paths parsers  focusing instead on depth-first algorithms. this restriction is imposed so that the models w i l l be consistent with the fact that people do not generally perceive ambiguities. in addition  it is an attempt to stay in l i n e with the hypothesis that people parse sentences in linear time. the idea is that the fewer alternatives considered  the faster the parse time should be. the fastest 
way  of course  would be a depth f i r s t parse which made the right choice at every step of the way  hence the interest in deterministic parsers. the attempt to find principles which would guide a parser correctly through a depth-first search  led  kimball  1  to formulate the principles of 
right association and closure.  frazier and fodor  1  propose their own principles: minimal 
attachment 	and 	local 	association. 	in 	addition 
 church  1  proposes the a-over-a early closure principle  and  ford  bresnan  and kaplan  1  propose the principles of lexical preference and final arguments. a l l of these principles try to account for how a top-down depth-first parser could get the preferred readings of sentences. however  the point i would l i k e to make here is that for each choice point in an ambiguous example  there is a second alternative which the parser needs to be able to get at least some of the time. as  crain and steedman  1  point out  the fact that people generally only perceive one reading of a sentence is perfectly consistent with a parsing model which finds a l l the possible syntactic ramifications from looking at a word  does some contextual f i l t e r i n g to decide which alternative s  to keep  and then looks at a new word and repeats. given t h i s   it is not obvious 
that the breadth-first approach is inferior. 
1. a breadth first parser 
　　since  earley's  1  and  pratt's  1  demonstrations of how a parser can work both bottom-up  and top-down  there have been several proposals for how this information might be used to good effect in a psychological model. 
 chester  1  proposes a depth-first left-corner parser which uses top-down information.  martin  church  and patil  1  propose an all paths parser very similar to the one presented here. both of the parsers just mentioned f i t roughly into the framework presented in  kay  1    which allows for intelligent left-corner parsers which can be bottom-up or top-down  or anything in between. the basic idea behind these parsers is that they always have access to two kinds of information: what categories may come next  top-
down   and what word actually is next  bottom-up . the modification suggested here is the incorporation of two look-ahead buffers  and the 
abolishment of all inactive edges. 
1 . 1 . reachability 
we start with a discussion of what  pratt  
1  calls  precedence   and  kay 1  calls  reachability . a category a is reachable from a category r iff there is a derivation tree of r which has a on its left branch. in addition  a 
left corner of a rule  x -  y1  y1 ... yn  is the first element on the right side of the arrow  y1. in the example below  a  b  and c are all reachable from r. 


1. basic algorithm 
     the parser uses a chart as in  kay  1  . the junctures between the words are called vertices and are labelled with numbers. 
     the parser stores information on the vertices in the form of edges. these edges represent partially completed constituents. in kay's terms  they would be  active . each edge contains a category  a completion  and a start vertex. the category t e l l s what the constituent w i l l build up to when the rest of the daughters have been found. the completion  to use a term from  winograd  1   indicates which of the daughters s t i l l need to be found. the start vertex shows the l e f t end of the partial constituent. for instance consider the example below. 

     given that  ducks  can build to an np  and that there is a rule  s -  np vp  in the grammar  the example shows that an edge may be stored at vi going back to v1  and representing an s missing a vp. 
     the algorithm uses three main functions  parse  extend-edges  and new-constit. the function parse looks at the words in the input string one word at a time  considering a l l the ramifications from one word before going on to the next. it keeps track of where it is with the current vertex  the vertex immediately to the right of the newest word looked at. the previous vertex is the vertex immediately before i t . for each word  parse calls new-constit. 
     informally  for words and for each new constituent b u i l t   the procedure new-constit does three things. 1  it builds edges which are incomplete but which have just found their f i r s t daughter. 1  it t r i e s to add the new-constituent as a daughter to other incomplete constituents to i t s l e f t via the function ext end-edges. and 1  for unit productions which can build up the constituent  the procedure new-constit calls i t s e l f recursively. 
     the function extend-edges combines an incomplete constituent with a completed constituent to i t s right. if the result is a completed constituent  it calls new-constit. if the new constituent s t i l l lacks one or more daughters  a new edge is created and stored at the vertex just beyond the word the parser has most recently scanned. an edge is always stored at the vertex representing i t s right end. at the time it is stored this is always the current vertex. newly completed constituents always end at that vertex too. 
when a new constituent is b u i l t that spans the 
j. bear 1 
whole string  it is put into a l i s t of interpretations  not on the chart. only incomplete constituents are stored on the chart. 
1. top-down filtering 
     to incorporate top-down information  the vertices also need to keep track of which symbols could come next  based on the edges stored at a given vertex  and the precomputed information about reachability. given a constituent going from v i to v j   and a l i s t of edges stored at v i   
we can r e s t r i c t the construction of constituents and edges even further. now an edge is only proposed for a rule  if the root of the rule is reachable from a category which is f i r s t on the completion of one of the edges at v i . similarly  a constituent starting at v i is only b u i l t by some unit production if the root of the rule is reachable from a category which is f i r s t on the completion of one of the edges at vi. 

     at least some of the time   men  can build to an np. however in the example in  1   a constituent starting from vi nay only be b u i l t if it w i l l build to an n  or to something which is reachable from n. since an np is presumably not reachable from n  the np  men  does not get b u i l t . 
     so far  the procedure outlined f i t s within the schemata that  kay  1  describes. the next 
modification goes a l i t t l e further. in addition to the machinery already described  two one-word look-ahead buffers are added. now the word to the right of the current vertex is in the w-buffer 
 word-buffer . the other buffer  the m-buffer  meaning-buffer  is i n i t i a l l y empty for each word  and is a receptacle for information about a word's syntactic category or categories. 
     now before the parser decides that it can build a new edge or extend an old one  it checks the completion against the word in the w-buffer. if the word is not reachable from what would be the f i r s t element of the new completion  the edge does not get constructed. if the word in the buffer is syntactically ambiguous  i.e. has more than one possible syntactic category  then the category which allows the edge to be constructed is saved in the m-buffer. so by the time the parser is ready to advance  it has already narrowed down the lexical ambiguity somewhat. if there is s t i l l more than one possible category for the word by the time the parser gets to i t   the new word in the look-ahead buffer could conceivably f i l t e r out a l l the edges which would be proposed for one or more of the undesirable readings. if such is not the case  then the parser must resort to some sort of contextual f i l t e r i n g as alluded to above. 

1 	j. bear 
     the fact that the parser is breadth-first allows the parser to dispense with inactive edges. this turns out to have an interesting effect. although the parser may build constituents that it cannot use in the f i n a l parse tree  it does not keep them around. for instance in the sentence    b i l l likes the woman who jogs   the parser w i l l build up the vp  likes the woman  and then combine it with   b i l l   to form a sentence. if there are no rules in the grammar of the form   x -  s y   though  nothing more happens to the s. it simply never gets saved. 
1. conclusion 
　　the fact that regular languages may be parsed in linear time is due to the fact that for every 
nondeterministic f i n i t e state machine  there is an equivalent deterministic one. or put in terms of grammars  for every regular language there is an unambiguous linear grammar that generates i t . the problem with natural language  though  is that it has ambiguities. one may point out the need  in any model  to resort to contextual information to choose between different alternatives when ambiguity is encountered. to do this however  requires that the different choices exist in the model  i . e .   in terms of f i n i t e state machines it requires that the states of the machine have not been expanded to get r i d of the nondeterminism. in terms of grammars again  even though there exists a linear grammar for any regular language  it is certainly not true that a l l grammars of regular languages are linear. hence  the claim that natural language is regular cannot really be said to account for the fact that people seem to be able to parse natural languages in linear time. one needs to make the stronger claim that linear grammars can adequatly describe natural language. this precludes the existence of any ambiguity at a l l   and seems to be excessive considering the facts. 
     as an alternative to the deterministic parsers that have been proposed  we have suggested a breadth-first parser for context-free languages. it only pursues alternatives which are consistent with information about what has come before  and with what the next word i s . if the grammar taken as a whole specifies that there is only one alternative at some point  even though locally there might be more than one  then the parser only pursues that one alternative. if there is global ambiguity  the parser allows for i t . to account for the observation that ambiguity is usually not perceived  the parser only needs to have access to the same sort of contextual information that depth-first parsers need. 
1. 