 
frame knowledge representation systems lack two important capabilities that prevent them from scaling up to large applications: they do not support fast access to large knowledge bases  kbs   nor do they provide concurrent multiuser access to shared kbs. we describe the design and implementation of a storage subsystem that submerges a database management system  dbms  within a knowledge representation system. the storage subsystem incrementally loads referenced frames from the dbms  and can save to the dbms only those frames that have been updated in a given session. we present experimental results that show our approach to be an improvement over the use of flat files  and that evaluate several variations of our approach. 
1 	introduction 
the negligible impact of frame knowledge representation systems  frss  on the general practice of computing is an embarrassment to the field of artificial intelligence  ai . knowledge representation  kr  researchers have investigated this style of information management for roughly 1 years. although there is a rich history of theory and practice  a plethora of good ideas  and a large number of implemented systems  more than 1 by one count    kr systems have not seen widespread use -either within the ai community  or within the broader computing world  such as the commercial sector . 
　this situation is a pity because frss are superior to conventional database-management systems  dbmss  in a number of respects. they include inference capabilities based on production rules  which have been adopted in dbmss of late   classification  and inheritance. yet superior inference capabilities are not their only advantage  despite the focus of the kr community on inference. some frss support run-time schema alteration  which facilitates the evolution of complex knowledge base  kb  schemas. furthermore  whereas the database community discovered the conceptual benefits of the object-oriented data model only within the past 1 years  frss have used an object-oriented data model since their inception  and the frs object model is richer than that used in object-oriented dbs  consider facets and concept-definition languages . 
　our hypothesis is that inattention to engineering is one of the principal reasons for the failure of frss from a practical perspective. the architecture of frss does not scale up to support rapid storage and retrieval of large kbs  nor does it provide concurrent development of shared kbs by multiple users. nor do frss support the distributed  networked architecture that is the foundation of modern computing. without these capabilities  frss are doomed to remain applicable to building only small systems of at most a few thousand objects. 
our group is reengineering two frss - loom and 
theo - to contain an underlying storage subsystem that provides fast  distributed  multiuser access to large kbs. the storage subsystem is implemented using a 
dbms  but is invisible to the user. we therefore combine the advantageous knowledge-level capabilities of frss with the powerful symbol-level capabilities of dbmss to obtain the best of both worlds. 
　successful engineering relies on both design and experimentation. this paper presents our design of an frs storage subsystem and the results of experiments with a number of variations of that design to assess their relative merits.1 our results show that our architecture is an advance over current systems in several respects. we also present a prefetching strategy that can further improve storage-subsystem performance. 
1 	design of the storage subsystem 
all existing frss process their kbs in data structures that exist entirely in memory  forcing users to read the whole kb into memory from disk before its use. to provide persistence  kbs are written to disk files in their entirety. saving or loading a kb can therefore be an expensive operation  taking time proportional to the size of the kb. an effective cap is placed on the size of a kb by the amount of time that users are willing to wait for save and load operations  with an absolute cap based on the 
　　1 a lack of experimentation within the kr community is another factor behind the limited success of these systems. few publications provide us with any empirical understanding of how frss perform in practice  or of the practical tradeoffs among different frs features. 

size of virtual memory. in a more scalable arrangement  load time and memory usage would be proportional to the number of frames referenced; save time would be proportional to the number of frames updated. 
　our storage subsystem submerges a dbms within an frs. the frs retrieves frames incrementally  on demand  from the dbms. the frs tracks which frames have been modified and transmits those frames back to the dbms during a kb-save operation. given this basic architecture  other choices must be made: how should frs information be organized in the dbms  one of our goals is that the dbms should be invisible to the end user. the user should not be obliged to understand the dbms schema  nor to design a new dbms schema for every new kb. instead  we  as designers of the storage system  must create a generic schema that accommodates all potential frs information. in fact  we have designed and evaluated several such schemas empirically  results omitted here because of space limitations . 
　another choice concerns the granularity at which information is transferred between the dbms and the frs. our goals are for kb loading to take time proportional to the amount of information the application actually references; kb saving should take time proportional to the number of frames updated. the simplest mechanism that satisfies these constraints is to transfer a single frame from the dbms to the frs when the user application references a frame that is not currently in memory  analogous to page faulting in operating systems . we have also explored the transferring of several frames together. 
　which type of dbms is best suited to the role of a frame storage system  our storage-subsystem design does not place many requirements on the underlying dbms. the requirements are that frame definitions must be stored as uninterpreted collections of bytes  that frame definitions be saved and fetched in their entirety  and that only a few interframe relationships are stored explicitly in the dbms  such as the class-instance relationship. given the maturity of relational technology  it was logical to consider using a commercial relational dbms  rdbms  for persistent storage. however  the simple nature of our storage requirements led us to also consider using a low-level object manager. our reasoning is that a low-level storage manager might be more efficient than a sophisticated commercial rdbms. by using a small group of pertinent features  we might be able to avoid the overhead of a general-purpose system. we have experimented with both a commercial rdbms  and an extensible storage management system  exodus  from the university of wisconsin . here  we compare the performance results for exodus with previous results obtained using the rdbms . 
　the exodus storage manager is a flexible  low-level system intended for use as the foundation for domainspecific information management systems. unlike a full database system  it does not provide advanced features such as high-level schema creation and manipulation operations  or query specification and optimization. it is accessed directly through a library of client interface routines  rather than through a declarative query language 
1 	knowledge base technology 
such as sql. we thought that its simplicity  and consequent efficiency  might be well-suited for our project  where there is only one application  the frs   and the objects to be stored are relatively straightforward. 
1 storage subsystem implementation 
this section first provides an overview of the frame structures that loom and theo employ  and then discusses the architecture of the storage subsystem  and its interactions with loom and theo. 
1 	loom structures and operation 
a loom kb contains three types of frames: concepts  instances  and relations  we have simplified the description of loom for expository purposes . a concept  or class  consists of a name and a definition. the concept definition is a set of necessary and sufficient conditions that an instance must meet to be an instance of the concept. given this information  the loom classifier arranges all concepts into a subsumption  generalization  hierarchy. a loom relation  not to be confused with the database definition of a relation  is a kb-wide specification of the properties of a slot  such as its domain and range. 
　instances have one or more parent concepts and some set of slot  attribute  values. based on these characteristics  the loom classifier can infer the concepts to which the instance belongs. loom can perform both forward- and backward-chaining classification-based inference. all our tests and experiments have used loom's backward-chaining mode. we believe our system would also work with the forward-chaining mode. however  to make the required inferences  creation or modification of a single frame could trigger a large number of frame faults by loom's classifier  which could hurt performance. we do not currently support loom's production-rule inference. 
1 	theo structures and operation 
theo shares many characteristics with loom. theo frames are arranged in a generalization hierarchy  and theo frames consist of slots that contain values. however  theo classes do not have associated definitions  and theo does not perform classification. given the basic structural similarity of loom and theo  it is natural to develop a storage system that can serve both frss. 
　for simplicity  the remainder of our discussion usually mentions loom only. statements we make about the interaction of loom with our storage system also apply to theo except where we state otherwise. 
1 	exodus schema 
we have designed an organization of exodus storage structures that can simultaneously store multiple frame kbs. the organization for a sample kb is shown in figure 1. each kb is represented by two exodus files and six exodus indexes. an exodus file is a collection of objects; any object can be retrieved quickly given its ob-
ject id  oid  - an integer. an exodus index allows fast retrieval of a datum given a key. for example  the frames index in figure 1 allows us to map a symbolic frame name to its oid. each frame is stored as a single object in the frames file. a frame object contains the frame's body  an ascii string that encodes all information required to recreate the frame  and type. two types of frame are supported: classes and instances. loom relations are stored in a separate relations file. 
　the frames and relations indexes relate frame names to their corresponding oids. relationships among classes and instances are maintained in the other four indexes. the supers and subs indexes relate class names to their superclasses and subclasses  respectively. the instances and classes indexes relate classes to their instances and instances to their parent classes. our approach therefore stores the taxonomic hierarchy for a kb persistently. when the hierarchy changes  our storage system will store these changes persistently. this storage organization allows fast retrieval of individual frames by name  as well as retrieval of portions of the taxonomic hierarchy. 
　each exodus volume has a special root entry area  in which meta-information about a kb is stored. this information consists of handles for the two exodus files and six indexes  and meta-information about the kb that is used by the application. an application can have several kbs open simultaneously. the exodus client interface  eci  maintains a table of open kbs. 
1 	frame faulting 
users employ both function-call interfaces and declarative languages to manipulate loom kbs. these operations  such as retrieving or altering the value of a slot within a particular frame  generate frame references. loom resolves these references by searching internal tables that associate frame names with the data structures that implement frames. a frame fault occurs when an application  or loom itself  references a frame f that is not in memory. we have modified loom to call our storage system when a frame fault occurs. the storage subsystem faults a frame into memory by retrieving its body from the dbms server. because the eci is networked  multiple users can access and update the same kb in a distributed  but uncoordinated  fashion.  our future work will investigate methods of controlling multiple updates to a shared kb.  
　after retrieving the body of f from the dbms  the storage subsystem calls standard loom functions to create f within the loom kb. this process is complicated by the fact that most frames are related  connected  to other frames in the kb. for example  a concept is related to its superconcepts  subconcepts  and instances. an instance will contain references to its parent concepts  and possibly to other instances serving as fillers of its slots. loom normally expects all of these other frames  called the context of f  to be present in memory. because it can be expensive to fault in the entire context of f  we load in as small a portion of the context as possible. 
　part of the context is the ancestor frames of f. when processing a fault to frame f  we first fault in every direct parent of f that is not currently in memory  references to the parents of these parents are generated recursively . therefore  all parents of f are loaded before f is defined. 
　the second part of the context is those frames referred to by f. the loom frame data structures implement such interframe references as lisp pointers. imagine that f refers to a frame g that has not yet been faulted into memory; therefore no pointer to g can be defined. one solution to this problem is to fault g into memory - a solution we reject because when applied recursively it could conceivably cause the entire kb to be faulted in. the solution we chose is to create a stub object  a placeholder for g   to which a pointer can be created. if g is later faulted in  the storage subsystem replaces the stub in a manner that retains the validity of existing pointers. see  for more details on frame faulting and stub management. 
1 	performance experiments 
the goal of the experiments discussed herein was to measure storage system performance as a function of knowledge base size. we therefore generated a series of random kbs  identical in every respect except number of instances. each kb had 1 concepts  all primitive  with just one super each. instances averaged five slots apiece  with an average of two fillers per slot. half the slots were filled by integers  and the other half were filled by symbols. these parameters were chosen because they approximate the characteristics of socap  the transportation-planning kb that is driving our work with loom . knowledge bases were generated with 1  1  1  1  and 1 instances. for comparison  the same set of kbs was generated and saved to native loom flat files  to native theo flat files  to the dbms  both loom and theo versions   and to exodus  both loom and theo versions . these variations of five kbs form the basis for our experiments. 
　experiments were run using loom 1 and the february 1 version of theo  running on lucid common lisp 1.1. both the frs and the dbms server were running on the same workstation  a sparcstation1 model 1 with 1 mb of physical memory. lisp was restarted before every trial  to avoid caching effects  and a garbage collection was executed immediately before timing. each trial was repeated three times  and the results averaged  repetitions typically varied by less than 
1% . overall elapsed times were measured using the lisp time function. we measured the time spent in 
loom  theo  the eci  and the storage subsystem by monitoring key procedures using the cmu monitoring package. the cpu time spent in the dbms server process was measured using the unix ps utility to observe total cpu time before and after each experiment. 
　figure 1 shows the time required to reference some number of randomly chosen instances from kbs of different sizes for both loom and theo. each reference faults in at least one frame from the dbms  when the parent classes of an instance are not memory resident  they are also faulted in . each dashed line in these 
   1  all product names mentioned in this paper are the trademarks of their respective holders. 

figure 1: comparison of kb loading times for theo and for loom. the solid line shows the time required to load entire 
kbs of various sizes from flat files. the dashed lines show times required to fault frames from exodus in response to references to instances in kbs of various sizes. 
graphs shows the time required to reference n instances in kbs of different sizes. for example  the highest line in each graph shows the time required to reference 1 instances from kbs containing a total of 1  1  and 1 instances. figure 1 a  shows that for theo  the time required to reference 1 instances from a kb containing 1 total instances is about the same as the time required to load that kb in its entirety from a flat file. 
　figure 1 breaks down the total time spent processing frame faults into several components: the time spent in the exodus server  the eci  our storage system  the frs  loom or theo   and other processing  presumably i/o   as a function of the number of instances referenced for a fixed kb of 1 instances. 
　the next experiment measured the time required to save updates to some number of randomly chosen instances from kbs of various sizes. to be consistent with traditional loom behavior  updates are not written as they occur. rather  we wait until the user issues a com-
1 	knowledge base technology 
mand to save updates  and then all are written at once in a single transaction. selected results for loom are shown in figure 1. for comparison  we include the time to save kbs of varying sizes to loom flat files  the time is constant for a given kb regardless of the number of frames updated in that kb . kb save times for theo  not shown  are similar. 
1 	discussion 
our experiments answer several questions: does the performance of our dbms-based storage subsystem meet the goal of linear time as a function of number of frames referenced and number of updates stored  if so  is its speed fast enough to make the storage system usable in practice  and how do the different components of the storage subsystem such as the dbms server contribute to its overall performance  
　figure 1 demonstrates that our architecture achieves the linearity goal: the time spent loading frames is a linear function of the number of frames referenced. figure 1 


figure 1: the total elapsed time for referencing and faulting n instances into memory from an exodus kb of 1 instances is broken down into several components. the vertical distances between lines represent  starting at the bottom  time spent in the frs  in our storage subsystem  sss   in the eci  and in the exodus server. 

shows that frame loading time also depends to a small extent on kb size when a fixed number of instances are referenced. this dependency most likely occurs because  a  the parents of any referenced instance are faulted in along with the instance  if not already in memory   and  b  when a class is faulted in  the names of all its instances are also retrieved from the database. since our experimental kbs contained a fixed number of classes  the number of instances per class increases in proportion to kb size  requiring a greater amount of data to be retrieved per class for large kbs. 
　figure 1 lets us evaluate the relative merits of loading frames from the dbms versus loading from fiat files. for theo  loading n instances from the dbms is 1 to 
1 times slower than loading an entire kb of n instances from a flat file.1 for loom  which must perform expensive classification operations on newly loaded frames  loading n instances from the dbms is 1 times slower than loading that kb from a flat file. therefore  the performance of the dbms storage subsystem is on a par with a flat file when a user references up to 1% of the frames in a theo kb or 1% of the frames in a loom kb in a given session. we believe that the performance of the storage subsystem is acceptable in practice  given our assumption that as kb size grows  users will reference only a fraction of its frames in a given session. 
our experiments  data not included  show that our 
   1 retrieving n bytes from the dbms incurs a much higher overhead than retrieving n bytes from a disk file due to factors such as query processing  network delays  buffer management  etc. 1 in fact these classification operations are unnecessary since the dbms already stores the results of previous classifications of these frames. we will consult with the loom developers about how to quickly insert into a kb frames with known subsumption relationships. 
exodus-based architecture achieves the goal of saving kb changes in time linear in the number of updates. saving n updated frames to the exodus storage manager is roughly 1 times slower than saving an entire kb of n frames to a flat file. therefore  our storage subsystem for exodus is faster than the flat file for saving information when less than 1% of the kb has been changed. 
　an earlier paper  describes the results of timing experiments using a relational dbms in place of exodus. the outcome of the timing experiments with exodus reinforces the earlier results. in fact  not only are the shapes of the graphs similar  but so are the absolute values of the data points. although there are minor differences  the bottom line is that we found the difference in performance between exodus and the rdbms to be minimal. however  the rdbms is much easier to work with from a practical point of view  because sql provides a much higher level of interaction than does the extensive c++ programming necessary to interact with exodus. therefore  we have chosen to use the rdbms for our future work.1 
　another advantage of the rdbms declarative query language is its potential for evaluating complex kb queries within the dbms. the rdbms schema pre-
sented in  precludes such an approach because  like the schema presented in figure 1  every frame is an uninterpreted blob within the dbms. in subsequent work we have designed a more complex schema that makes individual slot values accessible to sql. using that schema we are able to index kb slots to support fast answers 
   1  note added in proof: recent optimizations to the rdbms storage subsystem have improved its performance substantially; the rdbms is now faster than flat files when up to 1%  rather than 1%  of the kb is referenced in a session. 

to declarative kb queries. the problem with this slotbased schema is that frame faulting is much slower because a number of dbms queries are required to retrieve all the slots of each frame. by combining the framebased schema with the slot-based schema we get the best of both worlds: we use the frame-based schema for faulting frames  and the slot-based schema for query processing. this approach does require redundant storage  and slows down the save operation. but if it is known in advance which slots will be queried  only those slots need be stored in the slot-based schema. note that many frss support no indexing whatsoever of their virtual memory data structures  except that provided by the taxonomic hierarchy  loom does support such indexing . 
　1 prefetching 
can the delays imposed by demand faulting of frames be decreased by prefetching frames that are likely to be referenced in the future  our current system does not consider memory to be the limiting resource - we assume that all kbs can fit entirely in virtual memory. our main concern is decreasing the time spent faulting frames. since we never discard a frame once it is faulted into memory 1 if a prefetched frame is ever referenced  then the prefetch did eliminate a demand fetch. thus  prefetching has a much greater chance of success in our system than it does in  for example  page management by an operating system  in which a page must be discarded for every page prefetched. 
　prefetching might improve performance in three different ways. if the application has idle time  e.g.  waiting for user input or disk i/o   any useful work that the storage subsystem can do during these periods will eliminate demand fetches. making effective use of idle time offers great potential for performance speedups  particularly 
　1 we expect to remove this restriction in future work so that kb size is not limited by virtual memory. 
1 	knowledge base technology 
in the situation in which a user is interactively browsing or editing a kb. however  such speedups will vary from application to application  making them difficult to evaluate experimentally. 
　second  if we can bundle a request for several frames into a single dbms query  we might decrease the overhead involved in query processing and data transmission  compared to that of fetching each frame individually. we performed experiments to determine the cost of fetching frames from the database as a function of the number of frames fetched at a time. we retrieved 1 socap frames  1 concepts and 1 instances  and 1 random kb frames  1 concepts and 1 instances   varying the fetch granularity from one frame at a time to 1 frames at a time. the average elapsed times per concept and per instance are shown as a function of the number of frames fetched at a time in figure 1. each value is an average of five trials. 
　for both kbs  and for both concepts and instances  as the number of frames fetched at a time increases  the time per frame drops sharply. at its minimum  the time per frame is one half that required to fetch frames individually. these results suggest that so long as 1 to 1 frames are fetched at a time  and more than half of the prefetched frames are actually referenced by the application  there will be a net gain in performance. 
　finally  if the dbms server is on a different machine than is loom  we might find an arrangement where both machines work in parallel.  parallelism cannot be obtained with demand fetching  since we can't continue with processing until the query has completed.  
　the prefetching scheme uses all three of the above strategies. the first two strategies are implemented for loom in conjunction with the rdbms. 
1 	implementation of prefetching 
one problem with prefetching is that fetching and loading a frame into loom requires a significant amount of 

computation on the local machine  for example  for classification. to allow this local processing and other user computation to occur in parallel with processing by the dbms server  we divide frame fetching into two components: that part associated with retrieving data from the database and that part spent inserting the frame into the loom kb. the majority of the time involved in retrieving data from the database is spent on the dbms server or in communication. thus  we can perform data retrieval  dr  in parallel with local processing without significantly impacting local performance. the framedefining  fd  task is performed locally  so we invoke it only when a frame is demanded or when the user process is idle. the dr component can obtain multiple frames with a single query  even though the fd component must define them one at a time. thus  this scheme allows us to maximize parallelism at minimal cost to the main  user  process  to retrieve data for multiple frames in a single query  and to use extra local cpu cycles when the main process is idle. 
　our implementation uses the lucid common lisp multitasking facility to define two processes  a dr process and an fd process  in addition to the main process. the dr process runs with the same priority as the main process  i.e.  they time-share . it chooses a frame or set of frames to retrieve  either a demanded frame or frames from a prefetch queue   initiates the appropriate dbms queries  organizes the resulting bodies  and either returns them  if required as part of a demand fetch  or adds the body of each frame to a hash table for storage until needed  if a prefetch operation . the fd process runs at a lower priority  so it runs only when the other processes block  as in the case of a demand fetch  or are idle. it chooses a frame to define  either one that has been demanded  or one from the prefetch queue   gets the frame body either from the above hash table or by requesting that dr process fetch it  and invokes procedures to define the loom frame. 
　figure 1 shows the interaction of the three processes and associated data structures. on a frame fault  the main process issues a request to the fd process to create the frame. the fd process first looks for the body in the hash table  and  if unsuccessful  asks the dr process to query the database. as the frame is being created  any unloaded frames that it references are added to the dr prefetch queue. when the dr process runs  it checks the dr prefetch queue for frames to prefetch  fetches and adds them to the hash table  and moves the frame references to the fd prefetch queue. when both other processes are idle or blocked  the fd process checks the fd prefetch queue for frames to define  obtains their bodies from the hash table  and creates the loom frames. 
1 	prefetching strategy 
an important decision concerns which frames to prefetch. in most cases  there is no way of knowing which frames will be referenced in the future.  there exist situations in which frame references are known in advance  in which case the application may inform the system of which frames to prefetch.  the principle of locality suggests that the frames most likely to be referenced in the future will be related to those referenced most recently. we consider three types of frame relationships: a frame can fill a slot in another frame  a frame can be a subconcept of another concept  and a frame can be an instance of a concept. 
　we decided against prefetching all instances of a referenced concept because in large kbs we expect many concepts to have large numbers of instances. in this case  the probability of prefetching the right instances is small. 
our first choice is to prefetch subconcepts of recently 
	karp and paley 	1 

retrieved concepts.1 the reason for this choice goes beyond the principle of locality to our intuition that concept frames are more likely to be needed than instance frames  because any reference to an instance frame also requires that its parent concepts be in memory. the probability of referencing a concept frame is the sum of the probabilities of referencing each of its subconcepts and instances. therefore  any concept frame in the current region of interest of the kb hierarchy is a good candidate for prefetching. when there are no more subconcept frames to prefetch  we prefetch slot-filler frames. 
　our initial experiments indicate that prefetching does improve performance in some cases  but our evaluation of prefetching is not yet complete. 
1 related work 
keeconnection couples the kee frs with a relational dbms  l  and the intelligent database interface  idi  couples loom with a relational dbms . in both systems the dbms and frs are loosely coupled peers. the advantage of this architecture is that it allows existing information from a database to be imported into an ai environment. its drawback is that the storage capabilities of loom are not enhanced transparently  as in our approach. users of keeconnection  and of the idi  must define mappings between class frames and tables in the rdbms; keeconnection creates frame instances from analogously structured tuples stored in the rdbms  and can store instance frames out to the dbms. however  only slot values in instance frames can be transferred to the database - class frames cannot be persistently stored using database techniques and cannot be accessed by multiple users. our approach allows all information in a loom kb to be permanently stored in the dbms. groups at ibm and at mcc have coupled frss to object-oriented dbmss  1; 1 . the ibm effort differs from our approach in that a kb is read from the dbms in its entirety when it is opened by a k-rep user  which we believe will be unacceptably slow for large kbs. 
　none of these researchers have published experimental investigations of alternative implementations  as we are doing. without systematic experiments it is impossible to evaluate the relative merits of their architectures. 
1 summary 
an frs that performs demand loading of referenced frames  combined with incremental saving of updated frames  will scale to large kbs much more gracefully than the current generation of frss. we presented an architecture for an frs storage subsystem that submerges a dbms within the frs in a manner that is transparent to the frs user. our experimental results with a prototype implementation show that this coupling performs well in practice  and that its performance is linear in the number of frames referenced or updated  as re-
　　1  note that although the principle of locality applies to recently referenced frames  we are using it only for recently fetched frames because the overhead of recording related frames is too high to invoke on every frame reference. 
1 	knowledge base technology 
quired. we have also presented a prefetching mechanism that will improve performance in certain situations. 
　our future work will investigate means of controlling multiuser access to shared kbs. 
acknowledgments 
　ira greenberg implemented the exodus-based storage subsystem. we are grateful to bob macgregor and other members of the loom group at isi for discussions of loom internals  answers to questions  and prompt bug fixes. we are also grateful to tom mitchell for supplying theo. this work was supported by rome laboratory contract no. f1-c-1  and by grant r1-lm-1a1 from nih. the contents of this article are solely the responsibility of the authors and do not necessarily represent the official views of the advanced research projects agency or of the nationalstitutes of health. 
