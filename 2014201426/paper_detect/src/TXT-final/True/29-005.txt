 
in this paper  we propose a convergent reinforcement learning algorithm for solving optimal control problems for which the state space and the time are continuous variables. 
the problem of computing a good approximation of the value function  which is essential because this provides the optimal control  is a difficult task in the continuous case. indeed  as it has been pointed out by several authors  the use of parameterized functions such as neural networks for approximating the value function may produce very bad results and even diverge. in fact  we show that classical algorithms  like q-learning  used with a simple look-up table built on a regular grid  may fail to converge. the main reason is that the discretization of the state space implies a lost of the markov property even for deterministic continuous processes. 
we propose to approximate the value function with a convergent numerical scheme based on a finite difference approximation of the hamilton-jacobi-bellman equation. then we present a model-free reinforcement learning algorithrn  called finite difference reinforcement learning  and prove its convergence to the value function of the continuous problem. 
1 	introduction 
this paper is concerned with convergence results of reinforcement learning  rl  algorithms in the continuoustime  continuous-state-space case. we discuss the problem of the necessary discretization of the state space and propose a rl algorithm that converges to the optimal solution. 
   'dassault-aviation  dgt-dtn-el-et. avancees  1 quai marcel dassault  1 saint-cloud  france 
1 	learning 
¡¡the objective of rl is to find -thanks to a reinforcement signal- an optimal strategy for solving a dynamical control problem  such as target or obstacle problems  viability or optimization problems. the technique used belongs to the dynamic programming  dp  methods which define an optimal feed-back control by approximating the value function  vf   which is the best expected cumulative reinforcement as a function of initial state. 
¡¡in the continuous case  the vf has to be represented  with a function approximator with a finite number of parameters. several author have pointed out that the combination of rl algorithms with general approximation systems  such as neural networks  fuzzy sets  polynomial approximators  etc.  may produce unstable or divergent results even for very simple problems  see  boyan and moore  1    baird  1    gordon  1  . here we show that classical rl algorithms  like q-learning  see  watkins  1    used with a look-up table built from a simple discretization of the state space may produce a very bad approximation of the value function. the main reason is that the discretization of a deterministic continuous process is not markovian. so algorithms such as q-learning which estimate the value of a state as an average of the values of successive states according to their occurrence will not converge. we propose as an alternative an algorithm that averages the values of the next states according to the state dynamics. 
¡¡section 1 proposes a formalism for optimal control problems in the continuous case. the vf is introduced and the hamilton-jacobi-bellman  hjb  equation is stated. section 1 discusses the lost of the markov property with the discretization of the state space and studies the q-learning algorithm with a look-up table. section 1 describes the discretization of the hjb equation by a finite difference  fd  method  which leads to a dp equation for a finite markov decision process  mdp  and whose solution approximates the vf. section 1 presents the algorithm  called finite difference reinforcement learning  fdrl   that converges to the 


	munos 	1 

1 	learning 

	munos 	1 

	1 	learning 

