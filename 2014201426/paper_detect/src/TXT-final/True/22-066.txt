 
the transformation of constructive program synthesis proofs is discussed and compared with the more traditional approaches to program transformation. an example system for adapting programs to special situations by transforming constructive synthesis proofs has been reconstructed and is compared with the original implementation  goad  1b  goad  1a . a brief account of more general proof transformation applications is also presented. 
1 introduction 
a current dilemma emerging in the field of computer science  and hence also artificial intelligence  is that demands for quantity and complexity of software are outstripping the tools currently available. a solution to this problem is offered by the field of automatic programming. this can be broken down into three main  interelated  sub-fields: 
  the automatic generation  synthesis  of programs from specifications  input-output relations . 
  the automatic verification that a program meets its specification. 
  the automatic transformation of inefficient programs into more efficient programs with the same specification. 
so by tackling these issues  software reliability can be improved  provided that it is easier to write bug-free specifications than bug-free programs. 
for several years the mathematical reasoning group  
mrg  in the department of artificial intelligence  under the direction of prof a bundy  have undertaken a great deal of research into the field of automatic programming  bundy et a/.  1  bundy et a/.  1  bundy  1 . the first two issues above have been successfully tackled within the oyster proof refinement environment:1 by using logic programming and constructive logic  the task of generating programs is treated as the task of proving 
    this research is supported by serc grant gr/d/1 and a studentship to the author. thanks to alan s ma ill and alan bundy for their help. 
1
　　 oyster is the edinburgh prolog implementation of nuprl; version ''nu'' of the proof refinement logic system originally developed at cornell  horn  1  constable et a/.  1 . 
a theorem. hence knowledge of theorem proving  and in particular automatic proof guidance techniques  are used  section ♀ . 
　the third issue  that of program transformation  is the latest to be tackled by the mrg and forms the main contextual subject of this paper: the automatic transformation of programs by transforming their synthesis 
proofs  sections 1 and 1 -
　this research involved developing transformation techniques which increase the efficiency of the original program  the source  by transforming its synthesis proof into one  the target  which yields a computationally more efficient algorithm. this process is known as program optimization section 1 
　the same basic techniques where also used in a process known as specialization whereby the task  the inputoutput relation  of the source program is either altered or adapted to a special situation. 
　as an example of the proof transformation approach to program optimization we will concentrate on this specialization process since it represents a well circumscribed  and fully implemented  sub-domain of the proof transformation field  section s . 
　however  a brief account of the more general proof transformation approach to program optimization is provided to set the specialization research in context  section 1 - for a detailed account of the proof transformation research see  madden  1c . 
1 the environment 
within oyster  a program specification  comprising the desired input-output relationship  is represented by a statement in constructive logic  specifically  martin-lof type theory. 
　the martin-lof type theory is a constructive  higher order  typed logic. this is especially suitable for the task of program synthesis since executable code is built up as the proof is constructed such that all elements of the former have a one to one correspondence with elements of the latter  the vice-versa is not true  proofs contain additional information: section 1 . 
　by finding a constructive proof of the program specification we can routinely extract an algorithm from the proof which satisfies the desired input-output relation. hence the techniques of theorem proving can be brought to bear on the program synthesis and transformation domains: 
　if we represent the program specification as specification input output  and let the symbols v and 1 rep-
	madden 	1 

　oyster reasons backwards from the theorem to be proved using a sequent calculus notation  which includes rules of inference for mathematical induction. the search for a proof must be guided either by a human user or by a prolog program called a tactic. the process of applying the tactics to a proof specification will yield sub-goals to which further tactics may be applied. the proof terminates  or is complete  when the application of tactics produces no further sub-goals. this process is known as proof refinement  the rules which apply the tactics are the refinement rules  constable  1 . 
　the oyster system has an open ended variety of such program synthesizing tactics. these tactics control the application of the object-level knowledge  such as rewrite rules  axioms and definitions. the idea is that they embody heuristic knowledge about theorem proving in constructive logic. as such  the program synthesizing tactics perform meta-level inference. 
1 	transforming proofs 
the field of program transformation is not a new one. considerable research has been done on the transformation of programming languages of one form or another  for example: darlington and burstall have designed a research tool for the development of functional program transformation methodologies  burstall and darlington  1  darlington  1 . tamaki and sato have carried out similar research except they are concerned with the transformation of logic programs  tamaki and t.sato  1 . grant and zang have developed heuristics for the automatic transformation of prolog programs  grant and zhang  1 . 
　however  all these prior approaches are concerned with the direct transformation of executable code and not with the transformation of constructive proof structures. the latter approach has considerable advantages: 
  intuitively  a proof will contain more information than the program which it specifies since a program need contain no more information than that required for execution. a proof  on the other hand  will contain the thinking behind the program design. 
  furthermore  the logical structure of a proof is better understood than that of a program. see  for example   kreisel  1  . indeed  much of the proving power of the oyster system is due to the incorporation of extensive knowledge of the structure of inductive proofs  which are responsible for synthesizing recursive programs  see section 1 and  boyer and moore  1  . 
  the exploitation of proof theory for the synthesis of programs is a fairly well established field  kowalski  1  kreisel  1  and the proof transformation approach is not limited to the oyster system  cf.  goad  1b  . however  this particular environment does combine the important features that both synthesis and target languages share the same 
1 	automated deduction 
formal language and that this language  martin-lof type theory  is constructive in nature. these features mean that synthesis and  specifically  transformation can be treated uniformly  henceforth referred to as the proof-program uniformity: for each transformation operation performed on a synthesis proof there will be a one-to-one corresponding transformation in the target program language. 
  the proof-program uniformity prevents the proliferation of complexity that occurs when the specification and implementation languages are different. 
  unlike the proof refinement environment  usual pro-gram transformations do not have a specification present  so transformations have to be restricted to those that preserve input/output behaviour. proof transformation is not so restricted. 
a more thorough comparison between  on the one hand  the traditional approaches to program transformation and  on the other  the transformation of constructive synthesis proofs can be found in  madden  1b . 
1 adapting algorithms 
one researcher who is concerned with transforming proof structures as opposed to transforming more conventional descriptions of algorithms  be they functional or in some logic programming formalism  is c.a.goad  goad  1b . information contained in proofs  which goes beyond that needed for simple execution  is exploited in the adaptation of algorithms to special situations  for example  adapting a sorting algorithm or a bin-packing algorithm to operate  with maximum efficiency  on input lists of a specific length . goad's system has been successfully reconstructed  and extended  in the oyster proof refinement environment and subjected to test on a number of examples  madden  1c . 
　the main feature of this specialization system is an open-ended set of pruning transformation operators which are guaranteed to reduce the size of the source proof and corresponding source algorithm. the pruning mechanism  which need not be limited to specialization  is designed to remove those branches from the proof tree which result in redundant computation. 
1 	specialization and p r u n i n g . 
the reconstructed specialization/pruning processes  and their explanation  differ from goad's in that information gleaned from the constructive existence proof is exploited  during transformation  on that proof itself. this is as opposed to transforming the algorithm extracted from the proof. 
　there are three distinct stages to the transformation of the proof in its adaption to a special situation: 
1. specialization amounts to the partial evaluation of a constructive existence proof. specialization alone  i.e. with no subsequent pruning  can increase the efficiency of the resulting algorithm  especially where induction schemata are involved  seesection 1. 
1. following specialization  the first stage of pruning may be attempted: normalization. normalization is designed to  
 a  remove certain branches from the specialized proof tree which will never be satisfied under 
　
the particular constraints  instantiations  of the desired adaption  partial evaluation   
 b  set up the proof for the second stage of pruning: 
1. dependency pruning is designed to remove those branches from the specialized and normalized proof tree which result in redundant computation. this pruning is guided by a kind of dependency information which does not appear in ordinary programs  kreisel  1 . such redundancies will also not be present  or only implicitly/potentially so  in the original proof. 
a few additional points are worth mentioning at the outset: 
  the partial evaluation can be done on an incom-plete proof with unproved lemmas without compromising the computational usefulness of the proof as a whole. 
  although specialization followed by pruning is not guaranteed to increase efficiency  it will do so most of the time simply because its purpose is to tailor algorithms  or proofs  to a specific task  or rather to a specific class of input. pruning is  however  guaranteed to reduce the size of the algorithm. 
  pruning is guaranteed to preserve the validity of an algorithm for the specification embodied in the root node of the proof describing the algorithm. that is  given the constructive proof and a partial evaluation  pruning is guaranteed to prune only the computationally redundant parts of the proof tree without effecting the input/output relation which specifies that proof. 
  conventional computational descriptions  such as the conditional form or some logic programming description  are not subject to the pruning transformations. this is because any valid transformations on conventional descriptions must preserve extensional meaning since they only contain information about the function to be computed. a nice example of the benefits of proof transformation as opposed to program transformation. 

simplified  by the use of dependency pruning  to the expression; 
   1   x + 1  
　this is because the constructive existence proof will contain a case analysis whereby the case split is dependent on the size of x. now  the fact that  x + 1  is an upper bound for both  x + 1  and 1 x x does not depend on x being less than one. this dependency information is contained in the proof and  via partial evaluation and pruning  allows the removal of the  computationally redundant'' case split according to the size of x. note that  1  and  1  are different functions  eg different input/output behaviour is observed for x = 1 . however  as far as the partial evaluation  specialization  is concerned  in this case y being set to 1  subsequent normalization will preserve input/output behaviour. in other words whilst normalization will transform the algorithm  reducing its size  it is guaranteed to preserve the input/output behaviour. dependency pruning  on the other hand  may change both the algorithm and the function. 
　this is a major advantage of the oyster transformation system since usual program transformation do not have a specification present  so transformations have to be restricted to those that preserve input/output behaviour. 
　for practical purposes  dependency pruning should hence be employed where the desired output is more qualitative than quantitative. by a qualitative output we mean some specific result which may be achieved regardless of the algorithms behaviour such as bin-packing or sorting. for ease of explanation we do  in fact  prune the upper-bound algorithm  and consequently alter the function. 1 
　the representation of the above 1 conditional expressions as constructive existence proofs allows our reconstruction to do the corresponding transformations  
l = specialization=  1 = normalization=  
1 = dependency 	pruning=  1   
completely automatically. 
1.1 	relations between the source and target specifications 
　to be safe  specialized proofs should be sound where as pruned proofs need not be since we are concerned only with computational output. however the source proof  the specialized target  the normalized target and the fully pruned target proof should all satisfy that inputoutput relation formalised in the source specification: the source specification will be an over generalization as far as the target is concerned. the converse does not apply; the source proof will not satisfy the specialized  normalized or fully pruned target proof specifications. 
　the input-output specification for many synthesis proofs may well be very much under specified  for example  it may simply state that from an input list of integers the output will be an integer. such a specification is generally employed in oyster when we wish to do a program synthesis  with the onus on the user to construct the proof  as opposed to program verification. at 
1
　　the upper-bound algorithm is a convenient example which has neatly nested case analysis which allows us to demonstrate the full specialization and pruning transformation procedures. 
	madden 	1 
　
such a level of generality there is ample scope for specialization but the subsequent application of pruning  if possible  would imply that the prior synthesis had not been done properly; one would not expect to build in much redundancy when synthesizing an algorithm from a very general specification. 
　however  if the specification is far more concrete  in particular  if it states certain conditions on the input and output  and not merely of what type they are  then we may expect that upon specialization  partial evaluation  certain of these conditions will never  or need never  be satisfied  be redundant . hence pruning becomes a more viable option where proofs satisfying  more or less  complete specifications are concerned. 
1 	different approaches of the reconstruction 
there is far less structure required for goad's natural deduction proofs than for the oyster sequent calculus counterparts. for example  if we have a case split in goad's system then we do not have to provide a decision procedure for the case in order to complete the proof. this does not effect the fact that the prawitz natural deduction system  employed by goad  is rich enough in content to represent the dependency information. however  unless the prawitz system is explicitly given a decision procedure in the actual proof specification then the proof itself cannot be transformed in order to adapt the corresponding algorithm. more generally  goad is prevented from performing transformations on the proof itself because the proof lacks in  computational content . what he is therefore forced to do is transform the extract-term. 
　the greater  computational content* of oyster proofs means that we transform the proof rather than the extract. this approach has certain advantages: 
1.1 	differences w i t h respect to specialization: 
  emphasis is placed on specialization  as well as any subsequent pruning  as a means to increase the efficiency of an algorithm  in particular  any recursive behaviour . the specialization process when applied to oyster proofs containing induction schemata  greatly decreases the computational effort required to unfold that schema. 
  specialization is particularly suited to those in-stances of oyster proof synthesis where top level goals are under specified  i.e have little computational content  expressing only the input/output relation between types . this difference is as a consequence of the differences between the environments discussed in section 1. 
  furthermore  and of particular importance within such applications as specialization and partial evaluation  the proof environment entails that transformations are not restricted to preserving input/output behaviour because proof specifications themselves can undergo transformation. 
  in goad's system information gained by the special-ization of the proof is used to guide the pruning of the proof extract. as far as the reconstruction is concerned  normalization can be performed on both the proof or extract and dependency pruning is performed on the proof. this approach has considerable advantages  see next section 1.1 . 
1 	automated deduction 
1.1 	differences w i t h respect to pruning normalization 
　as alluded to above  goad in fact deploys his pruning on the extract term of his proof. what goad is not doing is transforming the proof tree. this does not make the  proofs over programs  approach to transformation a red herring in goad's paper since it is only due to the information contained in the  partially evaluated  proof that the dependency information is made explicit. so what goad is doing is using information in the proof to transform the extract as opposed to our approach of transforming the proof itself. 
dependency p r u n i n g 
　goad also applies the dependency pruning to the extract terms of his proofs and what was said above applies here to. 
　fortunately for goad  within the system he describes the uninstantiated variable of a redundant case split does appear in the extract hence signaling the appearance of redundant code. 
　the oyster extraction process is a bit smarter; it hides anything which does not require instantiating in order to compute the  partially evaluated  output. this makes pruning the extract somewhat difficult in practice but not in principle  if oyster were really smart it would prune what it hides and complete the proof accordingly . 
　in the reconstruction it is the proof tree which is both automatically specialized  normalized and pruned. 
w h y prune the proof rather than the extract  
　there are three main reasons why it is preferable to use the dependency information contained in the specialized  and normalized  proof to transform that proof itself rather than the extract. 
　the first reason is methodological; rather than extract an algorithm at each stage of the transformation process  it is computationally more efficient to have the transformations operate on the proof and then  once completed  extract an algorithm from the transformed proof. indeed  this approach allows specialization and normalization to be done concurrently. 
　the second reason is that by always performing transformations on the proof we can  at each stage of the transformation  check to see that we still have a viable algorithm. if at each stage the resultant proof can be marked and copied then we at least know that we have a sound algorithm which preserves the desired input/output relation  up to  and including  that stage. 
　the third point is of particular importance if we wish to extend the transformation system beyond specialization. the whole idea of working with proofs is that they contain information which goes beyond that required for simple execution. the dependency information utilized for pruning is one such example. however if  as with goad's system  such information is used to guide transformations on the extract then whatever stage of the overall process we are in we still have the same source proof. this means that by the time dependency pruning has been completed the proof will not be a faithful formalization of the target algorithm. if  however  it is the proof that is transformed then the target algorithm just is the extract for the transformed proof. so the transformed proof will faithfully represent the target algorithm and can be exploited further if desired. 
　the specialization system has been successfully subjected to test on numerous example proofs  some of which are very lengthy and complex: for example  the adaption of sorting programs to special situations  such as the number of elements to be sorted  by the specialization  and pruning  of synthesis proofs which yield sorting algorithms. these proofs contain nested induction schemata and consequently require recursive calls on the whole specialization process. they do  however  present a preliminary indication of the frequency with which prunable redundancies occur in the  real  computational world 
　a full account of these examples can be found in  madden  1c . 
1 transformation of recursive algorithms 
the computational efficiency of a recursive algorithm is directly related to the form of recursion  and the way in which an algorithm recurses on its input can be controlled by the way in which mathematical induction is 
employed in the algorithms synthesis. 
　the clues that the recursive argument position and structure of a function give us as to the best induction schema and induction variable to use have been incorporated  in the form of heuristics  into the oyster system. boyer and moore have done extensive work on heuristics for inductive proofs  boyer and moore  1 . the relationships between induction and recursion which they established have been generalized such that most recursive structures have a corresponding induction schema which can be employed to synthesise programs exhibiting the desired recursive behaviour  aubin  1  stevens  1 . 
　there is ample in the literature concerning recursive structures and there relative efficiency. for a general and well established account see  peter  1 . the relative efficiency of recursive structures synthesized with the oyster system has been investigated in  madden  
1 . 
　we can now see how the proof refinement environment assists the automatic optimization: a proof from which a program is derived will contain more information than that required for simple execution. using notions from analogy and past work on synthesizing recursive algorithms  my thesis research involved systematically relating proofs in such a way that more efficient programs can be obtained by transforming the associated proofs. the crucial element in the transformation is that recursive programs are optimized by transforming the induction schema employed within the corresponding synthesis proofs. 
　for ease of explanation we will consider a very simple example: the transformation of an algorithm which computes the fibonacci function. 
　at least two alternative induction schemata can be employed when synthesizing the fibonacci program: course of values induction and stepwise induction. 
　to employ course of values induction in the synthesis of an algorithm which takes as input n requires appealing to all  or a subset of  the output values obtained when the input is any value less than n. we will not show the proofs since they are very lengthy but the extracted algorithm can be represented thus: 

by employing course of values induction we obtain an algorithm such that in order to calculate fib n  one must first calculate fib n - l  and fib n - 1 . each of these sub-goals leads to another two recursive calls on fib and so on. in short the computational tree is exponential where the number of recursive calls on fib approaches 
1n. 
　however  we can  with relative ease  transform the course of values inductive proof into a target proof that  in effect  employs stepwise induction without having to do the synthesis from scratch. when we are dealing with integers as input/output values then stepwise induction simply corresponds to the standard mathematical induction on integers. 
　although the automatic oyster proof transformation system operates on proofs  as opposed to programs  the rationale employed by the system falls within the fold/unfold framework: we first construct a function  say g  which combines the values of the two step cases of the less efficient course of values definition  the function maketuple x y  can loosely be interpreted as a variadic which simulates the action of tuples . 
　by folding the base and step cases of this new function definition with the original source equations  or unfolded versions thereof  we end up with a proof that yields a function definition which corresponding to the following; 

in this case there is no recourse to the original fib definition and g n  requires only n recursive calls  stepping down to the base case g 1  . in other words  the computational tree resulting from stepwise induction is linear and hence the resulting algorithm requires far less computational effort in computing fib n  than that synthesized by employing course of values induction. 
1 	automation: generating n e w 
function/predicates and their definitions 
in general  the transformation of the source into the target is controlled by  collapsing  the less efficient induction schema into the more efficient one whilst making repeated checks that the type information and resulting syntax are correct.1 
　the process of providing new functions/predicates  along with definitions  is known in the transformation field as the eureka step. nearly all program transformation systems rely on generating such new procedures so that the resulting function/predicate can be folded with equations from the current equation set  thus introducing recursion into the target program  burstall and 
darlington  1  darlington  1  tamaki and t.sato  
1  grant and zhang  1  bruynooghe et a/.  1  
　　d  type errors occur when a  sub goal of the proof is of the wrong constructive type. 
	madden 	1 
madden  1a  madden  1 . such a process is notoriously hard to automate and practically all such unfold/fold systems rely on user interaction. even those that have achieved limited success in automating the eureka step rely heavily on some form of user-provided control program. 
　the reason for this difficulty is due to a trade of between the degree of automation in any particular application  e.g introduction of tail recursion  avoiding redundant computation  pruning mechanisms  partial evaluation  and the siie of the class of unfold/fold transformations one wishes the system to encompass. 
　within the oyster proof transformation environment  the eureka step has been completely automated for relatively simple functions  such as fibonacci  the exponent function and some simple sorting algorithms  by working out from the source proof induction step exactly what the auxilliary function  or tuple  is comprised of. this task is also neatly combined with the procedures for accessing and mapping components of the source proof. 
　this explanation is very much a simplification concerning the workings of  and programs transformed by  the proof transformation system. 
　a far more thorough and detailed account  especially of the flexibility of control that the induction schemas give us over the resulting recursive structures  can be found in  madden  1a . 
