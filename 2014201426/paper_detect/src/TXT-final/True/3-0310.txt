 
this paper introduces latent relational analysis  lra   a method for measuring semantic similarity. lra measures similarity in the semantic relations between two pairs of words. when two pairs have a high degree of relational similarity  they are analogous. for example  the pair cat:meow is analogous to the pair dog:bark. there is evidence from cognitive science that relational similarity is fundamental to many cognitive and linguistic tasks  e.g.  analogical reasoning . in the vector space model  vsm  approach  to measuring relational similarity  the similarity between two pairs is calculated by the cosine of the angle between the vectors that represent the two pairs. the elements in the vectors are based on the frequencies of manually constructed patterns in a large corpus. lra extends the vsm approach in three ways:  1  patterns are derived automatically from the corpus   1  singular value decomposition is used to smooth the frequency data  and  1  synonyms are used to reformulate word pairs. this paper describes the lra algorithm and experimentally compares lra to vsm on two tasks  answering college-level multiple-choice word analogy questions and classifying semantic relations in noun-modifier expressions. lra achieves state-of-the-art results  reaching human-level performance on the analogy questions and significantly exceeding vsm performance on both tasks.  
1 introduction 
this paper introduces latent relational analysis  lra   a method for measuring relational similarity. lra has potential applications in many areas  including information extraction  word sense disambiguation  machine translation  and information retrieval.  
　relational similarity is correspondence between relations  in contrast with attributional similarity  which is correspondence between attributes  medin et al.  1 . when two words have a high degree of attributional similarity  we say they are synonymous. when two pairs of words have a high degree of relational similarity  we say they are analogous. 
for example  the word pair mason:stone is analogous to the pair carpenter:wood; the relation between mason and stone is highly similar to the relation between carpenter and wood.  
　past work on semantic similarity measures has mainly been concerned with attributional similarity. for instance  latent semantic analysis  lsa  can measure the degree of similarity between two words  but not between two relations  landauer and dumais  1 .  
　recently the vector space model  vsm  of information retrieval has been adapted to the task of measuring relational similarity  achieving a score of 1% on a collection of 1 college-level multiple-choice word analogy questions  turney and littman  1 . the vsm approach represents the relation between a pair of words by a vector of frequencies of predefined patterns in a large corpus.  
　lra extends the vsm approach in three ways:  1  the patterns are derived automatically from the corpus  they are not predefined    1  the singular value decomposition  svd  is used to smooth the frequency data  it is also used this way in lsa   and  1  automatically generated synonyms are used to explore reformulations of the word pairs. lra achieves 1% on the 1 analogy questions  statistically equivalent to the average human score of 1%. on the related problem of classifying noun-modifier relations  lra achieves similar gains over the vsm. for both problems  lra's performance is state-of-the-art. 
　to motivate this research  section 1 briefly outlines some  possible applications for a measure of relational similarity. related work with the vsm approach to relational similarity is described in section 1. the lra algorithm is presented in section 1. lra and vsm are experimentally evaluated by their performance on word analogy questions in section 1 and on classifying semantic relations in nounmodifier expressions in section 1. we discuss the interpretation of the results  limitations of lra  and future work in section 1. the paper concludes in section 1.  
1 applications of relational similarity 
many problems in text processing would be solved  or at least greatly simplified  if we had a black box that could take as input two chunks of text and produce as output a measure of the degree of similarity in the meanings of the two chunks. we could use it for information retrieval  question answering  machine translation using parallel corpora  information extraction  word sense disambiguation  text summarization  measuring lexical cohesion  identifying sentiment and affect in text  and many other tasks in natural language processing. this is the vision that motivates research in paraphrasing  barzilay and mckeown  1  and textual entailment  dagan and glickman  1   two topics that have lately attracted much interest.  
　in the absence of such a black box  current approaches to these problems typically use measures of attributional similarity. for example  the standard bag-of-words approach to information retrieval is based on attributional similarity  salton and mcgill  1 . given a query  a search engine produces a ranked list of documents  where the rank of a document depends on the attributional similarity of the document to the query. the attributes are based on word frequencies; relations between words are ignored.  
　although attributional similarity measures are very useful  we believe that they are limited and should be supplemented by relational similarity measures. cognitive psychologists have also argued that human similarity judgements involve both attributional and relational similarity  medin et al.  1 . 
　consider word sense disambiguation for example. in isolation  the word  plant  could refer to an industrial plant or a living organism. suppose the word  plant  appears in some text near the word  food . a typical approach to disambiguating  plant  would compare the attributional similarity of  food  and  industrial plant  to the attributional similarity of  food  and  living organism   lesk  1; banerjee and pedersen  1 . in this case  the decision may not be clear  since industrial plants often produce food and living organisms often serve as food. it would be very helpful to know the relation between  food  and  plant  in this example. in the text  food for the plant   the relation between food and plant strongly suggests that the plant is a living organism  since industrial plants do not need food. in the text  food at the plant   the relation strongly suggests that the plant is an industrial plant  since living organisms are not usually considered as locations.  
　a measure of relational similarity could potentially improve the performance of any text processing application that currently uses a measure of attributional similarity. we believe relational similarity is the next step  after attributional similarity  towards the black box envisioned above. 
1 related work 
let r1 be the semantic relation between a pair of words  a and b  and let r1 be the semantic relation between another pair  c and d. we wish to measure the relational similarity between r1 and r1. the relations r1 and r1 are not given to us; our task is to infer these hidden  latent  relations and then compare them. 
　in the vsm approach of turney and littman   we create vectors  r1 and r1  that represent features of r1 and r1  and measure the similarity of r1 and r1 by the cosine of the angle  between r1 =r1   r1 n  and r1 =r1  r1 n  : 
                                 n r1 i  r1 i i=1	=	r1  r1	. cosine θ  = 
we make a vector  r  to characterize the relationship between two words  x and y  by counting the frequencies of various short phrases containing x and y. turney and littman  use a list of 1 joining terms  such as  of    for   and  to   to form 1 phrases that contain x and y  such as  x of y    y of x    x for y    y for x    x to y   and  y to x . these phrases are then used as queries for a search engine and the number of hits  matching documents  is recorded for each query. this process yields a vector of 1 numbers. if the number of hits for a query is x  then the corresponding element in the vector r is log x+1  .  
　turney and littman  evaluated the vsm approach by its performance on 1 college-level multiple-choice sat analogy questions  achieving a score of 1%. a sat analogy question consists of a target word pair  called the stem  and five choice word pairs. to answer an analogy question  vectors are created for the stem pair and each choice pair  and then cosines are calculated for the angles between the stem vector and each choice vector. the best guess is the choice pair with the highest cosine. we use the same set of analogy questions to evaluate lra in section 1.  
　the best previous performance on the sat questions was achieved by combining thirteen separate modules  turney et al.  1 . the performance of lra significantly surpasses this combined system  but there is no real contest between these approaches  because we can simply add lra to the combination  as a fourteenth module. since the vsm module had the best performance of the thirteen modules  turney et al.  1   the following experiments focus on comparing vsm and lra. 
　the vsm was also evaluated by its performance as a distance measure in a supervised nearest neighbour classifier for noun-modifier semantic relations  turney and littman  1 . the problem is to classify a noun-modifier pair  such as  laser printer   according to the semantic relation between the head noun  printer  and the modifier  laser . the evaluation used 1 noun-modifier pairs that have been manually labeled with 1 classes of semantic relations  nastase and szpakowicz  1 . for example   laser printer  is classified as instrument; the printer uses the laser as an instrument for printing. a testing pair is classified by searching for its single nearest neighbour in the labeled training data. the best guess is the label for the training pair with the highest cosine; that is  the training pair that is most analogous to the testing pair  according to vsm. lra is evaluated with the same set of noun-modifier pairs in section 1.  
1 latent relational analysis 
lra takes as input a set of word pairs and produces as output a measure of the relational similarity between any two of the input pairs. lra relies on three resources   1  a search engine with a very large corpus of text   1  a broadcoverage thesaurus of synonyms  and  1  an efficient implementation of svd. lra does not use labeled data  structured data  or supervised learning.  lra proceeds as follows: 
1. find alternates: for each word pair a:b in the input set  look in the thesaurus for the top num sim words  in the following experiments  num sim = 1  that are most similar to 
a. for each a＞ that is similar to a  make a new word pair 
 a＞:b . likewise  look for the top num sim words that are most similar to b  and for each b＞   make a new word pair a:b＞ . a:b is called the original pair and each a＞:b or 
 a:b＞ is an alternate pair. the intent is for alternates to have almost the same semantic relations as the original. 1. filter alternates: for each original pair a:b  filter the 1 〜 num sim alternates as follows. for each alternate pair  send a query to the search engine  to find the frequency of phrases that begin with one member of the pair and end with the other. the phrases cannot have more than max phrase words  we use max phrase = 1 . sort the alternate pairs by the frequency of their phrases. select the top num filter most frequent alternates and discard the remainder  we use num filter = 1  so 1 alternates are dropped . this step tends to eliminate alternates that have no clear semantic relation. 
1. find phrases: for each pair  originals and alternates   make a list of phrases in the corpus that contain the pair. query the search engine for all phrases that begin with one member of the pair and end with the other  with a minimum of min inter intervening words and a maximum of max inter intervening words  we use min inter = 1  max inter = 1 = max phrase - 1 . we ignore suffixes when searching for phrases that match a given pair. these phrases reflect the semantic relations between the words in each pair. 
1. find patterns: for each phrase found in the previous step  build patterns from the intervening words. a pattern is constructed by replacing any or all or none of the intervening words with wild cards  one wild card can only replace one word . for each pattern  count the number of pairs  originals and alternates  with phrases that match the pattern  a wild card must match exactly one word . keep the top num patterns most frequent patterns and discard the rest  we use num patterns = 1 . typically there will be millions of patterns  so it is not feasible to keep them all. 
1. map pairs to rows: in preparation for building a matrix x   suitable for svd  create a mapping of word pairs to row numbers. for each pair a:b  create a row for a:b and another row for b:a. this will make the matrix more symmetrical  reflecting our knowledge that the relational similarity between a:b and c:d should be the same as the relational similarity between b:a and d:c.  mason is to stone as carpenter is to wood. stone is to mason as wood is to carpenter.  the intent is to assist svd by enforcing this symmetry in the matrix. 
1. map patterns to columns: create a mapping of the top num patterns patterns to column numbers. for each pattern p  create a column for  word1 p word1  and another column for  word1 p word1 . thus there will be 1 〜 num patterns columns in x . 
1. generate a sparse matrix: generate a matrix x in sparse matrix format. the value for the cell in row i and column j is the frequency of the j-th pattern  see step 1  in phrases that contain the i-th word pair  see step 1 . 1. calculate entropy: apply log and entropy transformations to the sparse matrix  landauer and dumais  1 . each cell is replaced with its logarithm  multiplied by a weight based on the negative entropy of the corresponding column vector in the matrix. this gives more weight to patterns that vary substantially in frequency for each pair. 
1. apply svd: after log and entropy transformations  apply svd to x . svd decomposes x into a product of three matrices uΣvt   where u and v are in column orthonormal form  i.e.  the columns are orthogonal and have unit length  and Σ is a diagonal matrix of singular values  hence svd   golub and van loan  1 . if x is of rank r   then Σ is also of rank r . let Σk   where k   r   be the diagonal matrix formed from the top k singular values  and let uk and vk be the matrices produced by selecting the corresponding columns from u and v . the matrix 
ukΣkvkt is the matrix of rank k that best approximates the original matrix x   in the sense that it minimizes the approximation errors  golub and van loan  1 . we may think of this matrix ukΣkvkt as a  smoothed  or  compressed  version of the original matrix. svd is used to reduce noise and compensate for sparseness. 
1. projection: calculate ukΣk  we use k = 1  as recommended by landauer and dumais  . this matrix has the same number of rows as x   but only k columns  instead of 1 〜 num patterns columns; in our experiments  that is 1 columns instead of 1 . we do not use v   because we want to calculate the cosines between row vectors  and it can be proven that the cosine between any two row vectors in ukΣk is the same as the cosine between the corresponding two row vectors in ukΣkvkt . 
1. evaluate alternates: let a:b and c:d be any two word pairs in the input set. from step 1  we have  num filter + 1  versions of a:b  the original and num filter alternates. likewise  we have  num filter + 1  versions of c:d. therefore we have  num filter + 1 ways to compare a version of a:b with a version of c:d. look for the row vectors in ukΣk that correspond to the versions of a:b and the versions of c:d and calculate the  num filter + 1 cosines  in our experiments  there are 1 cosines .  
1. calculate relational similarity: the relational similarity between a:b and c:d is the average of the cosines  among the  num filter + 1 cosines from step 1  that are greater than or equal to the cosine of the original pairs  a:b and c:d. the requirement that the cosine must be greater than or equal to the original cosine is a way of filtering out poor analogies  which may be introduced in step 1 and may have slipped through the filtering in step 1. averaging the cosines  as opposed to taking their maximum  is intended to provide some resistance to noise. 
　in our experiments  the input set contains from 1 to 1 word pairs. steps 1 and 1 can be repeated for each two input pairs that are to be compared.  
　in the following experiments  we use a local copy of the waterloo multitext system  wmts  as the search engine  with a corpus of about 1 〜 1 english words. the corpus was gathered by a web crawler from us academic web sites  clarke et al.  1 . the wmts is a distributed  multiprocessor  search engine  designed primarily for passage retrieval  although document retrieval is possible  as a special case of passage retrieval . our local copy runs on a 1-cpu beowulf cluster. 
　the wmts is well suited to lra  because it scales well to large corpora  one terabyte  in our case   it gives exact frequency counts  unlike most web search engines   it is designed for passage retrieval  rather than document retrieval   and it has a powerful query syntax.  
　as a source of synonyms  we use lin's  automatically generated thesaurus. lin's thesaurus was generated by parsing a corpus of about 1 〜 1 english words  consisting of text from the wall street journal  san jose mercury  and ap newswire  lin  1 . the parser was used to extract pairs of words and their grammatical relations. words were then clustered into synonym sets  based on the similarity of their grammatical relations. two words were judged to be highly similar when they tended to have the same kinds of grammatical relations with the same sets of words. 
　given a word and its part of speech  lin's thesaurus provides a list of words  sorted in order of decreasing attributional similarity. this sorting is convenient for lra  since it makes it possible to focus on words with higher attributional similarity and ignore the rest.  
　we use rohde's svdlibc implementation of the singular value decomposition  which is based on svdpackc  berry  1 .  
1 experiments with word analogy questions 
table 1 shows one of the 1 sat analogy questions  along with the relational similarities between the stem and each choice  as calculated by lra. the choice with the highest relational similarity is also the correct answer for this question  quart is to volume as mile is to distance .  
table 1. relation similarity measures for a sample sat question. 
stem: 	 quart:volume relational similarity choices: 	 a  day:night 1  	 b  mile:distance 1  	 c  decade:century 1  	 d  friction:heat 1  	 e  part:whole 1 　lra correctly answered 1 of the 1 analogy questions and incorrectly answered 1 questions. four questions were skipped  because the stem pair and its alternates did not appear together in any phrases in the corpus  so all choices had a relational similarity of zero. since there are five choices for each question  we would expect to answer 1% of the questions correctly by random guessing. therefore we score the performance by giving one point for each correct answer and 1 points for each skipped question. lra attained a score of 1% on the 1 sat questions.  
　the average performance of college-bound senior high school students on verbal sat questions corresponds to a score of about 1%  turney and littman  1 . the difference between the average human score and the score of lra is not statistically significant. 
　with 1 questions and 1 word pairs per question  one stem and five choices   there are 1 pairs in the input set. in step 1  introducing alternate pairs multiplies the number of pairs by four  resulting in 1 pairs. in step 1  for each pair a:b  we add b:a  yielding 1 pairs. however  some pairs are dropped because they correspond to zero vectors  they do not appear together in a window of five words in the wmts corpus . also  a few words do not appear in lin's thesaurus  and some word pairs appear twice in the 
sat questions  e.g.  lion:cat . the sparse matrix  step 1  has 1 rows  word pairs  and 1 columns  patterns   with a density of 1%  percentage of nonzero values . 
　table 1 compares lra to vsm with the 1 analogy questions. vsm-av refers to the vsm using altavista's database as a corpus. the vsm-av results are taken from turney and littman . we estimate the altavista search index contained about 1 〜 1 english words at the time the vsm-av experiments took place. turney and littman  gave an estimate of 1 〜 1 english words  but we believe this estimate was slightly conservative. vsm-wmts refers to the vsm using the wmts  which contains about 1 〜 1 english words. we generated the vsm-wmts results by adapting the vsm to the wmts.  
table 1. lra versus vsm with 1 sat analogy questions. 
 vsm-av vsm-wmts lra correct 1 1 1 incorrect 1 1 1 skipped 1 1 1 total 1 1 1 score 1% 1% 1% all three pairwise differences in the three scores in table 
1 are statistically significant with 1% confidence  using the fisher exact test. using the same corpus as the vsm  lra achieves a score of 1% whereas the vsm achieves a score of 1%  an absolute difference of 1% and a relative improvement of 1%. when vsm has a corpus ten times larger than lra's corpus  lra is still ahead  with an absolute difference of 1% and a relative improvement of 1%. 
　comparing vsm-av to vsm-wmts  the smaller corpus has reduced the score of the vsm  but much of the drop is due to the larger number of questions that were skipped  1 for vsm-wmts versus 1 for vsm-av . with the smaller corpus  many more of the input word pairs simply do not appear together in short phrases in the corpus. lra is able to answer as many questions as vsm-av  although it uses the same corpus as vsm-wmts  because lin's  thesaurus allows lra to substitute synonyms for words that are not in the corpus. 
　vsm-av required 1 days to process the 1 analogy questions  turney and littman  1   compared to 1 days for lra. as a courtesy to altavista  turney and littman  inserted a five second delay between each query. since the wmts is running locally  there is no need for delays. vsm-wmts processed the questions in one day. 
1 experiments with noun-modifier relations 
this section describes experiments with 1 noun-modifier pairs  hand-labeled with 1 classes of semantic relations  nastase and szpakowicz  1 . we experiment with both a 1-class problem and a 1-class problem. the 1 classes of semantic relations include cause  e.g.  in  flu virus   the head noun  virus  is the cause of the modifier  flu    location  e.g.  in  home town   the head noun  town  is the location of the modifier  home    part  e.g.  in  printer tray   the head noun  tray  is part of the modifier  printer    and topic  e.g.  in  weather report   the head noun  report  is about the topic  weather  . for a full list of classes  see nastase and szpakowicz  or turney and littman . the 1 classes belong to 1 general groups of relations  causal relations  temporal relations  spatial relations  participatory relations  e.g.  in  student protest   the  student  is the agent who performs the  protest ; agent is a participatory relation   and qualitative relations  e.g.  in  oak tree    oak  is a type of  tree ; type is a qualitative relation . 
　the following experiments use single nearest neighbour classification with leave-one-out cross-validation.  for leave-one-out cross-validation  the testing set consists of a single noun-modifier pair and the training set consists of the 1 remaining noun-modifiers. the data set is split 1 times  so that each noun-modifier gets a turn as the testing word pair. the predicted class of the testing pair is the class of the single nearest neighbour in the training set. as the measure of nearness  we use lra to calculate the relational similarity between the testing pair and the training pairs. 
　following turney and littman   we evaluate the performance by accuracy and also by the macroaveraged f measure  lewis  1 . the f measure is the harmonic mean of precision and recall. macroaveraging calculates the precision  recall  and f for each class separately  and then calculates the average across all classes.  
　there are 1 word pairs in the input set for lra. in step 1  introducing alternate pairs multiplies the number of pairs by four  resulting in 1 pairs. in step 1  for each pair a:b  we add b:a  yielding 1 pairs. some pairs are dropped because they correspond to zero vectors and a few words do not appear in lin's thesaurus. the sparse matrix  step 1  has 1 rows and 1 columns  with a density of 1%. table 1 shows the performance of lra and vsm on the 1-class problem. vsm-av is vsm with the altavista corpus and vsm-wmts is vsm with the wmts corpus. the results for vsm-av are taken from turney and littman . all three pairwise differences in the three f measures are statistically significant at the 1% level  according to the paired t-test. the accuracy of lra is significantly higher than the accuracies of vsm-av and vsm-wmts  according to the fisher exact test  but the difference between the two vsm accuracies is not significant. using the same corpus as the vsm  lra's accuracy is 1% higher in absolute terms and 1% higher in relative terms. 
　table 1 compares the performance of lra and vsm on the 1-class problem. the accuracy and f measure of lra are significantly higher than the accuracies and f measures of vsm-av and vsm-wmts  but the differences between the two vsm accuracies and f measures are not significant. using the same corpus as the vsm  lra's accuracy is 1% higher in absolute terms and 1% higher in relative terms. 
table 1. comparison of lra and vsm on the 1-class problem. 

	 	vsm-av 	vsm-wmts 	lra 

correct 1 1 1 incorrect 1 1 1 total 1 1 1 accuracy 1% 1% 1% precision 1% 1% 1% recall 1% 1% 1% f 1% 1% 1% table 1. comparison of lra and vsm on the 1-class problem. 
 vsm-av vsm-wmts lra correct 1 1 1 incorrect 1 1 1 total 1 1 1 accuracy 1% 1% 1% precision 1% 1% 1% recall 1% 1% 1% f 1% 1% 1% 1 discussion 
the experimental results in sections 1 and 1 demonstrate that lra performs significantly better than the vsm  but it is also clear that there is room for improvement. the accuracy might not yet be adequate for practical applications  although past work has shown that it is possible to adjust the tradeoff of precision versus recall  turney and littman  1 . for some of the applications  such as information extraction  lra might be suitable if it is adjusted for high precision  at the expense of low recall.  
　another limitation is speed; it took almost nine days for lra to answer 1 analogy questions. however  with progress in computer hardware  speed will gradually become less of a concern. also  the software has not been optimized for speed; there are several places where the efficiency could be increased and many operations are parallelizable. it may also be possible to precompute much of the information for lra  although this would require substantial changes to the algorithm. 
　the difference in performance between vsm-av and vsm-wmts shows that vsm is sensitive to the size of the corpus. although lra is able to surpass vsm-av when the wmts corpus is only about one tenth the size of the av corpus  it seems likely that lra would perform better with a larger corpus. the wmts corpus requires one terabyte of hard disk space  but progress in hardware will likely make ten or even one hundred terabytes affordable in the relatively near future. 
　for noun-modifier classification  more labeled data should yield performance improvements. with 1 nounmodifier pairs and 1 classes  the average class has only 1 examples. we expect that the accuracy would improve substantially with five or ten times more examples  but it is time consuming and expensive to acquire hand-labeled data. 
　another issue with noun-modifier classification is the choice of classification scheme for the semantic relations. the 1 classes of nastase and szpakowicz  might not be the best scheme. other researchers have proposed different schemes  rosario and hearst  1 . it seems likely that some schemes are easier for machine learning than others. 
1 conclusion 
this paper has introduced a new method for calculating relational similarity  latent relational analysis. the experiments demonstrate that lra performs better than the vsm approach  when evaluated with sat word analogy questions and with the task of classifying noun-modifier expressions. the vsm approach represents the relation between a pair of words with a vector  in which the elements are based on the frequencies of 1 hand-built patterns in a large corpus. lra extends this approach in three ways:  1  the patterns are generated dynamically from the corpus   1  svd is used to smooth the data  and  1  a thesaurus is used to explore reformulations of the word pairs. 
　just as attributional similarity measures have proven to have many practical uses  we expect that relational similarity measures will soon become widely used. relational similarity plays a fundamental role in the mind and therefore relational similarity measures could be crucial for artificial intelligence  medin et al.  1 . lra may be a step towards the black box that we imagined in section 1  with many potential applications in text processing. 
　in future work  we plan to investigate some potential applications for lra. it is possible that the error rate of lra is still too high for practical applications  but the fact that lra matches average human performance on sat analogy questions is encouraging. 
acknowledgments 
for data and software used in this research  my thanks go to michael littman  vivi nastase  stan szpakowicz  egidio terra  charlie clarke  dekang lin  doug rohde  and michael berry. 
