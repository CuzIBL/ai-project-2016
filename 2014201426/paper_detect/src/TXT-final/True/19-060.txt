 
     lair is a system that incrementally learns conjunctive concept descriptions from positive and negative examples. these concept descriptions are used to create and extend a domain theory that is applied  by means of constructive induction  to later learning tasks. important issues for constructive induction are when to do it and how to control it lair demonstrates how constructive induction can be controlled by  1  reducing it to simpler operations   1  constraining the simpler operations to preserve relative correctness   1  doing deductive inference on an as-needed basis to meet specific information requirements of learning subtasks  and  1  constraining the search space by subtaskdependent constraints. 
i. introduction 
     in this paper  we describe an incremental learning system called lair  which learns conjunctive concept descriptions from positive and negative examples. incremental learning systems need mechanisms for creating or inducing a concept description from one example  and for modifying that description as additional examples are presented. several algorithms exist for inducing a concept description that consists of only features and values that appear in the example descriptions  1  1 . the process of introducing new features or descriptors into the concept description  termed constructive generalization  has been formalized as a transformation rule . however  its role in altering the knowledge representation space is not well-understood. of all the knowledge that could be retrieved or derived from the descriptors that occur in the examples  what is relevant to transforming the current concept description  and transforming it correctly   simply put  the unresolved issues about constructive generalization are when to do it and how to guide it. these are the issues we have tried to address in lair. 
ii. overview of approach 
     in this section  we outline the general framework and approach we have taken in lair. first  we consider  climbing the generalization tree     which introduces new values for already-seen descriptors  and  constructive generalization   which introduces new descriptors  as cases of constructive induction. second  we view constructive induction as based on inference and deduction. third  we use constraints that are inherent in the learning task to limit the knowledge retrieval and deduction. an important result of the second and third points is that lair has no  constructive induction  rule among its concept transformation rules. the constructive induction rule is derived from inference rules and concept revision rules that use description constraints. our approach also uses an  information conservation  principle when generalizing a concept description. finally  we have a relaxed definition of a  correct  concept description. in lair  a transformation of a 
     concept description might make the concept description incorrect with respect to previously-seen  but forgotten  examples  but it is guaranteed to converge on the correct description with enough examples. 
a. conjunctive concept descriptions descriptions of concepts and examples have form: 

     where each pij x  is of form pn t1 ... ttn  where pn is an n-adic predicate symbol chosen from some fixed  finite set of such symbols  one of the ti is x  and the other ti are constants or skolem functions of x. we will refer to the unnegated pij as requireds  reqs  and the negated py as nots. 
     examples are denoted by constants  e.g. ex-1  and are described by applying a description to the constant: 
xx pos arch  x  a block f x   x  a block g x   x  a ontop f x  g x  x   ex-l  
 ex-1 is a positive example of the concept arch; in ex-1  there are two blocks  one of which is on top of the other.  
one of the predicates in each example description must be 
pos concept  x  if the example is positive  or pos concept  x  if the example is negative. 
     a description d is relatively correct at time t iff for every remembered positive example ex of the concept  
     kb a ex-description ex  h d ex  and for every remembered negative example ex of the concept  
     kb a ex-description ex  d ex  where ex-description ex  is the description of ex  and kb is the knowledge base.** lair remembers only one past positive example and the current example  but this definition holds true for any set of remembered examples. relative correctness is equivalent to correctness in systems that can remember all the examples it has seen at any time. 
b. deductive inference 
     lair uses the following rules of inference to determine whether constraints on the concept description are satisfied  to classify examples  and to extend example descriptions by adding req's and nots. 

* this work was supported by nserc operating grant a1 to renec elio. 
　the knowledge base includes predicates and inference rules that relate predicates  and is described in more detail in section iii. 
	watanabe and elio 	1 


c. constraints on the concept description 
　　lair keeps track of whether a predicate is provable or unprovable for some positive example: 
drop p  t  - at or prior to time t  p was inferred false of some positive example 
some p  t  - at or prior to time t  p was inferred true of some positive example. 
　　this knowledge constrains the concept description: concept descriptions cannot include drop'd predicates as reqs  or some'd predicates as nots. 
d- concept revision rules 
lair uses the following concept revision rules: 
  the add-req rule: 
if a predicate has never been drop'd from the concept description 
& it can be proven true of all the remembered positive examples 
then add the predicate to the concept description & remember it is true of some positive example 
  the add-not rule: 
if 	a predicate has not been proven of some positive examples 
& it can be proven true of the current negative example 
& it cannot be proven true of the remembered past positive example 
then add its negation to the concept description 
  the drop-req rule: 
if a predicate in the concept description cannot be proven true of the remembered positive examples 
then drop it from the concept description 
& remember it has been drop'd 
  the drop-not rule: 
if a negated predicate p in the concept description can be proven true of any remembered positive example 
then drop p from the concept description & remember it is true of some positive example 
　　notice lair does not have a constructive induction rule. this rule would be written in our framework as follows: 
  the constructive induction rule-applies domain knowledge to generalize a concept description. 
	if 	the concept description includes a predicate p 
& the knowledge base contains a rule p implies q 
then drop p from the concept description 
& add q to the concept description 
e. relative completeness and relative correctness 
　　two important properties of these rules as implemented in lair are  preservation of relative correctness  and  relative completeness.  a description revision rule r preserves relative correctness iff given description d  if d is relatively correct then r revises d to a description d' that is also relatively correct. 
　　completeness is defined relative to lair's ability to use the concept description to classify examples. suppose there exists a correct description that be proven true of all the positive examples and false of all the negative examples. then a set of concept revision rules is relatively complete iff a correct description d' can be derived from every intermediate description d derivable from any set of positive and negative examples  using the rules. since lair is designed as a nonbacktracking system that does not maintain multiple concept descriptions  these are important properties to ensure that lair eventually does find a relatively correct concept description. the constructive induction rule is not relatively complete  so it is not suitable for use in a non-backtracking system that maintains only a single concept description. 
　　the deductive rules and relaxed versions of concept revision rules are equivalent to the constructive induction rule  providing the initial description is relatively correct. the addreq rule is relaxed by removing the drop constraint  and the drop-req rule is relaxed by removing all its constraints. the proof in  essentially reduces constructive induction to the following steps: use the deduction rule to infer q true of some positive example  use the add-req rule to add q to the concept description  and use the drop-req rule to drop p. 
iii. lair's knowledge base 
     lair's knowledge base consists of frames that represent knowledge about examples  concept descriptions  constraints on concept descriptions  and prior or learned knowledge about the domain. 
a. frames representing predicates 
　　knowledge is organized around predicates over examples. frames corresponding to these entities are created during learning as instantiations of more general predicate frames stored in the prior knowledge base. there are two types of predicate frames: most-general-predicate frames and less-general-predicate frames. a most-general-predicate frame corresponds to a predicate expression that has more than one argument  e.g.  a x y z body x  y  z  . a less-generalpredicate frame corresponds to a predicate expression that has only one argument  e.g. a z body a z   b a z    z  . since lessgeneral-predicatcs are the  building blocks  of concept descriptions  constraints on concept descriptions are stored on these frames. the most important slots of a less-generalpredicate frame are: 

   this rule is based on the assumption that example descriptions are complete with respect to the fixed set of predicates from which descriptions are constructed. 
1 	knowledge acquisition 
definition 	lambda expression defining the predicate 
most-general-predicate the frame representing the most general predicate corresponding to the predicate 

propositions instances of the predicate some t iff the predicate is true of some positive example required t iff the predicate is an unnegated predicate in th  concept description. not t iff the predicate is a negated predicate in the concept description. dropped t iff the predicate is not derivable for some positive example. required-space t iff the predicate has been considered as a possible req in a 
concept description during the current learning subtask.**** not-space t iff the predicate has been considered as a possible not in a concept description during the current learning subtask. 
     the second kind of predicate frame  the most-generalpredicate frame  is used to organize knowledge about relationships between predicates and rules. less-generalpredicates can inherit inference rules from most-generalpredicates. most-general-predicates can inherit propositions from less-general-predicates. the most important slots are: 
definition 
less-general-predicate 
consequent-of 
antecedent-of 
neg-antecedant-of lambda expression defining the predicate 
inverse of the most-general-predicate slot on less-general-predicate frames. 
rules in which the predicate  or one of its less general predicates  is a consequent. 
rules in which the predicate  or one 
of its less general predicates  is an unnegated antecedent 
rules in which the predicate  or one 
of its less general predicates  is a negated antecedent b. rule frames 
　　rule frames represent knowledge corresponding to logical implications. the most important slots on a rule frame are: 
consequents consequents of the implication. antecedents unnegated antecedents of the implication. neg-anteceaents negated antecedents of the implication a typical rule frame might be: graspable-1 
consequents graspable y  z  antecedents cyl y  z   small y  z   
light y  z   body x  y  z  neg-antecedents hot y  z  this rule corresponds to the implication statement if something is light with a small  cylindrical body that is not hot  then it's graspable. 
iv. how lair learns 
　　to describe how lair learns  we will explain learning a concept description for  cup  . assume lair has a number of rules relating to the concepts  hot    stable    open-vessel    graspable   and  liftable.  lair remembers only one 
these subtasks are described in section iv. 
previous example  which must be positive  plus the current example. 
     the first example is  cha-cup +   so lair forms the following description as the past positive example:  a positive example of a cup is something with a flat bottom  an upwardspointing concavity  a small  cylindrical body  that was lifted.  
　　since initially there are no constraints on the concept description  all of the features can be added to the concept description  which was initially empty. the second example   typical-cup +   has the description:  a positive example of a cup is something with a flat bottom  an upwards-pointing concavity  a small  cylindrical body  that is light and has a handle.  the concept description is revised to include the new features  handle  and  light  
　　relative correctness is checked by trying to prove the extended concept description true of the remembered positive examples. lair restores relative correctness by dropping the  was-lifted  predicate  not true of example 1  and the  handle  and  light  predicates  not true of the remembered past positive example . 
　　although the concept description is relatively correct  in some sense information has been  lost.  lair attempts to conserve the lost information by specializing the revised concept description with inferences it can make from the dropped req's. essentially  this means  docs this descriptor  which i've decided to drop  imply something else that is provable of both the current example and the past positive example s    information conservation is important in lair for several reasons. first  when the current example is positive  then lair is essentially learning from positive-only examples. systems that learn from positive-only examples must have methods to avoid over-generalization. information conservation avoids over-generalization by compensating for the loss of information in generalization by a specialization step. further  specialization is focused only on descriptors that can be inferred from the dropped req's. 
　　this information conservation involves searches for inferrable descriptors by looking at rule-generalizations of the dropped req's. a predicate p is a rule-generalization of a predicate q if  1  q has a most-gen-pred that is an antecedentof a rule that has p as a consequent  or  1  p is a rulegeneralization of another predicate that is a rule-generalization of q. the relationship between p and q is shown below. most-gen-pred antecedent-of consequent 
	 c  	k d 	  1 	k d 
ako 
	less-gen-pred 	most-gen-pred 	rule 	most-gen-pred 
figure 1. rule generalizations 
i 
a subtask is created for each dropped req by initializing a 
 req-boundary  to the dropped req. each subtask is an heuristic search of the space of descriptors to find a less general predicate of a rule-generalization of the dropped req that can be add-req'd to the concept description. the subtask chooses a req-boundary element p and tries to add-req p  or a less general predicate than p . if this succeeds  then the subtask terminates successfully. otherwise  p is deleted from the reqboundary and p's rule-generalizations are added to the reqboundary if p  or a less general predicate than p  satisfies the constraint that it is true of a remembered  positive example of the concept  and is not currently a req. if the req-boundary is empty  then the subtask terminates unsuccessfully  otherwise a new req-boundary element is chosen and the process is repeated. 
	watanabe and elio 	1 

　　for the  typical-cup +  example  the first req-boundary is initialized to { was-lifted }. since  was-lifted  was dropped  it cannot be add-req'd  and is removed from the req-boundary. however   was-lifted  has a rulegeneralization: if an object was-lifted  then it is lift able. 
　　since  was-lifted  satisfies the req-boundary constraint  its rule-generalization can be added to the req-boundary. lair proves that  liftable  is true of the current example  ex-1  and the past positive example  ex-1  by accessing and applying inference rules such as: if something is graspable and light  then it is liftable; if something has a handle then it is graspable. 
therefore   liftable  is added to the concept description. 
req-boundaries arc also created for  handle  and  light   but no further information can be conserved for these cases. the resulting  relatively correct concept description is:  something that is liftable  with a flat bottom  upwards pointing concavity  and a small  cylindrical body.  
as one further positive example  consider  balanced-cup 
+   whose description is:  a positive example of a cup is something that is balanced  contains something  has a handle  and is light.  lair revises the concept description  and conserves information as for the  typical-cup +  example  resulting in the desired concept description:  something that is liftable  stable  an open-vessel  and has a body.  note that the  balanced-cup +  example differed from the concept description in several ways. this allows lair to drop more irrelevant features and add more relevant features. using information conservation  lair learns faster when positive examples show the typical variance in the concept  i.e.  when  far-hits  are presented. second   body  is not eliminated from the concept description  although it is not necessary for relative correctness. lair's goal is just to find a correct description; demanding  minimally  correct descriptions is beyond its scope. 
     lair learns concept descriptions with negated predicates using a method similar to the one outlined above. assume the current concept description is:  something with a small  cylindrical body.  the past positive example is  insulatedobject +   whose description is:  a positive example of 'graspable' is something insulated with a small  cylindrical body  and hot contents.  the current negative example is  uninsulated-object -   whose description is:  a negative example of 'graspable' is something with a small  cylindrical body  and hot contents.  the concept description incorrectly classifies this example as positive  so the concept description is specialized by an  add-req subtask   with a req-boundary  and an  add-not subtask   with a not-boundary . neg-rulegeneralizations are used by this task. a predicate p is a negrule-generalization of a predicate q iff q has a most-gen-pred that is a neg-antecedent-of a rule that has p as a consequent. 
     the add-req subtask initializes the req-boundary to all predicates true of the past positive example but not of the current negative example. next  the add-req subtask chooses a req-boundary element q and tries to add-req it to the concept description. if the attempt is successful  then the differencing task terminates. otherwise  q is deleted from the req-boundary. if q is true of the past example  false of the curr example  and is not currently a req'd  then its rulegeneralizations are added to the req-boundary  and its negrule-generalizations are added to the not-boundary. 
     the add-not subtask initializes the not-boundary to all predicates true of curr but not of past. next the add-not subtask chooses a not-boundary element q and tries to addnot it to the concept description. if the attempt is successful  then the differencing task terminates. otherwise  q is deleted from the not-boundary. if q is true of the curr example  false of the past example  and is not currently a not  then its rule-generalizations are added to the not-boundary  and its neg-rule-generalizations are added to the req-boundary. 
　　lair alternates between these two subtasks  since each adds new elements to the other's boundary. success by either the add-req subtask or the add-not subtask produces a relatively correct description. 
v. implementation details 
　　lair is implemented as a frame and rule-based system in interlisp-d and ops1 running on a xerox 1 workstation. lair learns the domain theory for the  cup  domain  consisting of 1 rule for  hot  and  cup   1 rules each for  stable    open-vessel   graspable   and  liftable.  learning requires 1 examples and takes about 1 seconds of cpu time. 
vi. conclusions 
     lair makes some prelimary steps towards suggesting how constructive induction can be guided. first  constructive induction can be decomposed to simpler operations. second  the simpler operations can be constrained so that they preserve relative correctness and are relatively complete. third  inference need not be done all at once upon presentation of an example  but on an as-needed basis to meet the specific needs of the learning process at each point in time. this contrasts with a related approach in marvin  that computes the logical closure of each presented or generated example as a preliminary step to revising the concept description. fourth  constraints can be formulated on how the space of descriptors is searched by specifying boundary conditions for specific learning subtasks. 
　　lair can be compared to explanation-based methods for learning concepts such as  cup.  explanation-based systems  require that the domain knowledge and the goal concept be given to the system whereas lair induces both these kinds of knowledge from the examples. however  lair may not learn the best description for efficient recognition of instances of the concept. explanation-based systems try to induce efficient descriptions by imposing an operationality criterion  on learned descriptions. 
