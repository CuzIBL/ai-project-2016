
dimensionality reduction is a much-studied task in machine learning in which high-dimensional data is mapped  possibly via a non-linear transformation  onto a low-dimensional manifold. the resulting embeddings  however  may fail to capture features of interest. one solution is to learn a distance metric which prefers embeddings that capture the salient features. we propose a novel approach to learning a metric from side information to guide the embedding process.
our approach admits the use of two kinds of side information. the first kind is class-equivalence information  where some limited number of pairwise  same/different class  statements are known. the second form of side information is a limited set of distances between pairs of points in the target metric space. we demonstrate the effectiveness of the method by producing embeddings that capture features of interest.
1 introduction
many machine learning approaches use distances between data points as a discriminating factor. in some cases  the plain euclidean distance between points is meaningful and such methods can work well. such distances are the starting point of many popular dimensionality-reduction techniques  such as mds  cox and cox  1   lle  roweis and saul  1   isomap  tenenbaum et al.  1   lem  belkin and niyogi  1   and sde  weinberger and saul  1 . often  however  this distance does not capture the distinction one is trying to characterize. approaches such as kernelized methods address this issue by mapping the points into new spaces where euclidean distances may be more useful. an alternative approach is to construct a new distance metric over the points and use it in place of euclidean distances  as explored by xing et al.  xing et al.  1 . in this approach  some amount of side information is employed to learn a distance metric that captures the desired distinction.
　in some situations it may be possible to obtain a small amountof informationregardingthe similarity of some points in a particular data set. consider a large collection of images.
while it would be expensive to have a human examine and label the entire set  it might be feasible to have a human examine a small subset and provide information on how pairs of images relate to each other. similarly  some expensive experiment might yield useful similarity information for a subset of the data. this paper will show how two kinds of such side information can be used in a preprocessing step for embedding techniques  leading to embeddings that capture the target properties. the method improves over earlier work in this area by using standard  off-the-shelf  optimization methods and by allowing more flexibility in the side information used.
　the first kind of side information identifies pairs of points that belong to the same class or pairs that belong to different classes. note that this information is about the classequivalence/inequivalence of points but does not give the actual class labels. consider four points  x1 x1 x1  and x1. given side information that x1 and x1 are in the same class  and that x1 and x1 also share a class  we cannot be certain whether the four points fall into one or two classes.
　the second kind of side information takes the form of partial information about the similarity of points in the form of distances. for some pairs of points  one may have distances corresponding to an informative metric space  obtained by some expensivemeasurement. molecularconformationproblems are a good example. some of the distances between pairs of atoms in a given molecule can be determined by nuclear magnetic resonance spectroscopy  but the procedure is costly and certainly not guaranteed to provide all such distances. determining the rest of the distances can help in classifying the conformation of the molecule  see  crippen and havel  1  for more details .
　these two kinds of side information can be used to learn a new distance metric. the distances between points in this new space can then be used with any embedding technique. we start by showing how class-equivalence side information can be used to learn such a metric and show the effect of using this new metric with a variety of embedding techniques. we then show how multiple applications of this method can be used to combine several learned distance metrics together into one in order to capture multiple attributes in the embedding. we also discuss how the class-equivalence approach can be kernelized  allowing for nonlinear transformations of the metric. finally  we formulate a method for using the second type of side information  where we have partial information about desirable target distances between some pairs of points. experimental results demonstrate the value of this preprocessing step.
1 related work
our use of class equivalence relations to learn a metric follows  xing et al.  1 . in that work  a new distance metric is learned by considering side information. xing et al. used side information identifying pairs of points as  similar . they then construct a metric that minimizes the distance between all such pairs of points. at the same time  they attempt to ensure that all  dissimilar  points are separated by some minimal distance. by default  they consider all points not explicitly identified as similar to be dissimilar. they present algorithms for optimizing this objective and show results using the learned distance for clustering  an application in which an appropriatedistance metric is crucial. other work on learning distance metrics has been primarily focused on classification  with side information in the form of class labels  globerson and roweis  1; weinberger et al.  1; goldberger et al.  1 .
1 learning a metric from class-equivalence side information
we will start with the simpler similar/dissimilar pair case  formalizing this notion of side information and stating an objective that will be optimized using standard semidefinite programming software. the use of this kind of side information allows one to select a characteristic for distinction. for example  one may have several images of faces. one sensible cluster is by presence or absence of a beard. another is by the presence or absence of glasses. different indications of similarity allow the capturing of either distinction. the following takes the same basic approach as  xing et al.  1  but offers a simpler optimization procedure using  off-the-shelf  optimization methods instead of their iterative method.
1 derivation
given a set of t points  {xi}ti=1   rn  we identify two kinds of class-related side information. first  a set class-equivalent  or similar  pairs of points
	s :	 xi xj  （ s	if xi and xj are similar
and  second  a set of class-inequivalent  dissimilar  pairs
	o :	 xi xj  （ o	if xi and xj are dissimilar
　we then wish to learn a matrix a that induces a distance metric d a  over the points

where.
　we define the following loss function  which  when minimized  attempts to minimize the squared induced distance between similar points and maximize the squared induced distance between dissimilar points

the optimization problem then becomes
	 and tr a  = 1	 1 
　the first constraint  positive semidefiniteness  ensures a euclidean metric. the second excludes the trivial solution where all distances are zero. the constant in this constraint is arbitrary  affecting only the scale of the space. this objective will be optimized using standard semidefinite programming software and so it must be converted to a linear objective. expanding the loss function

each squared distance term must be converted. we start by observing that vec xy z  =  zt   x vec y    where vec   simply rearranges a matrix into a vector by concatenating columns and   is the kronecker product. note that  xi   xj ta xi   xj  = vec  xi   xj ta xi   xj   because the left-hand side is a scalar. using this and the fact that  at   bt  = vec bat t  we can rewrite the squared distance terms as
 xi   xj ta xi   xj 
=vec  xi   xj ta xi   xj  =  xi   xj t    xi   xj t vec a =vec  xi   xj  xi   xj t tvec a =vec a tvec  xi   xj  xi   xj t 　the linear loss function is then vec a tvec  xi   xj  xi   xj t   
vec a tvec  xi   xj  xi   xj t 
	=	vec

　this form  along with the two constraints from  1   can be readily submitted to an sdp solver to optimize the matrix a1. aside from this convenient form  this formulation has other advantagesover that used by xing et al.  especially with respect to the side information we can convey.
　xing et al. require at least one dissimilar pair in order to avoid the trivial solution where all distances are zero. the constraint on the trace that we employ means that we need not place any restrictions on pairings. side information can consist of similar pairs only  dissimilar pairs only  or anycombination of the two; the method still avoids trivial solutions.
furthermore  in the absence of specific information regarding dissimilarities  xing et al. assume that all points not explicitly identified as similar are dissimilar. this information may be misleading  forcing the algorithm to separate points that may  in fact  be similar. the formulation presented here allows one to specify only the side information one actually has  partitioning the pairings into similar  dissimilar  and unknown.
1 results
once a metric has been learned  the new distances can be used with any embedding technique. to demonstrate the benefits of this approach  we generated embeddings with and without the preprocessing step  informing the preprocessed embeddings about some characteristic of interest  and then examined the result. to show that the side information truly informs the embedding  two sets of informed embeddings were generated from the same data set  each with side information pertaining to two different characteristics. structure in the resulting embeddings that captures the two different characteristics within a single data set is evidence that the method works as intended.
　embeddings were generated using a variety of techniques including mds  isomap  lem  lle  and sde. the data set consisted of 1 images of faces and two distinctions are identified by side information: faces with beards vs. faces without beards and faces with glasses vs. faces without glasses. in one set of experiments  which we will call all-similar   all similar pairs were identified but no dissimilar pairs. the second set  which we will call five-pairs   simulating a situation where labelling is expensive  identifies only four similar pairs and one dissimilar pair. the pairs were selected at random. techniques using the preprocessor are labelled as  equivalence-informed   e.g.  equivalenceinformed mds . in each case  a two-dimensional embedding is shown. the two classes are marked with x and o  respectively. additionally  a subset of the images are displayed on the plot  plots with all images are unreadable . some plots have been omitted due to space constraints.
　inspection reveals that  in general  the informed versions of these embeddings manage to separate the data based on the target property  whereas the uninformed versions are typically chaotic. even when separation is poor in the informed embedding  there is usually much more structure than in the uninformed embedding. equivalence-informed mds  figures 1 and 1  five-pairs beards and all-pairs glasses omitted  offers mixed results  especially with five-pairs for glasses vs. no glasses  figure 1   but mds is a comparatively crude embedding technique in any case. isomap with all-pairs works very well  figure 1  glasses omitted and with five-pairs it still manages to group the two classes but does not separate them well  figure 1  glasses omitted . lem separates well with all-pairs  figure 1  glasses omitted  but shows weak separation in the five-pairs case  figure 1  glasses omitted . the same effective grouping and varying separation occurs for lle  figures omitted . finally  sde groups well  but is weak in separation in all cases  figures 1 and 1  glasses omitted .
　these results show that the side information can be effectively exploited to capture characteristics of interest where uninformed embedding techniques fail to automatically discover them. moreover  two different characteristics  beards and glasses  have been captured within the same data set  even when using only small quantities of class-equivalence side information. finally  the preprocessor is effective with a wide range of embedding techniques.
	mds	equivalence informed mds

figure 1:	mds and equivalence-informed mds with
 un bearded distinction  all equivalent pairs 
	mds	equivalence informed mds

figure 1: mds and equivalence-informed mds with glasses/no glasses distinction  1 equivalent/1 inequiv. pairs 
	isomap	equivalence informed isomap 

figure 1: isomap and equivalence-informed isomap with  un bearded distinction  all equivalent pairs 
　finally  figures 1 and 1 are provided to give a numerical notion of the improvement offered by informed embeddings. the plots show the misclassification rate of k-means clustering in recoveringthe true classes of the images  after applying each of the uninformed and informed embeddings  indicated by  e.g.   mds  and  inf. mds  . the informed methods achieve a lower error rate than their uninformed counterparts by better separating the data  except 1-pairs sde .
	isomap	equivalence informed isomap 

figure 1: isomap and equivalence-informed isomap with
 un bearded distinction  1 equivalent/1 inequivalent pairs 
	lem	equivalence informed lem

figure 1:	lem and equivalence-informed lem with
 un bearded distinction  all equivalent pairs 
1 multiple-attribute metric learning with class-equivalence side information
in some cases  there may be more than one distinction to capture in data  e.g.  glasses vs. no glasses and beards vs. no beards . the method above can be extended to construct a distance metric using multiple sets of side information  each corresponding to a different criterion. multiple metrics are learned and then combined to form a single metric.
　suppose there are k different sets of side information over the same set of points.	using the optimization described above  k transformations  a1 ，，， ak can be learned. from each ai  the dominant eigenvector vi can be taken and a new matrix assembled where each column is one of these eigenvectors  a． =   v1	，，，	vk  . this combined matrix delem	equivalence informed lem

figure 1:	lem and equivalence-informed lem with
 un bearded distinction  1 equivalent/1 inequivalent pairs 
	sde	equivalence informed sde

figure 1:	sde	and	equivalence-informed sde	with
 un bearded distinction  all equivalent pairs 
	sde	equivalence informed sde

figure 1:	sde	and	equivalence-informed sde	with
 un bearded distinction  1 equivalent/1 inequivalent pairs 
fines a rank k linear subspace onto which we can project the data. applying singular value decomposition to a．  one can form a matrix u from the k eigenvectors corresponding to the largest eigenvalues. the final transformation is then a  = uut  which is an orthonormal basis combining the distinctions drawn by each kind of side information.
1 results
we demonstrate the effectiveness of this method by creating a single embedding that captures two distinctions. we repeat the earlier experiments but using both the  un bearded and glasses/no glasses criteria together. results were generated for all the embedding methods and their multipleattribute  equivalence-informed versions but some are omitted for space. in all cases  the side information consisted of all similar pairs and no dissimilar pairs.
　the informed embeddingstend to discover the four distinct categories of faces  combinationsof the presence and absence of beards and glasses . informed mds  figure 1  separates

figure 1: clustering error for beards vs. no beards  left: all equivalent pairs right: 1 equiv/1 inequiv pairs 

figure 1: clustering error for glasses vs. no glasses  left: all equivalent pairs right: 1 equiv/1 inequiv pairs 
these quite well  as do informed isomap  figure 1  and informed lle  figure 1 . results with informed lem  figure omitted  are roughly grouped. finally  informed sde  figure 1  does has trouble separating two categories  but a lot of useful structure is present. in all cases  the informed embeddings have distinct structure that the uninformed embeddings fail to capture. the only feature the uninformed embeddings seem to capture is the relative darkness of the images.
	mds	multi attribute informed mds

figure 1: mds and multi-attribute informed mds with
 un bearded and  no glasses attributes  all equivalent pairs 
　these results show that the multiple-attribute approach can capture the desired properties in a single embedding  while the uninformedversions fail to capture either of the properties of interest. again  the method is straightforward to apply and works with a variety of embedding techniques.
1 kernelizing metric learning
not uncommonly  nonlinear transformations of the data are required to successfully apply learning algorithms. one efficient method for doing this is via a kernel that computes a
	isomap	multi attribute informed isomap

figure 1: isomap and multi-attribute informed isomap with
 un bearded and  no glasses attributes  all equivalent pairs 
	lle	multi attribute informed lle

figure 1:	lle and multi-attribute informed lle with
 un bearded and  no glasses attributes  all equivalent pairs 
	sde	multi attribute informed sde

figure 1:	sde and multi-attribute informed sde with
 un bearded and  no glasses attributes  all equivalent pairs 
similarity measure between any two data points. in this section  we show how to learn a metric in the feature space implied by a kernel  allowing our use of side information to be extended to nonlinear mappings of the data.
　conceptually  the points are mapped into a feature space by some nonlinear mapping Φ   and a metric is learned in that space. actually applying the mapping is often undesirable  e.g.  the feature vectors may have infinite dimension   so we employ the well known kernel trick  using some kernel k xi xj  that computes inner products between feature vectors without explicitly constructing them.
　the squared distances in our objective have the form  xi   xj ta xi   xj . because a is positive semidefinite  it can be decomposed into a = wwt. w can then be expressed as a linear combination of the data points  w = xβ  via the kernel trick. rewriting the squared distance
 xi   xj ta xi   xj = xi   xj twwt xi   xj = xi   xj txββtxt xi   xj = xti x   xtj x ββt xtxi   xtxj = xtxi   xtxj ta xtxi   xtxj where a = ββt  we have now expressed the distance in terms of the matrix to be learned  a  and inner products between data points  which can be computed via the kernel  k. the optimization of a proceeds as in the non-kernelized version but with one additional constraint. the rank of a must be t because β is t by n and a =ββt. however  this constraint is problematic  so we drop it during the optimization and then find a rank t approximation of a via singular value decomposition.
1 using partial distance side information
we will now discuss the use of the second kind of side information mentioned at the beginning of this paper. recall that in this case  we have side information that gives exact distances between some pairs of points in some space natural to the data. such partial distance information can be used to inform our learned metric. given a set of similarities in the form of pairs for which distances are known
   s :  xi xj  （ s if the target distance dij is known the following cost function is employed  which attempts to preserve the set of known distances

the optimization problem is then
	minl a 	s.t. 
a
a convenient form for this optimization is obtained using the same approach for quadratic terms used earlier

vec a tvec bij vec bij tvec a 
+d1ij   1dijvec a t vec bij 
where bij =  xi   xj  xi   xj t. note that the d1ij term in the above is a constant so it can be dropped for the purposes of minimization  and the loss rewritten as
l a 	=	vec
	=	vec a t  qvec a    1r 
where  and r	=
. this objective is still quadratic but
a linear objective can be obtained via the schur complement  boyd and vandenberghe  1   as we outline here.
　the schur complement relates the positive semidefiniteness of the matrix on the left and the expression on the right by an if-and-only-if relation

by decomposing q = sts  a matrix of the form

can be constructed. by the schur complement  if  then the following relation holds
	1vec a t r + t   vec a tstsvec a 	−	1
note that this quantity is scalar. as long as j is positive semidefinite  the scalar t is an upper bound on the loss vec a tstsvec a    1vec a tr
	=	vec a tqvec a    1vec a tr	＋ t
therefore  minimizing t subject to also minimizes the objective. this optimization problem can be readily solved by standard semidefinite programming software
	mint	s.t.		and	
a
1 results
again  we demonstrate effectiveness by generating uninformed and informed embeddings but with target distances specified for some pairs of points. the distances used here are a portion of the distance da computed in section 1. that is  there are 1 images of faces and two distinctions are identified by class-equivalence side information: faces with beards vs. faces without beards and faces with glasses vs. faces without glasses. for each distinction  the distance matrix da is computed  where no dissimilar pairs but all similar pairs are identified. then 1 pairs of points are selected at random and their distances in da are used as the side information for learning a new distance metric. this new distance metric is then used to inform the embeddings.
　results using these distances are somewhat noisier than with the class-equivalence informed embeddings but still provide good structure compared with the uninformed versions  and they group the data. informed mds  figure 1  glasses omitted  and informed isomap  figures 1  glassed omitted  group quite effectively  with a few images placed inappropriately. informed lem  figures 1 and 1   informed lle  figures omitted   and informed sde  figures omitted  all give results similar to one another  and all show noisier performance on the glasses attribute than on the beards  beards are likely easier since they create a more substantial difference in an image than glasses . all methods group beardedness quite well. these last results show that preprocessing based on partial distance information is effective  although less so than the class-equivalence information. the informed embeddings are still more useful than the uninformed.
1 conclusions
many machine learning techniques handle complex data by mapping it into new spaces and then applying standard techniques  often utilizing distances in the new space. learning a distance metric directly is an elegant means to this end. this research shows how side information of two forms  classequivalence information and partial distance information  can
	mds	distance informed mds

figure	1:	mds	and	distance-informed	mds	with
 un bearded distinction  1 distances 
	isomap	distance informed isomap

figure 1:	isomap and distance-informed isomap with
 un bearded distinction  1 distances 
be used to learn a new distance as a preprocessing step for embedding. the results demonstrate that side information allows us to capture attributes of the data within the embedding in a controllable manner. even a small amount of side information can improve the embedding's structure. furthermore  the method can be kernelized and also used to capture multiple attributes in the same embedding. its advantages over existing metric learning methods are a simpler optimization formulation that can be readily solved with off-the-shelf software and greater flexibility in the nature of the side information one can use. in all  this technique represents a useful addition to our toolbox for informed embeddings.
	lem	distance informed lem

figure	1:	lem	and	distance-informed	lem	with
 un bearded distinction  1 distances 
	lem	distance informed lem

figure	1:	lem	and	distance-informed	lem	with
 no glasses distinction  1 distances 
