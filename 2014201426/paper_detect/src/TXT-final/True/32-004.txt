ion model: 
unsupervised learning of topic hierarchies from text data 
thomas hofmann 
computer science division  uc berkeley &; 
international cs institute  berkeley  ca hofmann cs.berkeley.edu 

abstract 
this paper presents a novel statistical latent class model for text mining and interactive information access. the described learning architecture  called cluster-abstraction model  cam   is purely data driven and utilizes contact-specific word occurrence statistics. in an intertwined fashion  the cam extracts hierarchical relations between groups of documents as well as an abstractive organization of keywords. an annealed version of the expectation-maximization  em  algorithm for maximum likelihood estimation of the model parameters is derived. the benefits of the cam for interactive retrieval and automated cluster summarization are investigated experimentally. 
1 	introduction 
intelligent processing of text and documents ultimately has to be considered as a problem of natural language understanding. this paper presents a statistical approach to learning of language models for context-dependent word occurrences and discusses the applicability of this model for interactive information access. the proposed technique is purely data-driven and does not make use of domain-dependent background information  nor does it rely on predefined document categories or a given list of topics. 
　the cluster-abstraction model  cam   is a statistical latent class or mixture model  mclachlan and basford  1  which organizes groups of documents in a hierarchy. compared to most state-of-the-art techniques based on agglomerative clustering  e.g.   jardine and van rijsbergen  1; croft  1; willett  1   it has several advantages and additional features: as a probabilistic model the most important advantages are: 
  a sound foundation in statistics and probabilistic inference 
  a principled evaluation of generalization perfor-mance for model selection  
  efficient model fitting by the em algorithm  
1 	machine learning 
  an explicit representation of conditional indepen-dence relations. 
additional advantages are provided by the hierarchical nature of the model  namely: 
  multiple levels of document clustering  
  discriminative 	topic 	descriptors 	for 	document groups  
  coarse-to-fine approach by annealing. 
　the following section will first introduce a nonhierarchical probabilistic clustering model for documents  which is then extended to the full hierarchical model. 
1 	probabilistic clustering of documents 
let us emphasize the clustering aspect by first introducing a simplified  non-hierarchical version of the cam which performs 'flat' probabilistic clustering and is closely related to the distributional clustering model  pereira et a/.  1  that has been used for word clustering and text categorization  baker and mccallum  1 . 
let 	denote documents and denote words or word stems. moreover let wd refer to the vector  sequence  of words wdt constituting d. word frequencies are summarized using count variables n d w  which indicate how often a word w occurred in a document denotes the document length. 
　following the standard latent class approach  it is assumed that each document d belongs to exactly one cluster  where the number of clusters is assumed to be fixed for now. introducing class conditional word distributions probabilities p c   stacked in a parameter vector 
the factorial expression reflects conditional independence assumptions about word occurrences in wd  bagof-words model . 

　starting from  1  the standard em approach  dempster et ai  1  to latent variable models is employed. in em two re-estimation steps are alternated: 
  an expectation  e -step for estimating the posterior probabilities of the unobserved clustering variables for a given parameter estimate 
  a maximization  m -step  which involves maximiza-tion of the so-called expected complete data log-like-
lihood for given posterior probabilities with respect 
	to the parameters. 	  
the em algorithm is known to increase the observed likelihood in each step  and converges to a  local  maximum under mild assumptions. 
　an application of bayes' rule to  1  yields the following e-step re-estimation equations for the distributional clustering model 

the m-step stationary equations obtained by differentiating c are given by 

these equations are very intuitive: the posteriors encode a probabilistic clustering of doc-
uments  while the conditionals . represent average word distributions for documents belonging to group c. of course  the simplified flat clustering model defined by  1  has several deficits. most severe are the lack of a multi-resolution structure and the inadequacy of the 'prototypical' distributions p w c  to emphasize discriminative or characteristic words  they are in fact typically dominated by the most frequent word occurrences . to cure these flaws is the main goal of the hierarchical extension. 
1 document hierarchies and abstraction 
1 	the cluster-abstraction model 
most hierarchical document clustering techniques utilize agglomerative algorithms which generate a cluster hierarchy or dendogram as a by-product of successive cluster merging  cf.  willett  1  . in the cam we will use an explicit abstraction model instead to represent hierarchi-
cal relations between document groups. this is achieved by extending the 'horizontal' mixture model of the previous section with a 'vertical' component that captures the specificity of a particular word w in the context of a document d. it is assumed that each word occurrence wdt has an associated abstraction node a  the latter being identified with inner or terminal nodes of the cluster hierarchy  cf. figure 1  a  . 
　to formalize the sketched ideas  additional latent variable vectors ad with components adt are introduced which assign words in d to exactly one of the nodes in the hierarchy. based on the topology of the nodes in the hierarchy the following constraints between the cluster variables cd and the abstraction variables adt are imposed: 
		 1  
the notation  will be used as a shortcut to refer to nodes a above the terminal node c in the hierarchy. eq.  1  states that the admissible values of the latent abstraction variables adt for a particular document with latent class cd are restricted to those nodes in the hierarchy that are predecessors of cd. this breaks the permutation-symmetry of the abstraction nodes as well as of the document clusters. an abstraction node a at a particular place in the hierarchy can only be utilized to  explain  words of documents associated with terminal nodes in the subtree of a. a pictorial representation can be found in figure 1  b : if d is assigned to c the choices for abstraction nodes for word occurrences wdt are restricted to the 'active'  highlighted  vertical path. one may think of the cam as a mixture model with a horizontal mixture of clusters and a vertical mixture of abstraction levels. each horizontal component is a mixture of vertical components on the path to the root  vertical components being shared by different horizontal components according to the tree topology. 
　generalizing the non-hierarchical model in  1   a 
　probability distribution  over words is attached to each node  inner or terminal  of the hierarchy. after application of the chain rule  the complete data model  i.e.  the joint probability of all observed and latent variables  can be specified in three steps 
and 
note that additional document-specific vertical mixing proportions  over abstraction nodes above cluster c have been introduced  with the understanding that 
 = 1 whenever it is not the case that a | c. if one makes the simplifying assumption that the same mixing proportions are shared by all documents assigned 
to a particular cluster  the so-
lution degenerates to the distributional clustering model since one may always choose  however  we propose to use this more parsimonious model and fit p a|c  from held-out data  a fraction of words held out from each document   which is in the spirit of model interpolation techniques  jelinek and mercer  1 . 
1 	em algorithm 
as for the distributional clustering model before  we will derive an em algorithm for model fitting. the e-step requires to compute  joint  posterior probabilities of the form  after applying the chain 
1 


figure 1:  a  sketch of the cluster-abstraction structure   b  the corresponding representation for assigning occur* rences to abstraction levels in terms of latent class variables. 

rule one obtains: 

the m-step re-estimation equations for the conditional word distributions are given by 
which is evaluated on the held-out data. finally  it may be worth taking a closer look at the predictive word probability distribution p w d  in the cam which is given by 
if we assume for simplicity that for some c  hard clustering case   then the word probability of d is modeled as a mixture of occurrences from different abstraction levels a. this reflects the reasonable assumption that each document contains a certain mixture of words ranging from general terms of ordinary language to highly specific technical terms and specialty words. 
1 	annealed em algorithm 
there are three important problems which also need to be addressed in a successful application of the cam: first and most importantly  one has to avoid the problem of overfitting. second  it is necessary to specify 
1 	machine learning 
a method to determine a meaningful tree topology including the maximum number of terminal nodes. and third  one may also want to find ways to reduce the sensitivity of the em procedure to local maxima. an answer to all three questions is provided by a generalization called annealed em  hofmann and puzicha  1 . annealed em is closely related to a technique known as deterministic annealing that has been applied to many clustering problems  e.g.  rose et al. 1; pereira et al.  1  . since a thorough discussion of annealed em is beyond the scope of this paper  the theoretical background is skipped and we focus on a procedural description instead. the key idea in deterministic annealing is the introduction of a temperature parameter   applying the annealing principle to the clustering variables  the posterior calculation in  1  is generalized by replacing n d w  in the exponent by 
	for 	this dampens the likelihood con-
tribution linearly on the log-probability scale and will in general increase the entropy of the  annealed  posterior probabilities. in annealed em  t is utilized as a control parameter which is initialized at a high value and successively lowered until the performance on the heldout data starts to decrease. annealing is advantageous for model fitting  since it offers a simple and inexpensive regularization which avoids overfitting and improves the average solution quality. moreover  it also offers a way to generate tree topologies  since annealing leads through a sequence of so-called phase transitions  where clusters split. in our experiments  t has been lowered until the perplexity  i.e.  the log-averaged inverse word probability  on held-out data starts to increase  which automatically defines the number of terminal nodes in the hierarchy. more details on this subject can be found in  hofmann and puzicha  1 . 
1 	results and conclusion 
all documents used in the experiments have been preprocessed by word suffix stripping with a word stemmer. a standard stop word list has been utilized to eliminate the most frequent words  in addition very rarely occurring words have also been eliminated. an exam-

saul  l.; jordan  m.i. learning in boltnnann trees. 
neural computation  nov. 1  vol1  {no.1 :1~m. 
	 a  verbatim 	 b  word stems 
introduces a large family of boitimann machines that can be trained by standard introdnc larg famili boltzmann machin train gradient descent. the networks can have one or more layers of hidden units  with standard gradient descent network layer hidtree-like connectivity. we show how to implement a supervised learning algorithm den unit connect implem supervis learn algo-
for these boltzmann machines exactly  without resort to simulated or mean-field rithm boltzmann machin exactli simul anneal annealing. the stochastic averages that yield the gradients in weight space are stochast averag yield gradient weight space 
	computed by the technique of decimation. we present results on the problems of 	techniqu present result problem pariti detec 
	n-bit parity and the detection of hidden symmetries. 	hidden symmetri 
 c  ghost writer 
level 1 	paper model base new method gener process differ effect approach provid set studi develop author level 1 	function propos model error method input optim gener neural paramet paper obtain shown appli output level 1 	gener number set neural propos function perform method inform data given obtain approxim dynamic input level 1 	neural pattern rule number process recogni rate perform classif propos gener input neuron time properti data level 1 	converg neural optim method rule rate dynamic process pattern paramet studi statist condition adapt limit level 1 	perceptron exampl error gener rale onlin calcul deriv backpropag simpl output asymptot solution separ unsupervis level 1 	error neural architectur perform entropi statist multilay activ backpropag gener number maximum pattern phase level 1 	teacher delta output introdnc sampl replica decai nois projec correl student temperatur gain dynamic predic 
figure 1:  a  abstract from the generated learn document collection   b  representation in terms of word stems  
 c  words with lowest perplexity under the cam for words not occurring in the abstract  differentiated according to the hierarchy level . 

figure 1: group descriptions for exemplary inner nodes by most frequent words and by the highest probability words 

from the respective cam node. 
ple abstract and its index term representation is depicted in figure 1  a   b . the experiments reported are some typical examples selected from a much larger number of performance evaluations. they are based on two datasets which form the core of our current prototype system: a collection of 1 recent papers with 'learning' as a titleword  including all abstracts of papers from machine learning vol. 1  learn   and a dataset of 1 recent papers with 'cluster' in the title  cluster . 
　the first problem we consider is to estimate the probability for a word occurrence in a text based on the statistical model. figure 1  c  shows the most probable words from different abstraction levels  which did not occur in the original text of figure 1  a . the abstractive organization is very helpful to distinguish layers with trivial and unspecific word suggestions  like 'paper'  up to highly specific technical terms  like 'replica' . 
　one of the most important benefits of the cam is the resolution-specific extraction of characteristic keywords. in figure 1 and 1 we have visualized the top 1 levels for the dataset learn and cluster  respectively. the overall hierarchical organization of the documents is very satisfying  the topological relations between clusters seems to capture important aspects of the inter-document similarities. in contrast to most multiresolution approaches the distributions at inner nodes of the hierarchy are not obtained by a coarsening procedure which typically performs some sort of averaging over the respective subtree of the hierarchy. the abstraction mechanism in fact leads to a specialization of the inner nodes. this specialization effect makes the probabilities p w a  suitable for cluster summarization. notice  how the low-level nodes capture the specific vocabulary of the documents associated with clusters in the subtree below. the specific terms become automatically the most probable words in the component distribution  because higher level nodes account for more general terms. to stress this point we have compared the abstraction result with probability distributions obtained by averaging over the respective subtree. figure 1 summarizes some exemplary comparisons showing that averaging mostly results in high probabilities for rather unspecific terms  while the cam node descriptions are highly discriminative. the node-specific word distribution thus offer a principled and very satisfying solution to the problem of finding resolution-specific index terms for document groups as opposed to many circulating ad hoc heuristics to distinguish between typical and topical terms. 
　an example run for an interactive coarse-to-fine retrieval with the cluster collection is depicted in figure 1  where we pretend to be interested in documents on clustering for texture-based image segmentation. in a real interactive scenario  one would of course display more than just the top 1 words to describe document groups and use a more advanced shifting window approach to represent the actual focus in a large hierarchy. in addition to the description of document groups by inner node word distributions  the cam also offers the possibility to attach prototypical documents to each of the nodes  the ones with maximal probability p a|d    to compute most probable documents for a given query  etc. all types of information  the cluster summaries by 
1 


figure 1: top 1 levels of the cluster hierarchy for the learn dataset. nodes are represented by their most probable words. left/right successors of nodes in row 1 are depicted in row 1a and 1b  respectively. similarly  left successors of nodes in row 1a/1b can be found in rows 1aa/1ba and right successors in rows 1ab/1bb. 


figure 1: example run of an interactive image retrieval for documents on 'texture-based image segmentation' with one level look-ahead in the cam hierarchy. 
1 	machine learning 
 locally  discriminant keywords  the keyword distributions over nodes  and the automatic selection of prototypical documents are particularly beneficial to support an interactive retrieval process. due to the abstraction mechanism the cluster summaries are expected to be more comprehensible than descriptions derived by simple averaging. the hierarchy offers a direct way to refine queries and can even be utilized to actively ask the user for additional specifications. 
　conclusion: the cluster-abstraction model is a novel statistical approach to text mining which has a 
　sound foundation on the likelihood principle. the dual organization of document cluster hierarchies and keyword abstractions makes it a particularly interesting model for interactive retrieval. the experiments carried out on small/medium scale document collections have emphasized some of the most important advantages. since the model extracts hierarchical structures and supports resolution dependent cluster summarizations  the application to large scale databases seems promising. 


figure 1: top 1 levels of the cluster hierarchy for the cluster dataset  cf. comments for figure 1 . 

	acknowledgments: 	helpful comments of jan 
puzicha  sebastian thrun  andrew mccallum  tom mitchell  hagit shatkay  and the anonymous reviewers are greatly acknowledged. thomas hofmann has been supported by a daad postdoctoral fellowship. 
