
given sensors to detect object use  commonsense priors of object usage in activities can reduce the need for labeled data in learning activity models. it is often useful  however  to understand how an object is being used  i.e.  the action performed on it. we show how to add personal sensor data  e.g.  accelerometers  to obtain this detail  with little labeling and feature selection overhead. by synchronizing the personal sensor data with object-use data  it is possible to use easily specified commonsense models to minimize labeling overhead. further  combining a generative common sense model of activity with a discriminative model of actions can automate feature selection. on observed activity data  automatically trained action classifiers give 1% precision/recall on 1 actions. adding actions to pure object-use improves precision/recall from 1% to 1% over 1 activities.
1 introduction
systems capable of recognizing a range of human activity in useful detail have a variety of applications. key problems in building such systems include identifying a small set of sensors that capture sufficient data for reasoning about many activities  identifying the features of the sensor data relevant to the classification task and minimizing the human overhead in building models relating these feature values to activities. the traditional approach  using vision as the generic sensor  moore et al.  1; duong et al.  1   has proved challenging because of the difficulty of identifying robust  informative video features sufficient for activity recognition under diverse real-world conditions and the considerable overhead of providing enough labeled footage to learn models.
　a promising recent alternative  philipose et al.  1; tapia et al.  1  is the use of dense sensing  where individual objects used in activities are tagged with tiny wireless sensors which report when each object is in use. activities are represented as probabilistic distributions over sequences of object use  e.g.  as hidden markov models. these simple models perform surprisingly well across a variety of activities and contexts  primarily because the systems are able to detect consistently the objects in use. further  because objects used in day-to-day activities are common across deployment conditions  e.g.  most people use kettles  cups and teabags in making tea   it is possible to specify broadly applicable prior models for the activities simply as lists of objects used in performing them. these priors may either be specified by hand or mined automatically off the web  perkowitz et al.  1 . given these simple models as priors and unlabeled traces of objects used in end-user activities  a technique for learning with sparse labels  e.g.  em  may be used to produce customized models with no per-activity labeling  wyatt et al.  1 .
　object use is a promising ingredient for general  lowoverhead indoor activity recognition. however  the approach has limitations. for instance  it may be important to discern aspects of activities  e.g.  whether a door is being opened or closed  indistinguishable by object identity  it is not always practical to tag objects  e.g.  microwaveable objects  with sensors  and multiple activities may use similar objects  e.g.  clearing a table and eating dinner . one way to overcome these limitations is to augment object-use with complementary sensors.
　wearable personal sensors  especially accelerometers and audio   which provide data on body motion and surroundings  are a particularlyinterestingcomplement. these sensors are unobtrusive relatively insensitive to environmentalconditions  especially accelerometers   and previous work  lester et al.  1; bao and intille  1; ravi et al.  1  has shown that they can detect accurately a variety of simple activities  such as walking  running and climbing stairs. we focus on how to use these sensors to detect fine-grained arm actions using objects  such as  scoop with a spoon    chop with a knife    shave with a razor  and  drink with a glass ; note that the physical motion depends on the kind of action and the object used   and also how to combine these actions with object-use data from dense sensors to get more accurate activity recognition.
　we present a joint probabilistic model of object-use  physical actions and activities that improves activity detection relative to models that reason separately about these quantities  and learns action models with much less labeling overhead than conventional approaches. we show how to use the joint prior model  which can be specified declaratively with a little  common sense  knowledge per activity  and could in principle be mined from the web   to automatically infer distri-

figure 1: sensors: ibracelet and msp  left   rfid tagged toothbrush & paste  right   tags circled.
butions over the labels based on object-use data alone: e.g.  given that object  knife  is in use as part of activity  making salad   action  chop with a knife  is quite likely. we therefore call our technique common sense based joint training. given class labels  viola and jones  1  have shown how to automatically and effectively select relevant features using boosting. we adapt this scheme to work over the distributions over labels  so that our joint model is able to perform both parameter estimation and feature selection with little human involvement.
　we evaluate our techniques on data generated by two people performing 1 activities involving 1 actions on 1 objects. we defined simple commonsense models for the activities using an averageof 1 english words peractivity. combining action data with object-use in this manner increases precision/recall of activity recognition from 1% to 1%; the automated action-learning techniques yielded 1% precision/recall over 1 possible object/action classes. to our knowledge this work is the first to demonstrate that simple commonsense knowledge can be used to learn classifiers for activity recognizers based on continuous sensors  with no human labeling of sensor data.
1 sensors
figure 1 shows the sensors we use. we wear two bracelets on the dominant wrist. one is a radio frequency identification  rfid  reader  called the ibracelet  fishkin et al.  1   for detecting object use  and the other is a personal sensing reader  called the mobile sensing platform  msp   lester et al.  1   for detecting arm movement and ambient conditions.
　the ibracelet works as follows. rfid tags are attached to objects  e.g.  the toothbrush and tube of toothpaste in figure 1  whose use is to be detected. these tags are small  1-cent battery-free stickers with an embedded processor and antenna. the bracelet issues queries for tags at 1hz or faster. when queried by a reader up to 1cm away  each tag responds  using energy scavenged fromthe reader signal  with a unique identifier; the identifier can be matched in a separate database to determine the kind of object. the bracelet either stores timestamped tag ids on-board or transmits read tags wirelessly to an ambient base station  and lasts 1 to 1 hours between charges. if a bracelet detects a tagged object  the object is deemed to be in use  i.e.  hand proximity to objects implies object use.
the msp includes eight sensors: a six-degree-of-freedom


figure 1: joint models for activity and action recognition.  a  full joint dbn  above   and  b  a derived layered model used in this paper  below . dotted arrows represent the assignment of the map estimate of a variable in one layer as its observed value in the next.
accelerometer  microphones sampling 1-bit audio at 1khz  ir/visible light  high-frequency light  barometric pressure  humidity  temperature and compass. the data  roughly 1 samples per second  is shipped in real time to an off-board device  such as a cellphone which currently stores it for later processing. in the near future  we hope to perform the processing  inference  in particular   in real-time on the cellphone. to compact and focus the data  we extract f = 1 features from it  including mean  variance  energy  spectral entropy  fft coefficients  cepstral coefficients and band-pass filter coefficients; the end-result is a stream of 1dimensional feature vectors  generated at 1hz. we will write sn = s1 ... sn for the stream of sensor readings  where each si is a paired object name and a vector of msp features.
1 techniques
1 problem statement
let a = {ai} be a set of activity names  e.g.   making tea    b = {bi} be a set of action names  e.g.   pour    o = {oi} be a set of object names  e.g.   teabag   and m = {mi} be the set of all possible msp feature vectors.
　we assume coarse  commonsense  information linking objects  activities and actions. let oa = { a oa |a （ a oa = {o1 ... ona} s.t. oi is often used in a}  e.g.   a oa  =  make tea {kettle milk sugar teabag}  . let boa = { a o ba o = {b1 ... bma o} |a （ a o （
oa s.t. bi is performed when using object o as part of a}  e.g.   make tea milk {pour pick up}  .
　in monitoring peoples' day-to-day activities  it is relatively easy to get large quantities of unlabeled sensor data sn. getting labeled data is much harder. in what follows  we assume no labeling at all of the data. finally  although training data will consist of synchronized object-use and personal sensor data  the test data may contain just the latter: we expect endusers to transition between areas with and without rfid instrumentation.
　given the abovedomaininformationand observeddata  we wish to build a classifier over msp data that:
  infers the current action being performed and the object on which it is being performed.
  combines with object-use data o  when available  to produce better estimates of current activity a.
  for efficiency reasons  usesfeatures of m.
1 a joint model
the dynamic bayesian network  dbn  of figure 1 a  captures the relationship between the state variables of interest . the dashed line in the figure separates the variables representing the state of the system in two adjacent time slices. each node in a time slice of the graph represents a random variable capturing part of the state in that time slice: the activity  a  and action  b  currently in progress  and the msp data  m  and the object  o  currently observed. the directed edges are inserted such that each random variable node in the induced graph is independent of its non-descendants given its parents. each node xi is parameterized by a conditional probability distribution  cpd  p xi|parents xi  ; the joint distribution
p x1 ... xn  = Πi=1...np xi|parents xi  .
　our dbn encodes the following independence assumptions within each time slice:
1. the msp signal m is independent of the ongoing activity a given the current object o and action b. for instance  hammering a nail  action  hammer   object  nail   will yield the same accelerometer and audio signatures regardless of whether you're fixing a window or hanging a picture. on the other hand  m is directly dependent both on o and b: the particular hand motion varies for action  pull  depending on whether object  door  or object  laundry line  are being pulled. similarly  pulling a doorentails a differentmotion from pushing it.
1. the action depends unconditionally both on current activity and the object currently in use. for instance  if the activity is  making pasta  the probability of action  shake  will depend on whether you are using object  salt shaker  or  pot . similarly  if the object is  pot  the probability of action  scrub  will depend on whether the activity is  making pasta  or  washing dishes .
1. the object used is directly dependent on the activity. inparticular  even if the action is a generic one such as  lift   the use of the object  iron  significantly increases the probability of activity  ironing .
　given initial guesses  derivable from oa and boa  for the conditional probabilities  the problem of jointly reestimating the parameters  estimating p m|o b  and selecting a small discriminatory subset of features of m based on partially labeled data can be viewed as a semi-supervised structure learning problem  traditionally solved by structural variants of em  friedman  1 . however  structural em is known to be difficult to apply tractably and effectively. we therefore trade off the potential gain of learning a joint model for the simplicity of a layered model.
1 a layered model
figure 1 b  shows the layered model we use. the new structure is motivatedby the fact that recent work  viola and jones  1; lester et al.  1  has shown that a tractable and effective way to automatically select features from a  sea of possible features  is to boost an ensemble of simple classifiers sequentially over possible features. our new model therefore replaces the node m of the original dbn  with a separate discriminative classifier smoothed by an hmm. classification in the layered structure proceeds as follows:
1. at each time slice  the f features are fed into the boosted classifier  described further below  to obtain the most likely current action-object pair b  . the classification so obtained is prone to transient glitches and is passed up a level for smoothing.
1. the hmm is a simple temporal smoother over actions: its hidden states are smoothed action estimates  and its observations are the unsmoothed output b  of the ensemble classifier. the map sequence of smoothed action/object pairs  b s  is passed on to the next layer as observations. the hmm is set to have uniform observation probability and uniform high self-transition probability ts. if we just require action classification on msp data  we threshold the smoothed class by margin; if the margin exceeds threshold tsmooth  set once by hand for all classes   we report the class  else we report  unknown .
1. if we wish to combine the inferred action with objectdata to predict activity  we pass the smoothed action class up to the dbn. the dbn at the next level  a truncated version of the one described previously  treats o and  the action part of  b s as observations and finds the most likely activity a.
　inference requires parameters of the dbn to be known. the commonsense information oa and boa can be converted into a crude estimate of conditional probabilities for the dbn as follows:
 otherwise. essentially 
we divideup probabilitymass p  typically 1 among likely objects  and divide up the remainder among the unlikely ones.
 otherwise  as
above  note ma o and na are defined in section 1 .
  and  other-
wise. this temporal smoothing encourages activities to continue into adjacent time slices.
initialize weights wi = 1/n i = 1 ... n iterate for m = 1 ... m :
1. for all features f
1. fm ○	i c wi ic  c =	fm si  
1. f  ○ arg minfefm
1. e	/e
1.
1. re-normalize output
t s  = arg max
return λs. + if sgn s   t  = sgn t+   t    else
table 1: the virtualboost algorithm
hyperparameters and  are currently selected once by hand and do not change with activities to be classified or observed data. if these crude estimates are treated as priors  the parameters of the dbn may be re-estimated with unlabeled data sn using em or other semi-supervised parameter learning schemes.
　it remains to learn the boosted classifier. we take the approach of viola and jones  1 . their scheme  as any conventional boosting-based scheme  requires labeled examples. given that we only have unlabeled msp data sn  we need to augment the usual scheme. a simple way  which we call map-label  to do so would be to use the object-use data o1 ... on from sn and the dbn to derive the most likely sequence b1 ... bn of actions  and to treat the pair  bi oi  as the label in each time step. this approach has the potential problem that even if many action/object pairs have comparable likelihood in a step  it only considers the top-rated pair as the label and ignores the others. we therefore explore an alternate form of the feature selection algorithm  called virtualboost  that works on data labeled with label distributions  rather than individual labels: we use the dbn and object-use data to generate the marginal distribution of b at each time slice  we refer to this technique as marginal-label . when a boosted classifier is trained using this  virtual evidence   pearl  1   virtualboost allows the trainer to consider all possible labels in proportion to their weight.
　table 1 specifies virtualboost. for simplicity we assume two classes  although the algorithm generalizes to multiple classes. the standard algorithm can be recovered from this one by setting pic to 1 and removing all sums over classes c. the goal of both algorithms is to identify ＋ m features  and corresponding single-feature multiple-class classifiers tf m and weights αm  the weighted combination of the classifiers is maximally discriminative.
　the standard algorithm works by  for each iteration m  fitting classifiers tfm for each feature f to the labeled data weightedto focuson previouslymisclassified examples  identifying as tf m the classifier that minimizes the weighted er-
1make tea  1 
1eat cereal  1 lift to mouth
1make sandwich  1 scoop
1make salad  1 chop
1dust  1 spread
1brush teeth  1 dust
1tend plants  1 brush
1set table  1 wipe horizontally
1clean windows  1 wipe vertically
1take medication  1 drink
1shave  1 shave
1shower  1 
table 1: activities  l  and actions  r  performed

figure 1: overall precision/recall
ror  attributing a weight αm to this classifier such that high errors result in low weights  and re-weighting the examples again to focus on newly problematic examples. virtualboost simply replaces all computations in the standard algorithm that expect individual class labels ci with the expectation over the distribution pic of the label. it is straightforward to show that this new variant is sound  in the sense that if d =  s1 p1c  ...  si pic  ...  sn pnc  is a sequence that when input to virtualboost yields classifier t  then the  sample sequence  where the cij are samples from pic  when fed to the conventional algorithm  will produce the classifier t for large k.
1 evaluation methodology
our evaluation focuses on the following questions:
1. how good are the learned models  in particular  howmuch better are the models of activity with action information combined  and how good are the action models themselves  why 
1. how does classifier performance vary with our designchoices  with hyperparameters 
1. how necessary were the more sophisticated aspects ofour design  do simple techniques do as well 
　to answer these questions  we collected ibracelet and msp data from two researchers performing1 activities containing 1 distinct actions of interest using 1 objects in an instrumented apartment. the activities are derived from a state-
precisionrecall
figure 1: precision/recall breakdown
mandated activities of daily living  adl  list; monitoring these activities is of interest to elder care professionals. four to ten executions of each activity were recorded over a period of two weeks. ground truth was recorded using video  and each sensor data frame was annotated in a post-pass with the actual activity and action  if any  during that frame  and designated  other  if not.
　table 1 lists the activities and actions performed. each activity is followed by the number of tagged objects  and the number of actions labeled  in that activity; e.g.  for making tea  we tagged 1 objects  spoon  kettle  tea  milk  sugar and saucer  and tracked one action  scoop with a spoon . we restricted ourselves to one action of interest per activity  although our system should work with multiple actions. we tag multiple objects per activity; 1 of the activities  make tea  eat cereal  make salad  make sandwich  set table  clean window  share at least one object with another activity. note that for our system to work  we do not have to list or track exhaustively either the objects or the actions in an activity  an important consideration for a practical system.
　in our experiments  we trained and classified the unsegmented sensor data using leave-one-out cross-validation over activities: we trained over all but one of the instances for each activity and tested on the last one  rotated the one that was left out  and report aggregate statistics. since many time slices were labeled  other  for actions  activities or both  we use both precision  fraction of slices when our claim was correct  and recall  fraction of slices when we detected a non- other  class correctly . in some cases  we report the f-measure = 1pr/ p + r  a standard aggregate goodness measure  where p and r are precision and recall.
1 results
figure 1 displays overall activity/action detection precision/recall  p/r  for four configurationsof interest. each configuration has the form  n = 1/o +a  . n = 1 attempts to detect activities 1 of table 1  whereas n = 1 detects them all. in the n = i|o configuration  we assume just rfid

1
1 1 1 1
                       recall  %  figure 1: precision/recall vs. margin threshold
observations  whereas n = i|o + a assumes msp observations for the action classifier in addition. for each configuration  we report p/r of activity and action detection. the n = 1 configuration yields similar results to n = 1  so we focus on n = 1 below.
　three points are key. first  comparing corresponding o and a bars for activity  we see that adding msp data does improve overall p/r from 1% to 1%. second  our action classifier based on msp data has 1% p/r; given that we have 1 underlying action/object classes all fairly evenly represented in the data  this is far better than any naive guesser. the low precision reflects the automatically-trained classifier's inability to identify when no action of interest is happening: 1% of our false positives are mis-labeled  other  slots. third  the relatively high action p/r  1%  with just objects  n = 1/o  reveals a limitation of our dataset: if object-use data is available  it is possible to predict the current action almost as well as with additional msp data. in our dataset  each activity/object combination has one action associated with it  so guessing the action given activity/object is fairly easy.
　figure 1 breaks down p/r in the n = 1 case over activities 1 and actions 1. the main point of interest here  unfortunately not apparent in the figure  is the interaction between object and msp-based sensing: combining the two sensors yields a jump in recall of 1  1  1 and 1% respectively in detecting activities 1  1  1 and 1 respectively  with no activity deteriorating. in all these cases the activities involved shared objects with others  so that actions were helpful. activity 1  making salad  is particularly hard hit because both objects used  knife and bowl  are used in other activities. resolving this ambiguity improves p/r for the sole action associated with making salad by 1%.
　figure 1 shows how p/r of action detection changes when margin threshold tsmooth of section 1 varies from -1 to 1. we use tsmooth =  1 in this section. we also varied boosting iterations m to 1  1  1  1 and 1; the best matching p/r rates were 1  1  1  1 and 1%. we use n = 1.
figure 1 compares virtualboost to simpler approaches.

figure 1: impact of virtual evidence handling techniques
the first bar shows the effect on action detection  on fmeasure  of map-label combined with regular boostingbased feature selection; the second shows marginal-label with virtualboost. the third shows marginal-label with the single class with highest probability chosen as label  if probability is higher than a fixed threshold   combined with regular boosting. the fourth uses marginal-label  picks the mostlikely class as label if above threshold  and feeds it along with its probability to a variant of virtualboost. given that all our actions are associated with a single object  it is not surprising that using the most likely action performs quite well. in some cases  because of overlap in object-use  marginal-label sprinkles in incorrect labels with correct ones. in these cases  the third approach ascribes too much weight  i.e.  1  to the  possibly incorrect  answer  whereas the fourth approach mitigates the label choice via its weight. complete virtualboost seems to get distracted by needing to analyze all 1 actions every time.
1 conclusions
feature selection and labeling are known bottlenecksin learning sensor-based models of human activity. we have demonstrated that it is possible to use data from dense object-use sensors and very simple commonsense models of object use  actions and activities to automatically interpret and learn models for other sensors  a technique we call common sense based joint training. validation of this technique on much larger datasets and on other sensors  vision in particular  is in progress.
