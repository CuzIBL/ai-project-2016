 
different qualitative models have been proposed for decision under uncertainty in artificial intelligence  but they generally fail to satisfy the principle of strict pareto dominance or principle of  efficiency   in contrast to the classical numerical criterion - expected utility. in  dubois and prade  
1j qualitative criteria based on possibility theory have been proposed  that are appealing but inefficient in the above sense. the question is whether it is possible to reconcile possibilistic criteria and 
efficiency. the present paper shows that the answer is yes  and that it leads to special kinds of expected utilities. it is also shown that although nu-consequence  the topic of qualitative decision theory is a nat-
ural one to consider  pearl  1; dubois and prade  1; brafman and tennenholtz  1; dubois et al  1b; doyle and thomason  1; giang and shenoy  1; dubois et al  1   giving up the quantification of utility and uncertainty has led to give up the expected utility  eu  criterion as well: the principle of most theories of qualitative decision making is to model uncertainty by an ordinal plausi-
bility relation on events and preference by a complete preordering on consequences. in  dubois and prade  1; dubois et ai  1b  two qualitative criteria based on possibility theory  an optimistic and a pessimistic one  are pro-
posed and axiomatized whose definitions only require a finite ordinal scale l = {1l   뫴뫴    1l} for evaluating both utility and plausibility: merical  these expected utilities remain qualitative: they lead to two different decision procedures based on min  max and reverse operators only  generalizing the leximin and leximax orderings of vectors. 
decision theory 	1 

most of the qualitative approaches  pearl  1; dubois 
and prade  1; brafman and tennenholtz  1; giang and 	: 
shenoy  1   fail to satisfy pareto dominance. but this is to a probability distribution: not the case within expected utility theory  since this model obeys the following sure-thing principle  stp  that insures 
that identical consequences do not influence the relative pref- model. a probabilistic transerence between two acts: formation of l w.r.t. n is a mapping x ; l -   1  such that 
is a probability distribution. 
notice the presence of the condition  that ex-
presses the fact that the impossibility of an event  represented by a degree of 1l  in possibility theory  is expressed by a null probability. but the most plausible events  possibility degrees of 1l  obviously do not receive a probability degree of 1  since they may be mutually exclusive. notice also that we are looking for a unique function x for transforming l - both p and u will be built upon this transformation. this is due to the fact that we assume that preference and uncertainty levels arc commensurate and belong to the same scale : it is 
모this means that possibilistic decision criteria cannot obey the stp  except in a very particular case: when the actual state of the world is known  i.e. when there is no uncertainty at all! so  we cannot stay in the pure qpu framework and escape the drowning effect altogether. the idea is then to cope with the difficulty by proposing refinements of the possibilistic criteria that obey the sure thing principle 1. 
모this paper shows  section 1  that any possibilistic model can be refined by an expected utility. the kind of expected thus natural to transform the degrees regardless whether they model uncertainty or preference. 
모moreover  and originally represent all the information available to the user  both in terms of uncertainty of the actual state of the world and preference over possible consequences. so  no undesirable arbitrary information should be introduced in the refined decision model and p and u must be as close 
as possible to the original information: we are looking for  unbiased  transformations of l. formally: utility that is at work  and the very special probability measure that underlies it  are studied in section 1 under the light of related work. it is also shown  section 1  that although numerical  these expected utility criteria remain qualitative  since they lead to a decision procedure based solely on min  max and reverse operators - these new procedures generalize well known leximin and leximax decision procedures . 
1 	decision theory 

모at this point in the paper we have proved an important result for bridging qualitative possibilistic decision theory and expected utility theory: we have shown than any optimistic or pessimistic qpu model can be refined by a eu model. 

모notice that proposition 1 does not mean that the numbers attached to the states by  and p1 - x1 뫢   nor the ones attached to the consequences by u1 -    o and are the same - it only means that the two models are ordinally equivalent  that they make the same decisions and order the events and the consequences in the same way. it also implies that the refinements that docs not belong to this class  they may exist  e.g. those which introduce a total order in s or in x  cannot be unbiased : they must either introduce a strict preference between equivalent consequences or order equi-plausiblc states. 
so  we get the following result for optimistic qpu models: 
thus  we may conclude that  i  possibilistic decision criteria are compatible with the classical expected utility criterion and 
 ii  choosing a eu model is advantageous  since it leads to a eu-refinement of the original rule  thus overcomes the lack of decisiveness of the possibilisitic criteria   it satisfies the stp and the principle of pareto. but this does not mean that qualitativeness and ordinality are given up. in section 1  we will show that  although probabilistic and based on additive manipulations of numbers  these criteria remain ordinal. this is very natural: since we start with an ordinal model and do not accept any bias  we produce another  probabilistic but  ordinal model  in which the numbers only encode orders of magnitude - this is the topic of the next section. 

decision theory 	1 

such measures are often encountered in the ai literature. first  they have much in common with  spohn  1 's k  - functions: these disbelief degrees can indeed be interpreted as the order of magnitude of a e probability  pearl  1; giang and shenoy  1   which is obviously a big stepped probability. moreover  big stepped probabilities also form a special class of lexicographic probabilities in the sense of  blume et al  1; lehmann  1  - we add the restriction that here all the states within a single cluster arc equiprobable. indeed each cluster corresponds to a class of equipossible states and since we are looking for unbiased transformations  equipossibility leads to equiprobability. finally  definition 1 generalizes the notion of big-stepped probability of  snow  1; benferhat et al.   1  - which is recovered when each cluster is a singleton. big stepped probabilities have also been proposed by  dubois et a/.  1a  as a way to refine any possibility/necessity measure 1. 
모this reasoning on the order of magnitude also applies to utility: in a discrete setting  big-stepped utilities can be defined in the same way: 
are big stepped utilities. it is also the case of the c utilities that underly k-utility functions  pearl  1; wilson  1; bonet and geffner  1; giang and shenoy  1 . these works have advocated an approach to decision under uncertainty based on k-functions  but without taking the stp into account  decision is made on the order of magnitude only  with a criterion comparable to optimistic utility . the present work makes a step further: in order to satisfy pareto optimally  we go back to the underlying e utilities and probabilities  using double exponents for epsilons instead of simple ones - we remain  big stepped  on the join scale. the other contribution of our approach is that it can be followed to encode pessimistic utilities as well. 
모 la valle and fishburn  1; hammond  1; lehmann  1  have studied decision models of lexicographic probabilities or lexicographic utilities  but in these models  the lexicographic characteristic is used only on one of the two dimensions  either the likelihood level  or the utility level . we operate on both dimensions simultaneously using a join transformation. 
1 	qpu refinements are qualitative 
although probabilistic and based on additive manipulations of utilities  our eu criteria remain ordinal  as paradoxical as it may seem at first sight. to establish this claim  this section relates the previous eu criteria to the ordinal comparison of 
1 
vectors. when s is finite  the comparison of acts can indeed be seen as a comparison of vectors of pairs of elements of l: 
definition 1  maxmin relation  ordinal comparison of degrees and we will show the equivalence between this purely syntactical decision rule and the above eu models. 
1 	case of total ignorance 
and min operators is well known  as it is known that it suffers from a lack of decisive power. that is why refinements of definition 1  leximax  leximin  
and are very efficient: the only pairs of ties are vectors that are identical up to a permutation of their elements. 
1 	general case 
since the leximax and leximin comparisons are good candidates in a particular case  we have imagined an extension of 
decision theory 


decision theory 	1 


