 
efficient implementations of dpll with the addition of clause learning are the fastest complete satisfiability solvers and can handle many significant real-world problems  such as verification  planning  and design. despite its importance  little is known of the ultimate strengths and limitations of the technique. this paper presents the first precise characterization of clause learning as a proof system  and begins the task of understanding its power. in particular  we show that clause learning using any nonredundant scheme and unlimited restarts is equivalent to general resolution. we also show that without restarts but with a new learning scheme  clause learning can provide exponentially smaller proofs than regular resolution  which itself is known to be much stronger than ordinary dpll. 
1 introduction 
in recent years the task of deciding whether a cnf propositional logic formula is satisfiable has gone from a problem of theoretical interest to a practical approach for solving realworld problems. sat procedures are now a standard tool for hardware verification  including verification of super-scalar processors  velev and bryant  1; biere et al  1 . open problems in group theory have been encoded and solved using satisfiability  zhang and hsiang  1 . other applications of sat include circuit diagnosis and experiment design  konuk and larrabee  1; gomes et al  1b . 
　the most surprising aspect of such relatively recent practical progress is that the best complete satisfiability testing algorithms remain variants of the dpll procedure  davis and putnam  1; davis et al.  1  for backtrack search in the space of partial truth assignments. the main improvements to dpll have been better branch selection heuristics  e.g.   li and anbulagan  1    and extensions such as randomized restarts  gomes et al  1a  and clause learning. one can argue that clause learning has been the most significant of these in scaling dpll to realistic problems. 
　clause learning grew out of work in ai on explanationbased learning  ebl   which sought to improve the perfor-
* research supported by nsf grant itr-1 
mance of backtrack search algorithms by generating explanations for failure  backtrack  points  and then adding the explanations as new constraints on the original problem  de kleer and williams  1; stallman and sussman  1; genesereth  1; davis  1 . for general constraint satisfaction problems the explanations are called  conflicts  or  no goods ; in the case of boolean cnf satisfiability  the technique becomes clause learning. a series of researchers  bayardo jr. and schrag  1; marques-silva and sakallah  1; zhang  1; moskewicz et al  1; zhang et al  1  showed that clause learning can be efficiently implemented and used to solve hard problems that cannot be approached by any other technique. 
　despite its importance there has been little work on formal properties of clause learning  with the goal of understanding its fundamental strengths and limitations. a likely reason for such inattention is that clause learning is a rather complex rule of inference - in fact  as we describe below  a complex family of rules of inference. a contribution of this paper is that we provide a precise specification of clause learning. 
　another problem in characterizing clause learning is defining a formal notion of the strength or power of a reasoning method. this paper uses the notion of proof complexity  cook and reckhow  1   which compares inference systems in terms of the sizes of the shortest proofs they sanction. a family of formulas c provides an exponential separation between systems s1 and 1 if the shortest proofs of formulas in c in system s1 are exponentially smaller than the corresponding shortest proofs in s1. from this basic propositional proof complexity point of view  only families of unsatisfiable formulas are of interest  because only proofs of unsatisfiability can be large; minimum proofs of satisfiability are linear in the number of variables of the formula. nevertheless  achlioptas et al  have shown how negative proof complexity results for unsatisfiable formulas can be used to derive time lower bounds for specific inference algorithms running on satisfiable formulas as well. 
　proof complexity does not capture everything we intuitively mean by the power of a reasoning system  because it says nothing about how difficult it is to find shortest proofs. however  it is a good notion with which to begin our analysis  because the size of proofs provides a lower-bound on the running time of any implementation of the system. in the systems we consider  a branching function  which determines 
satisfiability 

which variable to split upon or which pair of clauses to resolve  guides the search. a negative proof complexity result for a system tells us that a family of formulas is intractable even with a perfect branching function; likewise  a positive result gives us hope of finding a branching function. 
　a basic result in proof complexity is that general resolution is exponentially stronger than the dpll procedure  bonet et al.  1; ben-sasson et al  1 . this is because the trace of dpll running on an unsatisfiable formula can be converted to a tree-like resolution proof of the same size  and tree-like proofs must sometimes be exponentially larger than the dag-like proofs generated by general resolution. although resolution can yield shorter proofs  in practice dpll is better because it provides a more efficient way to search for proofs. the weakness of the tree-resolution proofs that dpll finds is that they do not reuse derived clauses. the conflict clauses found when dpll is augmented by clause learning correspond to reuse of derived clauses in the associated resolution proofs and thus to more general forms of resolution proofs. an intuition behind the results in this paper is that the addition of clause learning moves dpll closer to general resolution while retaining its practical efficiency. 
　it has been previously observed that clause learning can be viewed as adding resolvents to a tree-like proof  marquessilva  1 . however  this paper provides the first mathematical proof that clause learning is exponentially stronger than tree-like resolution. further  we provide a family of formulas that exponentially separates clause learning from regular resolution  a system that is known to be intermediate in strength between general and tree resolution. the proof uses a new clause learning scheme called firstnewcut that we introduce. we also show that combining clause learning with restarts is as strong as general resolution. 
　although this paper focuses on basic proof complexity  we briefly indicate how the understanding we gain through this kind of analysis may lead to practical applications. for example  our proofs describe an improvement to the clause learning rules previously suggested in the literature  and suggest an approach to leveraging the structure of a problem encoded as a cnf formula in order to create a branching heuristic that takes the greatest advantage of clause learning. as an example  we apply these ideas to certain natural satisfiable and unsatisfiable formulas where we obtain significant speed-ups over existing methods. 
1 	preliminaries 
a cnf formula f is an and  of clauses  where each clause is an or of literals  and a literal is a variable or its negation it is natural to think of f as a set of clauses and each clause as a set of literals. a clause that is a subset of another is called its subclause. 
　let p be a partial assignment to the variables of f. the restricted formula  is obtained from f by replacing variables in with their assigned values. f is said to be simplified if all clauses with at least one true literal are deleted  all occurrences of false literals are removed from clauses  and the resulting formula  if different  is simplified recursively. 
1 the dpll procedure 
the basic idea of the davis-putnam-logemann-loveland 
 dpll  procedure  davis and putnam  1; davis et al.  1  for testing satisfiability of cnf formulas is to branch on variables  setting them to true or false  until either an initial clause is violated  i.e. has all literals set to false  or all variables have been set. in the former case  we backtrack to the last branching decision whose other branch has not been tried yet  reverse the decision  and proceed recursively. in the latter  we terminate with a satisfying assignment. if all possible branches have been unsuccessfully tried  the formula is declared unsatisfiable. to increase efficiency  pure literals  those whose negation does not appear  and unit clauses  those with only one unset literal  are immediately set to true. 
definition 1. a branching sequence for a cnf formula f is a sequence  of literals of f  possibly with 
repetitions. dpll on f branches according to a if it always picks the next variable v to branch on in the literal order given by o  skips it if v is currently assigned a value  and branches further by setting the chosen literal to false otherwise. 
　in this paper  we will use the term dpll to denote the basic branching and backtracking procedure described above. it will  for instance  not include extensions such as learning conflict clauses or restarting  but will allow intelligent branching heuristics. note that this is in contrast with the occasional use of the term dpll to encompass practically all branching and backtracking approaches  including those involving learning. 
1 	resolution 
resolution is a simple proof system that can be used to prove unsatisfiability of cnf formulas. the resolution rule states that given clauses a n d w e can derive clause  a  b  by resolving on a resolution derivation of c from a cnf formula f is a sequence 
c  where each clause ci is either a clause of f  an initial clause  or derived by applying the resolution rule to  and 
 a derived clause . the size of is s  the number of clauses occurring in it. we will assume that each in r is used to derive at least one other clause any derivation of the empty clause a from f  also called a refutation or proof of f  shows that f is unsatisfiable. 
　despite its simplicity  resolution is not efficiently implementable due to the difficulty of finding good choices of clauses to resolve; natural choices typically yield huge storage requirements. various restrictions on the structure of resolution proofs lead to less powerful but easier to implement variants such as tree-like  regular  linear and positive resolution. tree-like resolution uses non-empty derived clauses exactly once in the proof and is equivalent to an optimal dpll procedure. regular resolution allows any variable to be resolved upon at most once along any  path  in the proof from an initial clause to a. both these variants are sound and complete but differ in efficiency - regular resolution is known to be exponentially stronger than tree-like  bonet et al.  1; ben-sasson et al.  1   and general resolution is exponentially stronger than regular  alekhnovich et al.  1 . 
definition 1. a resolution derivation  is trivial iff all variables resolved upon are distinct and each 

is either an initial clause or is derived by resolving an initial clause. 
　a trivial derivation is tree-like as well as regular. moreover  the condition that each derived clause  in its derivation makes it linear. as we will see  trivial derivations correspond to conflicts in clause learning algorithms. 
1 clause learning 
clause learning proceeds by following the normal branching process of dpll until there is a  conflict  after unit propagation. if this conflict occurs without any branches  the formula is declared unsatisfiable. otherwise  the  conflict graph  is analyzed and the  cause  of the conflict is learned in the form of a  conflict clause.  we now backtrack and continue as in ordinary dpll  treating the learned clause just like initial ones. a clause is said to be known at a stage if it is either an initial clause or has already been learned. the learning process is expected to save us from redoing the same computation when we later have an assignment that causes conflict due in part to the same reason. 
   if a given cnf formula f is unsatisfiable  clause learning terminates with a conflict without any branches. since all clauses used in this conflict themselves follow directly or indirectly from f  this failure of clause learning in finding a satisfying assignment constitutes a logical proof of unsatisfiability of f. our bounds compare the size of such a proof with the size of a  possibly restricted  resolution proof of unsatisfiability of f. 
   variations of such conflict driven learning  bayardo jr. and schrag  1; marques-silva and sakallah  1; zhang  1; moskewicz et al.  1; zhang et al.  1  include different ways of choosing the clause to learn and possibly allowing multiple clauses to be learned from a single conflict. although many such algorithms have been proposed and demonstrated to be empirically successful  a theoretical discussion of the underlying concepts and structures needed for our bounds is lacking. the rest of this section focuses on this formal framework. 
definition 1. a clause learning proof-k of an unsatisfiable 
cnf formula f under scheme s and induced by branching sequence a is the result of applying dpll with unit propagation on f  branching according to a  and using scheme s to learn conflict clauses such that at the end of this process  there is a conflict without any branches. the size of the proof  size tt % is |er|. 
　all clause learning algorithms discussed in this paper are based on unit propagation  which is the process of repeatedly applying the unit clause rule followed by formula simplification until no clause with exactly one unassigned literal remains. in this context  it is convenient to work with residual formulas at different stages of dpll. let p be the partial assignment at some stage of dpll on formula f. the residual formula at this stage is obtained by simplifying f p and applying unit propagation. 
　when using unit propagation  variables assigned values through the actual branching process are called decision variables and those assigned values as a result of unit propagation are called implied variables. decision and implied literals are analogously defined. upon backtracking  the last decision variable no longer remains a decision variable and might instead become an implied variable depending on the clauses learned so far. the decision level of a decision variable x is one more than the number of current decision variables at the time of branching on x. the decision level of an implied variable is the maximum of the decision levels of decision variables used to imply it. the decision level at any step of the underlying dpll procedure is the maximum of the decision levels of all current decision variables. 
1 	implication graph and conflicts 
unit propagation can be naturally associated with an implication graph that captures all possible ways of deriving all implied literals from decision literals. 
definition 1. the implication graph g at a given stage of dpll is a directed acyclic graph with edges labeled with sets of clauses. it is constructed as follows: 
1. create a node for each decision literal  labeled with that literal. these will be the indegree zero root nodes of g. 

1. add to g a special node a. for any variable x which occurs both positively and negatively in g  add directed 
	edges from 	and 	to 	 
　since all node labels in g are distinct  we identify nodes with the literals labeling them. any variable x occurring both positively and negatively in g is a conflict variable  and x as well as  arc conflict literals. g contains a conflict if it has at least one conflict variable. dpll at a given stage has a conflict if the implication graph at that stage contains a conflict. a conflict can equivalently be thought of as occurring when the residual formula contains the empty clause a. 
　by definition  an implication graph may contain many conflict variables and several ways of deriving any single literal. to better understand and analyze a conflict  we work with a subgraph  called the conflict graph  see figure 1   that captures only one among possibly many ways of reaching a conflict from the decision variables. the choice of the conflict graph is part of the strategy of the solver. it can also be thought of as giving power to clause learning by adding nondeterminism. 
definition 1. a conflict graph h is any subgraph of an implication graph with the following properties: 
1. h contains a and exactly one conflict variable. 
1. all nodes in h have a path to a. 
1. every node i in h other than a either corresponds to a decision literal or has precisely the nodes as predecessors where  is a known clause. 
satisfiability 

　consider the implication graph at a stage where there is a conflict and fix a conflict graph contained in that implication graph. pick any cut in the conflict graph that has all decision variables on one side  called the reason side  and a as well as at least one conflict literal on the other side  called the con-
flict side. all nodes on the reason side that have at least one edge going to the conflict side form a cause of the conflict. the negations of the corresponding literals forms the conflict clause associated with this cut. 
proposition 1. every conflict clause corresponds to a cut in a conflict graph that separates decision variables from a and a conflict literal. 
proof let s denote the set containing the negations of the literals of a given conflict clause c and pred s  be the set of all predecessors of these literals in the underlying implication graph. let t denote the set containing all literals obtained by unit propagation after setting literals in s to true. since c is a conflict clause  t must contain a conflict literal. consider the subgraph gs t of the implication graph induced by a and the literals in  but having no edges going from preid s  to t. fix any conflict graph that is a subgraph of g s t the cut in this conflict graph with t as the conflict side has c as the conflict clause.  
proposition 1. if there is a trivial resolution derivation of a clause c from a set of clauses f  then setting all literals ofc to false leads to a conflict. 

1 	different learning schemes 

proposition 1. any conflict clause can be derived from known clauses using a trivial resolution derivation. 
proof. in the light of proposition 1  assume that we have a conflict clause associated with a cut  in a fixed conflict graph. let  denote the set of variables on the conflict side of o  but including the conflict variable only if it occurs both positively and negatively on the conflict side. 
different cuts separating decision variables from a and a conflict literal correspond to different learning schemes  see figure 1. one can also create learning schemes based on cuts not involving conflict literals at all  zhang et al  1   but their effectiveness is not clear. these will not be considered here. 
　it is insightful to think of the non-deterministic scheme as the most general learning scheme. here we pick the cut nondeterministically  choosing  whenever possible  one whose associated clause is not already known. since we can repeatedly branch on the same last variable  non-deterministic learning subsumes learning multiple clauses from a single conflict as long as the sets of nodes on the reason side of the corresponding cuts form a  set-wise  decreasing sequence. for simplicity  we will assume that only one clause is learned from any conflict. 
in practice  however  we employ deterministic schemes. 
the decision scheme  zhang et al  1   for example  uses the cut whose reason side comprises all decision variables  re 1 - sat  bayardo jr. and schrag  1  uses the cut whose conflict side consists of all implied variables at the current decision level. this scheme allows the conflict clause to have exactly one variable from the current decision level  causing an automatic flip in its assignment upon backtracking. 
　this nice flipping property holds in general for all unique implication points  uips   marques-silva and sakallah  1 . a u1p of an implication graph is a node at the current decision level d such that any path from the decision variable at level d to the conflict variable as well as its negation 

must go through it. intuitively  it is a single reason at level d that causes the conflict. whereas r e l - s a t uses the decision variable as the obvious u1p  grasp  marques-silva and sakallah  1  and zchaf f  moskewicz et al  1  use firstuip  the one that is  closest  to the conflict variable. grasp also learns multiple clauses when faced with a conflict. this makes it typically require fewer branching steps but possibly slower because of the time lost in learning and unit propagation. 
　the concept of uip can be generalized to decision levels other than the current one. the iuip scheme corresponds to learning the firstuip clause of the current decision level  the 1uip scheme to learning the firstuip clauses of both the current level and the one before  and so on. zhang et al  1   present a comparison of all these and other learning schemes and conclude that 1uip is quite robust and outperforms all other schemes they consider on most of the benchmarks. 

figure 1: a conflict graph depicting various learning schemes 
the firstnewcut scheme 
we propose a new learning scheme called firstnewcut whose ease of analysis helps us demonstrate the power of clause learning. we would like to point out that we use this scheme here only to prove our theoretical bounds. its effectiveness on other formulas has not been studied yet. 
　the key idea behind firstnewcut is to make the conflict clause as relevant to the current conflict as possible by choosing a cut close to the conflict literals. this is what the firstuip scheme also tries to achieve in a slightly different manner. for the following definitions  fix a cut in a conflict graph and let s be the set of nodes on the reason side that have an edge to some node on the conflict side  1 is the reason side frontier of the cut . let  be the conflict clause associated with this cut. 
definition 1. minimization of conflict clause cs is the process of repeatedly identifying  if one exists  a node  s  all of whose predecessors are also in s  moving it to the conflict side  and removing it from s. 
definition 1. firstnewcut scheme: start with a cut whose conflict side consists of a and a conflict literal. if necessary  repeat the following until the associated conflict clause  after minimization  is not already known: pick a node on the conflict side  pull all its predecessors except those that correspond to decision variables into the conflict side. finally  learn the resulting new minimized conflict clause. 
　this scheme starts with the cut that is closest to the conflict literals and iteratively moves it back toward the decision variables until a new associated conflict clause is found. this backward search always halts because the cut with all decision variables on the reason side is certainly a new cut. note that there are potentially several ways of choosing a literal to move the cut back  leading to different conflict clauses. the firstnewcut scheme  by definition  always learns a clause not already known. this motivates the following: 
definition 1. a clause learning scheme is non-redundant if on a conflict  it always learns a clause not already known. 
1 	fast backtracking and restarts 
most clause learning algorithms use fast backtracking where one uses the conflict graph to backtrack not only the last branching decision but also all other recent decisions that did not contribute to the conflict  stallman and sussman  1 . this adds power to clause learning because the current conflict might use clauses learned earlier as a result of branching on the apparently redundant variables. hence  fast backtracking in general cannot be replaced by a  good'* branching sequence that does not produce redundant branches. for the same reason  fast backtracking cannot either be replaced by simply learning the decision scheme clause. however  the results we present in this paper are independent of whether or not fast backtracking is used. 
　restarts allow clause learning algorithms to arbitrarily restart their branching process. all clauses learned so far are however retained and now treated as additional initial clauses  baptista and silva  1 . as we will show  unlimited restarts can make clause learning very powerful at the cost of adding non-determinism. unless otherwise stated  clause learning proofs will be assumed to allow no restarts. 
1 	clause learning and general resolution 
lemma 1. let f be a cnf formula over n variables. if f has a general resolution proof of size s  then it also has a clause learning proof of size at most ns using any nonredundant learning scheme and at most s restarts. 
proof 	a  be a general resolution proof of f where each is either an initial clause or derived by resolving two clauses and occurring earlier in if contains a derived clause whose strict subclause c  can be derived by resolving two previously occurring clauses  then we can replace with do trivial simplifications on further derivations that used and obtain another proof  of f of size at most s. doing this repeatedly will remove all such redundant clauses and leave us with a simplified proof no larger in size. hence we will assume without loss of generality that has no such clause. 
　a clause learning proof of f can be constructed by choosing derived clauses of in order  learning each of them  and restarting. suppose every clause is already known and we are at decision level zero. this is trivially true when are initial clauses. if there are two 
known clauses 	and 	whose resolution generates 
in this case we have a conflict from the known clauses at decision level zero and our clause learning proof is complete. 
satisfiability 

otherwise  let and assume without loss of generality that cp is derived by resolving two known clauses  where both a and b are subclauses of cp. our clause learning proof will choose to branch on and set all of  to false. this will falsify both a and b and thus imply both x and  after unit propagation  resulting in a conflict. it is easy to see that the only new clause that can be learned from this conflict is accordingly  we will use any non-redundant learning scheme  learn cv and restart to get back to decision level zero. due to the non-redundancy of cp assumed earlier in the proof  there is no premature conflict until all of  have been branched upon. this makes sure that our clause learnclauses of f together with a trace clause for each clause c  s and each literal 
　we first show that if a formula has a short general resolution refutation  then the corresponding proof trace extension has a short clause learning proof. intuitively  the new trace variables allow us to handle every resolution step of the original proof individually  effectively letting us restart after learning each derived clause. 
lemma 1. let f be any cnf formula and  be a resolution refutation of it. then pt f    has a clause learning proof of size less than using the firstnewcut scheme and no restarts. 

ing proof proceeds as described above and we learn precisely 
the clause  
　we learn at most .s clauses and each learning stage requires branching on at most n variables and exactly one restart. this gives the desired bounds on the size of the constructed clause learning proof and the number of restarts it uses.  
lemma 1. let f be a cnf formula over n variables. f has a clause learning proof of size s using any learning scheme and any number of restarts  then f also has a general resolution proof of size at most ns. 
proof given a clause learning proof  of f  a resolution proof can be constructed by sequentially deriving all clauses that  learns  which includes the empty clause  from 
proposition 1  all these derivations are trivial and hence require at most n steps each. consequently  the size of the resulting clause learning proof is at most ns. note that since we derive clauses of  individually  restarts in  do not change the construction.  
combining lemmas 1 and 1  we immediately get 
theorem 1. clause learning with any non-redundant scheme and unlimited restarts is equivalent to general resolution. 
　note that this theorem strengthens the result from  baptista and silva  1  that clause learning together with restarts is complete. our theorem makes the stronger claim that clause learning with restarts can find proofs that are as short as those of general resolution. 
1 	clause learning and regular resolution 
here we prove that clause learning even without restarts is exponentially stronger than regular resolution on some formulas. we do this by first introducing a way of extending any cnf formula based on a given resolution proof of it. we then show that if a formula exponentially separates general resolution from regular resolution  its extension exponentially separates clause learning from regular resolution. finally  we cite specific formulas called that satisfy this property. 
1 the proof trace extension 

lemma 1. let f be a cnf formula over n variables that has a polynomial  in n  size general resolution proof n but requires exponential size regular resolution proofs. then pt f    has a polynomial size clause learning proof using the firstnewcut scheme and no restarts  but requires exponential size regular resolution proofs. 
proof lemma 1 immediately implies that has a polynomial size firstnewcut clause learning proof. 
　for the other part  we use a simple reduction argument. suppose  has a regular resolution refutation  of 
size s. consider the restriction that sets every trace variable of this formula to true  keeps original clauses of f intact and trivially satisfies all trace clauses  thereby reducing the initial clauses of  to precisely f. recall that regularity of resolution proofs is preserved under arbitrary restrictions. consequently  applying  gives us a regular resolution refutation of f of size at most 1. by the assumption in the lemma  s must be exponential in n. note that itself is of size polynomial in n because of our choice of hence s is also exponential in the size of itself.  
1 the gtr; formulas 
we use the proof trace extension of an explicit family of unsatisfiable cnf formulas called  to obtain an exponential separation between regular resolution and clause learning. note that in place of  we could also have used any other formulas satisfying the conditions on f in lemma 1  such as the modified pebbling formulas of alekhnovich et al . 
the  formulas are based on the ordering principle 
that any partial linear ordering on the set have a maximal element. the original formulas  called were first considered by krishnamurthy l1  and later used by bonet and galesi  to show the optimality of the size-width relationship of resolution proofs. recently  alekhnovich et al  used  to show an exponential separation between general and regular resolution. we refer the reader to this paper for exact specification of the formulas. for our bound  we only need the following result: 
lemma 1  i alekhnovich et al.  1 . 
mial size general resolution refutation but requires exponential size regular resolution proofs. 
let  be the polynomial size resolution refutation of 
gt'n described in  alekhnovich et al.  1 . it follows from 
lemmas 1 and 1 that pt  has a polynomial size 
clause learning proof using the firstnewcut scheme but requires exponential size regular resolution proofs. hence  
theorem 1. there exist cnf formulas on which clause learning using the firstnewcut scheme and no restarts provides exponentially smaller proofs than regular resolution. 
1 	experimental results 
table 1 reports the performance of variants of z chaff on grid pebbling formulas. we conducted experiments on a 1 mhz linux machine with memory limit set to 1mb. base code of  was extended to allow a  partial  branching sequence to be specified as part of the input. we used the more popular 1uip learning scheme of  instead of 
firstnewcut because for these formulas  both schemes provide small proofs. 
　pebbling formulas based on pebbling graphs are known to be hard for tree-like resolution but easy for regular resolution  ben-sasson and wigderson  1 . we refer the reader to this paper for exact specification of the formulas. for our experiments  we worked with a uniform  grid like version  where every non-leaf node has precisely two predecessors. for a k layer graph  the corresponding formula has 1 k1  variables and clauses. these formulas are minimally unsatisfiable. we also used a satisfiable version obtained by deleting a randomly chosen clause. the branching sequence was generated based on depth first traversal of the underlying pebbling graph. results are reported for z chaff with no learning or specified branching sequence  dpll   with specified branching sequence only  with clause learning only  original zchaff   and both. 
1 	discussion and open problems 
this paper has begun the task of formally understanding the power of clause learning from a proof complexity perspective. we have seen that clause learning can be more powerful than even regular resolution  and that learning with restarts yields general resolution. 
　in practice  a solver must employ good branching heuristics as well as implement a powerful proof system. our result that modified pebbling formulas have small clause learning proofs depends critically upon the solver choosing a branching sequence that solves the formula in  bottom up  fashion  so that the learned clauses have maximal reuse. as we describe in a subsequent paper  sabharwal et al.  1   this branching sequence can be efficiently generated by a combination of breadth-first and depth-first travcrsals of the original pebbling graph even for more general classes of pebbling formulas. as shown in table 1  one needs both clause learning as well as a good branching sequence to efficiently solve large problem instances. 
　of course  pebbling graphs  which correspond to problems involving precedence of tasks  represent a narrow domain of applicability. however  logic encodings of many kinds of real-world problems  such as planning graphs  kautz and selman  1   exhibit layered structure not unlike pebbling graphs. an important direction of our current research is to generate branching sequences that allow clause learning to work well on the general classes of structures that arise in encodings of particular problem domains. 
　different learning schemes are likely to vary in their effectiveness when used on formulas from different domains. we introduced firstnewcut as a new scheme and used it in this paper to derive our theoretical results. how well it performs on encodings of real-world problems is still open. it would be interesting to know if there is a class of practical problems 
satisfiability 

on which firstnewcut works better than other schemes. 
　this paper inspires but leaves open several interesting questions of proof complexity. we have shown that with arbitrary restarts  clause learning is as powerful as general resolution. however  judging when to restart and deciding what branching sequence to use after restarting adds more non-determinism to the process  making it harder for practical implementations. can clause learning with no or limited restarts also simulate general resolution efficiently  we showed that there are formulas on which clause learning is much more efficient than regular resolution. in general  can every small regular refutation be converted into a small clause learning proof  or are regular resolution and clause learning incomparable  
