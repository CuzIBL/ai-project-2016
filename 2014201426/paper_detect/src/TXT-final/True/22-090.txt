 
a significant component of human observational learning is the ability to focus attention toward important or relevant input features. amechanism with this capability can serve as an inductive bias to facilitate learning in both humans and machines. past attempts to model attentional focus for human learning have postulated a single salience value for each feature  such that features with greater salience command more attention. these models  however  assume that the feature's salience is not dependent on context  whereas studies of human attention show sensitivity to context. this paper presents a mechanism for contextually focused attention in observational learning. 
1 introduction 
observational learning is a form of inductive knowledge acquisition in which there is no external guidance  such as explicit feedback. however  some guidance or learning bias is required to make general induction tractable  eg. rendell et al.  1 . since humans do engage in some observational learning  billman et al.  1   there must be a method for internally guiding this learning. discovering such methods will prove useful both for understanding human learning and for designing computer programs that learn from observation  eg. fisher  1 . zeaman and house  1  and billman and licit  1  have argued that attention directed by learnable feature saliences may provide some internal guidance for human learners. they each proposed a mechanism for doing this and were able to confirm the approach for simple learning. neither method used context - what already is known about an example - to help focus attention. other researchers  however  have found that human attention and other cognitive processes vary with context  loftus & mackworth  1; barsalou & medin  1 . 
as well  the use of context can allow learning of more 
   *the author wishes to thank dorrit billman for providing constructive comments on all aspects of this research  and janet kolodner an anonymous reviewer for assistance with earlier versions of the manuscript. this research was supported by the army research institute for the behavioral and social sciences under contract no. mda-1-c-1. 
1 	cognitive models 
complex examples. the noncontext approach assumes that there is only one important subset of features that are always salient. when this assumption is violated  learning is not facilitated. 
   given that attention is useful for human and machine observational learning  it is important to ensure that proposed attentional mechanisms support a useful type of learning. human observational learning most clearly is useful for natural language and concept based predictions. additionally  many machine learning studies of observational learning have concentrated on concept based prediction  schlimmer  1; fisher  1 . these types of knowledge have frequently been described as capturing correlational feature structure  rosch  1; medin & schaffer  1; fisher  1 . in other words  category structure and linguistic structure can be represented partially by correlational rules or conditional probabilities of the form: p feature} - valuel | feature'1 = value1 . thus  a  rule  such as  recovering = feathers   locomotion = wings   records the frequency with which 'feathers1 occur given that 'wings' is true. anderson  1  has further argued that even if human category structure is not implemented using conditional probabilities  it and other phenomena are best described and explained by such probabilities. similarly  recent machine learning models of concept acquisition have proposed that categories can be best learned by maintaining conditional probabilities  schlimmer  1; fisher  1 . these psychological and machine learning studies suggest that an adequate model of human and machine attentional learning should demonstrate how the attention mechanism can facilitate learning of conditional probabilities or estimates thereof. 
   this paper presents a new model of the use of attention for observational learning. this model  called contextually focused sampling  introduces a context controlled attention mechanism  and is proposed as a method for both human and machine observational learning. this use of context partially was motivated by the need for dynamic learning biases  eg. rendell et al.  1  and machine learning studies of the use of probabilistic context for generalization  eg. fisher  1 . cfs is compared to an important non-context alternative  focused sampling  billman & heit  1   to demonstrate its similar behavior for simple learning and its superior behavior for more complex learning in 

which there are multiple important subsets of features. 
1 focused sampling 
billman and heit's  1  cari implementation of the 
focused sampling  fs  method describes how attention can be used to facilitate observational learning. put simply  fs allocates more attention to those features that participate in strong correlations or rules. the 'rules' and 'correlations' referred to by billman and heit are simply the conditional probability relationships between features. 
1: choose two features  fl and f1. the probability of choosing any given feature is that featured salience divided by the sum of the a l l saliences  luce  1 . 
1: sample fl: 	observe the value  v1  for fl. 
1: given v l   predict the value for f1. the probability of predicting a value  v1  is equal to the probability from vl to v1 divided by the sum of a l l rule strengths between vl and values of f1. 
1: sample f1: 	observe the value  v1  for f1. 
1: - if predicted matches the true value  increment the rule strength from vl to v1 and increment the salience of fl and f1. 
- if they do not match  decrement rule strength and saliences. 
figure 1: focused sampling algorithm 
   in cari  figure 1   two features  such as color and size  are sampled  and a prediction of the value of the second feature is made on the basis of the value for the first feature. all training examples are assumed to be collections of feature/value pairs  and sampling a feature reveals that feature's value. for example  the color feature  when sampled  might be found to have the value 'green'. if the prediction of the second value is correct  then the saliences of the features and the strength of the prediction are incremented. otherwise  these values are decremented. the adjustment of the values is based on an estimator of conditional probabilities called the delta rule. this estimator  in similar forms  has been used in many psychological learning models  rescorla  1; rumelhart  hinton  & williams  1 . cari updates the rule strength and feature saliences by  
	sn = 	sn-1 	+ a t - sn-1  
sn = salience or strength; ¦Á = learning rate t = 1  if prediction is correct; t = 1  otherwise. 
   fs is an attentional learning mechanism that supports learning of correlational structure  billman et al.  1; billman & heit  1 ; and there are two major learning behaviors of the model that any viable alternative must also demonstrate. 
  first  fs produces a facilitation in learning as com-pared to random sampling of features  billman & heit  1 ; 
  second  particular rules are learned faster when they are part of a system of interrelated rules than when they occur in isolation. billman and her colleagues term this effect clustered feature facilitation. human subjects have demonstrated this effect for observational learning of a novel language  billman et al.  1 . 
1 contextually focused sampling 
there are many reasons to suspect that context is important for attention. first  humans are able to use information that they already know about an example to direct their attention to unusual aspects of the same example  loftus & mack worth  1 . second  some multiplelook attention models  trabasso &; bower  1  suggest an averaging method for using what is known about an example when generating a response. finally  algorithms like fs would not allow a human or machine learner to focus on different cohesive subparts of an example. for instance  there are many subsets of animal features that internally cohere  like food-type and size or habitat and means-of-locomotion. fs  though  assumes that there is only one important subset. an alternative model will be proposed that introduces a limited form of context for feature sampling. the use of the word 'context' in this work refers specifically to known feature values of a particular example. using context for attention therefore refers to using those feature values that have already been observed to help choose other features to which to attend. 
¡¡the method proposed by this paper  contextually focused sampling  cfs   samples attributes based on their estimated predictability. it calculates those estimates using estimates of conditional probability between feature values. in cfs  figure 1  then  choosing a feature depends upon that feature's predictability given what values are already known. this method allows the probability of sampling a particular feature to vary with the context. 
   the cfs algorithm  like the fs  uses the delta rule to update the estimates of conditional probabilities and no-context feature saliences. the no-context feature saliences are used for sampling when nothing is yet known about an example. feature saliences in context are based solely on the estimates of the conditional probabilities. an important difference between cfs and fs is that cfs allows multiple samples to be taken from each example in order to provide context. 
   cfs requires an algorithm for estimating predictiveness in context  i.e.  when several features have already been sampled. it is not reasonable to maintain all such higher order probabilities  because there are exponentially many of them. the most straightforward alternative is to use a bayesian estimate assuming independence  martin  1 . however  pilot studies have shown that an arithmetic average  trabasso fc bower  1  is better correlated to actual higher order conditional probabilities for the types of training example being used. davis 
	martin 	1 

 1  gives an argument for the use of a geometric average in a similar machine learning system. 
1: choose starting feature  fl. probability of sampling is featured no-context salience divided by sum of a l l no-context saliences. 
1: sample fl: observe the value  v l   for fl. 
loop for i = 1 to n 
1: choose feature  fi  based upon an estimate of probability of f's values given known values. 
1: using an estimate of joint conditional probability  predict the value of fi. 
1: sample fi: observe the value  v i   for fi. 
1: - if predicted value matches v i   increment strengths between a l l previously sampled values and v i . 	if i=1  increment saliences of fl and fi. 
- if the predicted value does not match  decrement rule strengths. if i=1  decrement the saliences. 
figure 1: contextually focused sampling 
the input was provided as lists of digits in the form   1 
1 1   to represent that the features 1 through 1 have the values 1  1  1  1 respectively. these number vectors are used for simplicity but are meant to represent vectors such as   covering=fur  habitat=land  size=big  locomotion=legs . in all three experiments  the inputs consisted of eight features  figure 1 . these inputs were presented one at a time as examples. each trial consisted of presenting one example that was selected randomly from all available inputs. these trials were divided into blocks of 1. as in billman and heit  1   the strengths between the values of the first two features were averaged to measure learning after each block of trials. in all sets of training examples  the first two features were related strongly. the test strengths were  {f  = 1 -  f1 = l f1 = 1 - f1 = 1 fi = 1 -   f 1 = 1 f1 = 1 -  f1 = 1}. statistical comparisons were made based on this average target strength after a criterion number of trial blocks. the criterion was set for each experiment when the mean strength for iis was equal to 1 ¡À 1  as in billman and licit  1 . 


1 experimental tests of cfs 
cfs was compared to fs in three experiments. the first two experiments were performed to demonstrate that cfs is a viable alternative to fs. experiment iii was conducted to determine whether cfs is superior to fs for more complex inputs. 
1 	algorithms 
the experiments performed comparisons between three algorithms  random sampling  rs   fs  and cfs. in general  it is difficult to compare algorithms because they often differ by more than one characteristic. for instance  cfs and fs differ not only by how a feature is sampled but also by how many features are processed per example. fs samples exactly two features  while cfs can sample several. these extraneous differences can confound a comparison on the characteristic of interest. it is therefore important to remove as many extraneous differences as possible before comparisons are made. the fs algorithm was modified to incorporate the loop from the cfs algorithm. the only difference between the fs and cfs algorithms was that the former always used salience to select features for sampling. the rs algorithm was like the cfs algorithm  except that it selected features independently of salience and estimates of conditional probabilities. these versions of the rs  fs  and cfs algorithms were used in all experiments. 
1 	general m e t h o d 
the general method used for all three experiments was very similar to that used by billman and heit  1 . 
1 	cognitive models 
figure 1: input vectors. 
   the variable parameters were set to the values used by billman & heit  1  and were held constant for all experiments. the initial strength values were set to 1  initial feature saliences were set to 1  and delta learning rates were set to 1. cfs and the modified fs and rs algorithms have one additional parameter  the number of features sampled per example. pilot studies demonstrated that as this parameter is increased  learning facilitation increases. this parameter was set at 1 samples per example for all algorithms throughout the experiments to reflect the limited capacity of attention and to achieve some benefit of context for cfs. 
fifteen simulated subjects were run in each condition. 
these subjects varied due to probabilistic sampling and random example selection. 
1 experiment i & ii 
cfs should be able to demonstrate the significant behaviors of focused sampling. the first of the two important fs behaviors is a facilitation of learning as compared to random sampling. in experiment i  cfs was predicted to produce a learning facilitation because  like fs  cfs's focusing mechanism leads it away from irrelevant features. 

¡¡cfs should also show increasing facilitation for a larger set of interrelated features. in experiment ii  cfs is predicted to show clustered feature facilitation because the cfs sampling is biased toward features that are interpredictive. 
1 	m e t h o d 
in experiment i  the subjects received the inputs presented in figure 1a. these inputs were chosen to maximize fs benefit by interrelating half the features  billman & heit  1  experiment 1 . the remaining four features were termed irrelevant because they each are related randomly to every other feature. experiment i compared the different attention methods  random sampling  fs  and cfs. 
   the method for experiment ii was the same as for experiment i  except that both sets of inputs from figure 1a and figure 1b were used. fifteen simulated cfs subjects received inputs with two clustered features and 1 received inputs with four clustered features. the learning measure used for each set of inputs was the difference in learning between using the cfs and rs algorithms. 
1 	results 
the learning rates depicted in figure 1 show clear effects of attention method. 

   an anova was performed using the strength values at the criterion number of trials as defined above. attention method showed a significant effect  f 1  - 1 p   1. tukey's usd was used to compare means for the attention method to determine significant differences: hsd 1  1  = 1  p  1. the comparisons revealed that both cfs and ps showed facilitation over the random method. although cfs produced a greater facilitation than fs  this difference was not significant. these data demonstrate that cfs produces the same type of learning facilitation as fs. 
¡¡in experiment ii  there was a greater facilitation for clusters of four features rather than two. a t-test was performed at the learning criterion for the four clustered feature condition  ¡ê 1  = 1 p   1. the test showed that  as with fs and in accord with human data  cfs demonstrated clustered feature facilitation. 
1 experiment i i i 
as predicted  cfs produces two findings which motivated the fs model. it was assumed that because it was sensitive to context  cfs would predict greater learning facilitation for more complex inputs than would fs. both experiment i and ii and the experiments of billman and heit  1  have used inputs in which there is only one important cluster of features. that is  there are some relevant features and some irrelevant features  and all relevant features are intercorrelated. however  more realistic inputs would allow for multiple clusters of relevant features. for example  in humans  hair-color and eye-color are somewhat intercorrelated as are armlength and height. all four of these features are relevant to feature clusters  but are not all intercorrelated. 
¡¡context is important for attentional learning in domains with multiple clusters because it allows the human or machine learner to concentrate on a single subcluster at a time. the non-context approach used by fs would set the saliences of the features independently of the cluster  permitting sampling across clusters. for example  if hair-color is the most salient and height the second most salient then the most frequent sampling pair would be across clusters. cfs can help alleviate this problem  because it allows the feature saliences to vary depending on what has already been sampled. in the above example  after hair-color is sampled  then the most salient feature becomes eye-color. the saliences in cfs are modified to have the learning focus on one cluster at a time. 
because of these considerations  it was predicted that 
cfs would be found to be superior to fs when there were multiple unrelated clusters of features in the input. as well  fs was expected to have a decreased learning facilitation as compared to random sampling. 
1 method 
the method was the same as for experiment 1 and ii  except that the inputs had three clusters of three features each and three irrelevant features. the three clusters of features were independent of each other. all three algorithms were compared on these inputs. the learning measure  as in experiment 1  was the average strength of the rules relating features one and two; and statistical comparisons were made at the criterion number of trials. 
g.1 	results 
figure 1 shows the learning curves for each block of 1 trials. there was an increased facilitation for cfs over fs and rs. 
¡¡an anova was performed that indicated a significant difference between algorithms: f 1  = 1  p   
	martin 	1 

o.ol tukey's hsd  hsd 1  = 1  p   1  revealed a significant difference between cfs and both other groups. fs was not found to be significantly different from random sampling. 

figure 1: learning for examples with multiple clusters. 
¡¡as predicted  cfs was superior to fs and rs for inputs with multiple clusters. as well  the multiple clusters prevented a significant learning facilitation for fs over rs. 
1 discussion 
attention can serve as an important learning bias for learning by observation. the type of attention mechanism used  however  should be sensitive to context if the training examples are complex  i.e.  have multiple clusters. contextually focused sampling  cfs  was proposed to be a better match to human attentional processes than earlier models that were not sensitive to context  billman & heit  1 . cfs is also an important candidate for one type of attentional bias in machine learning systems. 
¡¡these computational experiments suggest several interesting predictions about human learning. first  if humans use either fs or cfs  then they must show faster learning than random sampling  rs  would permit. second  if humans use cfs and not fs  they should demonstrate faster learning of multiple clusters than either fs or rs would permit. finally  cfs would predict that humans show different probabilities of sampling particular features depending upon what they have already sampled. 
¡¡for machine learning  the results imply that cfs can be used to make induction more feasible. a learning bias is some restriction or ordering on what can be learned  and a good bias is one that allows faster learning. cfs 
1 	cognitive models 
represents one simple type of bias that gradually comes to ignore certain features that are irrelevant and thereby accelerates learning of informative conditional probabilities. one important aspect of cfs is that if irrelevant features become relevant  then those features gradually will come to be sampled more and more often. 
¡¡also of interest for machine learning  the results of experiment iii suggest how cfs might be used to divide a set of training examples into appropriate categories of interpredictive features. because cfs is capable of finding and learning about multiple subclusters of interrelated features  it can provide a method for constructing a hierarchy of probabilistic concepts. the use of focused attention to easily isolate these concepts may result in an algorithm that is more efficient and more powerful than current concept learning methods  fisher  1 . such concept acquisition also would allow a cfs system to learn higher order conditional probabilities to improve its inference capabilities  chalnick & billman  1; davis  1 . 
   an important extension of contextually focussed sampling is to augment the attribute value lists with structure  such as predicate-style relationships between values. a naive approach would be to maintain conditional probabilities between values and other values  values and relationships  and relationships and relationships. this  however  results in an unreasonable growth in the size of storage. future research must determine how optimal context dependent attention can be approximated for structured knowledge without resorting to complete interconnectivity. 
