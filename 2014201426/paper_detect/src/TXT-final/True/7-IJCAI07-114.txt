
many real-world classification problems involve large numbers of overlapping categories that are arranged in a hierarchy or taxonomy. we propose to incorporate prior knowledge on category taxonomy directly into the learning architecture. we present two concrete multi-label classification methods  a generalized version of perceptron and a hierarchical multi-label svm learning. our method works with arbitrary  not necessarily singly connected taxonomies  and can be applied more generally in settings where categories are characterized by attributes and relations that are not necessarily induced by a taxonomy. experimental results on wipo-alpha collection show that our hierarchical methods bring significant performance improvement.
1 introduction
many real-world classification tasks involve large numbers of overlapping categories. prominent examples include the international patent classification scheme  approx. 1 patent groups   the open directory project  approx. 1 categories for web pages   and the gene ontology  approx. 1 terms to describe gene products . in most cases  instances are assigned to more than one category  since categories are rarely mutually exclusive. this leads to large scale multi-label classification problems. the categories are typically organized in hierarchies or taxonomies  most commonly by introducing superordinate concepts and by relating categories via 'is-a' relationships. multiply connected taxonomies are not uncommon in this context.
모we believe that taxonomies encode valuable domain knowledge which learning methods should be able to capitalize on  in particular since the number of training examples for individual classes may be very small when dealing with tens of thousands or more classes. the potential loss of valuable information by ignoring class hierarchies has been pointed out before and has led to a number of approaches that employ different ways to exploit hierarchies  mccallum et al.  1; wang et al.  1; dumais and chen  1 .
모in this paper  we present an approach for systematically incorporating domain knowledge about the relationships between categories into the perceptron learning and svm classification architecture. the rest of the paper is organized as follows. section 1 introduces two ways of representing taxonomy knowledge. first  derivepairwise similarities between categories based on their relative locations in the taxonomy in order to tie learning across categories. second  adapt the standard 1 loss function to weigh misclassification errors in accordance with the taxonomy structure. in section 1 we formulate the hierarchical learning problem in terms of a joint large margin problem  for which we derive an efficient training algorithm. in section 1 we propose a hierarchical perceptron algorithm that exploits taxonomies in a similar fashion. section 1 examines related work. section 1 presents experimental results which show the two new hierarchical algorithms bring significant improvements on all metrics. conclusions and future work are discussed in section 1.
1 utilizing known taxonomies
1 problem setting
we assume the patterns such as documents are represented as vectors  x 뫍 x   rd  which can be mapped to a higher dimensional feature space as 뷋 x . we denote the set of categories by y = {1 ... q}  a category by y 뫍 y  and a label set by y 뫍 p y  where p y  is the power set of y. a taxonomy is a directed acyclic graph  v e  with nodes
v   y such that the set of terminal nodes equals y  formally
. note that we do
not assume that a taxonomy is singly connected  tree or forest   but allow for converging nodes. in some cases one wants to express that items belong to a super-category  but to none of the terminal categories in y  we suggest to model this by formally adding one terminal node to each inner node  representing a  miscellaneous  category; this avoids the problem of partial paths.
모in multi-label learning  we aim at finding a mapping f : x 뫸 p y   based on a sample of training pairs { xi yi   i = 1 ... n}   x 뫄 p y . a popular approach as suggested  for instance by  schapire and singer  1   is to actually learn a ranking function over the categories for each pattern  g : x 뫸 sq  where sq is the set of permutations of ranks 1 to q. in order to get a unique subset of labels  one then needs address the additional question on how to select the number of categories a pattern should be assigned to.
모it is common to define the ranking function g implicitly via a scoring function f : x 뫄 y 뫸 r  such that  whenever  i.e. categories with higher f-values appear earlier in the ranking  for ease of presentation we ignore ties . notation g y  is used when clear from context.
1 class attributes
following  cai and hofmann  1  we suggest to use scoring functions f that are linear in some joint feature representation 붯 of inputs and categories  namely f x y;w  뫖
  where w is a weight vector. following  tsochantardis et al.  1; cai and hofmann  1  붯 will be chosen to be of the form 붦 y    뷋 x   where 붦 y  =
 뷂1 y  ... 뷂s y  t 뫍 rs refers to an attribute vector representing categories and   is the kronecker product. one can interpret w in terms of a stacked vector of individual weight vectors  i.e.   leading to the additive decomposition. the general idea is that the notion of class attributes will allow generalization to take place across  similar  categories and not just across training examples belonging to the same category. in the absence of such information  one can set s = q and define 뷂r y  = 붻ry which leads to.
모how are we going to translate the taxonomy information into attributes for categories  the idea is to treat the nodes in the taxonomy as properties. formally  we define
	v 	if v 뫍 anc y 
 1 
	 	otherwise 
where tv 뫟 1 is the attribute value for node v. in the simplest case  tv can be set to a constant  like 1. we denote by anc y  the set of ancestor nodes of y in the taxonomy including y itself  for notational convenience . this leads to an intuitive decomposition of the scoring function f into contributions from all nodes along the paths from a root node to a specific terminal node.
1 loss functions
a standard loss function for the multi-label case is to use the symmetric difference between the predicted and the actual label set  i.e. to count the number of correct categories missed plus the number of incorrect categories that have been assigned .
모yet  in many applications  the actual loss of a predicted label set relative to the true set of category labels will depend on the relationship between the categories. as a motivation we consider the genericsetting of routing items based on their membershipat nodes in the taxonomy. for instance  in a news routing setting  readers may sign-up for specific topics by selecting an appropriate node  which can either be a terminal node in the taxonomy  e.g. the category  soccer   or an inner node  e.g. the super-category  sports  . note that while we assume that all items can be assigned to terminal nodes of the taxonomy only  customers may prefer to sign-up for many categories en bloc by selecting an appropriate super-category. we assume that there is some relative sign-up volume sv 뫟 1 for each node as well as costs c  of missing a relevant item and c+ for assigning an irrelevant item. for any label set y   define anc y   뫖 {v 뫍 v :  y 뫍 y v 뫍 anc y }. now we can quantify the loss in the following manner:

note that only nodes in the symmetric difference anc anc y   contribute to the loss. in the following we will simplify the presentation by assuming that c  = c+ = 1. then  by further setting sv = 1   v 뫍 v  one gets 
|anc. intuitively  this means that one colors all nodes that are on a path to a node in y with one color  say blue  and all nodes on paths to nodes in y  with another color  say yellow. nodes that have both colors  blue+yellow=green  are correct  blue nodes are the ones that have been missed and yellow nodes are the ones that have been incorrectly selected; both types of mistakes contribute to the loss proportional to their volume.
모during training  this loss function is difficult to deal with directly  since it involves sets of labels. rather  we would like to work with pairwise contributions  e.g. involving terms
	.	 1 
in singly connected taxonomies eq.  1  is equivalent to the length of the  undirected  shortest path connecting the nodes
  suggested by  wang et al.  1 . in order to relate the two  we state the following proposition.
proposition 1. for any y y    y satisfying  and
 

for learning with ranking functionsg  we translate this into
	anc.	 1 
we look at every pair of categories where an incorrect category comes before a correct category in the order defined by g and count the symmetric difference of the respective ancestor sets as the corresponding loss.
1 hierarchical support vector machines
1 multi-label classification
we generalize the multiclass svm formulation of  crammer and singer  1  to a multi-label formulation similar to that in  elisseff and weston  1 . for a given set of correct categories yi we denote the complement by y몬i = y   yi. then following  elisseff and weston  1  we approximate the separation margin of w with respect to the i-th example as
	.	 1 
our formulation aims at maximizing the margin over the whole training set  i.e. . this is equivalent to minimizing the norm of the weight vector w while constraining all  functional  margins to be greater than or equal to 1. a generalized soft-margin svm formulation can be obtained by introducing slack variables 뷅i's. the penalty is scaled proportional to the loss associated with the violation of the respective category ordering  a mechanism suggested before  cf.  tsochantardis et al.  1; cai and hofmann  1  . putting these ideas together yields the convex quadratic program  qp 
 1 
 
s.t.
where 붻붯i y y몬  뫖 붯 xi y    붯 xi y몬 .
모this formulation is similar to the one used in  schapire and singer  1  and generalizes the ranking-based multi-label svm formulation of  elisseff and weston  1 . following  crammer and singer  1  we have not included bias terms for categories. the efforts are put into correctly ordering each pair of positive/negative labels. we can use a size prediction mechanism such as the one in  elisseff and weston  1  to convert the category ranking into an actual multi-label classification.
the dual qp of  1  becomes
  s.t..
note that

herein one can simply replace the inner products by corresponding kernel functions. it is straightforward to observe that  yields an upper bound on the training loss of the resulting classifier measured by  in the following sense. define the maximum loss as
	.	 1 
the maximal loss over a set of examples is defined as
		 1 
proposition 1. denote by  w  뷅   a feasible solution of the
qp in  1 . thenis an upper bound on the empirical maximal loss.
algorithm 1 hierarchical multilabel svm
1: inputs: training data	  tolerance
1: initialize si =    붸iyy몬 = 1   i  y 뫍 yi  y몬 뫍 y몬i.
1: repeat
1:	select i = argmax
1:select  y   y 몬  = argmaxy뫍y i y몬뫍y몬 i g iyy몬1:
1:
1:expand working set: s i = s i 뫋 { y   y 몬 } solve qp over subspace {붸 iyy몬 :  y y몬  뫍 s i} reduce working set: s i = s i   { y y몬  : 붸 iyy몬 = 1}1: until

모note that to minimize  an upper bound on  the loss in eq.  1   we could simply assign one slack variable 뷅iyy몬 for every triplet of instance  positive label  and negative label. this leads to a dual program similar to eq.  1  except the second set of constraints become.
we have yet to explore this direction.
1 optimization algorithm
the derived qp can be very large. we therefore employ an efficient optimization algorithm that is inspired by the smo algorithm  platt  1  and that performs a sequence of subspace ascents on the dual  using the smallest possible subsets of variables that are not coupled with the remaining variables through constraints. our algorithm successively optimizes over subspaces spanned by {붸iyy몬 : y 뫍 yi y몬 뫍 y몬i} for some selected instance i. moreover an additional variable selection is performed within each subspace. this strategy is also known as column generation  demiriz et al.  1 . define
w	 1 
 1 
 1 
   if 붽i = 1 	if 붽i   1 
 1 
where. define 뷍i 뫖 li   ui.
by derivation similar to that in  cai and hofmann  1   it can be shown that 뷍i = 1  i  is the necessary and sufficient condition for a feasible solution to be optimal. hence the score 뷍i is used for selecting subspaces. giyy몬 is also used to select new variables to expandthe active set of each subspace. the resulting algorithm is depicted in algorithm 1.
모more details on convergenceand sparseness of a more general class of algorithms can be found in  tsochantardis et al.  1 .
1 hierarchical perceptron
although svm is competitive in generating high-quality classifiers  it can be computationally expensive. the perceptron algorithm  rosenblatt  1  is known for its simplicity algorithm 1 hierarchical minover perceptron algorithm

1: inputs: training data {xi yi}ni=1  desired margin c.
1: initialize 붸iyy몬 = 1   i  y 뫍 yi  y몬 뫍 y몬i
1: repeat 1:	if then
1:	  i y   y 몬  = argmin
1: terminate with a satisfactory solution 1: else
1:	
1:	end if
1: until maximal number of iterations are performed

and speed. in this section  we propose a hierarchical perceptron algorithm 1 using the minimum-overlap  minover  learning rule  krauth and me뫣zard  1 .
모the minover perceptron uses the instance that violates the desired margin the worst to update the separating hyperplane. we can also deal with the instances sequentially  as in a truly online fashion. using the minimum overlap selection rule effectively speeds up the convergence and yields sparser solutions.
모if using taxonomy-based class attribute scheme in eq.  1  with tv = 1  the simple update rule in step 1 of algorithm 1 can be decomposed into
wv 뫹 wvanc 몬y  wv 뫹 wv : 뫍  몬y    anc y 
모only the weight vectors of those nodes that are predecessors of y or y몬 but not both will be updated. other nodes are left intact. this strategy is also used in  dekel et al.  1  for online multiclass classification. the more severe the loss is incurred  the more dramatic the updatewill be. moreoverstep 1 not only updates the scoring functions of the two classes in question  but also spread the impact to other classes sharing affected ancestors with them.
1 related work
many approachesfor hierarchicalclassification use a decision tree like architecture  associating with each inner node of the taxonomy a classifier that learns to discriminate between the
children  dumais and chen  1; cesa-bianchi et al.  1 . while this offers advantages in terms of modularity  the local optimization of  partial  classifiers at every inner node is unable to reflect a more global objective.
모 cesa-bianchi et al.  1  introduces a loss function  called the h-loss  specifically designed to deal with the case of partial and overlapping paths in tree-structured taxonomies.  cesa-bianchi et al.  1  has proposed b-svm  which also uses the h-loss and uses a decoding scheme that explicitly computes the bayes-optimal label assignment based on the h-loss and certain conditional independence assumptions about label paths. the loss function we proposed in eq.  1  exploits the taxonomy in a different way from hloss  partly because we always convert partial path categories to complete path ones. our loss function is inspired by real applications like routing and subscription to a taxonomy. so misclassifications are penalized along all ancestors that miss relevant patterns or include irrelevant ones. in h-loss  however  if punishment already occurs to a node  its descendents are not penalized again. in addition  our loss function works with arbitrary taxonomy  not just trees.
모 rousu et al.  1  applies the maximum-margin markov networks  taskar et al.  1  to hierarchical classification where the taxonomy is regarded as markov networks. they propose a simplified version of h-loss that decomposes into contributions of edges so as to marginalize the exponentialsized problem into a polynomial one. in our methods  learning occurs on taxonomy nodes instead of edges. we view the taxonomy as a dependency graph of  is-a  relation.
모 cai and hofmann  1  proposes a hierarchical svm that decomposes discriminant functions into contributions from different levels of the hierarchy  the same way as this work. compared to  cai and hofmann  1   which was restricted to multiclass classification  however  we deal with the additional challenge posed by overlapping categories  i.e. the multi-label problem  for which we employ the category ranking approach proposed in  schapire and singer  1 .
모in summary  our major contributions are: 1  formulate multilabel classification as a global joint learning problem that can take taxonomy information into account. 1  exploit taxonomy by directly encoding structure in the scoring function used to rank categories 1  propose a novel taxonomybased loss function between overlapping categories that is motivated by real applications. 1  derive a sparse optimization algorithm to efficiently solve the joint svm formulation. compared to multiclass classification  sparseness is even more important now that there are more constraints and hence more dual variables. 1  present a hierarchical perceptron algorithm that takes advantage of the proposed methods of encoding known taxonomies.
1 experiments
1 experimental setup
in this section we compare our hierarchical approaches against their flat counterparts on wipo-alpha  a data set comprising patent documents.
모taxonomy-derived attributes are employed in the hierarchical approaches. for comparison purpose  tv in eq.  1  is set to 1/뫏depth where depth 뫖 maxy |{anc y }|  so that  for either the flat or the hierarchical models. in the experiments  hierarchical loss equals half the value in eq.  1  for historical reason. the hierarchical learning employs this hierarchicalloss while the flat one employsthe1 loss. we used a linear kernel and set c = 1. each instance is normalized to have 1-norm of 1. in most experiments  the test performance is evaluated by cross-validation and then macroaveraging across folds.
모the measures we used include one-accuracy  average precision  ranking loss  maximal loss  and parent one-accuracy. the first three are standard metrics for multilabel classification problem  schapire and singer  1; elisseff and weston  1 . one-accuracy  acc  measures the empirical probability of the top-ranked label being relevant to the document. average precision  prec  measures the quality of la-
section#cat#doc#cat
/docacc  % prec  % -lossrloss  % pacc  % flathierflathierflathierflathierflathiera111111111111b111111111111c111111111111d111111111111e111111111111f111111111111g111111111111h111111111111table 1: svm experiments on wipo-alpha corpus. each row is on categories under the specified top level node  i.e. section . the results are from random 1-fold cross-validation. better performance is marked in bold face.  #cat/doc  refers to the average number of categoriesper document   flat  the flat svm and  hier  the hierarchical svm.

figure 1: the four columns  from left to right  depict one accuracy for flat and hierarchical perceptron  and average precision for flat and hierarchical perceptron.
sectionacc  % prec  % -losspacc  % flat	hierflat	hierflat	hierflat	hiera1 11 11 11 1b1 11 11 11 1c1 11 11 11 1d1 11 11 11 1e1 11 11 11 1f1 11 11 11 1g1 11 11 11 1h1 11 11 11 1table 1: svm experiments on wipo-alpha corpus with subsampling. three documents or less are sampled for each category.
bel rankings. precision is calculated at each position where a positive label occurred  as if all the labels ranked higher than it including itself are predicted as relevant. these precision values are then averaged to obtain average precision. ranking loss  rloss  measures the average fraction of positive label and negative label pairs that are misordered. these metrics are described in details in  schapire and singer  1 .
모maximal loss  denoted by x  was introduced in eq.  1 . it is measured by the hierarchical loss function. we also evaluate parent one-accuracy  pacc   which measures the oneaccuracy at the category's parent nodes level.
1 experiments on wipo-alpha collection
wipo-alpha collection comprises patent documents released by the world intellectual property organization  wipo  1. which are classified into ipc categories. ipc is a 1-level hierarchy consisting of sections  classes  subclasses  and groups. the categories in our experiments refer to main groups which are all leaves at the same depth in the hierarchy. each doc-

figure 1: flat and hierarchical svm on section d data  with varying training set size. a small number of documents are sampled from each category for training purpose. the learned classifiers are tested on all remaining documents. this is repeated 1 times for each sampling number. the bars depict sample standard deviation.
ument is labeled with one primary category as well as any number of secondary categories. both types of categories are used to form a multi-label corpus. we have performed independent experiments on taxonomies under the 1 top-level sections.
모document parsing was performed with the lemur toolkit 1. stop words are removed. stemming is not performed. word counts from title and claim fields are used as document features. table 1 summarizes the svm performance across the sections. the hierarchical svm significantly outperforms the flat svm in terms of x-loss  ranking loss and parent accuracy in each individual setting. this can be attributed not only to the fact that the hierarchical approach explicitly optimizes an upper bound on the x-loss  but also to the specific hierarchical form of the discriminant function. moreover  hierarchical svm often produces higher classification accuracy and average precision with gains being more moderate. to see if the improvement is statistically significant  we conducted 1-fold cross-validation on section e and then paired permutation test. the achieved level of significance is less than 1 for one accuracy  and less than 1 for the other four measures.
모figure 1 depicts the performance of perceptron algorithm with the same setting. we allow perceptron to run until convergence. it takes significantly less time than svm but reaches lower performance. we observe the hierarchical perceptron performs better in all cases.
in addition we randomly sampled 1 documents from each

	flat perceptron ranking loss  % 	flat perceptron parent one accuracy  % 
figure 1: flat and hierarchical perceptron on subsampled data.
category to simulate the situation where data are only available in small quantities. the results in table 1 show that the hierarchical svm outperforms the flat svm in all cases. the relative gains are somewhat higher than for the complete training set. fig 1 demonstrates how the performance gains vary with the size of training data. we observe that hierarchical svm excels in all runs. the gains appear to be slightly larger when the training set is sparser  except for one accuracy.
모figure 1 compares flat and hierarchical perceptron with the same subsampling setting as above. each 1-fold cross validation on a random subset of documents under one section constitutes one sample in the figure  with each section contributing 1 samples. we observe the hierarchical approach helps with one accuracy most times. it always significantly improvesaverage precision  ranking loss and parent accuracy.
1 conclusions
in this paper a hierarchical loss function has been derived from real applications. we have proposed a large margin architecture for hierarchical multilabel categorization. it extends the strengths of support vector machine classification to take advantage of information about class relationships encoded in a taxonomy. the parameters of the model are fitted by optimizing a joint objective. a variable selection algorithm has been presented to efficiently deal with the resulting quadratic program. we have also proposed a hierarchical perceptron algorithm that couples the discriminant functions according to the given hierarchy and employs the hierarchical loss in its updating rule. our experiments show the proposed hierarchical methods significantly outperform the flat methods. although they aim at reducing the hierarchical loss  the taxonomy-based approaches improve other measures such as one accuracy and average precision. future work includes more directly working with the hierarchical loss we proposed and comparing our methods with other hierarchical methods.
