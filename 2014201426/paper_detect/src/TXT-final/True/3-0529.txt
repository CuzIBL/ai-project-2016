
prism is a logic-based turing-complete symbolicstatistical modeling language with a built-in parameter learning routine. in this paper we enhance the modeling power of prism by allowing general prism programs to fail in the generation process of observable events. introducing failure extends the class of definable distributions but needs a generalization of the semantics of prism programs. we propose a three valued probabilistic semantics and show how failure enables us to pursue constraint-based modeling of complex statistical phenomena.
1 introduction
modeling complex phenomena in the real world such as natural language understanding is a challenging task and prism is a symbolic-statistical modeling language proposed for that purpose  sato and kameya  1 . it is a relational probabilistic language for defining distributions over structured objects with arbitrary complexity. parameters in the defined distribution  probability measure  are estimated efficiently from data using dynamic programming by a generic em algorithm1 called the gem  graphical em  algorithm. for example parameter estimation for pcfgs  probabilistic context free grammars  by prism is carried out with the same time complexity as the inside-outside algorithm  a well-known em algorithm for pcfgs  sato and kameya  1 .1
　the modeling power of prism comes from the fact that the user can use arbitrary definite clause programs to define distributions. there is no restriction on programs such as range-restrictedness1 or finite domain. hence theoretically speaking  there is no limit on the complexity of modeling targets. nevertheless em learning1 is possible by the gem algorithm for a program as long as it satisfies certain conditions. consequently there is no or little barrier to the em learning of new statistical models. prism thus offers a new and ideal tool to develop complex models that require both logical and statistical knowledge.
　prism is primarily designed for generative modeling where a program models a process of generating observable events as a chain of probabilistic choices. in early versions of prism  programs are assumed to be failure-free  which means that a process never fails1 once a probabilistic choice is made and thus the probabilities of observable events sum to unity. although typical statistical models such as bayesian networks  hmms  hidden markov models  and pcfgs satisfy this condition  it makes constraint-based modeling  a widely used flexible methodology  impossible in which wrong solutions are filtered out by not satisfying constraints.1
　the elimination of the failure-free condition was attempted in  sato and kameya  1  but rather limited to the failure of definite clause programs. in this paper we propose to allow general programs 1 a super class of definite clause programs  to fail. although the introduction of general programs significantly increases the flexibility of prism modeling and expands the class of definable distributions  it comes with costs. first as the probabilities of observable events do not sum to unity  we have to deal with log-linear models1  abney  1; cussens  1  which require intractable computation of a normalizing constant. we do not go into the detail of this problem here though.
　second we are faced with a semantic problem. compared to definite clause programs where the least model semantics is unanimously accepted  there is no agreed-upon semantics

1
　　here and henceforth em learning is used a synonym of ml estimation by the em algorithm. 1
　　a process fails if it does not lead to any observable output. 1 constraints are relations on variables such as equality  disequality and greater-than. 1
　　a general program is a set of general clauses  non-horn clauses  which may contain negation in their body. 1
　　a distribution has the form where is a coefficient  a feature and a normalizing constant. in our setting is the sum of probabilities of all observable events.for general programs 1 much less probabilistic semantics. but without such semantics  mathematically coherent modeling would be hardly possible. we fill the need by generalizing distribution semantics  the existing semantics for positive  = definite clause  prism programs  sato  1  to a new threevalued semantics. it considers a ground atom as a random variable taking on true  false and undefined  and defines a distribution over the three-valued herbrand interpretations of a given general prism program.
　there remains one thing to solve however. to perform em learning of a general prism program that may fail  we need a 'failure program' which positively describes how failure occurs in the original program. we show that the failure program can be synthesized mechanically by eliminating negation using foc  first-order compiler . foc is a deterministic program transformation algorithm that compiles negated goals into executable form with disunification constraints  sato  1 . once a failure program is obtained  or more generally negation is eliminated  efficient em learning is possible by an appropriate modification to the gem algorithm. our contributions are summarized as follows.
a new three-valued semantics for probabilistic logic programs with negation.
a new approach to parameter learning of such programs through automated elimination of negation by foc.
　in the remainder of this paper we discuss each of the topics after reviewing prism briefly. because our subject intersects a variety of areas from logic programming  statistics  program synthesis  to relational modeling  we are unable to give a detailed and formal description due to the space limitation. necessarily the description is mostly based on examples and proofs are omitted. the reader is assumed to be familiar with logic programming  doets  1  and em learning  mclachlan and krishnan  1 . our notation follows prolog conventions. so we use ' ' for conjunction and ' ' for disjunction. variables begin with an upper case letter and variables in clause heads are implicitly universally quantified.
1 prism modeling
informally prism is prolog augmented with probabilistic built-ins and a parameter learning routine based on the gem algorithm  the reader is referred to  sato and kameya  1  for a formal description of prism . the essence of prism modeling is to write a logic program containing random switches represented by the built-in predicate where   the name of a switch and   a result of random choice are ground terms.1 the sample space for viewed as a random variable is declared by the predicate . the probability of each outcome of a switch  i.e. that of being true  is called a parameter. parameters can be set by the  predicate or learned from sample data. the following prism program  which simulates a sequence of bernoulli trials  defines a distribution of ground atoms of the form where is a list of outcomes of coin tosses. target ber 1 .	% observable event values coin  heads tails  . % outcomes :- set sw coin 1+1 .	% parameters
ber n  r|y  :n 1 
     msw coin r   % probabilistic choice n1 is n-1  ber n1 y . % recursion ber 1  .

figure 1: bernoulli program
　when this program is loaded  the parameters of the switch is set by  : the probability of is 1 and that of is 1. there are three modes of execution in prism  namely sampling execution  probability calculation  and learning. in sampling execution  this program is executed like in prolog except that the built-in
binds probabilistically	to	or	. probabilities are calculated by using the	built-in. for example  the query  -	calculates the probability	of having	after having	. parameters are learned by using the built-in	which takes a list of sample data as teacher data.
1 three-valued distribution semantics
1 two-valued distribution semantics
the bernoulli program in figure 1 is said to be positive because there is no negation in the program. for such positive prism programs  a formal semantics called distribution semantics is proposed  sato  1  that defines a -additive probability measure1 over the set of possible herbrand interpretations from which a distribution of each predicate is derived. in the sequel 'interpretation' always means an herbrand interpretation.
　the distribution semantics is a probabilistic generalization of the standard least model semantics for definite clause programs. a positive prism program is written as   where is a set of definite clauses and a set of ground atoms to which a basic distribution is given. sampling execution of w.r.t. a top-goal is considered as a partial exploration of the least model where is a set of true atoms1 sampled from . in this semantics  each ground atom is a binary random variable taking on t  true  or f  false . as there are a countably infinite number of ground atoms  we can use an infinite number of random variables in a program. this fact  for example  makes it possible to model pcfgs faithfully.
1 negation and a new three-valued semantics
the moment negation is introduced  we are faced with numerous problems concerning semantics and computation  apt and bol  1; fitting  1 . the biggest problem on the semantic side is the lack of the canonical semantics: there exist many competing semantics for general programs. the primary computational problem is that whatever semantics we may adopt  it is not computable in general.
　we adopt fitting's three-valued semantics and extend it to a three-valued probabilistic semantics following  sato and kameya  1 . fitting's three-valued semantics is a fixed point semantics similar to the least model semantics for positive programs but goals causing infinite computation are not considered false but considered undefined. it is simpler compared to other semantics such as stable model semantics  gelfond and lifshcitz  1  and well-founded semantics  van gelder et al.  1 . also it has an advantage that it always gives a unique denotation to any general program.
　assume that our language has countably many predicate and function symbols for each arity. especially it contains the predicate symbol . a general prism program is the union of a set of general clauses and a set of ground atoms such that no atom appears as a
head in . atoms are implicitly introduced by declarations in the program as their ground instantiations.
　we associate a probability measure	over the set of two-valued interpretations for	as follows. let	be a switch name 	be a sample space for	declared by	  and	are the parameters satisfying	. let	be a probability measure defined over	such that	 	 . we define an infinite product measure	of	s over where	is the set of all switch names. note
that	is considered a function such that	.
　let	be the set of interpretations for	. we define a mapping	such that	= true if and	for some	.	otherwise	= false.	finally we define	as the probability measure over	induced by	.	by construction it holds for the sample space	for	that
and	if	.
　a three-valued interpretation for is a mapping which maps a ground atom in to t  true   f  false  or u  undefined .1 we denote by the set of three-valued interpretations for . let be a general prism program. for   we denote by the least fixed point of the fitting's three-valued direct mapping  fitting  1  applied to the general program where
	. we consider	as a three-
valued interpretation for where atoms not appearing in receive u. then is a mapping from to .
induces a -additive probability measure over . we define as the denotation of in our new three-valued probabilistic semantics  denotational semantics . is a probabilistic extension of fitting's three-valued semantics for general logic programs. it is also a generalization of the twovalued counter part defined in  sato and kameya  1  with a slight complication.1
1 failure and log-linear models
having established a probability measure in section 1  we can now talk about various probabilities such as failure probability on a firm mathematical basis. we show here how failure in generative modeling leads to log-linear models.
1 coin flipping with an equality constraint
suppose we have two biased coins	and	.
also suppose we repeatedly flip them and record the outcome only when both coins agree on their outcome  i.e. both heads or both tails.1 the task is to estimate probabilities of each coin showing heads or tails from a list of records such as
.
　we generatively model this experiment by a prism program shown below where instantiates to or with probabilities specified elsewhere. assuming prolog execution  describes the process of flipping two coins that fails if their outcomes disagree. means there is some output whereas means there is no output. we denote by the defined distribution.

failure :- not success . success :- agree   .
agree a :-	% flip two coins and output msw coin a  a   % the result only when msw coin b  b   % both outcomes agree
	a=b.	%  equality constraint .

figure 1: a general prism program
　we say a logic program is terminating if an arbitrary ground atom has a finite sldnf tree  doets  1 . since is terminating for an arbitrary set of
atoms  no atom in the least model defined by the fitting's operator receives u  u implies an infinite computa-
tion . consequently
　holds for any parameter setting of atoms. on the other hand if some ground atom receives u in our semantics  we may not be able to compute due to infinite computation. we therefore focus on terminating programs in the following so that no ground atom receives u and is computable for every ground atom .

1
   suppose that is a definite prism program and let be a ground atom. we have	but not necessarily . the reason is that when causes only an infinite loop  it is false in the least model semantics
whereas it is undefined in fitting's three-valued semantics. 1
   this model is a very simplified model of linguistic constraints such as agreement in gender and number  abney  1 .
1 ml estimation for failure models
when we attempt ml estimation from observed data such as	  we must recall that our observation is conditioned on because we cannot observe failure. therefore the distribution to be used is a log-linear model  =
　　　　　　　　　　　  in which the normalizing constant is not necessarily unity.
　generally speaking ml estimation of parameters for loglinear models is computationally intractable. on the other hand if they are defined by generative processes with failure like in our example  a simpler estimation algorithm called the fam algorithm is known  cussens  1 . it is an em algorithm regarding the number of occurrences of failure as a hidden variable. recently  it is found that it is possible to combine the fam's ability of dealing with failure and the gem's dynamic programming approach and an amalgamated algorithm called the fgem algorithm is proposed as a generic algorithm for em learning of generative models with failure  sato and kameya  1 . it however requires a positive program that describes how failure occurs in the model. we next explain such a failure program can be automatically synthesized by foc  first-order compiler .
1 foc for probabilistic logic programs
1 compiling universally quantified implications
foc  first-order compiler  is a logic program synthesis algorithm which deterministically compiles negation  or more generally1 universally quantified implications of the form into definite clauses with disequality  disunification  constraints such as  sato  1 . since what foc does is deductive unfold/fold transformation  the compiled code is a logical consequence of the source program. more precisely we can prove from  sato  1 
theorem 1 let be a source program consisting of clauses whose bodies include universally quantified implications. suppose is successfully compiled into by foc and is terminating. then for a ground atom  
	comp	resp.	iff comp	resp.	.1
　figure 1 is a small example of negation elimination by foc. as can be seen  ' ' in the source program is compiled away and a new function symbol and a new predicate symbol  are introduced during the compilation process. they are introduced to handle so called continuation. usually the compilation adds disunification constraints but in the current case  they are replaced by a normal unification using sort information declared for foc  not included in the source program . since the compiled program is terminating  theorem 1 applies.
1 modifying foc for msw
foc was originally developed for non-probabilistic logic programs. when programs contain atoms and hence % source program even 1 . even s x  :- not even x  .
% compiled program even 1 . even s a  :- closure even1 a f1 . closure even1 s a    :- even a .

	figure 1: compilation of	program
probabilistic however  compilation is still possible. suppose we compile a universally quantified implication
into an executable formula where and are terms and is some formula such that and possibly contain . although is a probabilistic predicate  our  three-valued  distribution semantics allows us to treat it as if it were a nonprobabilistic predicate defined by some single ground atom  say . we therefore modify foc so that it complies1the above formula into
 +
where is a new variable. we can prove this compilation preserves our three-valued semantics when the compiled program is terminating using theorem 1  proof omitted . figure 1 shows the compiled code produced by the modified
foc for	in the	program in figure 1.

failure:- closure success1 f1 . closure success1 a :-closure agree1 a .
closure agree1   :msw coin a  a   msw coin b  b    +a=b.

	figure 1: the compiled program for	 part 
because this program is terminating for any sampled atoms  it exactly computes .
　in what follows we show two examples of generative modeling with failure. they are available at
.
1 dieting professor
in this section  we present a new type of hmms: hmms with constraints which may fail if transition paths or emitted symbols do not satisfy given constraints. here is an example.
　suppose there is a professor who takes lunch at one of two restaurants ' ' and ' ' everyday and probabilistically changes the restaurant he visits. as he is on a diet  he tries to satisfy a constraint that the total calories for lunch in a week are less than 1 calories. he probabilistically orders pizza  1  or sandwich  1  at   and hamburger  1  or sandwich  1  at  numbers are calories . he records what he has eaten in a week like and preserves the record if and only if he has succeeded in satisfying the constraint. we have a list of the preserved records and we wish to estimate his behavioral probabilities from it.

figure 1: an hmm model for probabilistic choice of lunch
　the behavior of the dieting professor is modeled by a constrained hmm described above and coded as a prism program below where failing to satisfy the calorie constraint on the seventh day causes failure thereby producing no output.

failure:- not success . success:- hmmf l r1 1 .
hmmf l r c n :- n 1 
msw tr r  r1   msw lunch r  d  
  r == r1  % pizza:1  sandwich:1
  d = p  c1 is c+1
; d = s  c1 is c+1  
; r == r1  % hanburger:1  sandwich:1
  d = h  c1 is c+1
; d = s  c1 is c+1     
 l= d|l1   n1 is n-1  hmmf l1 r1 c1 n1 . hmmf    c 1 :- c   1.

figure 1: constrained hmm for the dieting professor
　we verified the correctness of our modeling empirically by checking if the sum of probabilities of and becomes unity. we used the 'original value' given in table 1 as checking parameters.
 - prob success ps  prob failure pf   x is ps+pf.
x = 1
pf = 1
ps = 1
　after checking that the sum is unity  we repeated learning experiments. in an experiment we generated 1 samples by the program with the same parameter values used in the checking  and then let the program estimate them using the fgem algorithm. table 1 shows averages of 1 experiments where numbers should be read that for example  the probability of is originally and the average of estimated ones is . estimated parameters look close enough to the original ones.
sw nameoriginal valueaverage estimationtr r1 r1  1 	r1  1 r1  1 	r1  1 tr r1 r1  1 	r1  1 r1  1 	r1  1 lunch r1 p  1 	s  1 p  1 	s  1 lunch r1 h  1 	s  1 h  1 	s  1 table 1: em learning
we would like to emphasize that the computational strength of our approach is that the probability of is computed in a dynamic programming manner with no additional cost just like ordinary hmms thanks to the tabling mechanism of prism  zhou et al.  1 .
1 tic-tac-toe game
this example models the tic-tac-toe game by using negation and recursion. the purpose of this example is to illustrate 'em learning through negative goals'  which has never been attempted before to our knowledge. in tic-tac-toe  the 'o' player has to place three 'o's in a row  horizontally  vertically  or diagonally on the board of a 1 by 1 board before the opponent  the 'x' player  does so with 'x'. the program in figure 1 models the game. the basic idea of modeling is expressed by the following clause:
win x :- your opponent x y  not win y  
which states that unless your opponent wins  you win  or draw . in figure 1  '*' designates an

failure :- not success .
success :-
ini board =   * * *   * * *   * * *    tic tac toe o ini board .
tic tac toe turn board :-
select move turn board move   next board turn board move n board s  
  s  == continue
; s == continue 
     opposite turn turn n turn   opponent turn n turn n board   . opponent turn n turn n board :not tic tac toe n turn n board  .

figure 1: tic-tac-toe model
empty cell. the intended meaning of is 'o' wins or 'o' draws  so fails if 'x' wins. 	probabilistically selects
and returns the next move  	   given	and	  after	which		
judges the next board      and returns one of the outcomes       through . when compiled by foc  this program yields a terminating program  and hence the probabilities of and are correctly computed by the compiled program.1
　negative recursion in logic programs causes well-known difficulties from the semantics to the implementation even in the non-probabilistic case. nevertheless  this example shows that the combination of the three-valued probabilistic semantics and compilation by foc of general prism programs offers a promising approach to dealing with negation in the probabilistic context.
1 related work and conclusion
we only discuss related work combining negation and probability in the framework of logic programming due to the space limitation and other work related to stochastic logic programing  bayesian networks and markov random field is omitted. the most relevant one seems the formalism proposed by ngo and haddawy  ngo and haddawy  1 . in their approach negation is allowed in a program but restricted to non-probabilistic goals. semantics is defined locally in relation to a given problem. so no global semantics such as a probability measure over the set of possible herbrand interpretations is given. the problem of parameter learning is not discussed either. poole allows negation in his icl  independent choice logic  programs and probabilities can be computed through negative goals  poole  1 . stable model semantics is adopted as global semantics but the problem of learning is left open.
　in this paper we first tackled the problem of defining declarative semantics for probabilistic general logic programs that contain negative goals  and have proposed a new threevalued semantics which is unconditionally definable for any programs unlike other approaches. we then tackled the second problem  namely parameter learning. we have shown how parameter learning is made possible by negation elimination through the modified foc  first-order compiler  that preserves the new semantics. as a result  constraints that may cause failure can now be utilized as building blocks of even more complex symbolic-statistical models such as generative hpsg models which is to be reported elsewhere.
