 
several published reports show that instancebased learning algorithms yield high classification accuracies and have low storage requirements during supervised learning applications. however  these learning algorithms are highly sensitive to noisy training instances. this paper describes a simple extension of instancebased learning algorithms for detecting and removing noisy instances from concept descriptions. this extension requires evidence that saved instances be significantly good classifiers before it allows them to be used for subsequent classification tasks. we show that this extension's performance degrades more slowly in the presence of noise  improves classification accuracies  and further reduces storage requirements in several artificial and real-world database applications. 
1 introduction 
instance-based learning  ibl  algorithms have several notable characteristics. they employ simple representations for concept descriptions  have low incremental learning costs  have small storage requirements  can produce concept exemplars on demand  can learn continuous functions  kibler et a/.  1   and can learn nonlinearly separable categories  ibl algorithms have been successfully applied to such varied tasks as speech recognition  bradshaw  1   handwritten letter identification  kurtzberg  1   the cart-and-pole problem  connell and utgoff  1   power load forecasting  jabbour et a/.  1   and thyroid disease diagnosis  kibler and aha  1 . however  they are highly sensitive to noise. 
　the ability to tolerate noise is a necessity for robust  practical learning methods. algorithms should demonstrate graceful degradations in performance when presented with noisy data. pruning methods  based on tests of statistical significance  were developed to allow decision tree learning algorithms to tolerate noisy data  quinlan  1  niblett and bratko  1 . similar noisetolerant methods have been developed for rule learning algorithms  clark and niblett  1 . 
   *this work was partially supported by a grant from the hughes ai research center  malibu  ca. 
1 	machine learning 
　this paper introduces an extension for ibl algorithms  also based on a form of significance testing  that identifies and eliminates noisy concept description instances. we show that the resulting algorithms' classification accuracies degrade linearly with linear increases in noise in an artificial domain and improves classification performance on several complicated domains. we also compare the performances of our algorithms with previous ibl algorithms and an extension of id1  quinlan  1  that tolerates noise. 
1 instance-based learning algorithms 
ibl algorithms induce neither rules  decision trees  nor other types of abstractions. instead  instance-based concept descriptions are represented solely by a set of instances. in this paper  each instance is represented by a set of attribute-value pairs - a point in the instance space. ibl algorithms incrementally derive their concept descriptions from a sequence of training instances. classifications are made with respect to the concept description's extension  which is derived with respect to a similarity function and a classification function. 
1 	a framework for i b l algorithms 
more precisely  all ibl algorithms consist of the following three components: 
1. similarity function: given two normalized instances  this yields their numeric-valued similarity. 
1. classification function: given an instance i to be classified and its similarity with each saved instance  this yields a classification for i.  in this paper  a classification is expressed as a concept name.  
1. memory updating algorithm: given the instance being classified and the results of the other two components  this updates the set of saved instances and their classification records. 
each instance is normalized to ensure that attributes are assigned equal importance by the similarity function. assuming that attributes have equal importance is not necessarily correct  but it is a fair approach without prior knowledge of relative attribute saliencies. 
　all ibl algorithms in this paper define the similarity of two instances as the negation of their euclidean distance in the instance space. comparisons of different similarity functions is left for future research. these 

algorithms also employ the same algorithm for tolerating missing attribute values. calculating the similarity of two instances involves computing their pairwise attribute-value differences. if either value of a pair is missing  then they are assumed to be maximally different from each other. 
　the ibl algorithms described in this paper employ either the nearest neighbor or k-nearest neighbor classification function. the former classifies an instance as being a member of the same concept as its most similar instance. the latter does the same  but takes a majority vote among its k most similar instances  we set k to 1 . 
1 	a family of i b l algorithms 
the eight ibl algorithms described in this paper  summarized in table 1  differ primarily in their memory updating functions. the simplest ibl algorithms  proximity and k-nn  save all training instances. since most real-world domains exhibit regularities that make thern amenable to storage-reducing algorithms  the remaining ibl algorithms save only misclassified training instances  which are assumed to contain additional concept boundary information . storage-reducing ibl algorithms differ in how they update memory when a training instance is correctly classified. instance-filtering algorithms discard correctly classified instances while instance-averaging algorithms replace the classifying instance with an average of it and the instance being classified. the growth  table 1  and disjunctive spanning  bradshaw  1  ibl algorithms are identical except that the former performs instance-filtering while the latter does instance-averaging. we will also experiment with the same instance-filtering variant of the k-nn algorithm and noise-tolerant versions for the three storagereducing algorithms  ntgrowth  nt disjunctive span-
1 the noise-tolerant extension 
the ntgrowth algorithm  table 1  is a noise-tolerant extension of the growth algorithm. nt disjunctive spanning and nt k-nn growth are similar extensions of their respective algorithms. these noise-tolerant algorithms differ from their respective storage-reducing algorithms in three respects: 
1. they maintain classification records for all saved instances  i.e.  the number of correct and incorrect classifications of subsequent training instances   
1. only those saved instances with significantly good classification records are acceptable for use in subsequent classification tasks  and 
1. the noise-tolerant algorithms discard those saved instances that appear to be noisy  i.e.  those instances whose classification performance is poor after several classification attempts . 
for each training instance t  classification records are updated for all saved instances that are at least as similar as t's most similar acceptable neighbor.1 
　　during the initial stages of training  none of the saved instances are acceptable. in order to more closely rnimic: the behavior of the algorithms when at least one instance is acceptable  only a randomly chosen number of most similar saved instances' classification records are updated. 
	aha and kibler 	1 

　these extensions employ a significance test to decide whether saved instances are acceptable  noisy  or neither. instances are acceptable if their classification accuracy is statistically significantly greater than their class' observed frequency and dropped if it is statistically significantly less. confidence intervals are constructed around both the instance's current accuracy and its class' current frequency. if the accuracy interval's lowest value is greater than the class frequency interval's greatest  then the instance is accepted. similarly  instances are dropped when their accuracy interval's highest value is less than their class frequency interval's lowest. confidence intervals are constructed using formula 1-1 in  hogg and tanis  1  page 1 . 
　we designed the extensions to make it difficult for an instance to be accepted by employing a high  1%  confidence for acceptance. however  we selected a lower  1%  confidence level for dropping since we would like to drop those instances with even moderately poor classification accuracies. 
1 	machine learning 
1 benefits of the extension 
we trained growth and ntgrowth on 1 randomly drawn instances from a 1-dimensional instance space containing one concept. figure   reveals which instances were saved by each algorithm when each training instance's class was mislabeled with a probability of 1%. in this trial  growth saved 1 instances  1 of which were mislabeled. the saved noisy instances invariably recorded poor classification accuracies on subsequent training instances. since ntgrowth accepts only those instances with significantly good classification accuracies  we expected it to distinguish the noisy instances from those with good classification records. 
　in fact  figure 1 shows that ntgrowth successfully filtered noisy instances from the concept description. all instances accepted by ntgrowth had good classification accuracies  at least a 1% accuracy in this case . ntgrowth's performance in this example was typical. averaged over 1 trials  1% of growth's saved instances were noisy while only a mere 1% of ntgrowth's accepted instances were mislabeled  figure 1 . 

we experimented with the proximity  growth  and 
ntgrowth algorithms on this same instance space. the results  averaged over 1 trials per noise setting  are summarized in figure 1. our purpose was to discover how these algorithms' performances and concept descriptions degrade with increasing amounts of noise  which was varied from 1% to 1%. the three dependent variables were classification accuracy  storage requirements  number of instances in concept descriptions   and the quality of the concept descriptions  the percentage of concept description instances that were mislabeled . 
1. classification accuracy: while the three algorithms performed equally well with no noise  ntgrowth's accuracy degraded more slowly  linearly  with increasing noise levels. 
1. storage requirements: the proximity algorithm saved all training instances. growth's storage requirements were much lower  asymptoting towards 1%. however  ntgrowth's were significantly lower than growth's and asymptoted towards zero. this was expected: since none of the saved instance's accuracies were significantly good at high noise levels  ntgrowth accepted only a few of them into the concept description. 
1. concept description quality: the percentage of noisy instances in the proximity algorithm's concept description increased linearly with the noise level. growth's percentage of noise in the concept description rose far more quickly. however  ntgrowth's filtering effect drastically slowed the influx of noisy instances into its concept description. 
in summary  the noise-tolerant extensions assume that the classification records of noisy instances will distinguish them from non-noisy instances. noisy instances will have poor classification accuracies because their nearby neighbors in the instance space will invariably have other classifications. 
1 experiments and results 
ntgrowth's performance degraded more slowly than the other two algorithms in these and other experiments with artificial domains. this encouraged us to test the noisetolerant extensions on six more challenging domains to see if these benefits recur during more practical applications. for comparison purposes  c1  quinlan's modification of id1  quinlan  1  that performs pruning  was also tested. the databases' characteristics are given in table 1. the average results are summarized in table 1.  we have also included the benchmark algorithm frequency  which always guesses the class with the highest frequency. this algorithm provides a comparative measure of the other algorithm's utilities.  the instances chosen for the disjoint training and test sets were always randomly selected from the databases. 
　the led display and waveform domains  breiman et a/.  1  are artificial domains with large amounts of noise  each led attribute value has a 1% chance of being noisy and all waveform attribute values contain an added noise factor . for both domains  the three noise-tolerant extensions easily outperformed their respective unextended algorithms.  however  nt k-nn growth required 1 led training instances to reach a 
	aha and kibler 	1 

1% accuracy.  they also recorded equally good classification accuracies and incomparably lower storage requirements than their respective all-instance saving algorithms  proximity and k-nn . 
　the cleveland and hungarian databases consist of cardiological records recorded at the cleveland clinic foundation and hungarian institute of cardiology respectively. these domains contain a great deal of noise; detrano  reported that his discriminant analysis method for predicting heart disease resulted with accuracies of approximately 1%. the nt algorithms again significantly outperformed the other algorithms. 
the voting domain contains small amounts of noise. 
therefore the payoff of the noise-tolerant algorithms was smaller than in more noisy domains. finally  while ntgrowth and nt disjunctive spanning performed well on the primary tumor domain  nt k-nn growth required more training instances to perform well. 
　in summary  the noise-tolerant extensions of both the growth and disjunctive spanning algorithms always recorded higher classification accuracies and lower storage requirements than their ancestor algorithms. also  their classification accuracies were always as good or better than proximity's. however  while nt k-nn growth always recorded low storage requirements  the average learning curves generated from these experiments indicate it is a much slower learner than its ancestor algorithms. only when given enough instances was it able to achieve accuracies as good as or better than the k-nn and k-nn growth algorithms. 
	ntgrowth  	nt 	disjunctive 	spanning  	and 	c1 
recorded the most consistently high classification accuracies among the ten algorithms  i.e.  ntgrowth was within 1% and the others within 1% of the highest accuracy recorded for each database . this indicates that these two noise-tolerant ibl algorithms should perform well in a large number of database applications. 
1 advantages and limitations 
we suspect that instance-based and decision tree learning algorithms can learn the same classes of concepts  namely those whose disjuncts have shapes in instance space that can be described by piecewise linear approximations  kibler and aha  1 . however  ibl algorithms have certain behavioral advantages. first  most decision tree and rule learning algorithms can form only hyper-rectangular approximations of concept boundaries in instance space. after extensive training  these hyperrectangular partitions form more detailed approximations that closely resemble ibl's more general piecewise linear approximations. decision tree algorithms probably learn more slowly when the concepts' boundaries are not aligned with the attribute dimensions' axes. in fact  two studies have reported that ibl algorithms learn faster than decision tree algorithms  shepard  1  aha  1 . an extension of the cart decision tree learning algorithm employs linear combinations of attributes  i.e.  perceptrons  at its nodes to form piecewise linear approximations  breiman et a/.  1  section 1 . however  this extension is expensive  requiring an explicit search for the best hyperplane separator in the 
1 	machine learning 
instance space for each node's instances. ibl algorithms appear to be more convenient for learning some types of concepts. 
　second  ibl algorithms are cost-effective incremental learning methods  requiring only 1   | i | x  a   attribute examinations to update the concept description for a single instance  where  i  is the size of the training set and instances are described with  a  attributes.  ibl storage-reducing algorithms require significantly fewer attribute examinations.  in comparison  both c1 and an incremental variant  id1  schlimmer and fisher  1   require at most 1 |j| x  a 1  attribute examinations to incorporate a single instance. ids  another incremental variant   utgoff  1  requires only 1 |a| 1   examinations plus 1 |ai ix  aj  x |a|  additional examinations for each pullup  where iaii and  aj  are the sizes of the two attribute domains involved in id1's pullup process. however  this includes neither the costs for recursive pullups  numeric-value partitioning  nor pruning. 
　finally  aha  indicated that decision tree algorithms build huge trees when learning a set of concept descriptions that have different sets of relevant attributes. aha presented bloom  an extension of ntgrowth that addresses this problem by learning the relative attribute relevancies and building an independent concept description for each concept.  this allows bloom to represent independent  overlapping  graded  and non-exhaustive concept descriptions.  therefore  a better decision tree strategy may be to build a separate tree for each concept  or set of related concepts . 
　c1 recorded higher classification accuracies than ntgrowth in only two of the six experiments. however  the noise-tolerant ibl algorithms do not summarize their concept descriptions. we are currently working on solutions to this problem  including combining the instancebased and decision tree approaches  which would allow the ibl algorithms to  1  generate concise concept description summaries and  1  exploit an instance-indexing hierarchy so that similarities are computed only for a priori-known similar instances  thus further reducing incorporation  and classification  costs. the decision tree algorithm would benefit from reduced storage and incremental learning costs. case-based reasoning indexing schemes  e.g.   bareiss and porter  1   may likewise assist the ibl approach  while our noise-tolerant algorithms could assist in judging the utility of cases. however  our noise-tolerating methods don't distinguish noisy instances from exceptions. while our methods might detect noisy-looking cases  further analyses should be made when this distinction is critical. 
1 conclusions 
this paper described a noise-tolerant extension for instance-based learning algorithms. we showed that the ntgrowth algorithm's performance degrades more gracefully  in the presence of noise  than does the performance of previous instance-based algorithms. in addition  the noise-tolerant extensions recorded lower storage requirements and higher classification accuracies than previous instance-based algorithms on several domains  both artificial and real-world . these gains occurred because the noise-tolerant extensions decreased the number of noisy instances used in classification decisions. 
　the key contribution of this paper was the introduction of a simple voting method  combined with a statistical test  to assist in the detection and removal of noisy instances from concept descriptions. this method  like the pruning of decision trees  quinlan  1  niblett and bratko  1  and the testing of the quality of cn1's complexes  clark and niblett  1   is based upon a simple significance test. our method  which tolerates noise by gathering evidence of correctness before employing information for classification decisions  is a representation-independent technique. an amalgamation of ibl algorithms with those that yield compilations  in the forms of rules or decision trees  should result with a superior learning algorithm having lower updating costs  lower storage requirements  and the ability to present concept descriptions concisely. 
acknowledgements 
thanks to peter clark  doug fisher  wayne iba  fat 
langley  haym hirsh  jeff schlimmer  jim wogulis  david ruby  marc albert  and stephanie aha for their valuable comments on previous drafts of this paper. we would like to thank m. zwitter and m. soklic  institute of oncology  ljubljana  yugoslavia  for donating the primary tumor database  robert detrano  v.a. center  long beach  ca  for the cleveland database  and andras janosi  hungarian institute of cardiology  budapest  for the hungarian database. these are among the 1 databases available from the u.c.i. repository of machine learning databases. 
