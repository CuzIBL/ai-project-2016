
the goal of transfer learning is to use the knowledge acquired in a set of source tasks to improve performance in a related but previously unseen target task. in this paper  we present a multilayered architecturenamed case-based reinforcement learner  carl . it uses a novel combination of case-based reasoning  cbr  and reinforcement learning  rl  to achieve transfer while playing against the game ai across a variety of scenarios in madrtstm  a commercial real time strategy game. our experiments demonstrate that carl not only performs well on individual tasks but also exhibits significant performance gains when allowed to transfer knowledge from previous tasks.
1 transfer learning
transferring knowledge from previous experiences to new circumstances is a fundamental human capability. in general  the ai community has focused its attention on generalization  the ability to learn from example instances from a task and then to interpolate or extrapolate to unseen instances of the same task. recently  there has been growing interest in what has come to be known as transfer learning. transfer learning is somewhat like generalization across tasks; that is  after seeing example instances from a function or task  the transfer learning agent is able to show performance gains when learning from instances of a different task or function. for example  one might imagine that learning how to play checkers should allow one to learn to play chess  a related but unseen game  faster.
　one particular taxonomy of transfer learning has been proposed by darpa . there are eleven  levels   ranging from the simplest type of transfer  level 1  or memorization  to the most complex form of transfer  level 1  or differing . the levels measure complexity of the transfer problem in terms of speed  quality  reorganization  and scale. of note is that the levels emphasize characteristics of the types of problems to which the transfer applies  rather than the representations or semantic content of the knowledge needed for solutions. see table 1 for the first five transfer levels.
　we introduce a mechanism for achieving transfer learning in real time strategy  rts  games. our technical contributions are:
  a novel multi-layered architecture named case based reinforcement learner  carl . it allows us to capture and separate the strategic and tactical aspects of our domain  and to leverage the modular nature of transfer learning.
  a novel combination of case-based reasoning  cbr  and reinforcement learning  rl . in particular  we use cbr as an instance-based state function approximator for rl  and rl as a temporal-difference based revision algorithm for cbr.
　in the next section  we briefly introduce and motivate the choice of rts games as an application domain. section 1 details the hybrid cbr/rl approach for transfer learning. we then provide detailed experimental analyses demonstrating validity of the proposed techniques in section 1. finally  we describe related work before ending with a discussion of recommendations for future research.
1 real time strategy games
rts games are a family of computer games that involve several players competing in real time. typically  the goal is to kill all the other players in the game or to dominate most of the territories of the game.
　rts games are very popular  and have been created by non-ai developers for a non-ai community. therefore  rts games have not been designed as synthetic domains where ai techniques can easily exhibit superior behaviors. these games are interesting as they are characterized by enormous state spaces  large decision spaces  and asynchronous interactions  aha et al.  1 . rts games also require reasoning at several levels of granularity  production-economic facility  usually expressed as resource management and technological development and tactical skills necessary for combatconfrontation. rts games are even used in real-life applications such as military training systems.
transfer leveldescription1. memorizationnew problem instances are identical to those previously encountered during training. the new problems are solved more rapidly because learning has occurred.1. parameterizationnew problem instances have identical constraints as memorization  but have different parameter values chosen to ensure that the quantitative differences do not require qualitatively different solutions.1. extrapolatingnew problem instances have identical constraints as memorization  but have different parameter values that may cause qualitatively different solutions to arise.1. restructuringnew problem instances involve the same sets of components but in different configurations from those previously encountered during training.1. extendingnew problem instances involve a greater number of components than those encountered during training  but are chosen from the same sets of components.table 1: transfer learning levels 1 from a taxonomy of eleven levels.

figure 1: a source task  left  and target task  right  for exhibiting transfer learning level 1. this example is simpler than our actual experiments; however  it demonstrates the same changes in scale that characterizes  extending . in our experiments  both the number of territories and the number of troops is increased-from three and five  respectively  in the source task to five and eleven in the target task-necessitating a qualitatively different set of tactical decisions.　the particular rts game that we used in our research is madrtstm  a commercial rts game being developed for military simulations. figure 1 shows a source task  left  and target task  right  for achieving level 1 transfer   extending   in madrts. the experimental results presented in this paper were obtained with much larger maps and with higher numbers of troops  buildings and territories.
1 approach
because we are particularly interested in applying transfer learning to rts games  we have designed an architecture suited to that purpose. in particular  we have implemented a multi-layered architecture that allows us to learn at both strategic and tactical levels of abstraction. in this section  we describe in detail that architecture as well as our approach to case-based reasoning and reinforcement learning.
1 system architecture
case-based reinforcement learner  carl  is realized as a multi-layered architecture similar in spirit to other popular multi-tiered architectures  albus  1 . upper layers reason about strategy while lower levels are concerned with ever finer distinctions of tactics. the granularity and time scale of the actions generally reduces in the lower layers. actions for an upper layer ai are treated as goals by the layer immediately below. each layer is conceptually identical  as shown in figure 1. each layer includes these three representative modules:
  the planner makes decisions about action selection at that layer based on the current state.
  the controller is a reactive module that provides perception/action interfaces to the layer below.
  the learner modifies the representation used by the planner in situ  splitting and merging actions  updating features about the actions and states  and so on.
　for our experiments  we have defined three levels. the top-most strategic level is currently rather simple  based on a hand-coded strategy . the middle tactical layer uses a hybrid case-based reasoner and reinforcement learner. specifically  this layer makes tactical decisions over an action space of attack  explore  retreat and conquer nearest territory. the lowest layer incorporates a reactive planner  scripted to perform the tasks in a predefined manner specific to madrtstm. the lower layer responds to the tasks delegated from the tactical layer as if they were goals for the reactive planner.

state featuredescriptionavghealthaverage health of the agent's troops that are currently alive%spagent's alive troops as a percentage of their initial strength%soopponent's troops killed as a percentage of their initial strength%tpterritories owned by the agent as a percentage of the maximum available in the scenario%toterritories owned by the opponent as a percentage of the maximum available in the scenariotable 1: features representing state of the game at the tactical layer of carl. at any instance of time  a territory can be either owned by the agent  the opponent or neither of the two playing sides. a side owns a territory if it sends a constant number of troops to that territory and leaves behind a smaller fixed number of troops for patrolling.figure 1: a representative layer in our transfer learning architecture carl. the architecture contains several of these levels that are conceptually similar; the planner  learner  and controller are instantiated in different ways  depending upon the needs of that level.
1 case-based reinforcement learning
reinforcement learning  rl  is one natural approach for building interactive agents. in rl  problems of decisionmaking by agents interacting with uncertain environments are usually modeled as markov decision processes  mdps . in the mdp framework  at each time step the agent senses the state of the environment  and chooses and executes an action from the set of actions available to it in that state. the agent's action  and perhaps other uncontrolled external events  causes a stochastic change in the state of the environment. the agent receives a  possibly zero  scalar reward from the environment. the agent's goal is to develop an optimal policy that chooses actions so as to maximize the expected sum of rewards over some time horizon.
　many rl algorithms have been developed for learning good approximations to an optimal policy from the agent's experience in its environment. at a high level  most algorithms use this experience to learn value functions  or qvalues  that map state-action pairs to the maximal expected sum of reward that can be achieved starting from that stateaction pair. the learned value function is used to choose actions stochastically  so that in each state  actions with higher value are chosen with higher probability. in addition  many rl algorithmsuse some formof function approximation representations of complex value functions  both to map stateaction features to their values and to map states to distributions over actions  i.e.  the policy . see  sutton and barto  1  for an extensive introduction to rl.
　case-based reasoning  cbr  provides an approach to tackling unseen but related problems based on past experience  kolodner  1; leake  1 . it has been formalized into a four-step process: retrieve  reuse  revise and retain  aamodt and plaza  1 . one view of this work is that it uses cbr to perform online lazy learning of a value function using rl techniques to solve temporal credit assignment.
case representation
individual cases must represent information about a region in the generalized state space and provide utilities for executing different actions in that region of the state space. in general  the ith case  ci  is represented as a tuple of four objects
ci =  si ai qi ei  
where
  state si is a vector composed of features representing game state that the system has already experienced.
  action set ai is a list of the possible actions the agent can take at that level in the architecture.
  utilities qi is a vector of utilities for each action in ai available for the situation described by si.
  eligibility ei is a real number reflecting the cumulative contribution of the case in previous time steps. eligibilities are used to support td λ  updates during revision.
 1 ＋ ei ＋ 1 
　it is important to note that the agent need not have performed a particular action on a particular state for the corresponding cases to provide suggestions for carrying out those actions  i.e.  ci （ case library   aij （ ai  qij where j = 1...n and n = |ai|.
　table 1 enumerates the features used in the experiments reported here. the features are chosen such that they represent the temporal changes in the behavior of the different sides in the game. note that each feature is represented as a numeric value  so in our case ci （ r1.
retrieval
because our features are numeric  we can use euclidean distance as our similarity metric. k-nn sq  = {ci （ case library | di ＋ τk}  where sq is the queried state  di is the similarity  euclidean distance  of the state knowledge of every case ci with queried state sq and τk is a task-dependent smoothing parameter. a gaussian kernel function  k di  = exp  d1i/τk1   determines the relative contribution of individual cases. the utilities for a queried state is the weighted average of the utilities of the retrieved cases:

reuse
the planner chooses the action with the highest expected utility with a probability of . to encourage exploration   begins as a large value but decreases monotonically as a function of the game trials and the number of decisions made by the case-based planner during those trials.
　the action is passed as a goal for the lower layer to achieve. as noted earlier  the lowest layer is a scripted reactive planner that commands a series of primitive actions that directly affect the madrts game engine. for example  given the tactic explore  the lower planner would find a subset of idle troops and instruct them to carry out a set of exploratory actions as defined by the game engine's instructions.
revision
our revision phase uses rl-style updates to the utilities qi for actions ai chosen by the agent. in particular  a temporal difference  λ   sutton  1  update ensures that previous errors are incorporated in future decisions.
 case library
where q t+1 and q t are the utilities of the actions chosen by the decision cycle of the agent for state representation si at time instances t +1 and t  respectively; rt+1 is a reward; ei is an eligibility trace; α is the gradient-descent based utility update factor; and constant γ is the q-learning discount rate. the reward achieved in each decision cycle is a linear combination of temporal changes in the state features. eligibilities represent the cumulative contribution of individual state and action combination in the previous time steps. after factoring in   the exploration factor  the final action selected by the decision cycle has its eligibility increased in those cases that combined to form the k-nn. this helps perform corresponding utility updates based on the case's past contributions:
the chosen action
	λγei	otherwise
the remainder of the cases in the library receive a scaling down for eligibilities of all their actions by a factor of λγ.
retention
new cases are added to the case library only if the distance of the closest neighbor dmin is larger than the threshold parameter τd. thus  τd acts as both a smoothing parameter and as a mechanism for controlling the density of cases in memory.
1 evaluation and analysis
we evaluate our hybrid cbr/rl transfer learning agent in the madrtstmgame domain. our evaluation focuses on examining the hypothesis that the agent should do well with learning a given task in madrts  and should also be able to learn better across tasks by transferring experience.
　to quantify the learning that is being done by the agent on a single task  we compute a performance metric at the end of each completion of a single game on a particular map. the metric is computed as a linear combination of five features: average health of the remaining agent troops  time elapsed in the trial  enemy troops killed  agent's troops alive and territories owned by the agent as a percentage of their respective initial strengths. given the learning curve t  representing the performance of the agent when acting directly on the target task and the transfer learning curve t/s  representing the performance of the agent after transferring the knowledge from the source task  we use the following three metrics to determine the quality of transfer learning:
  jump start - difference in the mean value of the first ten data points  trials  of t from the mean value of the first ten data points of t/s
  asymptotic gain - difference in the mean value of the last ten data points  trials  of t from the mean value of the first ten data points of t/s
  overall gain - difference in the mean value of the all the data points  trials  of t from the mean value of all the data points of t/s
　these  in fact  measure different parameters about the transfer. for example  it is possible for two curves to have the same overall gain but for the initial advantage of transfer  jump start  to be vastly different.
1 experiments
in this section  we present the results of transfer at levels 1 and 1. all our experiments are carried out with large 1 〜 1 tile maps  with at least six territories to conquer and at least nine troops fighting from each side. to exhibit transfer learning at level 1  the target scenario consisted of an increase in the map components  numberof territories  enemy troops and buildings  of at least 1. the experimental setup for testing the transfer learning at level 1 involves mirroring the initial ownership of territories when moving from source scenario to target scenario. part of the difficulty is that whereas in the source scenario  the agent's troops start with initial positions in adjacent territories  large army   in the target task  the new territories are separated by a significant distance coupled with geographical obstructions. this forces the agent to develop new sequences of tactics: forces are now two smaller battalions of troops rather than one large army.
　a trial terminates only after all the agent's troops are dead or all the opponent troops are dead and the remaining agent troops have conquered the maximum number of territories possible  given their current strength . conquering a territory requiresthat a troops need to reach the territory kill any opponent troops  and leave behind a minimum number of troops.
figure 1: transfer learning on level 1. t  s and t/s are the respective performance of the learning curves obtained as average over five iterations of the agent solving target scenario alone  source scenario alone and target scenario after transfer of knowledge over the source scenario. trend *  is a smooth fitting of the points in the curve *.
　in these experiments  we use the value of the q-learning discount rate constant γ = 1  the temporal difference constant λ = 1  the exploration factor 1  gradient-descent based utility update factor α = 1  and number of nearest neighbors k = 1. the task dependent smoothing parameter tk is maintained larger than the density parameter td  enabling several cases to contribute to the value of the query point while  at the same time  ensuring that large portions of the domain are covered with fewer cases. we analyze the agent's performance by averaging over five independent iterations. each task was executed for a hundred trials with the worst case running time of 1 seconds per trial.
1 results
the results are shown in figure 1 for tl level 1 and in figure 1 for tl level 1. curves t  s and t/s represent the agent's performance as a function of the completed game trials with learning only on the target scenario; learning only on the source scenario; and learning on the target scenario after learning on the source scenario  respectively. for clarity  we also show trend lines that are smooth fits for each curve. both trend s  and trend t  show a continual increase in performance with s appearing easier than t. the vertical axis in figures 1 and 1 is the performance of the agent  p  measured at the end of each trial  a game of just source or target task : p = 1   avghealth + 1   %so + 1   %sp + 1  
%tp + 1/timeelapsed  see table 1 . timeelapsed is a quantity measured as a function of the time scale in the rts game. completion of each task is guaranteed to take more than 1 time units. hence  the best that anyone  our agent  a hard-coded agent  random player  can do in each trial is a performance of 1.
　both figures 1 and 1 show that t/s has a significant jump start and performs much better overall than t. table 1 summarizes all of the metrics. using the signed rank test  the configure 1: transfer learning on level 1.
tl metrictl level 1tl level 1jump start1 1 asymptotic gain1 -1overall gain1 1 table 1: results on transfer learning performance metrics for levels 1 and 1.   indicates that the results are statistically significant with a confidence of at least 1%.
fidence of the overall gain for transfer learning level 1 and 1 is 1%. all the resultant values  but one  have been verified to be at confidence of at least 1%. the only exception is the asymptotic gain of level 1  suggesting that curves t and t/s have converged.
　the results show effective jump start transfer learning. in level 1 this appears to be because the agent learns when to exploit the attack tactic. this gave the agent a valuable method for performing well immediately  even for the increased number of enemy troops in the target scenario. further  reusing the conquering nearest territory tactic from the existing case library worked well despite the larger number of territories. in case of tl level 1  the agent benefits from the learned use of the explore tactic. this proved useful with the player's troops starting as two geographically separated battalions. when acting without prior learning  the agent takes more trials to discover the use of that exploration tactic to unite smaller forces.
1 related work
there has been an increasing interest in transfer learning recently. for example  given a task hierarchy  mehta et al. transfer value functions within a family of variablereward markov decision process  mdps . their primary objective was speedup learning  and demonstrated on a discretized version of an rts game. rosenstein et al.  investigate whether transfer should be attempted when there is no prior guarantee that the source and the target tasks are sufficiently similar. to the best of our knowledge  there are no existing case-based reasoning approaches that address the problem of transfer learning.
　cbr and rl have also been used in other domains involving real-time  continuous-valued attributes  including  santamaria et al.  1  for robot navigation and  gabel and riedmiller  1  for robot soccer. santamaria et al. use a casebased function approximatorto learn and generalize the longterm returns of the agent across continuous state and action spaces. unlike previous approaches  our approach explicitly decomposes the decision-making tasks into several levels of abstraction  motor  tactical  and strategy  and applies the hybrid cbr/rl decision-making algorithm at the tactical level. aha et al.  provide one of the first cbr systems that can defeat dynamic opponents in an rts. unlike their approach  we did not require representation of state spaces using pre-determined lattices. our system is capable of various levels of transfer learning across classes of problems with varying complexity in the application domain.
1 discussion
we have presented an approach for achieving transfer learning using a hybrid case-based reasoning and reinforcement learning algorithm. further  our multi-layered architecture  carl  provides a useful task decomposition  allowing the agent to learn tactical policies that can be reused across different problem instances with similar characteristics  thereby accomplishing higher levels of transfer learning.
　our experiments demonstrate online transfer learning in the continuous state space of real time strategy games  using a real rts game as a testbed. specifically  we have shown that our agent's capability to perform transfer learning is not just limited to speeding up learning  but can also lead to either better or the same overall final performance in complex scenarios.
　we plan to extend our architecture  replacing the lowest layer with a module that learns new actions and passes them up to the next layer. similarly  the middle layer should pass new actions to the strategic level so that it may perform nontrivial reasoning. our definition of tactics should also be expanded to include actions that change internal state variables. for example  one might imagine an action that changes the weights on the state features as used by the reward function. the system might learn the appropriate time to update the weights of the state features that signify troop strengths as a mechanism for implementing strategically important goals.
　we would also like to refine the case-based approximator. for example  cbr would benefit from better similarity metrics  e.g. mahalanobis distance  and a domain-specific indexing scheme for the large number of cases in its library.
　finally  we would like to solve tasks requiring more complex forms of transfer. for example  target tasks should be recognized as similar to a number of different source instances only when they are reformulated through a wellspecified transformation that carl would need to discover.
acknowledgements
we would like to thank santi ontan ＞on for valuable feedback and the anonymous ijcai reviewers for a variety of suggestions on this paper. we acknowledge the support of darpa under contract no. hr1-1. we also thank the navy research laboratory for providing the interface tielt used between carl and the rts game as well as game developers  maddoc software  for providing tielt-enabled madrtstm.
