 
unlike conventional data or text  web pages typically contain a large amount of information that is not part of the main contents of the pages  e.g.  banner ads  navigation bars  and copyright notices. such irrelevant information  which we call web page noise  in web pages can seriously harm web mining  e.g.  clustering and classification. in this paper  we propose a novel feature weighting technique to deal with web page noise to enhance web mining. this method first builds a compressed structure tree to capture the common structure and comparable blocks in a set of web pages. it then uses an information based measure to evaluate the importance of each node in the compressed structure tree. based on the tree and its node importance values  our method assigns a weight to each word feature in its content block. the resulting weights are used in web mining. we evaluated the proposed technique with two web mining tasks  web page clustering and web page classification. experimental results show that our weighting method is able to dramatically improve the mining results. 
1 	introduction 
the rapid expansion of the internet has made web a popular place for disseminating and collecting information. however  useful information on the web is often accompanied by a large amount of noise such as banner ads  navigation bars  copyright notices  etc. although such information items are functionally useful for human viewers and necessary for web site owners  they can seriously harm automated information collection and mining on the web  e.g.  web page clustering  web page classification  and information retrieval. web noise can be grouped into two categories based on their granularities: 
global noise: it refers to redundant objects with large granularities  which are no smaller than individual pages. global noise includes mirror sites  duplicated web pages and old versioned web pages to be deleted  etc. 
local  intra-page  noise: it refers to irrelevant items within a web page. local noise is usually incoherent 
bing liu department of computer science 
university of illinois at chicago 1 s. morgan street 
chicago  il 1 liub cs.uic.edu 
 with the main content of the page. such noise includes banner ads  navigational guides  decoration pictures  etc. this work focuses on dealing with local noise in web pages. we call it web page cleaning. the work is challenging because it is a non-trivial task to decide which part of a web page is meaningful and which is noisy. despite its importance  relatively little research has been done on web page cleaning so far  see section 1 . in this paper  we propose an effective feature weighting technique to clean web pages with the purpose of improving web mining. our method does not decide and physically remove those noisy blocks of a page. instead  it assigns low weights to the features in those possibly noisy blocks. the advantage of our method is that there is no need to use a threshold to determine whether a block is noisy or not. setting a suitable threshold is very hard in practice. 
　our cleaning technique is based on the following observation. in a typical commercial web site  web pages tend to follow some fixed layouts and presentation styles as most pages are automatically generated. those parts of a page that also appear in many other pages in the site are more likely to be the noise  and those parts of the page that are quite different from other pages are usually the main content of the page. to capture the common structure and comparable blocks among web pages  we introduce the compressed structure tree to compress or to merge a set of web pages from a web site. we then propose a measure that is based on information theory  shannon  1  to evaluate each element node in the compressed structure tree to determine the importance of the node. based on the tree and node importance values  our method assigns a weight to each word feature in its content block. these feature weights are then directly used in web mining. 
　our experimental results based on two popular web mining tasks  i.e.  web page clustering and classification  demonstrate that the proposed cleaning technique is able to boost the mining accuracy significantly. for example  in classification  the average classification accuracy over all our datasets increases from 1 before cleaning to 1 after cleaning. 
1 	related work 
al and the internet 	1 although web page cleaning is an important problem  relatively little work has been done to deal with it. in  lin and ho  1   a method is proposed to detect informative blocks in web pages. their concept of informative blocks is similar to our concept of main contents of a page. however  the work in  lin and ho  1  is limited by the following two assumptions:  1  the system knows a priori how a web page can be partitioned into coherent content blocks; and  1  the system knows a priori which blocks are the same blocks in different web pages. as we will see  partitioning a web page and identifying corresponding blocks in different pages are actually two critical problems in web page cleaning. our system is able to perform these tasks automatically. besides  their work views a web page as a flat collection of blocks  and each block is viewed as a collection of words. these assumptions are often true in news web pages  which is the domain of their applications. 
in general  these assumptions are too strong. 
　in  bar-yossef and rajagopalan  1   web page cleaning is defined as a frequent template detection problem. they propose a frequency based algorithm to detect templates or patterns. their work is not concerned with the context of a web site  which can give useful clues for page cleaning. moreover  in their work  the partitioning of a web page is pre-fixed  which is not suitable for a 
　web site because a web site typically has common layouts and presentation patterns. we can exploit these common patterns to partition web pages and to clean them. 
　other related work includes data cleaning for data mining and data warehousing  e.g.   lee  et al  1  . however  they are only concerned with structured data. web page data are semi-structured  and thus require different cleaning techniques. 
　web page cleaning is also related to feature selection in traditional text learning  see  yang and pedersen  1  for a survey of feature selection techniques . in feature selection  features are individual words or attributes. however  items in web pages have some structures  which are reflected by their nested html tags. hence  different methods are needed in the context of the web. 
　 kushmerick  1  proposes some learning mechanisms to recognize banner ads  redundant and irrelevant links of web pages. however  these techniques are not automatic. they require a large set of manually labeled training data and also domain knowledge to generate classification rules. 
　 kao et al.  1  enhances the hits algorithm of  kleinberg  1  by using the entropy of anchor text to evaluate the importance of links. it focuses on improving hits in order to find informative structures in web sites. 
although it segments web pages into content blocks to avoid unnecessary authority and hub propagations  it does not detect or eliminate noisy contents in web pages. 
　our work is also related to segmentation of text documents  which has been studied extensively in information retrieval. existing techniques roughly fall into two categories: lexical cohesion methods  beeferman et a/.  1; eichman et al.  1; kaufmann  1; reynar  1  and multi-source methods  allan et al.  1; beeferman et al.  1 . the former identifies coherent blocks of text with similar vocabulary. the latter combines lexical cohesion with other indicators of topic shift  such as relative performance of two statistical language models and cue words. in  hearst and plaunt  1   hearst and plaunt discussed the merits of imposing structure on full-length text documents and reported good results of using local structures for information retrieval. instead of using pure text  which is unstructured  we process semi-structured data. we can make use of the semi-structures present in web pages to help our cleaning task. 
　finally  our feature weighting method is different from feature weighting methods used in information retrieval. one of the popular methods used in text information retrieval for feature weighting is the tfidf scheme  salton and mcgill  1; baeza-yates and bibeiro-neto  1 . this scheme is based on individual word  feature  counts within a page and among all the pages. it is  however  not suitable for web pages because it does not consider web page structures in determining the importance of each content block and consequently the importance of each word feature in the block. for example  a word in a navigation bar is usually noisy  while the same word occurring in the main part of the page can be very important. 
1 	the proposed technique 
the proposed cleaning technique is based on analysis of both layouts  structures  and contents of the web pages in a given web site. thus  our first task is to find a suitable data structure to capture and to represent common layouts or presentation styles in a set of pages of the web site. we propose the compressed structure tree  cst  for this purpose. below  we start by giving an overview of the dom  document object model 1 tree and showing that it is insufficient for our task. we then present the compressed structure tree  which is followed by our entropy measures for evaluating the nodes in the compressed structure tree for noise detection. 
1 	d o m tree 
each html page corresponds to a dom tree where tags are internal nodes and the actual text  images or hyperlinks are the leaf nodes. figure 1 shows a segment of html codes and its corresponding dom tree. in the dom tree  each solid rectangle is a tag node. the shaded box is the actual content of the node  e.g.  for the tag img  the actual content is  src=image.gif'' the order of child tag nodes is from left to right. notice that our study of html web pages begins from the body tag nodes since all the viewable parts are within the scope of body. each tag node is also attached with the display properties of the tag. display properties defined by css  cascading style sheet  
1  are treated as normal display properties if css is avail-
able in the page source  i.e.  if css is not external . for convenience of analysis  we add a virtual root node 
1
 http://www.w1.org/dom/ 

1
 http://www.w1.org/stvle/css/ 
1 	al and the internet 


al and the internet 	1 


1 	al and the internet 

figure 1: the distribution of f scores of clustering 

suits  in both clustering and classification . 
　all the five web sites contain web pages of many categories or classes of products. we choose the web pages that focus on the following categories of products: digital camera  notebook  tv  printer and mobile phone. 
table 1 lists the number of documents that we downloaded 

　　table 1: web pages and their classes from the 1 sites from each web site  and their corresponding classes. 
　since we test our system using clustering and classification  we use the popular f score measure in information retrieval to evaluate the results before and after cleaning. f score is defined as follows: 

where p is the precision and r is the recall. f score measures the performance of a system on a particular class  and it reflects the average effect of both precision and recall. we will also include the accuracy results for classification. 
1 	e x p e r i m e n t a l results 
we now report our experimental results before and after cleaning. cleaning is done on all the web pages from each web site separately using the proposed technique. 
clustering 
for clustering  we use the popular k-means algorithm  anderberg  1 . we put all the 1 categories of web pages into a big set  and use the k-means clustering algorithm to cluster them into 1 clusters. since the k-means algorithm selects the initial seeds randomly  we performed a large number of experiments  1  to show the behaviors of k-means before and after page cleaning. the f score for each of the 1 runs is drawn in figure 1  where x-axis shows the experiment number and y-axis shows the f score. f score for each run is the average value of the 1 
classes  which is computed as follow: by comparing the pages' original classes and the clustering results  we find the optimal assignment of class to each cluster that gives the best average f score for the 1 classes. 
　from figure 1  we can clearly observe that after cleaning the results are drastically better. over the 1 runs  the average f score for the noise case is 1  while the average f score for the clean case is 1  which is a remarkable improvement. 
　after cleaning  1% of the 1 results have f scores higher than 1  and only 1% lower than 1. before cleaning  only 1% of the 1 results  1 out of 1  have f scores higher than 1  and 1% lower than 1. thus  we can safely conclude that our feature weighting technique is highly effective for clustering. 
classification 
for classification  we use support vector machine  svm   which has been shown to perform very well for text classification by many researchers  e.g.   joachims 1  . we used the svmlight package  joachims  1  for all our experiments. 
al and the internet 	1 　in each experiment  we build classifiers based on two classes of pages from different web sites. for example  the two classes can be camera pages from amazon and 

notebook pages from cnet. then we use the classifier to classify the combined set of camera and notebook pages from the other web sites. we have experimented with all possible two class combinations of the 1 sites. table 1 gives the averaged f scores and accuracies before and after cleaning. in the table  f n  and a n  stand for f score and accuracy before cleaning respectively  while f c  and a c  stand for f score and accuracy after cleaning. from the results  we can see that after cleaning  the classifiers perform much better. on average over all the experiments  both the f score and accuracy improve by a large margin. 
1 	conclusions 
in this paper  we proposed an effective technique to clean 
web pages for web mining. observing that web pages in a web site usually share some common layouts and presentation styles  we propose a new tree structure  called compressed structure tree to concisely capture the commonalities of a web site. this tree provides us with rich information for analyzing both the structure and the contents of the web pages. an information based measure is also proposed to evaluate each tree node to determine its importance  which is then used to assign weights to features in their corresponding web pages. using the resulting feature weights  we can perform web mining tasks. experimental results with two popular web mining tasks show that the proposed technique is highly effective. 
