 
a real-time ai system in the real world needs to monitor an immense volume of data. to do this  the system must filter out much of the incoming data. however  it must remain responsive to important or unexpected events in the data. this paper describes some simple approaches to data management  shows how they can fail to be both adequately selective and responsive  and presents an approach that improves on the simple approaches by making use of information about the system's resources and ongoing tasks. the new approach has been applied in a system for monitoring patients in a surgical intensive-care unit. 
1 	introduction 
when an ai system meets the real world  it is confronted by an overwhelming amount of data. to maintain realtime performance  the system must reduce the flow of data to a manageable amount. some simple approaches have been used to filter the incoming data  but these have severe shortcomings. to operate in real time while remaining responsive to important external events  a system needs a more intelligent approach to data management. 
　the real world supplies a continuous stream of data for a system to monitor. the system has sensors that sample the data stream at a high enough rate to catch all of the data that the system needs. however  no system doing complex processing can keep up with the nearlycontinuous data stream from many sensors. some sort of data reduction is necessary. 
　the problem is not limited to reducing a continuous data stream  however. some sensors may notify a system of particular events in the world rather than transmitting a continuous stream of data. for instance  there may be a sensor that notices a person walking into a room. unless it had knowledge about when such an event would happen  a system could not accurately predict the event. if it were receiving data from this sensor  the system would need to be able to react quickly to unexpected or unpredictable events. 
　also  without a perfect model of the world  the value of any data point is unpredictable. the better the system's 
1 	real-time and high performance 
model of the world  the better a prediction it could make  but it must be prepared for some unexpected data values. these values should not be ignored  since they might indicate an important event that the system should handle. 
　a real-time ai system must be able to reduce the data stream to a manageable amount while remaining responsive to unexpected data arrival and values. the object of this paper is to show methods for intelligent data management that address these issues. first  some simple data reduction approaches will be discussed. these methods have been used in existing real-time systems  but they are inadequate to handle the kind of variability actually present in the real world. second  an approach will be introduced that makes use of more intelligent data management techniques  making use of knowledge about the system's resources and current tasks. finally  this approach will be shown as it works in guardian  a system for monitoring patients in a surgical intensivecare unit. 
1 	limitations of current approaches 
the fixed-sampling approach to data reduction involves fixed-time sampling of the sensor data. this method uses an explicit  preset sample rate  andersson  1  fagan et a/.  1  where the system checks the input once every time interval. two assumptions are implicit in this approach. first  the system must be able to complete its processing of one set of input before the next set arrives. however  the system may remain needlessly idle when it is performing little reasoning  since there may be interesting data between the sampled data. conversely  the system may be too slow when the data require a large amount of reasoning. second  the samples may not accurately reflect the actual data. for example  see figure 1  where the sampled data show little of what is actually happening. 
　one variant of the fixed sampling approach is the averaged-sampling approach  which uses the average value of a parameter over the sampling interval. this approach has mainly been used for parameters where instantaneous values either don't exist or are highly inaccurate  such as heart rate in  fagan et a/.  1 . this still suffers from the problems of a fixed time interval  but it begins to address the problem of errors due to data fluctuation. however  much of the detail of the data is still lost  as can be seen in figure 1. 
　another sampling method  polling  trades the difficulties of a fixed time interval for less reliable data. under this approach  the system samples the data  performs whatever reasoning necessary  and then repeats the procedure  kaemmerer and allard  1  nitao and parodi  1 . here the system is guaranteed to be neither idle nor overwhelmed by data  since it reads data whenever it has processed the previous data. but all the problems with fixed sampling in losing the detail of the data are compounded. whereas fixed sampling guarantees that any interesting data events lasting longer than the sampling interval will be noticed  polling's interval is dependent on the processing. this variable interval makes the behavior of the system less well-defined. figure 1 shows an example of such a system's behavior  where in this case the amount of reasoning depends on the magnitude of the data point. 
　a final approach  fixed thresholding  uses fixed thresholds to signal out-of-range data  generating interrupts for the system to handle  ali and scharnhorst  1  anderson et a/.  1  chen  1 . this requires the data to be well-behaved with respect to the thresholds chosen. variations just within the allowable range will escape notice  even when the system is idle. slightly wider variations that cross the threshold may cause multiple interrupts. in fact  one of the worst situations for a fixed threshold is when the value hovers near the threshold. figure 1 shows how fixed thresholding can cause both periods of inactivity and periods of overabundant data. 
1 	intelligent data management 
the characteristics of real-world data and the shortcomings of the simple methods for data management suggest a set of criteria for more effective data management: 
* the system should be responsive to changing resource requirements of the reasoning system. the amount of data sampled should depend on how much time the system needs to process those data. 
  the system should be responsive to important events in the data. the fact that the system is busy should not prevent it from noticing crucial data. 
  the system should be able to focus its attention dynamically. if some parameters are particularly relevant to the current reasoning of the system  they 
　

figure 1: fixed sampling. the input data are sampled once every fixed time interval. 

figure 1: averaged sampling. the input data are sampled once every fixed time interval. the value returned is the average of the parameter over the last sampling interval. 

figure 1: polling. the input data are sampled whenever the system finishes processing the previous data. 
　
should be monitored more closely. conversely  irrelevant parameters should be monitored only closely enough to ensure that critical events are not lost. 
　these criteria can be met by using a combination of sampling and thresholding  with the sampling rate and thresholds dynamically controlled. in addition  the thresholds are made relative to the last data value sent. the sampling rate and threshold define a dynamic filter on a parameter. 
　the sampling rate is not a strict specification of the interval between incoming data values  but rather a baseline sampling rate: if no data values for a parameter have been sent within the time interval defined by the parameter's sampling rate  a new data value is sent. thus a minimum sampling rate will be maintained. however the sampling rate may be higher  depending on the thresholds. the sampling rate is changeable by the system. 
　the real power of the approach comes from having a dynamic threshold set relative to the last data item received. this relative thresholding avoids the boundary conditions of the fixed thresholding approach. since the allowable range within the threshold is changeable  the system alters the threshold to adjust the data rate. 
　note that the use of thresholding as opposed to simple sampling provides information about the filtered data. more specifically  given a parameter's data value v and a threshold of v＼  v  the parameter is guaranteed to be in the range  v -  v  v + av  until another data value is received. 
　the baseline sampling guarantees the data to be up to date  or no more out of date than the sampling interval. if the threshold is relatively small  then this will prove unnecessary. but with larger thresholds  this might prove important. also  the working system uses this in its computation of data rates  this will be discussed in section 1 . 
　dynamic filters allow the system to adjust the incoming data rate to its needs. without any knowledge of its internal processing  the system could monitor the rate at which incoming data arrived in the system. if the rate rises too high or drops too low  the system could increase or decrease the filter threshold. this reactive adjustment acts like a control system with the function of keeping the rate within a given range. this approach is illustrated in figure 1. note that the rate of data sent to the system  the ones that the filters allow through  is independent of the rate at which data are sensed. instead  the values of the sensed data determine the data rate to the system. 
　by inspecting the reasoning component  the system may notice a backlog of tasks waiting to be executed. the input could be slowed to allow the system to catch up. this needs to affect the input rate controller  which could otherwise negate any change that the task-level controller might make. specifically  the task-level controller might slow the input rate down below the minimum allowable rate defined for the input-rate controller. the input-rate controller would then increase the data rate  possibly causing further backlogs on the task queue. 
　in addition to these reactive behaviors  it would be desirable to anticipate resource requirements and change the filters accordingly. the system might plan to execute a complex procedure  but before starting the procedure  it could change the filters to slow the data rate an amount appropriate for the complexity of the task. note that this need only be approximate  since the components for rate control and task-backlog control will adjust the actual input rate to match the ongoing reasoning. 
　

figure 1: fixed thresholding. the input data are sampled whenever they cross a fixed threshold. 

figure 1: dynamic filters. the input data are sampled when they cross a threshold around the previous data value sent. the size of the threshold varies according to the data rate. 
1 	real-time and high performance 
　
   monitoring the task queue and anticipating resource requirements combine to satisfy the criterion that the system be responsive to reasoning resources. since the system adjusts the filters in response to or in anticipation of reasoning  the incoming data rate remains at an appropriate level for the reasoning in the system. 
　the system still maintains its sensitivity to important data through the use of the dynamic thresholds of the filters. each parameter is guaranteed to be within the range defined by its filter  and any deviation from that range will cause a new data value to be sent to the system. 
　to handle the criterion that attention be allocated differentially among the parameters  the system might have some notion of relevance. when a parameter is relevant to the reasoning task the system is performing  any changes to the filters would try to favor the relevant parameters over the irrelevant parameters. for instance  when a complex procedure begins  the system may be able to achieve a sufficient overall data-rate reduction by only changing the filters of parameters irrelevant to the procedure. the filters of relevant parameters would change only when changes to the irrelevant parameters were not enough. 
　since the threshold ranges and the sampling rate are changeable  the system can control the amount of attention given a parameter. the system is then able to adjust the input rate for any parameter or set of parameters when it has knowledge that suggests that action. 
1 	data management in guardian 
these ideas for data management have been applied in the backlog component of the guardian system  ilayesroth et a/.  1 . guardian is an application  implemented in the bb1 blackboard system  hayes-roth  1   for monitoring patients in a surgical intensive-care unit. currently  guardian monitors twenty parameters  performing tasks such as data abstraction  associative diagnosis  model-based explanation  and model-based diagnosis. these tasks vary in their complexity and consequently in their demands on reasoning resources. the backlog subsystem of guardian is responsible for maintaining appropriate data input levels. 
there is a filter corresponding to each parameter that 
guardian receives. these filters reside on an external processor  between the external sensors and guardian. guardian can change a filter by specifying the size of the relative threshold for a parameter  as in the amount of change allowable before a new data value is sent  or specifying the baseline sampling rate. in practice  the sampling rate stays constant while the threshold changes. 
　the maintenance of the incoming data rate is performed when data arrive in the system. guardian has a number of input streams  each of which has an associated data rate. in the current system  there is one parameter per stream  but this is not required. the data rate is computed as a time-weighted average  decaying over time. specifically  at the time a new data value is sent  if at is the time in seconds since the last data value was sent  and r is the input rate  in items per hour  at the time the last data value was sent  then 

is the new data rate. this formula provides a reasonable balance between stability and responsiveness in the data input rate. since the input rate is computed only when input arrives - this is to avoid loading the processor with continual recomputations - the data rate may start getting out of date  since in reality there is a continuous decay of the data rate when no input is arriving. the baseline sampling rate overcomes this problem by making sure data get sent at least once in the sampling interval. this ensures that the system's calculated data rate is no more than the sampling interval out of date. 
　when the data rate falls outside of the acceptable range  this condition is posted on the blackboard  which triggers a knowledge source for correcting the condition. the knowledge source has a priority proportional to the amount the data rate is out of range  so minor adjustments may be passed over when the system is busy. when the knowledge source runs  the filter thresholds for the associated parameters are changed an amount proportional to the amount the data rate is out of range. specifically  if the maximum allowable data rate is ro  the current data rate is r   r1  the current threshold is av  and the range of possible parameter values is v  then the formula 

defines the new threshold. when the data are distributed uniformly over the possible range of data  then is the fraction of data actually being sent. since 
the data rate is a factor of too high  the desired fraction of data sent is . the threshold formula 
is defined using this desired fraction. if the data are distributed uniformly  the formula will adjust the threshold by the correct amount to bring the data rate within the allowable range. in general it adjusts the data rate in the direction of the allowable range. a similar formula exists for the case where the data range is below the minimum allowable data rate. 
　the backlog component also handles backlogs of pending tasks. the bb1 agenda is the queue for activated knowledge sources  and as such provides a ready indication of the number of outstanding tasks. when the agenda grows too large  a knowledge source is triggered  with a priority dependent on the size of the agenda. the knowledge source alters the input rate indirectly. since  as mentioned earlier  the task-backlog correction needs to take precedence over the data-rate correction  the knowledge source triggered by the agenda overflow decreases the minimum and maximum allowable data rates for each stream by a percentage proportional to the size of the agenda. the data-rate adjustments will continue  except that they will now adjust the data rate to be within a lower range. this indirect control over the input rate meets the design goal for solving task backlogs. if the agenda continues to grow  the allowable ranges continue to decrease. when the agenda shrinks  the allowable ranges increase back towards their original 
settings. 
　anticipation of reasoning-intensive tasks is done by monitoring the control plan of the blackboard. when a complex task is ready to run  the backlog component adjusts the filters in proportion to the estimated complexity of the task. currently this complexity is estimated ahead of time  but with more knowledge  the estimates could be computed at run time. as discussed earlier  the estimate need only be approximate  since the backlog component will continue to monitor the data rate and the agenda  adjusting the rate to keep the incoming data at a reasonable pace for the task being performed. 
　to help focus the system's attention on important data  the system recognizes relevance of parameters to the reasoning tasks. relevance is implemented by having objects on the blackboard linking parameters to plans  or more generally  parameters to an arbitrary blackboard object . this linking establishes the parameters as relevant in the context of the plan  and when the plan is active  changes to filters are made to other parameters first. the relevant parameters have their filters changed only when the changes to the other  irrelevant parameters result in less of an overall change than needed. 
1 	performance 
the backlog component of the guardian system has shown that it can meet the criteria for effective data management. the guardian system has been run on scenarios containing up to three hours of simulated data  performing its various reasoning tasks while remaining abreast of the latest events in the data. 
　although the large-scale changes in an intensive-care setting usually take place slowly  smaller variations may occur continually. in our current system  1 parameters are sensed approximately every 1 seconds  with an additional 1 parameters - lab results and machine settings - sensed when they change. the duration of the bb1 reasoning cycle  averaged over a run of 1 minutes  is about 1 seconds. since bb1 triggers at least one knowledge source for each data value it receives  and executes one per cycle  the filters must reduce the  1 1 = 1 data items sensed each cycle to one. this is a lower bound  and in reality  the number of sensed data versus the number of data processed is significantly higher. in a representative run of 1 minutes  each of 1 parameters was sensed 1 times  for a total of 1 data values. the filters allowed between 1 and 1 of the 1 sensed values into bb1  with a total of 1 data values reaching the reasoning component. thus the filters achieved greater than a 1% reduction in data  with the quality of the solution unaffected. 
　there is a delay between the system deciding to change the input rate and the change actually taking effect. at the least  there is some communication delay when the filtering component is on a separate machine  which is the case in the current system . in the case of agenda backlogs  the change is indirect - the system shifts the range of acceptable rates rather than changing the filters directly - so there may be a delay before any filter changes occur. 
1 	real-time and high performance 
　one possible problem with delays is when a sudden change occurs in the data variability. new filters are needed to preserve a reasonable rate of input to the system  but the filter change will be delayed  so either too much or too little data will arrive in the system. the current guardian system has fixed-length buffers for input to the system  so if too much data arrives  the older data will be lost. when there is too little data  the system waits until the next data value arrives  and at that point adjusts the filters. 
　the effectiveness of the adjustment that the system makes to the filters is dependent on the delay in changing the filters. in particular  suppose that there were short bursts of highly varying data  interleaved with nearly flat data. if the bursts were close enough together  the system would start compensating for one extreme just when the other extreme occurred. this could be handled by noticing periodicity in the input data  but no such facility exists in the current system. 
1 	ongoing work 
the current implementation of data management in the guardian system includes the ideas presented in this paper. further work is in progress to improve these ideas. in particular  a large part of the data management is being moved outside the reasoning component to avoid any unnecessary interference with the reasoning tasks  and the filter criteria are being expanded so that data may be filtered even more intelligently. 
　if a large portion of data management is done within the reasoning component of the system  it could potentially consume significant resources. this could make the overall performance of the system worse rather than better. in the current implementation of guardian  the data management component is carefully constructed so that it will not interfere with the normal operation of the system. for instance  knowledge sources for data management are rated in proportion to the severity of the problem they are to correct. this way minor corrections will be put aside when important reasoning is underway. 
　a better approach to reduce the resource demands of data management is to move as much of the work as possible to another processor. work is underway to move the data management task to the  remote  filtering machine  since a large part of its operation is independent of the reasoning. the parts that require information from the reasoning component are also being moved if they can get the information in a small amount of communication. for instance  the size of the agenda is easily communicated to a remote machine. 
　another direction of work in progress is making the filtering criteria sensitive to more features. the reasoning system can set expectations  and the filtering component will use a violated expectation as another reason to send a data value  tagged appropriately. also being added are classification ranges  so that when a data value changes from what the system considers low to normal  a data value will be sent. the filtering procedure allows these classification ranges to be set dynamically by the reasoning system. additionally  some simple trend analysis will be performed to ensure that the overall rate of change in some parameter is within expected bounds. all of these additions are expected to be in place shortly  making the incoming data much more meaningful to the reasoning system. 
1 	acknowledgements 
this work was supported by grants from darpa and nih and gifts from rockwell and fmc. nicholas parlante worked on an implementation of the external filtering program. micheal hewett helped on implementation and ideas in general. luc boureau is responsible for extending the ideas in the current system. andrew golding  micheal hewett  and andrew kosoresow provided helpful comments on earlier drafts. 
