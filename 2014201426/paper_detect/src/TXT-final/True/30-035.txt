 
case-base maintenance is gaining increasing recognition in research and the practical applications of case-based reasoning  cbr . this intense interest is highlighted by smyth and keane's research on case deletion policies. in their work  smyth and keane advocated a case deletion policy  whereby the cases in a case base are classified and deleted based on their coverage potential and adaptation power. the algorithm was empirically shown to improve the competence of a cbr system and outperform a number of previous deletion-based strategies. 
in this paper  we present a different case-base maintenance policy that is based on case addition rather than deletion. the advantage of our algorithm is that we can place a lower bound on the competence of the resulting case base; we demonstrate that the coverage of the computed case base cannot be worse than the optimal case ba.se in coverage1 by a fixed lower bound  and the coverage is often much closer to optimum. we also show that the smyth and keane's deletion based policy cannot guarantee any such lower bound. our result highlights the importance of finding the right ca.se-ba.se maintenance algorithm in order to guarantee the best case-base coverage. we demonstrate the effectiveness of our algorithm through an 
experiment in case-based planning. 
1 	introduction 
case-ba.se maintenance refers to the task of adding  deleting and updating cases  indexes and other knowledge in a case base in order to guarantee the ongoing performance of a cbr system. case-base maintenance is particularly important when a case based reasoning system becomes a critical problem solving system for an organization. this is because for any such organization  the knowledge may change over time and the need for different knowledge structures for problem solving may vary. the case-base size will increase with time  creating 
1 	case-based reasoning 
significant barrier to reasoning efficiency and the user's ability to understand the results. 
　in response to these problems  there has been a significant increase in case-base maintenance research. one branch of research has focused on the ongoing maintenance of case-base indexes through training and case base usage  cunningham et a/.  1; fox and leake  1; aha and breslow  1; zhang and yang  1 . another branch of research have focused on increasing the overall competence of the case base through case delet ion smyth and keane  1; markovicli and scott  
1; domingos  1; aha ct a/.  1; smyth and keane  1; racine and yang  1  in a way similar to utility-based control-rule deletion policies  minton  
1 . excellent surveys of this field can be found in  leake and wilson  1  and  watson  1 . 
　this recent surge of interest in case-base maintenance is highlighted by smyth and keane seminal work on competence-preserving case-deletion policy  smyth and keane  1 . in this work  the cases in a case base are classified into a type hierarchy based on their coverage potential and adaptation power. the deletion policy then selectively deletes cases from a case base guided by the classification of the cases until a limit on the case base size is reached. the algorithm was empirically shown to preserve the competency of a cbr system and to outperform a number of previous deletion based strategies. 
　in this paper  we present a detailed analysis of smyth and keane's deletion based policy and show that  this policy does not always guarantee the competence pre-
serving property. in particular  we show that using this policy can potentially result in a case base with significantly decreased performance. in response  we develop a different case-base maintenance policy that is based on case addition rather than deletion. by this policy  cases in an original case base are repeatedly selected and added to an empty case base until a certain size limit is 
reached  producing an updated case base which high coverage guarantee. the addition based policy will allow a more global view of the case base as a result of the maintenance operations. we show that both the smyth and keane's deletion-based policies and our addition-based policies have the same time complexity. the advantage 

of the addition-based policy is that we can place a lower bound on the competence of the resulting case base while the deletion-based policy cannot; we demonstrate that the coverage of the computed case base cannot be worse than the optimal case base in coverage by a fixed lower bound  and often is much closer to the optimal coverage. 
　our result highlights the importance of finding the right similarity metrics in order to guarantee the best case-base coverage. we contend that it is important to tie the definition of similarity-based metrics to adaptation costs. based on this observation  we demonstrate through case-based planning how to construct high-quality similarity metrics that lead to highly competent case bases  and discuss various implications of our result in practical implementation of case-base maintenance systems. finally  we confirm our competencepreserving claims through an experiment in case based planning. 
1 case-base maintenance and case-deletion policies 
1 	related work 
recently  there has been intense interest in case based reasoning research community on the problem of casebase maintenance. leake and wilson gave an in-depth summary and analysis of this field  leake and wilson  1 . for our purpose  the problem of case-base maintenance is divided into two broad categories: maintaining the case base indexes and maintaining the case base contents. in case-base index maintenance   cunningham el a/.  1  presents an introspective learning approach to 
learn adjusted case base indexes by monitoring the runtime processes of a case based reasoner. an extended approach is developed in  zhang and yang  1   where a layered architecture is adopted for representing case base indexes and a neural network algorithm is adapted for maintaining the feature weights. fox and leake  fox and leake  1  and aha and rreslow  aha and breslow  1  consider case-base index-revision policies that improve the performance of a case base in response to events such as plan failures. 
　researchers in case-base content maintenance are mainly concerned with the issue optimization. due to the large size of some case bases  it is necessary to delete cases as time goes by  and when retrieval become increasingly expensive  smyth and keane  1 . this issue is called the swamping problem. the main strategy is that of deciding which cases to delete based on an adaptation structure. these strategies include one for random deletion as advocated by  markovich and scott  1   and more sophisticated deletion based on the frequency with which each case is retrieved and deleted if they are not frequently accessed  minton  1 . the problem with both of these approaches is that  important  cases can be deleted by mistake. various approaches have been designed to address this problem. dorningos  domingos  1  and aha  kibler and albert  aha et a/.  1  consider instance-based learning approaches for generalizing to reduce the size of a case base without decreasing its problem-solving power. smyth and keane  smyth and keane  1  consider a competencepreserving approach to case deletion. watson  watson  1  presents methodologies for a human designer of a case base to consider for case-base maintenance. racine and yang  racine and yang  1  consider the problem of removing redundancy and inconsistency from a large semi-structured case base in order to improve the case base performance. 
1 	coverage and neighborhood functions 
 we define a case as a problem-solution pair. that is  each element c of a case base is a pair c =  x  .s   where  is a corresponding solution to a problem descrip-
tion x. for each problem x1 in a case base x1  x1 can represent the case i. 
　　hence we also call x1 a case. let   be the set of cases x1 whose solution  is close to  . more 
formally  

where l is a constant limit on the cost of adapting a solution. essentially  n defines a coverage of x . we call n x   the coverage or neighborhood of x . later in the paper  section 1   we define a distance metric d x1 x1   for case based planning using the number of adaptation steps to apply to the solution of x1 in order to solve x1. for now  we assume that the neighborhood function is given as done by smyth and keane  smyth and keane  1   and consider how to use this information to compute a near-optimal case base a'i from a given case base 
a'. 
　based on the above notion  the coverage of a case is determined by a similarity metric and adaptation costs. we can consider the coverage of a case as the neighborhood of the case within certain adaptation limits. hence  we consider the notion of a problem neighborhood and coverage interchangeable. similarity metrics are used to measure the similarities between cases. they are usually numerical  but a good similarity metric is not easy to find in many application domains. we therefore introduce the notion of a neighborhood which is a more intuitive notion. 
1 	analyzing case deletion policy 
when the size of a case base gets large  there1 is a need to select a subset of the cases to keep. to address this problem  smyth and keane  smyth and keane  1  suggest a case deletion based approach. the premise of this approach is that each case in the case base should be classified according to its competence. these classifications are made according to two key concepts: coverage and reachability. coverage refers to the set of problems that each case can solve. reachability is the set of cases that can be used to provide solutions for a problem. 
　cases that represent unique ways to answer a specific query are pivotal cases. auxiliary cases are those which are completely subsumed by other cases in the base. in 
	zhu and yang 	1 


figure 1: case base structure graph 
between these two extremes are the spanning cases which 
link together areas covered by other cases  and support cases which exist in groups to support an idea. the deletion algorithm deletes cases in the order of their classifications : auxiliary  support  spanning and then pivotal cases  smyth and keane  1   
　similarly  the definitions for spanning and support cases also rely on the concepts of coverage and reachability. in their evaluation of their algorithm  which consisted of 1 cases  smyth and keane restrict the size of the case base and the size of the problem space and manually identify the category in which each case falls. 
　the deletion based policy is motivated by the need to delete cases in order to maintain the competency of a case base at a reasonable size. however  no mention is made about why auxiliary cases should be deleted first  and how the quality of the resulting case base is ensured after the update is done. pivotal cases may be  important  or they may simply contain anomalies that distinguish them from the rest of the case base. but deleting the rest of the case base first only offers an intuitive solution to the case-base maintenance problem; there was no guarantee on the level of true competence preservation. 
　smyth and keane's terminologies can be illustrated graphically as in figure 1. in this figure  an arrow from a node x to a node y means that the case y is in the neighborhood of the case x. therefore  in figure 1    
n b  = {1} and n c  = {c}. according to smyth and 
keane's classification scheme  x is pivotal  y is spanning and a  1  c are auxiliary. 
　from these definitions  it is easy to see that pivotal problems are the most unique  spanning problems are less unique  but auxiliary problems are not unique. smyth and keane's footprint deletion  fd  and footprint-utility deletion  fud  policy delete auxiliary problems first  then support problems  then spanning problems  finally pivotal problems. this approach is better than a random deletion policy for preserving competence. the competence of a case base built by smyth and keane's footprint deletion  fd  or footprint-utility 
1 	case-based reasoning 
deletion  fud  policy is not guaranteed to be preserved. 
theorem 1 fd and fud can lose almost all the competence in the worst case. 
proof: 
　to prove this theorem  we only need to give an example. suppose that in some domain  we have the graph structure as shown in figure 1. 
in this figure  each node stands for a problem  or case . 
we see that the problem x is pivotal  y is spanning and a. 1 c are auxiliary. suppose that we want to build a case base with only one element; that is  we restrict our case base to be of size one. by the footprint deletion or footprint-utility deletion policies  cases a b c should be deleted first  followed by y. as a result  only problem x is left in the case base. the coverage is of the original competence. if we increase the number of auxiliary cases  such as a  b  c in the figure  to k  then the coverage is  of the original competence. the percentage approaches to zero as  therefore  the quality of the case base is arbitrarily bad. this completes the proof. 	d 
1 	case-addition based policy 
suppose that the neighborhoods of all cases in a case base are obtained. we say a case is good if its neighborhood is large. to select good cases  the distribution of the cases or the frequency of cases occurring should also be considered. for instance  in an example travel domain  section 1   suppose that more people prefer a travel plan between city 1 and city 1  then we should put this plan in a case base in order to minimize the cost of searching for such plans. taking this into account  we define case coverage as follows: 
　　given a domain  we have a case space a  and a solution space y. let x x be a case. denote n x  the neighborhood of x and   n xi  contains cases which are close to some other cases in x1. suppose that p is a a frequency function of the cases  between 1 and 1% ; equivalently there is a distribution of cases. then the case base coverage of x1 is defined as 

since x1 is a subset of a'  the case coverage is a real number between 1 and 1. if the function p does not exist  we assume that p = 1 - the constant function. 
　　the benefit of a case x with respect to a set w of cases is defined as bft x  = p y   where   the benefit of a case set {x1  x1       xk} is defined as p y -
　suppose we want to build the case base x  with at most k cases based on a set z of cases. we formulate this optimization problem as follows 
　　 *  choose cases maximize the benefit of 

　this optimization problem is np-compete. one can easily prove this by a reduction from set-covering  garey and johnson  1 . thus  we look for heuristics to find approximate solutions. consider the following case-addition algorithm based on selecting cases from z and adding them to the new case base: 
case-addition algorithm: 
1. determine the neighborhood n x  for every case 

1. select a case from z - x  with the maximal benefit with respect to n{x   and add it to x . 
1. repeat step 1 until n z  - n xi  is empty or x1 has k elements. 
　remark. here we consider the case coverage as the benefit. in fact  the benefit can be defined on other notions as long as it captures the concept of usefulness. 
　the case-addition algorithm is a greedy algorithm. therefore it may not give the best choice of x1 with respect to the case coverage. however  we can prove that its case coverage is at least 1% of the optimal case base  for any fixed case-base size k. 
theorem 1 the case addition algorithm produces a case base x1 such that the coverage of x1 is no less than 1% of the coverage of an optimal case base. 
　the proof for this theorem is derived from that of a 
　greedy algorithm for set covering. due to space limitations  we only provide an intuitive sketch for the proof here. suppose that  then 
x  = {x1  x1  ＊ ＊    xk  has k elements and labeled by the order of selection. let a; be the benefit of suppose that {y1  y1  .....  yk} is an optimal choice for x1. 
let bi    be the benefit of yi; under the index order. note that . we can then relate ai with bj and derive the following inequality: the ration 
of the coverage of  and the optimal case base coverage for k cases is 

our result is motivated by a similar result in  harinarayan et a/.  1  for constructing a data cube used in building a data warehouse. the main difference is that in cbr case adaptation is our main concern whereas in data warehousing the utility of a data view is of great importance. 
　analyses of both smyth and keane's case-deletion algorithm and our case-addition algorithms reveal that they require the same time complexity . this is because both algorithms are dominated by computing the coverage of cases  hence their costs should be about the same. when n is large  computing the competence 

figure 1: graph of coverage 
categories and the coverage of cases becomes expensive. however  the key point is it is only computed once as a start-up cost. we have developed a practical incremental case-addition algorithm which will will present in an extension of the paper. 
1 	relating case base size with coverage 
we have so far assumed that the case base size k is given. having a different k will result in a case base with a different coverage value. how should a case base maintainer choose an appropriate size for a case base  
　in this section we will estimate how a case base size determines its coverage. in our notation  we would like to estimate the ratio between . suppose that x1  x1   .... xn are cases selected by case-addition algorithm with the benefits of a  a1         an. then 

hence  the ratio  is between  and 
however  this estimation is rather rough. 
　our theorem below points out the precise relationship between case-base sizes and case-base coverage: 
theorem 1 let m be the size of the original case base before applying the case-addition algorithm for maintenance. let k be the size of the case base after maintenance. suppose that all cases have equal probability of appearance; that is  the frequency of all cases are the same. suppose also that when constructing the result case base the benefits of new cases decrease linearly. then we have 

where r = k/m. 
　again due to space limitations we omit the proof. instead  we plot the coverage curve as shown in figure 1. 
	zhu and yang 	1 


figure 1: state graph of a travel domain 


size of case base 
figure 1: coverage graph of a travel domain 

from the graph  we see that over 1% of cases can attain about 1% coverage while 1% of cases can attain about 1% coverage. in general  things are better. we have used the linear approximation to draw the coverage 
graph. in general  the difference  ak + 1 - ak  decreases quickly as k increases. hence  we can have a better model via replacing   
where h is an integer   1. the resulting graph of cov-
erage is similar  but the desired coverage can be reached much sooner. 
1 	experiments with the case-addition algorithms 
to fully validate our case-addition algorithm  we need to perform empirical tests in various domains. in the remaining of this paper  we present one such experiment in a travel planning domain. in this domain  a problem x can be considered a pair of states: x =  .si .sy   where st is an initial state and sg a goal state. a case is a pair  x p   where x is a problem definition and p a plan. we consider a plan as a sequence of actions taking one from the initial state to the goal state. there are many ways to define planning algorithms for finding a plan  kambhampati et al  1; yang  1 ; here we focus on how to reuse the previous plans. 
　in planning various similarity metrics have been proposed for determining the distances between two plans hammond  1; hanks and weld  1 . for ex-
ample  the foot-printed similarity metric  veloso  1; 1  compares relevant features of a case with features of a new problem where a relevant feature is considered relevant to a goal and a solution if the feature contributes to achieving the goal in the solution. rao:cbp1 presents a mechanism for adding plans to a plan library in order to limit the size of the case base. the adaptation guided metric  kinley ct al  1  exploits the adaptation knowledge of a case base. 
　in our example travel domain  as shown in figure 1  there is a map in which an agent will travel between two cities. the agent would like to remember a few useful paths so that a case base can be constructed. in order 
1 	case-based reasoning 
to do this  it is assumed that a neighborhood function is defined based on adaptation as follows: let a case x be a problem together with a solution soln  where 
and soln - . here s  = -s  and 
sk = s g . then the neighborhood n x  of problem x is defined as the neighborhood of the solution. more precisely  
		 i  
where the neighborhood of a state n sj  is defined as the set of states  that is  cities  that are one step from sj. we call this neighborhood function the  state-based  similarity measurement for planning. 
　in our experiment  an input problem is defined as any pair of cities  and the solution  which is a case  is a path going from an initial city to a destination city. 
　the state can be moved horizontally or along the main diagonal lines. the number of states = 1. the problem space has 1  = 1 problems. randomly select 1 problems from the problem space and solve these problems from scratch by a forward chaining and breathfirst planner. by computing the neighbors of these problems  we see that the union of all these neighbors covers 1 problems. applying the case-addition algorithm  we get the result which is shown in figure 1. 
1 	conclusions and discussion 
we conclude that it is important: to tie similarity metrics with adaptation costs. based on this concept  we have given an approximation algorithm for building a case base with near-optimal property. we attribute this property to the fact  that we use a case-addition rather than a case-deletion policy as done by smyth and keane. in the future  we will study how to maintain a case base in order to increase the quality of cases in addition to increasing the case bsae coverage. 
acknowledgment 
the authors are supported by grants from: 	natural 
sciences and engineering research council of canada  nserc   an ebco/epic nserc industrial chair fund  

bc advanced systems institute and canadian cable labs fund. we wish to thank ijcat reviewers for their in-depth suggestions. 
