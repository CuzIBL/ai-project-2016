 
   this work proposes a learning scheme which integrates characterization and discrimination activities with the aim of improving learning efficiency in large data sets. characterization is considered to be a process which builds up a rough concept description using only positive examples. this description already excludes most of the extreme negative examples. 
discrimination is considered to be an incremental learning process which begins with the characteristic description and refines it so as to make it consistent with the negative examples  near misses  which are still covered. during this phase learning efficiency is greatly improved by considering only near misses as counterexamples. finally  the description is simplified by dropping some characterizing but not discriminant parts of the description. 
   this learning scheme is discussed and compared with the traditional data reduction techniques. some experimental results are reported which show the gain in efficiency obtained  particularly on real applicative domains. 
1 	introduction 
　　machine learning has been one of the main theoretical topics of ai research over the past decade and several learning techniques have been widely investigated. among them inductive learning from examples has reached advanced level and is beginning to move out of the labs and face real applicative problems. in taking this step the learning systems must change their techniques of dealing with the few hand coded examples of artificial domains and they must be adapted so that they can manage the large number of real data present in the environment in which the learning system is operating. on one hand this impact with large data bases of samples is positive for the learning systems: in fact they can demonstrate their ability to learn rules which are not merely a summary of the examples but incorporate an ability to make predictions  which  in turn  can be tested on statistically relevant test sets. on the other hand  an increased number of samples can cause inefficiency due to the complex computations involved  especially in the systems which use a first order representation language  and their greater learning power calls for much greater computational effort. 
　　for this reason it is necessary to study techniques which will allow the examples to be used more efficiently in the inductive process. these techniques have often been called data reduction techniques  michalski and larson  1  cramm  1  pollack  1  and their aim is to cut down computational effort by reducing the number of examples involved in the learning process  without compromising the meaningfulness of the learned knowledge. unfortunately  the methods which have been proposed up to now are either inadequate for a first order representation language or  in their turn  arc computationally too expensive. after a brief revision of the present state of the art  a new data reduction technique is presented  which adopts an approximation of the characterization as the evaluation criterion to select the counter-examples for each class. this technique is a return to the classical concept of near miss introduced by winston   and proposes a more operational definition of the same concept  that is used to reduce the number of counterexamples which have to be taken into account during the discrimination process. 
　　the main idea is as follows: first a costless approximation  p* of the characterization  p is computed for each class  using only the positive examples; then  the class counter-examples covered by cp' are defined as near misses of the class  the most useful counter-examples for the computation of a discriminant description of the class ; finally  a discriminant description y of the class is obtained through an incremental learning process which  taking cp' as the starting hypothesis  specializes and simplifies it so as to make it consistent and less complex. the learned knowledge  a first order discriminant formula for each class  is proved to be complete and consistent  within a prescribed tolerance  with the original examples 
	gemello and mana 	1 
and counter-examples. in fact examples are not arbitrarily dropped but they are used more rationally so that there is no loss of information. 
　　in the technique proposed the characterization process can be seen as a bootstrap procedure which provides the discrimination process with a rough hypothesis which has to be refined using the counter-examples which are still covered. from another point of view this technique constitutes a proposal which could be applied to the problem of integrating characterization and discrimination activities so as to form a learning scheme for conceptual discrimination. 
1 	data reduction techniques 
　　data reduction techniques are embedded in systems which learn from examples in order to take into account the decline in performance when large data sets are considered. this decline is especially relevant in learning systems which use a first order representation language: in fact  this kind of languages offers greater learning power  rules to discriminate structured concepts may be learned  but it has to be paid for by an increased computational effort. a data reduction method aims at cutting down the computational effort by reducing the number of examples which have to be considered during the learning process. 
　　in the following sections some techniques which have been proposed in the literature are briefly outlined. they can be grouped in two different approaches: reduction using evaluation criteria and reduction using compression techniques. finally  we propose a new method which performs a data reduction by means of an innovative evaluation criterion. 
1 	using evaluation criteria 
　　given a set of examples e  the reduction process of e consists of finding a new set e' that satisfies the condition: 
		 1  
the reduced set e' is obtained by selecting a subset of elements which belongs to e  1 . the elements which belong to the set e' are chosen using an evaluation criterion: an example which belongs to e is an element of the set e' if the evaluation criterion suggests it. the aim of the evaluation criterion is to select the best examples to solve the learning problem. a possible definition for the evaluation criterion  proposed by michalski in  michalski and larson  1  cramm  1   is aimed at selecting the best examples to accomplish the discrimination task: it gives an estimate of the distance between two examples  i.e. it determines how similar two different examples are. the distance is measured between an example belonging to the set e and a fixed example used as a reference point. then  the examples of the new data set are chosen following a strategy which selects the examples that represent border line concepts. 
　　the main advantage of the evaluation criterion approach is that a simple evaluation criterion can be defined which ensures a high level of efficiency. on the other hand  the definition of the evaluation criterion is a difficult problem  because in the new data set some examples are dropped and only a good evaluation criterion preserves the effectiveness of the knowledge acquired during the subsequent learning process. further  the evaluation criteria proposed in the literature  such as the distance measure proposed in  michalski and larson  1   are suitable for examples expressed in an attribute-value language  but are not applicable to examples expressed in a 
　　first order language. 
　　in figl.a the typical behavior of an evaluation criterion based on a data reduction technique is reported. 
1 	using compression techniques 
　　given a set of examples e  the reduction process of e consists of finding a new set e' that satisfies the following conditions: 
 1  
 1  
where l  means more general than. 
　　the cardinality condition imposed by  1  is obtained by creating a set of new examples which generalize the original ones  1 . a new example e'e e' is the most specific generalization of a small set of examples belonging to e. in others words  each subset of e containing examples which are very similar can be compressed  generalized  into a new example. there are various ways of performing the compression  which arc more or less complex  depending on the generalization rules involved. the simplest form of data compression introduces the disjunction connective in the data representation language. in this way  data compression is performed by grouping the similar examples into disjunctive descriptions |frediani and saitta  1 . an interesting form of data compression involves the dropping condition rule. with this method the data are analyzed in order to identify the features which are irrelevant to the solution of the learning problem in the specific application domain. after that  the data compression is performed by dropping the irrelevant features from the examples. some experiments in this direction are reported in  pollack  1 . finally  the more complex form of data compression requires a new representation language in which more general descriptions can be expressed  frediani and saitta  1 . in this way  data compression is performed by rewriting the input examples in terms of the new language. this is a difficult form of compression 

because it involves a constructive learning mechanism. 
1 	machine learning 

　　the main advantage of the compression approach is that no information is lost during the compression process. in fact  the new examples generalize the original ones and so any concept description which generalizes the new examples will also generalize the original ones. 
　　the main problem is that the new examples generated by the compression process must not be overgeneralizations of the original ones. in others words any new example created by the compression of a set of original examples  instances of the class hi  must preserve consistency  i.e. it must not be a generalization for some example of another  disjoint  class. in the general case such a process turns out to be in its turn a learning activity  with a computational complexity which leads to no significant gain in efficiency in the global learning process  data reduction + inductive learning . 
　　in fig 1 .b the behavior of a compression based data reduction technique is reported. 
1 using concept characterization as evaluation criterion 
　　our proposal of data reduction follows the evaluation criterion approach. we argue that performing data reduction with an evaluation criterion approach is more suitable in real domains  where large data sets are to be managed. in fact  a good  and simple  evaluation criterion may dramatically reduce the number of examples to be used in the more time consuming phases of the inductive process  without itself being a complex operation. on the contrary  compression techniques may have the same complexity as the learning process  especially when a first order representation language is used. 
　　the crucial point in this approach is the definition of the evaluation criterion. let's build up a conceptual discrimination framework where there is a set h = { h 1   ... hn  of conceptual classes to be learned and for each class a set e hi  of examples of the class. given a class hj  e hj  is the set of positive examples of hj while 
ce h i   = uj= i e hj  is the set of negative examples or counter-examples. the evaluation criterion must establish which subset of examples is to be used and in which phase of the learning process they must be used. the central idea is to use  an approximation of  the characterization of each class hi as the criterion to select the examples and counterexamples which are to be used during the discrimination phase. the perfect characterization Φ of a class is a description that states all the facts and relations that are true of all examples in the class. an approximation Φ' of the characterization is a generalization of Φ so that ext Φ'  1 ext Φ   where ext Φ is the extension of the description ♀  the set of all the examples described by q. 
　　given a class hj  the proposed evaluation criterion to reduce the input data necessary to discriminate the class hi from each other class hj  j not= i  can be split in two parts. 
for positive 	examples: 
1. use all the positive examples to compute an approximation  p' hj  of the characterization Φ hi ; 
1. use the subset e' hj  = e hj  n ext Φ hi   during the discrimination phase. 
for negative examples: 
1. use no negative examples during the characterization; 
1. use the subset ce' hi  = ce hi  n ext Φ' hi   during the discrimination phase. 
the criterion for positive examples is aimed at removing the noisy examples from the set e hj  of positive examples of the class hj; in fact if an example does not belong to ext Φ' hi   because does not make allowance for some fact or relation common to most of the examples of the class  then it can be considered as a spurious example. the criterion for negative examples is the formalization of the intuitive idea that some counterexamples are more useful than others when learning discriminant descriptions. this idea was first introduced by winston with the near miss concept  winston  1 . 
near misses are the most useful counter-examples with which to learn a discriminant description of a class  because they avoid overgeneralizations and ensure that the description is specified by the facts and relations which are necessary to discriminate the class. winston defined a near miss as  a sample which does not qualify as an instance of the class being taught for some small number of reasons   winston  1  p.1 . such a definition is not operational  because the description of the class being taught is not known in advance and so we do not know how to select the near misses. we present an operational version of the above definition as follows:  a near miss is a counter-example which belongs to the extension of an approximation of the characterization of the class being taught . the operationalization consists in using the approximation of the characterization instead of the  unknown  class definition. the counter-examples which do not qualify as an instance of the class for  some small number of reasons  are those which are covered1 by the approximation of the characterization. they are selected as near-misses: in fact they are negative examples and are quite similar to most of the positive examples of the class. by adopting this technique of selecting the near misses we can consider the characterization to be a process which also performs a rough discrimination and so provides the discrimination process with an initial hypothesis and selects the useful counter-examples. this initial hypothesis is then specified to make it consistent with regards to near misses  and is simplified in order to reduce its complexity  by means of the incremental learning discrimination process. 
an example e is covered by a description ♀ if ee ext q. 
	gemello and mana 	1 


fig 1 - data reduction can be performed following two different approaches: using an evaluation criterion or performing data compression. in the first approach  a  the data reduction is achieved by means of the selection of the best concept examples  circled + . in the second approach  b  the reduced data set is composed of new examples  shaded shapes   which are generalizations of the old ones. in the picture  c  a specific evaluation criterion is presented to generate the best data for the concept discrimination: for each class h- a concept approximation Φ' is used to select the concept near misses. 

　　the main advantage of this data reduction proposal is that there is no loss of information. in fact no example is arbitrarily dropped but they are more efficiently used. a disadvantage is that  for very simple domains with well separated classes  the effort needed to learn the approximation of the characterization can be greater than the effort of the discrimination alone. as a consequence this method appears to be especially well suited for real domains with classes  which are difficult to separate. 
　　in fig 1 .c the desired behavior of the data reduction analysis using a characterization module as evaluation function is reported. 
1 integration of characterization and discrimination modules 
　　in the previous sections we have suggested a new technique which makes use of the characterization process as an evaluation criteria in order to reduce the input data for the discrimination process. from another point of view this technique can be considered to be a model for the integration of the characterization and discrimination modules of an inductive learning system aimed at solving more complex discrimination tasks. 
   the proposed learning scheme integrates characterization and discrimination modules with the aim of improving the efficiency of the learning process and the robustness of the learned knowledge. the input of the process for each conceptual class   is a set of examples of the class e hj  while the output for each conceptual class   is a discriminant rule which is a compromise between a purely sufficient condition and a necessary condition. the learning process is divided into three sequential steps. 
step 1 - characterization: the positive examples of 
hj are analyzed by the characterization module in order to find an approximation of the concept 
1
 means that if an unknown example is covered by e  the 
example is an instance of hi. 
1 	machine learning 
characterization. during this step learning is performed from only positive examples. learning the most specific characterization is a computationally very hard task  but  for our aims  it is necessary for this module only to find an approximation of the most specific description. 
 step 1 - discrimination: the concept approximation  is used to compute the concept's best positive and negative examples  near misses  and these new data sets arc the input of the discrimination module. further  the discrimination analysis uses concept approximation as an initial hypothesis from which to start the inductive 
drocess. the result of this step is a discriminant rule 
 this step performs incremental learning using a reasonable concept definition  as the initial hypothesis and the near misses of the concept as elements to specialize this hypothesis. 

   in this section we are going to discuss some experimental results in order to verify the soundness of the learning scheme which has been proposed. the experiments have been carried out in order to compare the computational performance of the learning system in two configurations: in the first  the traditional configuration  the discrimination module is used in isolation; in the second  the learning scheme proposed here  both characterization and discrimination modules are used in an integrated way  with the characterization being used as an evaluation criterion in order to reduce the number of examples for discrimination. the experiments are case studies of learning discriminant rules from examples. 
　　two different application domains are introduced and the time needed to discriminate each class is reported. the results described below have been obtained using an initial version of the proposed learning scheme which has been implemented on top of the rigel inductive learning tool  gcmello et al.  1   with a ti explorer hardware. the characterization module parameters have been tuned so as to create a very simple concept approximation consisting in a conjunction of numerically quantified formulae involving a single object  1 x n y x    where n can assume the following forms  = n      n      n  and  e  n..m j  and y is a first order formula . 
　　the first experiment concerns the trains domain  a well known artificial domain introduced by michalski  1j. in fig.1 the input examples are shown. the problem is to discriminate the trains going east from the trains going west. each train is described in a first order language. the domain features which have been chosen for the discrimination problem are the same as those proposed in . a carriage is characterized by its shape  number of wheels  length and number of loads while a load is characterized by its shape. two binary relations are used: the infront relation to describe the carriages sequence and the cont-load relation to specify which loads are contained into a carriage. 
　　using only the discrimination module  the learning strategy implements an inductive inference which takes into account all the examples and counter-examples. the system finds complete and consistent descriptions for both the classes and in particular: 
　　for the trains going east  the near misses set contains only the west train ev1 which has 1 or 1 carriages with one load and 1 or 1 loads with a triangle shape. for the trains going west the near misses are the trains ev1 and ev1. in fact  they arc the trains with exactly 1 long carriages  the engine is considered a long carriage . 
　　the second experiment is set in a 1d image recognition domain. the results which have been obtained are more interesting in this case because image recognition is in a real applicative domain where noise affects the images and a larger learning set is available. the goal consists in the discrimination of printed capital letters. the input data are generated from a set of 1d images of the letters  produced by a tv camera. the pixel map is analyzed by a low level module which describes the contour in terms of three primitives: angle  straight line and curve. for each primitive a set of features  which are independent of rotation and translation movements  are captured. a binary relation next is defined in order to specify the primitive sequence in the contour. the experiment has been carried out starting from 1 input examples distributed over 1 letters. in the table i the performance of the system in both the configurations are summarized. the reduction factor of the computational effort when the integrated approach is used is  in this domain = 1. 
	gemello and mana 	1 

1 	conclusions 
　　inductive learning systems which use a first order representation language pay for their increased learning power in terms of computational complexity. there are two ways of controlling this complexity which are commonly used: heuristic methods are used to guide the inductive search and data reduction methods are adopted to reduce the number of examples which have to be considered. 
　　this paper has focused attention on data reduction methods and has proposed a new technique which exploits the results of a preliminary characterization analysis so that the number of examples and counter-examples to be taken into account during the discrimination analysis can be reduced. from a theoretical point of view it has been shown that the method docs not suffer from the danger of loss of information  as many data reduction methods do. in fact there is not a preliminary trimming of the examples  but instead  thanks to the integration of the characterization and discrimination activities  they are used more rationally. from an experimental point of view  the method which has been proposed has proved to be more suitable in real domains than in simple artificial domains. in fact:  1  it is not guaranteed that the simplest discriminant description will be found but that a discriminant solution with some characterizing elements which is more robust to misclassifications will be provided. this is not of interest in toy domains  but it is essential in real applications;  1  if the data set is small  the characterization complexity may offset the gain in efficiency which is obtained during the 
1 	machine learning 
discrimination phase  so that there is no improvement in efficiency. 
   this has been tested in two domains and while in the artificial trains domain the method proposed worked well  providing a modest gain  1   there was a significantly more relevant gain  1  when it was used in the real domain of 1d image recognition . 
acknowledgements 
   the authors would like to thank lorenza saitta for helpful comments on drafts of this paper. 
