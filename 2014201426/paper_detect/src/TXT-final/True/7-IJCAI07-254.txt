
when data is scarce or the alphabet is large  smoothing the probability estimates becomes inescapable when estimating n-gram models. in this paper we propose a method that implements a form of smoothing by exploiting similarity information of the alphabet elements. the idea is to view the log-conditional probability function as a smooth function defined over the similarity graph. the algorithm that we propose uses the eigenvectors of the similarity graph as the basis of the expansion of the log conditional probability function whose coefficients are found by solving a regularized logistic regression problem. the experimental results demonstrate the superiority of the method when the similarity graph contains relevant information  whilst the method still remains competitive with state-of-the-art smoothing methods even in the lack of such information.
1 introduction
statistical language models  slm  attempt to capture the regularities of a natural language using statistical methods to construct estimates of distributions of various linguistic units  such as morphemes words  phrases  sentences  or documents. because of the categorical nature and the large vocabulary of natural languages  statistical techniques must estimate a large number of parameters  and thus depend critically on the availability of a large amount of training data. given the complexity of languages  such training data is rarely available. in the lack of a sufficiently large and rich dataset  simple approaches like straightforward maximum likelihood  ml  estimation tend to produce poor quality models.
¡¡smoothing in this context is typically used to refer to techniques that combine various ml probability estimates to produce more accurate models. an enormous number of techniques have been proposed for the smoothing of n-gram models  including discounting  recursively backing off to lower order n-grams  linearly interpolating n-grams of different orders. an excellent survey of these and some other techniques can be found in  chen and goodman  1 .
¡¡clustering models make use of smoothing ideas  but also attempt to make use of similarities between words. in the class-based n-gram estimation  brown et al.  1   a greedy algorithm searches the space of clustering to find one that increases the data likelihood the most. the method is extended to handle trigram by  ueberla  1   where a heuristic randomization was used to speed-up the clustering process with only a slight loss in performance.
¡¡these early works rely on short span relationships  such as bigram or trigram  and therefore tend to produce syntactically-oriented clusters. another approach proposed by  bellegarda et al.  1  exploits large span relationships between words and results in clusters that are semantically oriented. the idea is to exploit that documents can be thought of as a semantically homogenous set of sentences. hence  if a set of documents is available then this knowledgesource could be exploited to create semantically meaningful word-clusters. the algorithm forms a word-document matrix given the training data  performs singular-value decomposition  svd  on the resulting matrix  and then clusters over the projections according to a well-chosen measure.
¡¡clustering is a special case of constructing similarity information over the alphabet elements  most often over the words or word-forms of a language .  dagan et al.  1  proposed to use a kernel-based smoothing technique where the kernel function is built using similarity information derived from word-cooccurrence distributions. they have shown up to 1% reduction in perplexity for unseen bigrams as compared to standard back-off schemes.  toutanova et al.  1  investigated a markov chain model that exploits a priori similarity information  whose stationary distribution was used for solving prepositional phrase attachment problems.
¡¡our contribution in this paper we propose a method that implements a form of smoothing by exploiting similarity information of the alphabet elements. the idea is to view the log-conditional probability function as a smooth function defined over the similarity graph. the algorithm that we propose uses the eigenvectors of the similarity graph as the basis of the expansion of the log conditional probability function. the coefficients of the expansion are found by solving a regularized logistic regression problem. the experimental results demonstrate the superiority of the method when the similarity graph contains relevant information  whilst the method still remains competitive with state-of-the-art methods even in the lack of such information.
1 sequence prediction based on similarity information
consider a word prediction task over some vocabulary v. let p w|h  denote the conditional probability of word w ¡Ê v  where h ¡Ê v  is some history. if we constrain the size of histories to n 1 words we get the well known n-gram model. for the sake of simplicity in this paper we will only deal with bigrams  the methods proposed can be readily extended to higher order n-grams . in what follows the notation p y|x  will be used to denote the bigram probabilities.
¡¡given appropriate basis functions  { i}  the logprobabilities of the bigrams can be written in the form
	.	 1 
clearly  the simplest choice is to let i = v¡Áv and use the socalled euclidean basis functions   i x y  =   i1 i1  x y  = ¦Ä i1 x ¦Ä i1 y   where ¦Ä ¡¤ ¡¤  is the kronecker's delta function. with this basis function set  fitting this model to some dataset is equivalent to maximum likelihood training. the main idea in the paper is that given some similarity information over v it might be possible to construct another basis function set that facilitates generalization to unseen cases. since the basis functions that we will construct will form an overcomplete representation  we will resort to a form of regularization to prevent overfitting.
1 incorporating similarity information
let v = {w1 ... w|v|} be the words in the vocabulary. in order to estimate the p y|x  conditional probability distribution  where  x y  ¡Ê w  we used the similarity information hidden in the context. assume that the similarity information between words is represented as an undirected weighted graph g =  v e w   where e   v ¡Á v are the edges and assigns non-negative weights to wordpairs. for mathematical convenience  we use w to denote the |v| ¡Á |v| weight matrix  where the  i j th entry of w is the weight of the edge i ¡ú j. the idea is to construct basis functions that respect the similarity information in w. one way to accomplish this is to use spectral graph clustering which is known to yield basis functions that can be used to represent any square integrable function over g  chung  1 . in fact  spectral graph clustering methods construct basis functions that are natural for constructing geodesically smooth global approximations of the target function. in other words  smoothness respects the edges of the graph. in our case this means that similar words  or word-pairs  will be assigned similar probabilities. it is left for future work to determine if the results presented here can be improved by using other techniques  such as diffusion wavelets by  coifman and maggioni  1 .
¡¡in the particular method we have chosen  the basis functions are computed using the singular value decomposition of the matrix. here d is the diagonalvalency matrix; its diagonal elements are defined by.
this operator is spectrally similar to the normalized graph
laplacian operator  . in fact  elementary algebra yields that. the spectrum of the graph laplacian is known to have a close relationship to global properties of the graph  such as  dimension   bottlenecks  clusters  mixing times of random walks  etc. the spectral decomposition method employed is motivated as follows: the smoothness of a function  f : v ¡ú
r on the graph g can be measured by the sobolev-norm
.
the first term here controls the size of the function  whilst the second controls the gradient. using this norm  the objective to exploit similarity information can be expressed as the desire to find a loglikelihood function whose g-norm is small. now  the projection of a function f on the linear function space spanned by the top k eigenvectors of the laplacian is the smoothest approximation to f with k basis functions  in the sense of the g-norm. hence  influenced by  ding et al.  1  we decomposed p using singular value decomposition: p = usv t. for practical purposes  we truncated the svd of p in such a way that the
  where  is the frobenius norm  and  is the truncated version of p where only the k column vectors of u and k row vectors of v corresponding to the k largest singular values of s are kept. for proper normalization  the singular vectors in uk are multiplied by the square root of the respective singular values  dhillon  1;
ding et al..
¡¡now  the basis functions are obtained using the columns of uk: denoting these columns by ¦×1 ... ¦×k    i1 i1  x y  = ¦Ä i1 y ¦×i1 x   where i1 x y ¡Ê v  i1 ¡Ê {1 ... k}. since the similarity information might be unreliable we added
the euclidean basis functions  ¦Ä i1 x ¦Ä i1 y   to the set obtained. this way even when the automatically constructed basis functions are useless  the method still has a chance to recover. to handle unknown words  one may resort to nystro¡§m's method  e.g.  baker  1  : in order to extend a singular vector ¦×i to a new word  z  it suffices to know wxz for all x ¡Ê v in the set of known words. in fact  can be shown to be a good approximation.
¡¡in the experiments below we always used a single similarity graph  though the algorithm has the natural ability to use more graphs by simply merging the set of resulting basis functions. just like in regression  we may use all the basis functions  as well as all the products of two basis functions:
		 1 
in fact  this is a second order anova model  lin  1 . higher order models  products of more than two basis functions  may also be used. in practice  however  they are rarely used since second order models typically produce sufficiently good results. when more than one model sets are combined  the number of parameters increases rapidly. hence  the use of a regularized fitting method becomes of high importance.
1 solving the regression problem
since by assumption all of our basis functions depend only through the kronecker delta on y  equation  1  when raised to the exponent takes the form:
  where
is the normalizing factor   is the matrix of the weight parameters to be learned   ¦Áy contains the weight parameters of word y  and ¦Â x  is the vector formed from the basis functions evaluated at x. we shall call ¦Â x  the feature vector associated with x.
¡¡let the training data be the sequence d =  v1 ... vn   vi ¡Ê v. assuming that the data is generated by a first order markov model where the transition probabilities can be modeled by a multinomial model  1   the data log-likelihood takes the following form:
 1 
+ const
the maximum likelihood  ml  estimate of the parameter vector is the vector that maximizes this function. in order to prevent overtraining it is advisable to prefer smooth solutions  especially if the number of basis functions is big  in other words  if the feature vectors  ¦Â  are high dimensional . one way to enforce smoothness is to introduce a prior distribution over the parameters ¦Áij. several choices are possible for such priors. in this work we studied the behaviour of the method when it was used either with the laplacianprior  p ¦Áij  ¡Ø ¦Ëij exp  ¦Ëij |¦Áij|   or with the gaussianprior .
¡¡assuming one of the above priors  the training problem becomes the maximization of the following objective function:
¡¡  1  i.e. an	p penalty term is added to the ml objective function. here p = 1 or 1 depending on which prior one wishes to use: p = 1 corresponds to the laplacian prior  whilst p = 1 corresponds to the gaussian prior.
¡¡we used the smlr java implementation of  krishnapuram et al.  1  which implemented both the laplacian  l1  and gaussian priors  l1   to find the solution for equation  1 . the sketch of the algorithm is shown on algorithm 1.
1 experimental results
we compared the performanceof similarity based smoothing  sbs  on syntheticand real-world data to both plain bigramestimates  bi  and interpolated kneser-ney smoothing  ikn .
algorithm 1 similarity based smoothing  sbs 

input: similarity information between words  training data output: estimated probability distribution
1. build a similarity graph between words g =  v e w 
1. calculate the normalizedmatrix of
1. determine the svd decomposition of p = usv t
1. calculate the mapping operator from the singular vectorsof top singular values: 
1. train the logistic regression model weight parametersusing the training data

ikn is considered as one of the best smoothing techniques  goodman  1 .
¡¡in order to evaluate the methods we calculated the empirical cross-entropy of the trained model on held-out data  w1 ... wn:
	.	 1 
here p  wi|wi 1  is the probability assigned to the transition from wi 1 to wi by the model and p denotes the true distribution.  for the synthetic datasets we have wi ¡« p ¡¤|wi 1 .  since by assumption  the held-out data has the same distribution as the training data  by the shannon-mcmillan-breiman theorem we know that  under mild assumptions  hn is close to the cross-entropy of p  with respect to the true distribution p of the data. the cross-entropy of p  and p is the sum of the entropy of p and the kullback-leibler divergence of p and p  a non-negative quantity. the smaller the cross-entropy  the closer the estimated model is to the true model.
¡¡for each test  we calculated the cross entropies for the ml-trained bigram  ikn and the proposed similarity based smoothing technique. in the case of synthetic datasets we also estimated the entropy of the models by using the true transition probabilities in  1 . the corresponding points in the graphs are labeled as 'model'.
1 tests on synthetic data
the model generating the synthetic data in order to develop a good understanding of the possible advantages and limitations of the proposed method  we tested it in a controlled environment where data was generated using some designated bigram model. in order to make the test interesting  we decided to use  clustered  markov models of the following form: the probability of observing y given x is computed by
         p y|x  = p y |c y  p c y |c x     1  where c x  is the cluster of symbol  word  x  c : v ¡ú c with |c|   |v| . the idea is that the next observation depends on the past observation x only through its class  c x  and hence two past observations  and yield to identical future distributions whenever. note that when p y|c  = ¦Ä c y  c  then the model can be thought of as a hidden markov model  hmm  with state transitions defined over c  whilst when the observation probabilities are unconstrained then in general no hmm with |c| states is equivalent to this model. in any way  the model can be specified by two matrices   a b   with ac1 c1 = p c1|c1   a ¡Ê r|c|¡Á|c|  and by c = p y|c   b ¡Ê r|v|¡Á|c| . let us now describe how these matrices were generated.
¡¡for computing the matrix a  we start with a permutation matrix of size |c| ¡Á |c| and perturb it with random noise so that  i  all transition probabilities are nonzero and  ii  for each state the number of  significant  transitions lies between 1 and.
¡¡for computing b  we start from the idealized blockstructured matrix with  and then perturb it according to
 
where the elements ofare independent random variables uniformly distributed in   1 1 . if ¦Ä = 1 then the block structure of the source is clearly identifiable based on the outputs: given an observation y and knowing c we can infer with probability one that the hidden state is c y  and as noted before the model collapses to a hidden markov model with c states. when ¦Ä is non-zero this structure is blurred. in the experiments we used ¦Ä = 1 whilst we let ¦Ã change in  1 . note that ¦Ã = 1 roughly corresponds to a noise level of 1% in the observations.
similarity information provided for sbs we controlled how well the similarity information provided for sbs reflects the actual block structure of the data. the perfect similarity information assigns 1 to observations  words  in different clusters  whilst it assigns the same positive value  say 1  to observations in the same cluster. the corresponding matrix is denoted by s  s ¡Ê r|v|¡Á|v| : sxy = ¦Ä c x  c y  .
¡¡we then disturbed s by adding noise to it. for this a random |v|¡Á|v| matrix  z  was created whose entries for independent  uniformly distributed random variables taking values in  1 . given a noise parameter   the perturbed matrix is computed by
.
denotes the component wise product of matrices u and v   1 denotes the matrix with all of its entries being 1. in words  increasing  reduces the inter-cluster similarity and increases between-cluster similarity. in the extreme case when all similarity information is lost.
training and test datasets a training dataset normally contains ntr = 1 observation sequences  except when we experiment by changing this parameter   each having a random length that was generated by drawing a random number l from a normaldistribution with mean 1 and variance 1 and then setting the length to max 1 l . the test dataset  separate from the training dataset  had 1 sequences which was generated using the same procedure. all measurements were repeated 1 times and the average values are presented.
1 results
we performed a sensitivity analysis to test the effect of how the various parameters influence the results. in particular  we studied the performance of sbs as a function of the observation noise  ¦Ã  that masks the identifiability of the block structure  the noise in the similarity matrix    that gradually decreases the quality of the similarity information available for sbs  the number of training sentences  ntr  and the cluster structure. these parameters were changed one by one  whilst the others are kept at their default values which were ¦Ã = 1 
. the default cluster structure was to have 1 clusters  having observations 1 1 1 and 1  respectively  so that some clusters were bigger  some were smaller . thus |v|  was kept at 1. we present the results for both the laplacian and gaussian priors  the corresponding results are labeled as 'sbs gauss' and 'sbs lapl' in the graphs.
sensitivity to noise masking the block-structure when ¦Ã = 1  the observations can be used directly to infer the underlying classes and the estimation problem is easier. when the block-structure masking noise is increased the problem becomes harder.

figure 1: the cross-entropy of models built with various algorithms as a function of the block-structure masking noise parameter  ¦Ã 
¡¡figure 1 shows the results of the measurements. it can be observed that the proposed method performs significantly better over the considered spectrum of ¦Ã than its peers: in fact  as the masking noise gets bigger  ikn's performance converges to that of the bigram model. on the other hand  sbs was able to maintain its estimation accuracy for the whole range of ¦Ã. in all of these experiments sbs with the gaussian parameter-prior performed slightly better than sbs with the laplace prior. we believe that this is because the laplacian prior prefers sparse solutions and the number of basis functions is not high enough for making sparse solutions preferable.
robustness against the degradation of the similarity information in the next set of experiments we investigated the sensitivity of the method to the quality of the similarity information. for this we gradually increased the similarityinformation noise parameter    from 1 to 1. as we have discussed  when the similarity information is perfect  whilst when all similarity information is lost.

figure 1: the cross-entropy of models built with various algorithms as a function of the noise parameter    governing the quality of the similarity information
¡¡as expected  when the similarity information gets weaker  the performance of sbs degrades and converges to that of the ml-estimated bigram model. it is notable that even when
  when the similarity information is already verypoor  sbs performs as well as ikn. the reason is that although in these experiments we did not add the euclidean basis functions to the set of basis functions  we can expect the spectrum of a high-noise similar matrix to be uniform  hence covering 1% of the spectrum will add almost all singular vectors to the basis and thus the model automatically retains its power.
sparsity of training data the main promise of sbs is that by using the similarity information it is capable of achieving better performance even when the available training data is limited. in order to validate this  we performed a set of experiments when the number of sentences in the training data was varied. the results are shown in figure 1. both with the gaussian and the laplacian priors  sbs outperformed ikn for a wide range of values of ntr. it is interesting to note that the gaussian prior performed much better than any of the other methods when the number of sentences was very small.
clusterstructure we tested sbs with large variety of cluster setups  ranging from 1 to 1 clusters  and with vocabulary sizes between 1 and 1. the results are summarized on table 1.
¡¡it is clear that if the number of clusters is small or when all the words are in different clusters  then there is no significant structural information in the similarity. it is notable that in such cases sbs was still able to perform as well as ikn. on the other hand  if there is a significant hidden structure in the model  sbs greatly outperforms ikn.

figure 1: the cross-entropy of models built with various algorithms as a function of the number of sentences in the training data  ntr .
clustersh p p ikn    h p p sbs  average stddev1 1 1 1-111.1.1x1.1.1x1.1.1x1.1.1 1.1.1 1111 1111 1.1.1 1 1111 1x1x1.1.1 1 1 111table 1: the cross-entropydecrease of sbs as comparedwith
ikn for a number of different cluster structures
1 tests on real data
the task considered is to predict pos sequences on the wall street journal  wsj  corpus. we extracted a weighted directed sub graph based on all senses in all syntactic categories from wordnet  fellbaum  1  for 1 words. based on this information  a pos similarity graph  pg  was built as follows: using the brown corpus1  we first collected the possible pos categories for every word. next we created a prior pos-tag distribution for all the words. then for every unique word in the brown corpus  we added the out-links to the pos graph according to the wordnet graph  using the weight of the link in the wordnet graph and the weight of the pos tags in both sides of the link. for example  if we arrive at the word live and if the link live ¡ú bouncy in the wordnet graph has weight w then for all possible pos tag combinations of live and bouncy  the link between the pos-pairs  x y  is increased by p x|live  ¡¤ p y|bouncy  ¡¤ w. in the next step  we calculated the basis functions as outlined beforehand.
¡¡in another set of experiments a similarity graph was built by connecting with a constant weight those pos-tags that shared the same prefix  e.g. nn  nnp  nns were grouped together .
¡¡the training and test corpora were kept separate  with the training corpus having 1 sentences  1 pos-tags   whilst the test corpus consisted of 1 sentences. the vocabulary size was 1.
¡¡when the similarity graph extracted from wordnet was used  sbs failed to achieve a good performance. investigating the similarity graph  it turned out that it was very sparse  and it actually used 1 pos only out of the 1 pos tags. with the prefix-based similarity graph  the algorithm's performancebecamepractically indistinguishableto that of ikn. we guess that a more principled similarity graph building method would be needed in order to achieve a performance improvements in this task. sbs in theory is better suited to problems with larger alphabets. unfortunately  smlr  the software tool used for training sbs scales rather poorly with the alphabet size  seriously limiting the range of feasible experiments.
¡¡concentrating on the robustness of the estimate  we computed the perplexity reduction on those bigrams that occur only at most a few  say 1  times in the training corpus. it was found that the perplexity reduction of sbs compared to ikn is a mild 1%  whilst it is 1% as compared to the ml-trained bigram model. we also found if we have less sentences the sbs performs even a little better.
1 conclusion
we have proposed a novel smoothing method to train n-gram models: the method constructs basis functions in an automated way for a maximum-entropy model by the spectral decomposition of a weighted similarity graph. it was found that the method is able to outperform some competitors when the similarity information is substantially relevant  whilst it remained competitive even when the similarity informationwas not helpful.
¡¡the motivation of the method is the well-known fact that in continuous domains smoothness of the target function  or density  yields faster convergence. how is it possible to achieve similar results in discrete domains  the first problem is to define an appropriatenotion of smoothness. spectral graph theory suggests that such a notion can be defined based on the laplacian of a similarity graph and that the eigendecomposition of a normalized laplacian  and other related operators  yields an appropriate basis for functions that are  smooth   i.e. which respect the topology  geodesics  in the graph. to our best knowledge  this is the first work that suggests the use of automatically constructed basis functions  or features  in probabilistic sequence modeling in discrete domains. although here we only considered the simplest case  i.e.  constructing bigram models  the idea is that the underlying method can be generalized to other more complex problems  ranging from n-grams to even whole-sentencelanguage models. for such models the similarity information must be constructed over extended sequences. one possibility to accomplish this might be to use appropriately constructed string kernels. however  this investigation is left for future work.
