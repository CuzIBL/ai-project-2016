 
a parallel distributed computational model for reasoning and learning is discussed based on a belief network paradigm. issues like reasoning and learning for the proposed model are discussed. comparisons between our method and other methods are also given. 
1 	introduction 
comparing with the model of connectionist expert systems  ces   gallant  feb 1   belief networks  bn   lauritzen and spiegelhalter  1; pearl  1  has quite a few advantages  eg.  1  it has the probability theory as its theoretical basis and thus guarantees a consistent or correct result;  1  it allows reasoning in all directions  other methods like ces only allow single reasoning direction  eg. bottom-up  gallant  feb 1 ; and  1  it is easy to express higher level probabilistic  non-functional  domain knowledge in bn while it is difficult in ces. 
   on the other hand  however  ces model also has its attractive features  eg.   1  it is easy to implement parallel distributed ces; and  1  it is possible to automatically generate ces by a set of learning techniques. these make ces more and more popular in recent years. there were discussions about some vital issues for belief networks and some solutions were also proposed. 
  lauritzen and spiegelhalter  proposed a bn decomposition method to reduce the computational amount when dependencies exist among the variables. this method provides a potential way to make use of the parallelism inherent in b n . 
  pearl  proposed a parallel model for his bayesian network which is elegant and efficient but difficult to use in multi-connected networks. a method for learning causal tree structure is also discussed in pearl's book. 
  herskovits and cooper  used the maximum entropy   m e   / m i n i m u m cross entropy  mce  methods to learn conditional probabilities to construct b n . these methods assign uniform distributions to the unseen cases in the training set. 
  laskey  proposed a method to construct bayes networks using boltzmann machines  rumelhart et 
1 	qualitative reasoning 
al.  1 . while this method seems promising  it is not quite clear for this method how to handle the np-hardness of connectionist learning  judd  1 . 
based on  lauritzen and spiegelhalter  1; pearl  1   we proposed  wen  nov 1  a highly parallel distributed computational model for bn called boltzmannjeffrey machine network  bjmnet . in this paper  we will explore further the capabilities of bjmnet and propose a set of efficient methods of bn learning from statistical relational databases  srdb . a performance analysis and comparison between our method and other methods are also given. 
   we only consider the case of binary variables in this paper  but our results can be generalised to the general case containing arbitrary variables straightforwardly. 




is any belief change in the interfaces it will update its own distribution using  1   and then propagate the belief changes to its other rip and ris's. the new changes  in turn  may cause some other bjms to update their distributions. the network reaches an equilibrium when the gradients of all the original disturbing points are in some small ranges specified beforehand. the network can be reconfigured dynamically using some techniques such as microprogramming to fit it for different applications. 
¡¡based on the mce principle and the theory of markov fields we have proven  wen  july 1  
theorem 1: for an acyclic bjmnet  the following updating procedures are equivalent: 
1. global updating the whole network with single marginal constraint set by  1 . 
1. local updating one sub-bjm which contains the constraints and jeffrey propagation of the result to the whole bjmnet through the running intersections. 
example 1  cooper  1 : metastatic cancer  a  is a possible cause of a brain tumor  c  and is also an explanation for increased total serum calcium  b . in turn  either of these could explain a patient falling into a coma  d . severe headache  e  is also possibly associated with a brain tumor. 
the bn and bjmnet for this example are shown in fig. 1  where the arrows in fig. 1b show the rip/ris relationship between bjms. the bjm without decom-
figure 1: belief network and bjmnet for example 1 
position has 1 binary variables and thus 1 units. after decomposition using the techniques in  lauritzen and spiegelhalter  1; wen  1b   the bjm is decomposed into 1 sub-bjms: {a  b  c}  {b  c  d}  and {c  e}. only 1 + 1 + 1 = 1 units are needed here. further saving can be achieved considering that the units in the intersections  the black nodes in fig. 1b  are never active simultaneously  1 units are needed here . 
suppose we have an initial distribution in which 
1 	qualitative reasoning 

the above learning procedure may be trivial if the database is complete. however  we often cannot obtain a completely specified database in many practical cases. in these cases  we may have to choose one of the following methods to generalize the incomplete statistical information to a complete conditional distribution. 
	1 	f l o o r m e t h o d a n d m e / m c e m e t h o d 
floor method assigns a small nonzero probability to the states  tuples  which do not occur in the database. this adds the possibility of jeffrey updating to these states. however  this method makes no distinction between the cases which are impossible and those which could be encountered in the future. 
similarly  m e / m c e method or dirichlet distribution 
 herskovitz and cooper  1  simply assigns a uniform conditional distribution to the unseen cases. it uses formula  to obtain 
a conditional distribution  where x is a variable in the underlying b n   x is one of the vx values can be taken by x y iix is the set of parents of x  is a particular instantiation of iix  and is the number of tuples in the database that match the instantiated set of variables . for the unseen cases  the above conditional distribution becomes  and thus is uniform. 
	1 	c o n n e c t i o n i s t 	m e t h o d s 
the connectionist learning methods are interesting because of their generalisation ability. it has been proven that the output class probabilities of multilayer perceptron   m l p   and boltsmann machine are good approximations of the conditional probabilities needed by bn  bourlard and wellekens  1; laskey  1 . in example 1  using an m l p network with 1 input units  1 hidden units  and 1 output unit  we can learn p d b  c  in less than 1 epochs. the results are given in fig. 1. for 
example  in fig. 1a  data for p d b  c  are missing. after the network has learned the weights from the incomplete data we feed b = c = 1 to the network and get an output p d b c  = 1. 
	1 	n n / o r 	m e t h o d 
   according to the nearest neighborhood  nn  principle  the conditional probability assigned to an unseen case for the database depends on its neighbor conditional distributions. that is  if the unseen case has many neighbors who have high conditional probabilities then it is assigned a relatively high conditional probability  otherwise  it is assigned a low or even sero conditional probability. this method should be used with the occam's rasor  or  principle when there are two or more nearest neighbors having different conditional probabilities. table 1: statistical information in sub-relations when there are more than one candidate hypotheses for 

a given problem the or principle chooses the simplest one which is compatible or at least 1% consistent with the observed data. there are many criteria of simplicity  eg. kolmogorov complexity  minimum description length  formula complexity  etc. we use formula complexity in the following examples. 
e x a m p l e 1: suppose we have the training data in fig. 
	1a with the functional dependency 	the 
ple  it is easy to determine p d b c  = 1  fig. 	1a . however  if the information about p d b  c  is missing 

	figure 1: 	learning p d b c  by or or m l p 
 fig. 1b   we may have two choices for it: 1 and 1. because the former has a probabilistic logic expression 
1 x  + 1 x b which is simpler than that of the 
1 	qualitative reasoning 
latter we prefer the first choice  see fig. 1b . similarly  we have the conditional probabilities learned for .  when the corresponding information missing  fig. 1c d . 
1 	performance and comparisons 
figure 1: performance of parallel jeffrey's updating 
   comparing with perceptron models  our model has its obvious advantages.  minsky and papert  1  showed that no single-layer perceptron can solve x o r problem. for c e s / m l p   using back propagation method  rumelhart et a/.  1   a total sum of squares  tss  1 is obtained after 1 epochs of training in a feedforward network with 1 input units  1 hidden units  and 1 output unit. the test result for all the patterns is given in table 1. from this example  it can be seen that for recall from completely specified distributions of sparse spaces  the b j m model outperforms c e s / m l p model in the following aspects:  1  no time-consuming learning procedure is involved.  1  only addition  multiplication  and division operations but not complicated calculations like sigmoid are needed.  1  the result is accurate.  1  the reasoning direction can be arbitrary.  1  the network structure is much more regular than c e s / m l p . the granularity of bjmnet is also good. 

¡¡for large scale applications  neither mlp nor bjm are probably approximately correctly  pac  learnable 
 judd  1 . however  in  wen  1a  we have proven 
theorem 1: if the maximum bjm in the bjmnet has a fixed size  the bjmnet is pac learnable from statistical relational data. 
it is possible to decompose an mlp network in a way similar to that of bjmnet. gallant's connectionist expert systems  gallant  feb 1  may be thought of as one step towards this direction. however  gallant's method can handle only functional dependencies but not probabilistic dependencies  and does not work properly for problems like example 1. 
1 	c o n c l u s i o n s 
a parallel distributed model for reasoning/learning is proposed based on bn paradigm. issues about the structure  learning methods  and the reasoning method for the model are discussed. comparison between bjmnet and ces shows that the soundness and efficiency of the model can be guaranteed for a wide range of applications: 
  the model has probability theory and information theory as its theoretical basis. 
  it can be implemented automatically by a set of learning methods from statistical relational data. 
  reasoning in this model can be in any directions  in contrast to the ces models which can only reason in single direction  gallant  feb 1 . 
  np-hardness in both reasoning and learning is han-dled by decomposing the bn into small bjms. 
a prototype of simulator of bjmnet has been developed for the encore computer  a shared memory multiprocessor system  wen  nov 1 . 
a c k n o w l e d g e m e n t 
thanks to e. a. sonenberg  b. marksjo  a. kowalczyk  
h. liu  and l. fang for discussions and comments. the permission of the executive general manager  trl  to publish this paper is gratefully acknowledged. 
r e f e r e n c e s 
 beeri et al  july 1  catriel beeri  ronald fagin  david maier  and mihalis yannakakis. on the desirability of acyclic database schemes. journal of the acm  1 :1  july 1. 
 bourlard and wellekens  1  h. bourlard and c. j. wellekens. links between markov models and multilayer perceptrons. ieee trans. pattern analysis and machine intelligence  to appear  1. 
 cooper  1  g. f. cooper. nestor: a computerbased medical diagnostic aid that integrates causal and probabilistic knowledge. report hpp-1  stanford university  1. 
 gallant  feb 1  s. i. gallant. connectionist expert systems. communications of the acm  1 :1  feb. 1. 
 herskovits and cooper  1  e. herskovits and g. f. cooper. kutato: an entropy-driven system from construction of probabilistic expert systems from databases. in m. henrion and bonissone  editors  proc. 1th international conf. on uncertainty in ai  cambridge  ma.  july 1. north holland. 
 jeffrey  1  r. jeffrey. contributions to the theory of inductive probability. ph. d. thesis  department of philosophy  princeton university  1. 
 judd  1  s. judd. learning in networks is hard. in proc. ieee first international conf. on neural networks  volume 1  pages 1  san diego  ca  jun 1. ieee press. 
 kullback  1  s. kullback. information theory and statistics. dover publication  inc.  new york  1. 
 laskey  1  k. b. laskey. adapting connectionist learning to bayes networks. international journal of approximate reasoning  1-1  1. 
 lauritzen and spiegelhalter  1  s. l. lauritzen and d. j. spiegelhalter. local computations with probabilities on graphical structures and their application to expert systems. j. r. statist. soc. b  1   1. 
 lemmer  1  j. f. lemmer. generalized bayesian updating of incompletely specified distributions. large scale systems  1  1. 
 minsky and papert  1  m. minsky and s. papert. percepetrons. mit press  1. 
 pearl  1  j. pearl. probabilistic reasoning in intelligeni systems: networks of plausible inference. mor-
gan kaufmann publishers inc.  san mateo  california  1. 
 rumelhart et al.  1  d. e. rumelhart  j. l. mcclelland  and p. d. p. group. parallel distributed processing  explorations in micro structure of cognition. mit press  cambridge ma & london  1. 
 ullman  1  j. d. ullman. principles of database systems. computer science press  rockville  maryland  1. 
 wen  1a  w. x. wen. learning from statistical relational data. tech. report  ai systems  telecom research labs.  1 blackburn rd. clayton  vic. 1  australia.  1. 
 wen  1b  w. x. wen. optimal decomposition of belief networks. in max herion  piero bonissone  j. f. kanal  and j. f. lemmer  editors  to appear in uncertainty in artificial intelligence 1. north holland  1. 
 wen  july 1  w. x. wen. markov and gibbs fields and mce reasoning in general belief networks. technical report 1  computer science  the university of melbourne  july 1. 
 wen  nov 1  w. x. wen. parallel mce reasoning in recursive causal networks  in proc. 1 ieee international conference on systems  man  and cybernetics  boston  ma  usa  nov. 1. 
	wen and jennings 	1 

	wen and jennings 	1 

	wen and jennings 	1 







	wen and jennings 	1 

	wen and jennings 	1 

	wen and jennings 	1 







	wen and jennings 	1 

	wen and jennings 	1 

	wen and jennings 	1 

