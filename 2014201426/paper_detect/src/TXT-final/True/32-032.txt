 
there has been a tradition of combining different knowledge sources in artificial intelligence research. we apply this methodology to word sense disambiguation  wsd   a long-standing problem in computational linguistics. we report on an implemented sense tagger which uses a machine readable dictionary to provide both a set of senses and associated forms of information on which to base disambiguation decisions. the system is based on an architecture which makes use of different sources of lexical knowledge in two ways and optimises their combination using a learning algorithm. tested accuracy of our approach on a general corpus exceeds 1%  demonstrating the viability of allword disambiguation as opposed to restricting oneself to a small sample. 
1 	introduction 
the methodology and evaluation of word sense disambiguation  wsd  as a distinct task are somewhat different from those of others in nlp  and one can distinguish three aspects of this difference  all of which come down to evaluation problems  as does so much in nlp these days. first  researchers are divided over using a general method  one that attempts to apply wsd to all the content words of texts  the large vocabulary approach taken in this paper  versus one that is applied to only a small trial selection of words  for example  schutze  1  and  yarowsky  1  . the latter researchers have obtained very high levels of success: yarowsky quotes 1% correct disambiguation for the small vocabulary over which his system operates  results close to the figures for other  solved  nlp tasks  such as part of speech taggers. the issue is whether these small word sample methods and techniques will transfer to general wsd over a more complete vocabulary. 
others  besides ourselves  for example  mahesh et al.  
1  and  harley and glennon  1   have pursued the general option on the grounds that it is the real task and should be tackled directly  even with rather lower success rates. the division between the approaches probably comes down to no more than the availability of gold standard text in sufficient quantities  which is more costly to obtain for wsd than other tasks. in this paper we describe a method we have used for obtaining more test material by transforming one resource into another  an advance we believe is unique and helpful in this impasse. 
　secondly  there have also been deeper problems about evaluation  which led sceptics like  kilgarriff  1  to question the whole wsd enterprise  because it is harder for subjects to assign one and only one sense to a word in context  and hence produce the test material itself  than to perform other nlp related tasks. one of the present authors has discussed kilgarriff's arguments elsewhere  wilks  1  and argued that they are not  in fact  as gloomy as he suggests. again  this is probably an area where there is an  expertise effect : some subjects can almost certainly make finer  more inter-subjective  sense distinctions than others in a reliable way  just as lexicographers do  jorgensen  1  felbaum et al.  1 . 
　but there is a third  quite different  source of unease about the evaluation base: everyone agrees that new senses appear in corpora that cannot be assigned to any existing dictionary sense  and this is an issue of novelty  not just one about the difficulty of discrimination. if that is the case  it tends to undermine the standard mark-up-model-and-test methodology of most recent empirical nlp  since it will not then be possible to mark up sense assignment in advance against a dictionary if new senses are present. we shall not tackle this difficult issue further here  but press on towards experiment. 
　one further issue must be mentioned  because it is unique to wsd as a task and at the core of our approach. unlike other well-known nlp modules  wsd seems to draw upon a number of apparently different information sources. all the following have been implemented as the basis of experimental wsd at various times: part of speech  semantic preferences  collocating items or classes  thesaural or subject areas  dictionary definitions  synonym lists  among others  including bilingual equivalents in parallel texts . these linguistic phenomena seem different  so how can they all be  separately or in combination  informational clues to a single phenomenon  wsd  this is a situation quite unlike syn-

tactic parsing or part of speech tagging: in the latter case  for example  one can write a cherry-style rule tagger or an hmm learning model  but there is no reason the believe these represent different types of information  rather than different ways of conceptualising and encoding it. that seems not to be the case  at first sight  with the many forms of information for wsd. 
1 the methodology of combining knowledge sources 
in our work we adopted the methodology first explicitly proposed for wsd by  mcroy  1   and more recently  ng and lee  1  and  wilks and stevenson  1b   namely that of bringing together a number of partial sources of information about a phenomenon and combining them in a principled manner. this is in the ai tradition of combining  weak  methods for strong results  usually ascribed to  newell  1   and used in the crl-nmsu lexical work on the eighties  wilks et al  
1 . we shall present a system that combines three of the types of information listed above  together with part of speech filtering  and  more importantly  applies a learning algorithm to determine the optimal combination of such modules for a given word distribution; it being obvious  for example  that thesaurai methods work better for nouns than for verbs  and so on. 
　we shall use the machine readable version of the longman dictionary of contemporary english  ldoce   procter  1  for our experiments. this is a learners' dictionary  designed for students of english  which contains around 1 word types. ldoce was innovative in its use of a defining vocabulary of 1 word from which the textual definitions were written  if a learner of english could master this small core then  in theory  they could understand every entry in the dictionary. in ldoce  the senses for each word type are grouped into homographs  sets of senses with related meanings. for example  one of the homographs of  bank  means roughly 'things piled up'  the different senses distinguishing exactly what is piled up. it should be noted that the granularity of sense distinctions at the ldoce homograph level  eg.  bank  as 'edge of river' or 'financial institution'  is comparable to the distinctions made by small-scale wsd algorithms  eg.  schutze  1  and  yarowsky  1  . 
　it seems that there is a difference in the way in which different lexical knowledge sources can be useful for wsd in different ways. in experiments with ldoce part of speech codes and the brill tagger  wilks and stevenson  1a  suggest that this source can be used to discriminate between senses  or homographs  which are possibly correct in context  and those which are very likely not to be. this source could then be used to remove  or filter  senses from the set of possibilities for ambiguous words. however  this strategy can only be used for knowledge sources which we have confidence in since  if the correct sense is removed from consideration  then the tagger can never correctly disambiguate that word. we propose a framework in which separate knowledge sources can be used for wsd either to remove senses which are very unlikely or to suggest senses which may be correct. the first type of module shall be dubbed as a filter and the second type will be partial taggers. 
1 a sense tagger 
we now go on to describe and evaluate a sense tagger implemented within this methodology. our sense tagger makes use of several modules which perform disambiguation  each being a filter or partial tagger. ldoce is used to provide a set of senses and as a knowledge base to provide information upon which disambiguation decisions can be made. the architecture of the system is represented in figure 1  we now go on to describe each component in detail. 
1 	preprocessing 
before the filters or partial taggers are applied the text is tokenised  lemmatised  split into sentences and part of speech tagged using the brill syntactic tagger  brill  1 . a named entity identifier is then run over the text to mark and categorise proper names. these preprocessing stages are carried out by modules from sheffield university's information extraction system  lasie  gaizauskas et oi.  1 . 
　our system disambiguates only the content words in the text  the part of speech tags assigned by brill's tagger are used to decide which are content words  and does not attempt to disambiguate any of the words identified as part of a named entity. 
1 	part of speech 
our first module makes use of part of speech tags. we take the part of speech tags assigned by the brill tagger and use a manually created mapping to translate these to the corresponding ldoce grammatical category. any senses which do not correspond to the category returned are removed from consideration. in practice the part of speech filtering is carried out at the same time as the lexical lookup phase and the senses whose grammatical category does not correspond to the tag assigned are never attached to the ambiguous word. this avoids attaching senses which will be immediately removed by the filter. there is also an option to turn off filtering so that all senses are attached regardless of the part of speech tag. 
　it could be reasonably argued that removing senses is a dangerous strategy since  if the part of speech tagger made an error  the correct sense could be removed from consideration. as a precaution against this we have designed our system so that if none of the dictionary senses for a given word agree with the part of speech tag then all are kept. there is also good evidence from  wilks and stevenson  1  that this approach works well despite part of speech tagging errors. 
	stevenson and wilks 	1 

1 	dictionary definitions 
a method was proposed by  lesk  1  for carrying out sense disambiguation which used an overlap count of content words in dictionary definitions as a measure of semantic closeness. in this way it is possible  at least in theory  to tag each word in a sentence with its sense from any dictionary which contains textual definitions for its senses. however  it was found that the computations which would be necessary to test every combination of senses  even for a sentence of modest length  was prohibitive. 
the approach was made practical by  cowie et al.  
1  who computed the overlap using the simulated annealing optimisation algorithm which eliminated the need to calculate all possible combinations of senses. an initial guess at the solution to a given problem is made and the algorithm gradually moves towards an optimal solution by generating permutations of the current solution  and evaluating which of these are improvements. as with all hill-climbing algorithms  there is the danger that the algorithm will converge on a locally optimal solution rather than the desired optimal global' solution. simulated annealing avoids this by introducing a stochastic random element dependent on the temperature of the system  when the temperature is high there is a high probability that the algorithm will choose a solution that is worse than the current solution  with the probability of this happening reducing when the temperature is low. the temperature is high when the process begins and is gradually reduced as the algorithm proceeds. this random element allows the search to jump away from local minima and find the true global solution. this approach correctly disambiguated 1% of words to the sense level  and 1% to the homograph level. 
　in the cowie et. al. implementation the optimisation was carried out over a simple count of definition words in common  however this meant that longer definitions were preferred  since they have more words which can contribute to the overlap  and short definitions or definitions by synonym were correspondingly penalised. we attempted to solve this problem by computing the overlap in a different way. instead of each word contributing one we normalised its contribution by the number of words in the definition it came from. the cowie et. al. implementation returned one sense for each ambiguous word in the sentence  without any indication of the system's confidence in its choice  but we adapted the system to return a set of suggested senses for each ambiguous word in the sentence. we found that our changes led to an improvement in the algorithm's effectiveness and 1% of senses are correctly disambiguated by this module. 
1 	selectional restrictions 
ldoce senses contain simple selectional restrictions for each content word in the dictionary. a set of 1 semantic classes are used  such as h = human  m = human male  p = plant  s = solid and so on. each word sense for a noun is given one of these semantic types; senses for adjectives list the type which they expect for the noun they modify; senses for adverbs the type they expect of their modifier and verbs list between one and three types  depending on their transitivity  which are the expected semantic types of the verb's subject  direct object and indirect object. grammatical links between verbs  adjectives and adverbs and the head noun of their arguments are identified using a specially constructed shallow syntactic analyser  stevenson  1 . 
　the semantic classes in ldoce are not formed into a hierarchy  but  bruce and guthrie  1  manually identified hierarchical relations between the semantic classes  placing them in a hierarchy which we use to resolve the restrictions. we resolve the restrictions by returning  for each word  the set of senses which do not break the constraints  that is  those whose semantic category is at the same level  or lower  in the hierarchy . 
the selectional restriction resolution algorithm makes 

use of the information provided by the named entity identifier  section 1   although we are not disambiguating named entities they are still useful to help disambiguate other words: for example  if a verb has two senses one of which places the restriction h  =human  on its object  the other i  =inanimate  and the object was a named entity marked person then we would prefer the first sense for that verb. 
　we implemented another voting system for this partial tagger and found that 1% of words were correctly disambiguated by this module. 
1 	subject codes 
our final partial tagger is a reverse engineering of the broad context algorithm developed by  yarowsky  1 . this algorithm is dependent upon a categorisation of words in the lexicon into subject areas  yarowsky used roget large categories. in ldoce pragmatic codes indicate the subject area of senses and  since primary codes have a wider coverage  we chose them as our subject categories. since roget is a thesaurus each entry in the lexicon belongs to a large category  however not every ldoce sense has a primary pragmatic code. in order to counter this we created a dummy category  denoted by -  used to indicate a sense which is not associated with any specific subject area and this category is assigned to all senses without a pragmatic code. the differences between the structures of ldoce and roget meant that we had to adapt the original algorithm reported in  yarowsky  1 . space restrictions prevent us from reporting this in detail  however a detailed account is provided in  stevenson  1 . after this partial tagger has computed the most likely pragmatic code  the set of senses marked with that code are returned for each ambiguous word. we also implemented a voting system for this partial tagger and found that it was the most effective  disambiguating 1% of senses. 
1 	combining knowledge sources 
each partial tagger can only suggests possible senses for each word and so it is necessary to have some method to combine the results. we decided that the most effective way to carry this out would be to make use of the algorithms produced by the machine learning research community. consequently we experimented with several of the publicly available algorithms and examined three main approaches: inductive logic programming  ilp   rule induction and memory-based learning. ilp and rule induction approaches operate by representing data as a set of rules abstracted from training data. memory based learning stores training examples and classifies new instances by identifying the closest one. we examined the progol algorithm  muggleton  1  as a representative of ilp approaches  the cn1 algorithm  clark and niblett  1  for rule induction approaches and the timbl algorithm  daelemans et al.  1  for memory based learning. we found that the timbl algorithm was most suitable for our purposes since it carried out the required processing in a reasonable time as well as producing good results. 
　we presented the learning algorithm with a number of training words for which the correct sense is known. the senses for each training word are represented in a feature vector format  with a vector for each sense  apart from those removed by the part of speech filter  section 1 . the vector consists of the results from each of the partial taggers  frequency information and 1 basic collocations  first noun/verb/preposition to the left/right and first/second word to the left/right .  a simple module  the collocation extractor  is used to identify these from the source text.  each sense is marked as either appropriate  if it is the correct sense given the context  or inappropriate. the learning algorithm stores each of the example senses according to its classification  appropriate/inappropriate . 
　to disambiguate un-tagged text  the partial taggers and filters are run and the learning algorithm used to identify the training instance which is most similar to the new  unclassified  example. if the memory based learner suggests that more than one sense is appropriate for any given word then the first of those is chosen as a tie-breaker. 
　although the system is trained on a fixed vocabulary it is restricted to these. if a word is encountered which was not in the training data then the results of the partial taggers and frequency information can be used to make the disambiguation decision. 
1 producing an evaluation corpus 
since our system is designed to disambiguate all content words in text the most appropriate evaluation procedure will be to compare the output of the system against some  gold standard1' texts  but these are very labourintensive to obtain. lexical semantic markup is generally accepted as a more difficult and time-consuming task than part of speech markup. rather than expend a vast amount of effort on manual tagging we decided to adapt two existing resources to our purposes. we took semcor  landes et of.  1   a 1 word corpus with the content words manually tagged as part of the wordnet project. the semantic tagging was carried out under disciplined conditions using trained lexicographers with tagging inconsistencies between manual annotators controlled. sensus  knight and luk  1  is a large-scale ontology designed for machine-translation and was produced by merging the ontological hierarchies of wordnet and ldoce {bruce and guthrie  1 . to facilitate this merging it was necessary to derive a mapping between the senses in the two lexical resources. we used this mapping to translate the wordnet-tagged content words in semcor to ldoce tags. 
　the mapping is not one-to-one  and some wordnet senses are mapped onto two or three ldoce senses when the wordnet sense does not distinguish between them. the mapping also contained significant gaps  words and senses not in the translation . semcor 
stevenson and wilks 1 

contains 1 words tagged with wordnet synsets  1 of which are proper names which we ignore  leaving 1 words which could potentially be translated. 
the translation contains only 1 words tagged with 
ldoce senses  although this is a reasonable size for an evaluation corpus for this type of task; it is several orders of magnitude larger than those used by other researchers working in large vocabulary wsd  for example  cowie et al.  1    harley and glennon  1  and  mahesh et a/.  1 . this corpus was also constructed without the excessive cost of additional hand-tagging and does not introduce any inconsistencies may occur with a poorly controlled tagging strategy. 
1 results 
our system was tested using a technique known as 1fold cross validation. this process is carried out by splitting the available data into ten roughly equal subsets.  this is done by randomly selecting the first tenth  then choosing another from the remaining data and so on until only one tenth remains.  one of the subsets is chosen as the test data with the timbl algorithm being trained on the remainder. this is repeated ten times  so that each subset is used as test data exactly once  and results are averaged across each of the test runs. this technique provides two advantages; firstly  the best use can be made of the available data and  secondly  the computed results are more statistically reliable than those which would be obtained by simply setting aside a single portion of the data for testing. 
we found that the system correctly disambiguated 
1% of the ambiguous instances to the fine grained sense level and in excess of 1% to the homograph level. we also analysed the performance of our system over each of the four different grammatical categories it analysed and these results are shown in table 1.  yarowsky  1  comments that nouns tend to be disambiguated by broad contextual considerations while adjectives  adverbs and verbs are more affected by the words acting as their arguments. this would suggest that our partial taggers may have different effects over the four grammatical categories on which they operate. future research is planned to investigate this in detail. 
1 conclusion 
these experimental results show that it is possible to disambiguate a large vocabulary of content words to high levels of accuracy at both the rough-grained homograph and fine-grained sense levels. our system uses an optimised combination of diverse lexical knowledge sources and this appears to be a successful strategy for this problem. although the results reported here are slightly lower than those reported for systems which disambiguate a very restricted vocabulary  such as  yarowsky  1  who quotes 1% for a test set of 1 words  our figure is far greater than has been achieved so far by other large vocabulary disambiguation systems such as  harley and glennon  1 . 
　the fact that the optimised figure from the module learning  1%  is so much larger than that from the individual modules  which range between 1% and 1%  shows that the information content of the different modules must be different  i.e. are not notational variants of each other  or else the higher  optimised  figure would not be possible. 
acknowledgments 
the work described here was supported by the european union language engineering project ecran - extraction of content: research at near-market  le-1 . the authors are also grateful for the comments provided by the three anonymous reviewers of this paper. 
