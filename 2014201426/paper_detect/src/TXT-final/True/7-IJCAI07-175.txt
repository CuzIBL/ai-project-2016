
recently  generative probabilistic modeling principles were extended to visualization of structured data types  such as sequences. the models are formulated as constrained mixtures of sequence models - a generalization of density-based visualization methods previously developed for static data sets. in order to effectively explore visualization plots  one needs to understand local directional magnification factors  i.e. the extend to which small positional changes on visualization plot lead to changes in local noise models explaining the structured data. magnification factors are useful for highlighting boundaries between data clusters. in this paper we present two techniques for estimating local metric induced on the sequence space by the model formulation. we first verify our approach in two controlled experiments involving artificially generated sequences. we then illustrate our methodology on sequences representing chorals by j.s. bach.
1 introduction
topographic visualisation techniques have been an important tool in multi-variate data analysis and data mining. generative probabilistic approaches  bishop et al.  1b; 1a; kaba¡än and girolami  1; nabney et al.  1  demonstrated advantagesovernon-probabilisticalternativesin terms of a flexible and technically sound framework that makes various extensions possible in a principled manner.recently  tino et al.introduceda general frameworkfor visualizing sets of sequential data. loosely speaking  a smooth twodimensional manifold in the space of appropriate noise models  e.g. hidden markov models  hmm  of a given topology  is constructed so that hmms constrained on the manifold explain the observed sequences well. each sequence  data item  is then projected on the manifold by visualizing the relevant hmms likely to be responsible for generating that sequence.
¡¡it is possible in non-linear projections that visualizations of two data items get close in the visualization space  even though they may be separated by a large distance in the data space  and vice-versa . therefore  to be of practical use  nonlinear projection methods must be equipped with representations in the visualization space of metric relationships among data items in the original data space. otherwise  one would not be able to detect and understand the underlying cluster structure of data items. to that end  in the case of static vectorial data of fixed-dimensionality  bishop et al.  calculated magnification factors of the generative topographic mapping  gtm   bishop et al.  1b  using tools of differential geometry.
¡¡however  in the case of structured data types  such as sequences or trees  one has to be careful when dealing with the notion of a metric in the data space. the generative probabilistic visualization models studied in this paper naturally induce a metric in the structured data space. loosely speaking  two data items  sequences  are considered to be close  or similar  if both of them are well-explained by the same underlying noise model  e.g. hmm  from the two-dimensional manifold of noise models. due to non-linear parametrization of the manifold  projections of two quite different data items may end up being mapped close to each other. we emphasise  that in this framework  the distance between structured data items is implicitly defined by the local noise models that drive topographic map formation. if the noise model changes  the perception of what kind of data items are considered similar changes as well. for example  if the noise models were 1storder markov chains  sequences would be naturally grouped with respect to their short-range subsequence structure. if  on the other hand  the noise models were stochastic machines specializing at latching special events occurring within the data sequences  the sequences would naturally group with respect to the number of times the special event occurred  irrespective of the length of temporal separation between the events.
¡¡in this paper  we quantify the extend to which small positional changes on manifold of local noise models explaining structured data  for visualization purposes the manifold is identified with visualization space  lead to changes in the distributions defined by the noise models. it is important to quantify the changes in a parametrization-free manner we use approximations of kullback-leibler divergence. we present two techniques for estimating local metric induced on the structured data space by the model formulation. we first verify our approach in two controlled experiments involving artificially generated sequences. we then illustrate our methodology on sequences representing chorals by j.s. bach.
1 a latent trait model for sequential data
consider a set of sequences over the alphabet s of s symbols  s = {1 ... s}. the n-th sequence is denoted by s n  =  s tn  t=1:tn  where n = 1 : n and tn denotes its length. the aim is to represent each sequence using a two-dimensional latent  visualization  space v =   1 1. in order to exploit the visualization space fully  a maximum entropy  uniform prior distribution is imposed over the latent space. with each latent point x  we associate a generativedistribution over sequences p s|x . one possibility  considered in this paper  is to use hmm with k hidden states  rabiner and juang  1 :
 
h
 1 
where h is the set of all tn-tuples over the k hidden states.
¡¡for tractability reasons  we discretize the latent space into a regular grid of c points1 x1 ... xc. in order to have the hmms topologically organized  we  in the spirit of  bishop et al.  1b; kaba¡än and girolami  1   constrain the flat mixture of hmms 
 
by requiring that the hmm parameters be generated through a parameterised smooth nonlinear mapping from the latent space v into the hmm parameter space:
p h1 = k|x =gk a ¦Ð ¦Õ x    1 p ht = k|ht 1 = l x =gk a tl ¦Õ x    1 p st = s|ht = k x =gs a bk ¦Õ x    1 where indexes k l run from 1 to k; index s ranges from 1 to s  and
  the function g .  is the softmax function1 and gk .  denotes the k-th component returned by the softmax  i.e.

  ¦Õ .  =  ¦Õ1 .  ... ¦Õm .  t ¦Õm .  : r1 ¡ú r is an ordered set of m non-parametric nonlinear smooth basis functions  typically rbfs  
  the matrices a ¦Ð  ¡Ê rk¡Ám  a tl  ¡Ê rk¡Ám and a bk  ¡Ê rs¡Ám are free parameters of the model.
¡¡assuming the sequences s n   n = 1 : n  were independently generated  the model likelihood reads
.	 1  maximum likelihood estimates of the free parameters can be obtained via expectation-maximization  em  algorithm  tino et al.  1 .
¡¡given a sequence s  some regions of the latent visualization space are better at explaining it than the others. a natural representation  visualization  of s is the mean of the posterior distribution over the latent space  given that sequence:
.
1 quantifying metric properties of the visualization space
in this section  we present two approaches to quantifying metric properties of the visualization space v as sensitivities  measured by kullback-leiblerdivergence of local noise models  hmm  addressed by the points in v to small perturbations in v.
1 approximating kullback-leibler divergence through observed fisher information matrix
consider the visualization  latent  space v =   1 +1 and the two-dimensional manifold m of local noise models p ¡¤|x   e.g. hmm with 1 states  emissions over 1 symbols  parametrized by the latent space through  1  and  1 . the manifold is embedded in manifold h of all noise models of the same form  e.g. all hmms with 1 states and emissions over 1 symbols . consider a latent point x ¡Ê v. if we displace x by an infinitesimally small perturbation dx  the kullback-leibler divergence
tween the corresponding noise models p ¡¤|x   p ¡¤|x+dx  ¡Ê m can be determined via fisher information matrix
f x  =  ep ¡¤|x   1 logp .|x  
that acts like a metric tensor on the riemannian manifold m  kullback  1 :
.
the situation is illustrated in figure 1.
¡¡local noise models used in this paper  hmms  are latent variable models and there is no closed-form formula for calculating the fisher information matrix. however  lystig and hughes presented a framework for efficient calculation of the observed fisher information matrix of hmms based on forward calculations related to those used in maximum likelihood parameter estimation via em algorithm. we adapt the framework to a special kind of parametrization of hmm used in the latent trait model of section 1. each hmm has two parameters - latent space coordinates x =  x1 x1   these can be thought of as coordinates on the visualization screen .
¡¡consider a set s x  of n sequences independently generated by hmm p ¡¤|x . all sequences have equal length t. given the n-th sequence  we start recursion from the beginning of the sequence:


figure 1: two-dimensional manifold m of local noise models p ¡¤|x  parametrized by the latent space v through  1  and  1 . the manifold is embedded in manifold h of all noise models of the same form. latent coordinates x are displaced to x + dx. kullback-leibler divergencebetween the corresponding noise models p ¡¤|x   p ¡¤|x + dx  ¡Ê m can be determined via fisher information matrix f x  that acts like a metric tensor on the riemannian manifold m.recursive step:
 
where .
¡¡starting again at the beginning of the sequence  we recursively evaluate 1st-order derivatives with respect to latent coordinates x1 x1. letting q ¡Ê {1}  we have:
 
and
the calculations recursively employ and .
we also need 1st-order derivatives of initial state  state transition and state-conditional emission probabilities with respect to latent coordinates. we present only the equation for state transitions  other formulas can be obtained in the same manner:
 
where a i.  denotes the i-th row of the parameter matrix a . .
¡¡1nd-order derivatives are obtained in a recursive manner as well:
	 1	 n 
 n 
¦Ø1  k;xq xr  = p s1 |h1 = k x p h1 = k|x  
 xq xr
 n 	 x q x1 r  s1 n  ¡¤¡¤¡¤ st n  ht = k|x  p
	¦Øt	 k;xq xr  =	 n 	 n 
p s1  ¡¤¡¤¡¤ st 1|x 
and .
the calculations recursively employ and
. we also need both 1st- and 1nd-orderderivativesof initial state  state transition and state-conditional emission probabilities with respect to latent coordinates. again  we present only the equation for state transitions:

denoting
 
we calculate elements of the observed fisher information matrix f  x   given the set of sequences s x   as
f .
¡¡to illustrate metric properties of the visualization space  we calculate the observedfisher informationmatrices f  xc  in all latent centers xc  c = 1 ... c  and detect through eigen-analysis of f  xc  directions of dominant local changein kullback-leiblerdivergencebetween the hmm parametrized by xc and its perturbed version parametrized by xc + dx. in the visualization space  we signify magnitude of the dominant change  dominant eigenvalue of f  xc   by local brightness of the background as well as mark the direction of the dominant change by a piece of line.
1 direct recursive approximation of kullback-leibler divergence
another alternative approach to probe metric properties of the visualization space is the estimatex +
¦¤x   directly. do  presented an efficient algorithm for approximating kullback-leibler  k-l  divergence between two hmm of the same topology. the approximation is based on backward calculations related to those used in maximum likelihood parameter estimation via em algorithm.
¡¡with each hidden state k ¡Ê {1 ... k} we associate an auxiliary process ¦Âk t . given a sequence s =  st t=1:t  the likelihood p s|x  can be efficiently calculated by
  starting at the end of the sequence:
¦Âk t;x  = p st|k x 
  recursive step:

  final step:

for u v ¡Ê  1   define
u
¦Ê u v  = ulog .
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡v the kullback-leibler  k-l  divergence between two hmms   can be approximated as follows:
  recursion is initiated at end of the sequence:

  recursive step:

  final step: the empirical k-l divergence  given the sequence s  is bounded by
.
¡¡given the set of n sequences  s x   generated independently by the hmm addressed by x  an estimate of x + ¦¤x   is calculated as
.
¡¡again  to illustrate metric properties of the visualization space  we perturb latent centers xc in regular intervals around small circle centered at xc and for each perturbation ¦¤x calculate. as before  in the visualization space  we signify magnitude of the dominant change by local brightness of the background as well as mark the direction of the dominant change by a piece of line.
1 experiments
in this section we demonstrate techniques introduced in sections 1 and 1 to provide additional metric information when visualizing sequential data.
¡¡in all experiments the latent space centres xc were positioned on a regular 1 ¡Á 1 square grid  c = 1  and there were m = 1 basis functions ¦Õi. the basis functions were spherical gaussian functions of the same width ¦Ò = 1. the basis functions were centred on a regular 1¡Á1 square grid  reflecting uniform distribution of the latent classes. we account for a bias term by using an additional constant basis function ¦Õ1 x  = 1.
¡¡free parameters of the model were randomly initialized in the interval   1 . training consisted of repeating em iterations  tino et al.  1 . typically  the likelihood levelled up after 1 em cycles.
¡¡when calculating metric properties of the visualization space  each set s xc  consisted of 1 generated sequences of length 1.
1 toy data
we first generated a toy data set of 1 binary sequences of length 1 from four hmms  k = 1 hidden states  with identical emission structure  i.e. the hmms differed only in transition probabilities. each of the four hmms generated 1 sequences. local noise models p ¡¤|x  were imposed as hmms with 1 hidden states. visualization of the sequences is presented in figure 1 a . sequences are marked with four different markers  corresponding to the four different hmms used to generate the data set. we stress that the model was trained in a completely unsupervised way. the markers are used for illustrative purposes only. representations of induced metric in the local noise model space based on fisher information matrix  section 1  and direct k-l divergence estimations  section 1  can be seen in figures 1 b  and 1 c   respectively. dark areas signify homogeneous regions of local noise models and correspond to possible clusters in the data space. light areas signify abrupt changes in local noise model distributions as measured by k-l divergence and correspond to boundaries between data clusters. the visualization plot reveals that the latent trait model essentially discovered the organization of the data set and the user would be able to detect the four clusters  even without help of the marking scheme in figure 1 a . of course  the latent trait model benefited from the fact that the distributions used to generate data were from the same model class as the local noise models. there were few atypical sequences generated by the hmm marked with '+'  and this is clearly reflected by their projections in the lower left corner.
1 melodic lines of chorals by j.s. bach

	 a 	 b 	 c 
figure 1:  a  visualization of 1 binary sequences of length 1 generated by four hmms with 1 hidden states and with identical emission structure. sequences are marked with four different markers  corresponding to the four hmms. also shown are representations of induced metric in the local noise model space based on fisher information matrix  b  and direct k-lnext we subjected the latent trait model to a set of 1 chorales by j.s. bach from uci repository of machine learning databases. we extracted the melodic lines - pitches are represented in the space of one octave  i.e. the observation symbol space consists of 1 different pitch values. local noise models had k = 1 hidden states. figure 1 shows choral visualizations  while representations of induced metric in the divergence estimations  c .

figure 1: visualization of 1 melodic lines of chorales by j.s. bach.
local noise model space based on fisher information matrix and direct k-l divergence estimations can be seen in figures 1 a  and 1 b   respectively. the method discovered natural topography of the key signatures  corroborated with similarities of melodic motives. the melodies can contain sharps and flats other than those included in their key signature due to both modulation and ornaments. the upper region contains melodic lines that utilise keys with flats. central part of the visualisation space is taken by sharps  left  and almost no sharps/flats  center . the lower region of the plot contains chorals with tense patterns  e.g containing tritons  and is quite clearly strongly separated from other chorals. again  the overall clustering of chorals is well-matched by the metric representations of figures 1 a  and 1 b . flats  sharps and tense patterns are clearly separated  as are sharps and infrequent sharps/flats. there are more interesting features to this visualization  e.g. enharmony patterns  that cannot be addressed due to space limitations.
1 discussion and conclusion
when faced with the problem of non-linear topographic data visualization  one needs additional information putting relative distances of data projections in proportion to local stretchings/contractions of the visualization manifold in the data space. it is important to base representations of manifold stitchings/contractions on the particular notion of distance or similarity between data items that drives formation of the topographic mapping. if one visualizes structured data  such as sequences  this can be a highly non-trivial task  especially for the family of neural-based topographic mappings of sequences  koskela et al.  1; horio and yamakawa  1; voegtlin  1; strickert and hammer  1 . however  when the visualization model is formulated as a latent trait model  such calculations follow naturally from the model formulation. two sequences are considered to be similar if both of them are well-explained by the same underlying noise model  in our particular case - hmm . noise models are organized on a smooth two-dimensional manifold and we can study changes of the local metric  based on kullback-leibler divergence  due to non-linear parametrization. we have presented two approaches to metric analysis of the visualization space and experimentally verified that additional information provided by metric representations in the visualization space can be useful for understanding of the overall organization of data. we stress. that the data organization revealed is always with respect to the notion of similarity between data items imposed in the model construction.

	 a 	 b 
figure 1: representations of induced metric in the local noise model space based on fisher information matrix  a  and direct k-l divergence estimations  b  for visualizations of choral by j.s. bach.¡¡the results can be easily generalized e.g. to treestructured data using  for example  hidden markov tree models  hmtm   durand et al.  1  . we stress that the presented framework is general and can be applied to visualizations through latent trait models of a wide variety of structured data  provided a suitable noise model is used. for example  we are currently working on topographic organizations  and metric propertiesof such  of fluxes form binary star complexes  where the noise models are based on real physical models of binary stars.
