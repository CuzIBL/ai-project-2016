 
explanation-based generalization algorithms need to generalize the structure of their explanations. this is necessary in order to acquire concepts where a recursive or iterative process is implicitly represented in the explanation by a fixed number of applications. the fully-implemented bagger1 system generalizes explanation structures and produces recursive concepts when warranted. otherwise the same result as standard explanation-based generalization algorithms is produced. bagger1's generalization algorithm is presented and empirical results that demonstrate the value of acquiring recursive concepts are reported. these experimental results indicate that generalizing explanation structures helps avoid the recently reported negative effects of learning. the advantages of the new approach over previous approaches that generalize explanation structures are described. 
1. introduction 
explanation-based learning  ebl  systems acquire new concepts by generalizing explanations to specific solutions. it has been recognized that explanation structures that suffice for understanding a specific solution are not always satisfactory for generalizing the solution. instead  the explanation structure must often be augmented if a useful generalization is to be produced lshavlik1 . this paper addresses the important issue in ebl of generalizing to n  cheng1  cohen1  prieditis1  shavlik1  shavlik1 . this can involve generalizing such things as the number of entities involved in a concept or the number of times some action is performed. this type of generalization is necessary in order to acquire concepts where a general iterative or recursive process is implicitly represented by a fixed number of applications in the specific problem's explanation. 
　bagger1 is a fully-implemented system designed to generalize the structure of explanations. this system is the successor to an earlier structure-generalizing ebl system  shavlik1  that learned iterative concepts  manifested as linear chains of rule applications . unlike its predecessor  
　this research was partially supported by a grant from the university of wisconsin graduate school. 
1 	machine learning bagger1 is capable of acquiring recursive concepts involving arbitrary tree-like applications of rules  can perform multiple generalizations to n in one example  and can integrate the results of multiple examples. 
the next section presents the bagger1 algorithm. 
subsequent sections illustrate the new algorithm with a simple example  compare what it learns to the result of more standard ebl systems  present empirical evidence of the value of generalizing explanation structures  and compare bagger1 to other systems that generalize number. 
1. the bagger1 algorithm 
bagger1 extends the eggs algorithm  mooney1j  a standard ebl algorithm. both algorithms assume that  in the course of solving a problem  a collection of pieces of general knowledge  e.g.  inference rules  rewrite rules  or plan schemata  are interconnected  using unification to insure compatibility. in eggs  the resulting explanation structure is generalized by first stripping away the details of the specific problem and then determining the most general unifier that allows the general pieces of knowledge to be connected in the same way. this involves replacing the constants in the specific explanation with constrained variables. the result is a new composite knowledge structure that contains the unifications that must hold in order for the knowledge pieces to be combined in the given way. assuming tree-structured explanations  if the leaf nodes can be satisfied  the root  goal  node will also be satisfied. there is no need to again reason about combining the pieces of knowledge together to achieve the goal. since a substantial amount of work can be expended constructing the original solution  the new knowledge structure can lead more rapidly to a solution. notice  though  that the structure of the explanation is not changed. if some process is repeated three times in the specific problem's explanation  it will be repeated exactly three times in the concept acquired by eggs. 
　bagger1 generalizes explanation structures by looking for repeated intcr-dependent sub-structures in an explanation. figure 1 schematically presents this process. assume that in explaining how a goal is achieved  the same general sub-problem  p  arises several times. the full explanation can be grouped into several qualitatively different portions. first  there arc the sub-explanations where an instantiation of p is supported by the explanations of other instantiations of the general problem p. in the figure  these are the sub-explanations marked 1 and 1. second  there are the sub-explanations where an 

instantiation of p is explained without reference to another instantiation. these arc the sub-explanations labelled 1  1  and 1. finally  there are the portions not involving p  subexplanation 1 . 
　the explanation in figure 1 can be viewed as the trace of a recursive process. this is exactly what must be recognized in the explanation of a specific example if a recursive or iterative concept is to be learned. the generalizations of sub-explanations 1 and 1 form the recursive portion of the concept  while the generalizations of sub-explanations 1  1  and 1 produce the termination conditions. bagger1 partitions explanations into groups as illustrated by figure 1  from which a new recursive concept is produced. 
　the bagger1 generalization algorithm appears in figure 1. this algorithm is expressed in a pseudo-code  while the actual implementation is written in common lisp. the remainder of this section elaborates the pseudo-code. 
in the algorithm back arrows   -  indicate value assignment. the construct for each element in set do statement 
means that element is successively bound to each member of set  following which the statement is evaluated. 
　the bagger1 algorithm assumes explanations arc derivation trees  e.g.  something that could be produced by a horn clause theorem prover such as prolog . as is standard in explanation-based algorithms  an explanation structure is first produced from the specific problem's explanation. to build the explanation structure  each instantiated rule in the explanation is replaced by a copy of the original general rule.  if the same general rule is used multiple times  each time it appears in the explanation structure its variables are renamed. this prevents spurious equalities among variables in the explanation structure.  
　the algorithm starts at the root of the explanation structure. if something that unifies with the general goal appears elsewhere in the explanation structure  then a recursive rule  called a recurrence  is produced starting at the root node. otherwise  the general version of the antecedents arc collected and a new rule produced. a recurrence can also arise within an explanation structure  and this discussion will assume the root node does not directly lead to a recurrence. 
　collect generalantecedents produces the necessary requirements for the consequent of a rule to hold. ignoring for a moment the possibility of recurrences being constructed  this entails traversing through the explanation structure and stopping at operational  keller1  nodes. along the way all the unifications necessary to connect the rules in the explanation structure are collected  thus eliminating the need to check these when the acquired rule is later applied . operational nodes are either antecedents satisfied by a problem-specific fact or antecedents somehow judged to be easily satisfied. this portion of the algorithm is merely a rehash of the eggs algorithm. hence  notice that when bagger1 detects no potential generalizations to tv it produces the same result as the eggs algorithm. 
　more interesting is what happens when a potential recurrence is detected. this is done by seeing if  in the derivation of a general antecedent  a unifiable version of the antecedent appears  e.g.  the p's in figure 1 . if so  the explanation structure headed by the general antecedent is partitioned into two types of sub-explanations. those terminal proofs where a version of the antecedent docs not appear in its proof and those recursive proofs where at least one does. in the recursive proofs  the recursive subexplanations arc replaced by a call to the recurrence being constructed. these calls contain the term that must be unified with the consequent of the recurrence. hence  in figure 1  when cutting out sub-explanation 1  subexplanations 1 and 1 are removed. notice  then  that the cut-out sub-explanations are non-overlapping. 
　once the sub-explanations are produced  each is generalized by again calling the bagger1 algorithm. this means that another recurrence can be found within a subexplanation  allowing multiple generalizations to n in a single example. when generalizing the sub-explanations  the necessary unifications between the root of the subexplanation and the recurrence are collected. the generalizations of the sub-explanations are disjunctively combined and a recurrence produced. since two subexplanations may generalize to the same result  duplicate disjuncts arc removed from the acquired recurrence. 
　the recurrence is a separate entity from the rule produced for the full explanation. because of this  they support transfer of the the results learned during one task to the performance of another  provided the two tasks involve common sub-tasks. recurrences being separate entities also supports learning from multiple examples. if a new method for satisfying the consequent of a recurrence is encountered  it can be merged with the previous disjuncts.1 

　before bagger1 produces a new rule  it reorganizes the antecedents. this involves removing redundant antecedents and reordering them to increase the efficiency of future retrievals. in recurrences  if an antecedent  one independent of the variables in the recurrence's consequent  appears in every terminal disjunct  it can be removed from the recursive disjuncts. 
　assuming that explanations are logical proofs  the bagger1 algorithm can be proved correct. 
　theorem: the bagger1 algorithm is sound. that is  the rules it learns will never derive anything that cannot be derived by the initial domain theory  see  shavlik1  for the proof . 
	there are several 	shortcomings of the bagger1 

	algorithm. 	one  explanations must be trees. 	two  
1 	machine learning 

recurrences may not terminate. three  when a concept involves multiple recurrences  better performance often can be obtained by merging the recurrences together. four  there can be redundant computation in some cases. five  a recurrence can acquire too many disjuncts  thereby decreasing its utility. approaches to these problems are discussed in  shavlik1 . 
1. an example 
an sample application of the bagger1 algorithm appears in this section. circuit design is the domain used. rules  appearing in the appendix  determine how to implement a circuit depending on the type of gates available. assume only and and not gates are available. dcmorgan's law must be repeatedly applied in order to implement a circuit involving a collection of or gates  in which the final output is negated. an explanation of how this task can be accomplished can be produced using the rules provided. 
　if the eggs algorithm is applied to the resulting explanation  the rule in figure 1 results. notice that this rule not only requires a fixed number of inputs  but also a fixed topology. clearly the explanation structure needs to be generalized. 

　the result produced by bagger1 appears in figure 1. in this problem  the full explanation leads to a single recurrence. the recurrence  which is given a  gensym'ed  name  involves four disjuncts. the first applies when only a single application of demorgan's rule is necessary. and gates must be available if the resulting circuit is to be implemented. the remaining three disjuncts are recursive. the second and third disjuncts apply when one input is a wire. in this case  the rule recurs on the other input. in the final disjunct  recursion is needed for both inputs.  if the training example was simpler  all of these conditions may not have been encountered and multiple examples would be needed to learn the complete concept.  
　a couple of points about the notation in figure 1 are necessary. the match predicate unifies its two arguments. the italicized or's and and's describe the meaning of the rule  while the others refer to gates in the circuit being designed. the special predicate call calls the recurrence named in its first argument  recall that there can be recurrence calls within recurrence calls  which is why the name is needed . the second argument is unified with the consequent of the recurrence upon the recursive call. finally  bagger1 renames the variables in recurrences. 
variables starting with v appear in the consequent  while the e variables arc  local  variables. 
　the rule learned by bagger1 can be viewed as a general version of dcmorgan's law. it converts the negation of an n-input or gate into an n-input and gate. notice that it applies to a much larger class of problems than does the rule learned by eggs. 
　notice that the acquired recurrence does not refer to any of the initial rules. it is self-contained and is topologically similar to a recursive lisp function. the consequent specifics the parameters and the antecedents form something like a lisp cond. this ''function  is produced from a collection of simple declarative prolog-like rules. rules are called explicitly rather than seeing which rules in a large rulebase may be applicable. hence  bagger1 provides a way to transform a simple  but inefficient  logic program into a program in a more efficient language. 
1  empirical analysis 
a question arises. is it worthwhile to generalize explanation structures  generalizing explanation structures leads to acquiring more general rules  but because the resulting rules are more complicated  applying them entails more work. this question involves the relationship between the operaiionality and generality of acquired rules  keller1 . experiments reported in this section investigate whether it is better to the learn a more general recursive rule or whether it is better to individually learn the subsumed rules as they are needed. 
　using the circuit design rules  three systems are compared: bagger1  eggs  and no-learn  a system that does not learn any new rules . the two learning systems arc given some number of circuits to convert and if they use more than one rule to solve a problem  they generalize the resulting explanation and save the new rule. following this training phase  all three systems try to solve a new collection of ten problems  with the learning systems giving 
	shavlik 	1 

priority to their acquired rules. during this testing phase no learning occurs. all three systems use the same backwardchaining problem solver  which is basically a lisp implementation of prolog augmented to handle explicit calls to bagger1's recurrences. each experiment is repeated ten times. hence  each point plotted in the figures below is the mean of 1 measurements. 
　in the first experiment  training problems consistent of randomly generated implementations of 1-input or gates using binary or} the final output is negated. as in the last section  the task is to implement this gate using only not and binary and gates. 
　the percentage of test problems solvable using each systems acquired rules is plotted in figure 1. clearly  bagger1 needs many fewer training examples to learn the concept being taught. 
　the next issue  a more important one  is how long it takes each of the systems to solve a new problem. the learning systems are trained on or circuit problems of various sizes. the number of randomly-generated training examples for each problem type equals the possible number of binary circuits with that total number of inputs  see the formula in the previous footnote   eggs organizes its rules according to the number of inputs involved  i.e.  in six groups  and only possibly relevant rules are checked during problem solving. figure 1 contains the mean solution time on the test problems.  for eggs  only the time spent on problems solved by a learned rule is recorded. both this and the assumption about rule organization favor eggs.  
　figure 1 shows that merely learning all possible cases is worthwhile if there are only a few possible cases. however  as the number of possible cases grows  it soon becomes worthwhile to learn recursive rules. note that after awhile  it would be better to have not learned at all than to use eggs. a structure-generalizing ebl algorithm such as 
bagger1 helps avoid the negative effects of learning recently reported  minton1 . 
bagger1 has also been run on blocks-world problems 
 see  shavlik1  for details . the task is to teach a system how to build towers of a range of heights. figure 1 presents  on a logarithmic scale  the performance of the three systems on this task as the maximum tower height increases.  at each point  enough training examples are presented so that both learning systems completely learn the concept.  again  as the complexity of problems increases  bagger1 begins to out-perform eggs. 
1. related work 
besides bagger  shavlik1   which only learns iterative concepts  and bagger1  several other explanation-based approaches to generalizing number have been recently proposed. 
　prieditis  prieditis1  developed a system that learns macro-operators representing linear sequences of repeated 


strips-like operators. recursive rules are not learned  nor are disjunctive ones. in the fermi system  cheng1   cyclic patterns are recognized using empirical methods and the detected repeated pattern is generalized using explanation-based learning techniques. however  unlike the techniques implemented in bagger1  the rules acquired by fermi are not fully based on an explanation-based analysis of an example  and so are not guaranteed to always work. cohen  cohen1  recently developed and formalized another approach to the problem of generalizing number. his system generalizes number by constructing a finite-state control mechanism that deterministically directs the construction of proofs similar to the one used to justify the specific example. his approach can acquire recursive and disjunctive concepts  as well as learn from multiple 

1 	machine learning 

examples. however  his approach assumes that no new relevant facts or rules are added to the database after learning. this means that  unlike bagger1  a new concept cannot be learned in the presence of one set of facts and then applied under a new set of facts  e.g  from one blocks-

world scene to another . finally  in physics 1  shavlik1  the need for generalizing number is motivated by analyzing mathematical calculations. 
　the problem of generalizing to n has also been addressed within the paradigms of empirical  or similarity-based  learning  e.g.   sammut1   and automatic programming  e.g.   summcrs1  . a general specification of number generalization has been advanced by michalski 
 michalski1 . he proposes a set of generalization rules including a closing interval rule and several counting arguments rules which can generate number-generalized structures. the difference between such empirical approaches and bagger1's explanation-based approach is that the newly formed similarity-based concepts typically require verification from corroborating examples  whereas the explanation-based concepts are immediately supported by the domain theory. 
1. conclusion 
explanation-based learning systems must generalize explanation structures if they are to be able to fully extract general concepts inherent in the solutions to specific examples. a general approach for doing so has been presented. the bagger1 algorithm is capable of learning complicated recursive concepts  can integrate results from multiple examples  and has been shown to perform better than a standard ebl algorithm  eggs . experimental results indicate that generalizing explanation structures helps avoid the recently reported negative effects of learning  minton1 . on problems where learning a recursive rule is not appropriate  the system produces the same result as the eggs algorithm. applying the recursive rules learned only requires a minor extension to a prologlike system  namely  the ability to explicitly call a specific rule. this research brings ebl closer to its goal of being able to acquire the full concept inherent in the solution to a 
specific problem. 
appendix - initial rules for the circuit problem 
implement-by not not  x     y  : -
implcment-by  x  y . 
	implement-by not and  x  y   nand  a  b   	:-
have-nands  implcmcnt-by  x  a   implement-by  y  b . 
implement-by not  x  nand  y  1   :-
havc-nands  implement-by  x  y . 
	implement-by and  x  y  nand nand  a  b   1   	:-
havc-nands  implement-by  x  a   implement-by  y  b . 
implement-by not  x  not  y   	:have-nots  implement-by  x  y . 
implement-by or  x  y   or  a  b   : havc-ors  implement-by  x  a   implcment-by  y  b . 
implement-by or  x  y  nand  a  b   :have-nots  havc-nands  implement-by not  x   a   implement-by not  y   b . 
implcment-by not or  x  y   and  a  b   :have-ands  implemcnt-by not  x   a   implcmcnt-by not  y   b . implement-by  wire  wirc  :- wire  wire . 
