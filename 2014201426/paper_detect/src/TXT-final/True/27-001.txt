 
when learning classifiers  more extensive seaich for rules is shown to lead to lower prt liftiv  accuracy on many of the leal-world domains investigated tihs counter-intuitive re suit is particularly lchvant to recent system the seaich methods that use nsk-fiee prun-
ing to achieve the same outcome as exhaustive search we propose an iterated search method that commences with greedy search extending its scope at each aeration until a stopping criterion is satisfied this layered search is often found to produce theories that are more accurate than those obtained with either gree dy search or modcratrlv  extensive beam search 
1 	introduction 
mitchell  observes that the generalization implicit in learning from examples can beviewed as a search over the space of possible theories from this perspective most machine learning methods carry out a scries of local searches m the vicinity of tht current theorv selecting at each stop the most promising improvement covering algorithms like aq  michalski 1  cn1  clark and niblett 1  and fonl  quinlan 1  add new rules or 
horn clauses to a developing theory divide-and-conquer methods such as c ari  biennan  friedman  olshen and stone  1  and c1  quinlan  1  extend or re-vise a node of the current theory and selective mstance-based learners as exemplified by  cameron-jones 1  add an item to the current set of retained instances 
모theory spaces tend to be very large  so even these local searches must be constrained in the interests of efficiency decision tree methods typically use greedy search  cart c1  or low-plv lookahead {cls  hunt  marin and stone  1   while covering methods such as 
aqll and cn1 employ small-width beam search this limited search is guided by heuristics that are intended to identify simple theories consistent with the training 
set 
모모this research was made possible by a grant from the australian research council and assisted by research agreements with digital equipment corporation 
	r 	m 	c a m e r o n - j o n e s 
department of applied computing 
university of tasmania 
launceston 1 
모모모모모모모모australia mcameronoleven appcomp utas edu au 
모in stark contrast to this limited search murphy and pa/1am  tackle the daunting task of generating ill such consistent decision trees in extensive experiments with four datasets they find that the smallest trees typically have lower predictive accuracy than slightly larger trees  exhaustive search for the simplest consistent the-
ories does not necessarily lead to improvement 
모several investigators  notablv  rvmon 1 schhmmer 1 webb 1   have recently developed brant h and-bound systematic search methods that have the same outcome as exhaustive search again ths more extensive search has not led to the discovery of markedly better theories rvmon reports non-monotonic improvement using three artificial datasets webb describes opus  a system that resembles r n1 both are covering algorithms that repeatedly look for a rule with minimal laplace predicted error  discussed in section 1  despite the fact that opl s effectivelv explores all rules whereas e n1 uses limited beam search the latter finds more predictive theories on four of the five datasets studied 
모we believe that these rather discouraging results can be explained by noting that  for anv collection of training data there are 'fluke1 theories that fil the data well  according to whatever criterion is employed  but haye low predictive accuracy when a very large nuinbei of hypotheses is explored the probability of encountering sueh a fluke increases since systematic search has the same outcome as exhaustive search it will always find such a fluke if one exists on the other hand heuristic search explores only a vanishingh small propoition of the space of theories and so is less likely 1o encounler a fluke it is commonly held that the construction of the ones that are more complex than can be justified by the data leads to poor predictive performance  breiman 1 al 1  but see also schaffer 1  overfitting refers to the construction of a theory tailored to the data that has high  but misleading  apparent accuracy by analogy we use the term oversearching to describe the discovery by extensive search of a theorv that is not necessarily oycrcomplex but whose apparent accuracy is also misleading 
모in this paper we present empiric al evidence for the oversearching phenomenon and piopose a partial remed  first  exploring larger numbers of potential theories consistently leads to selection of better theories in only one of twelve domains investigated we deyelop a simple criterion for deciding whether a rule found after 
	quinlan and cameron jones 	1 
some amount of search should be preferred to an apparently superior rule found after more extensive search this criterion leads to a method for curtailing search and we rtport results demonstrating the benefits of this strategy both for finding individual rules and for learning  omplete theories fmaly we offer limited evidence for the proposition that oversearthmg is orthogonal to overfitting 
1 	learning individual rules 
this paper addresses the familiar propositional formalism in which each item belongs to one of it discrete classes and is specified by its valess for a fixed collection of attributes  quulan 1  the goal is to learn a classifier 
from a training set that predicts classes of unseen items we concentrate on classifiers expressed as a sequence of rules of the form if t  and t1 and and tu then class ct 
where a test t  takes one of four forms a j = t or a# for disciete attribute 1  and value v and 1 j   f o r 1   / for continuous attribute 1j and constant threshold t 
   in the first experiment we focus on learning single rules following webb  in searching for one thai minimizes the laplace predicted error define the true error rate of a rule as the probabihty that an item that satisfies the rule's left-hand side does not belong to the class given bv its right-hand side if a rule such as the above is satisfied b  n training items c of which belong to classes other than the class cx nominated bv its righthand side the estimated error rate of the rule on unseen items is given bv 
           f + a- - 1 뫩{1 c  = 
n + k 
where k is again the number of classes 
   to show the effects of increasing amounts of search rules art found with beam search of width u varving exponentially from 1 to 1 for a given class cr the initial beam at level 1 consists of tht w single tests that have the lowest laplace error rate as abovt a.t each subsequent level with up to u conjuncts in the current beam all wa s of extending each conjunct with an addi tional test are considered and the bestn of them retained for the next beam 
   notice that we can prune some combinations of tests without adding them to the beam if a conjunct r matches n training items with e errors  adding further tests to r can only make it more specific and thereby decrease the number of items that it covers an  conjunct of the form r and s can thus do no better than match n-t items with no errors unless 뫩   t w   1   is less than the laplace error estimate of the best conjunct found so far  no descendant of r could ever improve on this best conjunct allowing r to be discarded 
   search proceeds until the current beam is empty  whereupon the best conjunct found so far becomes tht left-hand side of the rule for cx 
   we have carried out experiments on twelve real-world datasets from the uci repository that are described in 
1 	learning 

table 1 the hrst four being tht real-world domains stuc led bv webb the size of each datasct the number i classes and the numhers of discrete  d  and eontirn ous     attributes are shown the following trial we repeated 1 times for each dataset 
split the data randomly into 1% trainnig and 1% test sets making the class distributions as uniform as possible 
	for beam widths u = 1 	1 	뫫1 
for each class in turn 
identify the rule with lowest 뫩 value found dunnq a beam search of width u determine the rule s error rate on the test set 
results of the se expe rime nts appear in figure 1 in whre error rates are plotted against beam width these   ror rates are weighted averages across the classes tl weights being the class relative frequencies in the trail ing set tht dotted lines in each graph show the ave age 뫩 values of tht rults selected without cxceplmi 뫩 values decline with beam width as more e xtcnsiy search discovers rules with lower predicted error rate the solid lines however  show the average true erre rate of the rules as measured on the unseen test dat.  the vertical bars show one standard error either sir of the mean  the open circles flag the beam corrcspone ing to the lowest true error rate and the asterisks ai explained in the next section   as can be seen the hi havior of tht true error rate is quite unlike that of tr estimated rate 뫩 with some datasets such as the pre moter domain  increasing search first lowers the true e ror rate  then causes it to rise  an example of the san non-monotonicity observed bv rymon  on oth  domains such as hepatitis  more extensive search is un formly counter-productive only for the glass datasi does the true error rate of the selected rule decline nea monotomcally with increased search 
   to understand what is going on  we examine in moi detail the chess endgame dataset  a particularly strikin example of non-monotonicity separating results for the two classes  figure 1   we can see that good rules ft the majority class are found from the complete datase with relatively small beam widths and thereafter in 

provenent is slight the i shape of the curve is due to th  mnioritv class fur wihch a marked change occurs at beam width 1g 
모in one typical tral search at beam width 1 finds a conjunction of three tests  r1  that is satisfied h  1 items of the minority class and none of the other class further specialization of ri can only decrease its cover and hence its 뫩 value howe er there is also a conjunction of five tests  r1  that covers 1 items of the minority class and 1 items of the other class now  in order to discover a rule with left-hand side t1 and t1 and and tn  the beam at level i must contain at least one conjunction oft of these tests for all values of z from 1 to n-l conjunct r1 is difficult to find because no single test or pair of tests has a low 뫩 value for this trial the 뫩 value of the best single test ranks sixth among all single tests so r1 is eliminated unless the beam width is at least g the best combination of two of the fi e tests has an 뫩  alut that ranks thirteenth among all two-test combinations so the beam width needs to be al least 1 if r1 is not to be eliminated at the second le el of the beam search once it is found however the large number of attnbules in this domain allows r1 to be refined b  the addition of seven further tests giving a rule r1 that covers 1 nems without error in terms of the 뫩 measure  r1 has a lower predicted error than r  and so is preferred 
모when evaluated on the test data howevei the com plex rule ra misclassifies five items of the 1 that it matches  approximately the same error rate as the conjunct r1 from which it was derived on the other hand  the rule r1 is more accurate  misclassifving one of the thirteen items that it covers increasing the beam width from 1 to 1 allows the  fluke' ra to be discovered with a consequent increase in the error rate 
	quinlan and cameron jones 	1 
1 	selecting a b e a m w i d t h 
having e-stabhshed that extensive search can lead to less accurate rules we now discuss a method for limiting search 
   for the domains of figure 1  the most accurate rule is often found with a beam width w greater than 1  where u=l corresponds to greedy search  but less than 1 taken here as an approximation to exhaustive search suppose now that a layered search were conducted b  starting with u =  and doubling the beam width at each iteration could we select the appropriate beam width so as to obtain the most accurate rule' this decision clearly cannot be made with reference to the 뫩  alue alone since this alwas decreases with further search 
   the following probabilistic argument was inspired b  the famous occam paper  blumer ehrenfeucht haussltr and warmuth  1  if the true error rate of a rule is r the probability that the rule will give no more than e e rrors m n trials is given bv 

i = 1 
if then art h rules all having an error rate of r or inore the probability that an  one of them will give e or less errors in n trials is at most  whether or not 
the rules are independent 
   now let hu denote the number of rules examined during the search with beam width u  and let r  satisfy 

if all these rules had error rate greater than or equal to 
tv there would be up to an even chance that one of them would give no more than ru errors in nu trials we use this value of ru  as a gut estimate of t he accuracy of the best rule selected from the hy candidates as w takes on the values 1 the corresponding values of hu   nv and e   can be determined and the value of rw computed we take the overall best rule to be that for which ru is minimal 
모there are numerous over-simplifications m this argu ment for instance  it ignores the effect of beam selection at each level search for the rule with minimal 뫩 value is guided bv the c values of partial rules  so that the 
1 	learning 
k
 cu errors in n  trials is not a fair experiment agam 'number of rules examined' is an imprecise concept manv putati e rules cover no examples and some inks are pruned as described in see tion 1 for these expel iments hu is taktn as the number of distinct altiibuu combinations considered during search on the basis that for each such combination there will be some test on every selected attribute that minimizes the inle s 뫩 value 
   table 1 illustrates the values for the positive class of the promoters dataset in one trial greedy searc h finds a rule that covers 1 items without error inecreading tin beam width to 1 causes a larger number of mles to be examined but vields a better rule covering 1 items still better rules are found at beam widths 1 and 1g in the latter case the number of rules examined mcieases  lie chance that the rule is a fiuke as reflected by its highe i r  value the rule encountered at beam width =1 is consequently chosen as the overall best 
   we can now explain the asterisks in figure 1    each trial and for each class a best beam width is selected as above using only the training data the astensk null rates the average beam width selec te d and the neiage of the corresponding error rates on the unseen test data ' with the notable exceptions of the chess endgame md glass datasets  the average beam widths chosen arc iu ai the lowest points on the curves  piovidmg some empiric l1 support for the beam width selec tion strategv 
1 	learning complete classifiers 
the search for individual rules can be extended to learn complete classifiers using the standard covering method  michalski 1  
	for each class cx 	in 	turn 
	mark all items of class ct 	as uncovered 
	while 	uncovered items 	of class cx 	remain 
find and retain the best rule mark as covered all class cx items that satisfy the rule 
모모the asterisk will not normally he on the solid curve because the beam width selected varies from class to class and from trial to trial 

breast cancer hoiiiie voting lymphography primary tumor auto insurance 
chess. endgarne  rrdil approval glass hepatitis 
pima diabetes promoters soybean 
ratio to ls 
table 1 results with greedv  gs  layered  ls  and extensive  es  search 

when  leterunning the best rule above onl  uncovered items. of class ct and all items of other classes are considtrcd whereas wlrbh  finds the rule with the guaranteed lowest c value at each iteration  we use the best ruk encountered l   three kinds of h  unstir seaich 
gs greed  search with beam width 
ls layered seanh witli beam widths u = 1   1  1 and so on ro a maximum of 1 for  cach beam width the rule with lowest l value entountcrcd during searrh is if tamed and its i  value determined the 
than es in 1 trials and worse in 1 both results are significant at better than p=1 
   the ratio to ls figures in the final row g ve an overview across the twelve domains each is the average ratio of a result to that for layered search for these datasets  the theories found using ls have less than 1% of the error of those produced b  either greedy or extensive search ls requires 1 times as much computation as gs but the absolute difference is small since the lat-
dverall best rail best rule bemg the ond of these with lowest times slower than layered search even though the latter ter is so economical extensive search  where u is hxed at 1  is 1 times slower than greedy search and 1 

r  th  layered se-inh is terminated whenever two successive valuesof in fail lo improve on the best valu  of   found so far 
es fxtuisive seairh willi fixed beam width w=1 again taken to appioxnnate exhaustive search 
an unsern item is classified by the ruleset b  finding the rule with lowest 뫩 value that matches it then assigning the item to the class speufied in that ruk s right-hand side item that satisfies no rule is assigned the most frequent class observed in the training set 
모the experimental design was similar to that described m secttion 1 for each dataset 1 trials were conducted splitting the data into stratified equal-sized training and test sets three classifiers were constructed from the training set using greedy  gs  layered  ls  and extensive  es} search  respectively and each clabsiher evaluated on the test set results averaged over the 1 repetitions appear in table 1 a. simple indicator of theorv complexity is provided by theory size  the total number of tests in all rules times are for a dec axp 1 workstation 
모those error rates for gs and es shown in bold face are significantly1 different from ls layered search is significantly better than greexly search in five domains and worse in three when compared with extensive search layered search is significantly better in six domains and worse in only one over the 1 trials  ls is better than gs in 1 trials and worse in 1  while it is better 
1
two-tailed sign test p=1 
requires repeated search with increasing beam widths 
1 	theory complexity and search 
discussion of the chess endgame example in section 1 might suggest that this problem is just another instance of overfittmg extensive search is leading to the construction of elaborate rules existing mechanisms for ovrrfitting avoidance such as rissanen s minimum dp scnptiom principle  quinlnlan and rivest  1 cameron jones  1   might thus be sufficient to prevent the choice of rules with low predictive accuracy we offer two arguments against this hypothesis 
    s can be seen in table 1 ranking the search methods by the complexity of the theory produced does not correlate well with the accuracy of the theories although es often finds more complex individual rules this complexity is counterbalanced by their increased coverage extensive search results in complete theories that arc simpler than those found by layered search and much simpler  1%  than those produced with greedy search yet on average the fs theories are less accurate than their ls counterparts and have similar accuracy to the 
gs theories 
   the second is empirical based on preliminary e  penments that assess the impact of oversearchmg on instances-based learning for these trials a classifier consists of a subset of the training items  with an unseen item assigned to the class of the most similar retained item all classifiers for a domain are constrained to ron-
	quinlan and cameron jones 	1 

sist of exactly the same number m of retained items  so that all theories have identical complexity beam searches of various widths are again carried out  this time to find the m items that give the lowest classification error on the training set results with the same twelve datasets are reminiscent of figure 1 increased search leads to better and better sets of retained items as assessed on the training data  but the classifier s performance on unseen test data exhibits either a continuous decline or a i -shaped curve in six of the twelve domains 
1 	conclusion 
this paper provides further evidence that more search does not necessarily result in better learned theories in most of the domains studied litre expanding search leads eventually to a decline in predictive accuracy as idiosvncrasies of the training set are uncovered and exploited thus phenomenon of oversearching has also been observed in other domains and indexe d with at least one other heuristic criterion  * 
모for the twelve datasets reported here an iterative lavered search with beam width limited bv a probabilistic criterion ru was found to have better overall performance than either greedy or extensive search even so  the argument that underpins the derivation of theru value and thereby selection of the best  beam width  is simplistic and we arc confident that a better criterion can be developed 
모we believe that oversearching cannot be controlled bv complexity-based mechanisms such as the mdl prin ciple  the disadvantages of oversearching seem to be somehow orthogonal to problems of overfitting mdl is nghtlv popular because it provides a well-justified framework for mapping apparent accuracv and theory com pltxitv into a uniform measure based on coding length ldeallv we would like to see oversearching dealt with in a similarly clean manner by the development of a single metric that embodies all three factors accuracy  theory complexity and extent of search 
acknowledgements 
we are grateful to pat langlev for his detailed and helpful comments on a draft of this paper  and to the anonvmous reviewers who suggested both improvements and areas for further research the breast cancer lymphography and primary tumor datast ts were provided by the ljubljana oncology institute slovenia thanks to the lci repository and to its maintainers  patrick murphy and david aha  for providing access to the datasets used here 
1
모모in place of the laplace estimate we have also tried a confidence limit function ucf  quinlan  1  page 1  this function turns out to be even more susceptible to coincidences in the training data  a majority of the domains discussed here show a monotonic decrease in predictive accuracv with increased beam width 
1 	learning 
