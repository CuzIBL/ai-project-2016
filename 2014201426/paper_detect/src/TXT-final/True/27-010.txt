 
the problem of making optimaj decisions in uncertain conditions is central to artificial intelligence if the state of the world is known at all times  the world can be modeled as a markov decision pro cess  mdp  mdps have been studied extensively and many methods are known for determining op timal courses of action or policies the more realistic case where state information is only partially observable partially observable markov decision processes  pomdps  have received much less attention the best exact algorithms for these problems can be very inefficient in both space and lime we introduce smooth partially observable value approximation  spova   a new approximation method that can quickly yield good approximations which can improve over time this mediod can be combined with reinforcement learning meth ods a combination that was very effective in our test cases 
1 	introduction 
markov decision processes  mdps  have proven to be useful abstractions for a variety of problems when a domain fits into the mdp framework  a variety of methods can be used that are practical for small- to medium-sized problems unfortunately many interesting domains cannot be modeled as mdps in particular domains in which the stale of the problem is not fully observable at all times cannot be modeled as mdps partially observable markov decision processes  pomdps  extend the mdp framework to include partially observable state information with this extension we are able to model a larger and more interesting class of problems but we are no longer able to use the solution methods that exist 
for mdps 
　pomdp algorithms are much more computationally m tensive than their mdp counterparts the reason for this complexity is that uncertainty about the true state of model induces a probability distribution over the model states most mdp algorithms work by determining the value of being in one of a finite number of discrete states while most pomdp algorithms are forced to deal with probability distributions this difference changes a discrete optimization problem into a problem that is defined over a continuous space this increase in complexity is manifested in the performance of pomdp 
1 	learning 
algorithms the best algorithms can take prohibitively large amounts of time even for very small problems 
　our approach  smooth partially observable value approx imation  spova  uses a smooth function that can be adjusted with gradient descent methods this provides an extremely simple improvement rule that is amenable to reinforcement learning methods and will permit an agent to gradually improve its performance over time 
　in our lest cases we found that agents using this rule could rapidly improve their behavior to near-oplimal levels in a frac-
tion of the time required to run traditional pomdp algorithms to completion 
　the following section will introduce ihe mdp formalism  and section 1 will show how this can be extended lo include partial observability section 1 introduces a smooth approximation lo the max function that is the basis of our spova algorithms a simple gradient descent spova algorithm is described in section 1  and results for this algorithm are presented in section 1 where it finds optimal policies for two test worlds an approach based on simulated exploration and reinforcement learning is introduced in section 1  where results are presented showing  his method rapidly finds good policies section 1 briefly discusses other related work  and section 1 contains concluding remarks 
1 	markov decision processes 
one useful abstraction for modeling uncertain domains is. the 
markov decision process or mdp an mdp divides the world into states with actions that determine transition probabilities between these mates the states are chosen so that each state summarizes all that is known about the current status of ihe world the probability of the next state is a function of the current slate and acuon only  not any of the previous states or actions more formally wesay that forany actions and string 
of slates and actions 
 this is called the markov property 
　an mdp is a 1-tuple   s a  ttr  where 1 is a finite set of stales mis a finite set of actions  t is a mapping from sxa into distributions over the states in s  and r is a reward function that maps from s to real-valued rewards this paradigm can be extended to distributions over rewards  or to map from s x a to rewards or distributions over rewards there may be an additional element  / which specifies an initial slate 
　a policy for an mdp is a mapping from s lo actions in a it can be an explicit mapping  or it can be implicit in a value function vt that maps from elements of s to real values this value function represents the value of being in any state as the expected sum of rewards that can be garnered from that point 

forward we can use a value function to assign actions to states in s by choosing the action that maximizes the expected value of the succeeding states policies can be defined for two types of problems  finile-honzon  where the number of steps or actions permitted has a hard limit  and infinite-horizon where there is no fixed ume limit the infinite-honzon case still can respect the value of time by incorporating a cost or negative reward with each step  or by discounting future rewards by a discount factor 
　for any mdp there exists an optimal value function v* that can be used to induce an optimal policy the present value of the rewards expected by an agent acting on an optimal policy will be at least as great as that received by an agent under any other policy there are several methods for determining optimal policies formdps one effective method for determining a value function for the infinite horizon case is value iteration  bellman  1  if the transition probabilities for the model are not known reinforcement learning  sutton  1  can be used to learn an optimal policy through exploration 
　when separate value functions are maintained for each action these functions are often called q functions when reinforcement learning is used to learn q-funuions it is called q learning  watkins  1  our algorithms do not maintain separate value functions for each action as we will discuss below  we regard this as simply an implementation detail and not an important distinction for our approach 
1 	partial observability 
it is important to realize that although actions have uncertain outcomes in mdps there is never any uncertainly about the current slate of the world before taking any action an agent may be uncertain about the consequences of its action but once the action is taken  the agent will know the outcome this can be an extremely unrealistic assumption about the ability of an agent s sensors to distinguish between world slates 
　a partially observable markov decision process  pomdp  is just like an mdp with outputs attached to the slates the outputs can be thought of as sensor observations lhai provide  usually  uncertain information about die state of the world as hints about the true state of the world or as sensor inputs more formally a pomdp is a 1-tuple  s a t r o   where s a  t  and r are defined as in an mdp and o maps from stales in 1 to a set of outputs note thai if o assigns a unique output to every stale and the initial stale is known then the pomdp becomes an mdp because the slate information is fully observable pomdps can be extended to make s map from states to distnbutions over outputs or from s xa to outputs or dislnbutions over outputs there may be an additional element  / lhal determines an iniual distribution over stales 
　the change to partial observability forces an important change in die type of information an agent acting in a world must maintain for the fully observable case  an agent will always know what slate i  is in but for the partially observ able case an agent that wishes to act optimally musi maintain considerably more information one possibility is a complete history of all actions taken and all observations made since this representation can become arbitranly large the maintenance of a joint probability distribution over the stales in 1 often is more tractable this distnbution sometimes is referred to as a belief state 
　it can be shown that the markov properly holds for the belief stales induced by a pomdp this means that in pnnciple we can construct an mdp from the belief states of a pomdp find an optimal policy for the mdp and then use this policy for the pomdp unfortunately most interesting pomdps induce a very large or infinite number of belief stales making direct application of mpd algorithms to pomdps impractical 
　a survey of existing pomdp algonlhms  lovejoy 1  shows lhal many pomdp algonlhms work by constructing a finite representation of a value function over belief stales ihen iteralively updating this representation  expanding the hon zon of the policy ll implies until a desired depth is reached for some classes of problems  sondik 1  infinite-horizon policies will have finite representations and value functions car be obtained for these problems by expanding ihe hon zon until the value function converges lo a stable value in practice infinite horizon policies often can be approximalcd by extremely long finite horizons even if convergence is not obtained regardless of whether they are run lo convergence existing exact algorithms can take an exponential amount of space and time lo compute a policy even if the policy itself does not require an exponential size representation these drawbacks have led lo a number of approximation algonlhms that work by discretizing the belief space the most advanced methods dynamically adjust the resolution of the discretization for different parts of the belief space but it is unclear whether this can be done efficiently for large problems 
　tl is worth noting for the reader unfamiliar with this area that most pomdps with known solutions have less than 1 stales and that exact solutions to pomdps with tens of stales can lake anywhere from minutes lo days if convergence is obtained at all 
　we will introduce a new approximate method for deter mining infinite horizon policies for pomdps this mediod differs from existing methods in that it uses a continuous and differenliable representation of the value function 
1 	the differentiable approximation 
the first and perhaps most important decision thai must be made in any approach to this problem is how to represent the value function sondik showed  that an optimal finilehonzon value function can be represented as the max over a finite set of linear functions of the belief state for a belief stale b a vector representing a distribution over the states of the world the value function can be represented as 

where t is a set of vectors the same dimension as b defining planes in value x belief space each 1  in v can be shown to represent a fixed policy  meaning that we are maximizing over a set of policies to find the one that is best in a particular region of the belief space  see  littman 1  for an indepth interpretation of the 1 vectors   graphically 1  is a hyperplane in value space and die max of these functions forms a convex piecewise linear surface the significance of sondik s result is that it provides a potentially compact means of representing the optimal value function for finile-honzon problems although it does not make any guarantees that  v  will be tractably small 
　for very large horizons ihe value function may be quite smooth as il may be comprised of a very large number of vectors for infinite horizons the value function may be comprised of an infinite number of pieces which means that it is likely to be smooth in at least parts of die belief space in any case  because it is the maximum of a set of linear functions  1 will be convex for these reasons  a good candi date for a differentiate approximation of the infinite horizon 
	parr and russell 	1 


figure 1 	closeness of the max approximation as k increases 
vajue function would be a convex function that behaves like max the following function works rather nicely and is the foundation of the smooth approximation made by spova ' 

　to keep things simple we will assume that the true value function  v   is always positive and that the individual components of the  are all positive this assumption comes with no loss of generality since we easily can shift the function into the positive part of the value space to satisfy 
these conditions this can be done by replacing with 
 where w is a constant offset 
　since the ft  s are always positive the second partial denva live in each of the dimensions is always positive and the function is always convex the function will behave like an over-estimate of max that is smoothed at the corners figure i shows a two-dimensional example of how this works 
we have chosen and graphed our differentiate max approximator for different values of it the convex piecewise linear function below the smooth curves is the max function  only one belief dimension is shown because the second is 1 minus the first  notice that as k increases  the approximation approaches the shape of the convex surface that is the max of the linear functions the height of the function is less important than the shape here since the policy induced by a value function depends on relative not absolute  value assignments 
　the k parameter gives us a great deal of flexibility for example  if we believe that the infinite horizon value function can be represented by the max of a small set of linear functions we may choose a large value for k and try for a very close approximation on the other hand  if we believe the optimal infinite horizon value function is complex and highly textured requiring more components than we have time or spate to represent  a smaller value of k will smooth the approximate value function to partially compensate for a lower number of 
1 vectors 
1 	the basic spova algorithm 
the main advantage of a continuous representation of the value function is that we can use gradient descent to adjust 
   'thereareotherpossiblechoicesfor soft max approximations see for example   mardnetz ctal 1  
1 	learning 
the parameters of the function to improve our approximation ideally  we could use data points from the optimal value func lion v   to construct t such information generally is not available bul an approach similar to value iteration for mdps can be to make our value equation look more like v* we know from value iteration that the optimal value equation for an mdpmust satisfy the following constraint 

since a pomdp induces an mdp in the belief states of the pomdp  we know mat tins equation must hold for the optimal value function for pomdps as well this gives us a strategy for improving the value function search for inconsistencies in our value function  then adjust the parameters in the di recti on that minimizes these inconsistencies this is done by computing the bellman residual  bellman 1   

where next{b  a  is the set of belief slates reachable from b on taking action a we can then adjust the 1s in the direction that minimizes the error by using a smooth max approximation described above  we are able to use a typical gradient descent approach  where a is interpreted as a step size or learning rale in this case die weights correspond to the components of the 1 vectors so the update for thejth component of the 
this equation for the gradient has several appealing properties the  b 1  * part increases with the contribution 1  makes to the value function so the 1 s that contribute most to the value function are changed the most this is then multiplied by b t reflecting ihe influence of die probability of being in state j on ihe/* component of die gradient of we also can interpret k as a measure of how 'ngid the system is for small values of k many weights will be updated with each change however for large values of k  the component of the gradient will permit only minuscule changes to all bul the the 1  that maximize/  1  figure 1 shows the spova algonthm 

figure 1 the spova algorithm 
　since it is impossible to sample all possible belief states  we used the simple approach of randomly selecting belief states empirically  we found that we obtained the best results when we varied k during the run-time typically we would start k at 1 and increase k linearly until it reached 1 when 1% of the requested number of updates were performed as shown in figure 1  small values of k make smoother and more general approximations small values of k also spread the effect of updates over a wider area  in some sense increasing 


the energy ' of the system this gradual increase in k can be thought of as a form of simulated annealing 
　the updates can be repealed until some termination con dition is met either a fixed limit in the number of iterations a minimum number of consecutive samples processed with out e exceeding some threshold or perhaps something more problem specific 
　while we do not yet have a convergence proof for this algo nthm  we are optimistic that with enough iterations decaying a and sufficiently large f that as k lends toward infinily the value function will converge to the optimal value function if the function has a finite piecewise linear representation this is because our error function will become arbitrarily close to the bellman residual as k increases for a large number of updates the system should move towards its only stable equilibrium point the point at which the value function is consistent and  therefore optimal for all points in the belief space 
　one question that has not been addressed is how lo pick  the number of 1 vectors to use for a sub-optimal number of vectors  the gradient descent approach will adjust these vectors in the direction of lower error even though convergence may not be possible our algorithms do not yet automatically determine the optimal number of vectors needed to converge to the value function in the limit one practical way lo in corporate this ability would be to code what we did by hand use a binary search to find the smallest number of vectors that gives an optimal policy  one that is no worse than the best policy produced with a larger number of vectors  
1 	spova results 
we tried the basic spova algorithm initialized with random 1 vectors for two grid worlds thai have appeared in the literature the first  shown in figure 1 is a 1x 1 world from  cassandra ei al 1  movement into adjacent squares is permitted in the four compass directions but an attempt to move off the edge of the world has no effect  returning the agent to its original state with no indication thai anything unusual has happened all slates have zero reward and the same appearance except for the bottom nght stale which has a +1 reward and a distinctive appearance 
　the initial state for this problem is a uniform distribution over all but the bottom right state any action taken from the boliom right state results in a transition to any one of the re maining zero reward slates with equal probability  1 e return to the initial distribution  for this problem we are interested in the optimal infinite-honzon policy with a discount factor of 
 with a momenl s thought  it should be clear that the opumal policy for this model alternates between moving east and south thus does not mean that the optimal infinite hon-

figure 1 policy quality vs number of iterations for gradient descent in the 1 world 
zon value function is easily obtainable in fact there are 1 belief stales that are reachable from tge initial state and the optimal value function defined over all belief states requires 
1 vectors using sondik's representation 
　we ran gradient descent with just i vector for 1 iterations and compared the value of the resulting approximate policy lo the value of ihe optimal policy at 1 iteration intervals we did this by laking a snapshot of the value func 
tion at each interval then simulating 1 steps through the world and counting uie average reward per slep garnered during this period this provides an estimate of the current policy quality we compared this against ihe policy quality for the known opumal policy for the same lime period figure 1 shows a graph of die average reward garnered per step vs the number of iterations performed the horizontal line is the value of the optimal policy computed using the witness algorithm  cassandra et al  1   perhaps the fastest known exact algorithm both algonthms required time on the order of cpu minulcs 
　our second problem shown in figure 1 is from  russell andnorvig 1  it is a1 grid-world with an obstruction al  1  the coordinates are labeled in x y pairs making 
 i 1  the  op left there is no discounting but a penalty of 1 is charged for every step that is taken in this world the two reward stales +1 and i are both directly connected lo a single zero reward absorbing stale originally this problem was used in a fully observable context  but wc have made it partially observable by limning state information to lhat obtained from one east-looking and one west-looking wall detector each is activated when uiere is a wall in the immediately adjacent square for example this makes  1   1  and  1  indistinguishable the initial slate is selected uniformly al random from the nonterminal stales 
　unlike lhe1 x 1 world  transitions are not deterministic every action succeeds wiih probability 1 and fails with proba bility 1 morning the agent in a direction perpendicular from the intended one if such a movement is obstructed by a wall ihen the agent will stay put instead moving right from  1   for example will move the agent right witi probability 1  down with probability 1 and nowhere with probability 1 
　we ran the gradient descent method for 1 iterations with 1 vectors and obtained the results in figure 1 the algorithm requires many samples about 1  1 cpu minutes   before it has enough data in the relevant portion of the space to calculate an approximately optimal policy the comparison policy shown in the figure with a reward per slep of 1 was obtained after over 1 cpu hours using 
	parh and russell 	1 

the witness algorithm and uses 1 vectors in this case the witness algorithm did not converge although recent results in  littman et al  in press  indicate that convergence or near convergence may not be necessary in all cases to obtain a good policy from the witness algorithm 
　one perhaps surprising aspect of our approximation method is thai the number of vectors required is drastically lower than that for an exact solution we were initially surprised to discover that the 1 x 1 problem requires a single vector  making the value function linear part of the savings comes from the fact that our simulations considered only reachable belief states while exact solutions like the witness algorithm construct policies that cover the entire belief space also many more vectors may be required to specify a correct value function than are needed to specify a correct policy from the policy perspective  it is sufficient to know the relative value of all of the belief states not their exact value  making the shape of the value function much more important than the specific values it returns for the 1 x 1 problem any function that assigns a higher value to belief slates that suggest that the agent is closer to the southeast comer of the world will be sufficient a simple linear function is all that is needed here 
　 the use of a smooth function also can reduce the number of vectors required for example  a complex bend that is formed by many hyperplanes in the exact value function often can be approximated very closely by a single smooth bend 
1 	a reinforcement learning approach 
the straightforward gradient descent method can bnng our approximate value function fairly close to the exact one wim a sufficient number of iterations the average difference over the entire state space will be very small a possible shortcoming 
1 	learning 
　for each transition  the algorithm applies the same 1   update as  he gradient descent algorithm but we compute em with to respect the belief stale that is encountered in the simu lation rather than by maximizing over all possible successor states where b is the belief state at time t and b' is the belief state al time we compute 
　to ensure sufficient exploration of the world we chose initial values for the 1 vectors that guaranteed an overestimate for every possible belief state this forced the algorithm to disprove the optimistic estimates by visiting different areas of the belief space this rather simplistic policy was sufficient for our examples but we are investigating the application of some of the methods that have been used fot mdps 1 improve the speed of convergence and to provide stronger guarantees that enough of the belief space will be covered 
　we ran the algorithm on the same two worlds as before the results are shown in figures 1 and 1 spova-rl finds an approximately optimal policy for the 1 world in about 
1 iterations  1 seconds  and for the 1 world in about 1 iterations  1 seconds  
　by focusing its efforts on the most important states in the belief space spova-rl is able to learn a nearly optimal policy extraordinarily quickly while some of this speed may come at the expense of accurate value estimations for rarely visited states this is an acceptable price to pay for many 
domains 
　as a final experiment  we investigated the world shown in figure 1 this world is designed to require a value function with more than one vector  intuitively  being in a linear combination of the a-states is much worse than being definitely in one or the other  figure 1 shows the expected result  namely that spova-rl effectively approximates an optimal 1-vector policy 


figure 1 performance of the spova-rl algorithm on the 
1 world showing the policy quality as a function of the number of epochs 

figure 1 performance of the spova-rl algorithm on the 
1 world showing the policy quality as a function of the number of epochs 

figure 1 a simple domain requiring more a nonlinear value function states labelled a are indistinguishable  but actions 
b and c can lead either to a +1 or a -i reward depending on which if the a-states the agent is in action a leads to a distinctive state  either c or d  which enables the agent to 
find out where it is 

figure 1 performance of the spova-rl algorithm on the environment shown in figure 1 with one vector  spova 
rl finds a policy of value 1  the lower horizontal line  
with three vectors spova-rl quickly finds the optimal onevector policy but after about 1 iterations abandons u in favour of the more complex 1-vector policy  which eventually reaches the optimal value of 1  the upper horizontal line  
1 	relation to other work 
many mdp and pomdp algorithms determine q values rather than a single value function as we have done here the problem of determining the best action from an ordinary value function requires an agent to consult a model to simulate one step into the future and consider the value of possible next sidles an agenl using q values docs not need to look ahead in this fashion since the value of each action is represented directly in the case of q learning a model is not even needed to construct die q values as they are learned directly from agent s experience this so-called model-free' property of q-leaming does not carry over to pomdps the agenl must know something about the dynamics of the world if a compact stale description is lo be maintained over time without a model this state description cannot be evolved and an agent would be forced either lo guess about its true location or to define value functions or q functions over its enure history thus  reengineenng a pomdp algorithm to compute q func tions rather than a value function may change the analysis of the algorithm  but it does not change fundamentally the nature of the problem as it is alleged to do for mdps in fact for the spova implementations we have discussed here it is a trivial change 
　another approach lo the problem of partial observability is lo simply pretend that the sensor observations correspond exactly lo states deterministic policies constructed for this sensor space usually fail miserably typically resulting in 
looping behavior this can be alleviated lo some extent by using tondomized policies of the kind first proposed for use in games of partial information jaakola et al tin press  have shown how to learn from reinforcement using randomized policies demonstrating thai the approach is not unreasonable in some cases 
　a linear value approximator is combined with a clever model learning mechanism in  mccallum 1  and  chnsman 1  it may be possible to generalize their approach lo include more complex functions like those represented by spova a neural network based approach is used in  lin and mitchell 1  they consider a vanely of approaches 
	parr and russell 	1 

thai can make use of an agent s history to learn hidden stale information the idea of a smoothed or soft  max has been around for a while it is the basic idea behind the use of the boltzman distribution for action selection in  watkins  1 and a similar approach has been used in neural networks in for example  martinetz et al 1  we suspect that it may be possible to adapt these approximators for use in pomdps using a similar approach to the one described here although we have not yet investigated this fully in recent work by littman et al fin press  an update rule was developed inde pendently that can be interpreted as a special case of spova this was shown to be adequate for determining good policies for problems with over 1 states 
1 	conclusions and future work 
we have investigated spova  an approximation scheme for partially observable markov decision problems based on a continuous  differentiate representation of the value function a simple ' value iteration' algorithm using gradient descent and random sampling is shown lo find approximately optimal policies but requires a large number of samples from the belief slate space we conjectured that many of these samples correspond to very unlikely or even unreachable belief states and therefore designed spova-rl  a reinforcement learning algonthm that focuses its value function updates on belief slates encountered during actual exploration of the slate space spova rl was able to solve the 1 and 1 worlds very quickly suggesting that optimism concerning the value of generalized approximation methods for pomdps may be 
justified 
　the nexl steps are to tackle larger problems  lo obtain con vergence results and to incorporate methods for learning the environment model we currently are investigating the application of a new algonthm for learning dynamic probabilistic networks  dpns   russell et al  1  such algorithms can find decomposed representations of the environment model that should allow very large stale spaces to be handled fur thermore  the dpn provides a reduced representation of the belief stale that may facilitate additional generalization in the representation of the value function we plan lo use the overall approach lo learn to drive an automobile 
1 	acknowledgement 
members of the u c berkeley mdp reading group  especially 
daphne koller provided helpful suggestions and feedback on the ideas contained in this paper we are very grateful lo tony cassandra and michael littman for sharing their witness algonthm results with us and running their algonthm on our 1 x 1 world michael littman also provided extensive comments on an early draft of this paper tim huang provided help with formatting and figures 
