 
in the last few years it has been shown that recurrent neural networks are adequate for processing general data structures like trees and graphs  which opens the doors to a number of new interesting applications previously unexplored. in this paper  we analyze the efficiency of learning the membership of do ags  directed ordered acyclic graphs  in terms of local minima of the error surface by relying on the principle that their absence is a guarantee of efficient learning. we give sufficient conditions under which the error surface is local minima free. specifically  we define a topological index associated with a collection of doags that makes it possible to design the architecture so as to avoid local minima. 
1 	introduction 
it is well-known that connectionist models are not only capable of dealing with static patterns  but also with sequential inputs. real world  however  often proposes structured domains that can hardly be represented by simple sequences. for instance  there are cases in which the information can be framed naturally in graphs of variable size  and one may be interested in processing these structures as a whole  and not pay attention specifically to their nodes. the ability to classify these structured data is fundamental in a number of different applications such as medical and technical diagnoses  molecular biology and chemistry  automated reasoning  software engineering  geometrical and spatial reasoning  and pattern recognition. neural networks for processing data structures have been proposed by pollack  and  recently  by sperduti  starita & 
goller   and by sperduti & starita . it has been shown that they can actually be used for classifying data structures by using an algorithm  referred to 
1 	neural networks 
as bpts  backpropagation through structure   that extends naturally the time unfolding carried out by bptt  in the case of sequences. it has also been pointed out that bpts is significantly better suited for dealing with long-term dependencies than bptt  because of its inherent unfolding through structures instead of simple lists  the data structure counterpart of sequences. as for any neural network learning algorithm  however  the efficiency of bpts  may be seriously plagued by the presence of local minima in the associated error function. in the epilogue of the expanded edition of perceptron  
minsky  pointed out that 
 ... as the field of connectionism becomes more mature  the quest for a general solution to all learning problems will evolve into an understanding of which types of learning processes are likely to work on which classes of learning problems. and this means that  past a certain point  we won't be able to get by with vacuous generalities about hill-climbing. we will really need to know a great deal more about the nature of those surfaces for each specific realm of problems that we want to solve.  
in this paper  we analyze the efficiency of learning the membership of doags  directed ordered acyclic graphs  in terms of local minima of the error surface by relying on the principle that their absence is a guarantee of efficient learning. we give a sufficient condition under which the error surface is local minima free. in particular  we define a topological index associated with a collection of doags that make it possible to design the architecture so as to avoid local minima. the condition we give holds for any training set composed of graphs with symbolic nodes and a neural network capable of learning the assigned data. 
1 recurrent networks for processing of data structures 
in this section  we review briefly the basic idea proposed in  sperduti and starita  1  concerning adaptive 


	frasconi  gori  & sperduti 	1 


1 	neural networks 

	frasconi  gori  & sperduti 	1 

figure 1: the problem of learning perfect binary trees. the associated error function has no local minima  provided that one uses 1 hidden units. 

1 	neural networks 


	frasconi  gori  & sperduti 	1 
