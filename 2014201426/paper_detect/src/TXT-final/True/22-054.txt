: this paper applies the idea of conspiracy numbers to derive two heuristic algorithms for searching and/or trees. the first algorithm is an ao* best-first algorithm but the standard guarantees do not apply usefully to it because it conforms to the economic principle of sunk costs. the second algorithm works depthfirst and guides the search done by an iterative deepening sld-resolution theorem prover that we have implemented. to avoid repeated effort  the prover caches successes and failures. it exploits the fact that a new goal matches a cached goal if it is a substitution instance of the latter  not just if the two are identical. experimental results indicate that conspiracy numbers and especially the new caching scheme are effective in practice. 
1 introduction 
this paper applies the idea of conspiracy numbers 
 mcallester  1  to derive two heuristic algorithms for searching and/or trees. the first algorithm turns out to be a member of the class of ao* best-first algorithms  nilsson  1   but it conforms to the principle of sunk costs  a rule of economic rationality not respected by traditional and/or tree search algorithms  and hence the standard guarantees of termination and admissibility do not apply usefully to it. 
¡¡the second algorithm works depth-first. it guides the search done by an iterative deepening sld-resolution theorem prover that we have implemented. in addition to caching successes  the prover caches failures and uses the latter to avoid repeated effort also. the prover exploits the fact that a new goal matches a cached success or failure if it is a substitution instance of the cached goal  not just if the two are identical. unlike many heuristics for guiding search and saving effort  the conspiracy number and caching ideas introduced in this paper can be implemented efficiently. our experimental results indicate that conspiracy numbers and especially the new caching scheme are effective in practice. 
¡¡section 1 develops the best-first conspiracy numbers algorithm  relates it to traditional and/or tree search 
   'this research was supported in part by the united states office of naval research through grant n1-k-1. algorithms  and then presents the depth-first conspiracy numbers algorithm. section 1 describes our theorem prover and compares it to a similar prolog-technology theorem prover  stickel  1 . the caching done by our prover is discussed in section 1. finally  our experimental results appear in section 1  and section 1 contains our conclusions. 
1 best-first and depth-first conspiracy number algorithms 
and/or trees are well-known  nilsson  1; pearl  1   and we shall describe them here only to the extent necessary to make the terminology of this paper understandable. briefly  an and/or tree is a tree where each node is an and-node or an or-node. the children of and-nodes are required to be or-nodes  and vice versa. an and/or tree may be completely or partially explored. each node of a completely explored and/or tree is solved or failed. an internal and-node is solved if each of its children is solved and it is failed if at least one of its children is failed. conversely  an internal or-node is solved if at least one of its children is solved  and failed if all its children are failed. a partially explored and/or tree also contains leaf nodes called unexpanded leaves whose children have not yet been discovered. the solved/failed status of an unexpanded leaf is unknown  as is the status of any internal node of a partially explored tree whose status is not fixed by the status of its children. 
¡¡a solution of an and/or tree is a subtree that demonstrates that the root of the tree is solved. the aim of searching an and/or tree is to find a solution. concretely  a solution is a subtree such that all its nodes are solved  all the children of each of its and-nodes belong to the subtree  and at least one of the children of each of its or-nodes also belongs to the subtree. the basic searching operation on a partially explored and/or tree is to expand an unexpanded leaf of the tree. when such a leaf is expanded there are three possible outcomes: it can be discovered to be solved  it can be discovered to be failed  or it can be discovered to have children  which are new unexpanded leaves. different searching algorithms choose in different ways which unexpanded leaf to expand next. 
	elkan 	1 

pearl  1 . ao* algorithms are interesting because under certain conditions  they are guaranteed to terminate and to find optimal solutions. lemma 1 shows that the function en is of the form used by ao* algorithms. however the standard guarantees do not apply usefully to best-first conspiratorial search  because it respects the principle of sunk costs. 
principle of sunk costs: amounts of resources expended in the past on different activities are irrelevant to the decision of which activity to pursue now. ¡ö 
although people often act contrary to the principle of sunk costs  it is a guideline for rational agents that commands universal assent among economists  mccloskey  1; p. 1 . the principle says that  for example  the costs incurred to build a nuclear power plant should have no bearing on the decision whether or not to operate the plant. 
¡¡for guaranteed termination at a minimal-cost solution  in deciding which node to expand next and when to halt  one must take into account the cost so far of in-
complete solutions. the principle of sunk costs disallows this. 
   termination. the nodes on an indefinitely long path from the root of an and/or tree can all have the same minimal conspiracy number. hence best-first conspiratorial search is not guaranteed to terminate. 
¡¡admissibility. ao* algorithms find optimal solutions because of their termination criterion  when they use 
conservative heuristic functions. 
let q be a function assigning measures to solutions. 
an ao* heuristic function h is a conservative estimator of q if for all nodes a  h ¦Á  is less than the lowest q measure of any solution of the and/or subtree rooted at ¦Á. the termination criterion for an ao* algorithm is to stop only when no incomplete solution induces an h value for the root lower than the h value induced by the best complete solution found so far. then the fact that h is a conservative estimator of q guarantees that all 
solutions not yet found are worse under q than this best solution  given that the h values induced by complete solutions are their q measures. 
¡¡the q that assigns the measure 1 to every solution is the only q according to which the en values induced by complete solutions are their q measures. best-first conspiratorial search does find optimal solutions according to this trivial measure. 
¡¡the en heuristic function is a conservative estimator of some non-trivial solution measures. for example en is a conservative estimator of the number of nodes that a solution contains. however if q is a non-trivial measure  then best-first conspiratorial search does not in general find optimal solutions according to q  because the en values induced by complete solutions are less than their q measures. 
	1 	depth-first conspiracy number search 
the intuition behind depth-first conspiracy number search is to explore an and/or tree in depth-first fashion  but to backtrack when the smallest conspiracy involving a leaf is too large. conspiratorial depth-first search has 
1 	search 

two advantages: the and/or tree being searched need never be stored explicitly  and the decision whether to expand a leaf can be made using only information available at that leaf. a disadvantage is that leaves are not expanded in best-first order. 
¡¡suppose an and/or tree is searched in depth-first fashion. then at any time one can identify a stack of ornodes whose subtree is currently being explored. the deepest node on this stack is the one whose subtree will be explored next. each of the or-nodes in the stack has some sibling or-nodes. if the deepest or-node can be solved  and the siblings of all the less deep or-nodes can also be solved  then a solution has been found for the whole tree. thus the deepest or-node on the stack and the siblings of the less deep or-nodes constitute a conspiracy. in fact  they constitute the only conspiracy of the current partial and/or tree  since the nodes on the stack and their siblings are the only nodes of the tree whose subtrees have not already been fully explored  and discovered to be failed. 
¡¡the information needed to decide during bounded depth-first search whether to expand a leaf is its conspiracy depth according to the following definition. 
definition 1: the conspiracy depth of a node in a partial and/or tree is the size of the smallest conspiracy of the whole tree involving that node. ¡ö 
the next lemma implies that conspiracy depths can be computed with only a constant amount of extra work per node  during depth-first exploration of an and/or tree. 
lemma 1: consider the function cd defined on the nodes of an and/or tree as follows. 
  let ¦Á be the root of the tree. then cd ¦Á  - 1. 
  let ¦Á be an and-node with cd ¦Á  - c. let a have it children ¦Á1 ... ar* which are explored in order. then cd ¦Áj  = c + k - j. 
  let ¦Á be an or-node with cd ¦Á  = c. let the children of ¦Á be ¦Á 1   . . .   ¦Ák. then cd ¦Áj  = c. 
at the time the decision is made whether or not to expand a leaf ¦Á during depth-first search of an and/or tree  the conspiracy depth of a is cd ¦Á . 
conspiratorially bounded depth-first search can be implemented efficiently. 
1 a conspiratorial sld-resolution theorem prover 
conspiratorially bounded depth-first search guides the search done by a theorem prover that we have implemented. the only inference rule of our prover is sld-resolution  lloyd  1 . the space that must be searched to find a proof of a given goal  an atomic firstorder predicate calculus formula  by sld-resolution is an and/or tree  where or-nodes correspond to subgoals that must be unified with the head of some matching clause  and and-nodes correspond to bodies of clauses. for such an and/or tree  a conspiracy consists of a set of subgoals that must all be proved together. in order to do so  one must find an an answer substitution for each subgoal such that none of the substitutions conflict. thus as a conspiracy gets larger  even if whether an answer substitution exists is a statistically independent event for each of its members  heuristically the chance that all the members can be compatibly proved decreases as if the events were negatively correlated. 
¡¡at the highest level  our prover uses the idea of iterative deepening  korf  1; stickel and tyson  1 : it repeatedly does depth-first search with increasing conspiracy depth bounds  until a solution is found. our prover is thus similar to the prolog-technology theorem prover pttp  stickel  1 . pttp does iteratively deepened depth-first search using a depth function sd which is the same as the conspiracy depth function cd except on and-nodes. if ¦Á is an and-node with k children ¦Á 1   . . . ¦Ák then sd ¦Áj  = c + k for each child ¦Áj;  whereas cd ¦Áj  = c + k - j. the function sd is not consistent with the principle of sunk costs. in depth-first search  the child ¦Áj; of an and-node is only expanded if its siblings ¦Á 1   . . . ¦Áj i have been solved. the effort required to do this is in the past at the moment the decision is made whether or not to expand ¦Áj. at that moment it is only relevant what subgoals remain to be solved  and there are c + k - j of them. 
¡¡a second difference between pttp and our prover is that pttp attains first-order completeness  because it uses a model elimination inference rule in addition to sld-resolution. in future work we plan to extend our prover similarly. 
¡¡using conspiracy depths to limit search leads to one difficulty: the conspiracy depth of an indefinitely large partial solution can be finite. if an and-node ¦Á has a single child ¦Á' then cd ¦Á'  = cd ¦Á   so in a partial and/or tree  it is possible for the nodes on an infinite path each to have the same conspiracy depth. this problem is circumvented in our prover by using a modified conspiracy depth function cd' such that if ¦Áj is any child of the and-node a  then cd' ¦Áj    cd' ¦Á . specifically  the definition of cd' is identical to that of cd  except that if a is an and-node with k children ¦Á 1   . . . ¦Ák then cd' ¦Áj  - c + k - j + e. for want of any principled way to choose e1 our prover takes c = 1. 
1 caching successes and failures 
this section explains the caching scheme implemented in our sld-resolution prover. the idea of caching is to store the results of past computations in order not to waste time repeating them in the future  keller and sleep  1; pugh  1 . caching is especially useful combined with iterative deepening  because results that have been cached during search to one depth bound can be reused during later searches. our caching scheme is distinctive in two respects. 
¡¡first  if we fail to prove a subgoal within a specified conspiracy depth bound  we cache the fact of the failure. if we attempt to prove the same subgoal later within the same or a lower conspiracy depth bound  the attempt is recognized as doomed to failure and it is aborted. some systems cache subgoals known to be false  which are often called contradictions or nogoods  de kleer  1 . our prover also caches resource-bounded failures. 
	elkan 	1 


1 	search 

1 	analyzing the benefits of caching 
when a subgoal is found to be an instance of a cache entry  then that subgoal gives rise to no child subgoals. thus one expects caching to reduce the effective branching factor of the and/or tree representing the space to be searched to find an sld-resolution proof. the following theorem says how much the effective branching factor must be reduced for caching to be beneficial. 
theorem 1: let the effective branching factor be bc with caching  and bo without caching. let the time to 
	or query 	operation 
then caching saves time if 
proof: let the time required to find a solution be to without caching  and tc with caching. we wish to know 

1 experimental results 
we have performed two sets of experiments to evaluate the usefulness of the ideas developed in this paper. each experiment involved using a different search algorithm to search the same and/or tree. the tree used in all our experiments is the tree representing the sld-resolution search space for a monkey-and-bananas situation-calculus planning problem  plaisted  1 . 
   the first set of experiments compared iterative deepening with three different depth functions: actual proof depth  the pttp depth function  and the modified conspiracy depth function cd!. the results of the experiments are shown in figure 1. measured by the total number of nodes expanded  the conspiracy depth function has a slight advantage.  the first solution to the monkey-and-bananas problem has actual proof depth 1  pttp depth 1  and modified conspiracy depth 1. search to any depth is cut off when a solution is found  so the maximum number of nodes expanded occurs with pttp depth bound 1 or conspiracy depth bound 1.  
   using iterative deepening with the conspiracy depth function cd'  the second set of experiments compared four caching regimes: no caching  caching of successes only  caching of failures only  and full caching. in each case the same search space was explored. note that with 
	elkan 	1 

caching of successes or with full caching  the first solution found is cached  so the problem can then be solved with just one node expansion when searching to depth 1  1  and so on. 
¡¡the results of the second set of experiments are shown in figure 1. with no caching the effective branching factor is approximately 1. caching successes reduces it to 1  caching failures to 1  and caching both to 1. the improvement is clear. it is especially encouraging that the benefits of caching successes and failures are cumulative. for our sample problem  the branching factor reduction achieved with full caching translates into a 1% reduction in the search space size. 
1 conclusion 
in this paper we have described heuristic best-first and depth-first algorithms for searching and /or trees based on the intriguing idea of conspiracy numbers. we have also described an sld-resolution theorem prover that uses the depth-first conspiracy numbers algorithm and a caching scheme that is sophisticated yet amenable to theoretical analysis. the theorem prover relies only on heuristics that can be implemented efficiently: it does not sacrifice speed for intelligence. 
   naturally many issues remain unresolved. we conjecture that an algorithm conforming to the principle of sunk costs provably dominates in some way an algorithm that violates the principle  but we do not know in what way. we also do not know how to characterize the class of theorem-proving problems for which the caching ideas described here are beneficial. 
acknowledgements. section 1 reports on work done jointly with david mcallester. discussions with james altucher  wilfred chen  david mcallester  and alberto segre were helpful in developing the ideas reported in section 1. 
