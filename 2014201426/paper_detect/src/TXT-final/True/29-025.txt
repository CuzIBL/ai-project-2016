 
this paper describes the linguistic part of a system called proverb  which transforms  abstracts  and verbalizes machine-found proofs into formated texts. linguistically  the architecture of proverb follows most application oriented systems  and is a pipe-lined control of three components. its macroplanner linearizes a proof and plans mediating communicative acts by employing a combination of hierarchical planning and focus-guided navigation. the microplanner then maps communicative acts and domain concepts into linguistic resources  paraphrases and aggregates such resources to produce the final text structure. a text structure contains all necessary syntactic information  and can be executed by our realizer into grammatical sentences. the system works fully automatically and performs particularly well for textbook size examples. 
1 	introduction 
proverb is a text planner that verbalizes natural deduction  nd  style proofs  gentzen  1; huang  1b . several similar attempts can be found in previous works. the system expound  chester  1  is an example of direct translation: although a sophisticated linearization is applied on the input nd proofs  the steps are then translated locally in a template driven way. nd proofs were tested as inputs to an early version of mumble  mcdonald  1   the main aim however  was to show the feasibility of the architecture. a more recent attempt can be found in thinker  edgar and pelletier  1   which implements several interesting but isolated proof presentation strategies. proverb therefore can be seen as the first serious attempt to build a comprehensive system that produces adequate argumentative texts from nd style proofs. 
　although a multitude of architectures have been proposed for nlg systems  proverb employs a pipe line architecture consisting of three parts  like most application-oriented systems. the architecture of proverb is illustrated in fig. i 1 . 
   * 	xiaorong 	huang's 	current 	email 	address 	is: xh c formalsys.ca 
l
   in the field of nlg the first two components are called  planners   since they make decisions that will be executed 

figure 1: architecture of proverb 
　the macroplanner of proverb accepts as input a natural deduction style proof  and produces proof communicative acts which are structured into hierarchical attentional spaces. to do so  it uses a strategy which combines hierarchical planning and focus-guided navigation. 
　more detailed linguistic decisions are made by the microplanner. it makes reference choices  chooses between linguistic resources for domain concepts  combines and reorganizes such resources into paragraphs and sentences. as the representation which supports all these operations  the microplanner of proverb adopts a variation of meteer's text structure  which is also its output. 
　our realizer  tag-gen  is a syntactic generator based on the grammar formalism tag  kilger and finkler  
1 . 
　section 1 and section 1 are devoted to the macroplanner and the microplanner  respectively. section 1 contains a complete example. finally  we shall conclude this paper with a discussion in section 1. by the realization component. many approaches differ significantly from general definitions of planner in ai. in particular  approaches towards microplanning often resemble more rule-based systems. 
	huang & fiedler 	1 
1 macroplanning: hierarchical planning and focus-guided 
navigation 
most current text planners adopt a hierarchical planning approach  hovy  1; moore and paris  1; dale  1; reithinger  1 . nevertheless there is psychological evidence that language has an unplanned  spontaneous aspect as well  ochs  1 . based on this observation  sibun  sibun  1  implemented a system for generating descriptions of objects with a strong domain structure  such as houses  ships and families. while a hierarchical planner recursively breaks generation tasks into subtasks  local organization navigates the domain object following the local focus of attention. 
　proverb combines both of these approaches within a uniform planning framework  huang  1a . the hierarchical planning splits the task of presenting a partic-
ular proof into subtasks of presenting subproofs. while the overall planning mechanism follows the rst-based planning approach  hovy  1; moore and paris  1; reithinger  1   the planning operators more resemble the schemata in schema-based planning  mckeown  1; paris  1 . local navigation operators simulate the unplanned aspect  where the next conclusion to be presented is chosen under the guidance of a local focus mechanism. 
　the two kinds of planning operators are treated differently. since hierarchical planning operators embody explicit communicative norms  they are given a higher priority. only when none of them is applicable will a local navigation operator be chosen. 
1 	p r o o f c o m m u n i c a t i v e a c t s 
proof communicative acts  pcas  are the primitive actions planned by the macroplanner of proverb. like speech acts  they can be defined in terms of the communicative goals they fulfill as well as their possible verbalizations. an example of a simplistic one conveying the derivation of a new intermediate conclusion is the pc a 

　depending on the reference choices  the following is a possible verbalization: 
 since a is an element of f and f is a subset of g  a is an element of g by the definition of subset.  
　there are also pcas that predicate actions planned for further presentation and thereby update the global attentional structure. for instance  the pca 
	 begin-cases goal: 	formula 
	assumptions: 	 a b   
creates two attentional spaces with a and b as the assumptions  and formula as the goal by producing the verbalization: 
 to prove formula  let us consider the two cases by assuming a and b.  

figure 1: proof schema case 
1 	h i e r a r c h i c a l p l a n n i n g 
hierarchical planning operators represent communicative norms concerning how the task of presenting a proof can be split into subtasks of presenting subproofs  and how the subproofs can be mapped onto some linear order. let us look at one such operator  which handles the goal of presenting a proof by case analysis. the corresponding schema of such a proof tree is shown in fig. 1  where the subproof rooted by l1 leads to f v g   while subproofs rooted by   l1 and  l 1 are the two cases proving q by assuming f or g  respectively. the applicability encodes the two scenarios of case analysis  where we do not go into details. in both circumstances this operator first presents the part leading to f v g  and then proceeds with the two cases. it also inserts certain pcas to mediate between parts of proofs. this procedure is captured by the planning operator below  note that the verbalizations given are only one possible paraphrase : 
case-implicit 

  features:  merarcrncal-plannmg compulsory implicit  
　the slot features indicates that this is a higher priority operator  compulsory  and should be chosen when a more implicit style is preferred by the user. 
1 	p l a n n i n g as n a v i g a t i o n 
the local navigation operators simulate the unplanned part of proof presentation. instead of splitting presentation goals into subgoals  they follow the local derivation relation to find a proof step to be presented next. 
the local focus the node to be presented next is suggested by the mechanism of local focus. in proverb  our local focus is the last derived step  while focal centers are semantic objects mentioned in the local focus. although logically any proof node which uses the local focus as a premise could be chosen for the next step  usually the one with the greatest semantic overlap with the focal centers is preferred. in other words  if one has proved a property about some semantic objects  one will tend to continue to talk about these particular ob-
jects  before turning to new objects. let us examine the situation when the proof below is awaiting presentation. 


　assume that node  is the local focus  {a  b} is the set of focal centers   is a previously presented node and node  is the current task.  is chosen as the next node to be presented  since it does not  re introduce any new semantic objects and its overlap with the focal centers  {a  b}  is larger than the overlap of  with the focal centers  {b} . due to space restrictions  no navigation operators are discussed in detail. 
1 microplanning: choosing and 
organizing linguistic resources 
many of the first nlg systems link their information structure to the corresponding linguistic resources eitherthrough predefined templates or via careful engineering for a specific application. therefore their expressivepower is restricted  see  meteer  1  for an extensive discussion . first experiments with proverb using a simplistic microplanning mechanism resulted in very mechanical texts. according to our analysis  there are at least two linguistic phenomena that call for appropriate microplanning techniques. 
1 	w h y is microplanning needed  
first  naturally occurring proofs contain paraphrases with respect to rhetorical relations  as well as to logical functions or predicates. for instance  the derivation of b from a can be verbalized as: 
	 since a  b.  	or as 	 a leads to b.  
　the logic predicate para cl c1   also  can be verbalized as: 
	 line c1 parallels line c1.  	or as 
 the parallelism of the lines c1 and c 1 .  
　second  without microplanning proverb generates text structured mirroring the information structure of the proof and the formulae. this means that every step of derivation included by the macroplanner is translated into a separate sentence  and formulae are recursively verbalized. as an instance of the latter  the formula 
		 1  
is verbalized as 
　 f is a set. f is a subset of g.  although the following is much more natural: 
 the set f is a subset of g.  
　therefore  we came to the conclusion that an intermediate level of representation is necessary that allows for flexible combinations of linguistic resources. in section 1 we describe how meteer's text structure can be adopted as our central representation. sections 1 and 1 are devoted to paraphrases and aggregation rules  two of the major tasks of our microplanner. 
1 	text structure in proverb 
text structure was first proposed by meteer  meteer  
1  in order to bridge the generation gap between the representation in the application program and the linguistic resources provided by the language. meteer's text structure is organized as a tree  in which each node represents a constituent of the text and is typed in terms of semantic categories. 
　the main role of the semantic categories is to provide vocabularies which specify type restrictions for nodes. they define how separate text structures can be combined  and ensure that the planner only build expressible text structures. for instance  if tree a should be expanded at node n by tree b  the resulting type of b must be compatible to the type restriction attached to n. following panaget  panaget  1   however  we split the type restrictions into two orthogonal dimensions: the ideational dimension in terms of the upper model  bateman et al.  1   and the hierarchy of textual semantic categories to be discussed below. technically speaking  the text structure in proverb is a tree recursively composed of kernel subtrees or composite subtrees: 
　an atomic kernel subtree has a head at the root and arguments as children  representing basically a predicate/argument structure. 
　composite subtrees can be divided into two subtypes: the first has a special matrix child and zero or more adjunct children and represents linguistic hypotaxis  the second has two or more coordinated children and stands for parataxis. 
　each node is typed both in terms of the upper model and the hierarchy of textual semantic categories. the upper model is a domain-independent property inheritance network of concepts that are hierarchically organized according to how they can be linguistically expressed. fig. 1 shows a fragment of the upper model in proverb. for every domain of application  domainspecific concepts must be identified and placed as an extension of the upper model. 

figure 1: a fragment of the upper model in proverb 
　the hierarchy of textual semantic categories is also a domain-independent property inheritance network. the concepts are organized in a hierarchy based on their textual realization. for example  the concept clause-modifier-rankingl is realized as an adverb  clause-modifier-rankingl i as a prepositional phrase  and clause-modifier-embedded as an adverbial clause. fig. 1 shows a fragment of the hierarchy of textual semantic categories. 
	huang & fiedler 	1 


figure 1: a fragment of the hierarchy of textual semantic categories in proverb 
1 	p a r a p h r a s i n g in proverb 
the mapping from the content to the linguistic resources now happens in a two-staged way. while meteer associates the application program objects  apos  directly with so-called resources trees  we map apos into upper model objects  which in turn are expanded to the text structures. a practical advantage of this two-stadged process is worth noting. instead of having to construct resource trees for apos  the user of our system only needs to define a mapping from the apos to upper model objects  umos . 
　when mapping apos to umos  the microplanner must choose among available alternatives. for example  the application program object para  that stands for the logical predicate denoting the parallelism relation between lines  may map in five different upper model concepts. in the 1-place case  para can be mapped into object leading to the noun  parallelism   or quality  leading to the adjective  parallel.  in the binary case  the choices are property-ascription that may be verbalized as  x and y are parallel   quality-relation that allows the verbalization as  x is parallel to y   or process-relation  that is the formula  x || y.  
　the mapping of upper model objects into the text structure is defined by so-called resource trees  i.e. reified instances of text structure subtrees. the alternative resource trees of an upper model concept are assembled in its realization class. 
　with the help of a concrete example we shall illustrate how the text structure generator chooses among paraphrases and avoids building inexpressible text structures via type checking. 
example 	we 	examine 	a 	simple 
apo derive para cl c1  ♀ . note that b stands for a conclusion which will not be examined here. 
　in the current implementation  the rhetorical relation derive is only connected to one upper model concept derive  a subconcept of cause-relation. the realization class associated to the concept  however  contains several alternative resource trees. the verbalization of two variations is listed below: 
  b  since a. 
  because of a  b. the resource tree of the first alternative is given in fig. 1. 
　the logic predicate para  c1  c1  can be mapped to one of the following upper model concepts  where we always include one possible verbalization: 
  quaiity-relation para  c1  c1   line c1 is parallel to c1  
  process-reiation para  c1  c1  
 c1||c1  

figure 1: a resource tree for derive with reason r and 
conclusion c 
  properfcy-ascription para  c1a c1  
 lines c1 and c1 are parallel  
the property-ascription version  in turn  can be realized in two forms  represented by the two resource trees in fig. 1. 

	as a verb phrase 	as a nominal phrase 
figure 1: textual variations in form of resource trees 
　type checking must ensure that the constructed text vstructure be compatible along both the ideational and the textual dimension. in this example  the combination of the tree in fig. 1 and the first tree in fig. 1 is compatible and will lead to the verbalization: 
 b  since c1 and c1 are parallel.  
　the second tree in fig. 1  however  can only be combined with another realization of derive  resulting in: 
 because of the parallelism of line c1 and line c1  b  
　in our current system we concentrate on the mechanism and are therefore still experimenting with heuristics that control the choice of paraphrases. one interesting rule is to distinguish between general rhetorical relations and domain specific mathematical concepts. while the former should be paraphrased to increase the flexibility  consistency of the latter helps the user to identify technical concepts. 
1 	a g g r e g a t i o n 
although paraphrase generation already increases the flexibility in the text  the default verbalization strategy will still expand the text structure by recursively descending the proof and formula structure  and thereby produces linguistic structures isomorphic to that of a formula. to achieve the second verbalization of equation  1  in section 1  however  we have to combine set f  and subset f g  to form an embedded structure subset set f  g . this textual operation eliminates one of the duplicates of f. we call it aggregation. 


	huang & fiedler 	1 


1 outlook 
this paper examines proverb as an integration of sophisticated linguistic technologies for a concrete application. the system works particularly well with textbook size examples and runs fully automatically for every new example. the output texts are close to detailed proofs in textbooks and are basically accepted by the community of automated reasoning. to benefit from the microplaning techniques which significantly improve the fluency of text  however  linguistic resources must be introduced with each new domain of application. we are working on an interface to simplify this process. 
　although developed for a specific application  we believe the main rationales behind of our system architecture are useful for natural language generation in general. the combination of hierarchical planning with focus-guided navigation provides an effective way of factoring out domain-dependent presentation knowledge from more general nlg techniques. while the macroplanning operators are designed for this specific domain  the framework and the rules of our microplanner represents domain-independent techniques. 
　with the increase of the number of examples tested  some of them over several pages  our experience already suggests some immediate adjustment and improvement of the techniques and strategies  in particular concerning the linearization in macroplanning  heuristic threshold values for discourse segmentation and reference choices  and the treatment of articles. 
