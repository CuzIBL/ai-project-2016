 
     this paper summarizes recent analytical investigations of the mathematical properties of heuristics and their influence on the performance of common search techniques. the results are reported without proofs  together with discussions of motivations and interpretations. 
     highlights include the following: relations between the precision of the heuristic estimates and the average complexity of the search  comparisons of the average complexities of a* and backtracking  procedures for comparing and combining non-admissible heuristic functions  the influence of the weight u   in f ¡ö  l-u  g    * h  on the complexity of a*  determination of the branching factors of alpha-beta and sss*  and the effects of successor ordering on the complexity of alpha-beta and of search depth on the quality of decisions. 
1 the mean complexity of admissible best-nftst alflorithms 
1 	introduction 
     research in this area has focused on unraveling the relation between the accuracy of the heuristic estimates and the complexity of the search which they control. we imagine the following probabilistic search space: a uniform m-ary tree t has a unique goal state g at depth n  at an unknown location. the best-first algorithm a*  searches for the goal state g using the evaluation function: f n   'g n  + h n  
where g n  is the depth of node n and h n  is a 
heuristic estimate of h* n   the distance or number of branches from n to g. the estimates h n  are assumed to be random variables ranging over  1  h* n    characterized by distribution functions 
f	x 	s
h n    	 k m * x  which may vary over the nodes 
in trie tree. 	we further assume that each fk/1  x  
depends only on the distance between n and g  i.e.  on h* n   and not on the heuristics assigned to neighboring nodes. our task is to characterize the 
this research was supported in part by the national 
science foundation under grants mcs 1 and mcs 1. 
influence of the distributions f wx  on e z   the expected number of nodes expanded by a*  for large values of n. 
     the first analysis of the effect of errors or inaccuracies on the performance of a* was conducted by pohl ; the topic has since been pursued by munyer   vanderbrug   pohl   and gaschnig . the basic motivation for these studies has been the following enigma: when a* employs a perfectly informed heuristic  h - h*   it is propelled directly toward the goal without ever getting sidetracked  spending only n computational steps  whereas at the other extreme  when no heuristic at all is available  h = 1   the search becomes an exhaustive  breadth first one  yielding an exponentially growing complexity. between these two extremes lies an unknown accuracy-complexity dependency containing the answers to important design questions. would the added computation invested in improving the accuracy of a given heuristic pay for itself in reduced search complexity  can some heuristics beat the  exponential explosion  that beset breadthfirst search  when 1s the large storage-space required by a* justified in view of lower storage procedures such as backtracking  
     some initial answers to these questions were obtained by pohl  and gaschnig . for instance  they found that if the relative error  h* n -h n  /h* n  remains constant  then the search complexity is exponential  but that when the absolute error h* n -h n  is constant  the search complexity 1s linear. these results  however  were derived for a worst case model where it is assumed that a clever adversary distributes the errors in such a way that a* exhibits its poorest performance. probabilistic extensions of these analyses are justified for two main reasons. first  worst case results are often too pessimistic for describing the typical behavior of an algorithm over a large class of problems. second  it is often hard to guarantee precise bounds on the magnitude of errors produced by a given heuristic  whereas probabilistic characterization of these magnitudes might be more natural. 
1 when 1s one heuristic better than another for admissible best-first algorithms  
     if one heuristic consistently provides a more accurate estimate of a*  it ought to be preferred. this 1s indeed the essence of a theorem by gel perin   who showed that if for each node of the search 

1 


1 

shapes of the distribution functions. thus  a nee essary and sufficient condition for maintaining a polynomial search complexity 1s that a* be guided by heuristics with logarithmic precision  that is  1  n    1 log n . 
impair the ability of a* to benefit from improved precision  the complexity of a* remains more sensitive to error reduction than that of backtracking. 
     backtracking control strategies  on the other hand  have several advantages over a*. they are typically simpler to implement and  more significantly  require much less storage. whereas a* stores all expanded nodes in either open or closed lists  backtracking need only store nodes along a single path  and therefore has a storage complexity linear in n. equation  1   though  reveals the price paid for this storage economy-an exponential time-complexity coupled with ineffective utilization of heuristic knowledge. mixed search strategies combining the storage economy of backtracking with the time savings of a* warrant empirical and theoretical investigations. 
1 the complexity of non-admissible heuristics 
	1 	introduction 
     admissible search strategies are cursed with two basic defects: they spend a disproportionate amount of time investigating av  equally meritorious alternative solutions  and they limit the selection of heuristic functions to only those which never over-estimate the optimal completion cost. the literature on heuristic search contains many examples of non-admissible heuristics which empirically out-perform any known admissible heuristics and yet yery frequently discover the optimal path. 
     the aims of theoretical investigations in this area are to answer several basic and pressing questions which until now have been treated only by lengthy simulations 1n a few  specific domains. when are non-admissible heuristics safe from a catastrophic over-computation  how often are they likely to miss the optimal solution  when is one 
heuristic better than another  is admissibility a virtue in cases where just any solution will do  how should one  debias  a given admissible heuristic  how should one aggregate the estimates provided by several heuristic functions  
     the results reported in the following sections provide answers to some of these questions and pave the way to resolve others. 
	1 	conditions for node expansion 
     the analysis of mean run time of admissible best-first algorithms is facilitated by the simplicity of the condition for expansion. we know that eyery node n in open whose evaluation function satisfies f n    f* root  must eventually be expanded and conversely  that eyery node satisfying f n    f* root  will not be expanded. in our standard model of an m-ary tree with one goal at depth n  these conditions amount to deciding whether 
f n  1s larger or smaller than n . 
1 


1 


1 

1 


1 a minimax algorithm better than alpha-beta  yes and no 
     stockman  has introduced a non-directional algorithm called sss* which consistently examined fewer nodes than a-¡ê. hopes were then raised that the superiority of stockman's algorithm reflected an improved branching factor over that of a-b. 
     a simple heuristic argument exists which refutes these hopes and indicates that sss* and a-b$ possess identical branching factors. it is based on the fact that when the terminal nodes are assigned only two values  say 1 and 1   sss* becomes directional  identical to a-b . now  since the search of continuous games must be harder than the search of any b1valued games of the same structure  and since directional algorithms for b1valued games may require  a branching factor of cn/ -cn* we conclude that sss* would also exhibit a branching factor of at least 
1 	when is successor ordering beneficial  
     the analyses presented in the preceding two sections assumed that the order in which a-b selects nodes for expansion 1s completely arbitrary  say from left to right. in practice  the successors of each expanded node are first ordered according to their static evaluation function. successors of max nodes are then expanded in a descending order of their evaluation function  their highest first  and those of min nodes 1n ascending order  their lowest first . it is normally assumed that such preordering increases substantially the number of cut-offs induced and results in a lower branching factor. 
     clearly  when the static evaluation function 1s well informed  correlating with the actual node value  such ordering will induce all possible cut-offs 
yielding 1   n /1. on the other hand  when the static evaluation is uninformed  the ordering is superfluous and yields  1f   un /1~tn  as *n random ordering. the analysis of this section attempts to quantify the relation between the informedness of the static evaluation function and the branching factor induced by successor ordering. for simplicity  we treat bivalued n-ary trees 
 a

with probability p   ¡ên that terminal node obtains the value 1. the information quality of the static evaluation function v can be characterized by two distribution functions: 

1 when is look-ahead beneficial  
     the basic rationale behind all game-searching methodologies is the belief that look-ahead followed by minimaxing improves the quality of decisions or  1n other words  that the  back-up  evaluation function has a greater discrimination power to distinguish between good and bad moves. although no theoretical model has supported this belief  it has become entrenched 1n the practice of game-playing and its foundation rarely challenged. 
     two heuristic arguments are usually advanced in support of look-ahead. the first invokes the notion of visibility  claiming that since the fate of the game is more apparent near its end  nodes at deeper levels of the game-tree will be more accurately evaluated and choices based on such evaluation should be more reliable. the second alludes to the fact that whereas the static evaluation is computed on the basis of the properties of a single game position  the back-up value integrates the features of all the nodes lying on the search frontier  and so should be more informed. this latter argument essentially takes after a filtering model; the more samples  the less noise. 
     a recent work of nau  demonstrated that the filtering argument is utterly fallacious. in a large class of game-trees  reaching deeper consistently degrades the quality of a decision. this phenomenon  which nau termed pathologicali  is not confined to the special game-trees considered by nau but can be shown to be a common occurrence in the ensemble of games defined by the standard probabilistic model of  h  n  p -trees. 
     assume that the evaluation function v computed at each terminal node of a  h  n  p*sn -tree reflects the likelihood of that node being a win position. the quality of such an estimate can be characterized by the pair of distribution functions defined in  1 . given the pair fy x  and fl x   one can compute the two conditional distributions of vd  the minimax value of the root node: 

     to understand why the filtering argument fails in the case of game-trees  consider the task of 
estimating the value of an arbitrary function ¡ê xi  x¡ê. ... xn  on the basis of the estimates xj  x1  ... xn of its arguments. 	knowing x1 ... xn can improve the estimation of y if we integrate them according to strict statistical rules  as by forming the conditional expectation of y given 
x1 ... xn. 	instead  the minimax procedure amounts to taking y xj  ¡ê1  ... xn  as an estimate for y- it computes the minimax value of the estimators rather than estimates the minimax. 
     this point can be further illustrated by the following game-tree. assume that the terminal values signify the probability that player 1 can force a win from these positions. which move should be selected  towards a or towards b  the minimax evaluation procedure would assign to node a the value .1  to node b the value .1  and so would lead player 1 to prefer a over b. on the other hand  if one computes the probabilities that nodes a or b are win  for player 1  on obtains: 

clearly  b is to be preferred. it is obvious from this example that if one wants to maximize the chances of choosing win positions  the minimax rule is inadequate and should be replaced by productpropagation rules  npj for min nodes and l-n l-pi  for max nodes. if these propagation rules 1 
are used  the phenomenon of pathology should be reduced  if not eliminated. 
     however  the example also facilitates a line of defense 1n favor of the minimax rule. in practical game-playing  one 1s not concerned with maintaining a winning position  winning regardless of what the opponent does  but simply with beating a fallible opponent. such an opponent  if he shares 
our assessment of the terminal values  can be predicted to choose the left move from position b and the right move from position a. therefore  on the basis of such a prediction  we have: 
p player 1 wins from a    .1 
p player 1 wins from b  ¡ö .1 
1 which agrees with the minimax calculation. 	in summary  although the minimax rule is inadequate when playing against an omnipotent opponent  tacitly it contains a realistic model of a fallible opponent who shares the knowledge and limitations of player 1  and therefore should be effective against such opponents. 	this  perhaps  may account for the fact that pathology 1s not observed in practical gameplaying programs; 	the quality of play usually improves with search depth. 
     another reason for the absence of pathology 1n practical game-playing could be that the evaluation functions become more accurate toward the end of the game  possessing increased visibility then  an effect not included in the preceding analysis. nau  has recently demonstrated that pathological behavior occurs 1n a specific game despite an increase in evaluation function accuracy. determining the rate of improved accuracy necessary for combating pathology remains an open theoretical problem. 
