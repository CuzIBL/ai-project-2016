 
the intelligent tutoring system autotutor uses latent semantic analysis to evaluate student answers to the tutor's questions. by comparing a student's answer to a set of expected answers  the system determines how much information is covered and how to continue the tutorial. despite the success of lsa in tutoring conversations  the system sometimes has difficulties determining at an early stage whether or not an expectation is covered. a new lsa algorithm significantly improves the precision of autotutor's natural language understanding and can be applied to other natural language understanding applications. 
1 	introduction 
the use of intelligent technology in education is on the rise. intelligent tutoring systems  once restricted to artificial intelligence labs at major universities  arc migrating to mainstream schools  koedinger et al  1 . intelligent tutoring systems  its  in this environment face a difficult challenge: to understand the student and manage the tutoring session in the face of vague orungrammatical input. for most itss  language understanding and dialog management are core components. particularly in a classroom  these systems live or die by their ability to understand what the student is trying to say. one technique  latent semantic analysis has been successfully developed for such purposes  landauer et ai  1b; landauer and dumais  1; foltz et ai  1; graesser et ai  1 . lsa  a statistical technique utilizing unsupervised learning  is both highly portable to other domains and adept at recognizing vague or incomplete input. 
¡¡this paper outlines some problems inherent in the traditional lsa algorithm and solutions to this problem by using a different lsa algorithm. not only does this new algorithm increase the precision of its language understanding  but it also offers a new perspective on a commonly used technique in cognitive science  computational linguistics and artificial intelligence. 
1 latent semantic analysis 
latent semantic analysis is a statistical  corpus-based language understanding technique that estimates similarities on a scale of-1 to 1 between the latent semantic structure of terms and texts  dccrwester et al.  1 . the input to ls a is a set of corpora segmented into documents. these documents are typically paragraphs or sentences. mathematical transformations create a large term-document matrix from the input. forcxample  if there are m terms in n documents  usually m and n are very large  for now  assume n m   then a matrix of. 	is obtained. the value of ftj is a function of the integer that represents the number of times term 1 appears in document is a local weighting of term 1. in document j and is the global weighting for term v. such a weighting function is used to differentially treat terms and documents to reflect knowledge that is beyond the collection of the documents. this matrix of a has  however  lots of redundant information. singular value decomposition reduces this noise by linearly decomposing the matrix a into three matrices a = is m x m and v is square matrices  such that  and is m x n diagonal matrix with singular values on the diagonal. by removing dimensions corresponding to small singular values and keeping the dimensions corresponding to larger singular values  the representation of each term is further reduced to a smaller vector witli only k dimensions. the new representation for the terms  the reduced u matrix  arc no longer orthogonal  but the advantage of this is that only the most important dimensions that correspond to larger singular values are kept. this method of statistically representing knowledge has proven to be useful in a range of studies. for instance  studies have shown that lsa performs as well as students on toefl  test of english as a foreign language  tests llandauer and dumais  
1   that it grades essays as reliably as humans  landauer ct al  1a  and that it reliably measures the coherence between a sentence and successive sentences  foltz ct ah  1 . finally  lsa has successfully been used in intelligent tutoring systems like autotutor  gracsser ct ai  1; 1 . 
1 autolwor 
autotutor is a conversational agent that assists students in actively constructing knowledge by holding a conversation in natural language with them  graesser et ai  1   in addition to latent semantic analysis  at least four other com-

poster papers 	1 

ponents can be distinguished: 1  a dialog management system guides the student through the tutor-student conversation. fuzzy production rules and a dialog advancer network form the basis of these conversational strategics; 1  curriculum scripts organize the pedagogical macrostructure of the tutorial. these scripts keep track of the topic coverage and follow up on any problems the student might have; 1  a talking head with facial expressions and synthesized speech is used for the interface. parameters of the facial expressions and rudimentary gestures are generated by fuzzy production rules; 1  mixed-initiative dialog  including the appropriate use of discourse markers to make the conversation smoother; a speech act classifier that accounts for the pragmatics of incoming expressions; and a question answering tool that dynamically answers student questions on a variety of topics. 
¡¡auto tutor was originally developed for computer literacy. over the last two years a web version of autotutor was developed that tutors in conceptual physics. in both domains world knowledge is provided by lsa spaces of domain specific text books. 
1 	an improved lsa algorithm 
at the beginning of a session  autotutor presents a question to the student  and the student responds to this question. autotutor analyzes the accuracy of this answer by comparing the student's answer with a series of expected ideal answers. over the course of the tutoring session  the student covers each of the expectations. the tutor allows the student to move to the next problem only once all expectations arc covered. 
   although possible in theory  a single contribution from a student usually does not cover all expectations at once. instead  the student simply types one sentence at a time in the conversation with the tutor. based on the student's responses  the tutor will then provide appropriate feedback based on the quality of responses. to provide feedback to a student's contributions  the tutoring system needs to know the following: 
1. information related to the expected answer elements that is 1  new  what was not in the previous contributions ; 1  old  what was in the previous contributions  
1. information not related to the expected answer elements that is 1  new  what was not in the previous contributions ; 1  old  what was in the previous contributions  
   depending on these four components  autotutor chooses the most appropriate feedback. this mechanism for student contributions is illustrated in table 1. for example  autotutor needs to provide highly positive feedback when students provide new relevant information  cell labeled ++ . for relevant but repeated information  cell labeled +   autotutor needs to provide only some non-negative feedback. for irrelevant contributions  autotutor needs to point out the repeated misconceptions  eventually with negative feedback  cell with -   . the system returns non-positive feedback in cases of irrelevant information occurring the first time  cell with - . 
¡¡the challenge for autotutor is to obtain information that belongs to each of the cells in the table. that is  the system needs to take into account both the relevance of the information and whether or not the information is new. 
type of feedback relevant irrelevant new 
old + + + -table 1: four types of feedback the system provides on the basis of relevance and newness of student contribution. 
  question: suppose a runner is running in a straight line at constant speed and throws a pumpkin straight up. where will the pumpkin land  explain why. 
  expectation: 	the pumpkin will land in the runner's hands. 
  student contributions: 
  1   1 think  correct me if i am wrong  it will not land before or behind the man. 
 1  the reason is clear  they have the same horizontal speed. 
 1  the pumpkin will land in the runner's hand. 
 1  did i say anything wrong  
 1  come on  i thought i have said that. 
old lsa new 
infor. new 
contribution new 
lsa  1  1 1% 1 1  1  1 1% 1 1  1  1 1% 1 1  1  1 1% 1 1 1 1% 1 1  1  
table 1: example of student contribution and evaluation based on two lsa methods 
   in earlier versions of the system  autotutor put all the student contributions  from the first response to the most recent response  together as one document and would then compare with the expectation. one of the reasons for this was that it has often been claimed that the best performance in lsa comes from paragraphs rather than sentences  see  foltz et al.  1  . however  simple vector algebra shows that vector summation of term-vectors for the combined contributions may in fact reduce the similarity between the expectation and contributions when contributions are added. this reduction is evident in the example given in second column of table 1. the tutor asks the student a question at the start of a conceptual physics problem. the student's answer is matched with an ideal expected answer. the question now is what happens to the lsa coverage scores if the student submits  new/old   relevant/irrelevant  multiple contributions. 
   from the expected answer we know that a student's answer like  1  is almost correct. now imagine that the student answers  1 . the cosine match drops  resulting in autotutor asking for more information. now assume that the student also answers  1   which is the exact ideal answer. using a traditional vector addition algorithm  the similarity is not 1. this loss of precision results from the noise introduced by the irrelevant information in the student's answers. by adding the contributions' vectors  the system cannot distinguish be-

1 	poster papers 

tween the different parts  new/old   relevant/irrelevant  of the student's contributions. so although lsa effectively compares semantic similarities between two large documents  lsa lacks precision for comparing smaller documents in a 
progressive sequence. under a vector-addition model  a student whose answers improve dramatically over the course of the tutoring session is  penalized  for an initial bad answer. to solve this limitation  we propose an alternative solution. instead of simply combining contributions into a larger document  each contribution is treated as  independent  in a vector subspace. the combination of the contributions is then not represented as a simple vector summation  but instead as a span in the subspace. this way  the vector for any new contribution can be algebraically decomposed into two components. one component is the projection of the vector to the spanned subspace of the previous contributions  and the other component is perpendicular to the subspace. these two components of the most recent contribution correspond to relevant information  projection to the subspace  and new information  the perpendicular component . finally  the cosine match of the new information with the expectation is the measure of new  additional  coverage of the expectations. we applied this method to the example such as table 1 and observed desirable increase in lsa's precision  as illustrated in column 1  and 1 of table 1. the rows of table 1 present the five student contributions. the column 'new info' gives the percentage of new information for each of the contributions  compared to the previous contributions. the third column is the relevance of the new contribution to the span. the final two columns give the old and new lsa scores. 
¡¡although contribution  1  is identical to the expectation  it still is only 1% new. the reason is that contributions  1  and  1  already contain some of the information from  1 . for example  although  1  contains 1% new information  it has only marginally  1  contributed as coverage. notice that the quality in the subsequent student contributions does not deteriorate  but the old lsa values do. the new lsa values  on the other hand  account for additional relevant information  even bringing the coverage score to the maximum value of 1. 
¡¡the method provided here can be used to compute all four cells in table 1  because it differentiates whether the information is new or old  and whether it is relevant or not. furthermore  since it provides information at every step  numerical information of the values can be used to provide secondary information for feedback. for example  the rate of increase in the new lsa algorithm provides us with information on the development student performance on a step-by-stcp basis. by being able to localize lsa scores  autotutor can now determine the effectiveness of its dialog moves. 
¡¡the proposed new algorithm can potentially be used in applications like essay grading  where the student's composition covers the key elements for a given essay. the algorithm can measure development of student performance and can take into account whether information is old or new  relevant or irrelevant. 
1 	conclusion 
this paper addressed the use of latent semantic analysis in intelligent tutoring systems like autotutor. despite the success of lsa in autotutor  previous versions were not able to differentiate between relevant/irrelevant or new/old information in student contributions. replacing the vector-addition based algorithm with a span-based algorithm does not only improve autotutor's evaluation of student contributions  but is most likely to improve lsa performance in a wide range of other natural language understanding applications. 
