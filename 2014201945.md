# 基于Libsvm的Scigen假论文识别鉴定
---------------------------------------------------------------
#### 一.基本思路
本次试验在ubuntu14.04上基于linux系统下的python进行,主要思路为:
1.对从老师处获取的ijcai论文集内容使用pdf转换器在windows下批量转换为*.txt格式,该转换器自动将图片部分删除,但保留数学公式部分
2.使用网络爬虫从scigen上爬取一定量的假论文,转换为txt格式
3.在linux系统下编写脚本,使用正则式表达规则进行预处理和分词,包括了单词的小写化,标点符号的去除和数字的去除
4.对每篇经过预处理的文章,使用词频统计的方法将词频前300的单词词频按降序排列在同一行之中,词频之间用逗号进行分隔,对所有真论文,其结果存于一个文件;对假论文,结果存于另一个文件
5.使用libsvm进行训练建立模型,真论文label为1,假论文label为-1,保存模型
6.使用已有模型对经预处理的新论文进行识别,label不为1或-1的情况下将通过给出label的方式指明论文类型,否则将给出准确率

#### 二.理论依据
- 由scigen生成的假论文,其内容为语句随机组合而来,相较于真论文而言其核心词汇的词频与其他词汇词频的差异性不明显,甚至于会出现无核心词汇的情况
- libsvm是支持向量机(Support vector machine)的一个linux版本库,提供了根据固定格式输入进行模型的训练,保存和根据已有模型对新输入数据的预测功能,其输入格式如下:
> Label 1:value 2:value ….#label为标签,表示类;value为对应的训练数据特征值
- 给定已有模型和新的输入数据,可以通过计算余弦相似度判断文本是否属于已有类别,这是文本分类的常用方法,可以应用于scigen假论文的判定

#### 三.实验步骤
1.dive.py:同时处理已存为.txt格式的真假论文
使用string模块进行小写化处理,re模块正则表达式进行内容的过滤,方法如下:
```
for eachline in f:
        eachline = eachline.lower()
        for line in re.split('[\s]',eachline): 
            for c in string.punctuation:
		line=line.replace(c,'')
	    for c in string.digits:
		line=line.replace(c,'')
            if not (line in stopList):
                myList.append(line)
        strs = " ".join(myList)
        f1.write(strs)
        myList = []
    f1.write('\n')
```
其中,stopList为停用词表,从网上查找英文常用停用词表所得
示意处理结果如图

![](https://github.com/LycxRUC/Slice/raw/master/Image/output_t.png)

2.count.py:同时生成真假论文集的词频降序排列文件
利用collections模块中的函数,逐一打开上一步中的输出文件夹中的.txt文件,统计词频并降序排列,取词频前300的单词词频存为输出结果的一行
```
def count(path):
	result = {}
    	with open(path) as file_obj:  
        	all_the_text = file_obj.read()
	for word in all_the_text.split():  
            if word not in result:  
                result[word] = 0  
            result[word] += 1   #词频统计
              
        return collections.OrderedDict(sorted(result.items(), key = lambda t: -t[1]))	#根据词频降序排列
```
对假论文集处理后的输出文件如下所示

![](https://github.com/LycxRUC/Slice/raw/master/Image/count_f.png)

3.train.py:根据保存的特征值进行libsvm模型的训练及本地化保存
这里需要导入libsvm模块，命令为
> from svmutil import * 

导入以后,分别逐行读取两个count文件中的数据,每一行存为一个列表,同一个count文件中的全部行构成一个大列表,另有其元素数的1/-1单独存于一个列表,表示该元素对应真/假论文
```
for line in f1.readlines():  #逐行取文件,转为整数列表,并将各列表存为一个大列表
		arr=[]
		arr.extend(line.replace(' ','').replace('\n','').split(','))
		if arr[0]!='':
			arr = map(int, arr)
			lists1.append(arr)
			answer1.append(1) #列表多一个元素则标签列表多一个标签,假论文对应-1
```
```
    lists1.extend(lists2)  #真假论文的标签和特征值合并为一个,通过svm_problem(label,value_set)函数转为输入格式
	answer1.extend(answer2)	
	prob=svm_problem(answer1,lists1)
	param=svm_parameter()
	m=svm_train(prob,param)  #训练模型
	svm_save_model("paper.model",m) #模型保存
```
一个训练结果如下:

![](https://github.com/LycxRUC/Slice/raw/master/Image/train_1.png)
![](https://github.com/LycxRUC/Slice/raw/master/Image/train_2.png)



> 参数含义:
> svm_type #所选择的svm类型，默认为c_svc
> kernel_type #训练采用的核函数类型，默认为RBF核
> gamma #RBF核的参数γ
> nr_class #类别数，此处为两分类问题
> total_sv #支持向量总个数
> rho #判决函数的偏置项b
> label #原始文件中的类别标识
> nr_sv #每个类的支持向量机的个数
> SV #各个类的权系数及相应的支持向量

可以得知,**论文的长度越长**,总数目较多时,能够更好地体现出真假论文之间的差距,这是因为人写作的论文和scigen生成论文之间关键词与一般词汇的词频差距会随着文章篇幅增加而趋于明显

4.judge.py:对新给定论文,通过已有模型进行分析判定
对一篇以artificial intelligence为作者名生成的scigen论文进行svm_predict结果如下：

![](https://github.com/LycxRUC/Slice/raw/master/Image/predict.png)

#### 四.反思总结
- 本次报告中我所使用的是通过关键词与其他一般词汇词频差别进行scigen假论文识别的方法，并没有考虑到通过抄袭真论文完整句子组成假论文的甄别，如果需要对这些更复杂的情况进行研究，应该通过word2vec进行词向量生成，并通过把词频最高的部分单词词向量作为特征值进行libsvm下的训练，从而实现语义相关文本分类
- 这一学期的上机课由于我自己的原因基本没能去听讲，错过了不少东西导致进度缓慢以及多次走进死胡同，这将成为以后的一个教训
- 在学习过程中，我通过查询资料更好的掌握了python以及linux的用法，对以前几乎没有了解的人工智能领域有了些许的认识，这将会给我更多的动力去钻研和掌握新事物

 *由于我并不会写网络爬虫，实验所用的假论文语料使用了佘天唯同学爬取的一千篇经过了txt化及分词处理的假论文

> **参考：**
> [1]*LibSVM学习详细说明 ( http://blog.csdn.net/zy_zhengyang/article/details/45009431 )*
> [2]*使用libsvm实现文本分类 ( http://shiyanjun.cn/archives/548.html )*
> [3]*Python官方文档 ( https://www.python.org/doc/ )*
