    Automatically Selecting Answer Templates to Respond to Customer Emails
                    Rahul Malik, L. Venkata Subramaniam+ and Saroj Kaushik
                              Dept. of Computer Science and Engineering,
                                  Indian Institute of Technology, Delhi,
                                     Hauz Khas, New Delhi, India.

                                       +IBM India Research Lab,
                                 Block I, Indian Institute of Technology,
                                           New Delhi, India.
                    Abstract
                                                       Please provide the current status of the rebate
    Contact center agents typically respond to email   reimbursement for my phone purchases.
    queries from customers by selecting predeﬁned an-
    swer templates that relate to the questions present I understand your concern regarding the mail-in rebate.
    in the customer query. In this paper we present a  For mail-in rebate reimbursement, please allow 8-14
    technique to automatically select the answer tem-  weeks to receive it.
    plates corresponding to a customer query email.    I understand your concern regarding the mail-in rebate.
    Given a set of query-response email pairs we ﬁnd   For mail-in rebate reimbursement, please allow <replace
    the associations between the actual questions and  this> (***weeks or months***) </Replace this> to receive
    answers within them and use this information to    it.
    map future questions to their answer templates.
    We evaluate the system on a small subset of the      Figure 1: An Example Query, Response/Template Pair
    publicly available Pine-Info discussion list email technique that tries to automatically handle the email queries
    archive and also on actual contact center data com- needs to use natural language processing to handle it.
    prising customer queries, agent responses and tem-  Today’s contact centers handle a wide variety of domains
    plates.                                           such as computer sales and support, mobile phones and ap-
                                                      parel. For each of these domains typical answer templates are
1  Introduction                                       deﬁned and provided to the agents to aid them in composing
                                                      their responses. Contact centers have lots of such email data
Contact center is a general term for help desks, information corresponding to each of the domains they handle. The cus-
lines and customer service centers. Many companies today tomer queries along with the corresponding agent responses
operate contact centers to handle customer queries, where, to them are stored and available in abundance.
customers are provided email support from a professional In this paper we have presented techniques to automatically
agent. Contact centers have become a central focus of most answer customer emails by
companies as they allow them to be in direct contact with
                                                        1. Extracting relevant portions from customer queries that
their customers to solve product and services related issues
                                                          require a response
and also for grievance redress. A typical contact center agent
handles a few hundred emails a day. Typically agents are 2. Automatically matching customer queries to predeﬁned
provided many templates that cover the different topics of the templates to compose a response
customer emails. When an agent gets a customer query she We evaluate the performance of our technique and show
selects the answer template(s) relevant to it and composes a that it achieves good accuracy on real life data sets.
response that is mailed back to the customer. Figure 1 shows
an example query, response/template pair.             2   Background and Related Work
  The work ﬂow for processing emails at contact centers is as
follows. When a customer sends in a query by email a human 2.1 Key Phrase Extraction
manually triages it and forwards it to the right agent. The Extracting sentences and phrases that contain important in-
agent has a number of predeﬁned response templates from formation from a document is called key phrase extraction.
which she selects the most appropriate ones and composes Key phrase extraction based on learning from a tagged cor-
a response email to the customer query by combining the pus has been widely explored [Hirao et al., 2002][Frank et
templates and inserting additional text fragments. Incoming al., 1999]. The problem of extracting key phrases from emails
mails are in free text format with very little structured content using parts of speech tagging has also been studied [Tedmori
in them. Usually the structured content are email ﬁelds like et al., 2004]. We mention this body of work in the context of
date, subject line, sender id, etc., which anyway do not help in this paper because in a given email it is important to identify
triaging the email query or in answering them. Therefore, any the portions that correspond to questions and answers.

                                                IJCAI-07
                                                  16592.2  Text Similarity                                  the questions and the corresponding answers in the response
Text similarity has been used in information retrieval to deter- emails are identiﬁed from the given corpus. In this way even
mine documents similar to a query. Typically similarity be- multiple questions in a query email will get identiﬁed and ad-
tween two text segments is measured based on the number of dressed. Our technique shows good accuracy.
similar lexical units that occur in both text segments [Salton
and Lisk, 1971]. Similarity measures have been used to ﬁnd 3 Problem Deﬁnition
similar documents and sentences [Metzler et al., 2005].How-
                                                      We have a repository of matching query and response emails
ever, lexical matching methods fail to take into account the
                                                      that have been exchanged between customers and call center
semantic similarity of words. Hence, I remained silent and
                                                      agents. The responses have been composed out of a set of
I was quiet would not be matched. To overcome this sev-
                                                      templates which have been provided to the agents. A single
eral techniques based on measuring word similarity have been
                                                      query may comprise many questions thus requiring multiple
proposed. In their work, Wu and Palmer [Wu and Palmer,
                                                      templates to be used in answering it. The problem is that
1994] measure the similarity of words based on the depth
                                                      of matching the individual questions in the queries to their
of the two concepts relative to each other in WordNet 1.In
                                                      corresponding templates.
[Mihalcea et al., 2006] several corpus and knowledge-based
                                                        Given the repository of query-response pairs and tem-
measures have been evaluated for measuring similarity based
                                                      plates, the ﬁrst problem is to learn the associations between
on the semantics of the words. In the context of this paper
                                                      questions and templates. Let {Q1,...,Qm} be the query
we need to identify similar questions and we need to match a
                                                      emails and {R1,...,Rm} be the corresponding responses.
question to its answer.
                                                      A query Qi comprises the questions {ql,...,qm} which are
2.3  Question Answering                               matched with templates {tl,...,tm} used to compose the
                                                      response Ri. The problem can now be deﬁned as follows:
Question Answering (QA) has attracted a lot of research. The
                                                      Given a query email Qi we need to ﬁnd the set of questions,
annual Text REtrieval Conference (TREC) has a QA track qs, in it and then match these to the corresponding templates,
where text retrieval systems are evaluated on common, real-
                                                      ts, that are used to compose the response Ri. Now that the
world text collections. The aim of a system in the TREC QA associations have been established we can tackle the problem
task is to return answers themselves, rather than documents of automatically composing responses for new queries. When
                                      [
containing answers, in response to a question Voorhees and a new query comes in, we need to identify the questions in it
         ]
Dang, 2006 . In essence what we are attempting to do in this and then compose a response for it using the mappings that
paper is to answer the customer queries by looking at ways in have previously been established.
which similar questions have been answered in the past.
2.4  Contact Center Email Processing                  4   Email Triaging
A lot of work hasebeenedone on automatic triaging of emails In a contact center typically customer query emails are man-
in contact centers [Nenkova and Bagga, 2003][Busemann et ually triaged. In the triaging step, emails are forwarded to the
al., 2000]. In these a classiﬁer is learnt based on existing concerned agent handling the particular class of queries. In
cases and using features such as words in the mail, their parts some cases a single agent handles all the classes of queries
of speech, type of sentence etc. So when new queries come in and hence the emails are not actually classiﬁed. So the clas-
they are automatically routed to the correct agent. Not much siﬁcation information is not available. We replace this step
work has been done on automatically answering the email with automatic triaging. Since obtaining labelled data for a
queries from customers. It is possible to learn a classiﬁer that new process is hard, we use clustering to ﬁrst identify the
map questions to a set of answer templates [Scheffer, 2004]. different classes and then learn a classiﬁer based on these
Also there has been some work on automatically learning classes. We classiﬁed the emails from the larger pool into an
question-to-answer mappings from training instances [Bickel equal number of query and response clusters using text clus-
and Scheffer, 2004]. Our work in this paper describes meth- tering by repeated bisection using cosine similarity. Match-
ods to automatically answer customer queries by selecting re- ing query and response clusters could contain different num-
sponse templates.                                     bers of emails. So, they were subsequently matched based on
                                                      maximal matching criteria. The cluster having the maximum
2.5  Contributions of this Paper                      match fraction was separated and the remaining clusters were
In this paper we present techniques to automatically locate rematched. This way a one-to-one mapping of the query and
relevant questions in a customer query email and then map response clusters was obtained. The non-matching emails for
each of these to predeﬁned answer templates. By putting each map were removed to create clean clusters having exact
together these templates a response email message is com- correspondence of query-response emails. An SVM classiﬁer
posed for the given query email. We automate the process of was then trained on the classes created by the clustering. This
composing the response. We are not aware of work where classiﬁer is then used to triage the emails.
a response is generated by putting together existing tem-
plates. Our technique is novel in that all the questions in the 5 Extraction of Question-Answer Pairs
query email are ﬁrst identiﬁed and the associations between
                                                      Given a query email we would like to extract the key ques-
  1http://wordnet.princeton.edu/                      tions in it. Then we would like to map these questions to

                                                IJCAI-07
                                                  1660their answers in the response email and determine the tem- that query-email, N = number of questions in that query-
plates that have been used.                           email, M = number of answers in that response-email.
                                                        Each answer is then mapped to a template. This is done by
5.1  Identiﬁcation of Questions and Answers           simply matching the answer with sentences in the templates.
We identify the questions based on the presence of key The agent typically makes very little changes to the template
phrases within them. We follow the approach of [Frank et while composing an answer. As a result of these steps we
al., 1999] to identify the key phrases of length up to 3 words. obtain the question to template pairs.
We use a set of training documents for which key phrases are Multiple questions can match the same template because
known to generate a naive Bayes model. This model is then different customers may ask the same question in differ-
used to ﬁnd key phrases in a new document.            ent ways. Hence, for example, while what is my account
  In actual fact the tagged training documents were not avail- balance and can i know the balance in my account can be
able. So we ﬁrst extracted the most frequent unigrams, bi- merged into one sentence, i need to know the carry forward
grams and trigrams and selected from this list. These ex- from last month cannot easily be linked with the earlier sen-
tracted phrases of length up to 3 words were then marked in tences. Hence while the mapping method gave the question-
the emails as being the key phrases. This tagged set was used to-template mappings, we need to merge similar sentences
to generate a naive Bayes model for the key phrases.  and keep the minimal set of questions associated with a tem-
  In a query email the questions are identiﬁed as those sen- plate. We prune the set of questions by removing questions
tences that contain key phrases in them. Similarly in the re- that have a very high similarity score between them. So that
sponse email, the replies are identiﬁed as those sentences that when a new question comes in, we only need to compare it
contain key phrases in them.                          with this minimal set.
5.2  Mapping Questions to Answers                     6   Answering New Questions
Once the questions and responses have been identiﬁed we
                                                      When we get a new query we ﬁrst identify the questions in
need to map each question to its corresponding response. A
                                                      it by following the method outlined in Section 5.1. Each of
given query email may contain multiple questions and there-
                                                      these questions now needs to be answered. To compose the
fore the response may contain multiple answers. To accom-
                                                      response email the answers to each of these questions are put
plish this mapping we ﬁrst partition each extracted sentence
                                                      together.
into its list of tokens. Stop words are removed and the re-
maining words in every transcription are passed through a 6.1 Mapping New Questions to Existing Questions
stemmer (using Porter’s stemming algorithm 2) to get the root
form of every word e.g. call from called. Now the similarity When a new question comes in we need to determine its sim-
between each question and answer is calculated to ﬁnd the ilarity to a question we have seen earlier and for which we
correct map. In order to calculate the overlap between the know the template. From the previous discussion we know
two sentences, we used Word Overlap Measure along with that many different questions can map to the same answer
the score being adjusted to take into account the inverse doc- template. If we are able to determine that the new question
ument frequency [Metzler et al., 2005]. The measure is given is similar to an existing question then we are done. The new
as:                                                   question is mapped to the template for the existing question
                                                      to which it is similar.
                   | ∩  |                              The similarity between two concepts is given as [Wu and
           (   )=  q   r (    (   (      ))
        sim q, r     | |       log N/dfw              Palmer, 1994]:
                     q   w∈q∩r
                                                                                      (    )
                               | ∩  |                              (  )=2∗       depth LCS
where, N = total number of words, q r = collection of          Sim  s, t     [     ( )+      ( )]
common words of q and r after stop-word removal and stem-                     depth s   depth t
ming, df w = document frequency of w, where w belongs to where, s and t denote the source and target words being
the common word set of q and r                        compared. depth(s) is the shortest distance from root node
  This gives us a score of similarity between question and to a node s on the taxonomy where the synset of s lies LCS
answer. In addition, we used a heuristic that if a question denotes the least common subsubmer of s and t. We are using
is asked in the beginning, then the chances that its response Wordnet.
would also be in the beginning are more. So, we biased them Further we need to compute the similarity between the con-
with a high weight as compared to those lying in the end cepts present in the sentences that we want to match. The
which were penalized with a low weight.               two sentences are ﬁrst tokenized, stop words are removed and
  So, the expression for score becomes:               Porter stemmed. Parts of speech labels are assigned to the to-
                                 ( )       ( )        kens. Nouns, verbs, adjectives and adverbs were selected. In
       (  )=     (   ) ∗ (1 −| pos q −  pos r  |)
  score q, r  sim q, r       (  +1)    (   +1)        addition, we also keep the cardinals since numbers also play
                              N         M             an important role in the understanding of text. We form 5
where, sim(q, r) = similarity score as obtained from above, matrices, one for each of the ﬁve classes. The rows of the
pos(q) = position of the Question in the set of Questions of matrix are the tokens from one sentence and the columns are
                                                      the tokens from the second sentence. Each entry in the matrix
  2http://www.tartarus.org/ martin/PorterStemmer      Sim(s, t) denotes the similarity as it has been obtained above

                                                IJCAI-07
                                                  1661for the pair. Also, if a word s or t does not exist in the dictio- I am working on creating an rpm for our university, and
nary, then we use the edit distance similarity between the two I need to put pine.conf into /usr/local/etc/pine.conf
words.                                                 instead of /etc/pine.conf. I am wondering if there are any
  The results from these are combined together in the follow- extra parameters that I can pass at build time to redirect pine to
ing manner to obtain the overall similarity score between the /usr/local/etc/pine.conf. If there isn’t a parameter that I can pass does
two sentences                                          anyone know of a patch that would do something like that?
  The similarity between two sentences is determined as fol- Yes, what I would recommend you is that you take a look at the build script.
lows [Mihalcea et al., 2006]:                          The port for RedHat, includes a "ALTDOCPATHS = 1", which
                                                     defines a few extra variables, in particular look at the
                                                       -DSYSTEM PINERC="/etc/pine.conf" and so on. That should
                        m(   j)+           m(    i)
                s∈qi Sim  s, q      s∈qj Sim  s, q     give you an idea of what to do.
Score(qi,qj)=
                            |qi| + |qj|
                                                        Figure 2: A typical Query, Response Pair from Pine-Info
where, Simm(s, qj) is the word in qj that belongs to the same
class as s (noun or verb, etc.) and has the highest semantic I signed up for wireless web 2 or 3 months ago and have
similarity to the word s in qi.                        been charged $5.00 each bill. This time it was $25.00.
  Using the above similarity we compare the new question What is the problem?
with the questions seen earlier. When a match is found we
select the response template of the matched question as the I am sorry for the inconvenience caused to you. I have
template for the new question.                         adjusted your account for $20. This will be reflected
                                                       on your May bill.
                                                       I am sorry for the inconvenience caused to you. I have
7  Evaluation                                          adjusted your account for <replace this> (***$INSERT
                                                       AMOUNT or MINUTES ***) </Replace this>. This will be
In this section we present the results over real life data sets. reflected on your <replace this> (***month***) </Replace
                                                       this> bill.
7.1  Contact Center Data
We chose two sets of data to work on. The Pine-Info discus- Figure 3: A Typical Query, Response/Template Pair
sion list web archive 3contains emails of users reporting prob-
lems and responses from other users offering solutions and 7.2 Measures
advice. The questions that users ask are about problems they
face in using pine. Other users offer solutions and advice to We measure the accuracy of the system by comparing the
these problems. The Pine-Info dataset is arranged in the form email response generated by a human agent with that gen-
of threads in which users ask questions and replies are made erated by our system. We use two criteria for comparison.
to them. This forms a thread of discussion on a topic. We In the ﬁrst case, we say that the system is correct only if it
choose the ﬁrst email of the thread as query email as it con- generates the exact answer as the agent. In the second case
tains the questions asked and the second email as the response we allow partial correctness. That is if the query contains
as it contains responses to that email. It may not contain an- two questions and the system response matched the agent re-
swers to all the questions asked as they may be answered in sponse in one of them then the system is 50% correct for that
subsequent mails of the thread. We randomly picked a total query.
of 30 query-response pairs from Pine-Info. The question sen- 7.3 Experiments
tences and answer sentences in these were marked along with
the mappings between them. On average, a query email con- In this section we show the results of our techniques both on
tains 1.43 questions and the ﬁrst response email contains 1.3 the pine dataset and the contact center emails. In pine there
answers and there are 1.2 question-answer pairs. We show are no templates. So in effect we are only testing whether
a query and response pair from Pine-Info in Figure 2. The we are able to map the manually extracted questions and an-
actual question and answer that have been marked by hand swers correctly. We used the 30 annotated pairs of query and
are shown in bold. In the example shown two questions (two response emails to measure this. As already mentioned we
sentences) have been marked in the query email but only one are looking for an answer in the ﬁrst response to the query.
answer is marked in the response email.               Hence, all questions in the query may not get addressed and
  The second data set used was obtained from an actual con- it may not be possible to get all the mappings. In Table 1 we
tact center process relating to mobile phone services. We ob- show the numbers obtained. Out of a total of 43 questions
tained 1320 query and response emails relating to the mobile only 36 have been mapped to answers in the manual annota-
phone support process. We also obtained the 570 answer tem- tion process. Using the method presented in Section 5.2 we
plates that the agents are provided to aid them in answering are able to ﬁnd 28 of these maps correctly.
the customer queries. On an average each response contains For the contact center emails we tested whether our method
2.3 templates, implying that there are about two key questions could accurately generate the responses for the queries. We
per customer query. In Figure 3 a query is shown along with built our system using 920 of the 1320 query-response pairs
its actual response and the template used in composing the of emails available to us. We then tested using the remaining
response.                                             400 pairs of emails. We ran two sets of experiments on these
                                                      emails. In the ﬁrst case we didnt use class information to gen-
  3http://www.washington.edu/pine/pine-info           erate the response emails and in the second case we triaged

                                                IJCAI-07
                                                  1662Table 1: Results on Pine-Info Dataset for Question-Answer Table 3: Results of Clustering Contact Center Dataset
Mapping                                                  Actual    descriptive total    total    templates
    total  total total actual correct  % correct          class     CLUTO       no.   templates    per
    mails  qns   ans   maps    maps     maps                         label    emails   in class   email
     30    43     39    36      28     77.78%            Rebate      rebate     62       48        2.1
                                                       Cancelling    cancel     55       42        3.4
                                                        Charges      charge     277      214       1.5
the emails ﬁrst to aid in the generation of templates.  Account     payment     238      194       2.2
  In Table 2 we show the results for generating the response payment
email without using any class information. Hence, in this General    phone      688      397       2.6
case, the question-to-template maps are established using the queries
920 training email pairs. For each template the questions as- Overall           920      570       2.3
sociated with them are found using the techniques of Section
5.2. For new questions in the testing set the response is au-
tomatically generated as discussed in Section 6.1. We gener- over 79% of the cases. Considering partial correctness this
ated the exact response that the human generated in 61% of accuracy goes up to about 85%. These results are shown in
the cases. Considering partial correctness this accuracy was Table 4.
about 73.4%.                                            It is seen that there is a marked jump from 61% to 79%
                                                      in the accuracy numbers for complete match if the emails are
                                                      triaged. We do not notice such a large jump in the case of
Table 2: Results on Contact Center Dataset without Triaging partial correctness. The reasons for this could be twofold.
  total  training testing  total    %       %         Firstly, the step of triaging the emails limits the search space
  email    set      set   correct correct  partial    over which the questions need to be matched. Secondly, if
  pairs                    resp.   resp.  correct     a query lands up in a wrong cluster due to misclassiﬁcation
  1320     920     400     244      61     73.4%      then it stands no chance of being partially correct because that
                                                      particular class may not have the template for it.
  For the case of triaging the emails. We ﬁrst clustered the
920 query and response sets separately. We used CLUTO
package 4 for doing text clustering. We experimented with Table 4: Results Contact Center Dataset with Triaging
all the available clustering functions in CLUTO but no one CLUTO  total  classiﬁcation    %        %
clustering algorithm consistently outperformed others. Also, class no.     accuracy     correct   partial
there was not much difference between various algorithms label   emails                responses correct
based on the available goodness metrics. Hence, we used the rebate 16        87.6        82.1     86.4
default repeated bisection technique with cosine function as cancel 16       83.4        78.2     78.8
the similarity metric. The clustering results using CLUTO charge   74        88.9        74.2     85.4
are shown in Table 3. We chose to cluster into 5 clusters be- payment 74     91.6        83.4     88.7
cause other values for the total number of clusters resulted in phone 212    92.7        82.1     86.3
lower purity numbers in CLUTO. We also generated the top overall   400       89.4        79.1     84.8
descriptive feature for each cluster using CLUTO and found
this to be very close to the actual label for that class which Also it should be mentioned here that often there are ques-
was assigned by a human. We found that the average number tions from customers in reply to which an agent does not use
of templates varied quite a lot between the classes. Many tem- any of the existing templates but actually composes her own
plates were used in two classes or sometimes even more. We reply.
used libSVM 5 to train an SVM classiﬁer on the clusters that
we obtained. We used the rbf kernel with γ =0.00012207 8  Conclusions
ρ =  −0.121623. This classiﬁer was used for triaging the
emails. The resulting overall accuracy for the classiﬁcation In this paper we have introduced an automatic email response
was over 89%. In this case, the question-to-template maps generation technique. Given a corpus of query-response
are established using the training email pairs for each class email pairs and answer templates, we determine the typical
separately. For each template the questions associated with questions for the given answer templates. When a new query
them are found using the techniques of Section 6.2. For new email is presented we identify the questions in it and ﬁnd the
questions in the testing set the email is ﬁrst triaged using the nearest questions to these that had been identiﬁed along with
classiﬁer that has been learnt. Then the response is automati- their answer templates. The answer templates relating to the
cally generated as discussed in Section 6.3. The questions in questions are put together and composed into a response.
this email are only matched to questions from the same class. The accuracies obtained using the methods presented in
We generate the exact response that the human generated in this paper point to the fact that such a system is practically
                                                      possible and useful to a contact center agent. When compos-
  4http://glaros.dtc.umn.edu/gkhome/views/cluto       ing a response to a customer query the agent has to select
  5http://www.csie.ntu.edu.tw/cjlin/libsvm˜           from a few hundred templates. The system presented in this

                                                IJCAI-07
                                                  1663