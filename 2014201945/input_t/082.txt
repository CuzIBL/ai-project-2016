          Semi-Supervised Learning with Explicit Misclassification Modeling 

                                 Massih-Reza Amini, Patrick Gallinari 
                                   University of Pierre and Marie Curie 
                                 Computer Science Laboratory of Paris 6 
                             8, rue du capitaine Scott, F-75015 Paris, France 
                                     {amini, gallinari }@poleia.lip6.fr 

                     Abstract                          mixture component whereas unlabeled data may belong to 
                                                       any components. Using the Expectation Maximization (EM) 
    This paper investigates a new approach for training 
                                                       algorithm [Dempster et al., 1977], proposed approaches usu•
    discriminant classifiers when only a small set of la•
                                                       ally attempt to optimize the likelihood of the whole labeled-
    beled data is available together with a large set of 
                                                       unlabeled data. Starting from an initial labeling, these ap•
    unlabeled data. This algorithm optimizes the clas•
                                                       proaches proceed by computing at each E-step, tentative 
    sification maximum likelihood of a set of labeled-
                                                       labels for unlabeled data using the current parameters of 
    unlabeled data, using a variant form of the Clas•
                                                       the model and update in the M-step, these parameters us•
    sification Expectation Maximization (CEM) algo•
                                                       ing the estimated labels. Our departure point here is the 
    rithm. Its originality is that it makes use of both un•
                                                       work of [Amini and Gallinari-a, 2002] who proposed a semi-
    labeled data and of a probabilistic misclassification 
                                                       supervised discriminant algorithm using a variant form of the 
    model for these data. The parameters of the label-
                                                       CEM algorithm [Celeux and Govaert, 1992]. Discrimina•
    error model are learned together with the classifier 
                                                       tive approaches which attempt to estimate directly posterior 
    parameters. We demonstrate the effectiveness of 
                                                       class probabilities are often considered superior to generative 
    the approach on four data-sets and show the advan•
                                                       models which compute these posteriors after learning class 
    tages of this method over a previously developed 
                                                       conditional densities. Tests on different datasets in I Amini 
    semi-supervised algorithm which does not consider 
                                                       and Gallinari-/?, 2002] led to the same conclusion for semi-
    imperfections in the labeling process. 
                                                       supervised learning. Like for other methods, at each step of 
                                                       the algorithm, the model computes tentative labels for unla•
1 Introduction                                         beled data. We extend here the system by incorporating a 
                                                       model which takes into account label errors. This provides an 
In many real-life applications, labeling training data for learn•
                                                       unifying framework for semi-supervised learning and learn•
ing is costly, sometimes not realistic and often prone to error. 
                                                       ing with label noise. To our knowledge, this form of model 
For example, for many rapidly evolving data bases available 
                                                       has not been studied yet. We detail the algorithm for the case 
via the web, there is not enough time to label data for differ•
                                                       of a logistic classifier and give a convergence proof for the 
ent information needs. In some cases, like medical diagnosis 
                                                       general case. We then show experimentally that modeling 
or biological data analysis, labeling data may require very 
                                                       the stochastic labeling noise, increases notably the perfor•
expensive tests so that only small labeled data sets may be 
                                                       mance, especially when only small labeled datasets are avail•
available. In other cases, like object identification in images, 
                                                       able. This paper is organized as follows; we first make a brief 
noise is inherent in the labeling process. 
                                                       review of work on semi-supervised learning and learning in 
The statistician and pattern recognition communities were the 
                                                       the presence of label noise (section 2). In section 3 we present 
first to consider the problem of forming discriminant rules us•
                                                       the formal framework of our model and describe in section 4 
ing either partially classified or random misclassified training 
                                                       the semi-supervised approach we propose. Finally we present 
data in order to cope with this type of situation. 
                                                       a series of experiments on four data sets. 
More recently this idea has motivated the interest of the ma•
chine learning community and many papers now deal with 
this subject. The use of partially classified data for training, 2 Related work 
known as semi-supervised learning, has been the subject of 
intense studies since 1998 and more recently there has been a 2.1 Learning with both labeled and unlabeled data 
resurgence of interest for training with misclassified data also The idea of using partially labeled data for learning started 
called learning in presence of label noise.            in the statistician community at the end of 60\s. The seminal 
We consider here semi-supervised learning for classification. paper by [Day, 1969] presents an iterative EM-like approach 
Most approaches to this problem make use of a mixture  for learning the parameters of a mixture density under the as•
density model where mixture components are identified as sumption of multivariate normal components with a common 
classes. Labeled data are known to belong to exactly one covariance matrix for two group-conditional distributions. 


LEARNING                                                                                               555  Other iterative algorithms for building maximum likelihood    3 A semi-supervised probabilistic model for 
 classifiers from labeled and unlabeled data based on the same     mislabeling 
 type of assumption followed [O'Neill, 1978] [McLachlan and 
 Ganesalingam, 1982]. Some other authors have suggested        We consider here the problem of semi-supervised learn­
 updating procedures for no-normal group conditional densi­    ing. We start from the Logistic-CEM algorithm described in 
 ties using for example kernel methods for modeling mixture    [Amini and Gallinari-a, 2002]. It is a generic scheme in the 
 components iMurray and Titterington, 1978]. There has been    sense that it can be used with any discriminant classifier esti­
 considerably fewer work on discriminative approaches. In      mating the posterior class probabilities. The classifier is first 
 his fundamental paper on logistic regression, Anderson sug­   trained on labeled data, it then alternates two steps until con­
 gests modifying a logistic regression classifier to incorporate vergence to a local maximum of the Classification Maximum 
 unlabeled data in order to maximize the likelihood function   Likelihood (CML) criterion [Symons, 1981]. Unlabeled data 
 [Anderson, 1979].                                             are first labeled using the output of the current classifier. Clas­
                                                               sifier parameters are then learned by maximizing the CML 
The semi-supervised paradigm has been recently rediscov­
                                                               function computed using the known labels of labeled data and 
ered by the machine learning community. Most papers pro­
                                                               the current estimated labels for unlabeled data. In this algo­
 pose mixture density models for combining labeled and unla­
                                                               rithm, at each iteration, labels computed for unlabeled data 
beled data for classification. [Miller and Uyar, 1996] consider 
                                                               are considered as desired outputs. 
a mixture density model where each class is described by sev­
eral component densities. [Roth and Steinhage, 1999] pro­      We suppose here that labels from the labeled dataset are cor­
pose kernel discriminant analysis as an extension to classical rect and that at each step of this algorithm, labels computed 
linear discriminant analysis. This framework can be used also  for unlabeled data are subject to error. We propose to model 
for semi-supervised learning. fNigam et al., 2000] propose a   the imperfection of these labels using a probabilistic for­
semi-supervised EM algorithm which is essentially similar to   malism and to learn the semi-supervised classifier by taking 
the one in [McLachlan, 1992] but makes use of naive Bayes      into account its labeling errors according to this error model. 
estimator for modeling the different densities. They present   Parameters of the error model and of the classifier will be 
empirical evaluation for text classification tasks. Some au­   learned simultaneously in this new algorithm. Compared to 
thors make use of discriminant classifiers instead of model­   the baseline algorithm [Amini and Gallinari-a, 2002], we ex­
ing conditional densities. For example [Joachims, 1999] pro­   plicitly take into account the fact that the current classifier is 
pose a transductive support vector machine which finds pa­     not optimally trained at each step, since many data labels are 
rameters for a linear separator using both the labeled data in missing and are only estimated. 
the training set and the current test data whose class is un­  In the following we present the general framework and the 
known. iBlum and Mitchell, 1998] introduce the co-training     learning criterion of our model. 
paradigm where each sample x is supposed to be described 
                                                               3.1 General framework 
by two modalities. Two classifiers are then used, one for each 
modality, operating alternatively as teacher and student. This We suppose that each example belongs to one and only one 
framework can be used for unsupervised and semi-supervised     class and that there are available a set of n labeled exam­
learning. Based on the multi-modal framework introduced        ples, D1 and a set of m unlabeled examples, Du. A discrim­
by [Blum and Mitchell, 1998], [Muslea et al., 2002] propose    inant classifier is to be trained on the basis of these n + ra, 
to combine both active and semi-supervised learning. Since     d-dimensional feature vectors1. For each labeled ex­
that, different authors have proposed semi-supervised learn­   ample xt in /)/, let ct and be respectively the 
ing schemes, they usually follow one of the above ideas.       class label and the indicator vector class associated to xt : 

2.2 Learning with imperfectly labeled data                     We suppose that for each unlabeled example Xi in Du, there 
                                                               exists a perfect and an imperfect label respectively denoted ci 
                                                               and ci. We propose to model the imperfections in the labels 
Practical applications of pattern recognition, like eg image   by the following probabilities: 
classification problems, have motivated in the early 80\s some 
work on the problem of learning in presence of mislabeled 
                                                                                                                      (1) 
data for fully supervised learning. [Chittineni, 1980] ob­
                                                               Which are subject to the constraint: 
tained error bounds on the performance of the Bayes and 
nearest neighbor classifiers with imperfect labels. [Krish-                                                           (2) 
nan, 1988] considered a 2-class classification problem for 
two group multivariate normal mixture when training samples 
are subject to random misclassification and derived the likeli­ In order to simplify the presentation, we consider in the fol­
hood estimation of parameters. [Titterington, 1989] proposed   lowing a two-class classification problem. 
a logistic-normal distribution model and worked out an EM 
algorithm for the estimation of its parameters. More recently, 
                                                               This is not restrictive since we can easily extend the analysis 
[Lawrence and Scholkopf, 2001] proposed an algorithm for 
                                                               to multi-class cases. 
constructing a kernel Fisher discriminant from training ex­
amples in the presence of label noise.                            'Components of x could also be discrete 


556                                                                                                           LEARNING  3.2 Classification Maximum Likelihhod estimation 
       in presence of label noise 
 The baseline semi-supervised discriminant algorithm has 
 been conceived as an extension to classification problems of 
 the CEM algorithm proposed by fCeleux and Govaert, 1992] 
 for clustering. CEM has been first proposed for learning the 
 parameters of gaussian mixtures using as training criterion 
 the classification maximum likelihood criterion initially de­
 scribed in [Symons, 1981]. 
 McLachlan has extended CML and CEM to the case where 
 both labeled and unlabeled data are used for learning 
 ([McLachlan, 1992], page 39). The CML criterion in this 
 case is the complete-data likelihood and writes: 


 The algorithm proposed by [McLachlan, 1992] optimizes this 
 criterion for a mixture of normal densities. In their algorithm 
 for training discriminant classifiers on labeled and unlabeled 
 data, [Amini and Gallinari-a, 2002] make use of the following 
 form of the log-classification-likelihood: 

                                                               4 Updating a discriminant function on the 
                                                                   basis of labeled and imperfect labeled data 
                                                               We now present an iterative discriminant CEM algorithm for 
                                                               learning a classifier for semi-supervised learning, which in­
 This writing makes apparent the posterior probabilities which corporates the mislabeling error model. The training criterion 
 are directly estimated in their algorithm instead of the condi­ is (6). For simplification, we consider a simple logistic classi­
 tional densities in [McLachlan, 1992]. As no assumptions      fier [Anderson, 1982], but the algorithm can be easily adapted 
 are made on the distributional nature of data, maximizing Lc  for training any discriminant classifier. Consider i a logistic 
 is equivalent to the maximization of L'c ([McLachlan, 1992],  classifier with parameters The output of 
 page 261). 
                                                               for input.. After 
                   n'2 
                                                               have been learned, are respectively used 
                                                               to estimatebe the 
                                                        (3) 
                                                               current partition for the unlabeled data, the param­
                                                               eters for the misclassification model and the logistic classifier 
                                                               at iteration p of the algorithm. The learning criterion (6) is a 
                                                               function of . An iterative approach is then adopted 
 Let us now introduce our misclassifiction model. For that, 
                                                               for its maximization (algorithm 1). Parameters are first ini­
 we will express the I in the second summation 
                                                               tialized by training the classifier on the labeled dataset D . 
 in (3) as a function of the mislabeling probabilities and of the                                                      1
                                                               Two steps are then iterated until the convergence of criterion 
 posterior probability of correct labels . Consider 
                                                               L'c. In the first step, the classifier is considered as an imper­
                                                               fect supervisor for the unlabeled data. Its outputs, G(x) and 
                                                               1 - G(x), are used to compute the posterior imperfect class 
                                                               probabilities p(c — k \ x) for each x in Du and , x 
                     can be decomposed with regard to the      is then labeled according to the maximum imperfect output. 
conditional class probabilities:                               In the second step, the parameters of the error model and of 
                                                               the classifier are updated using the imperfect labels obtained 
                                                          I    in the previous step as well as the labeled data. We adopted in 
                                                               this step, a gradient algorithm to maximize (6). An advantage 
Following [Chittineni, 1980] we make the assumption that the 
                                                               of this method is that it only requires the first order deriva­
density of an example, given its true label, does not depend 
                                                               tives at each iteration. In the following lemma, we provide a 
on its imperfect label: 
                                                               proof of convergence of the algorithm to a local maximum of 
                                                               the likelihood function for semi-supervised training. 


LEARNING                                                                                                             557                                                              adopt the text-span extraction paradigm which amounts at 
                                                             selecting a subset of representative document sentences. This 
                                                             is a classification problem where sentences are to be classi­
                                                             fied as relevant or non relevant for the extracted summary. 
                                                             All four problems are two-class classification tasks, Email 
                                                             Spam and Cmp.lg have continuous attributes, Credit 
                                                             vectors are mixed continuous and qualitative, Mushroom 
                                                             attributes are qualitative. 
                                                             For each dataset, we ran 3 algorithms - the semi-supervised 
                                                             learning algorithm using the label imperfections, the baseline 
                                                             semi-supervised algorithm [Amini and Gallinari-o, 2002] 
                                                             and a fully supervised logistic classifier trained only on the 
                                                             available labeled data. We compared the performance of 
                                                             these algorithms on 20 runs of arbitrary training and test 
                                                             splits, by varying the proportion of labeled-unlabeled data on 
                                                             the training set. 
                                                             For text summarization, we represent each sentence using 
                                                             a continuous version of features proposed by [Kupiec 
                                                             et al., 19951. This characterization has given good re­
                                                             sults in previous work [Amini and Gallinari-b, 2002]. 
                                                             Each sentence i, with length l(i) is characterized by 


Finally as there is a finite number of partitions of the example 
into 2-classes, the increasing sequence 
takes a finite number of values and thus, converges to a sta­ 5.2 Evaluation measures 
tionary value. ■                                             For the three UCI datasets there is approximately the same 
In the following section we will present results on four     proportion of examples for each class (table 1). We used as 
datasets, using a baseline logistic classifier trained with the 
                                                             performance criterion the percentage of good classification 
updating scheme presented above. 
                                                             (PGC) defined as: 
5 Experiments 
5.1 Data sets 
In our experiments we used the Spambase, Credit              For text summarization, we followed the SUMMAC evalua­
screening and Mushroom collections from the                  tion by using a 10% compression ratio. Hence, for each doc­
UCI repository   as well as the Computation and              ument in the test set we have formed its summary by select­
               2                                             ing the top 10% sentences having higher score with respect 
Language (Cmp.lg) collection of TIPSTER SUMMAC3 
for text summarization. Table 1 summarizes the characteris­  to the output of the classifier. For evaluation we compared 
tics of these datasets. We removed 37 samples with missing   these extractive sentences with the desired summary of each 
attributes from the Credit data set. For summarization, we   document. The desired extractive summaries were generated 
                                                             from the abstract of each article using the text-span align­

   2ftp://ftp.icsS.uci.edu/pub/machine-leaming-databases/    ment method described in [Banko et al., 1999]. Since the 
   3 http://www.itl. nist.gov/iaui/894.02/related-projccts/tipster_summac/ collection is not well balanced between positive and negative 


558                                                                                                        LEARNING Figure 1: Performance curves for the 4 datasets showing, for a classical logistic classifier trained in a fully supervised scheme 
(square), the baseline semi-supervised Logistic-CEM algorithm (circle) and our semi-supervised algorithm with label imper•
fections (star). Each point represents the mean performance for 20 arbitrary runs. The error bars show standard deviations for 
the estimated performance. 

examples, PGC is meaningless, we used the average preci•     1996]. 
sion (AP) measure for the evaluation. Let be the number      All figures exhibit the same behavior. In all four datasets, 
of sentences extracted by the system which arc in the target semi-supervised algorithms are over the baseline classifier 
summaries, and be the total number of sentences extracted    trained only on the labeled training data. For example, if 
by the system. The precision is defined as the ratio         we consider text summarization, using only 5% of labeled 
                                                             sentences, our algorithm allows to increase performance by 
5.3 Results                                                  15% compared to a fully supervised algorithm trained on 
For each dataset and each cross validation, 25% of examples  the 5%. 15% labeled data are needed to reach the same 
are held aside as a test set. We vary the percentage of labeled- performance with the baseline method. Our model is uni•
unlabeled data in the remaining 75% training set of the collec• formly better than the two reference models used in our ex•
tions. Figure 1, shows the performance on the test sets for the periments. It provides an important performance increase es•
four datasets as a function of the proportion of labeled data in pecially when there are only few labeled data available which 
the training set. On the x-axis, 5% means that 5% of data in is the most interesting situation in semi-supervised learning. 
the training set were labeled for training, the 95% remaining For the Credit card problem, the dataset is small and 
being used as unlabeled training data. Each experiment was   semi-supervised learning allows for a smaller increase of per•
carried on twenty paired trials of randomly selected training- formance than for other datasets where there are a lot of un•
test splits. On the y-axis, each point represents the mean per• labeled data available. Semi-supervised learning allows to 
formance for the 20 runs and the error bars correspond to the reach 'optimal' performance with about 50% of labeled data 
standard deviation for the estimated performance [Tibshirani, (only 30% for the Mushroom set). 


LEARNING                                                                                                          559 