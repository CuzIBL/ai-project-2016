                           Dynamic Probabilistic Relational Models 

                          Sumit Sanghai Pedro Domingos Daniel Weld 
                           Department of Computer Science and Engineeering 
                                        University of Washington 
                                         Seattle, WA 98195-2350 
                               {sanghai,pedrod,weld}@cs.washington.edu 

                     Abstract                          human beings. This paper addresses these two problems by 
                                                       introducing an extension of DBNs that exposes the domain's 
   Intelligent agents must function in an uncertain world, relational structure, and by developing methods for efficient 
  containing multiple objects and relations that change inference in this representation. 
  over time. Unfortunately, no representation is currently Formalisms that can represent objects and relations, as op•
  available that can handle all these issues, while allowing posed to just variables, have a long history in AI. Recently, 
  for principled and efficient inference. This paper ad• significant progress has been made in combining them with a 
  dresses this need by introducing dynamic probabilistic principled treatment of uncertainty. In particular, probabilis•
  relational models (DPRMs). DPRMs are an extension    tic relational models or PRMs [Friedman et al, 1999] are an 
  of dynamic Bayesian networks (DBNs) where each time  extension of Bayesian networks that allows reasoning with 
  slice (and its dependences on previous slices) is repre• classes, objects and relations. The representation we intro•
  sented by a probabilistic relational model (PRM). Parti• duce in this paper extends PRMs to sequential problems in 
  cle filtering, the standard method for inference in DBNs, the same way that DBNs extend Bayesian networks. We thus 
  has severe limitations when applied to DPRMs, but we call it dynamic probabilistic relational models, or DPRMs. 
  are able to greatly improve its performance through a We develop an efficient inference procedure for DPRMs by 
  form of relational Rao-Blackwellisation. Further gains adapting Rao-Blackwcllised particle filtering, a state-of-the-
  in efficiency arc obtained through the use of abstrac• art inference method for DBNs [Murphy and Russell, 2001]. 
  tion trees, a novel data structure. We successfully apply We introduce abstraction trees as a data structure to reduce 
  DPRMs to execution monitoring and fault diagnosis of the computational cost of inference in DPRMs. 
  an assembly plan, in which a complex product is gradu• Early fault detection in complex manufacturing processes 
  ally constructed from subparts.                      can greatly reduce their cost. In this paper we apply DPRMs 
                                                       to monitoring the execution of assembly plans, and show that 
1 Introduction                                         our inference methods scale to problems with over a thou•
                                                       sand objects and thousands of steps. Other domains where 
Sequential phenomena abound in the world, and uncertainty we envisage DPRMs being useful include robot control, vi•
is a common feature of them. Currently the most power• sion in motion, language processing, computational modeling 
ful representation available for such phenomena is dynamic of markets, battlefield management, cell biology, ecosystem 
Bayesian networks, or DBNs [Dean and Kanazawa, 1989].  modeling, and the Web. 
DBNs represent the state of the world as a set of variables, and The rest of the paper is structured as follows. The next two 
model the probabilistic dependencies of the variables within sections briefly review DBNs and PRMs. We then introduce 
and between time steps. While a major advance over previ• DPRMs and methods for inference in them. The following 
ous approaches, DBNs are still unable to compactly represent section reports on our experimental study in assembly plan 
many real-world domains. In particular, domains can contain monitoring. The paper concludes with a discussion of related 
multiple objects and classes of objects, as well as multiple and future work. 
kinds of relations among them; and objects and relations can 
appear and disappear over time. For example, manufactur• 2 Dynamic Bayesian Networks 
ing plants assemble complex artifacts (e.g., cars, computers, 
aircraft) from large numbers of component parts, using mul• A Bayesian network encodes the joint probability distribu•
tiple kinds of machines and operations. Capturing such a do• tion of a set of variables, as a directed acyclic 
main in a DBN would require exhaustively representing all graph and a set of conditional probability models. Each node 
possible objects and relations among them. This raises two corresponds to a variable, and the model associated with it 
problems. The first one is that the computational cost of us• allows us to compute the probability of a state of the vari•
ing such a DBN would likely be prohibitive. The second is able given the state of its parents. The set of parents of 
that reducing the rich structure of the domain to a very large, denoted is the set of nodes with an arc to 
"flat" DBN would render it essentially incomprehensible to in the graph. The structure of the network encodes the as-


992                                                                             PROBABILISTIC INFERENCE sertion that each node is conditionally independent of its this weighted distribution. The particles will thus tend to stay 
non-descendants given its parents. The probability of an ar• clustered in the more probable regions of the state space, ac•
bitrary event can then be computed as                  cording to the observations. 
                                                         Although particle filtering has scored impressive successes 
  Dynamic Bayesian Networks (DBNs) are an extension of in many practical applications, it also has some significant 
Bayesian networks for modeling dynamic systems. In a DBN, limitations. One that is of particular concern to us here is that 
the state at time t is represented by a set of random variables it tends to perform poorly in high-dimensional state spaces. 
                     The state at time t is dependent on This is because the number of particles required to main•
the states at previous time steps. Typically, we assume that tain a good approximation to the state distribution grows very 
each state only depends on the immediately preceding state rapidly with the dimensionality. This problem can be greatly 
(i.e., the system is first-order Markovian), and thus we need attenuated by analytically marginalizing out some of the vari•
to represent the transition distribution . . This can  ables, a technique known as Rao-Blackwellisation [Murphy 
be done using a two-time-slice Bayesian network fragment and Russell, 2001 ]. Suppose the state space can be divided 
(2TBN) . which contains variables from whose into two subspaces and such that 
parents are variables from and/or and variables from   can be computed analytically and efficiently. Then we only 
   without any parents. Typically, we also assume that the need to sample from the smaller space , requiring far fewer 
process is stationary, i.e., the transition models for all time particles to obtain the same degree of approximation. Each 
slices are identical: Thus a                           particle is now composed of a sample from 
DBN is defined to be a pair of Bayesian networks I     plus a parametric representation of P\ For 
where represents the initial distribution P{ZO), and is example, if the variables in are discrete and independent of 
a two-time-slice Bayesian network, which as discussed above each other given we can store for each variable the vector 
defines the transition distribution                    of parameters of the corresponding multinomial distribution 
  The set is commonly divided into two sets: the unob• (i.e., the probability of each value). 
served state variables and the observed variables . The 
observed variables are assumed to depend only on the cur• 3 Probabilistic Relational Models 
rent state variables . The joint diwStribution represented by 
a DBN can then be obtained by unrolling the 2TBN:      A relational schema is a set of classes 
                                                       where each class C is associated with a set of propositional 
                                                       attributes ,and a set of relational attributes or refer•
                                                       ence slots . The propositional attribute A of class C 
                                                       is denoted C.A, and its domain (assumed finite) is denoted 
                                                       V(C.A). The relational attribute R of C is denoted C.R, 
                                                       and its domain is the power set of a target class 
  Various types of inference in DBNs are possible. One of In other words, C.R is a set of objects belonging to some 
the most useful is state monitoring (also known as filtering or class For example, the Aircraft schema might be used 
tracking), where the goal is to estimate the current state of the to represent partially or completely assembled aircraft, with 
world given the observations made up to the present, i.e., to classes corresponding to different types of parts like metal 
compute the distribution Proper state                  sheets, nuts and bolts. The propositional attributes of a bolt 
monitoring is a necessary precondition tor rational decision• might include its color, weight, and dimensions, and its rela•
making in dynamic domains. Inference in DBNs is NP-    tional attributes might include the nut it is attached to and the 
complete, and thus we must resort to approximate meth• two metal sheets it is bolting. An instantiation of a schema is 
ods, of which the most widely used one is particle filter- a set of objects, each object belonging to some class 
ing [Doucet et 0/., 2001]. Particle filtering is a stochas• with all propositional and relational attributes of each object 
tic algorithm which maintains a set of particles (samples) specified. For example, an instantiation of the aircraft schema 
             to approximately represent the distribution of might be a particular airplane, with all parts, their properties 
possible states at time given the observations. Each parti• and their arrangement specified. 
cle contains a complete instance of the current state, i.e., a A probabilistic relational model (PRM) encodes a proba•
sampled value for each state variable. The current distribution bility distribution over the set of all possible instantiations / 
is then approximated by                                of a schema [Friedman et al., 1999]. The object skeleton of 
                                                       an instantiation is the set of objects in it, with all attributes 
                                                       unspecified. The relational skeleton of an instantiation is the 
                                                       set of objects in it, with all relational attributes specified, and 
                                                       all propositional attributes unspecified. In the simplest case, 
where is 1 if the state represented byis same as       the relational skeleton is assumed known, and the PRM spec•
  and 0 otherwise. The particle filter starts by generating TV ifies a probability distribution for each attribute A of each 
particles according to the initial distribution P Then, at class C. The parents of each attribute (i.e., the variables 
each step, it first generates the next state for each parti• it depends on) can be other attributes of C, or attributes of 
cle by sampling from It then weights these 
samples according to the likelihood they assign to the ob• lC.R can also be defined as a function from but we 
servations, and resamples N particles from             choose the simpler convention here. 


PROBABILISTIC INFERENCE                                                                               993  classes that are related to C by some slot chain. A slot chain 
 is a composition of relational attributes. In general, it must 
be used together with an aggregation function that reduces a 
 variable number of values to a single value. For example, a 
parent of an attribute of a bolt in the aircraft schema might be 
avg(bolt.plate.nut.weight), the average weight of all the nuts 
on the metal plates that the bolt is attached to. 
 Definition 1 A probabilistic relational model (PRM) II for a 
relational schema S is defined as follows. For each class C 
and each propositional attribute A € -4(C), we have: 

                                                         DPRMs are extended to the case where only the object 
                                                       skeleton for each time slice is known in the same way that 
                                                       PRMs are, by adding to Definition 2 a set of parents and con•
                                                       ditional probability model for each relational attribute, where 
                                                       the parents can be in the same or the previous time slice. 
                                                       When the object skeleton is not known (e.g., if objects can 
                                                       appear and disappear over time), the 2TPRM includes in ad•
                                                       dition a Boolean existence variable for each possible object, 
                                                       again with parents from the same or the previous time slice.3 
                                                       As with DBNs, we may wish to distinguish between observed 
                                                       and unobserved attributes of objects. In addition, wc can con•
  A PRM and relational skeleton can thus be unrolled into a sider an Action class with a single attribute whose domain is 
large Bayesian network with one variable for each attribute the set of actions that can be performed by some agent (e.g., 
of each object in the skeleton.2 Only PRMs that correspond painting a metal plate, or bolting two plates together). The 
to Bayesian networks without cycles are valid.         distribution over instantiations in a time slice can then de•
  More generally, only the object skeleton might be known, pend on the action performed in that time slice. For example, 
in which case the PRM also needs to specify a distribution the action may with high probability pro•
over the relational attributes [Getoor et al., 2001]. In the air• duce and with lower probability set 
craft domain, a PRM might specify a distribution over the Part J.mate to some other object of Part2's class (i.e., be im•
state of assembly of an airplane, with probabilities for differ• properly performed, resulting in a fault). 
ent faults (e.g., a bolt is loose, the wrong plates have been Just as a PRM can be unrolled into a Bayesian network, so 
bolted, etc.).                                         can a DPRM be unrolled into a DBN. (Note, however, that 
                                                       this DBN may in general contain different variables in dif•
4 Dynamic Probabilistic Relational Models              ferent time slices.) In principle, we can perform inference 
In this section we extend PRMs to modeling dynamic sys• on this DBN using particle filtering. However, the filter is 
tems, the same way that DBNs extend Bayesian networks. likely to perform poorly, because for non-trivial DPRMs its 
We begin with the observation that a DBN can be viewed as a state space will be huge. Not only will it contain one variable 
special case of a PRM, whose schema contains only one class for each attribute of each object of each class, but relational 
Z with propositional attributes Z\,..., Za and a single rela• attributes will in general have very large domains. We over•
                                                       come this by adapting Rao-Blackwellisation to the relational 
tional attribute previous. There is one object Zt for each time 
slice, and the previous attribute connects it to the object in setting. We make the following (strong) assumptions: 
the previous time slice. Given a relational schema S, we first 1. Relational attributes with unknown values do not appear 
extend each class C with the relational attribute C.previous, anywhere in the DPRM as parents of unobserved at•
with domain C. As before, we initially assume that the re• tributes, or in their slot chains. 
lational skeleton at each time slice is known. We can then 
                                                      2. Each reference slot can be occupied by at most one object. 
define two-time-slice PRMs and dynamic PRMs as follows. 
                                                       Proposition 1 Assumptions 1 and 2 imply that, given the 
                                                      propositional attributes and known relational attributes at 
                                                      times t and t — 1, the joint distribution of the unobserved 
                                                      relational attributes at time t is a product of multinomials, 
                                                      one for each attribute. 
                                                         Notice also that, by Assumption 1, unobserved proposi•
                                                      tional attributes can be sampled without regard to unobserved 
                                                      relational ones. Rao-Blackwellisation can now be applied 

                                                         3Notice that the attributes of nonexistent objects need not be 
   Plus auxiliary (deterministic) variables for the required aggre• specified, because by definition no attributes of any other objects 
gations, which we omit from the formula for simplicity. can depend on them [Getoor et al., 2001 ]. 


994                                                                             PROBABILISTIC INFERENCE with as the propositional attributes of all objects and 5 Experiments 
as their relational attributes. A Rao-Blackwellised particle is In this section we study the application of DPRMs to fault 
composed of sampled values for all propositional attributes detection in complex assembly plans. We use a modified ver•
of all objects, plus a probability vector for each relational at• sion of the Schedule World domain from the AIPS-2000 Plan•
tribute of each object. The vector element corresponding to 
                                                       ning Competition.4 The problem consists of generating a plan 
        is the probability that relation R holds between obj for assembly of objects with operations such as painting, pol•
and the zth object of the target class, conditioned on the values ishing, etc. Each object has attributes such as surface type, 
of the propositional attributes in the particle, etc.  color, hole size, etc. We add two relational operations to the 
  Rao-Blackwellising the relational attributes can vastly re• domain: bolting and welding. We assume that actions may 
duce the size of the state space which particle filtering needs be faulty, with fault model described below. In our experi•
to sample. However, if the relational skeleton contains a ments, we first generate a plan using the FF planner [Hoff•
large number of objects and relations, storing and updating mann and Nebel, 2001], We then monitor the plan's execu•
all the requisite probabilities can still become quite expen• tion using particle filtering (PF), Rao-Blackwellised particle 
sive. This can be ameliorated if context-specific independen• filtering (RBPF) and RBPF with abstraction trees. 
cies exist, i.e., if a relational attribute is independent of some We consider three classes of objects: Plate, Bracket and 
propositional attributes given assignments of values to oth• Bolt. Plate and Bracket have propositional attributes such as 
ers [Boutilier et aly 1996]. We can then replace the vector weight, shape, color, surface type, hole size and hole type, 
of probabilities with a tree structure whose leaves represent and relational attributes for the parts they are welded to and 
probabilities for entire sets of objects. More precisely, we de• the bolts bolting them to other parts (e.g., Plate73.bolt4 corre•
fine the abstraction tree data structure for a relational attribute sponds to the fourth bolt hole on plate 73). The Bolt class has 
obj.R with target class _ as follows. A node of the tree is propositional attributes such as size, type and weight. Propo•
composed of a probability p and a logical expression over sitional actions include painting, drilling and polishing, and 
the propositional attributes of the schema. Let be the change the propositional attributes of an object. The rela•
set of objects in that satisfy the 's of and all of 's an- tional action Bolt sets a bolt attribute of a Plate or Bracket 
                                                       object to a Bolt object. The Weld action sets a welded-to at•
                                                       tribute of a Plate or Bracket object to another Plate or Bracket 
                                                       object. 
                                                         The fault model has a global parameter, the fault proba•
                                                       bility , With probability 1 - an action produces the 
                                                       intended effect. With probability , one of several possible 
                                                       faults occurs. Propositional faults include a painting oper•
                                                       ation not being completed, the wrong color being used, the 
                                                       polish of an object being ruined, etc. The probability of dif•
                                                       ferent propositional faults depends on the properties of the 
                                                       object being acted on. Relational faults include bolting the 
                                                       wrong objects and welding the wrong objects. The proba•
                                                       bility of choosing a particular wrong object depends on its 
                                                       similarity to the intended object. Similarity depends on dif•
                                                       ferent propositional attributes for different actions and differ•
                                                       ent classes of objects. Thus the probability of a particular 
                                                       wrong object being chosen is uniform across all objects with 
                                                       the same relevant attribute values. 
                                                         The DPRM also includes the following observation model. 
                                                       There are two instances of each attribute: the true one, which 
                                                       is never observed, and the observed one, which is observed 
                                                       at selected time steps. Specifically, when an action is per•
                                                       formed, all attributes of the objects involved in it are ob•
                                                       served, and no others. Observations are noisy: with proba•
                                                       bility the true value of the attribute is observed, and 
                                                       with probability an incorrect value is observed. Incorrect 
                                                       values for propositional observations are chosen uniformly. 
                                                       Incorrect values for relational observations are chosen with 
                                                       a probability that depends on the similarity of the incorrect 
                                                       object to the intended one. 
                                                         Notice that, if the domain consisted exclusively of the 
                                                       propositional attributes and actions on them, exact inference 
                                                       might be possible; however, the dependence of relational at•
                                                       tributes and their observations on the propositional attributes 

                                                         4URL: http://www.cs.toronto.edu/aips2000 


PROBABILISTIC INFERENCE                                                                                995 creates complex dependencies between these, making ap•
proximate inference necessary. 
   A natural measure of the accuracy of an approximate infer•
ence procedure is the K-L divergence between the distribu•
tion it predicts and the actual one [Cover and Thomas, 2001]. 
However, computing it requires performing exact inference, 
which for non-trivial DPRMs is infeasible. Thus we estimate 
the K-L divergence by sampling, as follows. Let D be 
the K-L divergence between the true distribution p and its 
approximation p, and let be the domain over which the 
distribution is defined. Then 


                                                       Figure 1: Comparison of RBPF (5000 particles) and PF 
                                                       (200,000 particles) for 1000 objects and varying fault prob•
The first term is simply the entropy of X, H(X), and is a ability. 
constant independent of the approximation method. Since 
we are mainly interested in measuring differences in perfor•
mance between approximation methods, this term can be ne•
glected. The K-L divergence can now be approximated in the 
usual way by taking S samples from the true distribution: 


where is the probability of the zth sample according 
to the approximation procedure, and the H subscript indi•
cates that the estimate of is offset by H(X). We 
thus evaluate the accuracy of PF and RBPF on a DPRM by 
generating S = 10,000 sequences of states and observations 
from the DPRM, passing the observations to the particle fil•
ter, inferring the marginal probability of the sampled value 
of each state variable at each step, plugging these values into 
the above formula, and averaging over all variables. Notice Figure 2: Comparison of RBPF (5000 particles) and PF 
that whenever a sampled value is not rep•              (200,000 particles) for fault probability of 1% and varying 
resented in any particle. The empirical estimates of the K-L number of objects. 
divergence we obtain will be optimistic in the sense that the 
true K-L divergence may be infinity, but the estimated one 
will still be finite unless one of the values with zero predicted K-L divergence of RBPF increases only very slowly, for all 
                                                      combinations of parameters tried. Abstraction trees reduced 
probability is sampled. This does not preclude a meaningful 
                                                       RBPF's time and memory by a factor of 30 to 70, and took 
comparison between approximation methods, however, since 
                                                      on average six times longer and 11 times the memory of PF, 
on average the worse method should produce 
                                                      per particle. However, note that we ran PF with 40 times 
earlier in the time sequence. We thus report both the average 
                                                      more particles than RBPF. Thus, RBPF is using less time and 
K-L divergence before it becomes infinity and the time step 
                                                      memory than PF, and performing far better in accuracy. 
at which it becomes infinity, if any. 
                                                         We also ran all the experiments while measuring the K-L 
  Figures 1 and 2 show the results of the experiments per• divergence of the full joint distribution of the state (as op•
formed. The observation noise parameter was set to the posed to just the marginals). RBPF performed even better 
same value as the fault probability throughout. One action compared to PF in this case; the latter tends to blow up much 
is performed in each time step; thus the number of time steps sooner (e.g., from around step 4000 to less than 1000 for 
is the length of the plan. The graphs show the K-L divergence 1% and 1000 objects), while RBPF continues to de•
of PF and RBPF at every 100th step (it is the same for RBPF grade only very slowly. 
with and without abstraction trees). Graphs are interrupted 
at the first point where the K-L divergence became infinite 
in any of the runs (once infinite, the K-L divergence never 6 Related Work 
went back to being finite in any of the runs), and that point is Dynamic object-oriented Bayesian networks (DOOBNs) 
labeled with the average time step at which the blow-up oc• [Friedman et al, 1998] combine DBNs with OOBNs, a pre•
curred. As can be seen, PF tends to diverge rapidly, while the decessor of PRMs. Unfortunately, no efficient inference 


                                                                                PROBABILISTIC INFERENCE 