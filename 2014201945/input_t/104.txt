                A Bayesian Approach to Imitation in Reinforcement Learning 

                              Bob Price                                       Craig Boutilier 
                 University of British Columbia                           University of Toronto 
               Vancouver, B.C., Canada V6T 1Z4                       Toronto, ON, Canada M5S 3H5 
                         price@cs.ubc.ca                                  cebly@cs. toronto. edu 


                        Abstract                               communication channel, a sufficiently expressive represen•
                                                               tation language, a transformation between possibly different 
      In multiagent environments, forms of social learn•
                                                               agent bodies, and an incentive to communicate. In dynamic, 
      ing such as teaching and imitation have been shown 
                                                               competitive domains, such as web-based trading, it is unreal•
     to aid the transfer of knowledge from experts to 
                                                               istic to expect all agents to be designed with compatible rep•
      learners in reinforcement learning (RL). We re•          resentations and altruistic intentions. Observation-based tech•
     cast the problem of imitation in a Bayesian frame-        niques, in which the learning agent observes only the outward 
     work. Our Bayesian imitation model allows a               behaviors of another agent, can reduce the need for explicit 
      learner to smoothly pool prior knowledge, data ob•       communication. Implicit communication through passive ob•
     tained through interaction with the environment,          servations has been implemented as implicit imitation [Price 
     and information inferred from observations of ex•         and Boutilier, 1999; 2001]. In this model, the effects of other 
     pert agent behaviors. Our model integrates well           agents' action choices on the state of the environment can be 
     with recent Bayesian exploration techniques, and          observed, but the internal state of other agents and their ac•
     can be readily generalized to new settings.               tion control signals are not observable. Independent explo•
                                                               ration on the part of the observer is used to adapt knowledge 
 1   Introduction                                              implicit in observations of other agents to the learning agent's 
                                                               own needs. Unlike classic imitation models, the learner is not 
 Reinforcement learning is a flexible, yet computationally 
                                                               required to explicitly duplicate the behavior of other agents. 
 challenging paradigm. Recent results demonstrating that un•
                                                                 In this paper, we recast implicit imitation in a Bayesian 
 der certain assumptions the sample complexity of reinforce•
                                                               framework. This new formulation offers several advantages 
 ment learning is polynomial in the number of problem states 
                                                               over existing models. First it provides a more principled, 
 [Kearns and Singh, 1998] are tempered by the sober fact that 
                                                               and more elegant approach to the smooth pooling of infor•
 the number of states is generally exponential in the number 
                                                               mation from the agent's prior beliefs, its own experience and 
 of the attributes defining a learning problem. With recent in-
                                                               the observations of other agents (e.g., it eliminates the need 
terest in building interacting autonomous agents, reinforce-
                                                               for certain ad hoc tuning parameters in current imitation mod•
ment learning is increasingly applied to multiagent tasks, a 
                                                               els). Second, it integrates well with state-of-the-art explo•
 development which only adds to the complexity of learning 
                                                               ration techniques, such as Bayesian exploration. Finally, the 
 [Littman, 1994; Hu and Wellman, 1998]. In this paper, we ex•
                                                               Bayesian imitation model can be extended readily to partially-
 amine multi-agent reinforcement learning under the assump•
                                                               observable domains, though the derivation and implementa•
 tion that other agents in the environment are not merely ar•
                                                               tion are considerably more complex and are not reported here. 
bitrary actors, but actors "like me". That is, the other agents 
may have similar action capabilities and similar objectives. 
This assumption radically changes the optimal learning strat•  2 Background 
egy. Information about other agents "like me" can give the     We assume a reinforcement learning (RL) agent is learning to 
 learning agent additional information about its own capabili• control a Markov decision processes (MDP) 

ties and how these capabilities relate to its own objectives. A with finite state and action sets S,Ao, reward function 
number of techniques have been developed to exploit this, in•  S R, and dynamics D. The dynamics D refers to a 
cluding imitation [Demiris and Hayes, 1999; Mataric, 2002],    set of transition distributions The actions and 
learning by watching [Kuniyoshi et al., 1994], teaching or     rewards are subscripted to distinguish them from those 
programming by demonstration [Atkeson and Schaal, 1997]        of other agents (see below). We assume throughout that the 
behavioral cloning [Sammut et al., 1992], and inverse rein•    agent knows but not the dynamics D of the MDP (thus 
forcement learning [Ng and Russell, 2000].                     we adopt the "automatic programming" perspective), and has 
   Learning by observation of other agents has intuitive ap•   the objective of maximizing discounted reward over an infi•
peal; however, explicit communication about action capabil•    nite horizon. Any of a number of RL techniques can be used to 
ities between agents requires considerable infrastructure: a   learn an optimal policy We focus here on model-


712                                                                                             MULTIAGENT SYSTEMS based RL methods, in which the observer maintains an esti•
mated MDP based on the set of experiences 
          obtained so far. At each stage (or at suitable inter•
vals) this MDP can be solved exactly, or approximately using 
techniques such as prioritized sweeping [Moore and Atkeson, 

 1993]. Since R0 is known, we focus on learning dynamics. 
   Bayesian methods in model-based RL allow agents to in•      Figure 1: Dependencies among model and evidence sources 
corporate priors and explore optimally. In general, we em•
ploy a prior density P over possible dynamics D, and update it between the two [Nehaniv and Dautenhahn, 1998]. Wc as•
with each data point (s, a, t). Letting H0) =                  sume full observability of the mentor's state space; but we do 
denote the (current) state history of the observer, and A0 =   not assume the observer can identify the actions taken by the 
              _ be the action history, we use the poste•       mentor—it simply observes state transitions. 
rior to update the action Q-values, which are                    We make two additional assumptions regarding the men•
used in turn to select actions. The formulation of Dearden     tor's dynamics: the mentor implements a stationary pol•
et al 1999 renders this update tractable by assuming a con•    icy which induces a Markov chain = 
venient prior: P is the product of local independent densi•    Pr ; and for each action taken by the mentor, 
ties for each transition distribution and each den•            there exists an action such that the distributions 
sity is a Dirichlet with parameters To model                             and are the same. This latter assump•
         we require one parameter for each possible            tion is the homogeneous action assumption and implies that 

successor state . Update of a Dirichlet is straightforward:    the observer can duplicate the mentor's policy.2 As a con•
given prior and data vector (where is sequence we can treat the dynamics D as the same for both 
the number of observed transitions from s to t under a), the   agents. Note that we do not assume that the learner knows 
posterior is given by parameters Thus the poste•               a priori which of its actions duplicates the mentor's (for any 
rior in Eq. 1 can be factored into posteriors over local families: given state s), nor that the observer wants to duplicate this pol•
                                                               icy (as the agents may have different objectives). 
                                                        (1)      Since the learner can observe the mentor's transitions 
where is the subset of history composed of transitions         (though not its actions directly), it can form estimates of the 
from state s due to action a, and the updates themselves are   mentor's Markov chain, along with estimates of its own MDP 
simple Dirichlet parameter updates.                            (transition probabilities and reward function). In [Price and 
   The Bayesian approach has several advantages over other     Boutilier, 1999], this estimate is used to augment the normal 
approaches to model-based RL. First, it allows the natural in• Bellman backup, treating the observed distribution Pr(s,.) as 
corporation of priors over transition and reward parameters.   a model of an action available to the observer. Imitators using 
Second, approximations to optimal Bayesian exploration can     augmented backups based on their observations of a mentor 
take advantage of this approach, and the specific structural as• often learn much more quickly, especially if the mentor's re•
sumptions on the prior discussed above [Dearden et al., 1999]. ward function or parts of its policy overlap with that of the ob•
                                                               server. Techniques like interval estimation [Kaelbling, 1993] 
3 Bayesian Imitation                                           can be used to suppress augmented backups where their value 
                                                               has low "confidence." 
In multiagent settings, observations of other agents can be      In the Bayesian approach, the observer incorporates obser•
used in addition to prior beliefs and personal experience to   vations of the mentor directly into an augmented model of its 
improve an agent's model of its environment. These obser•      environment. Let Hm denote the history of mentor state tran•
vations can have enormous impact when they provide infor•      sitions observed by the learner. As above, H0 and A0 repre-
                                                               sents the observer's state and action history respectively. Fig-
mation to an agent about parts of the state space it has not yet 
                                                               ure 1 illustrates the sources of information available to the im-
visited. The information can be used to bias exploration to•   itator with which to constrain its beliefs about Z), and their 
wards the most promising regions of state space and thereby    probabilistic dependence. While the observer knows its own 
reduce exploration costs and speed convergence dramatically.   action history, A0 it has no direct knowledge of the actions 
  The flexibility of the Bayesian formulation leads to an ele• taken by the mentor: at best it may have (often weak) prior 
gant and principled mechanism for incorporating these obser•   knowledge about the mentor's policy The learner's be•
vations into the agent's model updates. Following Price and    liefs over D can then be updated w.r.t. the joint observations: 
Boutilier 1999, we assume two agents, a knowledgeable men•
tor 77i and a naive observer o, acting simultaneously, but in•

dependently, in a fixed environment.1 Like the observer, the 
mentor too is controlling an MDP with the 
same underlying state space and dynamics (that is, for any ac•
                                                                  2The homogeneous action assumption can be relaxed [Price and 
tion the dynamics are identical). The assump•                  Boutilier, 2001]. Essentially, the observer hypothesizes that viola-
tion that the two agents have the same state space is not criti• tions can be "repaired" using a local search for a short sequence of 
cal: more important is that there is some analogical mapping   actions that roughly duplicates a short subsequence of the mentor's 
                                                               actions. If a repair cannot be found, the observer discards the mentor 
    We assume that the agents are performing non-interacting tasks. influence (at this point in state space). 


MULTIAGENT SYSTEMS                                                                                                   713                                                                have independent distributions over Am for each s— 
   We assume that the prior P(D) has the factored Dirichlet    this update can be factored as well, with history elements at 
 form described above. Without mentor influence, a learner     state s being the only ones relevant to computing the posterior 
 can maintain its posterior in the same factored form, updating over , We still have the difficulty of evaluating the in•
 each component of the model P independently. Unfor•           tegral over models. Following Dearden et al. 1999, we tackle 
 tunately, complications arise due to the unobservability of the this by sampling models to estimate this quantity. Specif•
 mentor's actions. We show, however, that the model update     ically, we sample models from the factored Dirichlet 
 in Eq. 2 can still be factored into convenient terms.         P over P.4 Given a specific sample with 
   We derive a factored update model for P describ•            parameter vector , and observed counts , the likelihood 
 ing the dynamics at state s under action a by considering two of is: 
 cases. In case one, the mentor's unknown action could be 
 different than the action a. In this case, the model factor 
 would be independent of the mentor's history, and we can em•
ploy the standard Bayesian update Eq. 1 without regard for the 
mentor. In case two, the mentor action is in fact the same       We can combine the expression for expected model fac•
as the observer's action a. Then the mentor observations are   tor probability in Eq. 3 with our expression for mentor policy 
relevant to the update of P                                    likelihood in Eq. 5 to obtain a tractable algorithm for updating 
                                                               the observer's beliefs about the dynamics model D based on 

                                                               its own experience, and observations of the mentor.5. 
                                                                 A Bayesian imitator thus proceeds as follows. At each 
                                                               stage, it observes its own state transition and that of the men•
                                                               tor, using each to update its density over models as just de•
                                                               scribed. Efficient methods are used to update the agent's value 
   Let be the prior parameter vector for P , and 
                                                               function. Using this updated value function, it selects a suit•
     denote the counts of observer transitions from state s 
via action a, and the counts of the mentor transitions         able action, executes it, and repeats the cycle. 
from state s. The posterior augmented model factor density       Like any RL agent, an imitator requires a suitable explo•
P is then a Dirichlet with parame•                             ration mechanism. In the Bayesian exploration model [Dear-
ters that is, we simply update with the sum                    den et al, 1999], the uncertainty about the effects of actions 
of the observer and mentor counts:                             is captured by a Dirichlet, and is used to estimate a distribu-

                                                               tion over possible Q-values for each state-action pair.6 No-
                                                               tions such as value of information can then be used to approx•
                                                               imate the optimal exploration policy. This method is compu•
                                                               tationally demanding, but total reward including reward cap•
   Since the observer does not know the mentor's action we 
compute the expectation w.r.t. these two cases:                tured during training is usually much better than that provided 
                                                               by heuristic techniques. Bayesian exploration also eliminates 
                                                               the parameter tuning required by methods like -greedy, and 
                                                               adapts locally and instantly to evidence. These facts makes it 
                                                               a good candidate to combine with imitation. 

 This allows a factored update of the usual conjugate form, but 4  Experiments 
where the mentor counts are distributed across all actions, 
                                                               In this section we attempt to empirically characterize the 
weighted by the posterior probability that the mentor's policy 
                                                               applicability and expected benefits of Bayesian imitation 
chooses that action at state  
   With a mechanism to calculate the posterior over the men•   through several experiments. Using domains from the liter•
tor's policy, Eq. 3 provides a complete factored update rule for ature and two unique domains, we compare Bayesian imi•
incorporating evidence from observed mentors by a Bayesian     tation to non-Bayesian imitation [Price and Boutilier, 1999], 
model-based RL agent. To tackle this last problem—that of      and to several standard model-based RL (non-imitating) tech-
updating our beliefs about the mentor's policy—we have:        niques, including Bayesian exploration, prioritized sweeping 
                                                               and complete Bellman backups. We also investigate how 
                                                               Bayesian exploration combines with imitation. 
                                                                 First, we describe the agents used in our experiments. The 
                                                               Oracle employs a fixed policy optimized for each domain, 

                                                                  4Sampling is efficient as only one local model needs to be resam-
  If we assume that the prior over the mentor's policy is fac• pled at any time step. 
tored in the same way as the prior over models—that is, we        5Scaling techniques such as those used in HMM's may be re•
                                                               quired to prevent underflow in the term in Eq. 5. 
   3This assumes that at least one of the observer's actions is equiv• 6The Q-value distribution changes very little with each update 
alent to the mentor's, but our model can be generalized to the het• and can be repaired efficiently using prioritized sweeping. In fact, the 
erogeneous case. An additional term is required to represent "none Bayesian learner is cheaper to run than a full Bellman backup over 
of the above".                                                 all states. 


714                                                                                             MULTIAGENT SYSTEMS                                                               providing both a baseline and a source of expert behavior 
                                                              for the observers. The EGBS agent combines greedy ex•
                                                              ploration (EG) with a full Bellman backup (i.e., sweep) at 
                                                              each time step. It provides an example of a generic model-
                                                              based approach to learning. The EGPS agent is a model-based 
                                                              RL agent, using e-greedy (EG) exploration with prioritized 
                                                              sweeping (PS). EGPS use fewer backups, but applies them 
                                                              where they are predicted to do the most good. EGPS does not 
                                                              have a fixed backup policy, so it can propagate value multiple 
                                                              steps across the state space in situations where EGBS would 
                                                              not. The BE agent employs Bayesian exploration (BE) with 
                                                              prioritized sweeping for backups. BEBI combines Bayesian 
                                                              exploration (BE) with Bayesian imitation (Bi). EGBI com-
                                                              bines c-greedy exploration (EG) with Bayesian imitation (Bl). 
               Figure 2: Flagworld Domain                     The EGNB1 agent combines e-greedy exploration with non-
                                                              Bayesian imitation. 
                                                                In each experiment, agents begin at the start state. The 
                                                              agents do not interact within the state space. When an agent 
                                                              achieves the goal, it is reset to the beginning. The other agents 
                                                              continue unaffected. Each agent has a fixed number of steps 
                                                              (which may be spread over varying numbers of runs) in each 
                                                              experiment. In each domain, agents are given locally uniform 
                                                              priors (i.e., every action has an equal probability of resulting 
                                                              in any of the local neighbouring states; e.g., in a grid world 
                                                              there are 8 neighbours). Imitators observe the expert oracle 
                                                              agent concurrently with their own exploration. Results are re•
                                                              ported as the total reward collected in the last 200 steps. This 
                                                              sliding window integrates the rewards obtained by the agent 
                                                              making it easier to compare performance of various agents. 
                                                              During the first 200 steps, the integration window starts off 
                                                              empty causing the oracle's plot to jump from zero to optimal in 
                                                              the first 200 steps. The Bayesian agents use 5 sampled MDPs 
                                                              for estimating Q-value distributions and 10 samples for esti•
                                                              mating the mentor policy from the Dirichlet distribution. Ex•
                                                              ploration rates for e-greedy agents were tuned for each exper•
           Figure 3: Flag world results (50 runs)             imental domain. 
                                                                Our first test of the agents was on the "Loop" and "Chain" 
                                                              examples (designed to show the benefits of Bayesian explo•
                                                              ration), taken from [Dearden et al, 1999]. In these experi-
                                                              ments, the imitation agents performed more or less identically 
                                                              to the optimal oracle agent and no separation could be seen 
                                                              amongst the imitators. 
                                                                Using the more challenging "FlagWorld" domain [Dearden 
                                                              et al., 1999], we see meaningful differences in performance 
                                                              amongst the agents. In FlagWorld, shown in Figure 2, the 
                                                              agent starts at state S and searches for the goal state Gl. The 
                                                              agent may pick up any of three flags by visiting states Fl, 
                                                              F2 and F3. Upon reaching the goal state, the agent receives 
                                                              1 point for each flag collected. Each action (N,E,S,W) suc•
                                                              ceeds with probability 0.9 if the corresponding direction is 
                                                              clear, and with probability 0.1 moves the agent perpendicu•
                                                              lar to the desired direction. Figure 3 shows the reward col•
                                                              lected in over the preceding 200 steps for each agent. The Or•
                                                              acle demonstrates optimal performance. The Bayesian imita•
                                                              tor using Bayesian exploration (BEBI) achieves the quickest 
                                                              convergence to the optimal solution. The e-greedy Bayesian 
        Figure 4: Flag World Moved Goal (50 runs)             imitator (EGBI) is next, but is not able to exploit informa•
                                                              tion locally as well as BEBI. The non-Bayesian imitator (EG-
                                                              NBI) does better than the unassisted agents early on but fails 


MULTIAGENT SYSTEMS                                                                                                  715                                                                to find the optimal policy in this domain. A slower ex-
                                                               ploration rate decay would allow the agent to find the opti•
                                                               mal policy, but would also hurt its early performance. The 
                                                               non-imitating Bayesian explorer fares poorly compared to the 
                                                               Bayesian imitators, but outperforms the remaining agents, as 
                                                               it exploits prior knowledge about the connectivity of the do•
                                                               main. The other agents show poor performance (though with 
                                                               high enough exploration rates they would converge eventu•
                                                               ally). We conclude that Bayesian imitation makes the best use 
                                                               of the information available to the agents, particularly when 
                                                               combined with Bayesian exploration. 
                                                                 We altered the Flag World domain so that the mentor and the 
                                                               learners had different objectives. The goal of the expert Ora•
                                                               cle remained at location G1, while the learners had goal loca•
                                                              tion G2 (Figure 2). Figure 4 shows that transfer due to imita•
                                                              tion is qualitatively similar to the case with identical rewards. 
                                                              We see that imitation transfer is robust to modest differences 
                                                               in mentor and imitator objectives. This is readily explained by 
        Figure 5: Tutoring domain results (50 runs)           the fact that the mentor's policy provides model information 
                                                              over most states in the domain, which can be employed by the 
                                                              observer to achieve its own goals. 
                                                                 The tutoring domain requires agents to schedule the presen•
                                                              tation of simple patterns to human learners in order to min•
                                                              imize training time. To simplify our experiments, we have 
                                                              the agents teach a simulated student. The student's perfor-
                                                              mance is modeled by independent, discretely approximated, 
                                                              exponential forgetting curves for each concept. The agent's 
                                                              action will be its choice of concept to present. The agent re•
                                                              ceives a reward when the student's forgetting rate has been 
                                                              reduced below a predefined threshold for all concepts. Pre-
                                                              senting a concept lowers its forgetting rate, leaving it unpre-
                                                              sented increases its forgetting rate. Our model is too simple to 
                                                              serve as a realistic cognitive model of a student, but provides 
                                                              a qualitatively different problem to tackle. We note that the 
                                                              action space grows linearly with the number of concepts, and 
                                                              the state space exponentially. 
                Figure 6: No-south domain                        The results presented in Figure 5 are based on the presen•
                                                              tation of 5 concepts to a student. (EGBS has been left out as 
                                                              it is time-consuming and generally fares poorly.) We see that 
                                                              all of the imitators learn quickly, but with the Bayesian imita•
                                                              tors BEBI and EGBI outperforming EGNBI (which converges 
                                                              to a suboptimal policy).7 The generic Bayesian agent (BE) 
                                                              also chooses a suboptimal solution (which often occurs in BE 
                                                              agents if its priors prevent adequate exploration). Thus, we 
                                                              see that imitation mitigates one of the drawbacks of Bayesian 
                                                              exploration: mentor observations can be used to overcome 
                                                              misleading priors. We see also that Bayesian imitation can 
                                                              also be applied to practical problems with factored state and 
                                                              action spaces and non-geometric structure. 
                                                                 The next domain provides further insight into the combina•
                                                              tion of Bayesian imitation and Bayesian exploration. In this 
                                                              grid world (Figure 6), agents can move south only in the first 
                                                              column. In this domain, the optimal Oracle agent proceeds 
                                                              due south to the bottom corner and then east across to the goal. 
                                                              The Bayesian explorer (BE) chooses a path based on its prior 
                                                              beliefs that the space is completely connected. The agent can 
           Figure 7: No South results (50 runs) 
                                                                  Increasing exploration allows EGNBI to find the optimal policy, 
                                                              but further depresses short term performance. 


716                                                                                             MULTIAGENT SYSTEMS 