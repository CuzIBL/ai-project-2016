                                  Self Adaptive Particle Filter

                                              Alvaro Soto
                                Pontiﬁcia Universidad Catolica de Chile
                                    Department of Computer Science
                            Vicuna Mackenna 4860 (143), Santiago 22, Chile
                                            asoto@ing.puc.cl

                    Abstract                          Section 3 presents our method to adaptively estimate the num-
                                                      ber of particles. Section 4 presents our method to adaptively
    The particle ﬁlter has emerged as a useful tool for improve the propagation function. Section 5 shows the re-
    problems requiring dynamic state estimation. The  sults of applying the self adaptive ﬁlter to a visual tracking
    efﬁciency and accuracy of the ﬁlter depend mostly task. Finally, Section 6 presents the main conclusions of this
    on the number of particles used in the estimation work.
    and on the propagation function used to re-allocate
    these particles at each iteration. Both features are 2 Particle Filter
    speciﬁed beforehand and are kept ﬁxed in the reg-
    ular implementation of the ﬁlter. In practice this In Bayesian terms, the posterior distribution of the state can
    may be highly inappropriate since it ignores errors be expressed as:
    in the models and the varying dynamics of the pro-
                                                                 p(x /~y ) = β p(y /x ) p(x /~y )
    cesses. This work presents a self adaptive version              t  t        t  t   t  t−1         (1)

    of the particle ﬁlter that uses statistical methods to where β is a normalization factor; xt represents the state of
    adapt the number of particles and the propagation the system at time t; and ~yt represents all the information
    function at each iteration. Furthermore, our method collected until time t. Equation (1) assumes that xt totally
    presents similar computational load than the stan- explains the current observation yt.
    dard particle ﬁlter. We show the advantages of the  The particle ﬁlter provides an estimation of the poste-
    self adaptive ﬁlter by applying it to a synthetic ex- rior in Equation (1) in 3 main steps: sampling, weighting,
    ample and to the visual tracking of targets in a real and re-sampling. The sampling step consists of taking sam-
    video sequence.                                   ples (particles) from the so-called dynamic prior distribution,
                                                      p(xt/~yt−1). Next, in the weighting step, the resulting parti-
                                                      cles are weighted by the likelihood term p(yt/xt). Finally,
1  Introduction                                       a re-sampling step is usually applied to avoid the degeneracy
The particle ﬁlter is a useful tool to perform dynamic state es- of the particle set. The key point that explains the efﬁciency
timation via Bayesian inference. It provides great efﬁciency of the ﬁlter comes from using a Markovian assumption and
and extreme ﬂexibility to approximate any functional non- expressing the dynamic prior by:
linearity. The key idea is to use samples, also called particles,    Z
to represent the posterior distribution of the state given a se- p(xt/~yt−1) = p(xt/xt−1) p(xt−1/~yt−1)dxt−1. (2)
quence of sensor measurements. As new information arrives,
these particles are constantly re-allocated to update the esti- This expression provides a recursive implementation of the
mation of the state of the system.                    ﬁlter that allows it to use the last estimation p(xt−1/~yt−1)
                                                                          i
  The efﬁciency and accuracy of the particle ﬁlter depend to select the particles xt−1 in the next iteration. These par-
mainly on two key factors: the number of particles used to ticles are then propagated by the dynamics of the process
                                                            i
estimate the posterior distribution and the propagation func- p(xt/xt−1) to complete the sampling step.
tion used to re-allocate these particles at each iteration. The At each iteration the operation of the particle ﬁlter can be
standard implementation of the ﬁlter speciﬁes both factors be- seen as an importance sampling process [Tanner, 1996]. Im-
forehand and keeps them ﬁxed during the entire operation of portance sampling provides an efﬁcient way to obtain sam-
the ﬁlter. In this paper, we present a self adaptive particle ﬁl- ples from a density p(x), in cases where this function can be
ter that uses statistical methods to select an appropriate num- evaluated, but it is not affordable or possible to sample from
ber of particles and a suitable propagation function at each it directly. The basic idea in importance sampling is to use
iteration.                                            a proposal distribution q(x), also called importance function,
  This paper is organized as follows. Section 2 provides to obtain the samples xi, and then weigh each sample using
background information about the standard particle ﬁlter. a compensatory term given by p(xi)/q(xi). It is possible toshow [Tanner, 1996] that under mild assumptions the set of the true posterior distribution and the empirical distribution,
weighted-samples resembles the target distribution p(x). which is a well known nonparametric maximum likelihood
  The sampling and weighting steps of the particle ﬁlter cor- estimate. KLD-Sampling is based on the assumption that the
respond to the basic steps of an importance sampling pro- true posterior can be represented by a discrete piecewise con-
cess. In this case, given that the true posterior p(xt/~yt) is stant distribution consisting of a set of multidimensional bins.
not known, the samples are drawn from an importance func- This assumption allows the use of the χ2 asymptotic conver-
tion that corresponds to the dynamic prior p(xt/~yt−1). Using gence of the likelihood ratio statistic to ﬁnd a bound for the
this importance function, the compensatory terms are exactly number of particles N:
the non normalized weights used in the weighting step of the
                                                                             1  2
particle ﬁlter. The methods presented in this paper use re-             N >    χk−1,1−δ               (3)
sults from the theory of importance sampling to provide a                    2²
self adaptive version of the particle ﬁlter.          where ² is the upper bound for the error given by the KL-
                                                      divergence, (1 − δ) is the quantile of the χ2 distribution with
3  Adaptive Selection of the Number of                k − 1 degrees of freedom, and k is given by the number of
                                                      bins with support.
   Particles                                            The problem with KLD-Sampling is the derivation of the
The selection of the number of particles is a key factor in the bound using the empirical distribution, which has the implicit
efﬁciency and accuracy of the particle ﬁlter. The computa- assumption that the samples comes from the true distribution.
tional load and the convergence of the ﬁlter depend on this This is not the case for particle ﬁlters where the samples come
number. Most applications select a ﬁxed number of parti- from an importance function. Furthermore, the quality of the
cles in advance, using ad hoc criteria, or statistical methods match between this function and the true distribution is one of
such as Monte Carlo simulations or some standard statistical the main elements that determines the accuracy of the ﬁlter,
bound [Boers, 1999]. Unfortunately, the use of a ﬁxed num- hence the suitable number of particles. The bound given by
ber of particles is often inefﬁcient. The dynamics of most KLD-Sampling only uses information about the complexity
processes usually produces great variability in the complex- of the true posterior, but it ignores any mismatch between the
ity of the posterior distribution1. As a consequence, the initial true and the proposal distribution.
estimation of the number of particles can be much larger than To ﬁx the problem of KLD-Sampling we need a way to
the real number of particles needed to perform a good esti- quantify the degradation in the estimation using samples from
mation of the posterior distribution or, worse, at some point, the importance function. The goal is to ﬁnd the equivalent
the selected number of particles can be too small causing the number of samples from the importance and the true densities
ﬁlter to diverge.                                     that capture the same amount of information about the latter.
  The effect of the number of particles on the accuracy of In the context of Monte Carlo (MC) integration, [Geweke,
the ﬁlter is determined by two factors: the complexity of the 1989] introduced the concept of relative numerical efﬁciency
true density and how closely the proposal density mimics the (RNE), which provides an index to quantify the inﬂuence of
true density. Both factors are very intuitive; the estimation sampling from an importance function. The idea behind RNE
of a more complex pdf requires a greater number of sam- is to compare the relative accuracy of solving an integral us-
ples to correctly represent the less predictable shape of the ing samples coming from both the true and the proposal den-
function. Also, a greater mismatch between the proposal and sity. Accuracy is measured according to the variance of the
the true densities produces many wasted samples located in estimator of the integral.
irrelevant parts of the true distribution. Previous works to If we use MC integration to estimate the mean value of
adaptively determine an adequate number of particles have the state (EMC (x)), the variance of the estimator is given by
failed to consider these two factors together [Fox et al., 1999; [Doucet et al., 2001] 2:
Koeller and Fratkina, 1998; Fox, 2001].
                                                                   V ar[EN  (x)] = V ar (x)/N         (4)
  Here, we propose two methods based in the theory of                    MC           p
Statistics that can be used to adaptively estimate the num- where N is the number of samples coming from the true dis-
ber of particles to represent the target posterior distribution tribution p(x), and the subscript p expresses that the variance
without adding a signiﬁcant load to the normal operation of involved is computed using the target distribution.
the ﬁlter. At each cycle of the particle ﬁlter, these techniques When the samples come from an importance function q(x),
estimate the number of particles that, with a certain level of the variance of the estimator corresponds to the variance
conﬁdence, limits the maximum error in the approximation. of Importance Sampling (IS), which is given by [Geweke,
                                                      1989]:
3.1  KLD-Sampling Revised                                   N                     2    2          2
                                                      V ar[EIS(x)] = Eq((x−Ep(x))  w(x) )/NIS = σIS/NIS,
The KLD-Sampling algorithm [Fox, 2001] is a method to                                                 (5)
adaptively estimate the number of samples needed to bound where w(x) corresponds to p(x)/q(x) the weights of IS and
the error of the particle ﬁlter. The error is measured by
                                                      NIS  is the number of samples coming from the importance
the Kullback-Leibler divergence (KL-divergence) between function.
  1Here complexity is understood in terms of the amount of infor- 2In the rest of the section, we concentrate on one iteration of the
mation needed to code the distribution.               ﬁlter and we drop the subscript t.  To achieve similar levels of accuracy, the variance of both 3.3 Testing the Bounds
estimators should be equal. This allow us to ﬁnd a relation Figure 1 shows the distributions used to test the bounds.
that quantiﬁes the equivalence between samples from the true The true distribution corresponds to p(x) = 0.5 N (3, 2) +
and the proposal density,                             0.5 N (10, 2) and the importance function to q(x) =
                    N   V ar (x)                      0.5 N (2, 4) + 0.5 N (7, 4). In all the trials, the desired error
               N =   IS    p                    (6)
                         2                            was set to 0.01 and the conﬁdence level to 95%.
                        σIS
  Replacing (6) in (3) allows us to correct the bound given
                                                                 0.16                  Samples from p(x)
by KLD-Sampling when the samples do not come from the                                  Samples from q(x)
                                                                 0.14
                                                                                    ___
true distribution but from an importance function:                                     True distribution p(x)
                       2                                         0.12               − − −Proposal q(x)
                     σIS    1  2
            NIS  >           χk−1,1−δ.          (7)               0.1
                   V arp(x) 2²
                                                                 0.08
                                  2                              pr(x)
  Using MC integration, V arp(x) and σIS can be estimated
by:                                                              0.06
                              PN    2                            0.04
                                   x wi
                2         2     i=1 i           2                0.02
 V arp(x) = Ep(x ) − Ep(x) ≈   PN       − Ep(x)
                                    w                             0
                                 i=1  i                           −5    0     5    10    15   20
and                                                                             x
      P             P                 P
        N  x2 w2  2   N  x  w2 E (x)    N  w2 E  (x)2
 2      i=1  i i      i=1  i i  p       i=1  i  p     Figure 1: Inefﬁcient allocation of samples due to a mismatch be-
σIS ≈  P         −     P            +    P
          N               N                 N         tween p(x) and q(x).
          i=1 wi          i=1 wi            i=1 wi
                                                (8)
              Pn         Pn
  with Ep(x) =  i=1 xi wi/  i=1 wi. Equation (8) shows  Figure 2a shows the number of particles predicted by the
that using appropriate accumulators, it is possible to calculate different bounds. The predicted number of particles is highly
the bound incrementally keeping the O(N) complexity of the consistent. Also, the revised versions of KLD-Sampling con-
ﬁlter.                                                sistently require a larger number of particles than the original
                                                      algorithm. This is expected because of the clear mismatch
3.2  Asymptotic Normal Approximation                  between the proposal and the true densities.
Usually, the particle ﬁlter keeps track of the posterior density Figure 2b shows the resulting KL-divergence for each
with the goal of estimating the mean or higher order moments case. It is interesting to note how the practical results match
of the state. This suggests an alternative error metric to deter- closely the theoretical ones. Using the original version of
mine the number of particles. Instead of checking the accu- KLD-Sampling, the error in the estimation is signiﬁcantly
racy of the estimation of the posterior, it is possible to check greater than the one speciﬁed. However, when the same
the accuracy of a particle ﬁlter in the estimation of a moment predicted number of particles is sampled from the true dis-
of this density.                                      tribution (solid-line), the resulting error matches closely the
  Under weak assumptions and using the strong law of large one speciﬁed. This shows clearly that constraining the sam-
numbers, it is possible to show that at each iteration the esti- pling to the right assumption, the original bound predicted
mation of the mean given by the particle ﬁlter is asymptoti- by KLD-Sampling is correct. In the case of the revised ver-
cally unbiased [DeGroot, 1989]. Furthermore, if the variance sions of KLD-Sampling the resulting error using Equation (7)
of this estimator is ﬁnite, the central limit theorem justiﬁes matches closely the one speciﬁed. In the same way, the error
an asymptotic normal approximation for it [DeGroot, 1989], provided by the bound in Equation (11) also matches closely
which is given by:                                    the level speciﬁed.
                               2
           EP F (x) ∼ N (Ep(x), σIS/NIS)        (9)
where N (µ, σ2) denotes the normal distribution with mean µ 4 Adaptive Propagation of the Particles
and standard deviation σ.                             The regular implementation of the particle ﬁlter propagates
  Using this approximation, it is possible to build a one sided the particles using the dynamic prior p(xt/~yt−1). This strat-
conﬁdence interval for the number of particles that limits the egy has the limitation of propagating the samples without
error in the estimation of the mean:                  considering the most recently available observation, yt. Im-
            EP F (x) − Ep(x)                          portance sampling suggests that one can use alternative prop-
        P (|                |≤ ²) ≥ (1 − α)    (10)
                 Ep(x)                                agation functions that can provide a better allocation of the
                                                      samples, for example, a suitable functional of y . Unfor-
where | · | denotes absolute value; Ep(x) is the true mean                                      t
value of the state; ² corresponds to the desired error; and (1 − tunately, the use of an arbitrary importance function signif-
α) corresponds to the conﬁdence level.                icantly increases the computational load of the particle ﬁlter.
  Following the usual derivation of conﬁdence intervals, In this case, as opposed to the standard particle ﬁlter, the esti-
Equation (10) produces the following bound for the number mation of each weight requires the evaluation of the dynamic
of particles:                                         prior. This section shows a method to build an importance
                         2     2                      function that takes into account the most recent observation
                       Z1−α/2 σIS
                                                      yt without increasing the computational complexity of the ﬁl-
                 NIS ≥   2      2              (11)
                        ² Ep(xt)                      ter.                   4
                 x 10
                4.5                                   0.07

                              ↓ KLD−Sampling revised using Eq.(7)
                 4
                                                      0.06
                                                         ← Original KLD−Sampling

                3.5
                                                      0.05

                 3
                                                      0.04

                2.5

                                                      0.03


                 2            ↓ Using asymptotic normal approximation KL−Divergence _____ KLD−Sampling using samples from true distribution

                Number  of Particles                     −o−o− KLD−Sampling revised using Eq.(7)
                                                      0.02 −.−.−. Using asymptotic normal approximation
                1.5

                                                      0.01
                 1            ↓ Original KLD−Sampling

                                                  a)                                     b)
                0.5                                    0
                 0   5   10   15  20  25  30  35  40    0   5   10  15  20  25  30   35  40
                              Independent Runs                      Independent Runs

                     Figure 2: a) Number of particles given by each bound. b) Resulting KL-divergence.

                                                                                        k
4.1  Previous Work                                    drawn from mixture components p(xt/xt−1) associated with
In the literature about particle ﬁlters and importance sam- areas of high probability under the likelihood function.
pling, it is possible to ﬁnd several techniques that help to Under the importance sampling approach, it is possible to
                                                      generate a new set of coefﬁcients β∗ that takes into account y
allocate the samples in areas of high likelihood under the tar-                    k                    t
                                                                                         p(x   /~y )
get distribution. The most basic technique is rejection sam- by sampling from the importance function t−1 t . In this
                                                                         xi                    p(x /~y  )
pling [Tanner, 1996]. The idea of rejection sampling is to way, the set of samples t from the dynamic prior t t−1
accept only the samples with an importance weight above a is generated by sampling from the mixture,
                                                                        Xn
suitable value. The drawback is efﬁciency: there is a high                  ∗       k
rejection rate in cases where the proposal density does not                βk p(xt/xt−1)             (13)
match closely the target distribution. In [West, 1993], West            k=1
                                                                                    i
presents a kernel-based approximation to build a suitable im- and then adding to each particle xt a compensatory weight
portance function. However, the computational complexity given by,
of the method is unaffordable. In the context of mobile robot
                                                                p(xk /~y  )
localization, Thrun et al. [Thrun et al., 2000] propose to sam- i  t−1 t−1          i         k
                                                          wt =      k      ,  with xt ∼ p(xt/xt−1)   (14)
ple directly from the likelihood function, but in many appli-    p(xt−1/~yt)
cations this is not feasible or prohibitive.                                            i  i n
                                                      The resulting set of weighted samples {xt, wt}i=1 still comes
  Pitt and Shephard propose the auxiliary particle ﬁlter [Pitt from the dynamic prior, so the computational complexity of
and Shephard, 1999]. They augment the state representation the resulting ﬁlter is still O(N). The extra complexity of this
with an auxiliary variable. To sample from the resulting joint operation comes from the need to evaluate and to draw sam-
density, they describe a generic scheme with computational                             i
                                                      ples from the importance function p(xt−1/~yt). Fortunately,
complexity of O(N). The disadvantage is the additional com- the calculation of this function can be obtained directly from
plexity of ﬁnding a convenient importance function. Pitt and the operation of the regular particle ﬁlter. To see this clearly,
Sheppard provide just general intuitions about the form of a consider the following:
this function. In this paper we improve on this point by pre-
senting a method to ﬁnd a suitable importance function. p(xt, xt−1/~yt) ∝ p(yt/xt, xt−1, ~yt−1) p(xt, xt−1/~yt−1)
                                                                     ∝  p(yt/xt) p(xt/xt−1, ~yt−1)p(xt−1/~yt−1)

4.2  Adaptive Propagation of the Samples                             ∝  p(yt/xt)p(xt/xt−1)p(xt−1/~yt−1) (15)
Sampling from the dynamic prior in Equation (2) is equiva- Equation (15) shows that, indeed, the regular steps of the
lent to sample from the following mixture distribution: particle ﬁlter generate an approximation of the joint density
                                                      p(x , x  /~y ). After re-sampling from p(x /~y ), prop-
                       Xn                                t  t−1  t                         t−1  t−1
                                   k                  agating these samples with p(xt/xt−1), and calculating the
           p(xt/~yt−1) ≈   βk p(xt/xt−1)       (12)                                               i  i
                                                      weights p(yt/xt), the set of resulting sample pairs (xt, xt−1)
                       k=1                                                       i
                                                      with correcting weights p(yt/xt) forms a valid set of sam-
where the mixture coefﬁcients βk are proportional to  ples from the joint density p(xt, xt−1/~yt). Considering that
p(xt−1/~yt−1). The key observation is that under this scheme p(xt−1/~yt) is just a marginal of this joint distribution, the set
                                                                         i
the selection of each propagation density depends on the mix- of weighted-samples xt−1 are valid samples from it.
ture coefﬁcients βk’s, which do not incorporate the most re- The previous description provides an adaptive algorithm
cent observation yt. From an MC perspective, it is possible to that allows the particle ﬁlter to use yt in the allocation of
achieve a more efﬁcient allocation of the samples by includ- the samples. First, N particles are used to generate the im-
ing yt in the generation of the coefﬁcients. The intuition is portance function p(xt−1/~yt). Then, starting from this im-
that the incorporation of yt increases the number of samples portance function, another N particles are used to generatethe desired posterior p(xt/~yt). The relevant compensatory Figure 3 shows the results of tracking the targets using the
weights are calculated according to Equation (14) and the self adaptive particle ﬁlter. The bounding boxes correspond
likelihood term P (yt/xt). The resulting ﬁlter has a computa- to the most probable hypotheses in the sample set used to
tional complexity of O(2N).                           estimate the posterior distributions of the states. In the esti-
  In the previous algorithm the overlapping between a reg- mation of the number of particles, we just consider the x and
ular iteration of the regular particle ﬁlter and the process y coordinates of the center of the bounding boxes, assuming
of generating the importance function provides a convenient independence to facilitate the use of Equation (7). We set
way to perform an online evaluation of the beneﬁts of up- the desired error to 0.01 and the conﬁdence level to 95%. A
dating the dynamic prior with information from the last ob- minimum number of 1000 samples is always used to ensure
servation. While in cases of a poor match between the dy- that convergence has been achieved. In the adaptation of the
namic prior and the posterior distribution the updating of the propagation function we set the threshold for the entropy of
dynamic prior can be beneﬁcial, in cases where these distri- the weights in 2.
butions agree, the updating does not offer a real advantage, Figure 4-left shows the number of particles needed to esti-
and the extra processing should be avoided. To our current mate the posterior distribution of the ball at each frame with-
knowledge, this issue has not been addressed before.  out adapting the propagation function. Figure 4-right shows
  The basic idea is to quantify at each iteration of the par- the number of particles in the case of adapting the importance
ticle ﬁlter the trade-off between continuing drawing samples function. The tracking engine decides to adapt the importance
from a known but potentially inefﬁcient importance function function at all the frames where the ball travels from one child
p(xt−1/~yt−1) versus incurring in the cost of building a new to the other (Frames 3-7).
importance function p(xt−1/~yt) that provides a better alloca- In the case of tracking the child, the result shows that there
tion of the samples under the likelihood function. The impor- is not a major difference between the self adaptive particle
tant observation is that, once the regular particle ﬁlter reaches ﬁlter and the regular ﬁlter. The self adaptive ﬁlter needs a
an adequate estimate, it can be used to estimate both the pos- roughly constant number of particles during the entire se-
terior distribution p(xt/~yt) and the updated importance func- quence without needing to adapt the importance function.
tion p(xt−1/~yt).                                     This is expected because the child has only a small and slow
  The last step of the algorithm is to ﬁnd a metric that pro- motion around a center position during the entire sequence.
vides a way to quantify the efﬁciency in the allocation of the Therefore the stationary Gaussian motion model is highly ac-
samples. Considering that the efﬁciency in the allocation of curate and there is not a real advantage of adapting the num-
the samples depends on how well the dynamic prior resem- ber of particles or the propagation function.
bles the posterior distribution, an estimation of the distance In the case of the ball, the situation is different. During
between these two distributions is a suitable index to quan- the period that the ball travels from one child to the other
tify the effectiveness of the propagation step. We found a (Frames 3 to 7), it has a large and fast motion, therefore the
convenient way to estimate the Kullback-Leibler divergence Gaussian motion model is a poor approximation of the real
(KL-divergence) between these distributions, and in general motion. As a consequence there is a large mismatch between
between a target distribution p(x) and an importance function the dynamic prior and the posterior distribution. This pro-
q(x):                                                 duces an inefﬁcient allocation of the samples and the estimate
         KL(p(x), q(x)) ≈ log(N) − H(w ˆi).    (16)   without adapting the importance function needs a larger set of
  Equation (16) states that for a large number of particles, samples to populate the relevant parts of the posterior. In con-
the KL-divergence between the dynamic prior and the poste- trast, when adapting the importance function during Frames
rior distribution can be estimated by calculating how far the 3 to 7 it is possible to observe a signiﬁcant reduction in the
                                                      number of samples due to a better allocation of them.
entropy of the distribution of the weights, H(w ˆi), is from the
entropy of a uniform distribution (log(N)). This is an intu-
itive result because in the ideal case of importance sampling, 6 Conclusions
where p(x) = q(x), all the weights are equal. In consequence
the entropy of the weights is a suitable value to quantify the In this paper we present a self adaptive version of the parti-
efﬁciency in the allocation of the samples.           cle ﬁlter that uses statistical techniques to estimate a suitable
                                                      number of particles and to improve the propagation function.
                                                      In terms of the estimation of the number of particles, the vali-
5   Application                                       dation of the bounds using a synthetic example shows that the
To illustrate the advantages of the self adaptive particle ﬁlter, empirical results match closely the theoretical predictions. In
we use a set of frames of a video sequence consisting of two particular, the results indicate that by considering the com-
children playing with a ball. The goal is to keep track of the plexity of the true density and how closely the proposal den-
positions of the ball and the left side child. Each hypothesis sity mimics the true density, the new bounds show a clear im-
about the position of a target is given by a bounding box de- provement over previous techniques such as KLD-Sampling.
ﬁned by its height, width, and the coordinates of its center. The mechanisms used by the self adaptive ﬁlter to adapt
The motion model used for the implementation of the parti- the importance function and to identify when the adaptation
cle ﬁlter corresponds to a Gaussian function of zero mean and of the importance function may be beneﬁcial proved to be
diagonal covariance matrix with standard deviations of 20 for highly relevant. Using these mechanisms to track targets in
the center of each hypothesis and 0.5 for the width and height. a real video sequence, the self adaptive ﬁlter was able to ef-