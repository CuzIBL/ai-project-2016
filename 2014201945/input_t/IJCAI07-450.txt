                  Learning to Identify Unexpected Instances in the Test Set 

              Xiao-Li Li                          Bing Liu                        See-Kiong Ng
           Institute for Infocomm            Department of Computer             Institute for Infocomm
               Research,            Science, University of Illinois at Chicago, 851  Research,
         21 Heng Mui Keng Terrace,            South Morgan Street,            21 Heng Mui Keng Terrace,
            Singapore, 119613                Chicago, IL 60607-7053               Singapore, 119613
           xlli@i2r.a-star.edu.sg                liub@cs.uic.edu                 skng@i2r.a-star.edu.sg

                    Abstract                          may be formed at a later stage as the disease evolves due to
                                                      mutations or other cancer-causing agents. This phenomenon
    Traditional classification involves building a classi-
                                                      is not uncommon even in the seemingly simpler application
    fier using labeled training examples from a set of
                                                      domains. For example, in document classification, topics are
    predefined classes and then applying the classifier to
                                                      often heterogeneous and new topics evolve over time. A
    classify test instances into the same set of classes. In
                                                      document classifier built for classifying say, computer sci-
    practice, this paradigm can be problematic because ence papers, would face the similar problems as the cancer
    the test data may contain instances that do not be-
                                                      classifier described above. This is because computer science
    long to any of the previously defined classes. De-
                                                      is a heterogeneous and increasingly cross-disciplinary do-
    tecting such unexpected instances in the test set is an
                                                      main; it is also a rapidly evolving one with new topics being
    important issue in practice. The problem can be
                                                      created over time.
    formulated as learning from positive and unlabeled
                                                        Thus, a classifier created based on the notion of a fixed set
    examples (PU  learning). However, current PU      of predefined classes is bound to be inadequate in the com-
    learning algorithms require a large proportion of
                                                      plex and dynamic real-world in the long run, requiring the
    negative instances in the unlabeled set to be effec-
                                                      user to manually go through the classification results to re-
    tive. This paper proposes a novel technique to solve
                                                      move the unexpected instances. In practice, a competent clas-
    this problem in the text classification domain. The
                                                      sifier should learn to identify unexpected instances in the test
    technique first generates a single artificial negative
                                                      set so as to automatically set these unclassifiable instances
    document AN.ThesetsP and {AN} are then used to    apart. In some applications, this can be important in itself. For
   build a naïve Bayesian classifier. Our experiment
                                                      example, in the cancer example above, detection of the unex-
   results show that this method is significantly better
                                                      pected instances can alert the scientists that some new medi-
   than existing techniques.
                                                      cal discovery (a new cancer subtype) may have occurred.
                                                        In recent years, researchers have studied the problem of
1  Introduction                                       learning from positive and unlabeled examples (or PU
Classification is a well-studied problem in machine learn- learning). Given a positive set P and an unlabelled set U,a
ing. Traditionally, to build a classifier, a user first collects a PU learning algorithm learns a classifier that can identify
set of training examples that are labeled with predefined or hidden positive documents in the unlabeled set U.Our
known classes. A classification algorithm is then applied to problem of identifying unexpected instances in the test set
the training data to build a classifier that is subsequently can be modeled as a PU learning problem by treating all the
employed to assign the predefined classes to instances in a training data as the positive set P and the test set as the un-
test set (for evaluation) or future instances (in practice). labeled set U. A classifier can then be learned using PU
  This paradigm can be problematic in practice because learning algorithms to classify the test set to identify those
some of the test or future instances may not belong to any of unexpected (or negative) instances before applying a tradi-
the predefined classes of the original training set. The test set tional classifier to classify the remaining instances into the
may contain additional unknown subclasses, or new sub- original predefined classes.
classes may arise as the underlying domain evolves over time. However, as the current PU techniques operate by trying
For example, in cancer classification, the training set consists to identify an adequate set of reliable negative data from the
of data from currently known cancer subtypes. However, unlabeled set U to learn from, they require a large propor-
since cancer is a complex and heterogeneous disease, and still tion of unexpected instances in the unlabeled set U to be
a perplexing one to-date, it is likely that the test data contain effective. In practice, the number of unexpected instances
cancer subtypes that are not yet medically classified (they are in the test data can be very small since they are most likely
therefore not covered in the training data). Even if the training to be arising from an emerging class. This means that the
data do contain all the current cancer subtypes, new subtypes classifiers built with existing PU learning techniques will


                                                IJCAI-07
                                                  2802perform poorly due to the small number of unexpected  stances in test set T that do not belong to any of the training
(negative) instances in U.                            classes ci. In the next subsection (Section 3.1), we describe a
  In this paper, we propose a novel technique called LGN baseline algorithm that directly applies PU learning tech-
(PU Learning by Generating Negative examples), and we niques to identify unexpected instances. Then, in Section
study the problem using text classification. LGN uses an en- 3.2, we present our proposed LGN algorithm.
tropy-based method to generate a single artificial negative
document AN based on the information in P and U,inwhich 3.1 Baseline Algorithms: PU Learning
the features’ frequency distributions correspond to the de- To recapitulate, our problem of identifying unexpected in-
grees of “negativeness” in terms of their respective entropy stances in the test set can be formulated as a PU learning
values. A more accurate classifier (we use the naïve Bayesian problem as follows. The training instances of all classes are
method) can be built to identify unexpected instances with the first combined to form the positive set P. The test set T then
help of the artificial negative document AN. Experimental forms the unlabeled set U, which contains both positive in-
results on the benchmark 20 Newsgroup data showed that stances (i.e., those belonging to training classes ci) and nega-
LGN outperforms existing methods dramatically.        tive/unexpected instances in T (i.e., those not belonging to
                                                      any training class ci). Then, PU learning techniques can be
2   Related Work                                      employed to build a classifier to classify the unlabeled set U
PU learning was investigated by several researchers in recent (test set T) to identify negative instances in U (the unexpected
years. A study of PAC learning from positive and unlabeled instances). Figure 1 gives the detailed framework for gener-
examples under the statistical query model was given in ating baseline algorithms based on PU learning techniques.
[Denis, 1998]. [Liu et al., 2002] reported sample complexity
results and showed how the problem may be solved.     1.  UE = ;
  Subsequently, a number of practical algorithms [Liu et al., 2. P = training examples from all classes (treated as positive);
2002; Yu et al., 2002; Li and Liu, 2003] were proposed. They 3. U = T (test set, ignore the class labels in T if present);
all conformed to the theoretical results in [Liu et al., 2002] 4. Run an existing PU learning algorithm with P and U to build
                                                          a classifier Q;
following a two-step strategy: (1) identifying a set of reliable          ∈
negative documents from the unlabeled set; and (2) building a 5. For each instance di U (which is the same as T)
classifier using EM or SVM iteratively. Their specific differ- 6. Use a classifier Q to classify di
                                                      7.     If di is classified as negative then
ences in the two steps are as follows. S-EM proposed in [Liu           ∪
et al., 2002] is based on naïve Bayesian classification and the 8. UE = UE {di};
EM  algorithm [Dempster, 1977]. The main idea was to first 9. output UE
use a spying technique to identify some reliable negative Figure 1. Directly applying existing PU learning techniques
documents from the unlabeled set, and then to run EM to
build the final classifier. PEBL [Yu et al., 2002] uses a dif- Inthebaselinealgorithm,weuseasetUE to store the
ferent method (1-DNF) to identify reliable negative examples negative (unexpected) instances identified. Step 1 initializes
and then runs SVM iteratively to build a classifier.  UE to the empty set, while Steps 2-3 initialize the positive set
  More recently, [Li and Liu, 2003] reported a technique P and unlabeled set U as described above. In Step 4, we run
called Roc-SVM. In this technique, reliable negative docu- an existing PU learning algorithm (various PU learning tech-
ments are extracted by using the information retrieval tech- niques can be applied to build different classifiers) to con-
nique Rocchio [Rocchio, 1971], and SVM is used in the struct a classifier Q. We then employ the classifier Q to clas-
second step. In [Fung et al., 2005], a method called  sify the test instances in U in Steps 5 to 8. Those instances
PN-SVM   is proposed to deal with the situation when the that are classified by Q as negative class are added to UE as
positive set is small. All these existing methods require that unexpected instances. After we have iterated through all the
the unlabeled set have a large number of hidden negative test instances, Step 9 outputs the unexpected set UE.
instances. In this paper, we deal with the opposite problem, 3.2 The Proposed Technique: LGN
i.e. the number of hidden negative instances is very small.
  Another line of related work is learning from only positive In traditional classification, the training and test instances are
data. In [Scholkopf, 1999], a one-class SVM is proposed. It drawn independently according to some fixed distribution D
was also studied in [Manevitz and Yousef, 2002] and   over X × Y,whereX denotes the set of possible documents in
[Crammer, 2004]. One-class SVM builds a classifier by our text classification application, and Y ={c1, c2, ..., cn}de-
treating the training data as the positive set P. Those instances notes the known classes. Theoretically, for each class ci, if its
in test set that are classified as negative by the classifier can training and test instances follow the same distribution, a
be regarded as unexpected instances. However, our experi- classifier learned from the training instances can be used to
ments show that its results are poorer than PU learning, which classify the test instances into the n known classes.
indicates that unlabeled data helps classification.     In our problem, the training set Tr with instances from
                                                      classes c1, c2, ..., cn are still drawn from the distribution D.
3   The Proposed Algorithm                            However, the test set T consists of two subsets, T.P (called
                                                      positive instances in T)andT.N (called unexpected / negative
Given a training set {ci}(i =1,2,…,n) of multiple classes,
our target is to automatically identify those unexpected in- instances in T). The instances in T.P are independently drawn


                                                IJCAI-07
                                                  2803from D, but the instances in T.N are drawn from an unknown and with Laplacian smoothing,
                                                                              |D|
and different distribution Du. Our objective is to identify all           +             Ρ
                                                                         1  ∑  = N(wt , d i ) r(c j | d i ) (2)
                                                            Ρr(w | c ) =      i 1
the instances drawn from this unknown distribution D ,orin      t  j         |V | |D|
                                             u                        |V | +∑∑N(w       , d )Ρr(c | d )
other words to identify all the hidden instances in T.N.                     s==1 i 1  s  i    j  i
                                                      where N(wt,di) is the count of the number of times that the
  Let us now  formally reformulate this problem as a                                       ∈
two-class classification problem without labeled negative word wt occurs in document di and Pr(cj|di) {0,1} depend-
training examples. We first rename the training set Tr as the ing on the class label of the document.
positive set P by changing every class label ci ∈ Y to “+” Finally, assuming that the probabilities of the words are
(the positive class). We then rename the test set T as the independent given the class, we obtain the NB classifier:
                                                                                  |d |
unlabeled set U, which comprises both hidden positive in-                 Ρr(c )∏  i Ρr(w  | c )
                                                             Ρ        =       j   k=1   di ,k j
                                                              r(c j | di )
stances and hidden unexpected instances. The unexpected                   |C|       |di |
                                                                        ∑   Ρr(c )∏   Ρr(w   | c )    (3)
instances in U (or T) are now called negative instances with              r=1   r   k=1   di ,k r
the class label “−” (bear in mind that there are many hidden In the naive Bayesian classifier, the class with the highest
positive instances in U). A learning algorithm will select a Pr(cj|di) is assigned as the class of the document.
function f from a class of functions F: X → {+, −}tobe
used as a classifier that can identify the unexpected (nega- GENERATING NEGATIVE DATA
tive) instances from U. The problem here is that there are no In this subsection, we present our algorithm to generate the
labeled negative examples for learning. Thus, it becomes a negative data. Given that in a naïve Bayesian framework, the
problem of learning from positive and unlabeled examples conditional probabilities Pr(wt|-) (Equation (2)) are computed
(PU learning). As discussed in the previous section, this based on the accumulative frequencies of all the documents in
problem has been studied by researchers in recent years, but the negative class, a single artificial negative instance AN
existing PU techniques performed poorly when the number would work equally well for Bayesian learning. In other
of negative (unexpected) instances in U is very small. To words, we need to generate the negative document AN in such
                                                                           −
address this, we will propose a technique to generate artifi- awaytoensurePr(w+|+) Pr(w+|-) > 0 for a positive feature
                                                                    −
cial negative documents based on the given data.      w+ and Pr(w-|+) Pr(w-|-) < 0 for a negative feature w-.We
   Let us analyze the problem from a probabilistic point of use an entropy-based method to estimate if a feature wi in U
view. In our text classification problem, documents are com- has significantly different conditional probabilities in P and in
                                                      U (i.e, (Pr(wi|+) and Pr(wi|-))). The entropy equation is:
monly represented by frequencies of words w1, w2, ..., w|v| that
appear in the document collection, where V is called the vo-            = −
                                                               entropy(wi ) ∑Pr(wi | c) *log(Pr(wi | c)) (4)
                                                                           c∈{+,−}
cabulary.Letw+ represent a positive word feature that char-
acterizes the instances in P and let w- represent a negative The entropy values show the relative discriminatory
feature that characterizes negative (unexpected) instances in power of the word features: the bigger a feature’s entropy is,
U.IfU contains a large proportion of positive instances, then the more likely it has similar distributions in both P and U
                                                      (i.e. less discriminatory). This means that for a negative
the feature w+ will have similar distribution in both P and U.
However, for the negative feature w- , its probability distribu- feature w-, its entropy entropy(w-)issmallasPr(w-|-) (w-
tions in the set P and U are very different. Our strategy is to mainly occurring in U) is significantly larger than Pr(w-|+),
exploit this difference to generate an effective set of artificial while entropy(w+) is large as Pr(w+|+) and Pr(w+|-) are simi-
negative documents N so that it can be used together with the lar. The entropy (and its conditional probabilities) can
positive set P for a classifier training to identify negative therefore indicate whether a feature belongs to the positive
(unexpected) documents in U accurately.               or the negative class. We generate features for AN based on
   Given that we use the naïve Bayesian framework in this the entropy information, weighted as follows:
work, before going further, we now introduce naïve Baye-                        entropy(w )
                                                                  q(w ) = 1−            i             (5)
sian classifier for text classification.                             i
                                                                           max j=1,2...,|V | (entropy(w j ))

NAÏVE BAYESIAN CLASSIFICATION                           If q(wi)= 0, it means that wi uniformly occurs in both P
Naïve Bayesian (NB) classification has been shown to be an and U and we therefore do not generate wi in AN.Ifq(wi)=1,
effective technique for text classification [Lewis, 1994; we can be almost certain that wi is a negative feature and we
McCallum and Nigam, 1998]. Given a set of training docu- generate it for AN, based on its distribution in U.Inthisway,
ments D, each document is considered an ordered list of those features that are deemed more discriminatory will be

words. We use wdi,k to denote the word in position k of generated more frequently in AN. For those features with
document di, where each word is from the vocabulary V = q(wi) between the two extremes, their frequencies in AN are
{w1, w2, ..., w|V |}. The vocabulary is the set of all words we generated proportionally.
consider for classification. We also have a set of predefined We generate the artificial negative document AN as fol-
classes, C ={c1, c2,...,c|C|}. In order to perform classifica- lows. Given the positive set P and the unlabeled set U,we
tion, we need to compute the posterior probability, Pr(cj|di), compute each word feature’s entropy value. The feature’s
where cj is a class and di is a document. Based on the Baye- frequency in the negative document AN is then randomly
sian probability and the multinomial model, we have   generated following a Gaussian distribution according to
                         |D| Ρ                                                            ∈
                       ∑  = r(c j | di )              q(wi)=1-entropy(wi)/max(entropy(wj), wj V). The detailed
               Ρr(c ) =  i 1
                   j        | D |               (1)   algorithm is shown in Figure 2.


                                                IJCAI-07
                                                  28041. AN =;                                             that summarizes the unlabelled data set, but with the fea-
2. P = training documents from all classes (treated as positive); tures indicative of positive class dramatically reduced.
3. U = T (test set, ignore the class labels in T if present);
                                                      BUILDING THE FINAL NB CLASSIFIER
4. For each feature w ∈ U
                 i                                    Finally, we describe how to build an NB classifier with the
5.    Compute the frequency of w in each document d freq(w ,
                           i              k      i    positive set P and the generated single negative document
      dk), dk ∈ U;
                                                      AN to identify unexpected document instances. The detailed
                   ∑  freq(wi ,dk )
6.    Let mean     ∈            where Dw is the set of
                  dk Dw                i
             μ  =    i         ,                      algorithm is shown in Figure 3.
               wi
                      | Dw |
                         i                            1. UE =;
      documents in containing wi
                                                      2. Build a naïve Bayesian classifier Q with P and {AN}using
7.    Let variance 2   1                     ;
               σ   =         ∑( freq(w ,d ) − μ )2       Equations (1) and (2);
                wi       −          i k   wi
                    (| Dw | 1) d ∈D
                       i    k wi                      3. For each document di ∈ U
                  ∈
8. For each feature wi V                              4.    Using Q to classify di using Equation (3);
9.    Compute Pr(wi|+), Pr(wi |-) using Equation (2) assuming 5. If (Pr(-|di)>Pr(+|di))
      that all the documents in U are negative;       6.      UE = UE {di};
10.   Let          = −                      ;         7. output UE;
         entropy(wi ) ∑Pr(wi | c)*log(Pr(wi | c))
                     c∈{+,−}                          Figure 3. Building the final NB classifier
11. Let m =max(entropy(wj)), j =1,...,|V|;
12. For each feature wi ∈ V                             UE  stores the set of unexpected documents identified in U
               entropy(w )                            (or test set T), initialized to empty set in Step 1. In Step 2, we
13.   q(w ) =1−        i ;
         i         m                                  use Equations (1) and (2) to build a NB classifier by comput-

14.  For j =1to |Dwi|*q(wi)                           ing the prior probabilities Pr(+) and Pr(-), and the conditional
15.      Generate a frequency fnew(wi), using the Gaussian probabilities of Pr(wi|+) and Pr(wi|-). Clearly, Pr(wi|+) and
                            (x−μ )2
                           −   wi                     Pr(w |-) can be computed based on the positive set P and the
         distribution 1      2σ 2                         i
                          e    wi
                  μ    2π                             single negative document AN respectively (AN can be regarded
                    wi
16.      A  = A  {(w , fnew(w ))}                     as the average document of a set of virtual negative docu-
          N   N     i      i                          ments). However, the problem is how to compute the prior
17. Output AN
                                                      probabilities of Pr(+) and Pr(-). It turns out that this is not a
Figure 2. Generating the negative document AN         major issue  we can simply assume that we have generated
   In the algorithm, Step 1 initializes the negative docu- a negative document set that has the same number of docu-
                                                      ments as the number of documents in the positive set P.We
ment AN (which consists of a set of feature-frequency pairs)
to the empty set while Steps 2 and Step 3 initialize the posi- will report experimental results that support this in the next
tive set P and the unlabeled set U.FromStep4toStep7,for section. After building the NB classifier Q, we use it to clas-
                                                      sify each test document in U (Steps 3-6). The final output is
each feature wi that appeared in U, we compute its fre-
quency in each document, and then calculate the frequency the UE set that stored all the identified unexpected documents
                                                      in U.
mean and variance in those documents Dwi that contain wi.
These information are used to generate AN later. From Step
8 to Step 10, we compute the entropy of wi using Pr(wi|+) 4 Empirical Evaluation
and Pr(wi|-) (which are computed using Equation (2) by In this section, we evaluate our proposed technique LGN.
assuming that all the documents in U are negative). After We compare it with both one-class SVM (OSVM, we use
obtaining the maximal entropy value in Step 11, we gener- LIBSVM http://www.csie.ntu.edu.tw/~cjlin/libsvm/) and
ate the negative document AN in Steps 12 to 16. In particular, existing PU learning methods: S-EM [Liu et al., 2002],
Step 13 computes q(wi), which shows how “negative” a  PEBL [Yu  et al., 2002] and Roc-SVM [Li and Liu, 2003].
feature wi is in terms of how different the wi’s distributions S-EM and Roc-SVM are publicly available1.Weimple-
in U and in P are the bigger the difference, the higher the mented PEBL as it is not available from its authors.
frequency with which we generate the feature. Steps 14 to

16 is an inner loop and |Dwi|*q(wi) decides the number of 4.1 Datasets
times we generate a frequency for word w .Thus,ifq(w )is
                                   i           i      For evaluation, we used the benchmark 20 Newsgroup col-
small, it means that wi has occurred in both P and U with
similar probabilities, and we generate fewer w .Otherwise, lection, which consists of 11997 documents from 20 differ-
                                        i             ent UseNet discussion groups. The 20 groups were also
wi is quite likely to be a negative feature and we generate it
with a distribution similar to the one in U.Ineachiteration, categorized into 4 main categories, “computer”, “recrea-
                                                      tion”, “science”, and “talk”. We first perform the following
Step 15 uses a Gaussian distribution with corresponding w
    σ                                             i   two sets of experiments:
and  wi to generate a frequency fnew(wi)forwi.Step16
places the pair (wi, fnew(wi)) into the negative document AN. 2-classes: This set of experiments simulates the case in
Finally, Step 17 outputs our generated negative set AN. Note which the training data has two classes, i.e. our positive set
that the frequency for each feature wi in AN may not of an P contains two classes. The two classes of data were chosen
integer value as it is generated by a Gaussian distribution.
                                                         1
AN is essentially a randomly generated aggregated document http://www.cs.uic.edu/~liub/LPU/LPU-download.html


                                                IJCAI-07
                                                  2805from two main categories, “computer” and “science”, in Table 1. Experimental results for α =5%.
which the “computer” group has five subgroups, and the
“science” group has four subgroups. Every subgroup con- Data Set         OSVM   S-EM Roc-SVM  PEBL LGN
sists of 1,000 documents.                              graphic-crypt      22.5  46.3    17.2   0.0  82.1
  Each data set for training and testing is then constructed graphic-electro 17.9 54.1  15.8   3.5  78.0
as follows: The positive documents for both training and graphic-med      17.2  39.0    15.3   0.0  64.9
testing consist of documents from one subgroup (or class) in graphic-space 22.2 49.5    15.7   0.0  71.7
“computer” and one subgroup (or class) in “science”. This os-crypt        23.6  43.1    18.3   0.0  82.8
gives us 20 data sets. For each class (or subgroup), we parti- os-electronics 15.9 39.6 16.3   0.0  80.2
tioned its documents into two standard subsets: 70% for os-med            18.6  36.4    16.5   0.0  75.2
training and 30% for testing. That is, each positive set P for os-space   20.8  40.5    17.9   0.0  78.2
training contains 1400 documents of two classes, and each mac.hardware-crypt 23.1 46.0  17.5   1.2  84.8
test set U contains 600 positive documents of the same two mac.hardware-electro 18.8 42.4 17.5 0.0  84.3
classes. We then add negative (unexpected) documents to U, mac.hardware-med 18.3 52.6   16.5   0.0  70.4
which are randomly selected from the remaining 18 groups. mac.hardware-space 21.3 40.0  17.5   0.0  77.9
  In order to create different experimental settings, we vary ibm.hardware-crypt 25.4 46.5 16.9 0.0 82.9
the number of unexpected documents, which is controlled ibm.hardware-electro 19.6 47.5  17.1   1.3  82.4
by a parameter α, a percentage of |U|, i.e., the number of ibm.hardware-med 17.4 41.9   16.4   0.0  74.5
unexpected documents added to U is α × |U|.            ibm.hardware-space 21.3  41.5    17.4   1.3  75.5
3-classes: This set of experiments simulates the case in windows-crypt    22.6  54.1    17.3   2.3  82.0
which the training data has three different classes, i.e. our windows-electro 19.4 48.2 16.0   0.0  76.3
positive set P contains three classes of data. We used the windows-med    20.4  39.9    16.1   1.3  66.2
same 20 data sets formed above and added another class to windows-space   18.3  34.4    16.4   0.0  69.8
each for both P and U. The added third class was randomly Average         20.2  44.2    16.8   0.5  77.0
selected from the remaining 18 groups. For each data set, Figure 4 shows the macro-average results of all α values
the unexpected documents in U were then randomly se-
                                                      (from 5% to 100%) for all five techniques in the 2-classes
lected from the remaining 17 newsgroups. All other settings
                                                      experiments. Our method LGN outperformed all others sig-
were the same as for the 2-classes case.
                                                      nificantly for α ≤ 60%. When α was increased to 80% and
4.2  Experimental Results                             100%, Roc-SVM achieved slightly better results than LGN.
                                                      We also observe that OSVM, S-EM and Roc-SVM  outper-
2-classes: We performed experiments using all possible c1 formed PEBL since they were able to extract more reliable
and c2 combinations (i.e., 20 data sets). For each technique, negatives than the 1-DNF method used in PEBL. PEBL
namely,OSVM,S-EM,Roc-SVM,PEBLandLGN,we                needed a higher α (200%) to achieve similar good results.
performed 5 random runs to obtain the average results. In
                                                         100.0
each run, the training and test document sets from c1 and c2
as well as the unexpected document instances from the other 80.0
18 classes were selected randomly. We varied α from 5% to
100%. Table 1 shows the classification results of various 60.0
                                                                                                 LGN
                                               α                                                 S-EM
techniques in terms of F-score (for negative class) when =                                       Roc-SVM
                                                          40.0                                   PEBL
5%. The first column of Table 1 lists the 20 different com- F-score                              OSVM
binations of c1 and c2. Columns 2 to 5 show the results of 20.0
four techniques OSVM, S-EM, Roc-SVM   and PEBL re-
spectively. Column 6 gives the corresponding results of our 0.0
technique LGN.                                               5%   10%  15%   20%   40%  60%   80%  100%
  We observe from Table 1 that LGN produces the best re-                a % of unexpected documents
sults consistently for all data sets, achieving an F-score of Figure. 4. The comparison results with different percentages
77.0% on average, which is 54.8%, 32.8%, 60.2% and     of unexpected documents in U in the 2-classes experiments.
76.5% higher than the F-scores of existing four techniques
(OSVM, S-EM, Roc-SVM and PEBL) respectively in abso-  3-classes: Figure 5 shows the 3-classes results where LGN
lute terms. We also see that LGN is highly consistent across still performed much better than the methods when the pro-
different data sets. In fact, we have checked the first step of portion of unexpected documents is small (α ≤ 60%) and
the three existing PU learning techniques and found that comparably with S-EM and Roc-SVM when the proportion
most of the extracted negative documents were wrong. As a is larger. OSVM’s results are much worse than S-EM,
result, in their respective second steps, SVM and EM were Roc-SVM and LGN when α is larger, showing that PU
unable to build accurate classifiers due to very noisy nega- learning is better than one-class SVM in the problem.
tive data. Since the S-EM algorithm has a parameter, we Again, PEBL required a much larger proportion of unex-
tried different values, but the results were similar. pected documents to produce comparable results.


                                                IJCAI-07
                                                  2806