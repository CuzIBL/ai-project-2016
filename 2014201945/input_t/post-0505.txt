             Supervised Local Tangent Space Alignment for Classiﬁcation

                               Hongyu Li1, Wenbin Chen2, I-Fan Shen1
           1Department of Computer Science and Engineering      2Department of Mathematics
                                   Fudan University, Shanghai, China
                               {hongyuli,wbchen,yfshen}@fudan.edu.cn


                    Abstract                              The neighborhood set Ii represents the set of indices for
                                                          the k nearest-neighbors of xi. I is an N × N identity
    Supervised local tangent space alignment (SLTSA)      matrix.
    is an extension of local tangent space alignment
    (LTSA) to supervised feature extraction. Two al-    4. Computing the d + 1 smallest eigenvectors of B and set-
                                                                                                  T
    gorithmic improvements are made upon LTSA for         ting the global coordinates Y = [u2, . . . , ud+1] corre-
    classiﬁcation. First a simple technique is proposed   sponding to the 2nd to d + 1st smallest eigenvalues.
    to map new data to the embedded low-dimensional

    space and make LTSA suitable in a changing, dy-                              0.3
                                                                           Training 1              Training 1
                                                         0.25
                                                                           Training 2              Training 2
                                                                           Training 3 0.25
    namic environment. Then SLTSA is introduced to       0.2                                       Training 3
    deal with data sets containing multiple classes with 0.15                    0.2
                                                         0.1
                                                                                 0.15
    class membership information.                        0.05

                                                          0                      0.1

                                                         −0.05
                                                                                 0.05
                                                         −0.1
1  Introduction                                          −0.15                   0
                                                         −0.2
                                                                                −0.05
In many pattern recognition problems, original data taken −0.25
                                                                                 −0.1
with various capturing devices are usually of high dimension- −0.5 −0.4 −0.3 −0.2 −0.1 0 0.1 0.2 0.3 0.4 0.5 −0.2 −0.15 −0.1 −0.05 0 0.05 0.1 0.15 0.2 0.25 0.3
ality. Dimension reduction has emerged with an aim to obtain      (a)                     (b)

                                                                                 0.3
                                                                           Training 1              Training 1
                                                         0.25
compact representations of the original data while reducing                Training 2              Training 2
                                                                           Training 3 0.25
                                                         0.2                                       Training 3
                                                                           Test 1                  Test 1
unimportant factors. It can make the resulting classiﬁcation               Test 2
                                                         0.15                    0.2               Test 2
                                                                           Test 3                  Test 3
more efﬁcient as fewer variables are being considered which 0.1
                                                                                 0.15
                                                         0.05

reduces the complexity of the resulting classiﬁer. LTSA pro- 0                   0.1

                                                         −0.05
posed in [Zhang and Zha, 2004] is an unsupervised method                         0.05
                                                         −0.1
for nonlinear dimension reduction. It does not make use of −0.15                 0
                                                         −0.2
                                                                                −0.05
the class membership of each point to be projected and lacks −0.25

                                                                                 −0.1
generalization to new data. Here we extend LTSA to a super- −0.5 −0.4 −0.3 −0.2 −0.1 0 0.1 0.2 0.3 0.4 0.5 −0.2 −0.15 −0.1 −0.05 0 0.05 0.1 0.15 0.2 0.25 0.3
vised version and make it suitable in a changing environment.     (c)                     (d)

2  Local Tangent Space Alignment                      Figure 1: (a) and (b): Mapping training samples with LTSA
                                         m
LTSA maps a data set X = [x1, . . . , xN ], xi ∈ R globally and SLTSA respectively. (c) and (d): Mapping unknown test
                                 d
to a data set Y = [y1, . . . , yN ], yi ∈ R with d < m. The samples to the spaces discovered by LTSA and SLTSA.
brief description of LTSA is presented as follows:

 1. Finding k nearest neighbors xij of xi, j = 1, . . . , k. Set
    Xi = [xij, . . . , xik].                          3   Supervised LTSA
 2. Extracting local information by calculating the d Our attempt is to adapt LTSA to a situation when the data
    largest eigenvectors g1, . . . , gd of the correlation ma- come incrementally point by point. We assume that the di-
    trix (X − x¯ eT )T (X − x¯ eT ), and setting G =  mensionality of the embedded space does not grow after pro-
      √   i     i      i     i     P          i
                                 1                    jecting a new point to it, i.e., d remains constant.
    [e/ k, g1, . . . , gd]. Here x¯i = k j xij, e is a k-
    dimensional column vector of all ones.              The proposed technique is based on the fact that new points
                                                      are assumed to come from those parts of the high-dimensional
 3. Constructing the alignment matrix B (please refer to space that have already been explicitly mapped by LTSA. It
    [Zhang and Zha, 2004] for details) by locally summing
                                                      implies that when a new point arrives, a task of interpolation
    with initial B = 0 as follows:
                                                      should be solved. Let X as an input. For a new point xN+1,
                                  T
      B(Ii, Ii) ← B(Ii, Ii) + I − GiGi , i = 1, . . . , N. its closest neighbor xj is looked for from X. If yj is theprojection of xj to the embedded space, there must exist a
point yN+1 around yj corresponding to xN+1.
  LTSA belongs to unsupervised methods; it does not make
use of the class membership of each point. Supervised LTSA
(SLTSA) is proposed for classiﬁcation purposes. The term
implies that membership information is employed to form the
neighborhood of each point. That is, nearest neighbors of a
given point xi are chosen only from representatives of the
same class as that of xi. This can be achieved by artiﬁcially
increasing the shift distances between samples belonging to          (a) Three digits: 0, 1 and 2

different classes, but leaving them unchanged if samples are                     0.05
from the same class.                                     0.25
                                                         0.2
                                                                                 0
  In short, SLTSA is designed specially for dealing with data 0.15
                                                         0.1
sets containing multiple classes. The results obtained with 0.05                −0.05
                                                          0

the unsupervised and supervised LTSA are expected to be  −0.05
                                                                                 −0.1

                                                         −0.1
different as is shown in Fig.1. The iris data set [Blake and               Training 0 Training 0
                                                         −0.15             Training 1 Training 1
                                                                           Training 2 −0.15
                                                         −0.2                       Training 2
Merz, 1998] includes 150 4-D data belonging to 3 different                 Test 0   Test 0
                                                         −0.25             Test 1   Test 1
                                                                           Test 2   Test 2
classes. Here ﬁrst 100 data points are selected as training                      −0.2
                                                            −0.2 −0.1 0 0.1 0.2 0.3 0.4 −0.2 −0.15 −0.1 −0.05 0 0.05
samples and mapped from the 4-D input space to a 2-D fea-       (b) LTSA               (c) SLTSA
ture space using LTSA (Fig.1(b)) and SLTSA (Fig.1(a) ) re-
spectively. In Fig.1(b), three class centers on the feature space
                                                      Figure 2: Recognition of three digits: 0, 1 and 2. (a) Some of
are completely separated. However in Fig.1(a) two classes
                                                      these three digits are shown. They are represented by 20 × 16
(set 2 and 3) overlap such that their boundary cannot be accu-
                                                      binary images which can be considered as points on a 320-
rately determined. Fig.1(c; d) respectively show the mapping
                                                      dimensional space. (b) Mapping these binary digits includ-
of unknown test samples to the two feature spaces discovered
                                                      ing training and test samples to a 2-dimensional feature space
by LTSA and SLTSA. In Fig.1(d), test samples in different
                                                      with LTSA, where these three clusters overlap and are unsep-
classes can be well distributed around class centers. Thus the
                                                      arated. (c) The feature space obtained with SLTSA, which
classiﬁcation of test samples can be easily implemented on
                                                      provides better clustering information.
such space.

4  Results                                            linear manifolds. Thus, such linear methods as PCA cannot
                                                      perform well.
To examine its performance, SLTSA has been applied to a
number of data sets varying in number of samples, dimen- 5 Conclusions
sions and classes.                                    The supervised version of LTSA was proposed here. It takes
  The binarydigits set discussed here consists of 20×16- into account class membership during selecting neighbors.
pixel binary images of preprocessed handwritten digits. In Another enhancement to LTSA is to generalize it to new data
our experiment only three digits: 0, 1 and 2 are dealt with and make it suitable in a changing, dynamic environment.
and some of the data are shown in Fig.2(a). 90 of the 117 bi- We compare SLTSA with LTSA, PCA and supervised LLE
nary images are used as training samples and others as test on a number of data sets, in order to gain insight into what
samples. It is clear that the 320-dimensional binarydigits methods are suitable for data classiﬁcation. The SLTSA
data contain many insigniﬁcant features, so removing them method has been shown to yield very promising classiﬁcation
can make classiﬁcation more efﬁcient. The results after di- results in our experiments when combined with some simple
mension reduction from 320 to 2 with LTSA and SLTSA are classiﬁers.
respectively displayed in Fig.2(b) and 2(c) where two coor-
dinate axes represent the two most important features of the References
binarydigits data. The ﬁgures show that the feature space ob-
tained with SLTSA provides better classiﬁcation information [Blake and Merz, 1998] C.L. Blake and C.J. Merz. Uci
than LTSA. Actually no cases of misclassiﬁcation in the test repository of machine learning databases, 1998.
samples occurred if using the SLTSA method, but LTSA will [Kouropteva et al., 2002] O. Kouropteva, O. Okun, A. Ha-
result in the error rate of 18.52% in the best case.     did, M. Soriano, S. Marcos, and M. Pietikainen. Be-
  Our experimental results conﬁrm that SLTSA generally   yond locally linear embedding algorithm. Technical Re-
leads to better classiﬁcation performance than LTSA when port MVG-01-2002, Machine Vision Group, University of
combined with simple classiﬁers such as k-nn and lda. Be- Oulu, 2002.
side this, the performance of SLTSA is comparable with su- [Zhang and Zha, 2004] Zhenyue Zhang and Hongyuan Zha.
pervised LLE [Kouropteva et al., 2002] and ﬁner than with Principal manifolds and nonlinear dimension reduction via
PCA (please refer to the supplementary document). Maybe  local tangent space alignment. SIAM Journal of Scientiﬁc
the reason is that those points on a high-dimensional input Computing, 26(1):313–338, 2004.
space are more likely to lie close on nonlinear rather than