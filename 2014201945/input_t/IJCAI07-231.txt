          Argumentation Based Contract Monitoring in Uncertain Domains

                             Nir Oren, Timothy J. Norman, Alun Preece
            Department of Computing Science, University of Aberdeen, Aberdeen, Scotland
                                noren,tnorman,apreece@csd.abdn.ac.uk


                    Abstract                          framework. We then present a utility based heuristic that al-
                                                      lows agents to participate in the dialogue game.
    Few existing argumentation frameworks are de-       One possible application of such a framework is contract
    signed to deal with probabilistic knowledge, and  monitoring and enforcement. That is, given a contract and
    none are designed to represent possibilistic knowl- an environment, determining current contract state as well as
    edge, making them unsuitable for many real world  what sanctions should come into effect. Not only is our ap-
    domains. In this paper we present a subjective logic proach able to operate in partially observable domains with
    based framework for argumentation which over-     fallible sensors, but we also take into account the fact that
    comes this limitation. Reasoning about the state  probing a sensor might be associated with a cost. Other
    of a literal in this framework can be done in poly- systems for contract monitoring often require fully observ-
    nomial time. A dialogue game making use of the    able domains with “honest” sensors (e.g. [Xu and Jeusfeld,
    framework and a utility based heuristic for play- 2003]). Even when able to handle faulty sensors, some ap-
    ing the dialogue game are also presented. We then proaches assume that all domain states can be probed, and
    show how these components can be applied to con-  that all such probing is free [Daskalopulu et al., 2002].
    tract monitoring. The dialogues that emerge bear    In the next section, we provide an overview of subjective
    some similarity to the dialogues that occur when  logic. After that, we present our framework, and provide an
    humans argue about contracts, and our approach is illustrative example. We conclude the paper by examining re-
    highly suited to complex, partially observable do- lated research, and examine possible avenues of future work.
    mains with fallible sensors where determining en-
    vironment state cannot be done for free.
                                                      2   Subjective Logic
                                                                    1
1  Introduction                                       Subjective logic is an approach for combining and as-
                                                      sessing subjective evidence from multiple sources based
Most research in argumentation theory has focused on inter- on Dempster-Schafer theory. An opinion ω about an
actions between arguments [Prakken and Vreeswijk, 2002]. atomic proposition φ is represented as a triple ω(φ)=
Researchers have proposed various underlying logics capable b(φ),d(φ),u(φ) where b(φ) represents the level of belief
of representing different domain attributes. For example, a that φ holds; d(φ) stands for the level of disbelief (i.e. the
large body of work exists on representing and using defaults probability that φ does not hold), and u(φ) represents the
for legal argumentation [Prakken and Sartor, 1998]. A num- level of uncertainty of the opinion. b(φ)+d(φ)+u(φ)=1,
ber of researchers have also looked at how argument takes and b(φ),d(φ),u(φ) ∈ [0..1].
place in domains with uncertainty. Most of the work in this A number of operations on opinions have been deﬁned, al-
area is probability theory based, and includes Bayesian based lowing one to create compound opinions from atomic ones.
argumentation frameworks [Vreeswijk, 2004] and Pollock’s These are listed in Figure 1.
OSCAR system [Pollock, 1995].                           Subjective logic allows us to use common logical opera-
  Due to its basis in plausible reasoning, we believe that un- tors such as conjunction, disjunction and negation. The last
certainty theory is more applicable than probabilistic reason- of these is shown in the ﬁgure. The remaining operators in the
ing to many domains of argumentation. In this paper, we ﬁgure are unique to subjective logic. The ⊗ operator is called
present a subjective logic [Jøsang, 2002] based approach to the recommendation operator, where ω(φ) ⊗ ω(ψ) yields an
argumentation. Using subjective logic, we are able to natu- opinion about ω(ψ) given an opinion ω(φ) about the source
rally present arguments in which uncertainty exists, and can informing the computing agent of ω(ψ).Theconsensus oper-
encapsulate concepts such as accrual of arguments [Prakken, ator ⊕, yields a combination of two opinions about the same
2005], which many existing frameworks handle poorly.
  After describing the argumentation framework, we present 1We present a simpliﬁed form of subjective logic here, similar to
a dialogue game which allows agents to make use of the the one presented in [Daskalopulu et al., 2002].

                                                IJCAI-07
                                                  1434                                       ¬ω(φ)=d(φ),b(φ),u(φ)
           ω(φ) ⊗ ω(ψ)=b(φ)b(ψ),b(φ)d(ψ),d(φ)+u(φ)+b(φ)u(ψ)    where φ is an opinion about ψ
    ( ) ⊕   ( )= bA(φ)uB (φ)+uA(φ)bB (φ) dA(φ)uB (φ)+uA(φ)dB(φ) uA(φ)uB (φ)  = ( )+ ( ) − ( )   ( )
 ωA φ    ωB  φ             k         ,          k         ,    k     , k  uA φ    uB φ    uA φ uB  φ
                  Figure 1: The Subjective Logic negation, recommendation and consensus operators

                                                      0     (  ) 1 −   
thing. Note that the consensus operator is undeﬁned when no ,min bpi , bpi otherwise. This deﬁnition captures the
uncertainty in opinions exists.                       intuition that an argument is as strong as its weakest element.
                                                        Facts are a special type of argument, as their conclusions
3  Formalism                                          may not be negated literals. An opinion ω(F ) may be associ-
                                                      ated with a fact F =(,l) where l ∈ Σ+.
In this section, we present three strands of work that tie up to
                                                        Given a set of arguments about a literal and its negation,
form our framework. At the lowest level lies the argumenta-                               l
                                                      as well as a set of supporting facts, we may compute a ﬁnal
tion framework. It is here that we deﬁne arguments, and how
                                                      opinion regarding the value of the literal by utilising the con-
they interact with each other. Afterwards, we present a dia-
                                                      sensus operator ⊕. Slightly more formally, assuming we are
logue game, as well as the agents which play the game and
the heuristics they use for deciding which arguments to ad- given a set of arguments and facts S as well as their opinions
                                                        ( ) ∀ ∈  , we may compute an opinion ( ) for a literal
vance and which sensors to probe. Finally, we describe how ω s, s S                     ω l           l
                                                      as    ω(s)    ¬ω(t) for all s =(P ,c ),t =(P ,c ) ∈ S
to transform contracts into a form usable by our framework. s      t                 s  s       t  t
                                                      such that cs = l and ct = ¬l. A special case arises when
3.1  The Argumentation Framework                      no uncertainty exists. We handle this case in an intuitively
                                                      justiﬁable manner as follows: given opinions b1,d1, 0 and
Many frameworks for argumentation ignore probabilistic and
                                                      b2,d2, 0, we compute b = b1 + b2 − d1 − d2 and create the
possibilistic concepts, focusing instead purely on the interac-            c
                                                      combined opinion bc, 0, 1−bc if bc > 0,and0, −bc, 1+bc
tions (such as rebutting and undercutting attacks) between ar- otherwise.
guments. While important in their own right, applying frame-
                                                        We have shown how to compute the value of a literal given
works like these to domains in which uncertain evidence ex-
                                                      all the applicable arguments and facts, as well as opinions
ists is difﬁcult. We thus propose a new argument framework
                                                      on their values. To complete the description of the argu-
which, while borrowing ideas from existing work, suggests a
                                                      mentation framework, we must show how we can determine
different way of thinking about arguments.
                                Σ                     whether an argument is applicable, as well as give a procedure
  We deﬁne our language of discourse as the set of literals to decide the values of all literals in an instance of a dialogue.
and their negation, while Σ+ is the set containing only un-
                                                        Models of varying complexity have been proposed regard-
negated literals. A literal l has an opinion ω(l)=b ,d ,u 
                                            l l  l    ing whether an argument can be advanced [Toulmin, 1958].
associated with it.
                                          Σ           In this paper, we use a simple model stating that if the
  An argument A is a tuple (P, c) where P ∈ 2 and c ∈
                                                      premises of an argument hold, it can be used. For a premise
Σ. We assume that if a literal p appears in P ,then¬p does
                                                      to hold, it should exceed some threshold strength. Both com-
not appear, and vice-versa. P represents the premises of an
                                                      puting a premise’s strength, as well as setting the threshold
argument, while c is the argument’s conclusion. A fact a can
                                                      are domain speciﬁc tasks. For example, the strength of a
be represented by an argument ({},a) (which we will also
                                                      literal could be based on its level of uncertainty, strength
write as (,a)). Let Args(Σ) represent the set of all possible
                                                      of belief, or some other combination of its attributes (see
arguments in the language Σ.
                                                      [Jøsang, 2002] for more details). Terms from human legal
  Given a set of arguments and facts, as well as opinions stat-
                                                      reasoning such as “beyond reasonable doubt” and “on the
ing how likely those facts are to be true, we would like to
                                                      balance of probabilities” show us the subjective nature of
determine what conclusions we may reasonably be allowed
                                                      the threshold. Nevertheless, we assume that some function
to draw. This is done by repeatedly computing the conclu-
                                                      admissible(ω) → [true, false] exists and allows us to com-
sions of arguments from known (opinions about) facts, until
                                                      pute whether an opinion exceeds the required threshold.
nothing more can be deduced. At this stage, we can draw con-
clusions based on what literals are justiﬁed within the system. Before describing the algorithm to determine the state of
  A literal may be assigned a value in one of three ways. literals formally, we provide an informal overview. We can
First, it could be the conclusion of an argument. Second, it represent the set of arguments under consideration as a di-
might be based on a fact. Finally, it could be based on a rected graph containing two types of nodes; those represent-
combination of values from both arguments and facts.  ing arguments, and those representing literals. An edge con-
  Given an argument A  =(P, c), we assign an opin-    nects literals to arguments in which they appear as premises,
ion to it using a slightly modiﬁed form of statistical syllo- while edges also exist from arguments to those literals which
                                           2          appear in their conclusion. As shown in Figure 3, such a
gism. Let ω i be the opinion for premise p ∈ P .Then
          p                          i                graph has two types of edges, namely edges linking negative
ω(A)=min(b     i ), 0, 1 − b i  if c is not negated, and
               p          p                           literals to positive ones (or vice-versa), and all other edges.
  2
   Note that if pi is a negated literal, then the subjective logic nega- One weakening assumption we make (discussed further in
tion operator must be applied when computing its contribution to the Section 5) is that the resulting graph is acyclic.
argument’s opinion.                                     Apart from the arguments, we assume that a set of facts has

                                                IJCAI-07
                                                  1435been put forward. Our algorithm operates by repeatedly for- Given a graph G =(A, L; PE, NE),
ward chaining applicable arguments from the set of facts. In a set of opinions Ω over a =(,c) ⊆ A where c ∈ Σ
other words, we compute what literals we can derive from the an admissibility function admissible
facts, and propagate those forward into the arguments, com-
puting literals from the derived arguments, and repeating this sourceNodes(t)={s|(s, t) ∈ E}
procedure until nothing new can be computed. Two things targetNodes(s)={t|(s, t) ∈ E}
complicate this algorithm: accrual of arguments/evidence,
and the threshold of acceptability of literals. We cannot com- repeat
pute an opinion regarding the value of a literal until all the for each ω(n) ∈ Ω such that n/∈ V
arguments and facts regarding the literal have been taken into V = V ∪ n
account. Similarly, we cannot determine the status of an argu- for each t ∈ targetNodes(n)
ment until the value of all its premises have been computed. if sourceNodes(t) ⊆ V
As mentioned previously, if a literal’s strength falls below a compute ω(t)
certain level of acceptability, the literal cannot be applied. if t ∈ L and ¬admissible(ω(t))
  To deal with these issues, a node’s opinion is only com- for each r ∈ targetNodes(t)
puted once all inﬂuencing opinions have been calculated. If for each s ∈ targetNodes(r)
a literal falls below the threshold of acceptability, we erase E = E \{(r, s)}
all the arguments in which it appears as a premise. A literal for each s ∈ sourceNodes(r)
exceeding this threshold is referred to as admissible.       E  = E \{(s, r)}
                                                             =    \           ( )
  More formally, given a set of arguments A, and deﬁning the A  A  targetNodes t
                                                          Ω=Ω∪{      ( )}
non-negated set of all literals appearing in A as L, we create     ω  t
                                                           ∃ ( ) ∈ Ω          ∈
a directed graph of arguments GA =(A, L; PE , NE) with until ω n     such that n/V
                                                               ∈            ∈
nodes A and L. For convenience, we write E = PE ∪ NE. for each l L such that l/V
                                                       Ω=Ω∪{      ( )=0 0 1}
  An edge e appears in PE , going from a ∈ A to l ∈ L if        ω l      , ,
a is of the form (P, l). An edge would also appear in PE,
going from l ∈ L to a ∈ A if a =(P, c) and l ∈ P .An
edge will appear in NE, going from a ∈ A to l ∈ L if a is of Figure 2: The algorithm for propagating opinions through the
the form (P, ¬l). Alternatively, an edge will be a member of argumentation network. Note that E = PE ∪ NE
NE, going from l ∈ L to a ∈ A if a =(P, l) and ¬l ∈ P .
In other words, an edge is in NE if it links the negated and
                                                      l1,...,ln ∈ Σ, a function mapping the state of proven and
non-negated form of a literal, and is in PE otherwise. Differ-                              +
entiating between these types of edges allows us to compute unproven literals to a utility value G :Σ, Σ →,and a
an opinion correctly while only storing non-negated literals record of how much probing actions performed to date have
in our graph.                                         cost C ∈. Thus, for example, G({¬a, b}, {c, d}) would
  Given a set of opinions on facts ω(F ) where F ⊆ A with return theutility for the state where ¬a and b are proven, and
the restriction that ∀f =(P, c) ∈ F, P = {}, a graph rep- nothing is known about the status of c and d.
resenting a set of arguments generated as described above, Agents operate within an environment. This environment
and an admissibility function mapping opinions to truth and is a tuple of the form (Agents, CS, S, admissible, P C)
falsehood, we can perform reasoning according to the algo- where Agents is a set of agents, and contains a record of
rithm shown in Figure 2.                              all the utterances made by an agent (CS ⊆ 2Args(Σ)), as well
                                                      as a set of sensors S. The function admissible operates over
3.2  Agents and the Dialogue Game                     opinions, while PC maps probing actions to costs.
The argumentation framework described above allows us to A sensor is a structure (Ωs, Ωp).ThesetΩs contains an
                                                                                    +
statically analyse groups of arguments; letting us compute an opinion for a set of literals L ⊂ Σ , and represents the en-
opinion about speciﬁc literals. The process of arguing is dy- vironment’s opinion about the reliability of the sensor. The
namic, with agents introducing arguments at various points in opinion for a literal l ∈ L,isωs(l) ∈ Ωs. The set of probed
the conversation. In this section, we specify what our agents literals Ωp, stores the sensor’s opinions regarding the value of
should look like, what environment they argue within, and the literal l, ωp(l) ∈ Ωp. We compute a sensor’s opinion for l
the protocol they use to argue with each other. This protocol as ωs(l) ⊗ ωp(l).
is referred to as a dialogue game. The aim of our dialogue Agents take turns to put forward a line of argument and
game is “fairness”, that is, allowing an agent to utilise any probe sensors to obtain more information about the environ-
arguments at its disposal to win. While many dialogue games ment. Such an action is referred to as an agent’s utterance.
provide restrictions on agent utterances so as to maintain the In each turn, the contents of an agent’s utterance is added to
focus of a dispute, we leave this task to the heuristic layer of the global knowledge base; any sensors probed are marked
our framework.                                        as such (a sensor may not be probed more than once for the
  An agent α, is a tuple (Name,KB,Ωα,G,C), con-       value of a speciﬁc literal), and costs are updated. We are now
sisting of its name, Name, a private knowledge base   in a position to formally deﬁne the dialogue game.
KB   ⊆  2Σ containing arguments, a private opinion base The utterance function utterance : Environment ×
                                                                 Args(Σ)
Ωα =  {ωα(l1),...,ωα(ln)} storing opinions about literals Name → 2      × Probes takes in an environment and

                                                IJCAI-07
                                                  1436an agent (via its name), and returns the utterance made by the agent utility using one step lookahead. If multiple arguments
agent. The ﬁrst part of the utterance lists the arguments ad- still remain, one is selected at random.
vanced by the agent, while the second lists the probes the When making an utterance, an agent has to decide what ar-
                                           +
agent would like to undertake. Probes ∈ 2S×Σ  is the  guments to advance, as well as what probing actions it should
power set of all possible probes. The domain speciﬁc func- undertake. Given an environment Env and an agent α,let
tion PC :2Probes →allows us to compute the cost of   the set of possible arguments be PA =2KB.Wedeﬁnethe
                                                                                  =  {(  )|  =(Ω   Ω  ) ∈
performing a probe.Probing costs are deﬁned in this way to set of all possible probes as PP s, l s s, p
                                                                  ( ) ∈ Ω        ( ) ∈ Ω }
allow for discounts on simultaneous probes, which in turn al- S such that ωs l s and ωp l / p .Thecostfora
lows for richer reasoning by agents.                  probe ⊆ PP is PC(probe).
  We deﬁne a function turn : Environment × Name →       Our agents use multiple sources of information to estimate
Environment   that takes in an  environment and an    the results of a probe. First, they have opinions regarding lit-
                                                                    Ω
agent label, and  returns a  new  environment con-    erals, stored in α. Second, they can compute the current
taining the effects of an agent’s utterances. Given   most likely value of the probe by examining sensors that al-
                                                                     ( )
an  environment Env   and  an  agent α,wedeﬁne        ready contain ωp l . Lastly, they can infer the most likely
the turn function as follows: turn(Env,Name)=         value of l by computing the effects of a probing action.
(NewAgents,CS     ∪   Ar, NewSensors, P C)   where      A naive agent will believe that an unprobed sensor s will
                                                                        ( )
Ar, NewAgents    and  NewSensors     are  computed    return an opinion ωα l . A non-naive agent will alter its be-
from  the results of  the utterance function.    If   liefs based on existing probed values. One possible combina-
utterance(Env,Name)=(Ar, P r)    then NewAgents  =    tion function will have the agent believe that the sensor will
                                                                          (  ( ) ⊗  t ( )) ⊕ ( )
Agents \ α  ∪ (Name,KB,Ωα,G,C      + PC(Pr))   and,   return an opinion t∈P ωs l  ωp l    ωα l for a literal
∀(s, l) ∈ Pr,wherel is a literal sensor s is able to probe, l given an agent’s personal belief ωα(l), a set of sensors P
                                                                 t ( )       ∈
NewSensors  = Sensors \ s ∪ (Ωs, Ωp ∪ ωp(l)).         containing ωp l for any t P , and the unprobed sensor’s
  It should be noted that the utterance depends on agent ωs(l). Informally, this means that a na¨ıve agent will believe
strategy; we will deﬁne one possible utterance function in that a probed sensor will share the same opinion of a literal
the next section. Before doing so, we must deﬁne the dialogue as the agent has, while a non-na¨ıve agent will weigh its ex-
game itself.                                          pected opinion due to probes that have already been executed.
  We   may   assume  that  our  agents  are  named    Different combination functions are possible due to different
Agent0,Agent1,...,Agentsn−1  where n is the number of weights being assigned to different opinions.
agents participating in the dialogue. We can deﬁne the dia- For each possible utterance in PA × 2PP , the agent calcu-
logue game in terms of the turn function by setting turn0 = lates what the environment will look like, as well as the net
turn((Agents, CS0,S,admissible,PC),Agent0),andutility gain (using the functions  PC  and U). It then selects
               +1  =      (                 )
then having turni    turn turni,Agenti mod n .The     the utterance that will maximise its utility in the predicted fu-
game ends when turni ...turni−n+1 = turni−n.          ture environment.
  CS0 and Ω0 contains any initial arguments and opinions,
and are usually empty. Note that an agent may make a null 3.4 Contracts
utterance {, } during its move to (eventually) bring the game Having fully deﬁned the environment in which argument can
to an end. In fact, given a ﬁnite number of arguments and take place, we now examine one possible application domain,
sensors, our dialogue is guaranteed to terminate as eventu- namely contract monitoring. Since contract representations
ally, no utterance will be possible that will change the public are highly domain dependent, most of this section provides
knowledge base CS.                                    only guidelines as to how to transform contracts into a form
  We can compute an agent’s utility by combining its utility suitable for use within the framework. To simplify the dis-
gain (for achieving its goals) with the current costs. At any cussion, only two party contracts are examined.
stage of the dialogue, given the environment’s CS,andthe To make use of the framework, a number of things must
set of all opinions probed by the sensors {Ωp|s =(Ωs, Ωp) ∈ be deﬁned. The (domain dependent) sensor set S and prob-
S), and the environment’s admissibility function, we can run ing cost function PC must be initialised, and the agent’s
the reasoning algorithm to compute a set of proven literals as goal function G must also be speciﬁed. The agent’s private
proven = {l|l is a literal in CS and admissible(l)}. Liter- knowledge base KB and the environment’s starting knowl-
als which fall below the proven threshold, or for which noth- edge base CS0 must also be populated, as well as the agent’s
ing is known are unproven: unproven = { literals in CS \ prior opinions.
proven}.  Then the net utility gain for an agent α is   While nothing can be said about the generation of the sen-
U(α, proven)=G(proven, unproven)  − C.                sor set except that it should reﬂect the sensors within the en-
  At the end of the dialogue, we assume that agents agree vironment, probing costs can often be obtained from the con-
that literals in the proven set hold in the world.    tract. Contract based sanctions and rewards can also be used
                                                      to generate G. For example, if in a certain contract state an
3.3  The Heuristic                                    agent α must pay β a certain amount of money, the literal
Agents are able to use the underlying argumentation frame- representing that state might have an associated negative util-
work and dialogue game to argue. In this section, we will de- ity for α, and positive utility for β. One point that should be
scribe a heuristic which agents may use to argue. Informally, noted when assigning utility to various states is the uneven-
this heuristic attempts to make utterances which maximise ness of burden of proof [Prakken, 2001]. Assume that β gains

                                                IJCAI-07
                                                  1437                                                      0.6, 0.2, 0.2, ωs(tr)=0.8, 0.0, 0.2, ωs(p)=1, 0, 0,
                                                      and let the cost of probing each of these sensors be 20, except
                                                      for the sensor probing f1, which has a utility cost of 50.
                                                        Finally, let the utility for agent α showing g holds be
                                                      200, with agent β having utility -100 for this state. As-
                                                      sume α believes that literals c, h, f, ¬tr, ¬p hold with opin-
                                                      ion strength 0.9, 0, 0.1. Also assume that the contract
                                                      is initially stored in KB, and the admissibility function is
                                                      admissible(b, d, u)=b>0.5 (d>0.5) for a literal (or its
                                                      negation) to be admissible.
                                                        Agent  α  might begin with the following utterance
                                                      ({(,tr), (,p), ({¬p, ¬tr}, ¬ts), ({¬ts},g)}, {p, tr}).This
                                                      will cost it 40 utility. Given its reasoning algorithm, it will
Figure 3: An argument graph. Dashed lines represent negated assume that once probed, ω(tr)=0, 0.72, 0.18, ω(ts)=
edges, while solid lines are normal edges.            ω(g)=0,   0.72, 0.18. However, the sensor probing p re-
                                                      turns 0.8, 0.1, 0.1,making¬p fall below the threshold of
                                                      acceptability, thus invalidating this chain of argument. Agent
utility for proving a literal l, while α gains if the ﬁrst agent β passes.
does not (e.g. an agent has to pay a penalty to another in a cer- Agent α tries again, probing f1 and advancing the ar-
tain state). In such a situation, the second agent loses utility gument ({f},g), which returns 0.7, 0.1, 0.2 resulting in
if l is true. Utility is not lost if l is either false or unproven. an opinion 0.56, 0.08, 0.36 after the ⊗ operator is ap-
  We assume that a trivial mapping exists between contract plied. This means that g is now valid, with opinion
clauses and arguments (for example, a clause stating “if a and 0.64, 0.08, 0.28.
b then c” can be represented as ({a, b},c}). Then one must Agent β counters, probing h and claiming ({h}, ¬g).The
decide whether to place the translated clauses into the agent’s probe returns 0.8, 0, 1, 0.1, which means the argument is
private knowledge base KB,orCS0. Putting them into KB of sufﬁcient strength to make g inadmissible (as ω(g) ≈
will mean that agents will advance those clauses relevant to 0.47, 0.34, 0.19).
the dispute during the course of the argument. Having the ar- Agent α undercuts β’s argument by probing c and advanc-
guments in CS0 represents a more realistic scenario wherein ing ({c}, ¬h). Given the sensor’s opinion 0.6, 0.2, 0.2,this
the contract is public knowledge. KB can also contain do- reinstates g by disallowing argument ({h}, ¬g).
main speciﬁc arguments the agents might want to use in the Assume that β believes that f’s true value is 0, 0.8, 0.2.
course of the dialogue. Generating the agent’s opinions about It would then probe f2 with the utterance (, {f2}). Assume
the state of literals is also outside the scope of our work. this returns an opinion 0.1, 0.8, 0.1. g would then remain
                                                      admissible (ω(g)=0.51, 0.28, 0.21).
4  Example                                              Both agents now pass. Since g is admissible α has a net
The example we present here is based on a simpliﬁed contract utility gain of 90, while β’s utility loss is 140.
from the mobile multi–media domain. A supplier has agreed
to provide a consumer with a set of multi–media services, and 5 Discussion
the consumer believes that the package it has received has not
                                                      As seen in the preceding example, our framework is able to
lived up to the the provider’s contractual obligations. Assume
                                                      represent standard argumentation concepts such as rebutting
that we have a contract of the form
                                                      attacks, defeat and reinstatement, as well as higher level con-
         frameRate < 25 →  giveSanction               cepts such as accrual of arguments, which many frameworks
                                                      have difﬁculty dealing with. When compared to standard ar-
         horrorMovie →¬giveSanction
                                                      gumentation systems, the main shortcoming of our approach
         cinderella →¬horrorMovie                     lies in its inability to handle looping arguments. However,
         ¬textSent → giveSanction                     many other frameworks suffer from a similar (though weaker)
         ¬phoneFull, ¬textReceived →¬textSent         problem [Prakken and Vreeswijk, 2002]. The use of possi-
                                                      bilistic rather than probabilistic reasoning makes our frame-
From this, we can generate the argument graph shown in Fig- work suitable for situations where uncertainty exists regard-
ure 3, with the letter in the node being the ﬁrst letter of the ing the probabilities to be assigned to an event. Our sample
literal in the contract.                              domain is one area where such reasoning capabilities are use-
  Assume further that we have sensors for whether the ful.
movie’s name is “cinderella” (c), whether it is a horror Most argument frameworks can be described in terms of
movie (h), two sensors to detect whether the frameRate Dung’s [1995] semantics. These semantics are based on the
is less than 25 (f1,f2), whether a text was received (tr), notion of defeat. Since defeat in our framework is dynami-
and whether the phone’s memory was full (p). Let the  cally calculated, applying these semantics to our framework
opinion sensors for these ratings be ωs(c)=1, 0, 0, is difﬁcult. Investigating the semantics of our framework
ωs(h)=0.7, 0.2, 0.1, ωs(f1) = 0.8, 0.1, 0.1, ωs(f2) = more closely, and looking at how they compare to those of

                                                IJCAI-07
                                                  1438