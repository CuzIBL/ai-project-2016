                                Case-based Multilabel Ranking

                                Klaus Brinker    and Eyke Hullermeier¨
                                   Data and Knowledge Engineering,
                          Otto-von-Guericke-Universitat¨ Magdeburg, Germany
                              {brinker,huellerm}@iti.cs.uni-magdeburg.de


                    Abstract                          cation and sports are irrelevant, the former perhaps somewhat
                                                      less than the latter.
    We present a case-based approach to multilabel      From an MLC point of view, the additional order informa-
    ranking, a recent extension of the well-known prob- tion is not only useful by itself but also facilitates the postpro-
    lem of multilabel classiﬁcation. Roughly speak-   cessing of predictions (e.g., considering only the at most top-
    ing, a multilabel ranking reﬁnes a multilabel clas- k relevant labels). Regarding the relation between MLC and
    siﬁcation in the sense that, while the latter only MLR, we furthermore like to emphasize two points: Firstly,
    splits a predeﬁned label set into relevant and ir- as will be seen in the technical part below, MLR is not more
    relevant labels, the former furthermore puts the la- demanding than MLC with respect to the training informa-
    bels within both parts of this bipartition in a total tion, i.e., a multilabel ranker can well be trained on multilabel
    order. We introduce a conceptually novel frame-   classiﬁcation data. Secondly, inducing such a ranker can be
    work, essentially viewing multilabel ranking as a useful even if one is eventually only interested in an MLC.
    special case of aggregating rankings which are sup- Roughly speaking, an MLR model consists of two compo-
    plemented with an additional virtual label and in nents, a classiﬁer and a ranker. The interdependencies be-
    which ties are permitted. Even though this frame- tween the labels which are learned by the ranker can be help-
    work is amenable to a variety of aggregation pro- ful in discovering and perhaps compensating errors of the
    cedures, we focus on a particular technique which classiﬁer. Just to illustrate, suppose that the classiﬁer esti-
    is computationally efﬁcient and prove that it com- mates one label to be relevant and a second one not. The
    putes optimal aggregations with respect to the (gen- additional (conﬂicting) information that the latter is typically
    eralized) Spearman rank correlation as an underly- ranked above the former might call this estimation into ques-
    ing loss (utility) function. Moreover, we propose tion and thus repair the misclassiﬁcation.
    an elegant generalization of this loss function and
                                                        Hitherto existing approaches operating in ranking scenar-
    empirically show that it increases accuracy for the
                                                      ios are typically model-based extensions of binary classiﬁca-
    subtask of multilabel classiﬁcation.
                                                      tion techniques which induce a global prediction model for
                                                      the entire instance space from the training data [Har-Peled
                                                      et al., 2002; Furnkranz¨ and Hullermeier,¨ 2003]. These ap-
1  Introduction                                       proaches, brieﬂy reviewed in Section 3, suffer substantially
Multilabel ranking (MLR) is a recent combination of two from the increased complexity of the target space in multi-
supervised learning tasks, namely multilabel classiﬁcation label ranking (in comparison to binary classiﬁcation), thus
(MLC) and label ranking (LR). The former studies the prob- having a high level of computational complexity already for
lem of learning a model that associates with an instance x a moderate number of class labels.
a bipartition of a predeﬁned set of class labels into relevant In Sections 4 and 5, we present an alternative framework
(positive) and irrelevant (negative) labels, while the latter for MLR using a case-based methodology which is concep-
considers the problem to predict rankings (total orders) of tually simpler and computationally less complex. One of the
all class labels. A MLR is a consistent combination of these main contributions of this paper is casting multilabel rank-
two types of prediction. Thus, it can either be viewed as an ing as a special case of rank aggregation (with ties) within a
extended ranking (containing additional information about a case-based framework. While our approach is not limited to
kind of “zero point”), or as an extended MLC (containing ad- any particular aggregation technique, we focus on a compu-
ditional information about the order of labels in both parts of tationally efﬁcient technique and prove that it computes op-
the bipartition) [Brinker et al., 2006]. For example, in a docu- timal aggregations with respect to the well-known (general-
ment classiﬁcation context, the intended meaning of the MLR ized) Spearman rank correlation as an accuracy measure. In
[pol x eco][edu x spo] is that, for the instance (= doc- Section 6, we show that our case-based approach compares
ument) x, the classes (= topics) politics and economics are favorably with model-based alternatives, not only with re-
relevant, the former even more than the latter, whereas edu- spect to complexity, but also in terms of predictive accuracy.

                                                IJCAI-07
                                                   7022  Problem Setting                                    i.e., a calibrated ranking is simply a ranking of the extended
                                                      label set L∪{λ0}. Such a ranking induces both a ranking
In so-called label ranking, the problem is to learn a mapping             L                       P ,N
from an instance space X to rankings over a ﬁnite set of labels among the (real) labels and a bipartite partition ( x x)
                                                      in a straightforward way: Px is given by those labels which
L =  {λ1 ...λc}, i.e., a function that maps every instance
                                                      are ranked higher than λ0, Nx by those which are ranked
x ∈Xto a total strict order x, where λi x λj means that,
                                                      lower. The semantics of the virtual label becomes clear from
for this instance, label λi is preferred to (ranked higher than)
λ                L                                    the construction of training examples for the binary learners:
 j. A ranking over can conveniently be represented by a         λ
permutation τ of {1 ...c}, where τ(i) denotes the position Every label i known to be relevant is preferred to the virtual
                                                      label (λi x λ0); likewise, λ0 is preferred to all irrelevant la-
of label λi in the ranking. The set of all permutations over c
                                                      bels. Adding these preference constraints to the preferences
labels, subsequently referred to as Sc, can hence be taken as
the target space in label ranking.                    that can be extracted for the regular labels, a calibrated rank-
                                                      ing model can be learned by solving a conventional ranking
  Multilabel ranking (MLR) is understood as learning a            c
model that associates with a query input x both a ranking problem with +1labels. We have discussed this approach
                                                      in more detail as we will advocate a similar idea in extending
x and a bipartition (multilabel classiﬁcation, MLC) of the
label set L into relevant (positive) and irrelevant (negative) case-based learning to the multilabel ranking scenario.
labels, i.e., subsets Px,Nx ⊆Lsuch that Px ∩ Nx = ∅ and
Px ∪ Nx = L [Brinker et al., 2006]. Furthermore, the rank- 4 Case-based Multilabel Ranking
ing and the bipartition have to be consistent in the sense that
                                                      Case-based learning algorithms have been applied success-
λi ∈ Px and λj ∈ Nx implies λi x λj.
                                                      fully in various ﬁelds such as machine learning and pattern
  As an aside, we note that, according to the above con-
                                                      recognition [Dasarathy, 1991]. In previous work, we pro-
sistency requirement, a bipartition (Px,Nx) implicitly also
                                                      posed a case-based approach which is tailored to label rank-
contains ranking information (relevant labels must be ranked
                                                      ing, hence, it cannot exploit bipartite data and does not sup-
above irrelevant ones). This is why an MLR model can be
                                                      port predicting the zero point for the multilabel ranking sce-
trained on standard MLC data, even though it considers an
                                                      nario [Brinker and Hullermeier,¨ 2005]. These algorithms de-
extended prediction task.
                                                      fer processing the training data until an estimation for a new
                                                      instance is requested, a property distinguishing them from
3  Model-based Multilabel Ranking                     model-based approaches. As a particular advantage of de-
A common model-based approach to MLC is binary rele-  layed processing, these learning methods may estimate the
vance learning (BR). BR trains a separate binary model Mi target function locally instead of inducing a global prediction
for each label λi, using all examples x with λi ∈ Px as posi- model for the entire input domain from the data.
tive examples and all those with λj ∈ Nx as negative ones. To A typically small subset of the entire training data, namely
classify a new instance x, the latter is submitted to all models, those examples most similar to the query, is retrieved and
and Px is deﬁned by the set of all λi for which Mi predicts combined in order to make a prediction. The latter exam-
relevance.                                            ples provide an obvious means for “explaining” a prediction,
  BR can be extended to the MLR problem in a straightfor- thus supporting a human-accessible estimation process which
ward way if the binary models provide real-valued conﬁdence is critical to certain applications where black-box predictions
scores as outputs. A ranking is then simply obtained by order- are not acceptable. For label ranking problems, this appeal-
ing the labels according to these scores [Schapire and Singer, ing property is difﬁcult to realize in algorithms using com-
2000]. On the one hand, this approach is both simple and efﬁ- plex global models of the target function as the more com-
cient. On the other hand, it is also ad-hoc and has some disad- plex structure of the underlying target space typically entails
vantages. For example, good estimations of calibrated scores solving multiple binary classiﬁcation problems (RPC yields
(e.g., probabilities) are often hard to obtain. Besides, this ap- c(c +1)/2 subproblems) or requires embedding the training
proach cannot be extended to more general types of prefer- data in a higher dimensional feature space to encode prefer-
ence relations such as, e.g, partial orders. For a detailed sur- ence constraints (such as for CC).
vey about MLC and MLR approaches, including case-based  In contrast to the model-based methodology which suf-
methods, we refer the reader to [Tsoumakas et al., 2006]. fers substantially from the increased complexity of the target
  Brinker et al. [2006] presented a uniﬁed approach to cal- space in MLR, we will present a case-based approach where
ibrated label ranking which subsumes MLR as a special the complexity of the target space solely affects the aggrega-
case. Their framework enables general label ranking tech- tion step which can be carried out in a highly efﬁcient manner.
niques, such as the model-based ranking by pairwise compar- The k-nearest neighbor algorithm (k-NN) is arguably the
ison (RPC) [Furnkranz¨ and Hullermeier,¨ 2003] and constraint most basic case-based learning method [Dasarathy, 1991].In
classiﬁcation (CC) [Har-Peled et al., 2002], to incorporate its simplest version, it assumes all instances to be represented
                                                                                     
and exploit partition-related information and to generalize to by feature vectors x =([x]1 ...[x]N ) in the N-dimensional
settings where predicting a separation between relevant and space X = RN endowed with the standard Euclidian metric
irrelevant labels is required. This approach does not assume as a distance measure, though an extension to other instance
the underlying binary classiﬁers to provide conﬁdence scores. spaces and more general distance measures d(·, ·) is straight-
Instead, the key idea in calibrated ranking is to add a virtual forward. When a query feature vector x is submitted to the
label λ0 as a split point between relevant and irrelevant labels, k-NN algorithm, it retrieves the k training instances closest to

                                                IJCAI-07
                                                   703this point in terms of d(·, ·). In the case of classiﬁcation learn- ing σ (not necessarily unique) such that L(σ)=minτ L(τ).
ing, the k-NN algorithm estimates the query’s class label by The remaining step to actually solve multilabel ranking us-
the most frequent label among these k neighbors. It can be ing the case-based methodology is to incorporate methods
adapted to the regression learning scenario by replacing the which compute (approximately) optimal solutions for the lat-
majority voting step with computing the (weighted) mean of ter optimization problem. As we do not exploit any particu-
the target values.                                    lar property of the metric l, this approach provides a general
  In order to extend the basic k-NN algorithm to multilabel framework which allows us to plug in any optimization tech-
learning, the aggregation step needs to be adapted in a suit- nique suitable for a metric on rankings with ties in order to
able manner. To simplify our presentation, we will focus on aggregate the k nearest neighbors for a query instance x.
the standard MLC case where the training data provides only The complexity of computing an optimal aggregation de-
a bipartition into relevant and non-relevant labels for each in- pends on the underlying metric and may form a bottleneck
stance. Later on, we will discuss how to incorporate more as this optimization problem is NP-hard for Kendall’s tau
complex preference (ranking) data for training.       [Bartholdi et al., 1989] and Spearman’s footrule metric on
                                                                  [                ] 1
  Let us consider an example (x, Px,Nx) from a standard bucket orders Dwork et al., 2001 . Hence, computing an
MLC training dataset. As stated above, the key idea in cali- optimal aggregation is feasible only for relatively small la-
                                                             {λ ...λ }
brated ranking is to introduce a virtual label λ0 as a split point bel sets 1 c . There exist, however, approximate algo-
                                                                                     c
to separate labels from Px and Nx, respectively, and to asso- rithms with quadratic complexity in which achieve a con-
                                                                                                       L
ciate a set of binary preferences with x. We will adopt the stant factor approximation to the minimal sum of distances
idea of a virtual label but, instead of associating preferences, for Kendall’s tau and the footrule metric [Fagin et al., 2004].
use a more direct approach of viewing the sequence of the While approximate techniques in fact provide a viable op-
label sets (Px, {λ0},Nx) as a ranking with ties, also referred tion, we will present a computationally efﬁcient and exact
to as a bucket order [Fagin et al., 2004]. More precisely, a method for a generalization of the sum of squared rank differ-
bucket order is a transitive binary relation  for which there ences metric in the following section to implement a version
exist sets B1 ...Bm that form a partition of the domain D of our case-based multilabel ranking framework.
(which is given by D = L∪{λ0}   in our case) such that
λ   λ if and only if there are i, j with i<jsuch that 5 Aggregation Analysis
            
λ ∈ Bi and λ  ∈ Bj. Using this notation, the MLR sce- The Spearman rank correlation coefﬁcient, a linear transfor-
nario corresponds to a generalized ranking setting with three mation of the sum of squared rank differences metric, is a
buckets, where B1 = Px, B2 = {λ0} and B3 = Nx.        natural and well-known similarity measure on strict rankings
  If the training data provides not only a bipartition (Px,Nx) [Spearman, 1904]. It can be generalized to the case of rank-
but also a ranking (with ties) of labels within both parts, ings with ties in the same way as the Spearman footrule met-
this additional information can naturally be incorporated: As- ric, where (integer) rank values for strict rankings are sub-
sume that Px and Nx form bucket orders (B1 ...Bi−1) and stituted with average bucket locations. Hence, for any two
                                                                      
(Bi+1 ...Bj), respectively. Then, we can combine this addi- bucket orders σ, σ , the generalized squared rank difference
tional information into a single ranking with ties in a straight- metric is deﬁned as
                                                                            
forward way as (B1 ...Bi−1,Bi,Bi+1 ...Bj), where Bi =
                                                                  l σ, σ        σ i − σ i 2.
{λ0} represents the split point. Note that the following anal-    2(    )=      ( ( )   ( ))          (1)
ysis only assumes that the training data can be converted into              λi∈D
rankings with ties, with the virtual label specifying the rel- The following theorem shows that an optimal aggregation
evance split point. It will hence cover both training data of with respect to the l2 metric can be computed by ordering
the standard MLC case as well as the more complex MLR the labels according to their (generalized) mean ranks.
scenario.
                                                      Theorem  1. Let σ1 ...σk be rankings with ties on D
  A bucket order induces binary preferences among labels                                               =
                                                      {λ1 ...λc}. Suppose σ is a permutation such that the labels
but moreover forms a natural representation for general-                         
                                                      λ                        1   k  σ  i
izing various metrics on strict rankings to rankings with i are ordered according to k j=1 j( ) (ties are broken
ties. To this end, we deﬁne a generalized rank σ(i) for arbitrarily). Then,
each label λi ∈Das the average overall position σ(i)=            k                 k
           1                                                                      
     |Bl| +  (|Bj| +1)within the bucket Bj which con-
  l<j      2                                                         l2(σ, σj) ≤ min  l2(τ,σj)        (2)
    λ                                                                          τ∈Sc
tains i. Fagin et al. [2004] proposed several generalizations    j=1               j=1
of well-known metrics such as Kendall’s tau and the Spear-
man footrule distance, where the latter can be written as the Before we proceed to the formal proof, note that the key
                                                                                                 S
l1 distance of the generalized ranks σ, σ associated with the point in Theorem 1 is that the minimum is taken over c while
                                                                                      c
            l1 σ, σ          |σ i − σ i |             it is well-known that the minimizer in R would be the mean
bucket orders, (  )=    λi∈D  ( )    ( ) .
  Given a metric l, a natural way to measure the quality rank vector. For strict rankings with unique mean rank values,
of a single ranking σ as an aggregation of the set of rank- the optimal-aggregation property was proved in [Dwork et
ings σ1 ...σk is to compute the sum of pairwise distances:
                                                        1In the case of strict complete rankings, solving the aggregation
L σ      k   l σ, σ
 ( )=    j=1 (   j). Then, aggregation of rankings leads problem requires polynomial time for Spearman’s footrule metric
to the optimization problem of computing a consensus rank- [Dwork et al., 2001].

                                                IJCAI-07
                                                   704al., 2001]. A proof for the more general case of non-unique hence, providing a very efﬁcient aggregation technique. Note
rank values can be derived from [Hullermeier¨ and Furnkranz,¨ that this method aggregates rankings with ties into a single
2004].                                                strict ranking. The related problem of aggregating into a
  The following proof is an adaptation of [Hullermeier¨ and ranking where ties are allowed forms an interesting area of
Furnkranz,¨ 2004] where the ranking by pairwise comparison research in itself and for the case of the l2-metric the required
voting procedure for complete strict rankings was analyzed complexity is an open question. Moreover, multilabel rank-
in a probabilistic risk minimization scenario. An essential ing requires predicting strict rankings such that an intermedi-
building block of our proof is the subsequent observation on ate aggregation into a ranking with ties would entail an ad-
permutations:                                         ditional postprocessing step and hence forms a less intuitive
Lemma 2  ([Hullermeier¨ and Furnkranz,¨ 2004]). Let mi, i = approach to this problem.
                                                                                      λ
1 ...c, be real numbers ordered such that 0 ≤ m1 ≤ m2 ≤ As stated above, the virtual label 0 is associated with
                                                                      B      {λ }
···≤mc. Then, for all permutations τ ∈Sc,             the second bucket 2 =    0  in order to provide a rele-
                                                      vance split point. In an initial empirical investigation, we ob-
           c            c                                     l                   k
               i − m 2 ≤     i − m    2.              served that 2-optimal rankings in -NN multilabel ranking
              (     i)      (     τ(i))         (3)   yield good performance with respect to standard evaluation
           i=1           i=1                          measures on the ranking performance, while the accuracy in
                                    
                                def 1 k               terms of multilabel classiﬁcation measures reached a reason-
Proof [Theorem 1]. Let us deﬁne mi =      σj(i), i =
                                   k  j=1             able, yet not entirely satisfactory level. This observation may
1 ...c. Then,
                                                      be attributed to the fact that the l2-metric penalizes misplaced
  k            k c                                 labels equally for all labels including λ0. However, particu-
                                 2
      l2(τ,σj)=       (τ(i) − σj(i)                   larly in the context of multilabel classiﬁcation, λ0 carries a
  j=1           j=1 i=1                               special degree of importance and therefore misclassiﬁcations
                c k                                 in the aggregation step should be penalized more strongly. In
                       τ i − m    m  − σ  i 2         other words, reversing the preference between two labels is
             =        ( ( )    i +  i   j( ))                                          λ
                i=1 j=1                               especially bad if one of these labels is 0, as it means mis-
                                                      classifying the second label in an MLC sense.
                c k
                       τ i − m  2 −  τ i − m   ·        To remedy this problem, our approach can be extended in
             =        ( ( )    i)   2( ( )   i)       a consistent and elegant manner: Instead of a single virtual
                i=1 j=1
                                                      label λ0, we consider a set of virtual labels {λ0,1 ...λ0,p}
                                              2
                      (mi − σj(i)) + (mi − σj(i))     which is associated with the split bucket Bi. In doing so, the
                c  k                               theoretical analysis on the aggregation remains valid and the
                                  2                   parameter p provides a means to control the penalty for mis-
             =         (τ(i) − mi)
                i=1  j=1                              classiﬁcations in aggregating rankings. Note that the compu-
                                                      tational complexity does not increase as the expansion into a
                                  k
                                                     set of virtual split labels can be conducted implicitly. More-
                   −   τ i − m       m  − σ  i
                      2( ( )   i)   (  i   j( ))      over, on computing a prediction, the set of virtual labels can
                                 j=1                  be merged into a single label again in a consistent way as all
                      k                             labels have the same mean rank value.
                                    2
                   +     (mi − σj(i)) .                 To illustrate this “gap broadening” control mechanism, let
                      j=1                             us take a look at a simple aggregation example with three
                                                      MLC-induced rankings using a single virtual label:
In the last equation, the mid-term equals 0 as
     k               k   k        k                           {λ1}{λ0}{λ2,λ3,λ4,λ5}
         m  − σ  i       1    σ  i −    σ  i .                    {λ }{λ   }{λ   ,λ ,λ ,λ }
        ( i    j( )) =   k     l( )       j( )                      1      0      2  3  4  5
     j=1              j=1            j=1
                           l=1                                    {λ2}{λ0}{λ1,λ3,λ4,λ5}
                                     def k
Furthermore, the last term is a constant t = j=1(mi − These bucket orders would be aggregated into a total order
     2                                                        P   ∅    N    {λ  ...λ }  m
σj(i)) which does not depend on τ. Hence, we obtain   such that =   and   =   1     5 as  0 =2(mean rank
                                                      of λ0) and every other mean rank is greater, including m1 =
        k                 c                          .                                      m     m
                                         2            2 17. Using a set of two virtual labels, we obtain 0 = 1 =
           l2(τ,σj)=ct+   k   (τ(i) − mi) .           2.5, hence, the order of these labels is determined randomly.
        j=1                 i=1                       Finally, for three virtual labels, m0 =3and m1 =2.83 such
The proof follows directly from Lemma 2.              that the aggregated calibrated ranking corresponds to a multi-
                                                      label classiﬁcation P = {λ1} and N = {λ2,λ3,λ4 λ5}.
  We have proved that an l2-optimal aggregation with respect
to the set of permutations can be computed by ordering the la- 6 Empirical Evaluation
bels according to their mean ranks. Regarding the complex-
ity, this method requires computational time in the order of The purpose of this section is to provide an empirical com-
O(kc + c log c) for computing and sorting the mean ranks, parison between state-of-the-art model-based approaches and

                                                IJCAI-07
                                                   705        0.28                                          ramaniyan et al., 2005], we measured accuracy in terms of
        0.27                                          the (generalized) Spearman rank correlation coefﬁcient, nor-
                                                                                  −
        0.26                                          malized such that it evaluates to 1 for reversed and to +1
                                         p=1          for identical rankings (see Section 5).
        0.25                             p=2
                                         p=4            Support vector machines have demonstrated state-of-the-
        0.24                             p=8          art performance in a variety of classiﬁcation tasks, and
                                         p=16
        0.23                             p=32         therefore have been used as the underlying binary classi-
                                         p=64

       Hamming  loss 0.22                             ﬁers for the binary relevance (BR) and calibrated ranking
                                         p=128
        0.21                                          by pairwise comparison (CRPC) approaches to multilabel
                                                      learning in previous studies [Elisseeff and Weston, 2001;
        0.20
                                                      Brinker et al., 2006]. Regarding the associated kernel, we
        0.19                                          considered both linear kernels (LIN) with the margin-error
           1 5  9 13172125293337414549                       C  ∈{−4   ... 4}
                         k                            penalty      2      2  and polynomial kernels (POLY)
                                                      where the degree varied from 1 to 5 and C ∈{2−2 ...22}.
Figure 1: Gap ampliﬁcation on the (functional) Yeast dataset: For each parameter (combination) the validation accuracy
The estimated Hamming loss clearly decreases in the param- was estimated by training on a randomly selected subsample
eter p, which controls the number of virtual labels used for comprising 70% of the training set and testing on the remain-
splitting relevant and irrelevant labels.             ing 30%. Then, the ﬁnal model was trained on the whole
                                                      training set using the parameter combination which achieved
                                                      the best validation accuracy. Similarly, the number of nearest
our novel case-based framework (using the l2-minimizing ag- neighbors k ∈{1, 3, .., 11, 21, .., 151} was determined.
gregation technique). The datasets that were included in the In addition to the original k-NN MLR approach, we in-
experimental setup originate from the bioinformatics ﬁelds cluded a version (denoted by the sufﬁx “-r”) which only ex-
where multilabeled data can frequently be found. More pre- ploits the MLC training data, (Px,Nx), and a common exten-
cisely, our experiments considered two types of genetic data, sion in k-NN learning leading to a slightly modiﬁed aggrega-
namely phylogenetic proﬁles and DNA microarray expres- tion step where average ranks are weighted by the distances
sion data for the Yeast genome, consisting of 2465 genes.2 of the respective feature vectors to the query vector (referred
Every gene was represented by an associated phylogenetic to as k-NN∗).
proﬁle of length 24. Using these proﬁles as input features, The experimental results in Table 2 clearly demonstrate
we investigated the task of predicting a “qualitative” (MLR) that our k-NN approach is competitive with state-of-the-art
representation of an expression proﬁle: Actually, the proﬁle model-based methods. More precisely, k-NN∗ and CRPC-
of a gene is a sequence of real-valued measurements, each of POLY achieve the highest level of accuracy, followed by
which represents the expression level of that gene at a partic- k-NN with only a small margin. BR is outperformed by the
ular time point. Converting the expression levels into ranks, other methods, an observation which is not surprising as BR
i.e., ordering the time points (= labels) according to the as- only uses the relevance partition of labels for training and
sociated expression values) and using the Spearman correla- cannot exploit the additional rankings of labels. Similarly, the
tion as a similarity measure between proﬁles was motivated MLC versions of k-NN perform worse than their MLR coun-
in [Balasubramaniyan et al., 2005].3 Here, we further ex- terparts. Moreover, CRPC with polynomial kernels performs
tend this representation by replacing rankings with multilabel slightly better than with linear kernels, whereas for BR a sub-
rankings. To this end, we use the zero expression level as a stantial difference cannot be observed. The inﬂuence of gap
natural split point. Thus, the sets Px and Nx correspond, re- ampliﬁcation is demonstrated in Figure 1 on an MLC task
spectively, to the time points where gene x is over- and under- replicated from [Elisseeff and Weston, 2001], where genes
expressed and, hence, have an important biological meaning. from the same Yeast dataset discussed above have to be as-
  We used data from eight microarray experiments, giving sociated with functional categories. Moreover, as already an-
rise to eight prediction problems all using the same input fea- ticipated on behalf of our theoretical analysis in Section 5,
tures but different target rankings. It is worth mentioning Table 1 impressively underpins the computational efﬁciency
that these experiments involve different numbers of measure- of our approach from an empirical perspective.
ments, ranging from 4 to 18. Since in our context, each mea-
surement corresponds to a label, we obtain ranking problems 7 Concluding Remarks
of quite different complexity. Besides, even though the origi-
nal measurements are real-valued, there are many expression We presented a general framework for multilabel ranking us-
proﬁles containing ties. Each of the datasets was randomly ing a case-based methodology which is conceptually simpler
split into a training and a test set comprising 70% and 30%, and computationally less complex than previous model-based
respectively, of the instances. In compliance with [Balasub- approaches to multilabel ranking. From an empirical perspec-
                                                      tive, this approach is highly competitive with state-of-the-art
  2This data is publicly available at                 methods in terms of accuracy, while being substantially faster.
http://www1.cs.columbia.edu/compbio/exp-phylo           Conceptually, the modular aggregation step provides a
  3This transformation can be motivated from a biological as well means to extend this approach in several directions. For
as data analysis point of view.                       example, Ha and Haddawy  [2003] proposed an appealing

                                                IJCAI-07
                                                   706