 Combining Structural Descriptions and Image-based Representations for Image,
                                Object, and Scene Recognition∗

                        Nicolas Do Huu†, Williams Paquier and Raja Chatila
                                             LAAS-CNRS
                     7, avenue du Colonel Roche 31077 Toulouse Cedex 4 - France
                  nicolas.dohuu@laas.fr, william.paquier@laas.fr, raja.chatila@laas.fr

                    Abstract                          object is represented as a collection of volume parts. Thus,
                                                      there is no need of multiple view-tuned representations since
    Object and scene learning and recognition is a ma-
                                                      3D models can be virtually rotated and compared to the in-
    jor issue in computer vision, in robotics and in cog-
                                                      put image. Moreover, the use of such a model can make the
    nitive sciences. This paper presents the principles
                                                      recognition invariant to illumination and color.
    and results of an approach which extracts struc-
    tured view-based representations for multi-purpose  We can identify certain properties of the modeling process
    recognition. The structures are hierarchical and dis- for efﬁcient learning and recognition. Firstly, the training and
    tributed and provide for generalization and catego- thus the construction of new representations must be sufﬁ-
    rization. A tracking process enables to bind views ciently fast. Lack of speed is the principal cutoff of models
    over time and to link consecutive views. Scenes can based on structural descriptions because building 3D models
    also be recognized using objects as components. Il- from images remains a non trivial task. Secondly, an efﬁcient
    lustrative results are presented.                 modeling process must organize knowledge in a structured
                                                      way. A ﬁrst means of organizing the data is to build categor-
                                                      ical representations. Categorical structures make it possible
1  Introduction                                       to obtain capacities of class generalization at low cost. In-
Object and scene learning and recognition is a major issue in deed, to effectively exploit the extracted data, those must be
computer vision, in robotics, in neuroscience, and in cogni- describable at various levels of speciﬁcity: a bottle can be
tive sciences as well. And one of the main questions to this described as a container, a plastic object for recycling, etc.
respect is how to extract knowledge from 2D patterns of light Such a structure can also accelerate recognition that deals
in the camera or the retina.                          with large collections of stored objects by reducing the num-
  For the recognition of objects, two models were proposed ber of candidate object models. Another mode to structure
in the literature. In the ’image-based’ or ’view-based’ model, meanings is the decomposition of a representation as the set
an object is represented as a collection of view-speciﬁc lo- of its parts. Thanks to structural descriptions, RBC offers
cal features [Poggio and Edelman, 1990; Ullman, 1998; more robustness, in particular with respect to noise and occlu-
Tarr and Bulthoff,¨ 1998; Riesenhuber and Poggio, 1999]. sion. Moreover, it is interesting to share components among
Representations are organized in trees in which a set of view- several structural descriptions to save memory: one could use
tuned units constitutes the weighted inputs to a higher level the representation of a wheel to describe either a car, a truck
object-invariant unit. Each unit measures the similarity be- or a bike. Thirdly, a learning system for knowledge extrac-
tween the input image and its stored view, and the higher level tion and structuring must allow the addition of new repre-
unit computes the weigthed sum from its incoming connec- sentations at non-prohibitive memory cost and without ex-
tions. If the resulting value reaches a given threshold, then the plosion of complexity. This open-endedness property can be
learned object is recognized. Riesenhuber’s HMAX model approached by using the two kinds of data structures referred
of object recognition in the ventral visual stream of primates to above, in which a representation can be factorized in cate-
also proposes a similar grouping method where higher level gories or shared as components.
cells compute the maximum response of view-tuned cells  While considering object recognition one must mention
[Riesenhuber and Poggio, 1999].                       important results achieved by template-based approaches for
  The second model is based on structural descriptions. One object classiﬁcation and recognition [LeCun et al., 2004].
of the most important approaches is the ”Recognition-By- Nearest Neighbor methods, Support Vector Machines and
Components” (RBC) model [Biederman, 1987] in which each Convolutional Networks provide efﬁcient solutions but the
  ∗The work described in this paper was partially conducted within needs of structured knowledge, reusability, incremental and
the EU Integrated Project COGNIRON (”The Cognitive Compan- autonomous learning are several points addressed here which
ion”) funded by the European Commission Division FP6-IST Future are not, to the best of the authors’ knowledge, well dealt with
and Emerging Technologies under Contract FP6-002020.  by the pre-cited methods.
  †supported by the European Social Fund.               This paper presents an efﬁcient approach [Paquier, 2004](which is partially implemented) for building structured rep- many weight kernels as afferent maps. All the units of a
resentations without using 3D primitives and exhibiting the map are sharing the same set of weights, thus they can de-
properties mentioned above. The paper is organized as fol- tect and learn a pattern which is at different positions in the
lows. Section 2 presents the model we choose to both extract input maps. As shown in ﬁgure 1, the coordinates of a unit in
and organize representations. Each essential property is pre- a given map and its receptive ﬁeld in the afferent map are the
sented and illustrated by an example produced by the imple- same. Therefore the position of an active unit corresponds
mented system. Section 3 introduces the view-binding algo- to the position of the detected pattern, which provides for a
rithm and the incremental building of object-invariant detec- shift-invariance property.
tors. The use of objects as landmarks for buidling structured This type of mapping has been previously introduced by
representation of scenes is presented in section 4. We con- Fukushima’s Neocognitron and was successfully applied to
clude in section 5.                                   handwritten digit recognition [Fukushima, 2003]. Figure 2
                                                      illustrates this intrinsic map feature. In this experiment, we
2  Architecture for View-based Recognition            produce an image containing a set of four randomly dis-
                                                      tributed letters (k, p, s, u). After less than a minute, the system
We will ﬁrst describe the map structure which constitutes the has learned autonomously its own distributed representation
basic building block of our model, and discuss its inherent of the input image. Each resulting map can extract its learned
shift-invariant property. In the learning process, the maps will feature at any location in the input image. To obtain this result
specialize in certain features, and we will present the way we we must also provide our system with an inter-maps commu-
associate them in order to prevent redundancy in this special- nication channel, so that maps don’t learn the same feature
ization and structure extracted features.             several times. This procedure is called ”local competition”
2.1  Maps  and Inter-maps Connectivity                and is introduced next.
The Map: a Set of Local Classiﬁers                          a)             Weight kernels evolution
                                                              map0

      Weight kernel                   Learning stream         map
           W                                                     1
      x     q                         Integrate & Fire stream
                                                              map2

                    Integrate Max Learn                       map3
   y                & Fire
                                                            b)            Final pattern representation
                                                               Input image map0 (u) map1 (p) map2 (s) map3 (k)


Layerc, mapp Layerc+1, mapq Layerc+1, mapq, unitx,y

Figure 1: A map is a collection of units sharing the same set of
weights (kernel). Each unit is composed as a three stage cal- Figure 2: Letter extraction from noisy image input (20% ran-
culation pipeline. It receives signals incoming from previous dom noise, 20% scale and angle variation, 70 steps, 2 images
layers for integration and lateral signals from maps situated per second). a) weight kernels evolution across time. b) input
on the same layer for competition.                    image and associated maps activations.

  In order to satisfy real-time constraints, the learning and
recognition system must be able to update its representations Preventing Redundancy with Competition
at a frequency that is compatible with the modiﬁcations of the
environment, and process images at a rather high frequency  a)                 b)


(e.g., 15 fps). With this end in view we chose to implement   x                  x                blocking  signals
an integrate-and-ﬁre model in local classiﬁers built as calcu-

lation pipelines (ﬁgure 1).                                 y                  y
  We call a map a collection of local classiﬁers called units,
organized in a retinotopic1 way. Each map is associated with
one or more afferent maps which are the locations of its units’
input domains (or their receptive ﬁelds). Receptive ﬁelds of
each unit are static and each unit is thus associated to a set of Layer1 Layer2 Layer3 Layer1 Layer2 Layer3
afferent units in each afferent map. We denote Ωi the set of
                                                      Figure 3: Hierarchical maps connectivity and local compe-
afferent units of a given unit Ui.
                                                      tition. a) The letter ”L” is detected as a relative positioning
  At time t and for a given unit Ui, information ﬂows from
                                                      of horizontal and vertical bars. b) input image containing the
its receptive ﬁelds through a set of learning weights Wi(t).
Learning weights are organized in kernels, and there are as letters ”L” and ”T”: inter-maps competition with local inhi-
                                                      bition leads to distinct specialisations.
  1A retinotopic mapping means that stimuli that are adjacent to
each other in the visual world are processed by adjacent sets of units. One previously mentioned requirement concerns the struc-                                                       a)
tural decomposition into several components. To achieve    Input    Layer0   Layer1    Layer2   Layer3
such a decomposition, different maps observing the same in-
                                                                                ∈
puts must ”decide” to specialize into distinct component de-              map1,i, i [0, 7]
tectors. The undergoing process is illustrated in ﬁgures 3.a            zoom x 4   zoom x 2
and 3.b. In this example, we want to train two maps to de-
tect the letters ”L” and ”T” in the input image. A ﬁrst step
of extraction is composed of a layer of two maps which re-                               map2,0
spectively detect vertical and horizontal bars in their inputs.
When the letter ”L” is present in the image (ﬁgure 3.a), the        map0,0
pattern is decomposed as the positions of these two local ori- zoom x 8
entations. The relative positioning of the two patterns is then                                  map3,0
detectable and can be learned by a dedicated map on the next
                                                                    map
layer (Layer3).                                                        0,1
  From this point, if we add another letter (”T”) to the input
image and a second learning map to the third layer to learn it                           map2,1
(ﬁgure 3.b), then we must prevent this new map from learn-
ing the ”L” pattern one more time. This differentiation is
achieved by using a local competition in which we compare
map detection values and choose the best ﬁtted (See section
2.2 for the computation of the detection value).
  Right after calculating its detection value, every unit of
                                                       b)
each map broadcasts it to all other units on maps of the same map2,0   map2,0       map2,0      map2,0
layer and at the same coordinates. Thus, every unit whose input     input       input        input
                                                               map         map          map
coordinates correspond to the position of the letter receives     3,0         3,0         3,0       map3,0
incoming values at the ’Max’ stage of its pipeline architec-
ture (see ﬁgure 1). The ’Max’ stage, as its name indicates,
computes the maximum incoming value and sends it to its    map2,1      map2,1       map2,1      map2,1
own ’Learn’ stage. The learning process is then able to com-
pare the local unit activation incoming from the ’Integrate & Figure 4: An example of a hierarchical representation of
Fire’ stage to distant activations. By allowing learning only faces. a) network connectivity showing weight kernels and
to the best ﬁtted unit, we ensure that no map could learn one maps activities. Layer0, Layer1, Layer2, Layer3 respec-
pattern if another is already specialized to detect it. We illus- tively extract contrasts, oriented segments, eyes and mouth,
trate this inter-maps competition on Figure 3.b with the dark and face. b) recognition robustness for different subjects and
discs which represent the blocking signal.            variable postures.
Connecting Maps to Build Distributed and Hierarchical
Representations                                       ages of each of 40 distinct subjects2. In the same way as in
We adopted a layered hierarchical architecture for several rea- the previous example of letter extraction, Layer2 learns to
sons. Firstly we need this architecture for categorization. As detect the position of each eye in one map and the position of
our units achieve linear separation in their input set, complex the mouth in the other one, sharing the same segments extrac-
features cannot be extracted with a single layer. This is a tion in the previous layer (Layer1). In Layer3, we train an-
known limitation of the single layer perceptron that cannot other map which receives as inputs the detection values of the
simulate an exclusive disjunction (logical XOR). The second mouth and eyes specialized maps. As a result, this last map
reason is reusability. A huge number of extracted features learns the representation of the face with relative positioning
are encountered in different shapes: oriented segments, arcs of eyes and mouth. We also see in this example that the re-
of circles or color blobs are building blocks for more complex sulting face recognition is robust to high variations in subject
images. Since extracted information is shared in the network, posture and morphology. Nevertheless, this kind of recogni-
we avoid redundancy and the resulting computational over- tion only applies to images with limited face orientations. In
head. Following the same idea, similar meanings should be section 3, we will propose additional methods to recognize
encoded using shared sets of units. For example, the inter- 3D objects based on the pooling of overlapping views.
nal representations of a truck and of a car should intersect.
This intersection representing the shared meaning could con- 2.2 Integration, Firing and Learning
tain the internal representation of the four wheels, the steering Now that we have presented the global mechanisms involving
wheel, etc. Thanks to this distributed representation architec- the computing units, we present the internal workings of the
ture, it is easier to pool similar objects into cross categories system more precisely and the computation of the detection
(e.g., wheeled vehicules).                            values mentioned in section 2.1.
  This capability of building hierarchical and shared repre-
sentations is shown in ﬁgure 4. In this experiment we trained 2We used the faces database of the AT&T Laboratories, Cam-
a network to recognize faces using a set of ten different im- bridge http://www.uk.research.att.com/facedatabase.htmlIntegration                                           are learning, their output function must converge to a binary-
Given a unit Ui, we denote βi(t) ∈ [0, 1] its calculated output like response corresponding to a more strict linear separation.
burst at time t. The burst is the detection value mentioned be- At each step we compute a sample distribution of the input
fore. Let wij(t) ∈ Wi(t) be the learning weight associated to burst by discretizing the output burst value in 64 levels. The
                                        +
the connection between units Ui and Uj, and Ωi (t) a subset transfer function Fi is in fact the associative cumulative dis-
of afferent Ωi deﬁned as:                             tribution at time t:

              Wi(t) = {wij (t), Uj ∈ Ωi}        (1)                            X
                                                                     Fi(t, x) =     fi(t, b)          (8)
            +
           Ωi (t) = {Uj ∈ Ωi, βj (t) > 0}       (2)                           0<b≤x
We also deﬁne three weight sums as follows :
                                                      Where function fi is a sampling distribution of burst levels
            ?        X
           S (t) =         | w (t) |            (3)   updated at each step (ﬁgure 5.a). We thus obtain a sigmoidal
            i                 ij                      transfer function which permits a binary-like classiﬁcation
                  w (t)∈W (t)
                   ij    i                            behavior and a more effective competition process. Until
                β+       X
               Si (t) =       wij (t)           (4)   now, object detection is internally represented as the activa-
                           +                          tion of a limited set of computing units. Due to our connectiv-
                       Uj ∈Ωi (t)
              +         X                             ity scheme, recognitions are indeed only reﬂected at positions
             Si (t) =         wij (t)           (5)   located near the centre of the learned pattern. However pat-
                    w (t)∈W +(t)
                     ij    i                          terns presence are more than just their center since the whole
        +                                             surface they cover is relevant. So we increase the pool of ac-
where Wi (t) = {wij(t), Uj ∈ Ωi and wij(t) > 0}.      tivated units to cover a wider surface. As a ﬁrst approach, we
Then the integrated potential Pi at time t+1, which expresses                                      ?
a level of similarity between the learnt pattern and the inputs, chose a simple disc shape whose surface is given by Si (t).
is given by:                                          Learning
                                     β+               We use in our model a hebbian-like learning rule in which
                   αP Pi(t) + (1 − αP )Si (t)
         Pi(t + 1) =         +                  (6)   units tend to learn patterns responsible for their activation.
                            Si (t)                    In other words, a learning process occurs when one unit has
                                                      both ﬁred and won the competition in its layer. Moreover, we
Where αP  ∈ [0, 1] is a ﬁxed potential leak.
                                                      compute a stochastic standard deviation σij for each weight
                                                      wij which is computed and used as coefﬁcient in the learning
  a)  BURST LEVEL SAMPLING DISTRIBUTION b) CUMULATIVE SAMPLING DISTRIBUTION calculation as follows:
   0.1                      1
       step 0                  step 0
   0.09 step 500           0.9 step 500
   0.08 step 1500          0.8 step 1500                            µij (t + 1) = (1 − αW )µij (t) + αW | γij (t) |
   0.07                    0.7
   0.06                    0.6                         σ (t + 1)2 = (1 − α )σ (t) + α [µ (t + 1) − γ (t + 1)]2
   0.05                    0.5                          ij             W   ij     W   ij        ij
   0.04                    0.4
   0.03                    0.3                        wij (t + 1) = (1 − αW )wij (t) + αW [1 − 2σij (t + 1)]γij (t + 1)
   0.02                    0.2
   0.01                    0.1
    0                       0                          Where µij is a stochastic mean and αW the learning rate.
      5  10  15  20  25  30  35  40  45  50  55  60  5  10  15  20  25  30  35  40  45  50  55  60
            Burst levels            Burst levels      Hence we ensure that weights corresponding to noisy inputs
                                                      (with high standard deviation) will tend to 0 and then won’t
Figure 5: Burst sampling distribution and cumulative distri- take part in the representation. γij is a function of input burst
bution evolution. a) burst sampling distributions computed at of unit i which takes its values in {−1, 0, 1}. Maps among
steps 0, 500 and 1500. Burst values are discretized within the same layer are in competition as shown in ﬁgure 3. If
64 levels. b) corresponding cumulative distribution used as a at time t, an input burst from unit j is received γij(t) = 1,
transfer function for integration.                    else if a burst is received from any other afferent unit then
                                                      γij(t) = −1. Finally if no burst was present γij(t) = 0.

Firing                                                3   From Views to Objects
Output bursts βi generated by a unit Ui are produced both for
integration processes situated on the the next layer and for its In this section, we consider the 3D-object recognition prob-
own ”Learn” stage (see ﬁgure 1). In the ﬁrst case, burst levels lem. Let us consider a camera rotating around an object. For
are not taken into account since all positive bursts participate a given orientation we can train a dedicated map and obtain
to integration (see equation 2). In the second case, we previ- enough robustness to detect this view, say within about a 45
ously explained that we need to compare units activation in degrees angle interval centered on the learnt prototype. Wider
order to ﬁnd the best ﬁtted map in a competition process. The
burst level is thus computed as follows:              intervals can be obtained with ”simple” objects, such as balls
                                                      or paper cups, whose views are relatively invariant during
                 (
                  0          if Pi(t) < Ti,           view-point modiﬁcation, while turning around them. Our ap-
          βi(t) =                               (7)
                  Fi(t, Pi(t)) otherwise.             proch is to pool overlapping view detectors. Then, a view-
                                                      invariant (or object-tuned) detector can be obtained simply by
Where  Ti is a ﬁxed threshold and Fi is an adaptive transfer computing the maximum view-tuned map response [Riesen-
function whose variations across time are illustrated by ﬁg- huber and Poggio, 1999]. This will provide for continuous
ure 5.b. The underlying idea of this function is that, as units object recognition.            a)                                                        b)      8 orientation maps

            Input image                                               map1

              tracker                                                 map2
               map1
                                                                      map3
               map2
               map3                                                   map4

               map4                                                   map5
               map5
                                                                      map6
               map6
               map7                                                   map7

Figure 6: Object invariant detection with view-tuned maps. a) maps activities overlap to produce continuous detection. b)
resulting learning weights. These are composed of 8 kernels for each map which correspond to 8 afferent oriented segment
detectors situated on previous layer (see ﬁgure 4).

  Assuming the support of an external module producing the module. We call this process ”one-shot learning” because of
initial object detection (this could be simply a user’s selec- the use of a high learning rate. In the next frame, thanks to the
tion on the input image, but this module could be based on robustness of our model, the tracker-map can detect the ob-
saliency, motion, etc...), we must resolve several remaining ject even if it has started to move. According to the hebbian
problems. Firstly, the object-tuned map must keep track of rule, this detection is followed by a learning phase. Then,
the object during point of view modiﬁcations (temporal bind- another learning occurs which almost completly renews the
ing). Secondly, as we have no initial information about the stored prototype due to the high learning rate. From now
object’s complexity and, therefore cannot anticipate the exact the process can restart allowing continous detection. Tracker-
number of required view-tuned maps, we must ﬁnd a method maps can be viewed as short-term memories that never reach
which incrementally adds new maps to the network and asso- stable representations.
ciate them together.

Map-tracker : Short-term Memory for Temporal Binding  Temporal Binding
The different views of the same object tend to occur close to-
gether in time and space [Tarr and Bulthoff,¨ 1998]. Several Now that we can use temporal coherence to track the object
authors proposed to take advantage of this property [Stinger position, we must resolve the remaining problem of building
and Rolls, 2002; Wallis, 1996]. We particularly want to men- incrementally view-speciﬁc representations of the tracked ob-
tion Stinger and Rolls’s hypothesis about the functional archi- ject. To reach this goal we granted tracker-maps the capability
tecture and the operation of the ventral visual system tested in of creating new maps during runtime. As we are in the con-
their model called VisNet . In this model, each activation of text of moving objects and/or moving point of views, one-
a view-invariant neuron is maintained during a short period shot learning (learning with high learning rate will also be
of time. The pooling is then achieved by using an associative used to catch object’s views during movement. The very spe-
memory to link temporal memory trace and current view de- ciﬁc resulting prototype will be reﬁned as view-tuned maps
tection. Although this method seems biologically plausible, it compute standard learning rates. The binding algorithm de-
seems difﬁcult to apply to real-time robotics. As the view-to- velops as follows: if no view-tuned map recognizes the object
object membership is not a priori known, there is no criteria at the position of the tracker-map’s detection during a short
to decide whether or not a newly specialized map corresponds period of time (we use a 5 frames time-out), which is initially
indeed to the view of an object. If no preselection is applied, the case, then the tracker creates a new map and forces it to
the size of the associative memory could exceed computable learn the unknown view with a one-shot-learning signal. If a
capacities. So, it is crucial to restrict map specialization to view is detected, because a map has previously learnt it, then
relevant views. A solution comes from a tracking approach. a standard learning process occurs in this map and the detec-
In this domain, temporal coherence is also used but in an- tion produces a one-shot learning signal for the tracker-map
other way. The aim of tracking is not so much extracting in order to focus tracking on this view. This process ensures
knowledge but rather following the position of a collection of that long-term memory of a speciﬁc view has a higher priority
pixels. Temporal coherence can here be exploited by consid- during object detection than the tracker. This algorithm has
ering that persitent informations are preserved between two been used in the experiment of ﬁgure 6 in which the image of
frames and can thus be extracted. We propose to use the map a rotating object has been used to train a three-layers network.
architecture for pixel tracking in order to follow the position In ﬁgure 6.a, we see that the tracker always detects the object,
of the object and use this information as a ﬁrst step for an thanks to the feedback from long-term to short-term memory,
incremental view-tuned map creation.                  and that activity of map6 is about 3 times longer than the
  We can obtain the desired tracking capability by increasing others. Indeed, this map learned to detect orientations that
the learning rate αW of a map (see section 2.2), which we will are rather invariant during rotation. Our model can thus adapt
call tracker-map, to a value near 1. The tracker-map is trained the number of required maps depending on the complexity of
at the time the ﬁrst object position is produced by the external objects.