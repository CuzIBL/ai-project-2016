                   Analogical Learningin a Turn-Based Strategy Game

                             Thomas R. Hinrichs and Kenneth D. Forbus
                         Qualitative Reasoning Group, Northwestern University 
                                          2133 Sheridan Road 
                                          Evanston, IL, 60208 
                                {t-hinrichs, forbus}@ northwestern.edu 

                    Abstract                            In this paper, we describe how the integration of analogy, 
                                                      experimentation, and a qualitative model of city manage-
    A key problem in playing strategy games is learn- ment can support planning and learning in the game of 
    ing how to allocate resources effectively.  This can 
                                                      Freeciv [Freeciv, 2006].  In the rest of this section, we out-
   be a difficult task for machine learning when the 
                                                      line the context of this work, providing a brief synopsis of 
    connections between actions and goal outputs are  the Freeciv domain, our HTN planner, and analogical rea-
    indirect and complex.  We show how a combina-
                                                      soning and experimentation.  Next we describe how we are 
    tion of structural analogy, experimentation, and 
                                                      using a qualitative model of city economics to support credit 
    qualitative modeling can be used to improve per-  assignment and avoid local maxima.  We describe some 
    formance in optimizing food production in a strat-
                                                      experiments that demonstrate the ability to transfer and 
    egy game.  Experimentation bootstraps a case li-
                                                      adapt prior training on different cities, and we close by dis-
    brary and drives variation, while analogical reason- cussing related work and future plans. 
    ing supports retrieval and transfer.  A qualitative 
    model serves as a partial domain theory to support 1.1 The Freeciv Domain
    adaptation and credit assignment.  Together, these 
                                                      Freeciv is an open-source turn-based strategy game modeled 
    techniques can enable a system to learn the effects                                 ™
    of its actions, the ranges of quantities, and to apply after Sid Meier's series of Civilization  games [Freeciv, 
    training in one city to other, structurally different 2006].  The objective of the game is to start a civilization 
    cities.  We describe experiments demonstrating this from initial settlers in the Stone Age and expand and de-
    transfer of learning.                             velop it until you either conquer the world or win the space 
                                                      race and escape to Alpha Centauri.  In either case, the game 
                                                      can be characterized as a race to build your civilization and 
1   Introduction                                      technological sophistication faster than your opponents.  
When a novice learns how to play a strategy game, his ini- Along the way, there are many competing demands for lim-
tial focus of attention is usually on figuring out how things ited resources, investment, and development. For example, 
work.  Long before building up a repertoire of competitive players must improve terrain with irrigation and roads, 
strategies, he is trying out basic actions, seeing their effects while avoiding famine and military defeat. Too much em-
and discovering traps to avoid.  We believe analogy plays a phasis on military preparedness, for example, can make citi-
key role in this discovery process, especially in mapping zens unhappy and therefore less productive.  Money must be 
across structurally different situations.  Yet, analogy by it-
self cannot account for the very active process that learners 
go through in exploring a world and constructing explana-
tions.  In this paper, we show how experimentation strate-
gies and qualitative reasoning can support planning and 
analogy in learning resource allocation tasks in a turn-based 
strategy game.  
  Games of this sort have several interesting properties:  1) 
they involve incomplete knowledge of the world, 2) they 
entail complex interrelationships between actions and ob-
servable quantities, 3) goals may be more like optimization 
problems than states to be achieved, and 4) planning and 
executing are tightly interleaved.  Qualitative representa-
tions can serve as a partial domain theory to guide planning 
and learning at different levels of expertise.                            Figure 1: Freeciv


                                                IJCAI-07
                                                   853allocated to research into new technologies, such as iron- to defer a decision until execution time, thus implementing a 
making and democracy, which enable players to create new kind of conditional plan.  Other domain-independent primi-
improvements to cities, new types of units, and adopt new tives include bookkeeping methods for updating facts in 
types of governments, each with their own tradeoffs.   cases, and doCallback which suspends a plan until a 
  In our current set of experiments, we focus on the subtask condition becomes true, at which time it instantiates the plan 
of managing cities to optimize their use of resources, build with the new bindings and resumes execution.  This can 
infrastructure, improve terrain, and research technologies.  play an important role in evaluating delayed effects of ac-
While our planner can direct exploration, city management tions. 
tasks offer clearer evaluation metrics.  We also currently 
ignore military operations, focusing instead on how to make 1.3 Analogical Learning
a rich, productive civilization.                      A high-level goal of this research is to demonstrate how 
                                                      analogy and qualitative reasoning can support machine 
                                                      learning across increasingly distant transfer precedents.  To 
1.2 HTN Planning                                      do this, we are using the Structure Mapping Engine (SME) 
To support performing and learning in the strategy game, [Falkenhainer et al., 1989], the MAC/FAC retrieval mecha-
we have implemented a Hierarchical Task Network (HTN) nism [Forbus et al., 1995], and the SEQL generalization 
planner using the SHOP algorithm [Nau et al., 1999].  In the system [Kuehne et al., 2000] as core components.  These 
HTN planner, complex tasks are decomposed into primitive analogical mechanisms are tightly integrated in the underly-
executable tasks.  The primitives in Freeciv correspond to ing reasoning engine and provide the mechanisms for re-
packets that are sent to the game server, representing actions trieval, comparison, and transfer.  The Structure Mapping 
such as sending a unit to a particular location or telling a Engine in particular not only assesses the similarity of a 
city what to build.  Complex tasks are at the level of figur- precedent to the current situation, but also projects the pre-
ing out what a unit should do on a particular turn, or decid- vious solution into the new case by translating entities to 
ing how to ameliorate a crisis in a city (such as a famine or their mapped equivalents, in the form of candidate infer-
revolt.)  The planner generates plans for each unit and city ences. Thus, analogy provides the first level of adaptation 
at every turn and integrates them in a combined plan- automatically. 
ning/execution environment.  Planning is invoked partly in   The unit of comparison and retrieval is a case, and in this 
an event-driven manner, such that reified events from the approach, cases are not entire games (though some lessons 
game trigger certain decisions.  For example, the planning can certainly be gleaned from that granularity), nor even 
agent does not re-compute its global strategy on every turn, entire cities.  Instead, a case is an individual decision in the 
but checks to see if it has acquired any new technologies in context of a particular city at a particular moment in time in 
the last turn, and only then does it re-evaluate its strategy.  a given game.   For example, cases can capture a decision 
  A critical aspect of this game is that it requires planning about what improvements to build, what tiles to work, and 
with incomplete and uncertain information.  Terrain is not at the broader game level, what technologies to research.   
known until it is explored.  The outcomes of some actions For each type of decision, there is a set of queries repre-
are stochastic, for example, village huts may contain bar- sented in the knowledge base that are designated as possibly 
barians that will kill an explorer, or they may contain gold relevant to making the decision.  There is another set of que-
or new technologies.  There is also vastly more information ries that are relevant to capturing and assessing the case 
in the game than can be considered within a planning state.  solution.  When the decision is acted on in the game, a snap-
Consequently, the planner cannot plan an agent's actions shot of the case is constructed before and after execution 
starting with a complete initial state.  It must reify informa- and stored in the game context.  This case snapshot is used 
tion on demand by querying the game state.  At the same both for analyzing the effects of actions and supporting later 
time, the planner may project the effects of actions such that analogical transfer. 
the planned state deviates from the game state.  To reconcile   For retrieval purposes, cases are organized into case li-
these competing demands, we maintain two contexts (cf., braries from which MAC/FAC can extract the most struc-
Lenat 1995): a game context that always reflects the incom- turally similar precedent.  As the planner attempts to learn 
plete, but correct current state of the game and a planning decision tasks, it creates and populates libraries for each 
context in which states are projected forward.  Every query type of task.  After executing a plan, the current relevant 
for information that cannot be directly answered from the facts are queried and stored as a temporal sub-abstraction in 
planned state proceeds to query the game state.  Before re- the game context.   When the result of the action is evalu-
turning such game-state information, it is checked for con- ated with respect to the performance goal, the case is added 
sistency against the plan state, to ensure that, for example, a to the task-specific library of successful or failed cases, as 
unit is not believed to be in two places at the same time.  appropriate.  A case is considered successful if it both im-
This is a simple heuristic that checks for explicit negations proves the goal quantity and the quantity meets any thresh-
and for inconsistent values of functional predicates. old requirements.  By essentially “rewarding” actions that 
  In addition to reifying on demand, the other way we ac- improve the goal quantity, this can be viewed as a type of 
commodate incomplete information is through second-order learning by the method of temporal differences [Sutton, 
planning primitives.  The doPlan primitive allows a plan 1988].  However, the threshold provides an additional crite-


                                                IJCAI-07
                                                   854rion that helps prevent the accumulation of low-values cases tures of a game like Freeciv is the complexity of the quanti-
that tend to leave the system stuck in local maxima.  In other tative relationships in the simulation engine.  Understanding 
words, an action that improved a precedent from “terrible” the relationships is a critical factor in playing the game well. 
to merely “bad” probably won’t help much in the new situa- Figure 2 shows a small fraction of our model of cities in 
tion.  For maximizing goals, this initial threshold is simply Freeciv.   
“greater than zero”, while for minimizing and balance goals,   The primary way the qualitative model is currently used 
it is undefined.                                      is to determine which changes incurred by an action corre-
  One complication with mapping prior solutions to the spond to goals and whether those changes signify success or 
new problem is that particular choices may not have a clear failure.  The model allows the system to trace from local 
correspondence in the new problem.  When this happens, tasks to global goals to assign credit and blame. 
the mapping process produces an analogy skolem denoting   The second role of the model is in identifying leaf quanti-
an unmapped entity.  We resolve such skolems by collecting ties that could affect outputs, and spawning learning goals 
the facts that mention that entity in the base case, and treat- as described in ‘Overcoming local maxima’, below. 
ing them as constraints in the problem.  We then pick ran-   The greatest potential role for qualitative models will be 
domly from those choices that satisfy the constraints.   An in synthesizing higher-level strategies by proposing primi-
important insight was that it is not always necessary to re- tive actions that influence goal quantities.   This is on-going 
solve all skolems in a previous plan, but only those corre- work that is outside the scope of this paper. 
sponding to the current choice.  So for example, if a previ-
ous plan was to move a worker from one tile to another, but 
the current problem is to allocate an available worker, then 
it is not necessary to resolve the worker’s prior location.  
1.4 Experimentation
While analogy can be a powerful technique for learning, 
there must first be cases to retrieve and compare.  To boot-
strap a case library and ensure a variety of cases, we guide 
empirical learning via explicit learning goals [Ram and 
Leake, 1995].   Some learning goals are typically provided 
at the beginning of a game, such as a goal to learn the effect 
of the action of allocating workers.  Other learning goals 
may be posted as a byproduct of explaining failures,  such 
as learning the ranges of quantities.  These goals may persist 
across games. 
  The goal for learning the effects of actions determines 
how a decision task will be solved when there are insuffi-        Figure 2: A portion of the city model
cient analogical precedents or canned plans.  The experi-
mentation strategy it uses makes decisions randomly in or- 2.1 Overcoming local maxima
der to generate the requisite variation and provide cases that 
better cover the decision space.  This strategy is typically One of the difficulties in applying analogical learning to 
accompanied by additional learning goals to control parame- optimization problems of this type is that it is easy to fall 
ters (by suppressing decisions) in order to try to learn one into a local maxima, where the system performance stops 
effect at a time.   Such a trial and error approach tends to improving and keeps adopting the same precedents over and 
work best for simple low-level decisions.             over.  We have two ways to overcome this: 
  Within the context of a single game, poor choices are not First, when it fails to improve a goal, it attempts to ex-
revisited blindly, but are recorded as nogoods.  Farming the plain why.  By traversing the qualitative model, it collects 
desert typically results in that tile labeled as a nogood.  The the leaf quantities that ultimately affect the goal.  It then 
problem with this is that for some difficult cities, the system posts a learning goal to learn the maximum or minimum 
may run out of choices, at which point it must revisit the values of these quantities (e.g., the food produced on indi-
nogoods, but having tried them, it can now order them by vidual tiles).  Over several iterations, this provides a simple 
performance and choose the best of what remains.      expectation about what should be attainable.  The learner 
  Later, as successful cases accumulate, it becomes possi- uses this information to estimate the improvement that 
ble to solve problems analogically.  Still, when analogical should be possible by moving a worker from the least pro-
transfer fails or runs out of successful precedents, it falls ductive tile to a maximally productive tile.  Then, depending 
back on experimentation.                              on its current tolerance for risk, it sets the minimally accept-
                                                      able threshold for its goal quantity.  By raising the bar this 
                                                      way, it forces the learner to experiment some more to find a 
2   Exploiting a Qualitative Model                    better solution.  The tolerance for risk mentioned above is a 
A qualitative model is a partial domain theory that captures function of the penalty that will be incurred on a bad deci-
influences between quantities.   One of the outstanding fea- sion.  For example, when a city grows, its granary starts out 


                                                IJCAI-07
                                                   855empty.  Moving a worker to a less productive location can   Philadelphia             New York 
lead to a catastrophic famine if the granary is empty, but can 
be easily corrected if the granary is almost full. 
  The second method (not yet implemented) is by explicitly 
recognizing the lack of improvement.  In the same way that 
a person can look at a learning graph and recognize a local 
maxima, so should the system.  If the recent trend across 
games is flat or declining, this could serve as a spur to ex-
periment more and be less satisfied with the existing cases. 

3   Transfer Learning Experiments

In order to measure the generality of the learning mecha-               Figure 3: Freeciv Cities 
nism, we performed a number of transfer learning experi-
ments. Transfer learning involves training a performance   In order to simplify evaluation, we controlled the city 
system on one set of tasks and conditions and measuring its production queues to produce only coinage, and constrained 
effect on learning in a different but related set of tasks and the technology research agenda to work towards TheRepub-
conditions.  Three types of improvement are possible: lic.  Beyond this, it was also necessary to control for the 
higher initial performance (the "Y intercept"), faster learn- game’s unfortunate tendency to allocate workers for you 
ing rate, and/or higher final (asymptotic) performance.   In whenever a city grows or shrinks.  We did this by intercept-
the experiments we describe here, the system learned to ing these changes in the low-level packet handler routines 
allocate workers to productive tiles, while holding other and undoing the game’s interference before the learning 
potentially conflating decisions constant.            agent ever sees it.  In these experiments, we ran 10 training  
  To understand the worker allocation task better, it is nec- games on one city, “Philadelphia”, followed by 10 games on 
essary to understand that cities in Freeciv can only benefit a structurally different city, “New York” (See Figure 3).  
from working the 21 tiles in their immediate region, as Each game stopped after 50 turns, which is approximately 
shown in Figure 3.  Some tiles are more productive than long enough to show some variation in performance. 
others, by virtue of their terrain type and randomly placed   Figure 4 shows the learning curves for the trials with 
resource “specials”.   When a city is founded, it has one prior training and those without.  In this case, the trials with 
“worker” citizen available to work the land and produce prior training have flatlined, because they are essentially at 
food.  If more food is produced than consumed, then the ceiling.  The trials without prior training start out at 3 food 
city’s granary slowly fills over a number of turns.  When it points (the baseline performance) and rise to 5 after 5 
is full, the city grows by producing another worker that may games.  This could be viewed as a 60% improvement in Y-
be assigned to another tile.  If, on the other hand, more food intercept. 
is consumed than produced, then when the granary is empty   We next ran the same experiment with the cities reversed 
the city experiences a famine and loses a worker.  The task in order to determine how sensitive it is to the order of pres-
we have set for the learner is to learn which tiles will most entation.  Here, there is an improvement from 5 to 6 food 
improve food production.  The performance objective is to points, but games without prior training remain fixed at 5 
maximize the total food production at the end of 50 turns.   points, while after the third trial, the games with training 
Note that this is strongly affected by how fast the city start to oscillate between 3 and 5 points.  What is going on? 
grows, which in turn depends on making good decisions   When we looked at the saved game cases, we saw that 
early and avoiding famine.                            successful cases were transferred and applied both within 

7                                                    7

6                                                    6

5                                                    5

4                                                    4

3                                                    3

2                                with prior training on 
                                 Philadelphia        2                                 with prior training on 
                                                                                       New York 
1                                without prior training 1
                                                                                       without prior training
0                                                    0
    12345678910                                          12345678910
       Figure 4: Food production after 50 turns (New York) Figure 5: Food production after 50 turns (Philadelphia) 


                                                IJCAI-07
                                                   856 6                                                    ment learning, successful actions are rewarded so that they 
                                                      will be preferred in the future.  Here, successful actions are 
 5                                                    rewarded by adding them to the library of successful cases.  
                                                      Yet the emphasis is clearly different, because the process we 
 4                                                    have described is knowledge intensive, model-driven, and 
                                                      instance based.   Derek Bridge [2005] has also explored the 
 3                                                    relation between reinforcement learning and CBR, in the 
 2                                                    context of recommender systems. 
                             with prior training on     Qualitative models have been used in planning previ-
                             Philadelphia 
 1                                                    ously, notably Hogge’s [1988] TPLAN, which compiled a 
      N = 12                  without prior training  qualitative domain theory into planning operators, Forbus’ 
 0                                                    [1989] action-augmented envisionments, which integrated 
     12345678910                                      actions into an envisioner, and Drabble’s [1993] 
             Figure 6: Learning from failure          EXCALIBUR planning and execution system that used QP 
     (average food production in New York after 50 turns) theory with a hierarchical partial-order planner.   The strat-
                                                      egy game domain is more complex than any of the domains 
the same city and across cities.  However, in cases with tackled in previous efforts.  Our use of HTNs was inspired 
prior training, the system kept making the same mistakes by Muñoz-Avila & Aha [2004], who used HTN planning in 
over again.  Specifically what was happening was that as it a real-time strategy game. 
gained experience, it transferred successful cases early in   Prodigy/Analogy tightly integrated analogy with problem 
the game, reaching a population of 5 or 6.  Yet, there aren’t solving [Veloso, 1994].  Prodigy’s core means-ends analysis 
typically more than 3 or 4 uniquely mappable highly pro- strategy is perhaps more amenable to inferential tasks than 
ductive tiles in a city.  So above this size, the system fell the kinds of  optimization goals we face.  Liu and Stone 
back on random choice.  This in itself is not terrible, except [2006] also apply Structure Mapping to transfer learning in 
that the system had no facility for learning from prior failed the domain of RoboCup soccer. 
cases, and so would continue to try farming the desert and   Other researchers have also used Freeciv as a learning 
suffer famine.  Since this was not a case of negative transfer, domain.  Ashok Goel’s group at Georgia Tech has applied 
we modified the experimentation strategy to first retrieve model-based self-adaptation in conjunction with reinforce-
the most similar failed cases and map their choices into ment learning [Ulam et al, 2005].  We believe that analogy 
nogoods in the current case.  We ran 12 sequences of 10 will better support distant transfer learning, and that qualita-
games in each condition and plot the average learning tive models will ultimately permit strategic reasoning in 
curves in Figure 6.  These results show a 36% improvement ways that their TKML models will not.  
in initial (Y-intercept) performance, with a P-value of 0.017 
and a 95% confidence interval between 1.66 and 0.33, and 5 Summary
are therefore statistically significant.  We further analyzed 
                                                      This paper has presented initial results on integrating anal-
the saved cases and determined that the incidence of famine 
                                                      ogy, experimentation, and qualitative modeling in a system 
dropped in half.   Avoiding poor random decisions has the 
                                                      for planning, executing, and learning in strategy games.  We 
effect of converging more rapidly to an asymptote and re-
                                                      suggest that qualitative models provide an intermediate level 
ducing the variability in performance.  This suggests that 
                                                      of domain knowledge, comparable to what a novice human 
future learning experiments will require more complex, 
                                                      player might start with.  We described the use of analogy to 
open-ended tasks. 
                                                      compare before and after snapshots in order to extract the 
                                                      effects of actions.  On the basis of experimental results, we 
4   Related Work                                      implemented a minor change to the plans to enable learning 
This work is part of a government-funded program on   from failures.  This led to a pronounced improvement in 
Transfer Learning.  Other groups are pursuing similar goals behavior. 
in somewhat different ways.  ICARUS [Langley et al.,    Clearly, learning resource allocation decisions is only a 
2005] and SOAR [Nason and Laird, 2005] have both been first step to learning and transferring abstractions and high-
applied to learning in real-time game environments, using level strategies.   We are investigating the role of qualitative 
Markov Logic Networks and Bayesian techniques, respec- models for composing new plans.  We also intend to de-
tively.  ICARUS has also been extended to include an HTN velop more elaborate strategies for pursuing learning goals. 
planner to generate hierarchical skills that can be executed   This system does not currently construct explicit gener-
in the architecture.  Unlike ICARUS, our system does not alizations.  Consequently, this can be viewed as a kind of 
learn hierarchical skills, but instead employs analogy to transfer by reoperationalization [Krulwich et al., 1990], 
transfer instances of decisions.                      though by spawning learning goals in response to failures, 
  The type of learning our system is doing could be viewed we are part way to explanatory transfer.  A next step for us 
as a kind of reinforcement learning [Kaelbling et al., 1996], will be to employ SEQL to construct generalizations and 
in so far as it involves unsupervised online learning and rules once the conceptual distinctions are sufficiently cap-
requires exploration of the space of actions.  In reinforce- tured in the case base. 


                                                IJCAI-07
                                                   857