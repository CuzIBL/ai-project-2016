                    Sentence Extraction for Legal Text Summarisation
                                    Ben Hachey and Claire Grover
                                        University of Edinburgh
                                         School of Informatics
                                           2 Buccleuch Place
                                       Edinburgh EH8 9LW, UK
                                    {bhachey,grover}@inf.ed.ac.uk

                    Abstract                          2   Classiﬁcation and Relevance
    We describe a system for generating extractive    Following from [Kupiec et al., 1995], machine learning has
    summaries of texts in the legal domain, focus-    been the standard approach to text extraction summarisation
    ing on the relevance classiﬁer, which determines  as it provides an empirical method for combining different in-
    which sentences are abstract-worthy. We experi-   formation sources about the textual unit under consideration.
    ment with na¨ıve Bayes and maximum entropy es-    For relevance prediction, we performed experiments with
    timation toolkits and explore methods for selecting publicly available na¨ıve Bayes (NB) and maximum entropy
    abstract-worthy sentences in rank order. Evaluation (ME) estimation toolkits. The NB implementation, found in
    using standard accuracy measures and using corre- the Weka toolkit, is based on John and Langley’s [1995] al-
    lation conﬁrm the utility of our approach, but sug- gorithm incorporating statistical methods for nonparametric
    gest different optimal conﬁgurations.             density estimation of continuous variables. The ME estima-
                                                      tion toolkit, written by Zhang Le, contains a C++ implemen-
                                                      tation of the LMVM [Malouf, 2002] estimation algorithm.
1  Introduction                                       For ME, we use the Weka implementation of Fayyad and
In the SUM project we are developing a system for summaris- Irani’s [1993] MDL algorithm to discretise numeric features.
ing legal judgments that is generic and portable and which The features that we have been experimenting with for the
maintains a mechanism to account for the rhetorical struc- HOLJ corpus are broadly similar to those used by Teufel and
ture of the argumentation of a case. Following Teufel and Moens [2002]. They consist of location features encoding the
Moens [2002], we are developing a text extraction system position of the sentence in document, speech and paragraph;
that retains a ﬂavour of the fact extraction approach. This is a thematic words feature encoding the average tf*idf weight
achieved by combining sentence selection with information of the sentence terms; a sentence length feature encoding
about why a certain sentence is extracted—e.g. is it part of a the number of tokens in the sentence; quotation features en-
judge’s argumentation, or does it contain a decision regarding coding percentage of sentence tokens inside and in-line quote
the disposal of the case? In this way we are able to produce and whether or not the sentence is inside a block quote; entity
ﬂexible summaries of varying length and for various audi- features encoding the presence or absence of named entities
ences. Sentences can be reordered, since they have rhetorical in the sentence; and cue phrase features.
roles associated with them, or they can be suppressed if a user The term ‘cue phrase’ covers the kinds of stock phrases
is not interested in certain types of rhetorical roles. which are frequently good indicators of rhetorical status (e.g.
  We have prepared a new corpus of UK House of Lords  phrases such as The aim of this study in the scientiﬁc arti-
judgments (HOLJ) for this work which contains two layers of cle domain and It seems to me that in the HOLJ domain).
manual annotation: rhetorical role and relevance. The rhetor- Teufel and Moens invested a considerable amount of effort
ical roles represent the sentence’s contribution to the overall in building hand-crafted lexicons where the cue phrases are
communicative goal of the document. In the case of HOLJ assigned to one of a number of ﬁxed categories. A primary
texts, the communicative goal for each lord is to convince aim of the current research is to investigate whether this infor-
their peers of the soundness of their argument. In the current mation can be encoded using automatically computable lin-
version of the corpus there are 69 judgments which have been guistic features. If they can, then this helps to relieve the
annotated for rhetorical role. The second manual layer is an- burden involved in porting systems such as these to new do-
notation of sentences for ‘relevance’ as measured by whether mains. Our preliminary cue phrase feature set includes syn-
they match sentences in hand-written summaries. In the cur- tactic features of the main verb (voice, tense, aspect, modal-
rent version of the corpus, 47 of the 69 judgments which have ity, negation). We also use sentence initial part-of-speech and
been annotated for rhetorical role have also been annotated sentence initial word features to roughly approximate formu-
for relevance. A third layer of annotation is automatic linguis- laic expressions which are sentence-level adverbial or prepo-
tic annotation, which provides the features which are used by sitional phrases. Subject features include the head lemma,
the rhetorical role and relevance classiﬁers.         entity type, and entity subtype. These features approximate                     NB                ME                                    NB            ME
                  P    R     F     P     R    F                              I     C       I     C
   Cue         34.9  21.5  26.6  66.6 15.2  24.8             Cue         0.187  0.187  0.208  0.208
   Entities    30.7  26.4  28.4  66.8 15.4  25.1             Entities    0.103  0.211  0.056  0.219
   Them. Words 32.2  26.9  29.3  68.6 15.7  25.5             Them. Words 0.016  0.211  0.000  0.227
   Location    31.6  27.2  29.2  73.4 16.4  26.9             Location    0.104  0.229  -0.031 0.166
   Quotations  31.2  27.7  29.4  71.7 17.4  28.0             Quotations  0.092  0.233  0.093  0.187
   Sent. Length 31.7 29.4  29.8  71.4 16.9  27.3             Sent. Length 0.069 0.235  0.000  0.175

     Table 1: Accuracy measures for yes predictions.        Table 2: Point-biserial correlation coefﬁcients.

                                                                               2
the hand-coded agent features of Teufel and Moens. A main and maximum entropy (ME). The I column has scores for
verb lemma feature simulates Teufel and Moens’s type of ac- the individual feature sets and the C column has cumula-
tion and a feature encoding the part-of-speech after the main tive scores. The correlation results are strikingly different for
verb is meant to capture basic subcategorisation information. NB and ME. While NB successfully incorporates all features
                                                      (rpb = 0.235), ME performs best using only cue phrase, entity
                                                      and thematic word features (rpb = 0.208). For ME, the loca-
3  Experimental Results                               tion feature set actually gives a negative correlation. Judging
Table 1 contains cumulative precision (P), recall (R) and f- by these results, we would again be likely to choose NB.
scores (F) for the na¨ıve Bayes (NB) and maximum entropy
(ME) classiﬁers on the relevance classiﬁcation task.1 Though 4 Conclusions and Future Work
only the cue phrase feature set performs well individually, In this paper, we have presented work on the automatic sum-
all feature sets contribute positively to the cumulative scores marisation of legal texts for which we have compiled a new
with the exception of sentence length for ME and quotation corpus with annotation of rhetorical status, relevance and lin-
for NB. Both classiﬁers perform signiﬁcantly better than a guistic markup. We presented sentence extraction results
baseline created by selecting sentences from the end of the in classiﬁcation and ranking frameworks. Na¨ıve Bayes and
document, which obtains P, R and F scores of 46.7, 16.0 and maximum entropy classiﬁers achieve signiﬁcant improve-
23.8. F-scores for the best feature combinations are similar ments over the baseline according to standard accuracy mea-
to the partial results reported in Teufel and Moens [2002]. sures. We have also used the point-biserial correlation coefﬁ-
Taking the f-score as the best metric to optimise would lead cient for quantitative evaluation of our extraction system, the
us to choose NB.                                      results of which suggest difﬁerent optimal conﬁgurations. In
  However, a basic aspect of summarisation system design, current work, we are developing a user study that will help
especially a system that needs to be ﬂexible enough to suit determine empirically whether correlation coefﬁcients are a
various user types, is that the size of the summary will be vari- better evaluation metric than precision and recall accuracy
able. For instance, students may need a 20 sentence summary measures.
containing, for example, quite detailed background informa-
tion, to get the same information a judge would get from a 10 References
sentence summary. Furthermore, any given user might want
to request a longer summary for a certain document. So, what [Fayyad and Irani, 1993] U. Fayyad and K. Irani. Multi-interval
we actually want to do is rate how relevant/extract-worthy a discretization of continuous-valued attributes for classiﬁcation
                                                        learning. In IJCAI, 1993.
sentence is in such a way that will allow us to select sentences
in rank order. Bearing this in mind, precision is probably the [John and Langley, 1995] G. H. John and P. Langley. Esitmating
more important metric given that recall will be controlled by continuous distributions in bayesian classiﬁers. In UAI, 1995.
the size of the summary. So, ME with all but sentence length [Kupiec et al., 1995] J. Kupiec, J. Pedersen, and F. Chen. A train-
features actually appears to be the better approach for sen- able document summarizer. In SIGIR, pages 68–73, 1995.
tence extraction.                                     [Malouf, 2002] R. Malouf. A comparison of algorithms for maxi-
  Since we need a ranking rather than a yes/no classiﬁcation, mum entropy parameter estimation. In CoNLL, 2002.
this might actually be considered a regression task. However, [Teufel and Moens, 2002] S. Teufel and M. Moens. Summarising
due to the way the corpus was annotated, the target attribute scientiﬁc articles- experiments with relevance and rhetorical sta-
is in fact binary. As both of our classiﬁers are probabilistic, tus. Computational Linguistics, 28(4):409–445, 2002.
we use p(y = yes|~x) as a way to rank sentences. To evaluate [Wolf and Gibson, 2004] F. Wolf and E. Gibson. Paragraph-, word-
the ranking methods with respect to our binary gold standard, , and coherence-based approaches to sentence ranking: A com-
we use the point-biserial correlation coefﬁcient (rpb). Table parison of algorithm and human performance. In ACL, 2004.
2 contains correlation coefﬁcients between the gold standard
yes/no classiﬁcation and p(y = yes|~x) for na¨ıve Bayes (NB)

  1Note that this is a strict evaluation that counts only yes predic- 2It has been argued that this is actually a better evaluation than
tions. Micro- and macro-averaging over yes and no predictions give standard accuracy measures, which do not account for degree of
e.g. f-scores of 87.6 and 67.3 respectively for ME.   agreement [Wolf and Gibson, 2004].