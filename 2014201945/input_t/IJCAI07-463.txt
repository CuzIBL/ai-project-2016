                     On the Automatic Scoring of Handwritten Essays

                 Sargur Srihari, Rohini Srihari, Pavithra Babu, Harish Srinivasan
                Center of Excellence for Document Analysis and Recognition (CEDAR)
         University at Buffalo, State University of New York Amherst, New York 14228, U.S.A.
                                       srihari@cedar.buffalo.edu

                    Abstract
    Automating the task of scoring short handwritten
    student essays is considered. The goal is to as-
    sign scores which are comparable to those of hu-
    man scorers by coupling two AI technologies: op-
    tical handwriting recognition and automated essay
    scoring. The test-bed is that of essays written by
    children in reading comprehension tests. The pro-
    cess involves several image-level operations: re-
    moval of pre-printed matter, segmentation of hand-
    written text lines and extraction of words. Recog-
    nition constraints are provided by the reading pas-
                                                      Figure 1: From the New York English Language Arts assessment
    sage, the question and the answer rubric. Scoring is for Grade 8, 2001 – two of three pages of the story “American First
    based on using a vector space model and machine   Ladies” are shown.
    learning of parameters from a set of human-scored
    samples. System performance is comparable to that
    of scoring based on perfect manual transcription.

1  INTRODUCTION
Handwritten essays are widely used for student performance
evaluation in schools and colleges. Since this approach to
evaluation is efﬁcient and reliable it is likely to remain a key
component of learning. Assessing large numbers of handwrit-
ten essays is a relatively time-consuming and monotonous
task. In statewide examinations on reading comprehension in
the U.S. there is an intense need to speed up and enhance the Figure 2: Sample answer sheets of three students (a-c) based on
                                                      the reading comprehension passage of Fig. 1. The human assigned
process of rating handwritten essays while maintaining cost scores for these essays, on a scale of 0-6, were 2, 4 and 4 respec-
effectiveness. The assessment can also be used as a source of tively.
timely, relatively inexpensive and responsible feedback about
writing.
  Writing done by hand is the primary means of testing stu- scored by human assessors on a seven-point scale of 0-6. A
dents on state assessments of reading comprehension. Con- rubric for the scoring is given in Table 1. This is referred to
sider as an example the New York State English Language as a holistic rubric– which is in contrast to an analytic rubric
Assessment (ELA) administered statewide in grades 4 and 8. that captures several writing traits.
In the reading part of the test the student is asked to read a There is signiﬁcant practical and pedagogical value in
passage such as that given in Fig 1 and answer several ques- computer-assisted evaluation of such tests. The task of scor-
tions in writing.                                     ing and reporting the results of these assessments in a timely
  An example of a reading comprehension question based manner is difﬁcult and relatively expensive. There is also an
on the passage of Fig. 1 is the following:“How was Martha intense need to test later in the year for the purpose of cap-
Washington’s role as First Lady different from that of Eleanor turing the most student growth and at the same time meet the
Roosevelt? Use information from American First Ladies in requirement to report student scores before summer break.
your answer.” The completed answer sheets of three different The biggest challenge is that of reading and scoring the hand-
students to the question are given in Fig. 2. The answers are written portions of large-scale assessments.

                                                IJCAI-07
                                                  2880               6               5              4              3              2             1
         Understanding   Understanding      Logical        Partial       Readable        Brief
             of text        roles of     and Accurate   Understanding
                           ﬁrst ladies
        understanding of   Organized      Only literal    Drawing       Not Logical   Repetitive
         similarities and                understanding   conclusions
           differences                     of article   about roles of
         among the roles                                  ﬁrst ladies
         Characteristics Not thoroughly   Organized        Sketchy       Limited      Understood
          of ﬁrst ladies   eleaborate                                  Understanding only sections
           Complete,                         Too            Weak
          Accurate and                    generalized
           Insightful
            Focused,                     Facts without
           Fluent and                   synchronization
           Engaging

    Table 1: Holistic Rubric Chart for “How was Marth Washington’s role as First Lady different from that of Eleanor Roosevelt?”

  The assessment problem is a well-constrained problem in words, when considered in isolation, are illegible. Local con-
artiﬁcial intelligence (AI) whose solution will push forward text can resolve such ambiguity. The reading comprehension
existing technologies of handwriting recognition and auto- passage and the rubric provide a rich source of contextual in-
matic essay scoring. The task is a ﬁrst step in solving an formation that can be exploited to get high recognition rates.
inverse of a grand challenge of AI– that of a computer pro- However, the task itself is one of correct interpretation rather
gram to read a chapter of a freshman physics textbook and than that of recognizing every illegible word. It includes char-
answer the questions at the end [Reddy, 2003].        acter recognition (OCR), word recognition, part-of-speech
                                                      tagging, etc.
2  COMPONENT AI SUBSYSTEMS                              Prior to OHR, several image processing steps need to be
                                                      performed on answer sheets, e.g., detecting and eliminating
The major component AI systems for solving the task are extraneous information such printed instructions, questions,
optical handwriting recognition (OHR) and automatic essay ruled lines and margin lines. Within the handwritten text the
scoring (AES). Both subsystems involve a learning phase. ordering of the lines has to be determined and within each line
                                                      the words need to be segmented. These operations are sim-
2.1  Handwriting Recognition                          ilar to those for analyzing unconstrained handwritten pages
OHR is concerned with transforming an image of handwritten for forensic, or questioned document, analysis [Srihari et al.,
text into its textual form; a survey of OHR is [Plamondon and 2003]).
Srihari, 2000]. While computers have become indispensable Handwriting Interpretation is where the goal is not so much
tools for two of three R’s, viz., arithmetic and writing, their one of recognizing every character and word perfectly but to
use in the third R of reading is still emerging. OHR involves perform some overall task using recognition results. It in-
several processing steps such as form (or rule line) removal, volves using contextual information when there is uncertainty
line/word segmentation and recognition of individual words. in the speciﬁc components. Such approaches have found suc-
Word recognition relies on a lexicon of words-which could cess when the domain is limited and contextual information
be derived from the passage, question and rubric available in is available, e.g., in the postal domain the destination ZIP+4
statewide tests.                                      code can be assigned even when individual components are
  Recognition of characters and words is performed in a two poorly written [Srihari and Keubert, 1997].
step process of feature extraction followed by classiﬁcation.
Features can be at the character level (called analytic recogni- 2.2 Automatic Essay Scoring (AES)
tion) or at the word level (holistic recognition). Word recog- Automatic scoring of computer readable essays has been a
nition, which is the task of assigning a word image to a mem- topic of research for over four decades. A limitation of all
ber of a list of words (or lexicon), can be performed well for past work is that the essays or examinations have to be in
correctly segmented words with small lexicons. The process computer readable form. A survey of previous AES meth-
is error prone for mis-segmented text and large lexicons. ods has been made by Palmer, et. al (2002). Project Essay
  When the lexicon is limited, a majority of the words are Grade (PEG) (Page, 1961) uses linguistic features from which
correctly recognized although there are substitution errors a multiple regression equation is developed. In the Produc-
and missed words. These errors can be reduced by better tion Automated Essay Grading System a grammar checker,
word segmentation and by using linguistic context in the form a program to identify words and sentences, software dictio-
of transitional probabilities between words, between parts- nary, a part-of-speech tagger, and a parser are used to gather
of-speech tags, noun phrases, etc. It is possible that certain data. E-rater (Burstein, 2003) uses a combination of statisti-

                                                IJCAI-07
                                                  2881cal and NLP techniques to extract linguistic features. Larkey
(1998) implemented an AES approach based on text catego-
rization techniques (TCT). One approach to AES is based on
an information retrieval technique known as latent semantic
indexing. Its application to AES, known as latent semantic
analysis (LSA), uncovers lexical semantic links between an
essay and a gold standard. Landauer, et. al. (1998) developed
the Intelligent Essay Assessor using LSA. A matrix for the
essay is built, and then transformed by the algebraic method
of singular value decomposition (SVD) to approximately re-
produce the matrix using reduced dimensional matrices built
for the topic domain. Using SVD new relationships between
words and documents are uncovered, and existing relation-
ships are modiﬁed to represent their signiﬁcance. Using LSA
the similarity between two essays can be measured despite
differences in individual lexical items. Where a gold stan-
dard exists, LSA is a robust approach. It correlates as closely
with human raters as human raters correlate with each other
[Landauer et al., 2003].

3  SYSTEM     INTEGRATION                                      Figure 3: Answer Processor Architecture.
The overall task is that of handwriting interpretation for essay
scoring. A ﬁrst level of integration is to sequentially couple which the sufﬁx should be removed or replaced to form the
the OHR and AES systems by regarding OHR simply as a  stem of the word, which would be common among all varia-
transcription system. Both the OHR and AES components in- tions. For example the word reading after sufﬁx stripping is
volve machine learning at several levels. In the case of OHR, reduced to read.
lexicons are acquired from three sources: reading passage,
question and rubric. Learning of handwriting styles in the 3.2 AES
formation of letters and words is a classic pattern recognition
                                                      In the LSA approach, a good approximation of the computer
problem. In the case of AES the learning process acquires
                                                      score to a human score heavily depends on the optimal re-
a method associating content to score by learning from a set
                                                      duced dimensionality. This optimal dimension is related to
of human scored essays. A system to analyze and score the
                                                      the features that determine the term meaning from which we
scanned answer sheet(s)is shown in Fig. 3.
                                                      can derive the hidden correlations between terms and answer
3.1  OHR                                              documents. Reducing the dimensions is done by omitting in-
                                                      consequential relations and retaining only signiﬁcant ones. A
After performing image pre-processing steps, e.g., fore- factor analysis method such as Singular Value Decomposition
ground/background extraction, eliminating non-informative (SVD) helps reduce the dimensionality to a desired approxi-
material (rule lines and printed markings), determining the mation.
presence of handwriting, etc., the main tasks are:      The ﬁrst step in LSA is to construct a t x n term-by-
  (1)Word segmentation into lines and words in the presence document matrix M whose entries are frequencies. SVD or
of ambiguity. To determine whether a gap is a true gap or not two-mode factor analysis decomposes this rectangular matrix
by learning from the current document.                into three matrices [Baeza-Yates and Ribeiro-Neto, 1999].
  (2) Word recognition: When vocabularies are large con- The SVD for a rectangular matrix M can be deﬁned as
textual information needs to be exploited to dynamically limit
                                                                                    
word choices. Contextual information is available in the form             M  = TSD  ,                 (1)
of the passage to be read and the answer rubric.                  
  After words are recognized the resulting word sequences where prime ( ) indicates matrix transposition, M is the rect-
are written to text ﬁles. These text ﬁles are then pre-processed angular term by document matrix with t rows and n columns,
for AES which include the following steps: (a). Removing T is the t x m matrix, which describes rows in the matrix M
punctuations and special characters, (b). Converting upper as left singular vectors of derived orthogonal factor values, D
case to lower case for generalization, (c). Stop word removal is the n x m matrix, which describes columns in the matrix M
- removing common words such as a and the which occur as right singular vectors of derived orthogonal factor values,
                                                      S is the m x m diagonal matrix of singular values such that
very often and are not of signiﬁcant importance, (d). Stem-          
ming - morphological variants of words have similar seman- when T , S and D are matrix multiplied Mis reconstructed,
tic interpretations and therefore a stemming algorithm is used and m is the rank of M =min(t , n).
to reduce the word to its stem or root form. The algorithm To reduce dimensionality to a value k from the matrix S we
[Porter, 1980]] uses a technique called sufﬁx stripping where have to delete m − k rows and columns starting from those
an explicit sufﬁx list is provided along with a condition on which contain the smallest singular value to form the matrix

                                                IJCAI-07
                                                  2882                                                                         Mean score difference = 1.70 
                                                        6
                                                                                                computer score
                                                                                                human score

                                                        5


                                                        4


                                                        3
                                                       Grade


                                                        2


                                                        1


                                                        0
                                                         0   5    10  15   20   25  30   35   40   45  50
Figure 4: Projected locations of 50 Answer Documents in two                 Query documents
dimensional plane
                                                      Figure 5: Comparison of human scores and Manual Tran-
                                                     scription - Latent Semantic Analysis (MT-LSA) scores on 46
S1. The corresponding columns in T and rows in D are also
                                                     student responses to the American First Ladies question: MT-
deleted to form matrices T1 and D1 respectively. The matrix LSA scores (open circles) are within 1.70 of human scores
M1 is an approximation of matrix M with reduced dimen- (stars).
sions as follows

                              
                  M1  = T1S1D1.                 (2)     Two different sets of 96 transcribed essays were created,
                                                      the ﬁrst by manual transcription (MT) and the second by the
Standard algorithms are available to perform SVD. To illus-
                                                      OHR system. The lexicon for the OHR system consisted of
trate, from the document-term matrix constructed from 50 es-
                                                      unique words from the passage to be read, which had a size
says from the American First Ladies example shown in Fig 1
                                                      of 274. Separate training and validation phases were con-
and Fig 2, the ﬁrst two principal components are plotted in
                                                      ducted for the MT and OHR essays. For the MT essays, the
Fig.4. The principal components are the two most signiﬁcant
                                                      document-term matrix M had t = 521 and m =50and the
dimensions of the term by document matrix shown in Table 2
                                                      optimal value of k was determined to be 8. For the OHR es-
after applying SVD. This is a representation of the documents
                                                      says, the corresponding values were t = 164, m =50and
in semantic space. The similarity of two documents in such
                                                      k =5. The smaller number of terms in the OHR case is
a semantic space is measured as the cosine of the angle made
                                                      explained by the fact that several words were not recognized.
by these documents at the origin.
  The testing set consists of a set of scored essays not used Comparisons of the human-assigned scores (the gold-
in the training and validation phases. The term-document ma- standard) with (i) automatically assigned scores based on MT
trix constructed in the training phase and the value of k de- is shown in Fig. 5 and (ii) automatically assigned scores
termined from the validation phase are used to determine the based on OHR is shown in 6. Using MT the human-machine
scores of the test set.                               mean difference was 1.70 (Fig. 6). Using OHR the human-
                                                      machine difference was 1.65 (Fig. 6). Thus a 0.05 difference
                                                      is observed between MT and OHR using LSA scoring. These
4  PERFORMANCE EVALUATION                             preliminary results demonstrate the potential of the method
The corpus for experimentation consisted of 96 handwritten for holistic scoring and robustness with OHR errors.
answer essays for the “American First Ladies” task shown
in Fig. 1. Of these essays 73 were by students and 23 5   SUMMARY AND DISCUSSION
were by teachers. Each of the 96 answer essays were man-
ually assigned a score (the ”gold standard”) by education re- Automatically evaluating handwritten essays involves the in-
searchers. The essays were scored manually using the holis- tegration of optical handwriting recognition and automatic
tic grading rubric shown in Table 1. The essays were divided essay scoring methodologies. Handwriting recognition is as-
into 50 training samples (each of which also served as vali- sisted by constraints provided by the reading passage, ques-
dation samples in the leave one out cross validation method tion and rubric. Scoring based on latent semantic analysis
employed) and 46 testing samples. The training set had a (LSA)is robust with respect to recognition inadequacies. Re-
human-score distribution on the seven-point scale as follows: sults on a small testing set show that with manually tran-
1,9,9,11,5,8,7. The testing set had human-score distributions scribed (MT) essays, LSA scoring has on an average less than
of 0,8,9,10,5,8,6. The answer sheets were scanned as gray a two-point difference from human scoring. With the same
scale images at a resolution of 300 pixels per inch.  test set, OHR-LSA scoring has a minor difference from MT-

                                                IJCAI-07
                                                  2883                    Mean score difference = 1.65      [                      ]
  6                                                    Ishioka and Kameda, 2004 T. Ishioka and M. Kameda. Au-
                                         computer score
                                         human score     tomated japanese essay scoring system: Jess. 2004.
  5.5                                                 [Landauer et al., 2003] T. Landauer, D. Laham, and P. Foltz.

  5                                                      Automated scoring and annotation of essays with the in-
                                                         telligent essay assessor. Automated Essay Scoring, 2003.
  4.5
                                                      [Plamondon and Srihari, 2000] R. Plamondon and S. N. Sri-
  4                                                      hari. On-line and off-line handwriting recognition: A com-
                                                         prehensive survey. IEEE Trans. Pattern Analysis and Ma-
  3.5

 Grade                                                   chine Intelligence, 22(1):63–84, 2000.
  3                                                   [Porter, 1980] M.F Porter. An algorithm for sufﬁx stripping.
                                                         Program
  2.5                                                           , 14(3):130–137, 1980.
                                                      [Reddy, 2003] R Reddy. Three open problems in artiﬁcial
  2                                                      intelligence. Journal of the ACM, 50(1):1–4, 2003.
  1.5                                                 [Srihari and Keubert, 1997] S.N. Srihari and E. J. Keubert.
                                                         Integration of handwritten address interpretation technol-
  1
   0    5   10   15  20   25  30   35   40  45   50
                      Query documents                    ogy into the united states postal service remote computer
                                                         reader system. Proc. Int. Conf. Document Analysis and
Figure 6: Comparison of human scores and OHR-LSA scores  Recognition, Ulm, Germany, pages 892–896, 1997.
on 46 student responses to the American First Ladies ques- [Srihari et al., 2003] S. N. Srihari, B. Zhang, C. Tomai,
tion: OHR-LSA scores (open circles) are within 1.65 of hu- S. Lee, Z. Shi, and Y. C. Shin. A system for handwriting
man scores (stars).                                      matching and recognition. Proc. Symp. Document Image
                                                         Understanding Technology, Greenbelt, MD, pages 67–75,
                                                         2003.
LSA scoring.
  The results point out that despite errors in word recogni-
tion the overall scoring performance is good enough to have
practical value. This points out that when the evaluation of an
OHR system is based not so much on word recognition rates
but in terms of the overall application in which it is used, the
performance can be quite acceptable. The same phenomenon
has been observed in other OHR applications such as postal
address reading where the goal is not so much as to read every
word correctly but achieve a correct sortation.
  The LSA approach has the advantage that as a “bag of
words” or holistic technique it is robust with respect to word
recognition errors. However it ignores linguistic structures.
The analytic approach to scoring is based on idea develop-
ment, organization, cohesion, style, grammar, or usage con-
ventions. The result of analytic scoring will be more useful
to teachers and education researchers.
  Essay scoring based on language features such as general
vocabulary, passage related vocabulary, percentage of difﬁ-
cult words, percentage of passive sentences, rhetorical fea-
tures and usage of conjunctions, pronouns, punctuations etc
for connectedness could play a signiﬁcant role in improving
the performance of this system. This approach is employed in
the automated Japanese Essay Scoring System: Jess [Ishioka
and Kameda, 2004] where the ﬁnal weighted score is calcu-
lated by penalizing a perfect score based on features recog-
nized in the essay.

References
[Baeza-Yates and Ribeiro-Neto, 1999] R. Baeza-Yates and
  B Ribeiro-Neto. Modern information retrieval.NewYork:
  Addison-Wesley, 1999.

                                                IJCAI-07
                                                  2884