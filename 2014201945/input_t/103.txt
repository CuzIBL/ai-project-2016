      Taming Decentralized POMDPs: Towards Efficient Policy Computation for 
                                               Multiagent Settings 

        R. Nair and M. Tambe                           M. Yokoo                      D. Pynadath, S. Mar sella 
        Computer Science Dept.            Coop. Computing Research Grp.            Information Sciences Institute 
   University of Southern California            NTT Comm. Sc. Labs               University of Southern California 
         Los Angeles CA 90089                   Kyoto, Japan 619-0237                Marina del Rey CA 90292 
         {nair,tambe}@usc.edu                 yokoo@cslab.kecl.ntt.co.jp           {pynadath, marsella}@isi.edu 

                         Abstract                                 However, with a few exceptions, effective algorithms for 
                                                               deriving policies for decentralized POMDPs have not been 
      The problem of deriving joint policies for a group 
                                                               developed. Significant progress has been achieved in efficient 
      of agents that maximize some joint reward func•
                                                               single-agent POMDP policy generation algorithms [Mona-
     tion can be modeled as a decentralized partially 
                                                               han, 1982; Cassandra et al, 1997; Kaelbling et al, 1998]. 
      observable Markov decision process (POMDP). 
                                                               However, it is unlikely such research can be directly car•
      Yet, despite the growing importance and applica•
                                                               ried over to the decentralized case. Finding optimal poli•
     tions of decentralized POMDP models in the mul-
                                                               cies for decentralized POMDPs is NEXP-complete [Bern•
     tiagents arena, few algorithms have been devel•
                                                               stein et al, 2000]. In contrast, solving a POMDP is PSPACE-
     oped for efficiently deriving joint policies for these 
                                                               complete [Papadimitriou and Tsitsiklis, 1987]. As Bernstein 
      models. This paper presents a new class of lo•
                                                               et al. [2000] note, this suggests a fundamental difference in 
      cally optimal algorithms called "Joint Equilibrium-
                                                               the nature of the problems. The decentralized problem can•
     based search for policies (JESP)". We first describe 
                                                               not be treated as one of separate POMDPs in which individ•
      an exhaustive version of JESP and subsequently 
                                                               ual policies can be generated for individual agents because 
      a novel dynamic programming approach to JESP. 
                                                               of possible cross-agent interactions in the reward, transition 
      Our complexity analysis reveals the potential for 
                                                               or observation functions. (For any one action of one agent, 
     exponential speedups due to the dynamic program•
                                                               there may be many different rewards possible, based on the 
      ming approach. These theoretical results are ver•
                                                               actions that other agents may take.) In some domains, one 
      ified via empirical comparisons of the two JESP 
                                                               possibility is to simplify the nature of the policies considered 
     versions with each other and with a globally opti•
                                                               for each of the agents. For example, Chades et al. [2002] 
     mal brute-force search algorithm. Finally, we prove 
                                                               restrict the agent policies to be memoryless (reactive) poli•
     piece-wise linear and convexity (PWLC) proper•
                                                               cies. Further, as an approximation, they define the reward 
     ties, thus taking steps towards developing algo•
                                                               function and the transition function over observations instead 
     rithms for continuous belief states. 
                                                               of over states thereby simplifying the problem to solving a 
                                                               multi-agent MDP [Boutilier, 1996]. Xuan et al. [2001] de•
 1   Introduction                                              scribe how to derive decentralized MDP (not POMDP) poli•
                                                               cies from a centralized MDP policy. Their algorithm, which 
 As multiagent systems move out of the research lab into crit•
                                                               starts with an assumption of full communication that is grad•
 ical applications such as multi-satellite control, researchers 
                                                               ually relaxed, relies on instantaneous and noise free commu•
 need to provide high-performing, robust multiagent designs 
                                                               nication. Such simplifications reduce the applicability of the 
 that are as nearly optimal as feasible. To this end, researchers 
                                                               approach and essentially side-step the question of solving de•
 have increasingly resorted to decision-theoretic models as a 
                                                               centralized POMDPs. Peshkin et al. [2000] take a different 
 framework in which to formulate and evaluate multiagent de•
                                                               approach by using gradient descent search to find local op•
 signs. Given a group of agents, the problem of deriving sep•
                                                               timum finite-controllers with bounded memory. Their algo•
 arate policies for them that maximize some joint reward can 
                                                               rithm finds locally optimal policies from a limited subset of 
 be modeled as a decentralized POMDP (Partially Observable 
                                                               policies, with an infinite planning horizon, while our algo•
 Markov Decision Process). In particular, the DEC-POMDP 
                                                               rithm finds locally optimal policies from an unrestricted set 
 (Decentralized POMDP) [Bernstein et al., 2000] and MTDP 
                                                               of possible policies, with a finite planning horizon. 
 (Markov Team Decision Problem [Pynadath and Tambe, 
 2002]) are generalizations of a POMDP to the case where         Thus, there remains a critical need for new efficient 
there are multiple, distributed agents basing their actions on algorithms for generating optimal policies in distributed 
their separate observations. These frameworks allow a vari•    POMDPs. In this paper, we present a new class of algorithms 
ety of multiagent analysis. Of particular interest here, they  for solving decentralized POMDPs, which we refer to as Joint 
 allow us to formulate what constitutes an optimal policy for a Equilibrium-based Search for Policies (JESP). JESP iterates 
multiagent system and in principle derive that policy.         through the agents, finding an optimal policy for each agent 


 MULTIAGENT SYSTEMS                                                                                                   705  assuming the policies of the other agents are fixed. The it• tiger problem used in illustrating single agent POMDPs[Kael-
 eration continues until no improvements to the joint reward bling et ai, 1998] and create an MTDP ((S, A, P, O, R)) 
 is achieved. Thus JESP achieves a local optimum similar     for this example. In our modified version, two agents are 
 to a Nash Equilibrium. We discuss Exhaustive-JESP which     in a corridor facing two doors:"left" and "right". Behind 
 uses exhaustive search to find the best policy for each agent. one door lies a hungry tiger and behind the other lies un•
 Since this exhaustive search for even a single agent's policy told riches but the agents do not know the position of ei•
 can be very expensive, we also present DP-JESP which im•    ther. Thus, 5 = {SL,SR}, indicating behind which door 
 proves on Exhaustive-JESP by using dynamic programming      the tiger is present. The agents can jointly or individually 
 to incrementally derive the policy. We conclude with sev•   open either door. In addition, the agents can independently 
 eral empirical evaluation that contrast JESP against a glob• listen for the presence of the tiger. Thus, A1 = A2 — 
 ally optimal algorithm that derives the globally optimal pol• {'OpenLeft', lOpenRight', 'Listen'}. The transition func•
 icy via a full search of the space of policies. Finally, we prove tion P, specifics that every time either agent opens one of 
 piece-wise linear and convexity (PWLC) properties, thus tak• the doors, the state is reset to SL or SR with equal proba•
 ing steps towards developing algorithms for continuous initial bility, regardless of the action of the other agent, as shown 
 belief states.                                              in Table 1. However, if both agents listen, the state remains 
                                                             unchanged. After every action each agent receives an obser•
 2 Model                                                     vation about the new state. The observation function, 01 or 
                                                             02 (shown in Table 2) will return either HL or HR with dif•
 We describe the Markov Team Decision Prob•                  ferent probabilities depending on the joint action taken and 
 lem (MTDP) [Pynadath and Tambe, 2002] frame-                the resulting world state. For example, if both agents listen 
 work in detail here to provide a concrete illustra•         and the tiger is behind the left door (state is SL), each agent 
 tion of a decentralized POMDP model. However, receives the observation HL with probability 0.85 and HR 
 other decentralized POMDP models could poten•               with probability 0.15. 
 tially also serve as a basis [Bernstein et ai, 2000; 
 Xuan et al.,2001]. 
   Given a team of n agents, an MTDP [Pynadath and 
 Tambe, 2002] is defined as a tuple: 
 5 is a finite set of world states A = 
             , where A1,..., An, are the sets of action for 
 agents 1 to n. A joint action is represented as (a1,..., a„). 
 P(si,(a1,...,an) ,sf), the transition function, represents 
 the probability of the current state is .sf, if the previous               Table 1: Transition function 
 state is Si and the previous joint action is (o1,...,a„). 
            are the set of observations for agents 1 to n. 
 0(s, (a1,... ,an) ,u) , the observation function, represents 
the probability of joint observation if the current state is 
 $ and the previous joint action is (a1,..., an). The agents 
receive a single, immediate joint reward R(s,a1,.. . ,an) 
 which is shared equally. 
   Practical analysis using models like MTDP often assume 
that observations of each agent is independent of each other's 
observations. Thus the observation function can be expressed 
as 0(s, (ai,..., an), = O1(s, (au... ,a„), •...•                    Table 2: Observation function for each agent 

   Each agent i chooses its actions based on its local policy, If either of them opens the door behind which the tiger 
7Tj, which is a mapping of its observation history to actions. is present, they are both attacked (equally) by the tiger (sec 
Thus, at time t, agent i will perform action where Table 3). However, the injury sustained if they opened the 
                                  refers to the joint policy door to the tiger is less severe if they open that door jointly 
of the team of agents. The important thing to note is that in than if they open the door alone. Similarly, they receive 
this model, execution is distributed but planning is central• wealth which they share equally when they open the door to 
ized. Thus agents don't know each other's observations and   the riches in proportion to the number of agents that opened 
actions at run-time but they know each other's policies.     that door. The agents incur a small cost for performing the 
                                                             'Listen' action. 
3 Example Scenario                                             Clearly, acting jointly is beneficial (e.g., A1 — A2 = 
                                                             'OpenLeft') because the agents receive more riches and sus•
For illustrative purposes it is useful to consider a familiar tain less damage by acting together. However, because the 
and simple example, yet one that is capable of bringing out  agents receive independent observations (they do not share 
key difficulties in creating optimal policies for MTDPs. To  observations), they need to consider the observation histories 
that end, we consider a multiagent version of the classic    of the other agent and what action they are likely to perform. 


706                                                                                          MULTIAGENT SYSTEMS                 Table 3: Reward function A                       At each time step, the computation of performs 
                                                               a summation over all possible world states and agent 
                                                               observations, so the time complexity of this algorithm 

We also consider consider another case of the reward func•     is The overall search performs 
tion, where we vary the penalty for jointly opening the door   this computation for each and every possible joint pol•
to the tiger (See Table 4).                                    icy. Since each policy specifies different actions over pos•
                                                               sible histories of observations, the number of possible poli•
                                                               cies for an individual agent is O The 
                                                               number of possible joint policies for n agents is thus 
                                                               O , where and correspond to 
                                                               the largest individual action and observation spaces, respec•
                                                               tively, among the agents. The time complexity for find•
                                                               ing the optimal joint policy by searching this space is thus: 
                                                               O  

                Table 4: Reward function B                     5 Joint Equilibrium-based Search for Policies 
                                                              Given the complexity of exhaustively searching for the op•
                                                              timal joint policy, it is clear that such methods will not be 
                                                               successful when the amount of time to generate the policy 
                                                               is restricted. In this section, we will present algorithms that 
                                                              are guaranteed to find a locally optimal joint policy. We refer 
4 Optimal Joint Policy                                        to this category of algorithms as "JESP" (Joint Equilibrium-
                                                              Based Search for Policies). Just like the solution in Section 4, 
                                                              the solution obtained using JESP is a Nash equilibrium. In 
When agents do not share all of their observations, they must 
                                                              particular it is a locally optimal solution to a partially ob•
instead coordinate by selecting policies that are sensitive to 
                                                              servable identical payoff stochastic game(POIPSG) [Peshkin 
their teammates' possible beliefs, of which each agent's en•
                                                              et al., 2000]. The key idea is to find the policy that maxi•
tire history of observations provides some information. The 
                                                              mizes the joint expected reward for one agent at a time, keep•
problem facing the team is to find the optimal joint policy, 
                                                              ing the policies of all the other agents fixed. This process 
i.e. a combination of individual agent policies that produces 
                                                              is repeated until an equilibrium is reached (local optimum is 
behavior that maximizes the team's expected reward. 
                                                              found). The problem of which optimum the agents should se•
  One sure-fire method for finding the optimal joint policy   lect when there are multiple local optima is not encountered 
is to simply search the entire space of possible joint policies, since planning is centralized. 
evaluate the expected reward of each, and select the policy 
with the highest such value. To perform such a search, we     5.1 Exhaustive approach(Exhaustive-JESP) 
must first be able to determine the expected reward of a joint The algorithm below describes an exhaustive approach for 
policy. We compute this expectation by projecting the team's  JESP. Here we consider that there are n cooperative agents. 
execution over all possible branches on different world states We modify the policy of one agent at a time keeping the poli•
and different observations. We present here the 2-agent ver•  cies of the other n - 1 agents fixed. The function best-
sion of this computation, but the results easily generalize to Policy, returns the joint policy that maximizes the expected 
arbitrary team sizes. At each time step, we can compute the   joint reward, obtained by keeping n — 1 agents' policies 
expected value of a joint policy, for a team                  fixed and exhaustively searching in the entire policy space 
starting in a given state, with a given set of past observa•  of the agent whose policy is free. Therefore at each itera•
tions, and as follows:                                        tion, the value of the modified joint policy will always either 


MULTIAGENT SYSTEMS                                                                                                   707  increase or remain unchanged. This is repeated until an equi• steps. In this section, we show how we can exploit an anal•
 librium is reached, i.e. the policies of all n agents remains ogous optimality property in the multiagent case to perform 
 unchanged. This policy is guaranteed to be a local maximum    more efficient construction of the optimal policy within our 
 since the value of the new joint policy at each iteration is non- JESP algorithm. 
 decreasing.                                                     To support such a dynamic-programming algorithm, we 
                                                               must define belief states that summarize an agent's history 
 Algorithm 1 EXHAUSTIVE-JESPQ                                  of past observations, so that they allow the agents to ignore 
                                                               the actual history of past observations, while still supporting 
  1: prev random joint policy, conv 0 
                                                               construction of the optimal policy over the possible future. 
  2: while conv n - 1 do 
                                                               In the single-agent case, a belief state that stores the distri•
  3:   for i 1 to n do 
                                                               bution, =, is a sufficient statistic, because 
  4:     fix policy of all agents except i 
                                                               the agent can compute an optimal policy based on 
  5:     policy Space list of all policies for i 
                                                               without having to consider the actual observation sequence, 
  6:     new bestPolicy(i,policySpace,prev) 
  7:     if new.value = prev.value then                           [Sondik, 1971]. 
  8:        conv conv + 1                                        In the multiagent case, an agent faces a complex but nor•
  9:     else                                                  mal single-agent POMDP if the policies of all other agents are 
 10:        prev new, conv 0                                   fixed. However, is not sufficient, because the agent 
 11:     if conv = n - 1 then                                  must also reason about the action selection of the other agents 
 12:        break                                              and hence on the observation histories of the other agents. 
 13: return new                                                Thus, at each time t, the agent i reasons about the tuple — 
                                                                                                                        is 
   The best policy cannot remain unchanged for more than       the joint observation histories of all the agents except i. By 
n — 1 iterations without convergence being reached and in      treating e\ to be the state of the agent i at time t, we can define 
the worst case, each and every joint policy is the best policy the transition function and observation function for the single 
for at least one iteration. Hence, this algorithm has the same agent POMDP for agent i as follows: 
worst case complexity as the exhaustive search for a globally 
optimal policy. However, it could do much better in practice 
as illustrated in Section 6. Although the solution found by this 
algorithm is a local optimum, it may be adequate for some 
applications. Techniques like random restarts or simulated 
annealing can be applied to perturb the solution found to see 
if it settles on a different higher value. 
   The exhaustive approach to Steps 5 and 6 of the 
Exhaustive-JESP algorithm enumerates and searches the 
entire policy space of a single agent, i. There are 

Osuch policies, and evaluating each incurs                     where is the joint policy 
                                                               for all agents except . 
a time complexity of O Thus, using the ex•
                                                                 We now define the novel multiagent belief state for an 
haustive approach incurs an overall time complexity in Steps 
                                                               agent i given the distribution over the initial state, b(s) = 

5 and 6 of: O . Since we incur this                            Pr(S1 = s): 
                                                                                                                      (4) 
complexity cost in each and every pass through the JESP 
algorithm, a faster means of performing the bestPolicy           In other words, when reasoning about an agent's policy in 
function call of Step 6 would produce a big payoff in overall  the context of other agents, we maintain a distribution over 
efficiency. We describe a dynamic programming alternative      rather than simply the current state. Figure 1 shows different 
to this exhaustive approach for doing JESP next.               belief states B1 , B2 and for agent 1 in the tiger domain. 
                                                               For instance, B2 shows probability distributions over . In 

5.2 Dynamic Programming (DP-JESP)                                 = (SL, (HR))9 (HR) is the history of agent 2's observa•
If we examine the single-agent POMDP literature for inspi•     tions while SL is the current state. Section 5.3 demonstrates 
ration, we find algorithms that exploit dynamic programming    how we can use this multiagent belief state to construct a dy•
to incrementally construct the best policy, rather than simply namic program that incrementally constructs the optimal pol•
search the entire policy space [Monahan, 1982; Cassandra et    icy for agent i. 
al., 1997; Kaelbling et a/., 1998]. These algorithms rely on 
a principle of optimality that states that each sub-policy of an 5.3 The Dynamic Programming Algorithm 
overall optimal policy must also be optimal. In other words,   Following the model of the single-agent value-iteration al•
if we have a T-step optimal policy, then, given the history    gorithm, our dynamic program centers around a value func•
over the first t steps, the portion of that policy that covers the tion over a T-step finite horizon. For readability, this sec•
last T — t steps must also be optimal over the remaining T -t  tion presents the derivation for the dynamic program in the 


708                                                                                             MULTIAGENT SYSTEMS                                                               For the updated is 
                                                              obtained using Equations 2 and 3 and Bayes Rule and is given 
                                                              as follows: 


                                                                                                                    (10) 
                                                                We treat the denominator of Equation 10 (i.e., 
             Figure 1: Trace of Tiger Scenario                Pr as a normalizing constant to bring 
                                                              the sum of the numerator over all to be 1. This result 
two-agent case; the results easily generalize to the n-agent  also enters into our computation of future expected reward 
case. Having fixed the policy of agent 2, our value function, in the second term of Equation 6. Thus, we can compute 
                                                              the agent's new belief state (and the future expected reward 
Vt{Bt), represents the expected reward that the team will re•
ceive when agent 1 follows its optimal policy from the t-th   and the overall value function, in turn) using only the agent's 
step onwards when starting with a current belief state,       current belief state and the primitive elements of our given 
We start at the end of the time horizon (i.e., t = T), and then MTDP model. Having computed the overall value function, 
work our way back to the beginning. Along the way, we con•    Vt, we can also extract a form of the optimal policy, 
struct the optimal policy by maximizing our value function    that maps observation histories into actions, as required by 
over possible action choices:                                 Equations 8 and 10. 
                                                                Algorithm 2 presents the pseudo-code for our overall dy•
                                                              namic programming algorithm. Lines 1-6 generate all of 
                                                              the belief states reachable from a given initial belief state, 
We can define the action value function, recursively:                             . Since there is a possibly unique belief 
                                                              state for every seauence of actions and observations by agent 
                                                              1, there are O reachable belief states. This 
                                                              reachability analysis uses our belief update procedure (Algo•
                                                              rithm 3), which itself has time complexity 
                                                              when invoked on a belief state at time t. Thus, the over•
The first term in equation 6 refers to the expected immediate all reachability analysis phase has a time complexity of 
reward, while the second term refers to the expected future re• O Lines 7-22 perform the heart 
ward, is the belief state updated after performing action     of our dynamic programming algorithm, which also has a 
                                                              time complexity of 0( . Lines 23-
a  and observing . In the base case, t — T, the future 
 1                                                            27 translate the resulting value function into an agent policy 
reward is 0, leaving us with: 
                                                              defined over observation sequences, as required by our al•
                                                       (7)    gorithm (i.e., the argument). This last phase has a lower 
                                                              time and space complexity, O , than our 
The calculation of expected immediate reward breaks down      other two phases, since it considers only optimal actions for 
as follows:                                                   agent 1. Thus, the overall time complexity of our algorithm 
                                                              is O . The space complexity of the 
                                                              resulting value function and policy is essentially the product 
                                                              of the number of reachable belief states and the size of our 
Thus, we can compute the immediate reward using only the      belief state representation: O  
agent's current belief state and the primitive elements of our 5.4 Piecewise Linearity and Convexity of Value 
given MTDP model (See Section 2). 
                                                                    Function 
  Computation of the expected future reward (the second 
term in Equation 6) depends on our ability to update agent    Algorithm 2 computes a value function over only those be•
                                                              lief states that are reachable from a given initial belief state, 
1 's belief state from B1t to in light of the new observa•
                                                              which is a subset of all possible probability distributions over 
tion, For example, in Figure 1, the belief state B      is 
                                                      1       S  and To use dynamic programming over the entire set, 
updated to on performing action a     and receiving obser•     t
                                     1                        we must show that our chosen value function is piecewise 
vation . We now derive an algorithm for performing such 
                                                              linear and convex (PWLC). Each agent is faced with solving 
an update, as well as computing the remaining 
                                                              a single agent POMDP is the policies of all other agents is 
term from Equation 6. The initial belief state based on the 
                                                              fixed as shown in Section 5.2. Sondik [1971] showed that the 
distribution over initial state, 6, is: 
                                                              value function for a single agent POMDP is PWLC. Hence 
                                                       (9)    the value function in Equation 5 is PWLC. Thus, in addition 


MULTIAGENT SYSTEMS                                                                                                  709 