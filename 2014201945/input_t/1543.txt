               Word    Sense   Disambiguation       with   Distribution    Estimation

                                   Yee Seng  Chan  and  Hwee  Tou  Ng
                                    Department  of Computer  Science
                                    National University of Singapore
                                  3 Science Drive 2, Singapore 117543
                                   {chanys,  nght}@comp.nus.edu.sg

                    Abstract                            The work of these researchers showed that the different
                                                      sense distributions across domains have an important effect
    A  word  sense disambiguation (WSD) system        on WSD  accuracy. Therefore, estimation of the target do-
    trained on one domain and applied to a different  main’s sense distribution will be crucial, in order to build
    domain will show a decrease in performance. One   WSD  systems that are portable across different domains. A
    major reason is the different sense distributions partial solution to this would be to determine the predomi-
    between different domains. This paper presents    nant, or most frequently occurring sense of each word in a
    novel application of two distribution estimation al- corpus. Research has shown that the baseline heuristic of se-
    gorithms to provide estimates of the sense distribu- lecting the most frequently occurring sense of a word gives
    tion of the new domain data set. Even though our  high WSD accuracy. This is due to the often skewed distribu-
    training examples are automatically gathered from tion of word senses. McCarthy et al. [2004b] addressed this
    parallel corpora, the sense distributions estimated when they presented a method to automatically rank word
    are good enough to achieve a relative improvement senses in a corpus, in order to determine the predominant
    of 56% when incorporated into our WSD system.     sense of each word. They obtained good results when the
                                                      method is applied on the SENSEVAL-2 English all-words
1  Introduction                                       task [Palmer et al., 2001]. The same method is used in a re-
                                                                [                   ]
Many words have multiple meanings. The process of iden- lated work McCarthy et al., 2004a to identify infrequently
tifying the sense of a word in a particular context is known occurring word senses.
as word sense disambiguation (WSD). WSD is an important In the machine learning literature, algorithms to estimate
task of natural language processing (NLP), and is important class a priori probabilities have been developed. In this pa-
for applications such as machine translation and information per, we present novel application of two distribution estima-
retrieval. Past research has shown that a good approach to tion algorithms to provide estimates of the sense distribution,
WSD  is the use of supervised machine learning. With this or a priori probabilities of senses, and show that they are ef-
approach, a large text corpus in which each ambiguous word fective in improving the WSD accuracy of systems trained
has been annotated with the correct sense has to be collected and applied on different domains. Note that determining the
to serve as training data.                            predominant sense is a special case of trying to estimate the
  This corpus-based approach, however, raises the problem complete sense distribution. To the best of our knowledge,
of domain dependence faced by these supervised WSD sys- these algorithms have not been used in NLP or for WSD.
tems. A system trained on data from one domain (for exam- We recognize that a WSD system should be scalable. This
ple, sports), will show a decrease in performance when ap- is a problem faced by current supervised learning systems,
plied to a different domain (for example, economics). This as they usually relied on manually annotated data for train-
issue of domain difference affecting WSD performance had ing. To tackle this problem, our prior work[Ng et al., 2003]
been brought up by various researchers. Escudero et al. exploited parallel texts for WSD. Encouraging results were
[2000] showed that training a WSD system in one domain obtained in our evaluation on the nouns of SENSEVAL-2 En-
and applying it to another could lead to a drop of 12% to glish lexical sample task [Kilgarriff, 2001], which used the
19% in WSD  accuracy. It was shown that one of the rea- WordNet 1.7 sense inventory [Miller, 1990]. Note that the
sons is due to different distributions of senses across domains, usage of parallel texts as training data represents a natural
as a 9% improvement in accuracy was observed when they domain difference with the test data of SENSEVAL-2 English
sense-balanced the instances from the different domains. Re- lexical sample task, of which 91% is drawn from the British
cently, Agirre and Martinez [2004] conducted experiments National Corpus (BNC). In this paper, we chose to draw train-
with training data for WSD which were automatically gath- ing data for our experiments from parallel texts, and similarly
ered from the Internet. They reported a potential improve- base our evaluation on the nouns of SENSEVAL-2 English
ment of 14% in accuracy if they have access to the sense dis- lexical sample task.
tribution of their test data.                           This paper is structured as follows. In the next section,            Parallel corpora                        Size of English texts Size of Chinese texts
                                                  (in million words (MB)) (in million characters (MB))
            Hong Kong Hansards                         39.9 (223.2)          35.4 (146.8)
            Hong Kong News                             16.8 (96.4)           15.3 (67.6)
            Hong Kong Laws                              9.9 (53.7)            9.2 (37.5)
            Sinorama                                    3.8 (20.5)            3.3 (13.5)
            Xinhua News                                 2.1 (11.9)            2.1 (8.9)
            English translation of Chinese Treebank     0.1 (0.7)             0.1 (0.4)
            Total                                      72.6 (406.4)          65.4 (274.7)

                                 Table 1: Size of English-Chinese parallel corpora

we discuss the process of gathering training data from par- 3 Distribution Estimation
allel texts. In section 3, we describe the two distribution es- To estimate the sense distribution, or a priori probabilities of
timation algorithms used. We also give a brief description the different senses in a new data set, we made use of a con-
of the predominant sense method presented by [McCarthy fusion matrix algorithm [Vucetic and Obradovic, 2001] and
et al., 2004b]. In section 4, we evaluate the effectiveness an EM based algorithm [Saerens et al., 2002], which we de-
of the two algorithms and the predominant sense method in scribe in this section. A brief description of the predominant
improving WSD accuracy. Evaluation results on the nouns sense method [McCarthy et al., 2004b] is also given, before
in SENSEVAL-2  English lexical sample task are presented. we discuss how to make use of the estimated a priori proba-
Discussions are made in section 5, before we conclude in sec- bilities to improve classiﬁcation accuracy.
tion 6.
                                                      3.1  Confusion  Matrix
2  Training  Data  from  Parallel Texts               Let us assume that from a set of labeled data SL, a classiﬁer
                                                      is used to compute the conditional probabilities pL(ωj |ωi) ,
In this section, we brieﬂy describe the parallel texts used in which is an estimate of the probability of classifying an in-
our experiments, and the process of gathering training data stance as class ω , when in fact it belongs to class ω . Then,
                                                                    j                          b  i
from them.                                            one can apply this classiﬁer with known conditional prob-
                                                      abilities pL(ωj |ωi) to a set of unlabeled data SU to obtain
2.1  Parallel Text Alignment                          its predictions. From these predictions, one can estimate the
                                                      probability of predicting class ω on S , which we will de-
Table 1 lists the 6 English-Chinese parallel corpora (available b                j     U
from Linguistic Data Consortium) used in our experiments. note as q(ωj ) . The a priori probabilities p(ωi) on SU are then
The sentences of the 6 corpora were already pre-aligned, ei- estimated by solving the following equation:
ther manually or automatically, when they were prepared. Af- b      n                   b
ter ensuring the corpora were sentence aligned, we tokenized q(ωj ) = pL(ωj |ωi)p(ωi),   j = 1, . . . , n (1)
                                                                   i=1
the English texts, and perform word segmentation on the Chi-       X
nese texts. Next, word alignment was done on the parallel where bn represents bthe numberb of classes. Equation (1) can
corpora using the GIZA++ software [Och and Ney, 2000]. be represented in matrix form as q = P · p, from which the a
                                                      priori probabilities on SU , represented by p , can be estimated
                                                      by solving:
2.2  Target Translations Selection                                              −
                                                                          p = P  1 · q                (2)
We took the same approach as described in our previous work
                                                      To obtain estimates of P and q, bootstrap sampling [Efron
[Ng et al., 2003] to select some possible Chinese translations
                                                      and Tibshirani, 1993] is employed. We will ﬁrst describe the
for each sense of an English word. For instance, WordNet
                                                      basic idea of bootstrap before giving the bootstrap algorithm.
1.7 lists 5 senses for the noun nature. Senses will be lumped
                                                        In bootstrap sampling, given an original sample X with n
together if they are translated in the same way in Chinese.                    ∗
                                                      examples, bootstrap sample X is obtained by randomly sam-
For example, sense 2 and 3 of nature are lumped together
                                                      pling n examples from X with replacement. To estimate the
as they are both translated as “Ll” in Chinese. Then,
                                                      conditional probabilities pL(ωj |ωi), we need to split SL into
from the word alignment output of GIZA++, we select those                                ∗
                                                      Strain and Stest. Further, let us deﬁne n (ωj , ωi) as the
occurrences of the noun nature which have been aligned to                                test∗
                                                      number of examples in a bootstrap sample Stest predicted to
one of the Chinese translations chosen. The English side of                b                        ∗
                                                      be of class ωj when true class is ωi. Also, we deﬁne n (ωj )
these occurrences will then serve as training data for the noun                                ∗    U
                                                      as the number of examples in a bootstrap sample S predicted
nature, as they are considered to have been disambiguated                                      U
                                                      to be of class ω . We now give the bootstrap algorithm below:
and “sense-tagged” by the appropriate Chinese translations.       j
                                                        Given Stest, SU , and a classiﬁer, repeat the following for B
  The time needed to perform manual target translations of iterations (In our experiments, we set B to 200):
word senses is relatively short, compared to the effort needed
to manually sense annotate training examples. Also, this step • Generate a bootstrap sample from ntest examples of
                                                          S    and calculate:
could potentially be automated with the use of an English- test         ∗
                                                           ∗           ntest(ωj ,ωi)
Chinese dictionary.                                       p  (ωj|ωi) =    ∗           for i, j = 1, . . . , n
                                                           L          Pj ntest(ωj ,ωi)

                                                          b  • Generate a bootstrap sample from nU examples of SU where (4) represents the expectation E-step, (5) represents

    and calculate:∗                                   the maximization M-step, and N represents the number of in-
     ∗       n  ω
    q (ω ) =  U ( j ) for j = 1, . . . , n            stances in S . Note that in (4), the probabilities p (ω |x )
        j     nU                                                U                                L  i  k
                                                      and pL(ωi) will stay the same through the iteration steps s
                      ∗   (b)                         for each particular instance xk and class ωi . Observe that
  • Useb (2) to calculate p (ωi) for i = 1, . . . , n                                           b
                                                      for (4), the new a posteriori probabilities p(s)(ω |x ) at step
After B  iterations, estimate the a priori probabilities  b                                   i  k
                                                      s are simply the a posteriori probabilities in the conditions of
        1   B   ∗   (bb)
p(ωi) = B   b=1 p (ωi)                                the labeled data, pL(ωi|xk), weighted by the ratio of the new
                                                             (s)                         b
         P                                            priors p (ωi) to the old priors pL(ωi). The denominator in
3.2b EM  based bAlgorithm                             (4) is simply a normalizingb factor.
                                                                                                (s)
Once again, let us assume we train a classiﬁer on a set of At eachb iteration step s, bothbthe a posteriori p (ωi|xk)
                                                                             (s)
labeled data SL with n classes. Further, suppose we have a and a priori probabilities p (ωi) are re-estimated sequen-
set of N independent instances, or independent realizations of
                                                      tially for each new instance xk and each class ωbi, until the
the stochastic variable x, (x , x , . . . , xN ) from a new data                           (s)
                        1  2                          convergence of the estimatedb probabilities p (ωi). This it-
set. The likelihood of these N instances can be deﬁned as: erative procedure will increase the likelihood of (3) at each
                          N                           step.
                                                                                          b
    L(x1, x2, . . . , xN ) = p(xk)
                         k
                         Y=1
                          N    n                      3.3  Predominant  Sense
                     =           p(xk, ωi)
                         k   "i=1        #
                         Y=1  X
                          N    n                      A method of automatically ranking WordNet senses to deter-
                     =           p(xk|ωi)p(ωi)  (3)   mine the predominant, or most frequent sense, of a noun in
                         k=1 "i=1            #        the BNC corpus is presented in [McCarthy et al., 2004b]. A
                         Y    X
If we assume the generation of the instances within the thesaurus is ﬁrst acquired from the parsed 90 million words of
classes, and thus the within-class densities p(xk|ωi) (i.e., written English from the BNC corpus to provide the k nearest
the probabilities of observing xk given the class ωi) do not neighbors to each target noun, along with the distributional
change from the training set SL to the new data set, we can similarity score (dss) between the target noun and its neigh-
                                                                                      [                 ]
deﬁne p(xk|ωi) = pL(xk|ωi) . To determine the a priori bor. The WordNet similarity package Pedersen et al., 2004
probability estimates p(ωi) of the new data set that will max- is then used to give a WordNet similarity measure (wnss),
imize the likelihood of (3) with respect to p(ωi) , we can ap- which is used to weight the contribution that each neighbor
ply the iterative procedure of the EM algorithm. In effect, makes to the various senses of the target noun. Through a
                  b
through maximizing the likelihood of (3), we obtain the a pri- combination of dss and wnss, a prevalence score for each
ori probability estimates as a by-product.            sense of the target noun is calculated, from which the pre-
  Before we show the steps of the EM algorithm, it will be dominant sense of the noun in BNC is determined.
helpful to deﬁne some notations. When we apply the classi- Though the focus of the method is to determine the pre-
ﬁer trained on SL on an instance xk drawn from the new data dominant sense, we could obtain an estimated sense distribu-
set SU , we get pL(ωi|xk) , which we deﬁne as the probabil- tion by normalizing the prevalence score of each sense. As
ity of instance xk being classiﬁed as class ωi by the classiﬁer noted in section 1, 91% of the test examples of SENSEVAL-
trained on S . Further, let us deﬁne p (ω ) as the a priori
          L  b                   L  i                 2 English lexical sample task were drawn from BNC. Thus,
probabilities of class ωi in SL , which can be estimated by the it is reasonable to expect the sense distribution estimated by
                                         (s)
class frequency of ωi in SL. Also, we willb deﬁne p (ωi) and this predominant sense method to be applicable to the test ex-
 (s)
p  (ωi|xk) as estimates of the new a priori and a posteriori amples of SENSEVAL-2 English lexical sample task. We im-
probabilities at step s of the iterative EM procedure.b Assum- plemented this method in order to evaluate its effectiveness in
               (0)
ingb we initialize p (ωi) by the a priori probabilities of the improving WSD accuracy. Our implementation achieved ac-
classes in the labeled data:                          curacies close to those reported by [McCarthy et al., 2004b].
                                                      McCarthy et al. [2004b] reported a jcn measure predominant
              b  p(0)(ω ) = p (ω )
                       i    L  i                      sense accuracy of 54% on the Semcor corpus, while we mea-
then for each new instance xk in SU , and each class ωi, the sured 53.3% using our implementation. On the SENSEVAL-
EM algorithm providesb the followingb iterative steps: 2 English all-words task, a 64% precision and 63% recall

                                  (s)                 was reported. Our implementation gave 66.2% precision and
                                 p  (ωi)
                        pL(ωi|xk) b
         (s)                     pL(ωi)               63.4% recall. The differences could be due to some minor
                                 b
        p  (ωi|xk) =                (s)         (4)
                       n            p (ωj )           processing steps which were not described in detail in [Mc-
                          pL(ωj |xk) b
                       j=1          pL(ωj )                             ]
                        b           b                 Carthy et al., 2004b . Finally, note that this predominant
        b                  N                          sense method is only effective on a very large text corpus,
                     P  1
           p(s+1)(ω ) =   b  p(s)(ω |x )        (5)   as the thesaurus can only be effectively generated from a very
                   i   N          i  k
                          k                           large corpus.
                          X=1
           b                 b        Maximum        L     EM sample    TRUE − L       EM  − L       ConfM − L    Predominant − L
        number of par-
        allel examples
        per noun
                                        adjust sample adjust sample  adjust sample  adjust sample
        150           68.29    69.35    1.76    3.70   0.81   1.06    0.92   1.04   0.84    0.59
        200           69.29    71.33    1.58    3.75   0.88   2.04    0.90   1.20   0.59    0.29
        300           70.29    72.07    1.55    3.69   0.71   1.78    0.76   0.74   0.57    0.62
        500           72.21    73.57    1.37    2.40   0.77   1.36    0.79   0.95   0.35    0.09

                               Table 2: 5 trials micro-averaged scores for 22 SE2 nouns

           Maximum  number of parallel examples per noun EM − L    ConfM − L    Predominant − L
                                                  adjust sample  adjust sample  adjust sample
           150                                     46.02  28.65  52.27   28.11  47.73   15.95
           200                                     55.70  54.40  56.96   32.00  37.34   7.73
           300                                     45.81  48.24  49.03   20.05  36.77   16.80
           500                                     56.20  56.67  57.66   39.58  25.55   3.75

            Table 3: Relative improvement percentage from using the a priori probabilities estimated by the 3 methods

3.4  Improving  Classiﬁcation Based on A Priori         Saerens et al. [2002] reported that increasing the training
     Estimates                                        set size results in an improvement in the a priori estimation.
If a classiﬁer was trained to estimate posterior class probabil- To investigate this issue, we gradually increased the maxi-
                                                      mum  number of examples per noun selected from the parallel
ities pL(ωi|xk) when presented with a new instance xk from
S , it can be directly adjusted according to estimated class texts. For each condition, we sampled training examples from
 U                                                    the parallel text using the natural distribution of the parallel
probabilities p(ω ) on S .
    b         i     U                                 text, up to the maximum allowed number of examples per
  Denoting predictions of the classiﬁer as p (ω |x ), a priori
                                    L  i  k           noun. We noted the WSD accuracy achieved by the classiﬁer
probability of class ω from S as p (ω ), estimated a priori
           b      i      L    L  i                    without any adjustment, in the column labeled L in table 2. To
probability of class ω from S as p(ω ), adjusted predictions
                 i      U      i   b                  provide a basis for comparison, we adjusted the classiﬁer pos-
p     (ω |x ) can be calculated as:
 adjust i k                  b                        teriori probability output by the true sense distribution of the
                            b      p(ωi)              test data, using equation (6). The increase in WSD accuracy
                          pL(ωi|xk) b
b                                  pL(ωi)
        padjust(ωi|xk) =           b            (6)   thus obtained is listed in the column adjust under TRUE−L.
                                     p(ωj )
                           pL(ωj |xk) b               For example, if we knew exactly the true sense distribution of
                          j         pL(ωj )
                          b         b                 the test data, we would have achieved an accuracy of 68.29%
  Note thebsimilarity of (6) withP (4).               + 1.76% = 70.05%, when trained on a maximum of 150 ex-
                           b
                                                      amples per noun.
4  Evaluation                                           Besides adjusting the classiﬁer posteriori probability out-
In our experiments, we implemented the supervised WSD ap- put, one could also re-sample training examples from the par-
proach of [Lee and Ng, 2002], which was reported to achieve allel texts according to the true sense distribution. The in-
state-of-the-art accuracy. The knowledge sources used in- crease in WSD accuracy obtained using this approach is listed
clude parts-of-speech, surrounding words, and local colloca- in the column sample under TRUE−L. Note that scores listed
tions. We use the naive Bayes algorithm as our classiﬁer, under TRUE−L indicate the upper-bound accuracy achiev-
which was reported to achieve good WSD accuracy and it is able had we known the true sense distribution of the test data.
fast to train and test with the naive Bayes classiﬁer. Also, Increase in WSD accuracy by adjusting the classiﬁer out-
the naive Bayes classiﬁer outputs posteriori probabilities in put using the sense distribution estimated by the EM algo-
its classiﬁcations, which can be used directly to adjust the rithm is listed in column adjust under EM−L. Correspond-
predictions based on (6).                             ing increase by re-sampling training examples from the par-
                                                      allel texts according to the sense distribution estimated by
4.1  Results                                          the EM algorithm is listed in column sample under EM−L.
We used the approach outlined in section 2 to obtain training Similar increases by using sense distribution estimated by
examples from parallel texts. Two senses s1 and s2 of a noun the confusion matrix algorithm, and the predominant sense
are lumped together if they are translated into the same Chi- method, are listed under the columns adjust and sample, un-
nese word. Although there are 29 nouns in the SENSEVAL-2 der ConfM−L and Predominant−L. The corresponding rel-
English lexical sample task, the senses of 7 nouns are lumped ative improvement of the various accuracy improvements
into one sense (i.e., all senses of each of these 7 nouns are (comparing against using the true sense distribution) achieved
translated into one Chinese word). Thus, we limit our evalu- by the two algorithms and the predominant sense method is
ation to the remaining 22 nouns.                      given in Table 3. As an example, for 150 parallel examples                                               System Performance
             100

              90
                    79.89
                       77.62
                          77.15 76.94 76.60
                                  75.70
                                     75.41 75.10 75.09 74.94 74.72

              80                                  74.00 74.00 73.57 73.55
                                                             71.32 71.00
                                                                   70.45
                                                                     64.10
              70                                                                63.86

              60
                                                                                   47.15
              50
                                                                        37.73
                                                                                      35.84
                                                                                         35.09
          WSD  accuracy

              40                                                                           34.31
                                                                           32.61


              30                                                              24.10

              20

              10

               0

Figure 1: Comparison of Parallel-EM’s performance against SENSEVAL-2 participating systems. The single shaded bar represents
Parallel-EM, the empty white bars represent the supervised systems, and the pattern ﬁlled bars represent the unsupervised systems.

per noun, EM adjust gives an improvement of 0.81%, against column is obtained by adding the ﬁgures from the columns L
an improvement of 1.76% using TRUE adjust. Thus, relative and EM−L sample).
improvement is 0.81/1.76 = 46.02%.
  The results show that the confusion matrix and EM algo- 4.2 Comparison with SE2 Systems
rithms are effective in improving WSD accuracy. Also, the We compare our WSD   accuracy achieved with the
two algorithms are most effective when a large training data SENSEVAL-2 English lexical sample task participating sys-
set (which is 500 examples per noun in our experiments) is tems. We performed sense lumping on the publicly available
used. The results also indicate the potential of parallel text predictions of the SENSEVAL-2 participating systems. The
scaling up to achieve better WSD accuracy as more training WSD accuracy of each system is then calculated over the set
examples are used, as shown by the increasing accuracy ﬁg- of 22 nouns. In the SENSEVAL-2 English lexical sample
ures under the L column of Table 2. This is a phenomenon not task, there were 21 supervised and 51 unsupervised systems.
explored in [Ng et al., 2003], which used the same number of Figure 1 shows the relative ranking of our system (which we
parallel text examples as the ofﬁcial SENSEVAL-2 training named Parallel-EM) against the various SE2 systems.
data. At the same time, however, this caused a progressive re- We note that our system achieved better performance than
duction in the absolute amount of possible WSD accuracy im- all the unsupervised systems, and would have ranked at the
provement, as indicated by the reduction under the columns 64% percentile among the list of 22 (including ours) super-
adjust and sample of TRUE−L in Table 2 when more parallel vised systems.
texts examples were used. Our experiments also indicate that
application of the two algorithms achieved higher increases 5 Discussion
in WSD  accuracy when compared against the predominant
sense method, which has the primary aim of determining the The score of 73.57% achieved by Parallel-EM is competi-
predominant sense of a word in a corpus.              tive, considering that the SE2 supervised systems are trained
  We note that re-sampling the training examples gives bet- with the manually annotated ofﬁcial data, while our system
ter WSD improvements than merely adjusting the posteriori was trained with examples automatically extracted from par-
probabilities output by the classiﬁer. The number of training allel texts. Moreover, the following factors will also have to
examples for a sense will affect the accuracy of the classiﬁer be considered.
trained for that sense. Thus, if one has a reliable estimate Ofﬁcial Training Data has Similar Sense Distribution as
of the sense distribution, it might be more beneﬁcial to re- Test Data
sample the training examples. The results show that sampling
                                                      This is an advantage that the participating SENSEVAL-2 su-
a maximum of 500 examples per noun from the parallel texts
                                                      pervised systems enjoy. Note that the results in Table 2 show
according to the distribution estimated by the EM algorithm
                                                      that if we sample according to the true distribution with a
gives our best achievable WSD accuracy of 73.57% (72.21%
                                                      maximum  of 500 examples per noun, we could achieve a
+ 1.36%). In the column EM sample of Table 2, we show the
                                                      score of 74.61%.
WSD  accuracy resulting from sampling parallel text exam-
ples according to the EM estimated sense distribution, with 1There were 4 systems from IIT (2 were resubmissions), but sys-
different maximum number of examples per noun (i.e., this tem answers were only available for 2 of them.