      Location-Based Activity Recognition using Relational Markov Networks

                             Lin Liao   and  Dieter Fox  and  Henry Kautz
                            Department of Computer Science & Engineering
                                       University of Washington
                                           Seattle, WA  98195


                    Abstract                          extracted from geographic databases, including information
                                                      about the kinds of businesses in various locations; (3) sequen-
    In this paper we deﬁne a general framework for ac- tial information such as which activity follows which activ-
    tivity recognition by building upon and extending ity; and (4) global constraints such as the number of different
    Relational Markov Networks. Using the example     homes or workplaces. Additionally, it uses data collected by
    of activity recognition from location data, we show other users so as to improve the classiﬁcation of a speciﬁc
    that our model can represent a variety of features user’s activities. All these constraints are soft: for example,
    including temporal information such as time of day, on some days a person may do something unusual, such as go
    spatial information extracted from geographic data- to a movie in the middle of a workday. Furthermore, the norm
    bases, and global constraints such as the number  for some individuals may be different than our general com-
    of homes or workplaces of a person. We develop    monsense prior: for example, some people work two jobs, or
    an efﬁcient inference and learning technique based shuttle between two different homes. It is necessary to use a
    on MCMC. Using GPS location data collected by     rich and ﬂexible language to robustly integrate such a wide
    multiple people we show that the technique can ac- variety of both local and global probabilistic constraints.
    curately label a person’s activity locations. Further- Our system builds upon previous work on extracting
    more, we show that it is possible to learn good mod- places from traces of users’ movements, gathered by GPS or
    els from less data by using priors extracted from other localization technologies [Ashbrook and Starner, 2003;
    other people’s data.                              Hariharan and Toyama, 2004; Liao et al., 2004]. Our work
                                                      goes beyond theirs in that our system also recognizes the
1  Introduction                                       activities associated with the places. Moreover, previous
Activity recognition and context-aware computing are gain- approaches to modeling personal movements and place
ing increasing interest in the AI and ubiquitous computing patterns require a large amount of training data from each
communities. Most existing systems have been focused on user, and cannot be generalized to new places or new users.
relatively low level activities within small environments or By contrast, our relational approach requires less individual
during short periods of time. In this paper, we describe a training data by leveraging data collected by others. In
system that can recognize high level activities (e.g., work- summary, the contributions of this paper are:
ing, shopping, and dining out) over many weeks. Our system 1. A general framework for sensor-based activity
uses data from a wearable GPS location sensor, and is able to recognition based on Relational Markov Networks
identify a user’s signiﬁcant places, and learn to discriminate (RMNs) [Taskar et al., 2002], which are both highly
between the activities performed at these locations — includ- expressive and well-suited for discriminative learning;
ing novel locations. Such activity information can be used in 2. An extension of RMNs to incorporate complex, global
many applications. For example, it could be used to automat- features using aggregations and label-speciﬁc cliques;
ically instruct a user’s cell phone not to ring when dining at a 3. Efﬁcient Markov-chain Monte-Carlo (MCMC) algo-
restaurant, or it could support home rehabilitation of people rithms for inference and learning in extended RMNs,
                                 [                ]
suffering from traumatic brain injuries Salazar et al., 2000 and in particular, an MCMC algorithm to simultaneously
by providing automatic activity monitoring. Beyond estimat- evaluate a likelihood function and its gradient;
ing high-level activity categories, our system can be expanded
to incorporate additional sensor information, thereby recog- 4. Positive experimental results on real data from multiple
nizing ﬁne-grained indoor household tasks, such as those de- subjects, including evidence that we can improve accu-
scribed in [Philipose et al., 2004].                      racy by extracting priors from others’ data.
  Because behavior patterns can be highly variable, a reliable This paper is organized as follows. We will introduce our
discrimination between activities must take several sources relational activity model in Section 2. Inference and learning
of evidence into account. Our system considers (1) tempo- will be discussed in Section 3, followed by experimental eval-
ral information such as time of day; (2) spatial information uations. Conclusions and future work are given in Section 5.2  The Relational Activity Model                      plate must share the same weights wC . The resulting cliques
                                                      factorize the conditional distribution as
In this section we ﬁrst discuss RMNs and our extensions.
Then we show how to use them for modeling activities.                   1   Y    Y
                                                          p(y | x) =                 φ  (v )          (1)
                                                                       Z(x)           C   C
2.1  Relational Markov Networks                                             C∈C vC ∈C
                                                                        1   Y    Y         T
RMNs are extensions of Conditional Random Fields (CRFs),           =                 exp{w   · fC (vC )} (2)
                                                                       Z(x)                C
which are undirected graphical models that were developed                   C∈C vC ∈C
for labeling sequence data [Lafferty et al., 2001]. CRFs are            1
                                                                   =        exp{wT · f},              (3)
discriminative models that have been shown to out-perform              Z(x)
generative approaches such as HMMs and Markov random
ﬁelds in areas such as natural language processing [Lafferty where the normalizing partition function Z(x) =
                                                      P   Q     Q           0
et al., 2001] and computer vision [Kumar and Hebert, 2003]. 0      0   φC (v ).  (3) follows by moving the
                                                        y   C∈C   vC ∈C     C
RMNs extend CRFs by providing a relational language for products into the exponent and combining all summations
describing clique structures and enforcing parameter sharing into w and f.
at the template level. Thereby RMNs are an extremely ﬂex-
ible and concise framework for deﬁning features that can be 2.2 Relational Activity Models
used in the activity recognition context.             We will now describe our relational activity models. Even
  An RMN consists of three parts: a schema E for the do- though we illustrate the concepts using the example of
main, a set of relational clique templates C, and correspond- location-based activity recognition, our model is very ﬂexible
ing potentials Φ. The schema E speciﬁes the set of classes and can be applied to a variety of activity recognition tasks.
(i.e., entity types) and attributes in each class. An attribute The schema for activity recognition based on temporal and
could be a content attribute, a label attribute, or a reference spatial patterns is shown in Fig. 1(a). It includes three classes:
attribute that speciﬁes reference relation among the classes. Activity, Place, and Transition.
An instantiation I of a schema speciﬁes the set of entities Activity: Activity is the central class in the domain. Its at-
for each class and the values of all attributes for each entity. tribute Label is the only hidden variable. The set of possible
In our context an instantiation consists of the sequence of all labels in our experiments is {’AtHome’, ’AtWork’, ’Shop-
signiﬁcant locations visited by a user along with the temporal ping’, ’DiningOut’, ’Visiting’, ’Others’}. Attribute Id serves
and spatial attributes.                               as the primary key. The class also contains temporal infor-
  A relational clique template C ∈ C is similar to a relational mation associated with an activity, such as TimeOfDay, Day-
database query (e.g., SQL) in that it selects tuples from an in- OfWeek, and Duration, whose values are discretized when
stantiation I; the query result is denoted as C(I). We extend necessary. Finally, Place is a reference attribute that points
the deﬁnition of such templates in two ways. First, we allow to a Place entity where the activity has been performed.
a template to select aggregations of tuples. For example, we Place: The class Place includes two boolean attributes: Near-
can group tuples and deﬁne potentials over counts or other Restaurant and NearStore, which indicate whether there are
statistics of the groups. Second, we introduce label-speciﬁc restaurants or stores nearby.
cliques, whose structures depend on values of the labels. For Transition: Transition captures temporal succession relation-
example, our model can construct a clique over all activities ship among activities. The reference attributes From and To
labeled as “AtHome.” Because labels are hidden during in- refer to a pair of consecutive activities.
ference, such cliques potentially involve all the labels. Label- Based on the schema, we deﬁne the following relational
speciﬁc cliques can be speciﬁed by allowing label attributes clique templates. Each of them takes into account a number
to be used in the “Where” clause of an SQL query.     of discriminative features.
  Each clique template C is associated with a potential func-
tion φC (vC ) that maps a tuple (values of variables or aggre- 1. Temporal patterns: Different activities often have differ-
gations) to a non-negative real number. Using a log-linear ent temporal patterns, such as their duration or time of
combination of feature functions, we get the following repre- day. Such local patterns are modeled by clique templates
                        T                                 that connect each attribute with the activity label.
sentation: φC (vC ) = exp{wC ·fC (vC )}, where fC () deﬁnes
                        T                               2. Geographic evidence: Information about the types of
a feature vector for C and wC is the transpose of the corre-
sponding weight vector. For instance, a feature could be the businesses close to a location can be extremely useful
number of different homes deﬁned using aggregations.      to determine a user’s activity. Such information can be
  For a speciﬁc instantiation I, an RMN deﬁnes a condi-   extracted from geographic databases, such as Microsoft
tional distribution p(y|x) over labels y given the observed MapPoint [Hariharan et al., 2005] used in our experi-
attributes x. To compute such a conditional distribution, the ments. Since location information in such databases is
RMN generates an unrolled Markov network, in which the    not accurate enough, we consider such information by
nodes correspond to the content attributes and the label at- checking whether, for example, a restaurant is within a
tributes. The cliques of the unrolled network are built by ap- certain range from the location.
plying each clique template C ∈ C to the instantiation, which 3. Transition relations: The ﬁrst-order transitions between
can result in several cliques per template (see Fig. 1(b) for activities can also be informative. For example, stay-
an example). All cliques that originate from the same tem- ing at home followed by being at work is very common                  Activity                                                        TimeOfDay,DayOfWeek,Duration
                     Id                                                           NearRestaurant,NearStore
                     Label                                                        Label
  Place              TimeOfDay   Transition
     Id              DayOfWeek      From
     NearRestaurant    Duration     To                       1     2       3      4     5       6
     NearStore
                     Place                  (a)                                                       (b)
Figure 1: (a) The schema of the relational activity model. Dashed lines indicate reference relations among classes. (b) An example of an
unrolled Markov network with six activity locations. Solid straight lines indicate cliques generated by the templates of temporal, geographic,
and transition features; bold solid curves represent spatial constraints (activity 1 and 4 are associated with the same place and so are 2 and 5);
dashed curves stand for global features, which generate label-speciﬁc cliques (e.g., activity 1 and 4 are both labeled ’AtHome’).
    while dining out immediately followed by another din- by using MCMC for inference [Gilks et al., 1996]. In a nut-
    ing out is rare. The SQL query for this clique template shell, whenever the label of an object is changed during sam-
    is:                                               pling, we determine all cliques that could be affected by this
    SELECT a1.Label, a2.Label                         change and re-compute their potentials.
    FROM Activity a1, Activity a2, Transition t         We ﬁrst implemented MCMC using basic Gibbs sampling.
    WHERE t.From=a1.Id AND t.To=a2.Id                 Unfortunately, this technique performs poorly in our model
                                                      because of the strong dependencies among labels. To make
 4. Spatial constraints: Activities at the same place are often
                                                      MCMC mix faster, we ﬁrst make an additional spatial con-
    similar. In other words, the number of different types of
                                                      straint that all activities occurring in the same place must have
    activities in a place is often limited. We can express such
                                                      the same label (the relaxation of this constraint will be ad-
    a constraint using an aggregation function Count():
                                                      dressed in future work). This hard constraint allows us to
    SELECT COUNT(DISTINCT Label)                      put all activities occurring in the same place into a so-called
    FROM Activity                                     block. We then develop a mixture of two transition kernels
    GROUP BY Place                                    that converges to the correct posterior.
 5. Global features: Such features model global, soft con- The ﬁrst kernel is a block Gibbs sampler. At each step we
    straints on activities of a person. The number of differ- update the labels in a block simultaneously by sampling from
    ent home locations is an example of global constraints. the full conditional distribution
    Such a constraint is modeled by a clique template that                           T
    selects all places labeled as home and returns how many P (yk | y−k, x, w) ∝ exp{w · f(x, y−k ∪ yk)} (4)
    of them are different:
                                                      where k is the index of the block, yk is the label of block k,
    SELECT COUNT(DISTINCT Place)                      y−k are the labels for blocks other than k. The second kernel
    FROM Activity                                     is a Metropolis-Hasting (MH) sampler. To update the label
    WHERE Label=’AtHome’                              for block k, the MH sampler randomly picks a block j and
    Note that the label variable appears in the “Where” proposes to exchange label yk and yj. The acceptance rate of
    clause, so this is an example of label-speciﬁc clique. In the proposal follows as
    a different activity recognition context, global features
                                                                               exp{wT  · f(x, y0)}
    could also model information such as “the number of     a(y, y0) =   min  1,                      (5)
    times a person has lunch per day.”                                           exp{wT · f(x, y)}
                                                                   0
  In the ﬁrst three templates, the feature functions fC () are where y and y are the labels before and after the exchange,
just indicator functions that return binary values. They can respectively.
also return numbers, such as in the last two templates. The numbers of different homes and workplaces are stored
                                                      in the chains as global variables. This allows us to compute
3  Inference and Learning                             the global features locally in both kernels: in the Gibbs kernel
                                                      we increase or decrease the numbers depending on the labels
3.1  Labeling Activities                              of the given block and in the MH kernel the numbers remain
                                                      intact. At each time step, we choose the Gibbs sampler with
In our application, the task of inference is to estimate the la-
                                                      probability γ, and the MH sampler with probability 1 − γ.
bels of activities given a sequence of locations visited by a
person. To do so, our RMN converts a location sequence into 3.2 Supervised Learning
unrolled Markov networks, as illustrated in Fig. 1(b). Infer-
ence in our relational activity model is complicated by the fact We show how to learn generic activity models from labeled
                                                      activity sequences of N different users. Learning a cus-
that the structure of the unrolled Markov network can change tomized model for an individual user is a special case when
during inference because of the label-speciﬁc cliques. Using N = 1. The parameters to be learned are the feature weights
standard belief propagation in such networks would require w that deﬁne clique potentials in (3). To avoid overﬁtting,
the construction of cliques over all labels, which is obviously we perform maximum a posterior (MAP) parameter estima-
inefﬁcient [Taskar et al., 2002]. We overcome this problem tion and impose an independent Gaussian prior with constantvariance for each component of w, i.e., p(w) ∝ exp{−(w − input : the weights w provided by the optimizer
µ)T · (w − µ)/2σ2}, where µ is the mean and σ2 is the vari- output: L(w) and ∇L(w)
ance . We deﬁne the MAP objective function as the negative                ∇L(w)
log-likelihood of training data from N subjects plus the prior: //Evaluate the gradient
                                                        foreach subject j do
            N                                              Run MCMC with w and get M samples;
           X                                                                       (i)
 L(w)  ≡      {− log P (yj | xj , w)} − log p(w)
                                                           Get feature count difference ∆fj (1 ≤ i ≤ M) ;
           j=1                                          end
   N                                                    Compute the gradient ∇L(w) using Eq. (8) ;
  X                                (w−µ)T· (w−µ)
  =  {−wT· f(x , y ) + log Z(x , w)} +          (6)
              j  j         j           2σ2              //Evaluate the objective value L(w)
   j=1                                                  if First time calling this function then
where j ranges over different users and y are the activity L(w˜ ) = L(w) = 0; w˜ = w ;
                                    j                        ˜(i)    (i)
labels for each user. Since (6) is convex, the global minimum ∆fj = ∆fj for 1 ≤ j ≤ N,1 ≤ i ≤ M ;
can be found using standard optimization algorithms [Taskar else
et al., 2002]. We apply the quasi-Newton technique to ﬁnd  Compute L(w) using Eq. (7) ;
the optimal weights [Sha and Pereira, 2003]. Each iteration of if L(w) < L(w˜ ) then
                                                              L(w˜ ) = L(w); w˜ = w ;
this technique requires the value and gradient of (6) computed
                                                              ∆˜f (i) = ∆f (i) 1 ≤ j ≤ N 1 ≤ i ≤ M
at the weights returned in the previous iteration.               j      j for        ,         ;
                                                           end
Evaluating the objective function                       end
It can be intractable to compute exact objective values in (6) Algorithm 1: MCMC-based algorithm for simultaneously
for all but the simplest cases. This is due to the fact that, evaluating objective function and its gradient.
for a speciﬁc w, it is necessary to evaluate the partition func-
tion Z(x , w), which requires summation over all possible la-
       j                                                       (i)        (i)
bel conﬁgurations. We approximate the objective value using where ∆fj = f(xj, yj ) − f(xj, yj) is the difference be-
Monte-Carlo methods [Geyer and Thompson, 1992]. Sup-  tween the sampled and the empirical feature counts.
pose we already know the value of L(w˜ ) for a weight vector
w˜ . Then for each subject j, we use our MCMC inference to Algorithm
get M random samples, y˜(i)(1 ≤ i ≤ M), from the distri- If we compare (7) and (8), we see both require the difference
                      j                               between the sampled and the empirical feature counts. While
bution P (y | x , w˜ ). Then L(w) can be approximated as:
             j                                        samples in (7) are based on the weights w˜ , those in (8) are
                N         M                           based on w. Therefore, if we always keep the best weight
               X       1 X                   (i)
 L(w) ≈ L(w˜ ) +  {log(      exp{(w − w˜ )T · ∆˜f })}
                       M                    j         estimate as w˜ , we can reuse the sampled feature counts from
               j=1        i=1                         gradient estimation, thereby making objective value evalua-
       (w − µ)T · (w − µ) − (w˜ − µ)T · (w˜ − µ)      tion very efﬁcient.
      +                                         (7)
                       2σ2                              Our algorithm simultaneously estimates at each iteration
                                                      the value and the gradient of the negative log-likelihood (6)
        ˜(i)        (i)
where ∆fj  =  f(xj, y˜j ) − f(xj, yj) is the difference be- for given weights w. These estimates are used by the quasi-
tween sampled feature counts using w˜ and the empirical fea- Newton approach to compute new weights, and then the esti-
ture counts in the labeled data.                      mation is repeated. As shown in Alg. 1, both L(w˜ ) and L(w)
  Eq. (7) can only be used to estimate values of L(w) relative are initialized as 0 and thus all the objective values are evalu-
to L(w˜ ). Fortunately, such relative values are sufﬁcient for ated relative to the objective value of initial weights. In later
the purpose of optimization. It can be shown that the best ap- iterations, when we ﬁnd a better weight estimate that makes
proximation in (7) is obtained when w˜ is close to the optimal L(w) less than L(w˜ ), we update w˜ with the new w and also
w. Therefore, during optimization, our algorithm updates w˜                  (i)
                                                      keep the new L(w˜ ) and ∆˜f (1 ≤ j ≤ N, 1 ≤ i ≤ M). By
with better weight estimates whenever possible.                              j
                                                      doing that, we not only evaluate objective values very efﬁ-
Evaluating the gradient                               ciently, but are also able to get more accurate approximations
The gradient of the objective function, ∇L(w), equals to the as w˜ approaches closer to the optimal weights.
difference between the sampled feature counts and the em-
pirical feature counts, plus a prior term. To generate the 4 Experiments
sampled feature counts under w, we again run MCMC in- To evaluate our location-based activity recognition technique,
ference. Suppose we have obtained M random samples,
 (i)                                                  we collected two sets of location data using wearable GPS
yj (1 ≤ i ≤ M), from the distribution P (y | xj, w). We units. The ﬁrst data set (called “single”) contains location
can compute the gradient as:                          traces from a single person over a time period of four months
              N
              X                            w − µ      (see Fig. 2). It includes about 400 visits to 50 different places.
 ∇L(w)    =      {Ew[f(xj, y)] − f(xj, yj)} +         The second data set (called “multiple”) was collected by ﬁve
                                             σ2
              j=1                                     different people, about one week for each. Each person’s data
              N      M                                include 25 to 35 visits and 10 to 15 different places. We ex-
              X    1 X     (i)   w − µ
          ≈      {      ∆f   } +                (8)   tracted places / visits from the GPS logs by detecting loca-
                  M        j      σ2
              j=1    i=1                              tions at which a person spends more than 10 minutes [Hariha-ran and Toyama, 2004]. Each instance corresponds to an ac-
tivity. We then clustered nearby activity locations into places.
For training and evaluation, we let the subjects manually label
the types of activities. Then, we trained the models and tested
their accuracy. Accuracy was determined by the activities for
which the most likely labeling was correct.
Applying learned models to other people
In practice, it is of great value to learn a generic activity
model that can be immediately applied to new users without      AtHome       AtWork       Shopping
additional training. In this experiment, we used the “mul-      DiningOut    Visiting     Others
tiple” data set and performed leave-one-subject-out cross- Figure 2: Part of the locations contained in the “single” data set,
validation: we trained using data from four subjects, and collected over a period of four months (x-axis is 8 miles long).
tested on the remaining one. The average error rates are indi-
cated by the white bars in Fig. 3(a). By using all the features, Improved learning through priors extracted from others
the generic models achieved an average error rate of 18%. It When estimating the weights of RMNs, a prior is imposed
can be seen that global features and spatial constraints signif- in order to avoid overﬁtting. Without additional information,
icantly improve classiﬁcation. To gage the impact of different a zero mean Gaussian is typically used as the prior [Taskar
habits on the results, we also performed the same evaluation et al., 2002]. [Peng and McCallum, 2004] demonstrated that
using the “single” data set. In this case, we used one-month better accuracy can been achieved if feature-dependent vari-
data for training and the other three-month data for test, and ances are used. Our experiment shows that performance can
we repeated the validation process for each month. The re- also be improved by estimating the prior means of the weights
sults are shown by the gray bars in Fig. 3(a). In this case, the (µ in Eq. (6)) using data collected from other people.
models achieved an error rate of only 7% by using all the fea- In this experiment, we compared the models of a spe-
tures. This experiment shows that it is possible to learn good ciﬁc person trained using a zero-mean prior with the models
activity models from groups of people. It also demonstrates trained using an estimated prior. In the latter case, we ﬁrst
that models learned from more “similar” people can achieve learned the feature weights from other people and used those
higher accuracy. This indicates that models can be improved as the mean of the Gaussian prior. We evaluated the perfor-
by grouping people based on their activity patterns.  mance for different amounts of training data available for the
  Table 1 shows the confusion matrix of one experiment on test person. The results are shown in Fig. 3(c), in which the
generic models (rightmost white bar in Fig. 3(a)). As can be error rates are counted only on the novel places, i.e., places
seen, our approach is able to perfectly label homes and work- that were not visited in the training data and thus often very
places. The technique performs surprisingly well on the other irregular. We can see that using data from others to generate a
activities, given that they are extremely difﬁcult to distinguish prior boosts the accuracy signiﬁcantly, especially when only
based on location information alone. The confusion matrix small amounts of training data are available.
also shows that simply labeling places by the most frequent The Bayesian prior allows the model to smoothly shift
activity (home) would result in an error rate of 62%. from generic to customized: On one end, when no data
                                                      from the given subject are available, the approach returns the
                        Inferred labels               generic (prior) model; on the other end, as more labeled data
  Truth  Home    Work   Shop   Dining  Visit  Other   become available, the model adjusts more and more to the
                                                      speciﬁc patterns of the user.
 Home      57      0      0      0       0      0
  Work     0      34      0      0       0      0     Additional experiments
  Shop     0       0      8      2       0      4     For comparison, we also built basic HMMs in which the hid-
 Dining    0       0      3      6       0      4     den states are the labels and all the observations are inde-
  Visit    0       0      1      0       4      3     pendent given the states. Parameter estimation in HMMs
  Other    0       0      6      1       2     15     with labeled data is done via frequency counting. The most
Table 1: Confusion matrix of cross-validation on generic models likely labels can be found using the Viterbi algorithm. In the
with all features.                                    one-month-training cross-validation on the “single” data set,
                                                      the HMM produced an average error rate of 21.1% by using
  To evaluate the impact of number of people available for the temporal, geographic, and transition features. 1 Because
model learning, we trained our model using data from dif- of the advantages of discriminative learning, even using the
ferent numbers of subjects and tested on the remaining one same features, RMNs performed better than HMMs and re-
(all features were used). The average error rates of the cross- duced the relative error rate by about 10%.
validation are shown in Fig. 3(b). When trained using only In a separate set of experiments, we tested the performance
one subject, the system does not perform well (error rate of of our MCMC sampler. By visualizing the standard Gelman-
35%), mainly because many patterns speciﬁc to that person Rubin statistics [Gilks et al., 1996] generated from parallel
are applied onto others. When more subjects are used for
training, the patterns being learned are more generic and the 1Spatial constraints and global features do not satisfy the ﬁrst-
models achieve signiﬁcantly higher accuracy.          order Markov assumption and thus are difﬁcult to model as HMMs.