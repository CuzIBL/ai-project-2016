  GRAEL: an agent-based evolutionary computing approach for natural language 
                                            grammar development 

                                                    Guy De Pauw 
                                       CNTS - Language Technology Group 
                                            UIA - University of Antwerp 
                                                  Antwerp - Belgium 
                                             guy.depauw@ua.ac.be 

                        Abstract                               gins of grammar in a computational context [Batali, 2002; 
                                                               Kirby, 2001J or the co-ordinated co-evolution of grammatical 
     This paper describes an agent-based evolution•
                                                               principles [Briscoe, 1998]. Yet so far, little or no progress 
     ary computing technique called GRAEL (Grammar 
                                                               has been achieved in evaluating evolutionary computing as a 
     Evolution), that is able to perform different natu•
                                                               tool for the induction or optimization of data-driven parsing 
     ral language grammar optimization and induction 
                                                               techniques. 
     tasks. Two different instantiations of the GRAEL-
                                                                 The GRAEL     environment provides a suitable framework 
     environment are described in this paper: in GRAEL-                      1
                                                               for the induction and optimization of any type of grammar 
     1 large annotated corpora are used to bootstrap 
                                                               for natural language in an evolutionary setting. In this pa•
     grammatical structure in a society of agents, who 
                                                               per we hope to provide an overview of GRAEL as a grammar 
     engage in a series of communicative attempts, dur•
                                                               optimization and induction technique. We will first outline 
     ing which they redistribute grammatical informa•
                                                               the basic architecture of the GRAEL environment in Section 
     tion to reflect useful statistics for the task of pars•
                                                               2 on the basis of a toy example. Next, we introduce GRAEL-
     ing. In GRAEL-2, agents are allowed to mutate 
                                                               1 (Section 3) as a grammar optimization technique that can 
     grammatical information, effectively implementing 
                                                               enhance corpus-induce grammars. By adding an element of 
     grammar rule discovery in a practical context. A 
                                                               mutation in GRAEL-2 we implement a method to extend the 
     combination of both GRAEL-1 and GRAEL-2 can 
                                                               coverage of a corpus-induced grammar. We will also describe 
     be shown to provide an interesting all-round opti•
                                                               a combination of both GRAEL-1 and GRAEL-2 which can be 
     mization for corpus-induced grammars. 
                                                               shown to provide an interesting all-round optimization tech•
                                                               nique for corpus-induced grammars. 
1    Introduction 
Evolutionary computing has seen many interesting applica•      2 GRAEL - Grammar Evolution 
tions on a broad range of research domains over the years. Its 
ability to overcome the problem of local maxima in finding     A typical GRAEL society contains a population of agents in 
a solution to a particular problem, by recombining and mu•     a virtual environment. Each of these agents holds a number 
tating individuals in a society of possible solutions, has made of structures that allows it to produce sentences as well as 
it an attractive technique for problems involving large, com•  induce a probabilistic grammar to analyze other agents' sen•
plicated and non-linearly divisible search spaces. The evo•    tences. During an extended series of error-driven inter-agent 
lutionary computing paradigm has however always seemed         interactions, these grammars are updated over time. While 
reluctant to deal with natural language syntax, probably be•   the evolutionary computing approach of GRAEL is able to de•
cause it is essentially a recursive, non-propositional system, fine the quality of the grammars that are developed over time, 
dealing with complex issues such as long-distance dependen•    the agent-based aspect of GRAEL ensures that the grammar 
cies and constraints. This has made it difficult to incorporate optimization is grounded in the practical task of parsing it-
it in typically propositional evolutionary systems such as ge• self. From an engineering point of view, GRAEL provides a 
netic algorithms.                                              general framework for grammar optimization and induction, 
  A limited amount of GA-related syntactic research has fo•    but from a more theoretical point of view, GRAEL can also 
cused on linguistic data [Smith and Witten, 1996; Wyard,       help us to understand the dynamics of grammar emergence 
1991; Antonisse, 1991; Araujo, 2002], but none of these sys•   and evolution over time. 
tems are suited to a generic (treebank) grammar optimization     In the data-driven GRAEL experiments described in this pa•
task, mainly because the grammatical formalism and evo•        per, the grammatical knowledge of the agents in the society 
lutionary processes underlying these systems are designed      is bootstrapped by using an annotated natural language cor•
to fit a particular task, such as information retrieval [Losee, pus [Marcus et al., 1993]. At the onset of such a data-driven 
1995]. Other work on syntax in the evolutionary comput•
ing paradigm has either been involved in studying the ori-        'GRAmmar EvoLution 


NATURAL LANGUAGE                                                                                                     823                                                                be parsed, or two other agents in the GRAEL society will be 
                                                               randomly selected to play a language game. 
                                                                 These interactions, which introduce a concept of error-
                                                               driven knowledge sharing, extend the agents' grammars fast, 
                                                               so that the datasets can grow very large in a short period of 
                                                               time. A generation-based GRAEL society can be used to al•
                                                               low the society to purge itself of bad agents and build new 
                                                               generations of good parser agents, who contain a fortuitous 
                                                               distribution of grammatical knowledge. This involves the use 
                                                               of fitness functions that can distinguish good agents from bad 
                                                               ones. For a full overview of all the evolutionary parameters 
                                                               in the GRAEL environment, many of which have a significant 
                                                               impact on processing, we would like to refer to [Dc Pauw, 
                                                               2002]. In this paper, we will describe the most relevant subset 
                                                               of experiments that allows us to evaluate GRAEL as a gram•
                                                               mar induction and optimization technique. 

                                                               3 GRAEL-1: Probabilistic Grammar 
                                                                   Optimization 
                                                               Historically, most syntactic parsers for natural language have 
                                                               made use of hand-written grammars, consisting of a labo•
                                                               riously crafted set of grammar rules. But in recent years, 
                                                               a lot of research efforts employ annotated corpora to au•
                                                               tomatically induce grammars [Bod, 1998; Collins, 1999; 
                                                               De Pauw, 2000]. Yet, data-analysis of the output gener•
                                                               ated by these parsers still brings to light fundamental limi•
                                                               tations to these corpus-based methods [Klein and Manning, 
                                                               2001]. Even though generally providing a much broader cov•
                                                               erage than hand-built grammars, corpus-induced grammars 
                                                               may still not hold enough grammatical information to pro•
GRAEL society, the syntactic structures of a treebank are ran• vide parses for a large number of sentences, as some rules 
domly distributed over the agents, so that each agent holds a  that are needed to generate the correct tree-structures are not 
number of tree-structures in memory. These structures enable   induced from the original corpus (cf. Section 4). 
them to generate sentences, as well as provide grammars that 
                                                                 But even if there were such a thing as a full-coverage 
allow them to analyze other agents' sentences. 
                                                               corpus-induced grammar, performance would still be limited 
  The actual interaction between agents is implemented in      by the probabilistic weights attributed to its rules. A typi•
language games: an agent (agl) presents a sentence to an•      cal data-driven parser provides a huge collection of possible 
other agent (ag2). If ag2 is able to correctly parse agl's sen• parses for any given sentence. Fortunately, we can also in•
tence, the communication is successful. If on the other hand,  duce useful statistics from the annotated corpus that provides 
ag2 is lacking the proper grammatical information to parse     a way to order these parse forests to express a preference for 
the sentence correctly, agl shares the necessary information   a particular parse. These statistics go a long way in provid•
for ag2 to arrive at the proper solution.                      ing well ordered parse forests, but in many other cases, it can 
  Figure 1 displays a toy example of such a language game.     be observed that the ranking of the parse forest is sometimes 
In this example, a "treebank" of two structures has been dis•  counter-intuitive in that correct constructs are often overtaken 
tributed over a society of two agents. The two agents engage   by obviously erroneous, but highly frequent structures. 
in a language game, in which agl presents the sentence "1        This can easily be explained by the inherent nature of 
offered some bear hugs" to ag2 for parsing. At this point in   corpus-based grammars: the initial probabilistic values at•
time, ag2's grammar does not contain the proper grammatical    tached to the grammar rules induced from the annotated data 
information to interpret this sentence the way agl intended    are equal to their relative frequency in the corpus. It might be 
and so ag2 will return an incorrect parse, even though it is   the case however that, even though they are directly induced 
consistent with its own grammar.                               from the annotated corpus, the probabilities of these rules are 
  Consequently, agl will try and help ag2 out by reveal•       not suited to the disambiguation task as yet. It may therefore 
ing the minimal correct substructure of the correct parse that be useful to have the parser use the grammar to practice the 
should enable ag2 to arrive at a better solution. This informa• task of parsing and adjust the probabilistic weights of partic•
tion is incorporated in ag2's grammar, who will try to parse   ular structures according to these test cases. We then need to 
the sentence again with the updated knowledge. When ag2        consider the initial grammar as basic raw material in need of 
is able to provide the correct analysis (or is not able to after optimization, as it is merely a reflection of the original data 
a certain number of attempts) either agl's next sentence will  set and is not yet optimized for the task of parsing (unseen) 


824                                                                                               NATURAL LANGUAGE  sentences itself. 
   Typical methods of probabilistic grammar optimization 
 include, among others, bagging and boosting [Henderson 
 and Brill, 2000; Collins, 2000], re-estimation of the con•
 stituents probabilities [Goodman, 1998; Charniak, 2000] and 
 including extra information sources [Belz, 2001; Collins, 
 1999]. But we propose an agent-based evolutionary comput•
 ing method to resolve this issue. Grammar optimization us•
 ing a GRAHL environment is in this vein related to the afore•
mentioned bagging approach to grammar optimization, albeit 
with some notable differences. By distributing the knowl•
edge over a group of agents and having them interact with 
each other, we basically create a multiple-route model for 
probabilistic grammar optimization. Grammatical structures 
extracted from the training corpus, will be present in differ•
ent quantities and variations throughout the GRAEL society. 
While the agents interact with each other and in effect prac•
tice the task on each other's grammar, a varied range of prob•
abilistic grammars are optimized in a situation that directly 
relates to the task at hand. The evolutionary aspects of the 
system make sure that, while marginally useful grammatical 
information is down-toned, common constructs arc enforced, 
providing a better balanced model for statistical parsing. 
   The way GRAEL accomplishes a re-distribution of the orig•
inal probabilistic values is by using the default GRAEL archi•   Figure 2: Comparing parsers: baseline parser and GRAEL 
tecture described in Section 2. This type of error-driven learn•
ing makes sure that mistakes are being dealt with by transfer•
ring difficult grammatical constructs, thereby increasing their  For syntactic processing, the agents use the parsing sys•
probabilistic value in the other agent's grammar. This prob•   tem PMPG described in [De Pauw, 2000], which integrates 
abilistic adjustment will be taken into account during subse•  a CKY parser [Chappelier and Rajman, 1998] and a parse 
quent parsing attempts by this agent, hopefully triggering the forest ranking scheme that employs probabilistic information 
correct grammatical structure in the future.                   as well as a memory-based operator to maximize for each 
                                                               parse the number of nodes that can be retrieved from mem•
3.1 Experimental Setup                                         ory. A PMPG takes the form of a simple PCFG-type gram•
The overall setup of the GRAEL experiments is displayed in     mar, enriched with numerical indices that encode contextual 
Figure 2. Baseline accuracy is measured by directly induc•     information previously observed in a treebank. This memory-
ing a grammar from the training set to power Parser 1, which   based instantiation of Data-Oriented Parsing [Bod, 1998] en•
disambiguates the test set. This grammar takes on the form     sures that larger syntactic structures are used as the basis for 
of a PMPG as outlined in [De Pauw, 2000] (cf. infra). The      parsing, with a minimal loss of computational efficiency over 
same training set is also randomly and equally distributed     regular PCFGs. The PMPG approach can therefore be con•
over a number of agents in the GRAEL society, who will con•    sidered to introduce a psycho-linguistically relevant memory-
sequently engage in a number of language games. At some        based operator in the parsing process. 
point, established by the halting procedure (cf. infra), the so• The full experimental run varied society sizes, generation 
ciety is halted and the fittest agent is selected from the society. methods, fitness functions and halting procedures [De Pauw, 
This agent effectively constitutes a redistributed and proba•  2002]. The subset of experiments described in this paper em•
bilistically optimized grammar, which can be used to power     ployed the sexual procreation method to introduce new gen•
Parser 2. GRAEL-1 accuracy is achieved by having this parser   erations by combining the grammars of two fit agents in the 
disambiguate the same test set.                                society to create new generations of parser agents. The fitness 
   We used two data sets from the Penn Treebank [Marcus        of an agent is defined by recording a weighted average of the 

et al.9 1993]. The main batch of experiments was conducted     F-score during inter-agent communication (also see Figure 3) 
on the small, homogeneous ATlS-corpus, which consists of       and the F-score of the agent's parser on a held-out validation 
a collection of annotated sentences recorded by a spoken-      set. This information was also used to try and halt the so•
dialogue system. The larger Wall Street Journal Corpus         ciety at a global maximum and select the fittest agent from 
(henceforth WSJ), a collection of annotated newspaper arti•    the society. For computational reasons, the experiments on 
cles, was used to test the system on a larger scale corpus. The the WSJ-corpus were limited to two different population sizes 
common division between training set (Section 02-21) and       and used an approximation of GRAEL that can deal with large 
test set (Section 23) was used. Semantically oriented flags    datasets in a reasonable amount of time. The test set was not 
and numeric flags indicating internal relations were removed   used in any way during actual GRAEL-processing in agree•
to allow for more streamlined syntactic processing.            ment with blind-testing procedures. 


NATURAL LANGUAGE                                                                                                     825                                            Table 1: Baseline vs GRAEL-1 results 

3.2 Results                                                    dataset. 
Table 1 displays the exact match accuracy and F-scores for 
the baseline model, a standard PMPG parser using a grammar     4 GRAEL-2: Grammar Rule Discovery 
induced from the training set (cf. Figure 2). It also displays The functionality of GRAEL-1 can be extended by only apply•
scores of the GRAEL system for different population sizes.     ing minor alterations to the GRAEL system. With GRAEL-2 
We notice a significant gain for all GRAEL models over the     we wish to provide a grammar rule discovery method which 
baseline model on the ATIS corpus, but increasing population   can deal with the problem of grammar sparseness. Hand•
size over 20 agents seems to decrease exact match accuracy     written and corpus-induced grammars alike have to deal with 
on the ATIS corpus. Likewise, the small society of 5 agents    the fundamental issue of coverage. [Collins, 1999] for exam•
achieves only a very limited improvement over the baseline     ple reports that when using sections 2-21 of the wsj-corpus 
method. Data analysis showed that the best moment to halt      as a training set and section 23 as a test set, 17.1% of the sen•
the society and select the fittest agent from the society, is a tences in the test set require a rule not seen in the training set. 
relatively brief period right before actual convergence sets   Even for a large corpus such as the WSJ, sparse grammar is 
and grammars throughout the society are starting to resem•     indeed a serious accuracy bottleneck. 
ble each other more closely. The size of the the society seems   It would therefore be useful to have a method that can 
to be the determining factor controlling the duration of this  take a corpus-induced grammar and extend it by generating 
period.                                                        new rules. But doing so in a blind manner, would provide 
   Preliminary tests on a subset of the WSJ corpus had shown   huge, over-generating grammars, containing many nonsensi•
that society sizes of 20 agents and less to be unsuitable for  cal rules. The GRAEL-2 system described in this section, in•
a large-scale corpus, again ending up in a harmful premature   volves a distribute d approach to this type of grammar rule dis•
convergence. The gain achieved by the GRAEL society is less    covery. The original (sparse) grammar is distributed among a 
spectacular than on the ATIS corpus, but it is still statistically group of agents, who can randomly mutate the grammatical 
significant. Larger society sizes and full GRAEL processing    structures they hold. The new grammatical information they 
on the WSJ corpus should achieve a more significant gain.      create is tried and tested by interacting with each other. The 
   The experiments do show however, that GRAEL-1 is in•        neo-darwinist aspect of this evolutionary system will make 
deed an interesting method for probabilistic grammar redis•    sure that any useful mutated grammatical information is re•
tribution and optimization. Data analysis shows that many of   tained throughout the population, while noise is filtered out 
the counter-intuitive parse forest orderings that were appar•  over time. This method provides a way to create new gram•
ent in the baseline model, are being resolved after GRAEL-     matical structures previously unavailable in the corpus, while 
 1 processing. It is also interesting to point out that we are at the same time evaluating them in a practical context, with•
achieving an error reduction rate of more than 26% over the    out the need for an external information source. 
baseline method, without introducing any new grammatical         To accomplish this, we need to implement some minor al•
information in the society, but solely by redistributing what is terations to the GRAEL-1 system. The most important adjust•
already there. These experimental results indicate that anno•  ment occurs during the language games. We refer back to the 
tated data can indeed be considered as raw material that can   toy example of the language game in Figure 1 to the point 
be optimized for the practical use of parsing unseen data.     where agl suggests the minimal correct substructure to ag2. 
   [De Pauw, 2002] also describes experiments that directly    In GRAEL-1 this step introduced a form of error-driven learn•
compare GRAEL to the similar methods of bagging and boost•     ing, making sure that the probabilistic value of this grammati•
ing [Henderson and Brill, 2000], which are summarized in       cal structure is increased. The functionality of GRAEL-2 how•
the bottom two lines of Table 1. Bagging and boosting were     ever is different: in this step, we assume that there is a noisy 
shown to obtain significantly lower accuracy figures on al•    channel between agl and ag2 which may cause ag2 to misun•
most all accounts. Only the F-score for the bagging experi•    derstand agl's structure. Small mutations on different levels 
ment exceeded that of the optimal GRAEL configuration, but     of the substructure may occur, such as the deletion, addition 
this can be attributed to the fact that an approximation of    and replacement of nodes. This effectively introduces previ•
GRAEL was used for full processing on the extensive WSJ        ously unseen grammatical data in the GRAEL society, which 


826                                                                                                NATURAL LANGUAGE Table 2: Baseline vs GRAEL-1 vs GRAEL-2 VS GRAEL2+1 
Results 

                                                               Figure 3: GRAEL2+1 Experiment - F-scores during language 
will consequently be optimized over time.                      games 
   Preliminary experiments however showed that this does not 
work as such, since the newly created structures were largely 
                                                               interesting in this respect. GRAEL-1 outperforms GRAEL-2 
being ignored in favor of the gold-standard corpus structures. 
                                                               on this data set, but the combination of the two seems to be 
We therefore implemented another alteration to the GRAEL-1 
                                                               quite beneficial for parsing accuracy. 
system. Instead of just presenting the tree-structure originally 
                                                                 Evaluating a grammar rule discovery method poses an em•
assigned by the training set, we now require agl to parse the 
                                                               pirical problem in that it can never be clear what grammar 
string-only sentence using the grammar acquired during lan•
                                                               rules are missing until we actually need them. The test set we 
guage games, replacing the tree-structure from the training 
                                                               compiled to perform the GRAEL-2 ATis-experiments goes a 
set, with a possibly different tree-structure that incorporates 
                                                               long way in providing a touchstone to see how well GRAEL-
some of the mutated information. This alteration makes sure 
                                                               2 performs as a supervised grammar induction method. And 
that the mutated grammatical structures are actively being 
                                                               results indicate it performs quite well: mutated information 
used, so that their usefulness as grammatical constructs can 
                                                               becomes available that is able to create parses for difficult 
be measured in a practical context. 
                                                               constructions, while the number of structures that constitute 
Experimental Setup and Results                                 noise is limited and is attributed a small enough portion of 
                                                               the probability mass as not to stand in the way of actual use•
The setup for the GRAEL-2 experiments is the same as for the 
                                                               ful mutated structures. 
GRAEL-1 experiments (cf. Figure 2). To test the grammar-
rule discovery capabilities of GRAEL-2 we have compiled a 
special worst-case scenario test set for the ATIS corpus, con• 5 Concluding Remarks 
sisting of the 97 sentences in the ATIS that require a grammar This paper has presented one of the first research efforts that 
rule that cannot be induced from the training set. For the     introduces agent-based evolutionary computing as a machine 
WSJ-experimentthe normal test set was used. A 20-agent and     learning method for data-driven grammar optimization and 
a 100-agent society were respectively used for the ATIS and    induction. In recent years, many researchers have employed 
WSJ experiments.                                               ensemble methods to overcome any negative bias their train•
   The baseline and GRAEL-1 methods for the ATIS experi•       ing data might impose on their classifiers. It is indeed im•
ments trivially have an exact match accuracy of 0%, which      portant to view (annotated) data, not as an optimally dis•
also has a negative effect on the F-score (Table 2). GRAEL-2   tributed set of examples but as raw material that needs to be 
is indeed able to improve on this significantly, proving that it pre-processed before it can be used by a machine learning 
is indeed an effective grammar rule discovery method. Data-    classifier. The bagging and boosting approach for instance, 
analysis shows however that it has lost the beneficial proba•  tries to create resamplings of the original data, to overcome 
bilistic optimization effect of GRAEL-1.                       the local maxima the data might restrict the classifier to, but 
   We therefore performed another experiment, in which we      we believe GRAEL adds an extra dimension to the task: by 
turned the GRAEL-2 society into a GRAEL-1 society after the    splicing the data and incorporating it in a society of commu•
former's halting point. In other words: we take a society      nicating agents, we allow for the parallel development of sev•
of agents using mutated information and consequently ap•       eral grammars at once, enhanced in a practical context that 
ply GRAEL-1 's probabilistic redistribution properties on these mirrors the goal itself: parsing unseen data. 
grammars. Figure 3 shows the course of the GRAEL2+1 ex•          We described two instantiations of the GRAEL environ•
periment. In this figure we see the F-scores recorded during   ment. The basic GRAEL-1 system aims to provide a beneficial 
inter-agent communicative attempts. After an almost linear     re-distribution of the probability mass of a probabilistic gram•
increase during GRAEL-2 processing, the society is halted af•  mar. By using a form of error-driven learning in the course 
ter an extended period of convergence. Next, GRAEL-1 pro•      of language games between agents, probabilistic values are 
cessing resumes, which negatively affects F-scores for a brief adjusted in a practical context. This optimizes the grammars 
period of time, until the society reconverges. Even though the for the task of parsing data, rather than reflecting the prob•
F-scores do not seem to improve over those observed before     ability mass of the initial data set. This method favorably 
the transition to GRAEL-1, Table 2 shows that the fittest agent compares to established grammar optimization methods like 
selected from the society after the transition performs better bagging and boosting. 
on the held-out test set. The results on the wsj-corpus are also By adding an element of mutation to the concept of 


NATURAL LANGUAGE                                                                                                     827 