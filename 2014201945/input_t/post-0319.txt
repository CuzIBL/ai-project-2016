                 On the Evolution of Memory Size in the Minority Game
                                           (extended abstract)

                                Ricardo M. Araujo´   and  Luis C. Lamb
                            Institute of Informatics, Federal University of Rio Grande do Sul
                                        91501-970, Porto Alegre, Brazil
                                 email: rmaraujo@inf.ufrgs.br, LuisLamb@acm.org

1  Introduction
This work presents a study on the effects of different mem-
ory sizes in the Minority Game (MG) market model [Zhang,
1998]. We analyse the effects on an agent’s performance
when this agent is endowed with a different memory size with
respect to other agents in the game. Our aim is to identify in
which situations a large or small memory might be advanta-
geous in the game. From the obtained results we argue that
there exist convergence to and evolutionary stability around
certain memory sizes and we consider an evolutionary setup
which conﬁrms our hypothesis.
  The MG is deﬁned as an odd number of agents (N) which
must choose at each turn of the game whether they will be in
one of two possible groups. After all agents have made their
choices, agents in the minority group are rewarded. In order     Figure 1: Variance versus memory size
to make a decision, agents use an inductive learning algorithm
in order to try to exploit patterns from previous outcomes. efﬁciency. We are interested in the target-agent’s success rate
The number of past turns that agents consider to decide de- (number of correct decisions over total number of rounds).
ﬁnes their memory size (M).                             Figure 2 shows the success rate for different values of m
  One of the main properties in a MG is its efﬁciency, which for the environment in the inefﬁcient region (M = 3). It
                                  2
is evaluated by the statistical variance (σ ) of the number of is shown that the target-agent with larger memoriesperforms
agents in a group [Moro, 2004]. Efﬁciency is known to be better than average. This agrees with the common belief that
                                  2M [
a function of a control parameter α = N Manuca et al., having access to more information is beneﬁcial. Figure 3
2000]. For N ﬁxed, M becomes the main control parame- shows that this is not always the case. For M = 6 the en-
ter of the game. Figure 1 shows σ2 for a range of memory vironment is now in the efﬁcient region and increases in the
sizes. Three regions are observed: (i) for small M variance target-agent’s memory size does not translate into better per-
is very high, above the expected for the random case game formance. Actually, having access to more information is
(which we will call inefﬁcient region); (ii) for high values of shown to be harmful.
M, variance is exactly the one expected for the random case Finally, we have simulated our target-agent in the random
game (random case region); (iii) for intermediate M the sys- case region. We have observed that the target-agent showed
tem presents small variances (efﬁcient region).       no gains or losses of performance throughout all values of m.
                                                      This is mainly due to the fact that if agents are acting as if
2  Memory Size and Performance                        they were choosing randomly, as is the case in this region,
                                                      then there will be no pattern in the outcome time series to be
Firstly, we tackle the design aspects of agents by asking what exploited.
is the best memory size for an agent immersed in a MG where
                                                        Figure 4 depicts the gain ( ftarget ) of a target-agent with
agents have a different memory size. We do so by modify-                        favg
ing the typical MG setup, deﬁning two basic elements: (i) one more bit of memory than the environment for a range of
the environment, a classical MG with an even number (N) M (i.e. m = M + 1 for all M). We notice that gains are
of agents; (ii) a target-agent endowed with a possibly differ- reduced as the system approaches the efﬁcient region and in-
ent memory size (m) from that in the environment. We have deed become smaller than unit for every M in the efﬁcient
simulated different target-agents within environments in all region, reapproching unit when closer to the random case re-
3 efﬁciency regions, taking N as a large value, so that the gion. It is only in the inefﬁcient region that gains become
target-agent plays no role in the determination of the game’s larger than unit and agents with more memory are able toFigure 2: Target-agent’s success rate in the inefﬁcient region. Figure 4: Gain of a target-agent with m=M+1.
Dashed line is the average success rate of the environment.
                                                      A mutant agent with larger memory would be penalized, but
                                                      one with smaller memory would perform just as well and
                                                      we could expect that memory sizes would end up being dis-
                                                      tributed throughout values below M. However, this would
                                                      result in a reduction of the average memory size, causing
                                                      the system to move towards the inefﬁcient region and mak-
                                                      ing larger memories harmful, thus creating a new upper limit
                                                      in memory size. The process continues until M0 is reached.
                                                        We may say that M0 is not only evolutionary stable, as
                                                      no mutant agent can do better than the agents already in the
                                                      environment at this point, but it is also an attractor of mem-
                                                      ory sizes in the efﬁcient and inefﬁcient regions. By starting
                                                      in any point in these regions, the system is expected to con-
                                                      verge to M0. This reasoning was indeed conﬁrmed by sim-
   Figure 3: Target-agent’s success rate in the efﬁcient region. ulations. However we did not have a convergence towards
                                                      M0  = 4 as it would be expected by analysing the traditional
                                                      MG (see Fig. 1). We have observed an initial “arms race” of
exploit the environment. Interestingly, the same experiment
                                                      memory sizes up to M = 4 and then a fall towards a ﬁnal
repeated having m = M − 1 shows that smaller memories
                                                      equilibrium at M = 1. This may be explained if we consider
makes no difference to the target-agent’s performance, it be-
                                                      that two adaptations happen, adaptation of memory size and
comes the same as the environment’s average.
                                                      adaptation of strategies, for when an agent modiﬁes her mem-
                                                      ory size she also modiﬁes her strategies. Since the strategies
3  Evolution of memory size                           space is much larger than the memory sizes space, adaptation
Now we apply our results in an evolutionary setup, in a MG of the latter is faster. Thus, at the beginning of the simulation
where mutant agents - endowed with slightly different mem- we indeed have M0 = 4 and, as strategies adapt, a different
ory sizes - may appear and replace badly performing agents. dynamics emerge [Araujo´ and Lamb, 2004], with a different
Let the system be initially in an inefﬁcient phase. Our ex- value of M0.
periments have shown that an agent with a larger memory Acknowledgments: This work was partly supported by CAPES,
would be able to exploit the environment in this region while CNPq and FAPERGS, Brazil.
smaller memories provide no beneﬁts. Thus, agents with
larger memories would be able to survive and average mem- References
ory size would increase until the point where further increases [Araujo´ and Lamb, 2004] R. M. Araujo´ and L. C. Lamb. Towards
lead to no gains. This happens when the system enters the ef- understanding the role of learning models in the dynamics of the
ﬁcient region. M0 denotes the memory size in which the sys- minority game. In Proc. of 16th IEEE ICTAI 2004, pages 727–
tem ﬁrst becomes efﬁcient. The system cannot move beyond 731, 2004.
M0, as larger memories would cause harm to the mutant’s [Manuca et al., 2000] R. Manuca, Y. Li, R. Riolo, and R. Savit. The
performance. In addition, the system cannot move towards structure of adaptive competition in minority games. Physica A,
lower memory sizes: whenever a sufﬁcient number of agents 282:559, 2000.
convert to smaller memories, the system would again be in [Moro, 2004] E. Moro. The minority game: an introductory guide.
an inefﬁcient region and the push towards larger memories In E. Korutcheva and R. Cuerno, editors, Advances in Condensed
would reappear.                                         Matter and Statistical Physics. Nova Science Publishers, 2004.
  A similar reasoning might be applied to a MG starting at [Zhang, 1998] Y.-C. Zhang. Modeling market mechanism with evo-
any point M in the efﬁcient region, above or equal to M0. lutionary games. Europhysics News, March/April 1998.