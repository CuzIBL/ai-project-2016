           On the Design of Social Diagnosis Algorithms for Multi-Agent Teams 

                                        Meir Kalech and Gal A. Kaminka 
                                               The MAVERICK Group 
                                           Computer Science Department 
                                                  Bar Ilan University 
                                                   Ramat Gan, Israel 
                                            {kalechm, galk}@cs.biu.ac.il 

                        Abstract                               not. Given this diagnosis, the robots can negotiate to resolve 
                                                               the disagreement. 
     Teamwork demands agreement among team-
                                                                 Unfortunately, while the problem of detection has been ad•
     members to collaborate and coordinate effectively. 
                                                               dressed in the literature (e.g., [Kaminka and Tambe, 2000]), 
     When a disagreement between teammates occurs 
                                                               social diagnosis remains an open question. Naive implemen•
     (due to failures), team-members should ideally di•
                                                               tations of social diagnosis processes can require significant 
     agnose its causes, to resolve the disagreement. 
                                                               computation and communications overhead, which prohibits 
     Such diagnosis of social failures can be expen•
                                                               them from being effective as the number of agents is scaled 
     sive in communication and computation overhead, 
                                                               up, or the number of failures to diagnose increases. Previ•
     which previous work did not address. We present 
                                                               ous work did not rigorously address this concern: [Kaminka 
     a novel design space of diagnosis algorithms, dis•
                                                               and Tambe, 2000] guarantee disagreement detection with•
     tinguishing several phases in the diagnosis pro•
                                                               out communications, but their heuristic-based diagnosis of•
     cess, and providing alternative algorithms for each 
                                                               ten fails. [Dellarocas and Klein, 2000; Horling et al., 1999] 
     phase. We then combine these algorithms in dif•
                                                               do not explicitly address communication overhead concerns. 
     ferent ways to empirically explore specific design 
                                                               [Frohlich et al, 1997; Roos et al, 2001] assume fixed com•
     choices in a complex domain, on thousands of fail•
                                                               munication links, an assumption which does not hold in dy•
     ure cases. The results show that centralizing the 
                                                               namic teams in which agents may choose their communica•
     diagnosis disambiguation process is a key factor 
                                                               tion partners dynamically (see Section 5 for details). 
     in reducing communications, while run-time is af•
     fected mainly by the amount of reasoning about              We seek to examine the communication and computation 
     other agents. These results contrast sharply with         overhead of social diagnosis in depth. We distinguish two 
     previous work in disagreement detection, in which         phases of social diagnosis: (i) selection of the diagnosing 
     distributed algorithms reduce communications.             agents; and (ii) diagnosis of the team state (by the selected 
                                                               agents). We provide alternative algorithms for these phases, 
                                                               and combine them in different ways, to present four diag•
1 Introduction                                                 nosis methods, corresponding to different design decisions. 
One of the key requirements in teamwork is that agents come    We then empirically evaluate the communications and run•
to agree on specific aspects of their joint task [Cohen and    time requirements of these methods in diagnosing thousands 
Levesque, 1991;GroszandKraus, 1996;Tambe, 1997]. With          of systematically-generated failure cases, occurring in a team 
increasing deployment of robotic and agent teams in com•       of behavior-based agents in a complex domain. 
plex, dynamic settings, there is an increasing need to also be   We draw general lessons about the design of social diag•
able respond to failures that occur in teamwork [Tambe, 1997;  nosis algorithms from the empirical results. Specifically, the 
Parker, 1998], in particular to be able to diagnose the causes results show that centralizing the disambiguation process is 
for disagreements that may occur, in order to facilitate recov• a key factor in dramatically improving communications effi•
ery and reestablish collaboration (e.g., by negotiations [Kraus ciency, but is not a determining factor in run-time efficiency. 
et al 1998]). This type of diagnosis is called social diagno•  On the other hand, explicit reasoning about other agents is a 
sis, since it focuses on finding causes for failures to maintain key factor in determining run-time: Agents that reason explic•
designer-specified social relationships [Kaminka and Tambe,    itly about others incur significant computational costs, though 
2000].                                                         they are sometimes able to reduce the amount of communica•
  For instance, suppose a team of four robotic porters carry   tions. These results contrast with previous work in disagree•
a table, when suddenly one of the robots puts the table down   ment detection, in which distributed algorithms reduce com•
on the floor, while its teammates are still holding the table up. munications. 
Team-members can easily identify a disagreement, but they        The paper is organized as follows: Section 2 motivates the 
also need to determine its causes, e.g., that the robot believed research. Section 3 presents the diagnosis phases and alter•
the table reached the goal location, while its teammates did   native building blocks for diagnosis. Section 4 specifies diag-


370                                                                                                          DIAGNOSIS nosis methods which combine the building blocks in different   approach requiring high bandwidth is not likely to scale in 
ways, and evaluates them empirically. Section 5 presents re•   the number of agents [Jennings, 1995]. 
lated work, and Section 6 concludes. 
                                                               3 Building blocks for diagnosis 
2 Motivation and Examples                                      We distinguish two phases of social diagnosis: (i) select•
                                                               ing who will carry out the diagnosis; (ii) having the se•
Agreement (e.g., on a joint plan or goal) is key to establish• lected agents generate and disambiguate diagnosis hypothe•
ment and maintenance of teamwork [Cohen and Levesque,          ses. To explore these phases concretely, we focus on teams of 
 1991; Jennings, 1995; Grosz and Kraus, 1996; Tambe, 1997].    behavior-based agents. The control process of such agents is 
Unfortunately, complex, dynamic settings, sometimes lead       relatively simple to model, and we can therefore focus on the 
to disagreements among team-members (e.g., due to sensing      core communications and computational requirements of the 
failures, or different interpretations of sensor readings) [Del- diagnosis. 
larocas and Klein, 2000]. Given the critical role agreement      We model an agent as having a decomposition hierarchy of 
plays in coordinated, collaborative operation of teams, we fo• behaviors, where each behavior (node in the hierarchy) has 
cus on the diagnosis of disagreements in this paper.           preconditions (which allow its selection for execution when 
   The function of a diagnosis process is to go from fault de• satisfied), and termination conditions (which terminate its ex•
tection (where an alarm is raised when a fault occurs), to fault ecution if the behavior was selected, and the conditions are 
identification, where the causes for the fault arc discovered. satisfied). Each behavior may have actions associated with 
In diagnosing disagreements, the idea is to go beyond sim•     it, which it executes once selected. It may have child be•
ple detection that a disagreement exists, to identification of haviors, which it can select based on their matching precon•
the differences in beliefs between the agents, that lead to the ditions and preference rules. At any given time, the agent 
disagreement. Such differences in beliefs may be a result of   is controlled by a top-to-bottom path through the hierarchy, 
differences in sensor readings or interpretation, in sensor mal• root-to-leaf Only one behavior can be active in each level 
functions, or communication difficulties. Once differences in  of the hierarchy. Such a generic representation allows us to 
beliefs are known, then they can be negotiated and argued      model different behavior-based controllers (e.g. [Firby, 1987; 
about, to resolve the disagreements [Kraus et al., 1998]. Un•  Newell, 1990]). 
fortunately, diagnosis of such failures can be extremely ex•     We assume that a team of such agents coordinates through 
pensive both in terms of computation as well as in communi•    designer-provided definition of team behaviors, which are to 
cations (see Section 5 for quantitative evaluation), and thus a be selected and de-selected jointly through the use of commu•
key challenge is to minimize communications and computa•       nications or other means of synchronization. Team behaviors, 
tion.                                                          typically at higher-levels of the hierarchy, serve to synchro•
   To illustrate the problem we use an example, originally re• nize high-level tasks, while at lower-levels of the hierarchy 
ported in [Kaminka and Tambe, 2000], from the ModSAF           agents select individual (and often different) behaviors which 
domain (a virtual battlefield environment with synthetic he•   control their execution of their own individual role. 
licopter pilots). In the example a team of pilot agents is di•   For instance, in the ModSAF example, the scout should fly 
vided into two: scouts and attackers. In the beginning all     to look for the enemy, while the attackers should wait at the 
teammates fly in formation looking for a specific way-point    way-point. All agents are executing in this case a team be•
(a given position), where the scouts move forward towards      havior called wait-at-point, in service of which the attackers 
the enemy, while the attackers land and wait for a signal.     are executing individual just-wait (land) behaviors, while the 
Kaminka and Tambe [2000] report on the following failure       scout is executing a more complex fly-ahead behavior. 
case (which there failure detection technique captures): There   Disagreement between team-members is manifested by se•
are two attackers and one scout flying in formation, when the  lection of different team behaviors, by different agents, at the 
attackers detect the way-point and land, while the scout fails same time. Since team behaviors are to be jointly selected (by 
to detect way-point and continues to fly.                      designer specification), such a disagreement can be traced to 
   A diagnosis system, running on at least one of the agents,  a difference in the satisfaction of the relevant pre-conditions 
must now isolate possible explanations for the disagreement,   and termination conditions, e.g., agent A believes P, while 
of which the real cause (agents different in their belief that agent B believes -P, causing them to select different behav•
the way-point was detected) is but one. Each such explana•     iors. It is these contradictions which the diagnosis process 
tion is a diagnosis hypothesis since it reports a possible rea• seeks to discover. A set of contradictions in beliefs that ac•
son for the disagreement between the agents. Disambiguation    counts for the selection of different team behaviors by differ•
between these hypotheses seems very straightforward: Each      ent agents is called a diagnosis. 
agent can generate its own hypotheses, and then exchange 
these hypotheses with its teammates to verify their correct•   3.1 Disambiguating Diagnosis Hypotheses 
ness. Unfortunately, since each agent is potentially unsure of The first phase in the diagnosis process involves selection of 
what its team-members are doing, the number of hypotheses      the agent(s) that will carry out the diagnosis process. How•
can be quite large, which means communications will suf•       ever, the algorithms used for selection may depend on the 
fer greatly. Furthermore, having each agent simply report its  diagnosis process selected in the second phase (disambigua•
internal beliefs to the others, while alleviating communica•   tion), and so for clarity of presentation, we will begin by dis•
tions overhead somewhat, is also insufficient. Indeed, any     cussing the second phase. Assume for now that one or more 


DIAGNOSIS                                                                                                            371  agents have been selected to carry out the diagnosis process. (termination conditions of last behavior) (pre-conditions 
 The agents must now identify the beliefs of their peers.      of current behavior) (termination conditions of current 
   Perhaps the simplest algorithm for this is to have all team- behavior). 
 members send their relevant beliefs to the diagnosing agent     For instance, suppose an attacker is observed to have 
 (the diagnosing agent can inform the team-members of the      landed. The observer may conclude that either the halt or 
 detection of a disagreement to trigger this communication).   wait-at-point behavior has been selected. Furthermore, the 
 In order to prevent flooding the diagnosing agent with irrele• observer can conclude that the termination conditions for 
 vant information, each team-member sends only beliefs that    halt do not hold, or that the termination conditions for wait-
 are relevant to the diagnosis, i.e., only the beliefs that are as• at-point do not hold. If the observed action indicates that 
sociated with its currently selected behavior.                 the agent has just transitioned into the behavior associated 
   Upon receiving the relevant beliefs from all agents, the    with landing, then the preconditions of halt and wait-at-point 
generation of the diagnosis proceeds simply by comparing all   would also be inferred as true. 
beliefs of team-members to find contradictions (e.g., agent A    Once the hypotheses are known, the agent can send tar•
believes P, while agent B believes ->P). Since the beliefs of  geted queries to specific agents in order to disambiguate the 
the other agents are known with certainty (based on the com•   hypotheses. The queries are selected in a manner that mini•
munications), the resulting diagnosis must be the correct one. mizes the expected number of queries. Intuitively, the agent 
However, having all agents send their beliefs, may severely    prefers to ask first about propositions whose value, when 
impact network load.                                           known with certainty, will approximately split the hypothe•
   We thus propose a novel selective monitoring algorithm,     ses space. 
in which the diagnosing agent controls the communications, 
by initiating targeted queries which are intended to minimize  3.2 Selecting a Diagnosing Agent 
the amount of communications. To do ihis, the diagnosing       Let us now turn to the preceding phase, in which the agents 
agent builds hypotheses as to the possible beliefs held by     that will carry out the diagnosis are selected. Several tech•
each agent, and then queries the agents as necessary to dis•   niques are available. First, a design-time selection of one of 
ambiguate these hypotheses.                                    the agents is the most trivial approach. It requires a failure 
   This process begins with RESL, a previously-published be•   state to be declared in the team, such that the selected agents 
havior recognition and belief inference algorithm [Kaminka     know to begin their task. Of course, one problem with this ap•
and Tambe, 2000], which is only presented here briefly as a    proach is that it requires all agents to be notified of the failure. 
reminder. Under the assumption that each agent has knowl•      A second technique that circumvents this need is to leave the 
edge of all the possible behaviors available to each team-     diagnosis in the hands of those agents that have detected the 
member, i.e., their behavior library (an assumption com•       failure, and allow them to proceed with the diagnosis without 
monly made in plan recognition), each observing agent cre•     necessarily alerting the others unless absolutely necessary. 
ates a copy of the fully-expanded behavior hierarchy for each    We present a novel third approach, in which selection of 
of its teammates. It then matches observed actions with the    the diagnosing agent is based on its team-members' estimate 
actions associated with each behavior. If a behavior matches,  of the number of queries that it will send out in order to arrive 
it is tagged. All tagged behaviors propagate their tags up the at a diagnosis, i.e., the number of queries that it will send out 
tree to their parents (and down to their children) such as to  in the disambiguation phase of the diagnosis (previous sec•
tag entire matching paths: These signify behavior recogni•     tion). The key to this approach is for each agent to essentially 
tion (plan recognition) hypotheses that are consistent with the simulate its own reasoning in the second phase, as well as 
observed actions of the team-member.                           that of its teammates. Agents can then jointly select the agent 
   Once hypotheses for the selected behavior of an agent are   with the best simulated results (i.e., the minimal number of 
known to the observer, it may infer the possible beliefs of    queries). 
the observed agent by examining the pre-conditions and the       Surprisingly, all agents can make the same selection with•
termination conditions of the selected behavior. To do this,   out communicating, using a recursive modelling technique in 
the observer must keep track of the last known behavior(s)     which each agent models itself through its model of its team•
hypothesized to have been selected by the observed agent.      mates. This proceeds as follows. First, each agent uses the 
As long as the hypotheses remain the same, the only general    belief recognition algorithm to generate the hypotheses space 
conclusion the observer can make is that the termination con•  for each team-member other than itself. To determine its own 
ditions for the selected behaviors have not been met. Thus it  hypotheses space (as it appears to its peers), each agent uses 
can infer that the observed agent currently believes the nega• recursive modelling, putting itself in the position of one of 
tion of the termination conditions of selected behaviors.      its teammates and running the belief recognition process de•
  When the observer recognizes a transition from one be•       scribed above with respect to itself. Under the assumptions 
havior to another, it may conclude (for the instant in which  that all agents utilize the same algorithm, and have access to 
the transition occurred) that the termination conditions of the the same observations, an agent's recursive model will yield 
previous behavior, and the pre-conditions of the new behav•   the same results as the modelling process of its peers. At this 
ior are satisfied. In addition, again the termination conditions point, each team-member can determine the agent with the 
of the new behavior must not be satisfied; otherwise this new  minimal number of expected queries (the queries that split 
behavior would not have been selected. Therefore, the beliefs  the space of the queries). In order to guarantee an agreement 
of the observed agent (at the moment of the transition) are:   on the selected agent, each team-member has an ID number, 


372                                                                                                          DIAGNOSIS which is determined and known in advance. In case there       members. 
are two agents with the same minimal number of expected          Method 3. The next design choice we examine corre•
queries, the agent with the minimal ID is selected. This en•  sponds to a diagnosis algorithm, in which the designer pre•
tire process is carried out strictly based on team-members'   selects a neutral agent. When a failure is discovered (and is 
observations of one another, with no communications other     made known to all agents), all team-members communicate 
than an announcement of a disagreement.                       all their relevant beliefs to the pre-selected agent. 
  For instance, suppose there are three agents A, B and C. To    Method 4. A final algorithm attempts to alleviate the 
determine the diagnosing agent, A puts itself in B's position communications overhead. It uses the recursive modelling 
and considers the hypotheses B has about A and C, given A's   technique to have all team-members agree on which agent 
observable actions. A also uses the belief recognition pro•   is to carry out the diagnosis (this requires the detection of 
cess described earlier to determine the number of hypotheses  the disagreement to be made known). Once the agent is se•
available about B's beliefs, C's beliefs, etc. It now simulates lected (with no communications), it queries its teammates as 
selecting queries by each agent, and selects the agent (say,  needed. 
C) with the minimal number of expected queries. B, and C         Table 1 summarizes the different methods. Each method 
also run the same process, and under the assumption that each is presented in a different row. The columns correspond to 
agent's actions are equally observable to all, will arrive at the the different phases of the diagnosis process. The choice of 
same conclusion.                                              algorithm is presented in each entry, along with a marking 
  Summary. We presented a space of social diagnosis algo•     that signifies the number of agents that execute the selected 
rithms: Each such algorithm operates in two phases, and we    technique for the phase in question. 
presented alternative techniques for each phase. For the selec•
tion of the diagnosing agent, we have the following methods: 
(i) rely on pre-selection by the designer; (ii) let the agents that 
detected the fault do the diagnosis; or (iii) choose the agent 
most likely to reduce communications (using the distributed 
recursive modelling technique described). In terms of com•
puting the diagnosis, two choices are available: Either have 
all agents communicate their beliefs to the selected agents, or    Table 1: Summary of evaluated diagnosis methods 
allow the diagnosing agents to actively query agents as to the 
state of their beliefs, while minimizing the number of queries   For instance row 2 should be read as follows: In method 2, 
as described above.                                           the agents selected to perform the disambiguation are those 
                                                              who detected the disagreement. K such agents exist (where 
4 Evaluation and Discussion                                   K is smaller or equal than the total number of agents in the 
                                                              team, N), and they each execute a minimal-queries algorithm. 
The design alternatives define a space of diagnosis algo•     In contrast, row 3 indicates that a single pre-selected agent 
rithms. This section evaluates four intuitive design decisions executes the diagnosis, and it relies on reports from all agents 
in this space, and draws lessons about the effects of specific to carry out the diagnosis. 
design choices on overall computation and communication          Focusing on diagnosing teams of behavior-based agents in 
overhead.                                                     the ModSAF domain, we performed experiments in which 
  Method 1. The first design choice corresponds to arguably   the different diagnosis methods were systematically tested on 
the most intuitive diagnosis algorithm, in which all agents   thousands of failure cases, varying the number of agents, the 
are pre-selected to carry out the diagnosis. When a failure is behaviors selected by each agent, and the roles of the agents. 
detected (and is made known to all agents) each agent com•    The experiments were executed with teams of two to ten 
municates all its relevant beliefs to the others so that each and agents. For each n agents there are three sets of tests: (1) 
every team-member has a copy of all beliefs, and therefore    one attacker and n-1 scouts; (2) n-1 attackers and one scout; 
can do the disambiguation itself.                             (3) n/2 attackers and n/2 scouts. For each set of x attackers 
  Method 2. Method 1 uses redundant communications to         and y scouts, we systematically checked all possible disagree•
achieve this distribution, while arguably only a single agent ment cases for all team behaviors (a total 2372 tests for each 
really needs to have the final diagnosis in order to begin a  method). In each test we recorded the number of messages 
recovery process. Thus in method 2, the agents that de•       sent, and runtime of the diagnosis process. 
tected the disagreement automatically take it upon themselves    In Table 2 we present the results of a single test, where 
to carry out the diagnosis, unbeknownst to their teammates,   one scout and two attackersy?y in formation, when the scout 
and to each other. Because their team-mates do not know of    transitions to the wait-at-point behavior while the attackers 
the disagreement (or who has been selected to diagnose it),   continue to fly. The diagnosis is that the scout detected the 
they cannot rely on their team-mates to communicate their     way-point, while the attackers did not. 
beliefs without being queried (in phase 2). Instead, they       The first column in Table 2 reports the method used. The 
use the querying algorithm discussed in the previous section. second column presents the number of messages sent re•
However, because the diagnosing agents also do not know       porting on beliefs, or querying about their truth (one mes•
of each other, in cases where more than one agent detects     sage per belief). The third column reports the number of 
the disagreement, all such agents will query the other team-  messages sent informing agents of the existence of failures 


DIAGNOSIS                                                                                                            373  (we assume point-to-point communications). The next three 
 columns summarize the runtime of each agent. 


     Table 2: results of diagnosing a specific failure case 

   For instance, the number of messages reporting on beliefs 
 for method 3 is 8, and 2 failure messages were sent (i.e., one 
 of the agents detected the failure and informed the others). 
 The runtime of all the teammates for method 3 is 3 millisec•
 onds, except for the scout, which disambiguated the beliefs in        Figure 2: Average run-time per failure case 
 this case, and therefore took an additional 6 milliseconds (for 
 a total of 9 milliseconds). 
                                                               their run-time is much smaller than methods (here, methods 
                                                               2 and 4) which hypothesize about the beliefs of others. How•
                                                               ever, as Figure 1 shows, the goal of reducing communications 
                                                               is actually achieved, as method 4 does indeed result in less 
                                                               communications then method 3. The question of whether the 
                                                               cost in run time is worth the reduction in communications is 
                                                               dependent on the domain. 
                                                                 Indeed, we draw a second conclusion from Figure 1. De•
                                                               spite the additional savings provided by the minimal query 
                                                               algorithm, the choice of a centralized diagnosing agent is the 
                                                               main factor in qualitatively reducing the number of messages 
                                                               sent, as well as in shaping the growth curve as the number of 
                                                               agents is scaled up. These results contrast sharply with pre•
                                                               vious work in disagreement detection, in which distributed 
                                                               algorithms reduce communications [Kaminka and Tambe, 
                                                               2000]. 

                                                               5 Related Work 
   Figure 1: Average number of messages per failure case 
                                                               While diagnosis of a single-agent system is relatively well 
   Figures 1 and 2 summarize the results of the experiments.   understood, and known to be computationally difficult, social 
 In both figures, the x axis shows the number of agents in the diagnosis (diagnosis of social failures such as disagreements 
diagnosed team. Figure 1 presents the average number of be•    and coordination failures) remains an open area of research. 
 lief messages for the different failure and role variations, for In particular, to our best knowledge, an in-depth exploration 
each team size (failure messages were ignored in the figure,   of design choices in terms of communication and computa•
since their effect is negligible). Figure 2 presents the average tion has not been done before. The most closely-related work 
run-time (in milliseconds) of those same tests. The run-time   to ours is reported in [Kaminka and Tambe, 2000]. This pre•
of each test was taken as the maximum of any of the agents     vious investigation provides guarantees on detection of dis•
 in the test.                                                  agreements, but only presented a heuristic approach to di•
   Both figures show interesting grouping of the evaluated     agnosis, which indeed does not always succeed. The algo•
techniques. In Figure 1 (number of messages), methods 3        rithms we present here succeed in the same examples where 
and 4 show a slow, approximately-linear growth, while meth•    the previous heuristic approach fails. Parker [1998] reports 
ods 1 and 2 show a much faster non-linear growth. In Figure    on a behavior-based architecture which is very robust and is 
2 (runtime), the grouping is different: Methods 1 and 3 grow   able to recover from failures by having robots take over tasks 
significantly slower than methods 2 and 4.                     from failing teammates. This is done using continuous com•
   The first conclusion we draw from these figures is that run• munications, but with an out explicit diagnosis process such 
time is affected by the choice of a disambiguation method      as those described in this paper. 
(Figure 2, ref. Table 1). Methods (here, methods 1 and 3)        Dellarocas and Klein [2000] described failures handling 
that rely on the agents to report their relevant beliefs do not services that use a knowledge base of generic exceptions and 
reason about the hypothesized beliefs of others. Therefore,    a decision tree of diagnoses; the diagnosis process is done by 


374                                                                                                          DIAGNOSIS 