

A  Machine     Learning     Approach     to Identiﬁcation     and   Resolution    of One-Anaphora


                                                             ¡                      ¡
                                               
                    Hwee   Tou Ng   , Yu Zhou  , Robert Dale  and  Mary   Gardiner


              Department  of Computer  Science, National University of Singapore, Singapore
               ¡


               Centre  for Language  Technology, Macquarie   University, Sydney, Australia


                  ¢                                  ¢
                                                                     £
                   nght,zhouyu  £ @comp.nus.edu.sg,   rdale,gardiner  @ics.mq.edu.au


                    Abstract                          stance of one-anaphora, we must identify the antecedent tex-
                                                      tual material that provides the semantic content alluded to by
    We present a machine learning approach to iden-   the use of one. In Example (1), correctly interpreting the Eu-
    tifying and resolving one-anaphora. In this ap-   ropean one requires inferring that it has a relationship to the
    proach, the system ﬁrst learns to distinguish dif- noun phrase the Atlantic connection. The range of semantic
    ferent uses of instances of the word one; in the  relationships that can hold between an one-anaphoric form
    second stage, the antecedents of those instances of and its antecedent is broad and complex; for present purposes,
    one that are classiﬁed as anaphoric are then deter- we are interested in identifying the antecedent noun phrase
    mined. We evaluated our approach on written texts that contains the relevant semantic content.
    drawn from the informative domains of the British
                                                        We  designed two systems that are applied consecutively
    National Corpus (BNC), and achieved encouraging
                                                      to identify and resolve one-anaphora. In the case of Exam-
    results. To our knowledge, this is the ﬁrst learning-
                                                      ple (1), our one-expression classiﬁer ﬁrst determines that the
    based system for the identiﬁcation and resolution
                                                      European one is in fact a one-anaphor; then our one-anaphora
    of one-anaphora.
                                                      resolver would identify the noun phrase the Atlantic connec-
                                                      tion as the antecedent of the one-anaphor. In this paper, an
1  Introduction                                       antecedent is deﬁned as the noun phrase from which the head
                                                      sense of the one-anaphor can be determined.
The word one is a frequently used word in English: in the
100-million-wordBritish National Corpus (BNC), it accounts The pervasiveness of anaphoric reference in general means
for 0.26% of all the words, ranking as the 110th most com- that anaphora resolution is recognized as an important sub-
mon word, and the 14th most common pronominal form,   task in natural language processing; one-anaphora resolution,
more frequent than the pronoun us.1 Not all of these instances however, has been relatively neglected. As noted, the fre-
of one are anaphoric; the most common use is as a number (as quency of occurrence of the word one means that any real
in one book), and there are a number of other uses. However, NLP system cannot just ignore it; however, the knowledge
any natural language processing (NLP) system which tries to sources (typically, features of the linguistic context) used in
process anaphora in unrestricted text will need to be able to state-of-the-art noun phrase coreference resolution systems
                                                           [                                  ]
determine whether a particular instance of one is being used (e.g., Soon et al., 2001; Ng and Cardie, 2002 ) are not ap-
anaphorically, and when this is the case, what the antecedent plicable to one-anaphora. Consequently, one-anaphora reso-
of the anaphor is.                                    lution is a task that requires special attention.
  In brief, a one-anaphor is an anaphoric noun phrase (NP) The rest of this paper is organized as follows. We ﬁrst in-
headed by the word one, as in the following example:  troduce the various uses of the word one in English and the
                                                      different types of antecedents of one-anaphora. Then we give
(1) Her greater sympathy for the Atlantic connection over an overview of how our one-expression classiﬁer and one-
    the European one is not widely shared by colleagues. anaphora resolver are applied consecutively to identify and
One-anaphora is sometimes referred to as “identity-of-sense resolve one-anaphora, followed by two separate sections giv-
anaphora”, in contrast to the more common pronominal  ing a detailed description and evaluation of the two systems.
“identity-of-reference” anaphora [Hankamer and Sag, 1976]. In the ﬁnal two sections, we describe related work and con-
In processing a pronominal reference, we are generally look- clude.
ing for an antecedent noun phrase that refers to the same en-
tity as the anaphoric form; however, one-anaphoric forms are
generally used to refer to something of the same kind as some- 2 Classes of One-Expressions
thing mentioned before. Hence, in order to interpret an in-
                                                      In this section, we introduce the various uses of one in English
  1These ﬁgures are based on Adam Kilgarriff's frequency lists at and the different types of antecedents of one-anaphora, along


http://www.itri.brighton.ac.uk/ ¤ Adam.Kilgarriff/bnc-readme.html. with some statistics of their distribution in the BNC.2.1  Uses of One and Corpus  Annotation                              Class     Frequency  %
                                                                  Numeric           739   46.9
The taxonomy of uses of the word one adopted in this sec-
                                                                  Partitive         399   25.3
tion is based on existing theories of the various uses of         Anaphoric         240   15.2
one [Halliday and Hasan, 1976; Webber, 1979; Dahl, 1985;          Generic           167   10.6
Luperfoy, 1991] and our own study of one-expressions. We          Unclassiﬁable      25    1.6
divided uses of one into six classes: Numeric, Partitive,         Idiomatic           7    0.4
Anaphoric, Generic, Idiomatic, and Unclassiﬁable.                 Total           1,577  100.0
 1. Numeric One: Modiﬁes a head noun to indicate singu-
    larity as in Example (2); this is the only adjectival use of Table 1: Distribution of uses of one in the annotated corpus.
    one.
(2) John has one blue T-shirt.
 2. Partitive One: Selects an individual from a set. It is fol-
    lowed by an of-prepositional phrase headed by a plural
    noun or pronoun as in Example (3).
(3) A special exhibition of books for children forms one of
    the centrepieces of the 41st annual Frankfurt Book Fair.
 3. Anaphoric One: Relates a set of properties to the set
    of properties mentioned by the antecedent. There are    Figure 1: How training and test data are used.
    three types of one-anaphors, distinguished by the type
    of their antecedents. First, the antecedent may be a
    kind, as in the noisy cameras of Example (4); second, 2.2 Annotating Antecedents of One-Anaphora
    the antecedent may be a set of entities, as in the set of We annotated the 240 examples of one-anaphors in our cor-
    two World Bank men of Example (5); and third, the an- pus a second time, marking the antecedent in each case. Not
    tecedent may refer to a single instance, as in this book of every one-anaphor has an explicit noun phrase antecedent as
    Example (6).                                      shown in the previous anaphoric examples; in such cases, the
(4) I have an aversion to noisy cameras, and this one rings reader has to infer the nature of the antecedent from the avail-
    several decibels before it's done with winding on the able text. We label those cases where there is no explicit an-
    ﬁlm.                                              tecedent as “one-anaphors with implicit antecedents”. The
(5) The two World Bank men, one German and one British, remaining one-anaphors are “one-anaphors with explicit an-
    strode across the tarmac.                         tecedents”. Of the 240 one-anaphors, 98.3% had explicit an-
                                                      tecedents.
(6) Would you like this book? Yes, I would like that one.
 4. Generic One: A pronominal use that refers to a generic 3 An Overview of Our Approach
    person or to the speaker of a sentence; often used in sub-
    ject position followed by a modal verb and a main verb Before we describe our two machine learning systems for
    that takes an animate subject.                    one-expression classiﬁcation and one-anaphora resolution,
                                                      we ﬁrst provide an overview of how the two systems are ap-
(7) One must think a little deeper to discover the underlying
                                                      plied consecutively to accomplish the overall task of identify-
    social roots of the problem.
                                                      ing and resolving one-anaphora. As an example, we use one
 5. Idiomatic One: Conventionalized uses whose seman- trial in the 10-fold cross validation to illustrate the process.
    tics appear not to be based on general use, but rather on Here, the identiﬁcation of one-anaphora is Step 1 (S1), and
    idiomatic patterns, as in Example (8).            the resolution of one-anaphora is Step 2 (S2).
(8) It would be perfect to have a loved one accompany me As shown in Figure 1, a gold standard corpus is divided
    in the whole trip.                                into two sets, containing 90% and 10% of the data respec-
 6. Unclassiﬁable One: Inevitably, there are instances tively. The larger set is used to train the S1 classiﬁer. In the
    which are difﬁcult to classify as any of the above, as in S1 training data, there is a set of anaphoric examples, Ana.
    Example (9).                                      This set is used to train the S2 classiﬁer.
                                                        The smaller set of 10% of the examples is used for test-
(9) Cursed be every one who curses you.               ing. First, the test examples are passed to the S1 classiﬁer,
For the present study, we randomly selected 1,577 one ex- which identiﬁes a set of anaphoric examples, Ana*. This set
pressions from the BNC 2, and manually annotated these with may contain errors in identiﬁcation (a one-anaphor not clas-
the six classes above. The distribution of each class in the siﬁed as anaphoric, or a non-anaphoric example classiﬁed as
annotated corpus is shown in Table 1, and it mirrors the dis-
tribution of one expressions in naturally occurring text. ceived as literary or creative) are not considered because they con-
                                                      tain a large amount of dialog, which makes one-anaphora resolu-
  2These one-expressions are from written text in the informative tion a harder task. This restriction follows the genre of MUC texts
domains of BNC. Spoken text and written text in the imaginative [MUC-7, 1997], a widely used data set in noun phrase coreference
domain (i.e., texts which are ﬁctional or which are generally per- research, which also only included written newspaper texts.anaphoric). Ana* is in turn passed to the S2 classiﬁer, which  Num in  Num       Num      R    P     F1
outputs the antecedents it ﬁnds, denoted as Antecedent*.       data set identiﬁed correct
When calculating accuracy, Ana* and Antecedent* are used NUM   739     730       693     93.8 94.9  94.3
to compare with the gold standard annotation to measure both PAR 399   412       390     97.7 94.7  96.2
the S1 accuracy and the overall accuracy of the combined sys- ANA 240  281       192     80.0 68.3  73.7
                                                        GEN    167     152       127     76.0 83.6  79.6
tems.                                                   Sub    1,545   1,575     1,402   90.7 89.0  89.8
                                                        Total
4  One-Expression    Classiﬁcation                      UNC    25      2         0        0    0     —
Our  one-expression classiﬁer classiﬁes a given one-    IDIO   7       0         0        0    —     —
expression into one of the six classes listed in Table 1. Total 1,577  1,577     1,402   88.9 88.9  88.9
4.1  The Features                                           Table 2: Accuracy of one-expression classiﬁcation.
We devised a set of features that is useful in determining
which of the six classes a given one-expression belongs to. MD POS tag, such as must). The possible values for this
We focus on the classes Numeric, Partitive, Anaphoric, and feature are true, false, or NA when one is not the subject.
Generic, since 98% of the instances are from these classes. Example (7) would get true for this feature.
  The Numeric use of one is the simplest case, since it is the
                                                        6. positionInNP indicates the position of the word one in
only adjectival use in English. It can be readily identiﬁed by
                                                          its host NP. It has four possible values: SingleOne when
the part-of-speech (POS) tag of the word one. Partitive use of
                                                          the NP only consists of the word one; Leftmost when
one can be identiﬁed by checking the syntactic context (i.e.,
                                                          the NP has multiple words and one is the leftmost word;
[one [of NP ]]).
         pl                                               Rightmost when the NP has multiple words and one is
  Discrimination between the Anaphoric and Generic classes the rightmost word; and Middle otherwise. For example:
is more complex. We used the three features isSubj, isAnimat-
eVerb, and isModalVerb to identify instances of the Generic a) SingleOne: [One] might suppose that . . .
class, in line with the observations mentioned in the deﬁnition b) Leftmost: I was concerned with [one thing] only.
of Generic One . We also noticed that the relative position of c) Rightmost: [the European one]
one in its host NP and the category of the word immediately d) Middle: on [the one hand]


preceding one both give strong hints as to the function of one
                                                              ¡



in its host NP. Therefore, we added two more features, posi- 7. W   POS is the POS tag of the word immediately pre-
               ¡

tionInNP and W   POS, to assist in classiﬁcation.         ceding one. Its possible values are the 45 POS tags in
  We experimented with a total of 7 features as described be- Penn TreeBank tagset and NA when one is the ﬁrst word
low. Each instance in the training and test data set is thus of the sentence.
represented as a feature vector of seven values. The feature The learning algorithm used in our one-expression classiﬁ-
                                             [
values are acquired by running the Charniak Parser Char- cation system is C4.5 [Quinlan, 1993]. This is a commonly
         ]
niak, 2000 over the corpus, in combination with information used decision tree learning algorithm and may be considered
                                 [             ]
about verbs of cognition from WordNet Fellbaum, 1998 : as a baseline against which other learning algorithms can be


 1. W ¢ POS is the POS tag of one assigned by the Charniak compared.
    Parser. Its possible values are CD, NN, NNP, PRP.
                                                      4.2  Evaluation and  Error Analysis
 2. isOfPlural checks whether one is followed by of prepo-
    sitional phrase with a plural head noun/pronoun. Its pos- Evaluation
    sible values are plural, notPlural, or NA when the word Table 2 gives the 10-fold cross validation results for the one-
    one is not followed by a PP headed by of. For example: expression classiﬁer for each of the 6 classes.
                                                        The accuracy of identifying one-anaphora is 73.7%, which
     (a) plural: One of the patients survived.
                                                      is not as high as the Numeric, Partitive, and Generic classes.
     (b) notPlural: The problem was the unusual one of a The remaining two classes, Idiomatic and Unclassiﬁable, are
        warmish, wet spring.                          poorly discriminated; when counted in, these pull down the
 3. isSubj checks whether the word one is in subject posi- overall classiﬁcation accuracy from 89.8% to 88.9%.
    tion. This feature value is inferred from the parse tree
    structure; its possible values are true or false. Error Analysis
                                                      Table 3 provides a matrix of the number of misclassiﬁcations
 4. isAnimateVerb checks whether the lemmatized verb  in each class. Row 4 and Column 4, highlighted, show that
    following subject one is a verb of cognition according the major confusion with the Anaphoric class comes from the
    to WordNet: some examples are think, judge, analyze, Generic and Numeric classes, with Partitive making a smaller
    and doubt. The possible values for this feature are true, contribution to the erroneous classiﬁcations.
    false, or NA when one is not the subject. Example (7) Our classiﬁer's performance is highly impacted by the ac-
    would get true for this feature.                  curacy of POS tagging and parsing: failures here caused most
 5. isModalVerb checks whether the verb phrase following of the confusion with Numeric and Partitive. For example,
    a subject one contains a modal verb (i.e., a word with one in (4) is classiﬁed as Numeric because rings is wrongly

 classiﬁed NUM*  PAR*  ANA*    GEN*   UNC*    IDIO*   Test Data
    ¡

 as   *
 NUM       na     2      35      8      1      0      In each trial, instances of one identiﬁed as being anaphoric in
 PAR       3      na     6       0      0      0      the test data of Step 1 were used to create Step 2 test instances
 ANA      14      18     na     15      1      0      in the corresponding trial as shown in Figure 1.
 GEN       9      0      31     na      0      0        Creation of test instances consisted of two steps. The ﬁrst
 UNC      10      1      12      2      na     0      step is the same as that for the training data; in the second
 IDIO      1      1      5       0      0      na     step, every base NP preceding the instance of one is a poten-
                                                      tial antecedent, so each of these NPs was paired with one.
       Table 3: One-expression classiﬁer error matrix.  When  doing testing, the one-anaphora resolution algorithm
                                                      starts from the immediately preceding base NP and proceeds
tagged as NNS; and Partitive one in the phrase one or more of backward in the reverse order of the NPs in the context until
them is classiﬁed as Anaphoric because it is wrongly parsed there is no remaining NP to test, or an antecedent is found
as [[one] or [more of [them]]].                       (i.e., the classiﬁer returns true).
  The confusion with the Generic class is mainly caused by
occurrences of anaphoric one as subject and generic one in 5.2 The Features
non-subject position. Such cases can also be confusing for To decide whether the anaphor in a given antecedent-anaphor
human readers: in a sentence like One is usually shunting pair refers to the candidate antecedent, we need features that
around in the yard, we might think we have a generic use, but show a preference for good candidates.
the one may refer to a previously mentioned locomotive. As with pronominal anaphora, an intuitively appealing fea-
                                                      ture to use is whether the candidate antecedent NP is in focus
5  One-Anaphora     Resolution                        [Sidner, 1981; Vieira and Poesio, 2000]. In anaphora res-
Our one-anaphor resolver attempts to identify the NP in the olution, two commonly used features for approximating the
preceding linguistic context that provides the semantic con- notion of focus are syntactic role and recency: a candidate
tent required for interpretation of a one-anaphor; we refer to that ﬁlls a salient syntactic role such as subject, or one that is
this NP as the antecedent NP.                         very recent, is often the focus of the discourse. We used four
                                                      features to measure this type of information: AnteIsSubj, An-
5.1  Experimental  Data                               teInRelClause, AnteIsNearestNP, and bothInPP. The feature
                                                      bothInPP allows us to take syntactic parallelism into consid-
We trained and tested a one-anaphor resolution classiﬁer us-
                                                      eration. The POS tag of the head word of the candidate an-
ing a set of positive/negative antecedent-anaphor pairs. A pair
                                                      tecedent is also a good feature in ﬁltering out improper can-
is positive when the candidate antecedent in this pair is the
                                                      didates: a proper noun should not be the antecedent of a one-
actual antecedent of the anaphor; otherwise, it is negative.
                                                      anaphor [Dahl, 1985].
Training Data                                           We  used a total of 5 features as described below. Their
In each trial of the 10-fold cross validation, the actual one- values are acquired from the Charniak Parser output of the
anaphora instances in the gold standard training corpus of training/test data set.
Step 1 are used to create Step 2 training instances in the cor-
                                                        1. hwPOSofAnte  is the POS tag of the head word of the
responding trial as shown in Figure 1.
                                                          candidate antecedent. Its possible values are the set of
  Creation of training instances consisted of three steps.
                                                          POS  tags of head words that occur in our training/test
First, each sentence containing one and its three preceding
                                                          data. In this paper, the head word of the candidate an-
sentences 3 were processed by RM NP chunker [Ramshaw
                                                          tecedent is deﬁned as the rightmost noun, or the right-
and Marcus, 1995] to carry out NP chunking. Second, each
                                                          most word if no noun is found, in the base NP candidate
pair of an anaphor and its actual antecedent were used to cre-
                                                          antecedent.
ate a positive training instance. Lastly, to generate negative
training instances, anaphors were paired with each of the NPs 2. bothInPP checks whether the candidate antecedent and
that appeared between the anaphor and its real antecedent. the one-anaphor are both in prepositional phrases (PP),
  In our experiment, we further adjusted the ratio of posi- and identiﬁes the types of PPs they are in. It has ﬁve
tive to negative instances in the training data by controlling possible values: NA when the candidate antecedent is
the number of negative instances randomly picked from the not in PP; OnlyAnteInPP when the candidate antecedent
whole set of negative instances. We decided to set the ratio is in PP, but the one-anaphor is not in PP; SharePP when
at 1:1, which introduces no preference for either one of the the candidate antecedent and the one-anaphor are in the
two assignments to the classiﬁer. This procedure produced same PP; CommonPreposition when both candidate an-
a set of 472 antecedent-anaphor pairs in total, of which 236 tecedent and one-anaphor are in different PPs, but the
(50%) were positive instances. As already noted, each trial prepositions of the two PPs are the same; and Differ-
used roughly 90% of this set as training data.            entPreposition when both candidate antecedent and one-
                                                          anaphor are in different PPs with different prepositions.
  3If the sentence containing one appeared close to the beginning
of the text and there were less than three preceding sentences, we 3. AnteIsSubj checks whether the candidate antecedent is
used all available preceding sentences.                   in subject position. Its possible values are true or false. 4. AnteIsNearestNP checks whether the candidate an-  wrongly given to closer and more salient candidates. In Ex-
    tecedent is the nearest NP preceding the one-anaphor. ample (10), the director has a strong syntactic preference and
    Its possible values are true or false.            it is considered before moustache, so the system wrongly re-
 5. AnteInRelClause checks whether the candidate an-  turns the director even before checking the actual antecedent.
    tecedent is in a relative clause. Its possible values are (10) He has this ridiculous moustache. Ken Russell, the di-
    true or false.                                        rector, insisted I grew one of my own, rather than wear a
  Again, the learning algorithm used in our one-anaphora  false one, so that I looked completely convincing.
resolution engine is C4.5.
                                                        We expect that such mistakes could be corrected by using
5.3  Evaluation and Error Analysis                    semantic features.
Evaluation
In order to evaluate the overall performance of our approach 5.4 The Contribution of the Features
to one-anaphora identiﬁcation and resolution, we conducted a
10-fold cross validation of the one-anaphora resolver, where To evaluate the relative contribution of the various knowledge
each trial is performed based on the result of our one- sources to the overall accuracy of one-anaphora identiﬁcation
expression classiﬁer in Step 1. The overall recall, precision, and resolution, we ran a series of leave-one-out classiﬁers,
and F-measure are presented in Table 4, together with the ac- where we ﬁrst used all Step 1 features and disabled one Step 2
curacy of one-anaphora identiﬁcation from Step 1.     feature at a time; then we used all Step 2 features and disabled
  The “S2 correct” in Table 4 Column 9 is the sum of two one Step 1 feature at a time. The contribution of the features
values. The ﬁrst value is the number of hits in correctly iden- measured in terms of the overall F-measure is shown in Table
tifying the explicit antecedent of a one-anaphor. A hit of this 4 Column 12.
type occurs when a one-anaphor with an explicit antecedent The critical features which cause a substantial reduction of
is correctly identiﬁed as anaphoric in Step 1 and the actual an- overall F-measure when disabled are Step 1 features position-

tecedent is found in Step 2. The second value is the number of InNP, isOfPlural, and W ¢ POS, as well as Step 2 features hw-
hits returning no antecedent for a one-anaphor when it has no POSofAnte and bothInPP. The remaining features only have
annotated antecedent. A hit of this type occurs when a one- a small impact on the overall F-measure.
anaphor without an explicit antecedent is correctly identiﬁed
as anaphoric in Step 1, and none of the candidate antecedents 6 Related Work
is accepted in Step 2.
  The overall recall is the sum in “S2 correct” divided by The literature on one-anaphora is small; we cited the most
the total number of one-anaphors with or without explicit signiﬁcant works in the area in Section 2. Most of the existing
antecedent in the data set (Column 1); the overall precision literature is more concerned with describing the phenomenon
is the sum in “S2 correct” divided by the total number of than in determining how it might be handled automatically.
one-anaphors with or without explicit antecedent identiﬁed in There is, of course, an extensive literature on computa-
Step 1 (Column 3). The F-measure ﬁnally achieved is 45.7%. tional techniques for resolving pronominal anaphora, going
  We compared two baseline heuristics with our accuracy: back to at least the 1970s. Of the more recent research in
nearestNP and nearestSubj, which always assign the nearest the area, important work is that of Lappin and Leass [1994]
NP or Subject preceding the one-anaphor as its antecedent. and Kennedy and Boguraev [1996], who provided heuristics
The two baseline accuracies are calculated by applying the that could be used to determine the antecedents of pronom-
two heuristics on the set of anaphoric ones identiﬁed in inal forms. Soon et al. [2001] and Ng and Cardie [2002]
Step 1; nearestNP (nearestSubj) heuristics achieved 28.8% used a machine learning approach for coreference resolution.
(23.8%) accuracy, considerably lower than the overall accu- However, most of the linguistic features used in the work on
racy of our system.                                   pronominal anaphora are not applicable to the one-anaphora
Error Analysis and Future Improvement                 problem. Markert et al. [2003] focused on the related phe-
Since we perform one-anaphora resolution on the output of nomenon of other-anaphora.
our one-anaphora identiﬁcation system, the 45.7% overall ac-
curacy is a combination of both Step 1 and Step 2 perfor- 7 Conclusion
mance. Errors introduced in Step 1 were never remedied in
Step 2, and they directly affected both overall recall and preci- In this paper, we have presented a machine learning ap-
sion. In other words, given a perfect Step 2 classiﬁer to work proach to the identiﬁcation and resolution of one-anaphora
on the current one-anaphora identiﬁcation output, the highest and achieved encouraging results; to our knowledge, this is
overall F-measure achievable is 73.7%. Therefore, any fur- the ﬁrst learning-based system for resolving one-anaphora.
ther improvement in Step 1 performance would signiﬁcantly There is scope for reﬁnement to improve both the accuracy
improve the overall accuracy. This could be done by adding of identifying anaphoric uses of one, and of identifying the
features to improve the performance of Step 1, or adding a antecedent noun phrase that contains the semantic content re-
remedy strategy in later processing.                  quired for interpreting the one-anaphor. Beyond this goal,
  Our system ﬁnds it difﬁcult to locate actual antecedents there are more complex challenges awaiting in terms of one-
that are far away from their one-anaphors: preference is anaphora interpretation.