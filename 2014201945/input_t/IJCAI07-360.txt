        Common Sense Based Joint Training of Human Activity Recognizers

           Shiaokai Wang      William Pentney                   Tanzeem Choudhury
                   Ana-Maria Popescu                             Matthai Philipose
                 University of Washington                           Intel Research


                    Abstract                          used in day-to-day activities are common across deployment
                                                      conditions (e.g., most people use kettles, cups and teabags in
    Given sensors to detect object use, commonsense   making tea), it is possible to specify broadly applicable prior
    priors of object usage in activities can reduce the models for the activities simply as lists of objects used in per-
    need for labeled data in learning activity models. It forming them. These priors may either be speciﬁed by hand
    is often useful, however, to understand how an ob- or mined automatically off the web [Perkowitz et al., 2004].
    ject is being used, i.e., the action performed on it. Given these simple models as priors and unlabeled traces of
    We show how to add personal sensor data (e.g., ac- objects used in end-user activities, a technique for learning
    celerometers) to obtain this detail, with little label- with sparse labels (e.g., EM) may be used to produce cus-
    ing and feature selection overhead. By synchroniz- tomized models with no per-activity labeling [Wyatt et al.,
    ing the personal sensor data with object-use data, 2005].
    it is possible to use easily speciﬁed commonsense
    models to minimize labeling overhead. Further,      Object use is a promising ingredient for general, low-
    combining a generative common sense model of      overhead indoor activity recognition. However, the approach
    activity with a discriminative model of actions can has limitations. For instance, it may be important to discern
    automate feature selection. On observed activity  aspects of activities (e.g., whether a door is being opened or
    data, automatically trained action classiﬁers give closed) indistinguishable by object identity, it is not always
    40/85% precision/recall on 10 actions. Adding ac- practical to tag objects (e.g., microwaveable objects) with
    tions to pure object-use improves precision/recall sensors, and multiple activities may use similar objects (e.g.,
    from 76/85% to 81/90% over 12 activities.         clearing a table and eating dinner). One way to overcome
                                                      these limitations is to augment object-use with complemen-
                                                      tary sensors.
1  Introduction                                         Wearable personal sensors (especially accelerometers and
Systems capable of recognizing a range of human activity in audio), which provide data on body motion and surround-
useful detail have a variety of applications. Key problems ings, are a particularly interesting complement. These sensors
in building such systems include identifying a small set of are unobtrusive, relatively insensitive to environmental condi-
sensors that capture sufﬁcient data for reasoning about many tions (especially accelerometers), and previous work [Lester
activities, identifying the features of the sensor data relevant et al., 2005; Bao and Intille, 2004; Ravi et al., 2005] has
to the classiﬁcation task and minimizing the human overhead shown that they can detect accurately a variety of simple ac-
in building models relating these feature values to activities. tivities, such as walking, running and climbing stairs. We
The traditional approach, using vision as the generic sensor focus on how to use these sensors to detect ﬁne-grained arm
[Moore et al., 1999; Duong et al., 2005], has proved chal- actions using objects (such as “scoop with a spoon”, “chop
lenging because of the difﬁculty of identifying robust, infor- with a knife”, “shave with a razor” and “drink with a glass”;
mative video features sufﬁcient for activity recognition under note that the physical motion depends on the kind of action
diverse real-world conditions and the considerable overhead and the object used), and also how to combine these actions
of providing enough labeled footage to learn models.  with object-use data from dense sensors to get more accurate
  A promising recent alternative [Philipose et al., 2004; activity recognition.
Tapia et al., 2004] is the use of dense sensing, where indi- We present a joint probabilistic model of object-use, phys-
vidual objects used in activities are tagged with tiny wireless ical actions and activities that improves activity detection rel-
sensors which report when each object is in use. Activities ative to models that reason separately about these quantities,
are represented as probabilistic distributions over sequences and learns action models with much less labeling overhead
of object use, e.g., as Hidden Markov Models. These simple than conventional approaches. We show how to use the joint
models perform surprisingly well across a variety of activ- prior model, which can be speciﬁed declaratively with a little
ities and contexts, primarily because the systems are able to “common sense” knowledge per activity (and could in prin-
detect consistently the objects in use. Further, because objects ciple be mined from the web), to automatically infer distri-

                                                IJCAI-07
                                                  2237                                                               A                       A  Pr(AtA t-1)

                                                                              Pr(OtA t)    Pr(BtO t,A t,B t-1)
                                                           O       B               O       B


                                                                                          Pr(MtO t, Bt)
                                                               M                       M

Figure 1: Sensors: iBracelet and MSP (left), RFID tagged       timeslicet-1            timeslicet
toothbrush & paste (right), tags circled.


                                                                                      Pr(A A )
                                                                      DynamicBayesNet    t t-1
butions over the labels based on object-use data alone: e.g.,  A                    A
given that object “knife” is in use as part of activity “making
                                                                             Pr(O A )   Pr(B O ,A )
salad”, action “chop with a knife” is quite likely. We there-                  t t         t t t
fore call our technique common sense based joint training. O      B              O      B
Given class labels, Viola and Jones (2001) have shown how                                 b
                                                                           HMM            s
to automatically and effectively select relevant features using   B                     B
                                                                                              MSP-based
boosting. We adapt this scheme to work over the distributions                                 object/action
over labels, so that our joint model is able to perform both      B’                    B’    classifier
parameter estimation and feature selection with little human                             b
                                                                         boosteddecision
involvement.                                                      B’     stumpensemble  B’
  We evaluate our techniques on data generated by two peo-
                                                             M    M                M    M
ple performing 12 activities involving 10 actions on 30 ob-   1    i   MF’          1    i   MF’
jects. We deﬁned simple commonsense models for the activi-
ties using an average of 4 English words per activity. Combin-
ing action data with object-use in this manner increases pre- Figure 2: Joint models for activity and action recognition. (a)
cision/recall of activity recognition from 76/85% to 81/90%; Full joint DBN (above), and (b) a derived layered model used
the automated action-learning techniques yielded 40/85% in this paper (below). Dotted arrows represent the assignment
precision/recall over 10 possible object/action classes. To our of the MAP estimate of a variable in one layer as its observed
knowledge this work is the ﬁrst to demonstrate that simple value in the next.
commonsense knowledge can be used to learn classiﬁers for
activity recognizers based on continuous sensors, with no hu-
man labeling of sensor data.                          accelerometer, microphones sampling 8-bit audio at 16kHz,
                                                      IR/visible light, high-frequency light, barometric pressure,
                                                      humidity, temperature and compass. The data (roughly
2  Sensors                                            18,000 samples per second) is shipped in real time to an
Figure 1 shows the sensors we use. We wear two bracelets off-board device, such as a cellphone which currently stores
on the dominant wrist. One is a Radio Frequency Identiﬁca- it for later processing. In the near future, we hope to per-
tion (RFID) reader, called the iBracelet [Fishkin et al., 2005], form the processing (inference, in particular), in real-time on
for detecting object use, and the other is a personal sensing the cellphone. To compact and focus the data, we extract
reader, called the Mobile Sensing Platform (MSP) [Lester et F = 651 features from it, including mean, variance, energy,
al., 2005], for detecting arm movement and ambient condi- spectral entropy, FFT coefﬁcients, cepstral coefﬁcients and
tions.                                                band-pass ﬁlter coefﬁcients; the end-result is a stream of 651-
  The iBracelet works as follows. RFID tags are attached to dimensional feature vectors, generated at 4Hz. We will write
objects (e.g., the toothbrush and tube of toothpaste in Fig- SN = s1,...,sN for the stream of sensor readings, where
ure 1) whose use is to be detected. These tags are small, each si is a paired object name and a vector of MSP features.
40-cent battery-free stickers with an embedded processor and
antenna. The bracelet issues queries for tags at 1Hz or faster. 3 Techniques
When queried by a reader up to 30cm away, each tag responds
(using energy scavenged from the reader signal) with a unique 3.1 Problem Statement
identiﬁer; the identiﬁer can be matched in a separate database Let A = {ai} be a set of activity names (e.g., “making tea”),
to determine the kind of object. The bracelet either stores B = {bi} be a set of action names (e.g., “pour”), O = {oi}
timestamped tag IDs on-board or transmits read tags wire- be a set of object names (e.g., “teabag”) and M = {mi} be
lessly to an ambient base station, and lasts 12 to 150 hours the set of all possible MSP feature vectors.
between charges. If a bracelet detects a tagged object, the We assume coarse “commonsense” information linking
object is deemed to be in use, i.e., hand proximity to objects objects, activities and actions. Let OA = {(a, Oa)|a ∈

implies object use.                                   A, Oa   =   {o1,...,ona } s.t. oi is often used in a} (e.g.,
  The MSP includes eight sensors: a six-degree-of-freedom (a, Oa)=(make tea, {kettle, milk, sugar, teabag})). Let

                                                IJCAI-07
                                                  2238BOA   =  {(a, o, Ba,o = {b1,...,bma,o })|a ∈ A, o ∈     Given initial guesses (derivable from OA and BOA)
Oa s.t. bi is performed when using object o as part of a} for the conditional probabilities, the problem of jointly re-
(e.g., (make tea, milk, {pour, pick up})).            estimating the parameters, estimating P (M|O, B) and se-
  In monitoring peoples’ day-to-day activities, it is relatively lecting a small discriminatory subset of features of M based
easy to get large quantities of unlabeled sensor data SN .Get- on partially labeled data can be viewed as a semi-supervised
ting labeled data is much harder. In what follows, we assume structure learning problem, traditionally solved by structural
no labeling at all of the data. Finally, although training data variants of EM [Friedman, 1998]. However, structural EM is
will consist of synchronized object-use and personal sensor known to be difﬁcult to apply tractably and effectively. We
data, the test data may contain just the latter: we expect end- therefore trade off the potential gain of learning a joint model
users to transition between areas with and without RFID in- for the simplicity of a layered model.
strumentation.
  Given the above domain information and observed data, we 3.3 A Layered Model
wish to build a classiﬁer over MSP data that:         Figure 2(b) shows the layered model we use. The new struc-
  • Infers the current action being performed and the object ture is motivated by the fact that recent work [Viola and Jones,
    on which it is being performed.                   2001; Lester et al., 2005] has shown that a tractable and effec-
                                                      tive way to automatically select features from a “sea of pos-
  •                            O
    Combines with object-use data (when available) to sible features” is to boost an ensemble of simple classiﬁers
                                        A
    produce better estimates of current activity .    sequentially over possible features. Our new model therefore
  • For efﬁciency reasons, uses F   F features of M. replaces the node M of the original DBN, with a separate dis-
                                                      criminative classiﬁer smoothed by an HMM. Classiﬁcation in
3.2  A Joint Model                                    the layered structure proceeds as follows:
The Dynamic Bayesian Network (DBN) of Figure 2(a) cap-  1. At each time slice, the F  features are fed into the
tures the relationship between the state variables of interest . boosted classiﬁer (described further below) to obtain the
The dashed line in the ﬁgure separates the variables represent- most likely current action-object pair b∗ . The classiﬁ-
ing the state of the system in two adjacent time slices. Each cation so obtained is prone to transient glitches and is
node in a time slice of the graph represents a random variable passed up a level for smoothing.
capturing part of the state in that time slice: the activity (A)
and action (B) currently in progress, and the MSP data (M) 2. The HMM is a simple temporal smoother over actions:
            O                                             its hidden states are smoothed action estimates, and its
and the object ( ) currently observed. The directed edges are                                b∗
inserted such that each random variable node in the induced observations are the unsmoothed output of the en-
                                                          semble classiﬁer. The MAP sequence of smoothed ac-
graph is independent of its non-descendants given its parents.            ∗
                                                          tion/object pairs, bs, is passed on to the next layer as ob-
Each node Xi is parameterized by a conditional probability
                                                          servations. The HMM is set to have uniform observation
distribution (CPD) P (Xi|Parents(Xi)); the joint distribution
P (X ,...,X )=Π        P (X |     (X ))                   probability and uniform high self-transition probability
    1      n     i=1...n   i Parents i .                  t
  Our DBN encodes the following independence assump-       s. If we just require action classiﬁcation on MSP data,
tions within each time slice:                             we threshold the smoothed class by margin; if the mar-
                                                          gin exceeds threshold tsmooth (set once by hand for all
 1. The MSP signal M is independent of the ongoing ac-    classes), we report the class, else we report “unknown”.
    tivity A given the current object O and action B.For
    instance, hammering a nail (action “hammer”, object 3. If we wish to combine the inferred action with object
    “nail”) will yield the same accelerometer and audio sig- data to predict activity, we pass the smoothed action
    natures regardless of whether you’re ﬁxing a window or class up to the DBN. The DBN at the next level (a trun-
                                       M                  cated version of the one described previously) treats o
    hanging a picture. On the other hand, is directly                         b∗
                     O     B                              and (the action part of) s as observations and ﬁnds the
    dependent both on  and  : the particular hand mo-                      A
    tion varies for action “pull” depending on whether object most likely activity .
    “door” or object “laundry line” are being pulled. Simi- Inference requires parameters of the DBN to be known.
    larly, pulling a door entails a different motion from push- The commonsense information OA and BOA can be con-
    ing it.                                           verted into a crude estimate of conditional probabilities for
                                                      the DBN as follows:
 2. The action depends unconditionally both on current ac-          p            1−p
                                                        • P (o|a)=      o ∈ Oa,
    tivity and the object currently in use. For instance, if        na if      |O|−na otherwise. Essentially,
    the activity is “making pasta” the probability of action we divide up probability mass p (typically  0.5) among
    “shake” will depend on whether you are using object   likely objects, and divide up the remainder among the
    “salt shaker” or “pot”. Similarly, if the object is “pot” unlikely ones.
    the probability of action “scrub” will depend on whether            p             1−p
                                                        • P (bt|a, o)=     if b ∈ Ba,o,       otherwise, as
    the activity is “making pasta” or “washing dishes”.                ma,o           |B|−ma,o
                                                          above (note ma,o and na are deﬁned in section 3.1).
 3. The object used is directly dependent on the activity. In
                                                        • P (a |a  )=         a    = a      1 − 
    particular, even if the action is a generic one such as   t t−1     |A|−1 if t−1    t,and       other-
    “lift”, the use of the object “iron” signiﬁcantly increases wise. This temporal smoothing encourages activities to
    the probability of activity “ironing”.                continue into adjacent time slices.

                                                IJCAI-07
                                                  2239    Initialize weights wi =1/N, i =1,...,N               1  make tea (6,1)
    Iterate for m =1,...,M :                             2  eat cereal (2,1)     a  lift to mouth
                                                         3  make sandwich (3,1)  b  scoop
    1. Tfm(s) ←ﬁt(W, f ) for all features f
      E    ←      w P  I(c = T  (s ))                   4  make salad (2,1)     c  chop
    2.  fm      i,c i ic      fm  i                      5  dust (1,1)           d  spread
        ∗
    3. f ←  arg minf Efm                                 6  brush teeth (2,1)    e  dust
                                                         7  tend plants (2,1)    f  brush
    4. αm ← log (1 − Ef ∗m/Ef ∗m)
                                                        8  set table (9,1)      g  wipe horizontally
      w  ←     P  w    (α  I(c = T ∗ (s )))
    5.  i     c ic i exp  m       f m  i                 9  clean windows (2,1)  h  wipe vertically
    6. Re-normalize wi to 1                             10  take medication (2,1) i drink
    Output                                             11  shave (3,1)          j  shave
                     M                                  12  shower (2,1)
    T (s)=arg maxc   m=1 αmI(c  = Tf ∗m(s))
                                                            Table 2: Activities (l) and actions (r) performed
    whereﬁt(W, f )=    
    t+ ←    i wiPi+si[f]/ i wiPi+
                                                           Action Precision Action Recall Activity Precision Activity Recall
    t− ←     wiPi−si[f]/   wiPi−
            i             i                                 100
    t ← (t+ + t−)/2
                                                            90
    return λs. + if sgn(s − t)= sgn(t+ − t), − else
                                                            80

                                                            70
          Table 1: The VirtualBoost Algorithm               60
                                                            50

                                                            40
                  

               p p                                        Percent  (%) Correct
Hyperparameters ,  and   are currently selected once by     30

hand and do not change with activities to be classiﬁed or ob- 20
served data. If these crude estimates are treated as priors, the
                                                            10
parameters of the DBN may be re-estimated with unlabeled
                                                             0
data SN using EM or other semi-supervised parameter learn-      N=5/O    N=5/O+a  N=12/O   N=12/O+a
ing schemes.                                                            Size of data set and observations
  It remains to learn the boosted classiﬁer. We take the ap-      Figure 3: Overall precision/recall
proach of Viola and Jones (2001). Their scheme (as any con-
ventional boosting-based scheme) requires labeled examples.
                                      SN
Given that we only have unlabeled MSP data , we need to ror, attributing a weight αm to this classiﬁer such that high
augment the usual scheme. A simple way (which we call errors result in low weights, and re-weighting the examples
MAP-Label) to do so would be to use the object-use data
o ,...,o      S                                       again to focus on newly problematic examples. VirtualBoost
 1     N  from  N and the DBN to derive the most likely simply replaces all computations in the standard algorithm
        b1,...,bN                            (bi,oi)
sequence          of actions, and to treat the pair   that expect individual class labels ci with the expectation
as the label in each time step. This approach has the potential over the distribution Pic of the label. It is straightforward to
problem that even if many action/object pairs have compara- show that this new variant is sound, in the sense that if D =
ble likelihood in a step, it only considers the top-rated pair (s1,P1c),...,(si,Pic),...,(sN ,PNc) is a sequence that
as the label and ignores the others. We therefore explore an when input to VirtualBoost yields classiﬁer T , then the “sam-
                                                                    
alternate form of the feature selection algorithm, called Vir- ple sequence” D = ...,(si,ci1), (si,ci2),...,(si,ciK ),...
tualBoost, that works on data labeled with label distributions, where the cij are samples from Pic,whenfedtotheconven-
rather than individual labels: we use the DBN and object-use tional algorithm, will produce the classiﬁer T for large K.
data to generate the marginal distribution of B at each time
slice (we refer to this technique as Marginal-Label). When 4 Evaluation Methodology
a boosted classiﬁer is trained using this “virtual evidence”
[Pearl, 1988], VirtualBoost allows the trainer to consider all Our evaluation focuses on the following questions:
possible labels in proportion to their weight.          1. How good are the learned models? In particular, how
  Table 1 speciﬁes VirtualBoost. For simplicity we assume much better are the models of activity with action infor-
two classes, although the algorithm generalizes to multiple mation combined, and how good are the action models
classes. The standard algorithm can be recovered from this themselves? Why?
one by setting Pic to 1 and removing all sums over classes c.
                                   ≤ M                  2. How does classiﬁer performance vary with our design
The goal of both algorithms is to identify features, and  choices? With hyperparameters?
corresponding single-feature multiple-class classiﬁers Tf ∗m
and weights αm, the weighted combination of the classiﬁers 3. How necessary were the more sophisticated aspects of
is maximally discriminative.                              our design? Do simple techniques do as well?
  The standard algorithm works by, for each iteration m,ﬁt- To answer these questions, we collected iBracelet and MSP
ting classiﬁers Tfm for each feature f to the labeled data data from two researchers performing 12 activities containing
weighted to focus on previously misclassiﬁed examples, iden- 10 distinct actions of interest using 30 objects in an instru-
tifying as Tf ∗m the classiﬁer that minimizes the weighted er- mented apartment. The activities are derived from a state-

                                                IJCAI-07
                                                  2240                                                             80
                       Precision  Recall
     100

      90                                                     70

      80

      70                                                     60

      60
                                                             50
      50
                                                            Precision  (%)

      40


    Percent  (%) Correct                                     40
      30

      20
                                                             30
      10                                                      20    30   40   50   60    70   80   90
       0                                                                      Recall (%)
         1 2 3 4 5 6 7 8 9 101112 a b c d e f g h i j
                     Activity / Action Class
                                                            Figure 5: Precision/recall vs. margin threshold
          Figure 4: Precision/recall breakdown

                                                      observations, whereas N = i|O + a assumes MSP observa-
mandated Activities of Daily Living (ADL) list; monitoring tions for the action classiﬁer in addition. For each conﬁgu-
these activities is of interest to elder care professionals. Four ration, we report P/R of activity and action detection. The
to ten executions of each activity were recorded over a pe- N =5conﬁguration yields similar results to N =12,sowe
riod of two weeks. Ground truth was recorded using video, focus on N =12below.
and each sensor data frame was annotated in a post-pass with Three points are key. First, comparing corresponding O
the actual activity and action (if any) during that frame, and and A bars for activity, we see that adding MSP data does
designated “other” if not.                            improve overall P/R from 76/85% to 81/90%. Second, our
  Table 2 lists the activities and actions performed. Each ac- action classiﬁer based on MSP data has 41/85% P/R; given
tivity is followed by the number of tagged objects, and the that we have 10 underlying action/object classes all fairly
number of actions labeled, in that activity; e.g., for making evenly represented in the data, this is far better than any naive
tea, we tagged 6 objects (spoon, kettle, tea, milk, sugar and guesser. The low precision reﬂects the automatically-trained
saucer) and tracked one action (scoop with a spoon). We classiﬁer’s inability to identify when no action of interest is
restricted ourselves to one action of interest per activity, al- happening: 56% of our false positives are mis-labeled “other”
though our system should work with multiple actions. We slots. Third, the relatively high action P/R (32/80%) with just
tag multiple objects per activity; 6 of the activities (make tea, objects (N =12/O) reveals a limitation of our dataset: if
eat cereal, make salad, make sandwich, set table, clean win- object-use data is available, it is possible to predict the cur-
dow) share at least one object with another activity. Note that rent action almost as well as with additional MSP data. In
for our system to work, we do not have to list or track ex- our dataset, each activity/object combination has one action
haustively either the objects or the actions in an activity, an associated with it, so guessing the action given activity/object
important consideration for a practical system.       is fairly easy.
  In our experiments, we trained and classiﬁed the unseg- Figure 4 breaks down P/R in the N =12case over activ-
mented sensor data using leave-one-out cross-validation over ities 1-12 and actions 1-10. The main point of interest here
activities: we trained over all but one of the instances for each (unfortunately not apparent in the ﬁgure) is the interaction
activity and tested on the last one, rotated the one that was left between object and MSP-based sensing: combining the two
out, and report aggregate statistics. Since many time slices sensors yields a jump in recall of 8, 15, 39 and 41% respec-
were labeled “other” for actions, activities or both, we use tively in detecting activities 4, 8, 9 and 2 respectively, with no
both precision (fraction of slices when our claim was correct) activity deteriorating. In all these cases the activities involved
and recall (fraction of slices when we detected a non-”other” shared objects with others, so that actions were helpful. Ac-
class correctly). In some cases, we report the F -measure tivity 4, making salad, is particularly hard hit because both
=2PR/(P    + R) a standard aggregate goodness measure, objects used (knife and bowl) are used in other activities. Re-
where P and R are precision and recall.               solving this ambiguity improves P/R for the sole action asso-
                                                      ciated with making salad by 62/41%.
5  Results                                              Figure 5 shows how P/R of action detection changes when
                                                      margin threshold tsmooth of section 3.3 varies from -0.8 to 0.2.
Figure 3 displays overall activity/action detection preci- We use tsmooth = −0.25 in this section. We also varied boost-
sion/recall (P/R) for four conﬁgurations of interest. Each con- ing iterations M to 5, 10, 20, 40 and 60; the best matching
ﬁguration has the form (N =5|12/O[+a]). N =5attempts  P/R rates were 36/61, 41/77, 40/85, 36/88 and 35/86%. We
to detect activities 1-5 of Table 2, whereas N =12detects use N =20.
them all. In the N = i|O conﬁguration, we assume just RFID Figure 6 compares VirtualBoost to simpler approaches.

                                                IJCAI-07
                                                  2241