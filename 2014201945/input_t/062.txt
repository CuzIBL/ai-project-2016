       Active Learning with Strong and Weak Views: A Case Study on Wrapper 
                                                      Induction 

                               Ion Muslea, Steven N. Minton, Craig A. Knoblock 
                             muslea@isi.edu, minton@fetch.com, knoblock@isi.edu 
               U. of Southern California, Fetch Technologies, Inc., U. of Southern California, 
                                                 4676 Admiralty Way 
                                             Marina del Rey, CA 90292 

                        Abstract                                 The main limitation of existing Co-Testing algorithms 
                                                               [Muslea et al, 2000; 2002a] is that they are designed to use 
     Multi-view learners reduce the need for labeled data 
                                                               only views that are adequate for learning, thus being unable 
     by exploiting disjoint sub-sets of features (views), 
                                                               to also exploit imperfect views that would permit a faster con•
     each of which is sufficient for learning. Such al•
                                                               vergence to the target concept. To address this problem, we 
     gorithms assume that each view is a strong view 
                                                               extend the multi-view learning framework by introducing the 
     (i.e., perfect learning is possible in each view). We 
                                                               idea of learning from strong and weak views. By definition, 
     extend the multi-view framework by introducing a 
                                                               a strong view consists of features that are adequate for learn•
     novel algorithm, Aggressive Co-Testing, that ex•
                                                               ing the target concept; in contrast, in a weak view one can 
     ploits both strong and weak views; in a weak view, 
                                                               only learn a concept that is more general or specific than the 
     one can learn a concept that is strictly more gen•
                                                               target concept. We introduce a novel algorithm, Aggressive 
     eral or specific than the target concept. Aggressive 
                                                               Co-Testing, that exploits both strong and weak views with•
     Co-Testing uses the weak views both for detecting 
                                                               out additional data engineering costs. We also describe a case 
     the most informative examples in the domain and 
                                                               study on wrapper induction, which shows that Aggressive Co-
     for improving the accuracy of the predictions. In a 
                                                               Testing clearly outperforms state-of-the-art algorithms. 
     case study on 33 wrapper induction tasks, our algo•
                                                                 To illustrate the idea of strong and weak views, consider 
     rithm requires significantly fewer labeled examples 
                                                               the task of extracting fax numbers from a directory of restau•
     than existing state-of-the-art approaches. 
                                                               rant Web pages such as Zagat. The two wrapper induction 
                                                               views described above are strong views because each of them 
1 Introduction                                                 is (typically) sufficient to extract the item of interest [Muslea 
                                                               et al., 2000]. In addition to these two strong views, we 
Labeling training data for learning algorithms is a tedious, er•
                                                               can also exploit a view that consists of tokens within the 
ror prone, time consuming process. Active learning addresses 
                                                               item to be extracted. In this view, we learn the grammar 
this issue by detecting and asking the user to label only the 
                                                               " ( Number ) Number - Number" that describes the content of 
most informative examples in a domain. In this paper, we 
                                                               the fax numbers. This additional view is a weak view because 
focus on Co-Testing [Muslea et al, 20001, an active learn•
                                                               the grammar above represents a concept more general than 
ing technique for domains with multiple views; i.e., domains 
                                                               the target one; i.e., it cannot discriminate between fax and 
with disjoint sub-sets of features, each of which is sufficient 
                                                               phone numbers that appear within the same Web page. 
for learning. Co-Testing is a 2-step iterative algorithm that 
                                                                 Aggressive Co-Testing for wrapper induction works as fol•
(1) uses the few available labeled examples to learn a hypoth•
                                                               lows: first, it uses a few labeled examples to learn a rule 
esis in each view and (2) queries (i.e., asks the user to label) 
                                                               in each view (i.e., one weak and two strong rules). Then it 
examples on which the views predict a different label. Such 
                                                               queries an unlabeled example on which the two strong rules 
queries are highly informative because they correct mistakes 
                                                               extract different strings, both of which are inconsistent with 
made by one of the views: whenever the views disagree, at 
                                                               the content-based grammar. Each such query is likely to rep•
least one of them must be wrong. 
                                                               resent a mistake not only in one, but in both strong views, thus 
  Co-Testing was successfully applied to wrapper induction 
                                                               leading to faster convergence. We use a collection of 33 dif•
[Muslea et al, 2000J, an industrially important application. 
                                                               ficult extraction tasks to show that using the weak view dra•
In wrapper induction the goal is to learn rules that extract the 
                                                               matically reduces the need for labeled data: compared with 
relevant data from collections of Web pages that share the 
                                                               existing state of the art active learners, our novel algorithm 
same underlying structure; e.g., extract the book titles and 
                                                               requires between 45% and 81% fewer labeled examples. 
prices from amazon. com. For wrapper induction, Co-Testing 
uses two views: the sequences of tokens that precede and fol•
                                                               2 Related work 
low the extraction point, respectively. The extraction rules 
learned in these views are finite automata that consume an     The idea of exploiting complementary information sources 
item's prefix or suffix within the page, respectively.         (i.e., types of features) appears in various multi-strategy 


INFORMATION EXTRACTION                                                                                                415  learners. Of particular interest are two recent papers [Kush-
 merick et al, 2001; Nahm and Mooney, 2000] in which the 
 authors use sets of features that clearly do not have the same 
 expressive power. This work can be seen as learning from 
 strong and weak views, even though it was not formalized as 
 such, and it was not used for active learning. 
   Kushmerick et al. 12001] focus on classifying the lines of 
 text on a business card as a person's name, affiliation, address, 
 phone number, etc. In this domain, the strong view consists 
 of the words that appear on each line, based on which a Naive 
 Bayes text classifier is learned. In the weak view, one can ex•
 ploit the relative order of the lines on the card by learning a 
 hidden Markov Model that predicts the probability of a par•
 ticular ordering of the lines on the business card (e.g., name 
 followed by address, followed by phone number). 
   This weak view defines a class of concepts that is more     Figure 1: Co-Testing algorithms repeatedly query examples 
general than the target concept: all line orderings are possi• for which the two views make a different prediction. 
ble, even though they are not equally probable. The order of 
 the text lines cannot be used by itself to accurately classify examples, and a set U of unlabeled ones, Co-Testing works 

the lines. However, when combined with the strong view, the    as follows: first, it learns the classifiers h1 and h2 by apply•
ordering information leads to a classifier that clearly outper• ing the algorithm C to the projection of the examples in L 

forms the stand-alone strong view [Kushmerick et al., 2001].   onto the two views, VI and V2. Then it applies h1 and h2 to 
   Another algorithm that can be seen as learning from strong  all unlabeled examples in U and detects the set of contention 

and weak views is DISCOTEX I Nahm and Mooney, 2000],           points, which are unlabeled examples for which h1 and h2 
which extracts job titles, salaries, locations, etc from com•  predict a different label. Finally, it asks the user to label one 
puter science job postings to the newsgroup austin. jobs.      of the contention points and repeats the whole process. 
 DISCOTEX proceeds in four steps: first, it uses RAPIER          The various members of the Co-Testing family differ from 
fCaliff and Mooney, 1999] to learn extraction rules for each   each other with two respects: the strategy used to select the 

item of interest. Second, it applies the learned rules to a large, next query, and the manner in which the output hypothesis1 
unlabeled corpus of job postings and creates a database that   is constructed. In other words, each Co-Testing algorithm 
is populated with the extracted data. Third, by text mining    is uniquely defined by the choice of the heuristics Select-
this database, DISCOTEX learns to predict the value of each    QueryO and CreateOutputHypothesis(). In turn, these two 
item based on the values of the other fields. For example, it  heuristics depend on the properties of both the application do•
may discover that "IF the job requirements include             main and the base learner C. 
C++ and CORBA THEN the development platforms                     We consider here two types of query selection strategies: 
include windows". Finally, when the system is deployed 
                                                               - random: randomly choose a contention point. This strategy 
and the RAPIER rules fail to extract an item, the mined rules 
                                                                    is appropriate for base learners that lack the capability of 
are used to predict the item's content. 
                                                                    estimating the confidence of their predictions. 
   In this scenario, the RAPIER rules represent the strong view 
because they are sufficient for extracting the data of interest. - max-confidence: choose the contention point on which both 
In contrast, the mined rules represent the weak view because        h1 and h2 make the most confident prediction. This 
they cannot be learned or used by themselves. Furthermore,          strategy is appropriate for high accuracy domains (e.g., 
as DISCOTEX discards all but the most accurate of the mined         wrapper induction), in which there is little or no noise. 
rules, which are highly-specific, it follows that the weak view     On such tasks, discovering examples that are misclassi-
can be used to learn only concepts that are more specific than      fied "with high confidence" translates into queries that 
the target concept. Nahm and Mooney show that these mined           "fix big mistakes," thus leading to fast convergence. 
rules improve the extraction accuracy by capturing informa•      We also consider two "output hypothesis" heuristics: 
tion that complements the RAPIER extraction rules.             - winner-takes-all. the output hypothesis is the one learned 
                                                                    in the view that makes the smallest number of mistakes 
3 Preliminaries                                                     over the TV queries. 
In this section we first explain the main idea behind Co-      - majority vote: examples are labeled according to the pre•
Testing algorithms [Muslea et al., 2000; Muslea, 2002], and         diction of most views (requires at least three views). 
then we describe the strong and weak views that we use for 
wrapper induction.                                             3.2 Wrapper induction: the strong views 
                                                               In wrapper induction, each item of interest is described by 
3.1 Background: the Co-Testing approach                        three strings of variable length: the item's content, together 

Figure 1 provides a formal description of the Co-Testing fam•     1 Once training is completed, the output hypothesis is used to pre•
ily of algorithms. Given a base learner £, a set L of labeled  dict the label of all new, unseen examples. 


416                                                                                         INFORMATION EXTRACTION                                                                  R3: (Number) Number - Number 

Figure 2: The forward and backward strong rules (i.e., Rl and R2) find the beginning of the phone number by consuming its 
suffix or prefix, respectively. R3 is a content-based grammar that describes the structure of the item to be extracted. 

with its prefix and suffix within the document. As this is not may exploit the fact that they can be described by a simple 
a typical machine learning representation in which an exam•    grammar: " ( Number ) Number - Number'. Similarly, when 
ple's description in each view consists of a fixed set of fea• extracting URLs, one can take advantage of the fact that a 
tures , we describe here in detail how Co-Testing can be ap•   typical URL starts with the string "http: //www.", ends with 
plied to wrapper induction. As a first step, we introduce the  the string ". html", and contains no HTML tags. 
basic ideas in STALKER iMuslea et al, 2001], which is the        In this paper, we use the following features to describe the 
wrapper induction algorithm that we use as base learner.       content of each item to be extracted: 
   Consider the illustrative task of extracting phone numbers  - the length range (in tokens) of the seen examples. For in•
from Web pages similar to the one shown in Figure 2. In             stance, phone numbers in the format" ( Number ) Number 
STALKHR, an extraction rule consists of a start rule and an         - Number' consist of six tokens (i.e., the three numbers, 
end rule that identify the beginning and the end of the item,       the dash, and the two parentheses). 
respectively. Given that start and end rules are extremely sim•
ilar, we describe here only the former. In order to find the   - the token types that appear in the training examples. This 
beginning of the phone number, one can use the start rule           feature consists of the set of the most specific wildcards 
                                                                    (e.g., Number, AllCaps, etc) that match the tokens en•
   Rl = SkipTo "Phone:<i>" 
                                                                    countered in the item to be extracted. For example, in 
This rule is applied forward, from the beginning of the doc•        the phone number case, this list consists of two wild•
ument, and it ignores everything until it finds the string          cards: Number and Punctuation. The complete hierar•
Phone: <i>. For a slightly more complicated extraction task,        chy of wildcards is described in Figure 3. 
in which toll-free numbers appear in italics and the other ones 
                                                               - a start-pattern such as "http: //www." or " ( Number )", 
in bold, one can use a disjunctive start rule such as 
                                                                    which describes the beginning of the item of interest. 
                                                               - an end-pattern such as "AlphaNum.html" or "Number -
                                                                    Number'', which describes the end of the item. 
   An alternative way to detect the beginning of the phone 
                                                                 In order to learn the content-based description of an item, 
number is to use the start rule 
                                                               we use as base learner a simplified version of the DataPro 
                                                               algorithm [Lerman and Minton, 2000]. After tokenizing each 
which is applied backward, from the end of the document.       of the user-provided examples of strings to be extracted, the 
R2 ignores everything until it finds "Fax" and then, again,    weak-view learner proceeds as follows: 
skips back to the first open parenthesis.                      - the length range is determined by finding the examples that 
   As shown in [Muslea et al., 2001], the above extraction          contain the largest and the smallest number of tokens; 
rules can be learned based on user-provided examples of 
items to be extracted. Note that Rl and R2 represent de•       - the token types are obtained by going through the tokens 
scriptions of the same concept (i.e., start of phone number) in     that appear in the labeled examples and adding to the set 
two different views. That is, the views VI {forward view) and       of "seen types" the most specific wildcard that covers it. 
V2 (backward view) consist of the sequences of tokens that     - a start-pattern of length one consists of the most specific 
precede and follow the beginning of the item, respectively.         wildcard that covers the first token in all labeled exam•
   Note that both VI and V2 represent strong views: as the          ples; if all examples start with the same token, such 
Web pages to be wrapped share the same underlying struc•            as "(" in the phone number example, the actual to•
ture, STALKER can be seen as uncovering and exploiting this         ken is preferred to the most specific wildcard. A start-
underlying structure for extraction purposes. Consequently,         pattern of length k is generated by repeating the proce•
both the forward and backward rules are expected to extract         dure above for the first, second,..., up to k-th position. 
the relevant data from any page. 
                                                               - the end-pattern is learned in the same manner as the start 
3.3 Wrapper induction: the weak view                                pattern, but using the A; tokens at the end of the item. 
Besides the two strong views above, one can also use a third,    Note that, unlike the forward and backward views, the 
content-based view, which describes the actual item to be ex•  content-based view is a weak view because, for many extrac•
tracted. For example, when extracting phone numbers, one       tion tasks, this view does not uniquely define the item of inter-


INFORMATION EXTRACTION                                                                                                417                                                               they use different query selection strategies and output hy•
                                                              potheses. More precisely, Naive Co-Testing randomly queries 
                                                              one of the contention points and generates a winner-takes-all 
                                                              output hypothesis (i.e., the rule that makes the fewest mis•
                                                              takes on the queries extracts the data from all documents). 

                                                              5 Empirical Evaluation 
                                                               The algorithms in the experimental comparison 
                                                              In this empirical evaluation we compare the following algo•
                                                              rithms: Aggressive Co-Testing, Naive Co-Testing, Qucry-
Figure 3: The hierarchy of wildcards used for wrapper induc•  by-Bagging, and Random Sampling. The first two algo•
tion. The parent-child relationship denotes the ls MoreGener- rithms were described in the previous section; Random Sam•
alThen relationship. For example, the most general wildcard   pling, which is used as strawman, is identical with Naive Co-
is AnyToken, which matches all possible tokens. non-Html,     Testing, except that it randomly queries one of the unlabeled 
which is a child of AnyToken, denotes all tokens than arc not examples instead of one of the contention points. 
HTML tags (i.e., alphanumeric tokens and punctuation signs).     Query-by-Bagging lAbe and Mamitsuka, 1998] is the only 
                                                              single-view active learner that can be used in a straightfor•
est. This is a consequence of the fact the view uses only fea• ward manner with STALKER and, more generally, for wrap•
tures that describe the content of each item. For Web pages   per induction.2 Even Query-by-Boosting [Abe and Mamit•
that contain several items with similar descriptions, such as suka, 19981, which is similar to Query-by-Bagging, cannot 
multiple email addresses, phone numbers, URLs, or names,      use STALKER as a base learner: as STALKER rarely - if ever -
the content-based grammar cannot discriminate between the     makes mistakes on small training sets, it eliminates the ability 
various items with similar descriptions.                      of the boosting algorithm to generate a diverse committee. 
                                                                 Query-by-Bagging is based on the idea of creating a com•
4 Aggressive Co-Testing                                       mittee of extraction rules and then querying the example on 
                                                              which the committee is the most split (i.e., the rules in the 
We introduce now Aggressive Co-Testing, which provides        committee extract the largest number of distinct strings); the 
a framework for naturally exploiting both strong and weak     algorithm's actual predictions are made by majority voting 
views. For Aggressive Co-Testing, the contention points are   the committee of rules. Query-by-Bagging generates a com•
defined as unlabeled examples on which the strong views pre•  mittee of 10 extraction rules, each of which is learned by 
dict a different label. For the wrapper induction problem, Ag• training STALKER with examples obtained by re-sampling 
gressive Co-Testing uses the two strong and one weak views    with replacement the original training set. We are forced to 
described earlier (i.e., the forward, backward, and content-  use such a small committee because of the scarcity of the 
based views). Consequently, the contention points are unla•   training data: as STALKER is expected to train on a handful of 
beled documents from which the forward and backward rules     examples, sampiing-with-replacement from a few examples 
extract different strings. Aggressive Co-Testing uses the la• leads to few distinct training sets for creating the committee. 
beled examples to learn one hypothesis in each view, detects  Query-by-Bagging is run once in each view, and we report 
the contention points, and then uses the following heuristics: only the best results, which are obtained in the forward view. 
- SelectQueryO returns the contention point on which both      The datasets 
     strong rules violate the largest number of constraints 
                                                              In order to empirically compare the algorithms above, we 
     learned in the weak view; e.g., the extracted strings are 
                                                              use the wrapper induction testbed introduced by Kushmerick 
     longer than the seen examples, the start- and end- pat•
                                                              [2000]. It consists of 206 extraction tasks from 30 Web-based 
     terns do not match, etc. This is a max-confidence query•
                                                              information sources.  As shown in [Muslea et al., 2001], 
     ing strategy because the content-based view is maxi•                         3
                                                              on most of these 206 tasks STALKER learns 100% accurate 
     mally confident that the strong rules extract incorrect 
                                                              rules from just one or two randomly-chosen labeled exam•
     strings. 
                                                              ples. We consider here the 33 most difficult tasks in the 
- CreateOutputHypothesisO uses the three views for ma•        testbed, which were also used in previous work on multi-view 
    jority voting. That is, given a new, unseen document,     learning [Muslea et al., 2000; 2002b]: 
     both strong rules are applied to it; if they extract the - the 28 tasks on which 20 random examples are insufficient 
     same string, this string is returned as the answer. Oth•      for learning 100%-accurate rules in both strong views; 
     erwise the "winner" is the strong rule that violates the 
     fewest constraints learned in the weak view. Note that      2Typical wrapper induction algorithms do not have the properties 
     this flexible approach allows Co-Testing to use the most that active learners require of their base learners; e.g., the ability to 
     appropriate strong rule for each document in the dataset. evaluate the confidence of each prediction [Lewis and Gale, 1994], 
                                                              or to randomly sample hypotheses from the version space [Seung et 
  To better understand how Aggressive Co-Testing works,       al., 1992], or to generate most specific and most general extraction 
we contrast it now with Naive Co-Testing [Muslea et al.,      rules [Cohn etai, 1994]. 

2000], which uses only the two strong views. Both algo•          3These datasets can be obtained from the RISE repository: 
rithms detect the contention points in the same manner, but   http: //www. isi .edu/~muslea/RISE/index.html. 


418                                                                                        INFORMATION EXTRACTION - the five additional tasks on which, in order to learn 100%-  Discussion 
     accurate rules in both strong views, STALKER requires a   The empirical results deserve several comments. First of all, 
     large number of random examples [Muslea, 2002].           the experiments illustrate the benefits of a framework that 
                                                               naturally integrates strong and weak views: Aggressive Co-
 The empirical results                                         Testing exploits the strengths and mitigates the weaknesses of 
                                                               each individual view. For example, we do not use the weak 
For each of these 33 tasks, we use 20-fold cross-validation 
                                                               view to identify the contention points because its mistakes 
to compare the performance of the algorithms above. Within 
                                                               may be "unfixable" (remember that in a weak view one learns 
each fold, the algorithms start with the same two randomly-
                                                               a concept more general/specific than the one of interest). On 
chosen examples and then make a succession of queries. In 
                                                               the other hand, we use the weak view both to detect the highly 
the end, the error rate is averaged over the 20 folds. 
                                                               informative contention points and to find the most appropriate 
   Figure 4 summarizes the algorithms1 performance over the    strong view for each prediction. 
33 tasks. In each graph, the X axis shows the number of          In contrast to Aggressive Co-Testing, existing multi-view 
queries made by the algorithm, while the Y axis shows the      learners [Blum and Mitchell, 1998; Muslea et al, 2000] can 
number of tasks for which a 100% accurate rule was learned     use only the strong views, thus losing an important source of 
after exactly X queries. Each algorithm is allowed to make     information. Similarly, single-view learners must either pool 
 18 queries, for a total of 20 labeled examples. By conven•    all features together or simply ignore all but one view. Note 
tion, the "19 queries" data point denotes tasks for which a    that, in practice, pooling the features together may not be a 
 100% accurate rule is not learned even after 18 queries.      straightforward task: 
   Aggressive Co-Testing clearly outperforms the other algo•   - in DISCOTEX iNahm and Mooney, 2000], the text min•
rithms: it makes an average of 2.43 queries over the 30 tasks       ing features (the weak view) are the extracted items, 
that are solved with 100% accuracy; furthermore, on 11 of           which become available only after the extraction rules 
these 30 tasks, a single query is sufficient to learn the cor•      are learned and applied to the unlabeled corpus. 
rect rule. In contrast, Naive Co-Testing, which comes sec•
ond, makes an average of 4.4 queries per task and converges    - the main contribution of iKushmerick et al, 2001 ] consists 
in a single query on just four of the 33 tasks. Also note that      of a novel algorithm that exploits features from both the 
Aggressive Co-Testing solves correctly two of the five tasks        strong and weak views (i.e., the words in each text line, 
that cannot be solved by Naive Co-Testing; the other two al•        and the lines's order within the business card). 
gorithms fail to solve 23 and 26 of the 33 tasks, respectively.  Second, we ran an additional experiment to determine the 
   Even though Aggressive Co-Testing makes 45% fewer           usefulness of the weak view with respect to each heuristic 
queries that Naive Co-Testing, at first glance the difference  (i.e., query selection and output hypothesis). We considered 
between 2.43 and 4.4 queries per task may seem small. How•     two hybrid algorithms between Aggressive and Naive Co-
ever, one must take into account that wrapper induction is     Testing: one uses the random and majority vote heuristics, 
used in information agents [Knoblock et al, 2001], which       while the other uses max-eonfidence and winner-takes-all; 
typically use hundreds of extraction rules; in this context, Ag• i.e., each hybrid exploits the weak view in only one of the two 
gressive Co-Testing makes a tremendous difference.             heuristics. Because of space constraints, we just summarize 
                                                               our findings: the two hybrids outperform Naive and under-
  To put our work into a larger context, we briefly compare 
                                                               perform Aggressive Co-Testing; more precisely, they make 
the results above with the ones obtained by WIEN [Kushm-
                                                               an average of 3.0 and 3.9 queries per task, respectively. In 
crick, 20001, which is the only wrapper induction system for 
                                                               other words, the weak view improves both the query selec•
which there are published results for all the extraction tasks 
                                                               tion and the output hypothesis. 
used here. As the two experimental setups are not identical,
                                                          4      Finally, note that on three of the 33 tasks, Aggressive Co-
this is just an informal comparison that contrasts Co-Testing 
                                                               Testing fails to learn 100%-accurate rules; in fact, on these 
with a state-of-the-art approach to wrapper induction. 
                                                               tasks Query-by-Bagging and Random Sampling obtain more 
  The results in [Kushmerick, 2000] can be summarized as       accurate rules than both Aggressive and Naive Co-Testing. 
follows: WIEN, which uses random sampling, learns the cor•     This happens because, on these three tasks, the backward 
rect extraction rule on 15 of the 33 task. On these 15 tasks,  view is significantly less accurate than the forward one. Con•
WIEN requires between 25 and 90 labeled examples5 to learn     sequently, the distribution of the queries is skewed towards 
the correct rule. For the same 15 tasks, both Aggressive and   mistakes of the "bad view", which arc not informative for 
Naive Co-Testing learn 100% accurate rules from at most        either view: the "good view" extracts the correct string any•
eight labeled examples (two random plus at most six queries).  way, while the "bad view" is inadequate to learn the target 
                                                               concept. To cope with this problem, we plan to use view val•

   4Instcad of using cross-validation, WIEN repeatedly splits the idation [Muslea et al, 2002b], which predicts whether or not 
dataset into randomly chosen training and test sets.           the strong views are appropriate for a particular task. 
   5For WIEN, an example consists of a document in which all items 
of interest arc labeled; e.g., a page with 15 labeled names repre• 6 Conclusion and Future Work 
sents a single example. In contrast, STALKER counts the 15 labeled 
strings as 15 examples. We convert the WIEN results into equiva• In this paper we introduce the concepts of strong and weak 
lent STALKER-like ones by multiplying the number of WIEN labeled views and present a novel active learner that naturally inte•
pages by the average number of item occurrences per page.      grates and exploits both types of views. In a case study on 


INFORMATION EXTRACTION                                                                                               419 