  Semantic Indexing of a Competence Map to support Scientific Collaboration in a
                                       Research Community

         Paola Velardi, Roberto Navigli                                 Michaël Petit
         Università di Roma “La Sapienza”                       University of Namur (FUNDP)
       Via Salaria, 113 – 00198 Roma Italy             Rue Grandgagnage, 21 – B-5000 Namur Belgium
         {velardi,navigli}@di.uniroma1.it                           mpe@info.fundp.ac.be

                    Abstract                          The target groups of the INTEROP KMap system are:
                                                      
   This paper describes a methodology  to semi-           Members  of the KMap management team, who are in
   automatically acquire a taxonomy of terms and          charge of producing a periodic diagnostics of current
   term definitions in a specific research domain. The    research in interoperability performed by INTEROP
   taxonomy is then used for semantic search and          partners in the first place and in Europe in the second
   indexing  of a  knowledge  base of  scientific         place;
                                                      
   competences, called Knowledge Map. The KMap            INTEROP partners   who contribute with information
   is a system to support research collaborations and     about their research and that of other researchers in the
   sharing of results within and beyond a European        domain of interoperability, and retrieve knowledge
   Network  of  Excellence. The methodology  is           about the current status of interoperability research;
                                                      
   general and can be applied to model any web            The   scientific community   in  the   field of
   community - starting from the documents shared         interoperability, including universities, research
   and exchanged among the community members -            institutes, researchers, companies, etc.
   and to use this model for improving accessibility of These objectives and targets can be considered relevant for
   data and knowledge repositories.                   any scientific web community in any research field.

                                                                                  Recommendations
                                                                                      for new
1  Introduction                                                                     collaborations
                                           1
The  NoE  (Network  of Excellence) INTEROP   is an           Researcher
                                                                          Evaluator
instrument for strengthening excellence of European
research in interoperability of enterprise applications, by
bringing together the complementary competences needed
to develop interoperability in a more global and innovative     INTEROP
                                INTEROP                        Knowledge
way. One of the main objectives of      has been to               Map              Recommendations
                                                                                     for orientation
build a so-called “Knowledge Map” (KMap) of partner                      Research
                                                               Classification         of research
                                                                       and collaboration
                                                               Framework
competences, to perform a periodic diagnostics of the extent           description data
of research collaboration and coordination among the NoE
members. The aim is to monitor the status of research in the
field of interoperability through a web-based platform that
allows the user to retrieve information according to his/her Figure 1. The INTEROP KMap.
actual need in a specific situation.
The main benefits of the KMap (Figure 1) for its users are: The KMap is a Knowledge Management application,
  To be able to diagnose current interoperability research exploiting recent research results in the area of Semantic
   inside INTEROP and in Europe;                      Web, Text Mining, Information Retrieval and Ontology
  To  receive an overview of all European research   Enrichment. These techniques have been put in place to
   activities on interoperability and subordinated topics; create a semantically indexed information repository, storing
  To receive an overview of organisations and experts as data on active collaborations, projects, research results, and
   well as research results;                          organizations. A query interface allows users to retrieve
  To find relevant information for specific needs quickly; information about partner collaborations, research results
  To find potential partners for collaborating in research and available (or missing) competences, as well as to obtain
   activities.                                        summarized information (presented in a graphical or tabular
                                                      format) on  the overall degree of  collaboration and
   1 http://www.interop-noe.org (2003-2007), NoE-IST 508011.

                                                IJCAI-07
                                                  2897overlapping competence, based on a measure of semantic    1998], and tailored to the domain using a word sense
similarity between pieces of information.                 disambiguation algorithm. The  validated set of
The paper is organized as follows: first, we provide a    hypernymy relations is used to automatically structure
general picture of the knowledge acquisition value chain, the terms in L into a forest F of taxonomically ordered
and the motivations behind the adopted approach. Then, we sub-trees;
summarize the learning techniques used to bootstrap the 5. (M) A taxonomy management and validation web
creation of a domain taxonomy (currently evolving towards application is used to: i) manually create a taxonomy C
an ontology). Finally, we describe the implementation and of “core” domain   terms ii) enrich C  with the
preliminary results of the semantically indexed KMap.     automatically created sub-trees F, and iii) allow a
Related research and future activities are dealt with in the collaborative validation of the resulting taxonomy, T;
Concluding Remarks section.                           6.  (M+A) The same application is being used (this is an
                                                          in-progress activity) to let the taxonomy evolve towards
2  The Knowledge Acquisition Value Chain                  the full power of an  ontology. Again, automatic
                                                          techniques are used to start the ontology enrichment
Figure 2 schematizes the Knowledge Acquisition Value      process, followed by a validation and refinement task.
Chain adopted in INTEROP. Progressively richer knowledge
structures (Lexicon, Glossary, Taxonomy, Ontology) are The idea behind this approach is that, despite many
first bootstrapped through automatic text mining techniques, progresses in the area of ontology building and knowledge
and then refined through manual validation and enrichment, acquisition, automated techniques cannot fully replace the
supported by appropriate tools and collaborative web  human experts and the stakeholders of a semantic web
interfaces. Each knowledge structure builds on previously application. On the other side, manual ontology building
acquired knowledge, e.g. automatic glossary extraction methods as for example METHONTOLOGY [Fernández et al.
exploits knowledge on domain terminology (the lexicon), 1997] or the NASA taxonomy development framework
automatic identification of taxonomic relations is based on [Dutra and Busch, 2003] are very costly and require an
glossary parsing, etc.                                effort in terms of time and competences, not affordable by
                                                      loosely structured web communities. In our view, automated
                                                      procedures are useful to achieve a significant speed-up
                Knowledge Management and              factor in the development of semantic resources, but human
                       Validation                     validation and refinement is unavoidable when resources are
                                                      to be used in real environments and applications.
                                                      In the rest of this paper, we overview the methodologies
         lexicon  glossary taxonomy   ontology
                                                      used for bootstrapping the knowledge acquisition process.
                                                      Because of space restrictions, and because some of the
                                                      methods have been already described in literature (see
              Text Mining and Machine Learning
                      Algorithms                      [Velardi et al., 2007] for steps 1 and 2 of the procedure
                                                      outlined above), we provide details only on the taxonomy
                                                      ordering algorithm (step 4). As far as the validation tasks are
Figure 2. The Knowledge Acquisition Value Chain.      concerned, (steps 3 and 5), only the final results of the
In short, the steps of the knowledge acquisition chain are the evaluation are presented and discussed; to obtain details, the
following2 (steps marked A are automatic, steps marked M interested reader is invited to access the deliverables of the
are manual, supported by web applications):           project3.
1. (A) Text and documents exchanged by the members of 2.2  Learning a domain terminology and glossary
   the research community are parsed to extract a set of Web communities (groups of interest, web enterprises,
   domain terms, constituting the domain lexicon L;   research communities) share information in an implicit way
2. (A) For each term t in L, one or more definitions are through the exchange of mail, best practices, white papers,
   automatically extracted from the available documents publications, announcements, etc. In INTEROP,many
   and from the web, constituting the glossary G;     documents   (state of  arts, deliverables, workshop
3. (M) The lexicon and glossary are validated through a proceedings, etc.) have been stored on the network
   collaborative web application by all the members of the collaborative platform, like state of arts, deliverables,
   community, who are also asked to express a fine-   workshop proceedings, etc.. We applied to these documents
   grained evaluation of the definition quality;      a terminology extraction algorithm based on four measures:
4. (A) Definitions in the validated glossary G are parsed to Lexical Cohesion [Park et al., 2002], Domain Relevance and
   extract hypernymy  (kind-of) relations. Additional Domain Consensus  [Navigli and Velardi, 2004] and Text
   hypernymy relations are extracted from a general-  Layout. The  algorithm puts together among the best
   purpose lexicalized taxonomy, WordNet [Fellbaum,   available term extraction techniques in the literature, and

   2 The examples throughout this paper are in the domain of enterprise
interoperability, but the reader can easily convince him/herself that the 3 http://interop-noe.org/backoffice/deliv/dg-1-interoperability-glossary
outlined procedure is fully general.

                                                IJCAI-07
                                                  2898proved to have a very high precision4 in different domains interface5,firstto validate the lexicon, rejecting
and applications [Navigli and Velardi, 2004]. The output of inappropriate terms in L,andthento express their judgment
this phase is a domain lexicon L.                     on the definitions of the survived terms. The actual decision
For each term t in L, candidate definitions are then searched to reject or accept a term or definition was based on the sum
in the document repository and on the web. Automatic  of all expressed votes (cumulated vote). As far as definitions
extraction of definitions relies on an incremental filtering are concerned, the request was for a fine-grained voting of
process:                                              definition’s quality. Votes were ranging from +1(adequate)
1. (A) Definitions are firstly searched in existing web to -3 (totally wrong). Partners were also requested to add
   glossaries. If not found, simple patterns at the lexical missing definitions (the coverage of the automated gloss
   level (e.g. “tisaY”,“t is defined as Y”, etc.)areused extraction procedure was about 70%) and to manually adjust
   to  extensively search an initial set of candidate some near-good definition. Table I summarizes the results
                                                      of this phase. The  performance  is comparable with
   definitions from web documents. Let D be the set of
                                      t               published results (e.g. [Park et al. 2002]) but the “real life”
   candidate definitions for each t in L.
                                                      value of the experiment increases its relevance. In the
2. (A) On the set Dt a first statistical filtering is applied, to literature, evaluation is mostly performed by two or three
   verify domain pertinence. A statistical indicator of domain experts with adjudication, or by the authors
                                             
   pertinence is computed for each definition dt Dt,  themselves. In our case, the validation was performed by an
   based on the number and statistical relevance of domain entire research community with rather variable expertise
   words (e.g. those in L) occurring in dt;           (mainly: enterprise modeling, architectures and platforms,
3. (A) A subsequent stylistic filtering is applied, based on knowledge management) and different views of the
   fine-grained regular expressions at the lexical, part-of- interoperability domain.
   speech and syntactic level. The objective is to select
   “well-formed” definitions, i.e. definitions expressed in Number of partners who voted the lexicon 35
   terms of genus (the kind a concept belongs to) and  Total expressed votes                 2453
   differentia (what specializes the concept with respect to Accepted terms                  1120 (59%)
   its kind).                                          Number of partners who voted the glossary 15
There are three advantages in applying the stylistic filtering Total expressed votes         2164
                                                       Analysed definitions                  1030
criterion: i) To prefer definitions adhering to a uniform Accepted definitions               595 (57.76%)
style, commonly adopted by professional lexicographers. Reviewed definitions6 (cumulated vote =-1) 260 (25.24%)
For example, the following definition is not well-formed in Rejected definitions (cumulated vote <-1) 175 (17%)
the stated sense: “component integration is obtained by New definitions added (terms without definition) 108
composing the component's refinement structures together, Table I. Result of collaborative lexicon and glossary evaluation.
resulting in (larger) refinement structures which can be 2.2.2 Computing the Speed-up factor for the Glossary
further used as components”, ii) To be able to distinguish The need to use automated glossary learning techniques in
definitions from non-definitions (especially when candidate INTEROP was motivated by the absence of skilled personnel
definitions are extracted from free texts, rather than to create a high quality glossary, but most of all, by the fact
glossaries). For example, “component integration has been that the knowledge domain of the INTEROP community was
recently proposed to provide a solution for those issues”is vaguely defined (as it often happens in emerging
not a definition; iii) To be able to extract from definitions communities), making it particularly difficult to identify the
the kind-of information, subsequently used to  help   truly pertinent domain concepts. However, as already
taxonomic ordering of terms. For example: ”In   the   remarked, the aim of the learning procedure described so far
traditional software engineering perspective, domain model is not to replace humans, but to significantly reduce the time
is a   precise representation of  specification and   needed to build lexico-semantic resources.
implementation concepts that define a class of existing To our knowledge, and after a careful study of the relevant
systems” is well-formed, and its parsing returns the  literature, no precise data are available on glossary
hypernym: representation.                             development costs, except for [Kon and Hoey, 2005] in
The regular expressions used for stylistic filtering are which a cost of 200-300 dollars per term is estimated, but no
domain-general, and the  patterns are learned from    details are given to motivate the estimate. We consulted
definitions in professional glossaries on the web.    several sources, finally obtaining the opinion of an
                                                      experienced professional lexicographer7 who has worked for
2.2.1 Collaborative Lexicon and Glossary Validation   many important publishers. The lexicographer outlined a
During the subsequent evaluation phase, all INTEROP   three-step procedure for glossary acquisition including: i)
partners were requested, through a collaborative voting internet search of terms ii) production of definitions iii)

                                                         5 http://interop-noe.org/backoffice/workspaces/wpg/ps_interop
                                                         6
   4 The interested reader can experiment through a web application, All the definitions with a cumulated vote “-1” (i.e. only one partner
TermExtractor, available from http://lcl.di.uniroma1.it, which allows users expressed the opinion “not fully adequate”) where manually adjusted.
to upload a document archive, obtain a domain terminology, and validate it. 7 We thank Orin Hargraves for his very valuable comments.


                                                IJCAI-07
                                                  2899harmonization of definitions style. The lexicographer        integration
evaluated the average time spent in each step in terms of 6          representation integration
minutes, 10 min. and 6 min. per definition, respectively.            model integration
Notice that the creation of a list of relevant terms (lexicon)              enterprise model integration
is not included in this computation. The lexicographer also          schema integration
pointed out that conducting this process with a team of              ontology integration
experts could be rather risky in terms of time, however he           knowledge integration
admits that in very new fields the support of experts is             data integration
necessary, and this could significantly increase the above
                                                                     information integration
figures (however he did not provide an estimate of this
increase). Starting from the professional lexicographer’s This “string inclusion” heuristics created a forest of 621
figures, that clearly represent a sort of “best case” isolated trees out of the 1120 validated terms in L (cf. Table
performance, we attempted an evaluation of the obtained I).
speed-up. The glossary acquisition procedure has three 2. (M) The trees in F are manually connected to a core
phases in which man-power is requested: lexicon and      taxonomy8 C of high-level concepts, defined by a team
glossary validation, and manual refinement of definitions. of experts who basically reused WordNet and previous
Each of these phases require from few seconds to few                                       9
                                                         available work on enterprise ontologies .LetT0=CF
minutes, but actions are performed both on “wrong” and   be the resulting, fully connected, taxonomy.
“good” data (with respect to the results of Table I, to obtain
                                                      In the INTEROP domain, C includes 286 concepts and
83 “good” definitions, 100 must be inspected: 58 of them                                               T0
just accepted, 25 to be manually adjusted, etc.). We omit for includes 1406 nodes in total.
the sake of space the details of the computation that led to 3. (A) The set of multi-word concept names in T0 is
over 50%  speed up with respect to the lexicographer’s   decomposed in a list L’ of singleton words, to which we
estimate. In this comparison we exclude the stylistic    added also the hypernyms automatically extracted from
harmonization (step (iii) of the lexicographer’s procedure), definitions. For example, if T0 is the sub-tree STint , L’ is:
which is indeed necessary to obtain a good quality glossary. representation, integration, model, data, ontology,
However, since this phase would be necessarily manual in specification, information, etc. Terms in L’ are used to
both cases, it does not influence the computation of the search hypernymy  relations in the WordNet sense
speed-up factor.                                         inventory. For example:
2.3 Learning taxonomic relations                                 representation#n#2  knowledge#n#1
                                                                 scheme#n#1  representation#n#2
The application of the well-formedess criterion discussed in            
section 2.2 (implemented with regular expressions), allows       data#n#1 information#n#2
                                                                           
to extract from definitions the kind-of information, as          workflow#n#1 development#n#1
defined by the author of a definition. This information may All the WordNet word senses in the above example have
help structuring the terms of L in taxonomic order.      a lexical counterpart in L’.LetRWn be the set of
However, ordering terms according to the hypernyms       extracted hypernymy relations.
                                               [
extracted from definitions has well-known drawbacks Ide Some of the senses in R  are not appropriate in the
and  Véronis, 1994]. Typical problems  found  when                            Wn
                                                      interoperability domain, e.g.: architecture#n#3  activity#n#1,
attempting to  extract (manually  or  automatically)
hypernymy relations from natural language definitions, are: which refers to the “profession” sense of architecture rather
over-generality of the provided hypernym (e.g. “Constraint than to computer architecture (sense #4 in WordNet).
checking is one of many techniques…”), unclear choices for However, the objective is to apply these relations in a
more general terms, or-conjoined hypernyms (e.g. “Non- restrictive way, i.e. only to sibling terms in T0. For example,
functional aspects define the overall qualities or attributes the first rule of the above list can be used to move a term
of a system”), absence of hypernym (e.g. “Ontological startingwith“representation” below a term starting with
analysis is accomplished by examining the vocabulary  “knowledge”iffif these two terms are siblings in some sub-
that…”), circularity of definitions, etc. These problems – tree of T0 (e.g. in STint). The number of “applicable” rules is
                                                                                T0 
especially over-generality – are more or less evident when therefore reduced to a subset RWn RWn .
analysing the hypernyms learned through glossary parsing. In our domain, L’ includes 607 different words, (since
To  reduce these problems, we defined the following   certain words occur many times in terminological strings),
procedure:                                                                                T0
                                                      RWn includes 4015 kind-of relations, but R Wn includes only
1. (A) First, terms in the lexicon L are ordered according 67 relations.
   to simple string inclusion. String inclusion is a very
   reliable indicator of a taxonomic relation, though it
   does not capture all possible relations. This step    8 “core” is a very basic and minimal taxonomy consisting only of the
                                                      minimal concepts required to understand the other concepts.
   produces a forest F of sub-trees. Let STint be one of such 9
   trees, for example:                                    We omit details of this work, for the sake of space and because it is
                                                      not central to the purpose of this paper.


                                                IJCAI-07
                                                  29004. (A) An on-line word sense disambiguation algorithm, (“find all the results – papers and projects – dealing with a
   SSI [Navigli and Velardi, 2005], is used to detect wrong subset of concepts in the taxonomy”). The user can select
        10    T0
   senses in R  Wn, with respect to the domain. We use concepts (referred to as knowledge domains,orsimply
   SSI to disambiguate each word in L’ that appears in at domains, in the query interface) by “string search” in the
                                    T0
   least one of the kind-of relations in R Wn. The context taxonomy (as in the example of Figure 3), they can arrange
   for disambiguation is provided by co-occurring words in concepts in boolean expressions, and perform query
   each sub-tree, e.g. in STint: representation, integration, expansion (including in the query all or some of the
                                       T0
   model,etc.LetRSSI be the relations in R Wn survived concept’s hyponyms).
   after this step.
Step 4 returned 196 sense selections, which have been
manually validated by two judges. 158 sense selections
(80.62%) were judged as correct, given the domain.

5. (A) Relations in RSSI are used to restructure T0.For
   example, according to the relations available in RSSI (e.g.
   those in the example of step 3), STint becomes:
         knowledge integration
            representation integration
              schema integration
            model integration
              enterprise model integration
            information integration
              data integration
            ontology integration

Let T1 be the resulting taxonomy after step 5. Following the
learn-and-validate methodology adopted throughout the Figure 3. Taxonomy-based search of INTEROP Research Results.
project, a web interface11 has been developed to allow a
collaborative validation of T1. Table II provides a summary It is also possible to obtain “global” information, e.g. a map
of the validation task.                               of member’s competence  similarity,orananalysisof
 Number of partners who voted the taxonomy    11      research results similarity. Figure 4 shows the screen dump
 Total number of activated polls              21      of a graph in which nodes represent INTEROP organizations
 Total number of performed actions            34      and the similarity value is highlighted by the thickness of
 Of which: Movement of single terms or term sub-trees 25
          Deleted core nodes                  3       edges. The number shown on each edge is the result of a
          Created core nodes                  6       semantic similarity computation (see [Velardi et al., 2007]
Table II. Results of collaborative taxonomy validation. for details). In short, the information (text or data)
                                                      concerning each organization and its affiliated partners, is
In Table II, “activated polls” refers to the fact that before automatically parsed, and a weighted vector PM of
making a change, partners need to activate a poll and taxonomy concepts is associated to each member M. The
receive consensus. The table shows that only 25 moves have well-known cosine-similarity measure is computed between
been approved. A  comparison between the number of    vector pairs, but rather than considering only direct matches
actions performed by partners in Table I and Table II between terms, we also consider indirect matches,i.e.term
suggests that domain specialists can easily perform certain
                                                      pairs tx  PM1 and ty  PM2 related by direct (tx  ty)orsemi-
tasks (i.e. lexicon pruning) but are less confident when
                                                      direct hypernymy relations (tx  tty).
asked to contribute in creating progressively more “abstract” In the current version of the KMap (to be enhanced in the
representations of their domain of expertise (from glossary last year of the project) indirect matches represent a 38.41%
to taxonomy and, eventually, to an ontology). This seems to
                                                      of the total matches used to compute partner similarity.
further support the use of automated techniques.

3  Semantic Indexing and Semantic Search              4   Related Work and Concluding Remarks
                                                      This paper illustrated (in a forcefully sketchy way) a
The taxonomy created through the procedure illustrated so
                                                      complete application of Semantic Web techniques to the
far has been used to semantically index the INTEROP KMap.
                                                      task of modeling the competences of a web-based research
Figure 3 shows the screen dump of a possible query type
                                                      community, INTEROP. We are not aware of any example of
                                                      fully implemented knowledge  acquisition value chain,
   10 In principle, the domain appropriateness of the 67 hypernymy where the acquired knowledge is first, extensively validated
relations could be verified manually, given the limited dimension of the through the cooperative effort of an entire web community,
task, but we used a WSD algorithm for the sake of generality. and then, put in operation, to improve accessibility of web
   11 Available online from http://lcl.di.uniroma1.it/tav


                                                IJCAI-07
                                                  2901