                               Nogood Recording from Restarts

        Christophe Lecoutre     and  Lakhdar Sais   and  Sebastien´ Tabary   and Vincent Vidal
                                        CRIL−CNRS FRE 2499
                                           Universit´e d’Artois
                                              Lens, France
                           {lecoutre,sais,tabary,vidal}@cril.univ-artois.fr

                    Abstract                          chus, 2003; 2005] for CSP. While standard nogoods corre-
                                                      spond to variable assignments, generalized nogoods also in-
    In this paper, nogood recording is investigated   volve value refutations. These generalized nogoods beneﬁt
    within the randomization and restart framework.   from nice features. For example, they can compactly cap-
    Our goal is to avoid the same situations to occur ture large sets of standard nogoods and are proved to be more
    from one run to the next one. More precisely, no- powerful than standard ones to prune the search space.
    goods are recorded when the current cutoff value    As the set of nogoods that can be recorded might be of ex-
    is reached, i.e. before restarting the search algo- ponential size, one needs to achieve some restrictions. For ex-
    rithm. Such a set of nogoods is extracted from the ample, in SAT, learned nogoods are not minimal and are lim-
    last branch of the current search tree. Interestingly, ited in number using the First Unique Implication Point (First
    the number of nogoods recorded before each new    UIP) concept. Different variants have been proposed (e.g. rel-
    run is bounded by the length of the last branch of evance bounded learning [Bayardo and Shrag, 1997]), all of
    the search tree. As a consequence, the total number them attempt to ﬁnd the best trade-off between the overhead
    of recorded nogoods is polynomial in the number   of learning and performance improvements. Consequently,
    of restarts. Experiments over a wide range of CSP the recorded nogoods cannot lead to a complete elimination
    instances demonstrate the effectiveness of our ap- of redundancy in search trees.
    proach.                                             In this paper, nogood recording is investigated within the
                                                      randomization and restart framework. The principle of our
1  Introduction                                       approach is to learn nogoods from the last branch of the
Nogood recording (or learning) has been suggested as a search tree before a restart, discarding already explored parts
technique to enhance CSP (Constraint Satisfaction Problem) of the search tree in subsequent runs. Roughly speaking, we
solving in [Dechter, 1990]. The principle is to record a manage nogoods by introducing a global constraint with a
nogood whenever a conﬂict occurs during a backtracking dedicated ﬁltering algorithm which exploits watched literals
                                                      [                   ]
search. Such nogoods can then be exploited later to pre- Moskewicz et al., 2001 . The worst-case time complexity of
                                                                                 n2γ       n
vent the exploration of useless parts of the search tree. The this propagation algorithm is O( )where is the number
                                                                    γ
ﬁrst experimental results obtained with learning were given of variables and the number of recorded nogoods. Besides,
                                                                  γ           ndρ       d
in the early 90’s [Dechter, 1990; Frost and Dechter, 1994; we know that is at most where is the greatest do-
                                                                  ρ
Schiex and Verfaillie, 1994].                         main size and is the number of restarts already performed.
                                                                                                    [
  Contrary to CSP, the recent impressive progress in SAT Remark that a related approach has been proposed in Bap-
                                                                   ]
(Boolean Satisﬁability Problem) has been achieved using tista et al., 2001 for SAT in order to obtain a complete restart
nogood recording (clause learning) under a randomization strategy while reducing the number of recorded nogoods.
and restart policy enhanced with a very efﬁcient lazy data
structure [Moskewicz et al., 2001]. Indeed, the interest of 2 Technical Background
clause learning has arisen with the availability of large in- A Constraint Network (CN) P is a pair (X , C ) where X is
stances (encoding practical applications) which contain some asetofn variables and C asetofe constraints. Each variable
structure and exhibit heavy-tailed phenomenon. Learning X ∈ X has an associated domain, denoted dom(X),which
in SAT is a typical successful technique obtained from the contains the set of values allowed for X. Each constraint C ∈
cross fertilization between CSP and SAT: nogood recording C involves a subset of variables of X , denoted vars(C),and
[Dechter, 1990] and conﬂict directed backjumping [Prosser, has an associated relation, denoted rel(C), which contains the
1993] have been introduced for CSP and later imported into set of tuples allowed for vars(C).
SAT solvers [Bayardo and Shrag, 1997; Marques-Silva and A solution to a CN is an assignment of values to all the vari-
Sakallah, 1996].                                      ables such that all the constraints are satisﬁed. A CN is said
  Recently, a generalization of nogoods, as well as an elegant to be satisﬁable iff it admits at least one solution. The Con-
learning method, have been proposed in [Katsirelos and Bac- straint Satisfaction Problem (CSP) is the NP-complete task

                                                IJCAI-07
                                                   131of determining whether a given CN is satisﬁable. A CSP in- Deﬁnition 4 Let P be a CN and Δ be a set of decisions. Δ
stance is then deﬁned by a CN, and solving it involves either is a nogood of P iff P |Δ is unsatisﬁable.
ﬁnding one (or more) solution or determining its unsatisﬁa- From any branch of the search tree, a nogood can be ex-
bility. To solve a CSP instance, one can modify the CN by tracted from each negative decision. This is stated by the fol-
                             [           ]
using inference or search methods Dechter, 2003 .     lowing proposition:
  The backtracking algorithm (BT) is a central algorithm for
solving CSP instances. It performs a depth-ﬁrst search in or- Proposition 1 Let P be a CN and Σ be the sequence of de-
der to instantiate variables and a backtrack mechanism when cisions taken along a branch of the search tree. For any nld-
dead-ends occur. Many works have been devoted to improve subsequence δ1,...,δi of Σ,thesetΔ={δ1,...,¬δi} is
its forward and backward phases by introducing look-ahead a nogood of P (called nld-nogood)1.
and look-back schemes [Dechter, 2003]. Today, MAC [Sabin Proof. As positive decisions are taken ﬁrst, when the negative
and Freuder, 1994] is the (look-ahead) algorithm considered
                                                      decision δi is encountered, the subtree corresponding to the
as the most efﬁcient generic approach to solve CSP instances.
                                                      opposite decision ¬δi has been refuted. 2
It maintains a property called Arc Consistency (AC) during
search. When mentioning MAC, it is important to indicate These nogoods contain both positive and negative deci-
which branching scheme is employed. Indeed, it is possible sions and then correspond to the deﬁnition of generalized no-
                                                            [
to consider binary (2-way) branching or non binary (d-way) goods Focacci and Milano, 2001; Katsirelos and Bacchus,
                                                          ]
branching. These two schemes are not equivalent as it has 2005 . In the following, we show that nld-nogoods can be re-
been shown that binary branching is more powerful (to refute duced in size by considering positive decisions only. Hence,
unsatisﬁable instances) than non-binary branching [Hwang we beneﬁt from both an improvement in space complexity
and Mitchell, 2005]. With binary branching, at each step of and a more powerful pruning capability.
search, a pair (X,v) is selected where X is an unassigned By construction, CSP nogoods do not contain two opposite
                                                                      x = v    x = v
variable and v a value in dom(X), and two cases are consid- decisions i.e. both and . Propositional resolution
                                                                             r =(α∨β)                x∨α
ered: the assignment X = v and the refutation X = v.The allows to deduce the clause   from the clauses
                                                          ¬x ∨ β
MAC algorithm (using binary branching) can then be seen and     . Nogoods can be represented as propositional
as building a binary tree. Classically, MAC always starts by clauses (disjunction of literals), where literals correspond to
assigning variables before refuting values. Generalized Arc positive and negative decisions. Consequently, we can extend
                                                                                                [
Consistency (GAC) (e.g. [Bessiere et al., 2005]) extends AC resolution to deal directly with CSP nogoods (e.g. Mitchell,
                                                          ]
to non binary constraints, and MGAC is the search algorithm 2003 ), called Constraint Resolution (C-Res for short). It can
that maintains GAC.                                   be deﬁned as follows:
  Although sophisticated look-back algorithms such as CBJ Deﬁnition 5 Let P be a CN, and Δ1 =Γ∪{xi = vi} and
(Conﬂict Directed Backjumping) [Prosser, 1993] and DBT Δ2 =Λ∪{xi   = vi} be two nogoods of P . We deﬁne Con-
(Dynamic Backtracking) [Ginsberg, 1993] exist, it has been straint Resolution as C-Res(Δ1, Δ2)=Γ∪ Λ.
shown [Bessiere and R´egin, 1996; Boussemart et al., 2004;
                                                        It is immediate that C-Res(Δ1, Δ2) is a nogood of P .
Lecoutre et al., 2004] that MAC combined with a good vari-
able ordering heuristic often outperforms such techniques. Proposition 2 Let P be a CN and Σ be the sequence of de-
                                                      cisions taken along a branch of the search tree. For any nld-
                                                                 Σ =            Σ       Δ=      (Σ) ∪
3  Reduced nld-Nogoods                                subsequence      δ1,...,δi of ,theset     pos
                                                      {¬δi} is a nogood of P (called reduced nld-nogood).
From now  on, we will consider a search tree built by a
                                                                             Σ           ≥  1
backtracking search algorithm based on the 2-way branching Proof. We suppose that contains k  negative de-
scheme (e.g. MAC), positive decisions being performed ﬁrst. cisions, denoted by δg1 ,...,δgk , in the order that they
                                                               Σ                         Σ
Each branch of the search tree can then be seen as a sequence appear in . The nld-subsequence of with k negative
                                                                  Σ   =                   
of positive and negative decisions, deﬁned as follows: decisions is 1      δ1,...,δg1 ,...,δgk . Its corre-
                                                      sponding nld-nogood is Δ1  =  {δ1,...,δ ,..., δ   ,
                =(X    C )                                                                  g1       gk−1
Deﬁnition 1 Let P     ,   be a CN and (X,v) be a pair ..., ¬δ  }, δ    being now  the last negative decision.
          ∈ X        ∈    (  )                 =             gk   gk−1
such that X    and v   dom X  . The assignment X v    The nld-subsequence of Σ with k − 1 negative decisions is
is called a positive decision whereas the refutation X = v is Σ =          
                                                        2   δ1,...,δg1 ,...,δgk−1 . Its corresponding nld-nogood
called a negative decision. ¬(X = v) denotes X = v and Δ   =  {             ¬     }
                                                      is  2     δ1,...,δg1 ,..., δgk−1 . We now apply C-Res
¬(X = v)       X = v                                                                 
         denotes      .                               between Δ1 and Δ2 and we obtain Δ1 = C-Res(Δ1, Δ2)=
                                                      {                                            ¬   }
Deﬁnition 2 Let Σ=δ1,...,δi,...,δm  be a sequence    δ1,...,δg1 ,...,δgk−2 ,...,δgk−1−1,δgk−1+1, ..., δgk .
of decisions where δ is a negative decision. The sequence The last negative decision is now δgk−2 , which will be
                 i                                                                            − 2
δ1,...,δi is called a nld-subsequence (negative last deci- eliminated with the nld-nogood containing k negative
sion subsequence) of Σ. The set of positive and negative de- decisions. All the remaining negative decisions are then
cisions of Σ are denoted by pos(Σ) and neg(Σ), respectively. eliminated by applying the same process. 2
Deﬁnition 3 Let P be a CN and Δ be a set of decisions.  One interesting aspect is that the space required to store
                                                      all nogoods corresponding to any branch of the search tree is
P |Δ is the CN obtained from P s.t., for any positive decision
(X =  v) ∈ Δ, dom(X) is restricted to {v}, and, for any  1          {δ ,...,¬δ }           {δ  ∈ Σ  | j<
                  = ) ∈ Δ                               The notation 1      i corresponds to j
negative decision (X v    , v is removed from dom(X). i}∪{¬δi}, reduced to {¬δ1} when i =1.

                                                IJCAI-07
                                                   132                                                      ﬁrst, a δi (resp. ¬δi) corresponds to a positive (resp. neg-
                                δ1                    ative) decision. Search has been stopped after refuting δ11
                                                      and taking the decision ¬δ11. The nld-nogoods of P are
                              ¬
                      δ2        δ2                    the following: Δ1 =  {δ1, ¬δ2, ¬δ6,δ8, ¬δ9,δ11}, Δ2 =
                                                      {δ1, ¬δ2, ¬δ6,δ8,δ9}, Δ3 = {δ1, ¬δ2,δ6}, Δ4 = {δ1,δ2}.
             δ3     ¬δ3         δ6      ¬δ6
                                                      The ﬁrst reduced nld-nogood is obtained as follows:
                                                          
                                                        Δ1 =  C-Res(C-Res(C-Res(Δ1, Δ2), Δ3), Δ4)
       δ4    ¬δ4          δ7   ¬δ7      δ8
                                                           =  C-Res(C-Res({δ1, ¬δ2, ¬δ6,δ8,δ11}, Δ3), Δ4)
                                                           =  C-Res({δ1, ¬δ2,δ8,δ11}, Δ4)
                                        ¬
  δ5   ¬δ5                        δ9     δ9                =  {δ1,δ8,δ11}
                                                        Applying the same process to the other nld-nogoods, we
                            δ10 ¬δ10     δ11  ¬δ11    obtain:
                                                          
                                                        Δ2 =  C-Res(C-Res(Δ2, Δ3), Δ4)={δ1,δ8,δ9}.
                                                          
                                                        Δ3 =  C-Res(Δ3, Δ4)={δ1,δ6}.
                                                        Δ =Δ    = {     }
   Figure 1: Area of nld-nogoods in a partial search tree 4    4    δ1,δ2 .
                                                        In order to avoid exploring the same parts of the search
                                                      space during subsequent runs, recorded nogoods can be ex-
polynomial with respect to the number of variables and the ploited. Indeed, it sufﬁces to control that the decisions of the
greatest domain size.                                 current branch do not contain all decisions of one nogood.
                             Σ                        Moreover, the negation of the last unperformed decision of
Proposition 3 Let P be a CN and be the sequence of de- any nogood can be inferred as described in the next section.
cisions taken along a branch of the search tree. The space
                                          2 2         For example, whenever the decision δ1 is taken, we can infer
complexity to record all nld-nogoods of Σ is O(n d ) while ¬         Δ     ¬              Δ
                                                 Σ     δ2 from nogood  4 and δ6 from nogood  3.
the space complexity to record all reduced nld-nogoods of Finally, we want to emphasize that reduced nld-nogoods
    n2d
is O(  ).                                             extracted from the last branch subsume all reduced nld-
Proof. First, the number of negative decisions in any branch nogoods that could be extracted from any branch previously
is O(nd). For each negative decision, we can extract a (re- explored.
duced) nld-nogood. As the size of any (resp. reduced) nld-
nogood is O(nd) (resp. O(n) since it only contains positive 5 Managing Nogoods
decisions), we obtain an overall space complexity of O(n2d2)
(resp. O(n2d)). 2                                     In this section, we now show how to efﬁciently exploit re-
                                                      duced nld-nogoods by combining watched literals with prop-
                                                      agation. We then obtain an efﬁcient propagation algorithm
4  Nogood Recording from Restarts                     enforcing GAC on all learned nogoods that can be collec-
In [Gomes et al., 2000], it has been shown that the runtime tively considered as a global constraint.
distribution produced by a randomized search algorithm is
sometimes characterized by an extremely long tail with some 5.1 Recording Nogoods
inﬁnite moment. For some instances, this heavy-tailed phe- Nogoods derived from the current branch of the search tree
nomenon can be avoided by using random restarts, i.e. by (i.e. reduced nld-nogoods) when the current run is stopped
restarting search several times while randomizing the em- can be recorded by calling the storeNogoods function (see
ployed search heuristic. For constraint satisfaction, restarts Algorithm 1). The parameter of this function is the sequence
have been shown productive. However, when learning is of literals labelling the current branch. As observed in Sec-
not exploited (as it is currently the case for most of the aca- tion 3, a reduced nld-nogood can be recorded from each neg-
demic and commercial solvers), the average performance of ative decision occurring in this sequence. From the root to
the solver is damaged (cf. Section 6).                the leaf of the current branch, when a positive decision is en-
  Nogood recording has not yet been shown to be quite con- countered, it is recorded in the set Δ (line 4),andwhena
vincing for CSP (one noticeable exception is [Katsirelos and negative decision is encountered, we record a nogood from
Bacchus, 2005]) and, further, it is a technique that leads, the negation of this decision and all recorded positive ones
when uncontrolled, to an exponential space complexity. We (line 9). If the nogood is of size 1 (i.e. Δ=∅), it can be di-
propose to address this issue by combining nogood recording rectly exploited by reducing the domain of the involved vari-
and restarts in the following way: reduced nld-nogoods are able (see line 7). Otherwise, it is recorded, by calling the
recorded from the last (and current) branch of the search tree addNogood function (not described here), in a structure ex-
between each run. Our aim is to beneﬁt from both restarts and ploiting watched literals [Moskewicz et al., 2001].
learning capabilities without sacriﬁcing solver performance We can show that the worst-case time complexity of
and space complexity.                                 storeNogoods  is O(λpλn)whereλp  and λn are the num-
  Figure 1 depicts the partial search tree explored when the ber of positive and negative decisions on the current branch,
solver is about to restart. Positive decisions being taken respectively.

                                                IJCAI-07
                                                   133Algorithm 1 storeNogoods(Σ=δ1,...,δm)               Algorithm 2 propagate(S : Set of variables) : Boolean
 1: Δ ←∅                                               1: Q ← S
 2: for i ∈ [1,m] do                                   2: while Q = ∅ do
                                                                       X      Q
 3:  if δi is a positive decision then                 3:   pick and delete from
                                                             | dom(X) |
 4:    Δ ←  Δ ∪{δi}                                    4:   if         =1then
 5:  else                                              5:    let v be the unique value in dom(X)
 6:    if Δ=∅ then                                     6:    if checkWatches(X = v)=false then return false
 7:      with δi =(X, v), remove v from dom(X)         7:   end if
 8:    else                                            8:   for each C | X ∈ vars(C) do
                                                                    Y ∈ Vars(C) | X = Y
 9:      addNogood(Δ ∪{¬δi})                           9:    for each                  do
10:    end if                                         10:      if revise(C,Y ) then
11:  end if                                           11:        if dom(Y )=∅ then return false
                                                                    Q  ← Q ∪{Y }
12: end for                                           12:        else
                                                      13: end while
                                                      14: return true
5.2  Exploiting Nogoods
Inferences can be performed using nogoods while establish- of exploiting nogoods to enforce GAC is O(n2γ). Indeed,
ing (maintaining) Generalized Arc Consistency. We show it checkW atches is O(nγ) and it can be called only once per
with a coarse-grained GAC algorithm based on a variable- variable.
oriented propagation scheme [McGregor, 1979]. The Algo- The space complexity of the structures introduced to man-
rithm 2 can be applied to any CN (involving constraints of any age reduced nld-nogoods in a backtracking search algorithm
arity) in order to establish GAC. At preprocessing, propagate is O(n(d + γ)). Indeed, we need to store γ nogoods of size at
must be called with the set S of variables of the network most n and we need to store watched literals which is O(nd).
whereas during search, S only contains the variable involved
in the last positive or negative decision. At any time, the prin- 6 Experiments
cipleistohaveinQ  all variables whose domains have been
reduced by propagation.                               In order to show the practical interest of the approach de-
  Initially, Q contains all variables of the given set S (line scribed in this paper, we have conducted an extensive experi-
1). Then, iteratively, each variable X of Q is selected (line mentation (on a PC Pentium IV 2.4GHz 512Mo under Linux).
3). If dom(X) corresponds to a singleton {v} (lines 4 to We have used the Abscon solver to run M(G)AC2001 (de-
7), we can exploit recorded nogoods by checking the consis- noted by MAC) and studied the impact of exploiting restarts
tency of the nogood base. This is performed by the function (denoted by MAC+RST) and nogood recording from restarts
checkW atches (not described here) by iterating all nogoods (denoted by MAC+RST+NG). Concerning the restart policy,
involving X = v as watched literal. For each such nogood, the initial number of allowed backtracks for the ﬁrst run has
                                                                10                         1 5
either another literal not yet watched can be found, or an in- been set to and the increasing factor to . (i.e., at each
                                                                                                      1 5
ference is performed (and the set Q is updated).      new run, the number of allowed backtracks increases by a .
  The rest of the algorithm (lines 8 to 12) corresponds to the factor). We used three different variable ordering heuristics:
body of a classical generic coarse-grained GAC algorithm. the classical brelaz [Brelaz, 1979] and dom/ddeg [Bessiere
For each constraint C binding X, we perform the revision and R´egin, 1996] as well as the adaptive dom/wdeg that has
of all arcs (C, Y ) with Y = X. A revision is performed by been recently shown to be the most efﬁcient generic heuris-
a call to the function revise, speciﬁc to the chosen coarse- tic [Boussemart et al., 2004; Lecoutre et al., 2004; Hulubei
grained arc consistency algorithm, and entails removing val- and O’Sullivan, 2005; van Dongen, 2005]. Importantly, when
ues that became inconsistent with respect to C. When the re- restarts are performed, randomization is introduced in brelaz
vision of an arc (C, Y ) involves the removal of some values and dom/ddeg to break ties. For dom/wdeg, the weight
in dom(Y ), revise returns true and the variable Y is added of constraints are preserved from each run to the next one,
to Q. The algorithm loops until a ﬁxed point is reached. which makes randomization useless (weights are sufﬁciently
  The worst-case time complexity of checkW atches is  discriminant).
  nγ       γ                                            In our ﬁrst experimentation, we have tested the three al-
O(  )where   is the number of reduced nld-nogoods stored                      1064
in the base and n is the number of variables2. Indeed, in the gorithms on the full set of instances used as bench-
worst case, each nogood is watched by the literal given in pa- marks for the ﬁrst competition of CSP solvers [van Don-
                                                      gen, 2005]. The time limit to solve an instance was ﬁxed
rameter, and the time complexity of dealing with a reduced 30
nld-nogood in order to ﬁnd another watched literal or make to minutes. Table 1 provides an overview of the results
               n                                      in terms of the number of instances unsolved within the time
an inference is O( ). Then, the worst-case time complexity #
of propagate is O(er2dr + n2γ)wherer is the greatest con- limit ( timeouts) and the average cpu time in seconds (avg
straint arity. More precisely, the cost of establishing GAC time) computed from instances solved by all three methods.
(using a generic approach) is O(er2dr) when an algorithm Figures 2 and 3 represent scatter plots displaying pairwise
such as GAC2001 [Bessiere et al., 2005] is used and the cost comparisons for dom/ddeg and dom/wdeg. Finally, Table 2
                                                      focuses on the most difﬁcult real-world instances of the Radio
  2In practice, the size of reduced nld-nogoods can be far smaller Link Frequency Assignment Problem (RLFAP). Performance
than n (cf. Section 6).                               is measured in terms of the cpu time in seconds (no timeout)

                                                IJCAI-07
                                                   134                                  MAC                                                 MAC
                              + RST    + RST + NG                                  + RST    + RST + NG
             #timeouts  365      378           337                   cpu     0.85     0.84         0.84
  dom/ddeg                                                scen11-f12
             avg time  125.0    159.0         109.1                 nodes    695      477           445
             #timeouts  277      298           261                   cpu     0.95     0.82         1.03
   brelaz                                                 scen11-f10
             avg time   85.1    121.7          78.2                 nodes    862      452           636
             #timeouts  140      123           121                   cpu     14.6     1.8           1.9
  dom/wdeg                                                scen11-f8
             avg time   47.8     56.0          43.6                 nodes   14068    1359          1401
                                                                     cpu     185      9.4           8.4
                                                          scen11-f7
                                                                    nodes   207K     9530          8096
                                                                     cpu     260      21.8         16.9
Table 1: Number of unsolved instances and average cpu time scen11-f6
                                         30                         nodes   302K    22002         16423
on the 2005 CSP competition benchmarks, given minutes                cpu    1067      105          82.3
                                                          scen11-f5
CPU.                                                                nodes  1327K     117K         90491
                                                                     cpu    2494      367           339
                                                          scen11-f4
                                                                    nodes  2826K     419K         415K
                                                                     cpu    9498     1207          1035
and the number of visited nodes. An analysis of all these re- scen11-f3
sults reveals three main points.                                    nodes   12M     1517K        1286K
                                                                     cpu     29K     3964          3378
                                                          scen11-f2
Restarts (without learning) yields mitigated results. First,        nodes   37M     5011K        4087K
                                                                     cpu     69K     9212          8475
                                                          scen11-f1
we observe an increased average cpu time for all heuristics         nodes   93M      12M           10M
and fewer solved instances for classical ones. However, a
close look at the different series reveals that MAC+RST com-
bined with brelaz (resp. dom/ddeg)solved27 (resp. 32) Table 2: Performance on hard RLFAP Instances using the
less instances than MAC on the series ehi. These instances dom/wdeg heuristic (no timeout)
correspond to random instances embedding a small unsatis-
ﬁable kernel. As classical heuristics do not guide search to- troduced for SAT. One can consider the base of nogoods as a
wards this kernel, restarting search tends to be nothing but unique global constraint with an efﬁcient associated propaga-
an expense. Without these series, MAC+RST would have  tion algorithm.
solved more instances than MAC (but, still, with worse per- Our experimental results show the effectiveness of our
formance). Also, remark that dom/wdeg renders MAC+RST approach since the state-of-the-art generic algorithm MAC-
more robust than MAC (even on the ehi series).        dom/wdeg is improved. Our approach not only allows to
Nogood recording from restarts improves MAC perfor-   solve more instances than the classical approach within a
mance. Indeed, both the number of unsolved instances and given timeout, but also is, on the average, faster on instances
the average cpu time are reduced. This is due to the fact that solved by both approaches.
the solver never explores several times the same portion of
the search space while beneﬁting from restarts.       Acknowledgments
Nogood recording from restarts applied to real-world in- This paper has been supported by the CNRS and the ANR
stances pays off. When focusing to the hardest instances [van “Planevo” project noJC05 41940.
Dongen, 2005] built from the real-world RLFAP instance
scen-11, we can observe in Table 2 that using a restart policy References
allows to be more efﬁcient by almost one order of magnitude. [Baptista et al., 2001] L. Baptista, I. Lynce, and J. Marques-Silva.
When we further exploit nogood recording, the gain is about
10%                                                     Complete search restart strategies for satisﬁability. In Proceed-
    .                                                   ings of SSA’01 workshop held with IJCAI’01, 2001.
  Finally, we noticed that the number and the size of the re- [Bayardo and Shrag, 1997] R.J. Bayardo and R.C. Shrag. Using
duced nld-nogoods recorded during search were always very CSP look-back techniques to solve real-world SAT instances. In
limited. As an illustration, let us consider the hardest RL- Proceedings of AAAI’97, pages 203–208, 1997.
               11 −  1              680
FAP instance scen   f  which involves  variables and  [Bessiere and R´egin, 1996] C. Bessiere and J. R´egin. MAC and
a greatest domain size of 43 values. MAC+RST+NG solved  combined heuristics: two reasons to forsake FC (and CBJ?) on
this instance in 36 runs while only 712 nogoods of average hard problems. In Proceedings of CP’96, pages 61–75, 1996.
   8 5                 33
size . and maximum size  were recorded.               [Bessiere et al., 2005] C. Bessiere, J.C. R´egin, R.H.C. Yap, and
                                                        Y. Zhang. An optimal coarse-grained arc consistency algorithm.
7  Conclusion                                           Artiﬁcial Intelligence, 165(2):165–185, 2005.
In this paper, we have studied the interest of recording no- [Boussemart et al., 2004] F. Boussemart, F. Hemery, C. Lecoutre,
goods in conjunction with a restart strategy. The beneﬁt of and L. Sais. Boosting systematic search by weighting constraints.
restarting search is that the heavy-tailed phenomenon ob- In Proceedings of ECAI’04, pages 146–150, 2004.
served on some instances can be avoided. The drawback is [Brelaz, 1979] D. Brelaz. New methods to color the vertices of a
that we can explore several times the same parts of the search graph. Communications of the ACM, 22:251–256, 1979.
tree. We have shown that it is quite easy to eliminate this [Dechter, 1990] R. Dechter. Enhancement schemes for constraint
drawback by recording a set of nogoods at the end of each processing: backjumping, learning and cutset decomposition. Ar-
run. For efﬁciency reasons, nogoods are recorded in a base tiﬁcial Intelligence, 41:273–312, 1990.
(and so do not correspond to new constraints) and propaga- [Dechter, 2003] R. Dechter. Constraint processing. Morgan Kauf-
tion is performed using the 2-literal watching technique in- mann, 2003.

                                                IJCAI-07
                                                   135