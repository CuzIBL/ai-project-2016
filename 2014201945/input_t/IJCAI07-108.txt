                        Learning by Analogy : a Classiﬁcation Rule
                                  for Binary and Nominal Data

                      Sabri Bayoudh    and  Laurent Miclet   and  Arnaud Delhay
                           IRISA/CORDIAL – Universit de Rennes 1, France
                        sabri.bayoudh, laurent.miclet, arnaud.delhay @enssat.fr


                    Abstract                          fourth being the object to classify). One property of the ana-
                                                      logical dissimilarity between four objects is that it is null
    This paper deals with learning to classify by using
                                                      when the objects are in analogical proportion.Letustake
    an approximation of the analogical proportion be-
                                                      a toy example to explain what is an analogical proportion and
    tween four objects. These objects are described by
                                                      how to learn with it. Let tomcat be the object to classify into
    binary and nominal attributes. Firstly, the paper re-
                                                      Feline or Ruminant. Suppose that there are only three in-
    calls what is an analogical proportion between four
                                                      stances in the training set: calf, bull and kitten. The animals
    objects, then it introduces a measure called ”ana-
                                                      are described by four binary attributes (1 stands for TRUE
    logical dissimilarity”, reﬂecting how close four ob-
                                                      and0forFALSE) which are  has claws(HC), is an adult(IA),
    jects are from being in an analogical proportion.
                                                      is a male(IM), feeds by suckling(FS).
    Secondly, it presents an analogical instance-based
    learning method and describes a fast algorithm.          Animals  HC    IA  IM   FS     class
    Thirdly, a technique to assign a set of weights to the     calf    0001Ruminant
    attributes of the objects is given: a weight is cho-       bull    0110Ruminant
    sen according to the type of the analogical propor-       kitten   1001                Feline
    tion involved. The weights are obtained from the         tomcat    1110                   ?
    learning sample. Then, some results of the method For each attribute, we notice that there is an analogical pro-
    are presented. They compare favorably to standard portion between the three objects, taken in this order, from the
    classiﬁcation techniques on six benchmarks. Fi-   training set and the object tomcat, since the four binary values
    nally, the relevance and complexity of the method have one of the forms below (we will see later what are all the
    are discussed.                                    possible analogical proportions between binary values).
    Keywords : instance-based learning, learning by
    analogy, analogical proportion, analogical dissim-          0isto0as1isto1       (HC)
    ilarity.                                                    0isto1as0isto1       (IA and IM)
                                                                1isto0as1isto0       (FS)
1  Introduction                                       Thus, the four objects are in analogical proportion since all
                                                      their binary attributes are in analogical proportion :
The aim of this article is to present a new non-parametric
classiﬁcation rule for objects described by binary and nom-      calf is to bull as kitten is to tomcat
inal attributes. Some deﬁnitions and original algorithms will We have to emphasize two points here. Firstly, taking these
be presented to explain this method, and results will be given. three objects (triplet) in a different order will not produce any-
The principle of non-parametric (or instance-based) learning more an analogical proportion with tomcat, except when bull
is to keep the whole learning set as a base for the decision in- and kitten are exchanged. Secondly, to make full use of the
stead of inducing an explicit classiﬁcation concept. The best training set, we must consider also the triplets containing two
known and simplest instance-based classiﬁcation rule is the or three identical objects. Consequently, a training set of m
k-nearest neighbors (k-nn) rule, which requires only a met- objects produces m3 different triplets. In this example, it is
ric space. This rule, in its simplest form, computes the dis- easy to verify that only two triplets among the 27 are in ana-
tance between the object to classify and all the instances in logical proportion with tomcat.
the learning set and considers the k nearest instances as elec- For the triplet (calf, bull, kitten), the resolution of an analogi-
tors of their class [DUDA et al., 2001]. The class of the new cal equation on the classes gives the class of tomcat:
object is the class which has the majority given by the k elec-
tors.                                                    Ruminant is to Ruminant as Feline is to class(tomcat)
Learning by analogy, as we present it here, is also an instance- whichsolvesin:class(tomcat) = Feline.
based technique which uses the concept of analogical dis- The second triplet (calf, kitten, bull) in analogy with tomcat
similarity between four objects (three in the training set, the gives the same result. Hence, the classiﬁcation by analogy of

                                                IJCAI-07
                                                   678tomcat on this learning set is Feline.                When  X is the boolean set B = {0,1} there are only six 4-
Notice that the 1-nearest neighbor technique, using the Ham- tuples in analogical proportion among the sixteen possibilities
ming distance, concludes in this example that class(tomcat) (and two special cases (a) and (b) in the other ten, as we shall
= Ruminant. The strength of the analogical classiﬁer stems see in the next paragraph):
from the fact that it can use objects in the training set be-
longing to some class ω1 to conclude that the unknown class   In analogical    Not in analogical
object is ω2. In terms of distance, an analogical classiﬁer can proportion        proportion
give importance to objects that are far from the object to be 0:0::0:0        0:0::0:      1
classiﬁed. It actually makes a different use of the informa-  0:0::1:1        0:0::1:      0
tion in the training set than distance classiﬁers or decision 0:1::0:1        0:1::0:      0
trees, because it uses a (restricted) form of analogical reason- 1:0::1:0     0:1::1:      0  (a)
ing [GENTNER et al., 2001]. Is the use of analogical pro-     1:1::0:0        0:1::1:      1
portions actually relevant for classiﬁcation? The goal of this 1:1::1:1       1:0::0:      0
article is to investigate in this direction, for data sets described          1:0::0:      1  (b)
by binary and nominal data.                                                   1:0::1:      1
  In Section 2, we give a formal deﬁnition of the notion of                   1:1::0:      1
analogical proportion and we introduce that of analogical dis-                1:1::1:      0
similarity. In Section 3, we present a naive algorithm of learn- Now, if we take objects described by m binary attributes, we
ing a classiﬁcation rule followed by a faster version, which can easily construct an analogical proportion on Bm.
makes use of the properties of analogical dissimilarity. In
Section 4, we propose to learn from the set of instances three Deﬁnition 1 Four objects in Bm are in analogical proportion
different weights for each attribute, and to use one of them if and only if all their attributes are in analogical proportion:
to modify the computation of the analogical dissimilarity (a
weight is chosen among the three according to the type of   a : b :: c : d ⇔ aj : bj :: cj : dj ∀ 1 ≤ j ≤ m
analogical proportion which is involved). We give results on
eight data bases from the UCI ML repository [NEWMAN et With this deﬁnition, it is straightforward to verify that the
al., 1998] and compare with various standard classiﬁcation above properties of analogical proportion are still veriﬁed.
algorithms in Sect. 5. Finally, Section 6 discusses these re-
sults and the complexity of the algorithm.            2.2  Analogical Dissimilarity between Binary
                                                           Objects
2  Analogical Proportion and Analogical
                                                      Up to now, four objects are either in analogical proportion
   Dissimilarity                                      or not. Here we introduce a new notion to measure, when
2.1  Analogical Proportion between Four Binary        they are not in analogical proportion, how far are four ob-
     Objects                                          jects from being in analogical proportion. This measure is
                                                      called Analogical Dissimilarity (AD). Another measure has
Generally speaking, an analogical proportion on a set X is
                                                 4    already been presented in the literature, for natural langage
a relation between four elements of X, i.e. a subset of X ,
                                                      processing, in the framework of semantic analogy [TURNEY,
which writes a : b :: c : d and reads:
                                                      2005] by considering similarity relations between two pairs
               a  is to b as c is to d                of words.
The elements a, b, c and d are said to be in analogical pro- In B, we give the value of 1 to every four-tuple which is not in
portion [DELHAY and MICLET, 2005]. As deﬁned by Lep-  analogical proportion, except for (a) and (b), which take the
age [LEPAGE and ANDO, 1996][LEPAGE, 1996], an analog- value of 2. 1 and 2 are the number of binary values to switch
ical proportion a : b :: c : d implies two other analogical to produce an analogical proportion.
proportions:                                          This deﬁnition leads to the following properties:
      Symmetry of the ”as” relation: c : d :: a : b   Property 1
            Exchange of the means: a : c :: b : d
                                                        1. ∀u, v, w, x ∈ B,AD(u, v, w, x)=0⇔ u : v :: w : x
And the following property is also required :
                                                          ∀u, v, w, x ∈ B,
                                       =                2.
        Determinism: if a : a :: b : x then x b           AD(u, v, w, x)=AD(w, x, u, v)=AD(u, w, v, x)
From the ﬁrst two properties one can deduce ﬁve more equiv-
alent analogical proportions, which gives eight equivalent 3. ∀u, v, w, x, z, t ∈ B,
ways to write that the objects a, b, c and d are in analogical AD(u, v, z, t) ≤ AD(u, v, w, x)+AD(w, x, z, t)
proportion :                                            4. In general, ∀u, v, w, x ∈ B :
                            c : a :: d : b                AD(u, v, w, x) = AD(v, u, w, x)
         c : d :: a : bd:       b :: c : a
                                                         Bm                                  AD(a, b, c, d)
         a : c :: b : dd:       c :: b : a            In   , we deﬁne the analogical dissimilarity
                                                           m   AD(a  ,b ,c ,d )
         a : b :: c : db:       a :: d : c            as   j=1      j  j  j  j , and the four properties above
                            b : d :: a : c            still hold true [MICLET and DELHAY, 2004].

                                                IJCAI-07
                                                   679        h(a) : h(b) :: h(c) : h(x) resolution                       h(a) : h(b) :: h(c) : h(x)
        ω0   : ω0  :: ω0  :?      h(x)=ω0                           ω0  :  ω1  :: ω1  :?
        ω1   : ω0  :: ω1  :?                                        ω1  :  ω0  :: ω0  :?
        ω1   : ω1  :: ω1  :?      h(x)=ω1                                     k
        ω      ω      ω                               Point 2 is comparable to the nearest neighbors method.
          0  :  1  ::  0  :?                          Since AD  takes integer values, there are in general several
                                                      triplets for which AD =0, AD =1, etc. That is why point
       Table 1: Possible conﬁgurations of a triplets  3 increases the value of k to take into account all the triplets
                                                      with the same value for AD. Finally, point 4 uses the same
                                                      voting technique as the k-nn rule.
2.3  Extension to Nominal Attributes                  Multiclass problem: we proceed in the same way as in the
To cope with nominal data, two approaches are possible: two classes problem, keeping only the triplets which solve on
                                                      the class label, for example:
  • The ﬁrst one (one-per-value encoding) consists in split-
    ting the nominal attribute. As a result, an n valued nom-          Analogical equations
    inal attribute is replaced by n binary attributes with ex-           with a solution
    actly one at 1.                                              ω1 : ω3 :: ω1 :?⇒   h(x)=ω3
                                                                 ω    ω    ω      ⇒  h(x)=ω
  • The second approach consists in keeping the nominal           4 :  4 ::  2 :?            2
    attribute as a single attribute. This requires to deﬁne an           with no solution
    analogical proportion on the values of the attribute.        ω1 : ω3 :: ω0 :?  ⇒  h(x)=?
                                                                 ω0 : ω1 :: ω3 :?  ⇒  h(x)=?
The second approach can be used when there is some knowl-
edge about the nominal values, like an order relation or a mea- Missing values problem: we consider a missing value as a
sure of distance between the values. Since it is not the case in value for which the distance to each valued nominal attribute
the data sets that we have worked on, we have chosen to use is the same. Hence, when splitting a nominal attribute, a
the one-per-value encoding to treat the nominal attributes. missing value would take the value 0 in all the split binary
                                                      attributes instead of taking the value 1 in exactly one of them
3  A Classiﬁcation Rule by Analogical                 (Sect. 2.3).
   Dissimilarity                                      Example
                                                          S =  {(a, ω ), (b, ω ), (c, ω ), (d, ω ), (e, ω )}
3.1  Principle                                        Let           0     0      1      1      2  be a set
                                                  of ﬁve labelled objects and let x ∈Sbe some object to be
Let S =   oi,h(oi) | 1 ≤ i ≤ m be a learning set, where classiﬁed. According to the analogical proportion axioms,
                                                                                   3           2
h(oi) is the class of the object oi. The objects are deﬁned by there is only 75 (=(Card(S) + Card(S) )/2) non-
binary attributes. Let x be an object not in S. The learning equivalent analogical equations among 125(= Card(S)3)
problem is to ﬁnd the class of x, using the learning set S. equations that can be formed between three objects from S
To do this, we deﬁne a learning rule based on the concept and x. Table (2) shows only the ﬁrst 14 lines after sorting
of analogical dissimilarity depending on an integer k,which with regard to some arbitrarily analogical dissimilarity.
could be called the k least dissimilar triplets rule. It consists The following table gives the classiﬁcation of an object x
of the following steps:                               according to k :
 1. Compute the analogical dissimilarity between x and all             k      1234567
    the n triplets in S which produce a solution for the class         k     1335577
    of x.                                                          votes for x 111??22
 2. Sort these n triplets by the increasing value of their AD
    when associated with x.                           3.2  A Fast Algorithm : FADANA*
                                                     As shown in Sect. 3.1, this learning by analogical proportion
 3. If the k-th object has the integer value p,thenletk
                                                     technique needs to ﬁnd the k least dissimilar analogical pro-
    be the greatest integer such that the k -th object has the
                                                      portion triplets in S. The naive way is to compute the analogi-
    value p.
                                                      cal dissimilarity of every possible triplet from the learning set
              
 4. Solve the k analogical equations on the label of the with the unknown class object as the fourth object of the ana-
                                           
    class. Take the winner of the votes among the k results. logical proportion. This method has a complexity of O(m3)
To explain, we ﬁrstly consider the case where there are only based on the number of all the non equivalent triplets that can
                                                      be constructed from the learning set S. To make it faster, we
two classes ω0 and ω1. An example with 3 classes will follow.
Point 1 means that we retain only the triplets which have one use the algorithm FADANA* (FAst search of the least Dis-
         1                                            similar ANAlogy) inspired from LAESA [MORENO-SECO
of the four following conﬁguration for their class (see Table             1
1). We ignore the triplets that do not lead to an equation with et al., 2000, ] for the -nn classiﬁcation rule. This algo-
a trivial solution on classes :                       rithm speeds up the computation on line but has to make off
                                                      line some preprocessing of the learning set. The number of
  1There are actually two more, each one equivalent to one of the off line computations depends on the number of ”base proto-
                                                                                2
four (by exchange of the mean objects).               types” (complexity of O(bp∗m )).Themorebaseprototypes

                                                IJCAI-07
                                                   680     o1 o2 o3  h(o1) h(o2) h(o3) h(x)  AD   k         element in an analogical proportion. The analogical propor-
     bad        ω0    ω0   ω1     ω1    0    1        tion weighting matrix is a d × C × C matrix, where d is the
     bde        ω0    ω1   ω2     ⊥     1             number of attributes and C is the number of classes.
     cde        ω     ω    ω      ω
                 1     1     2     2    1    2          For a given attribute ak of rank k, the element Wkij of the
     abd        ω     ω    ω      ω
                 0     0     1     1    1    3        matrix indicates which weight must be given to ak when en-
     cae        ω1    ω0   ω2     ⊥     2             countered in an analogical proportion on classes whose ﬁrst
     dce        ω     ω    ω      ω
                 1     1     2     2    2    4        element is ωi, and for which ωj is computed as the solution.
     dbc        ω1    ω0   ω1     ω0    2    5
                                                                          a
     ace        ω0    ω1   ω2     ⊥     2             Hence, for the attribute k:
     acc        ω0    ω1   ω1     ⊥     3                                    Last element (decision)
     abe        ω     ω    ω      ω
                 0     0     2     2    3    6                                   class ωi class ωj
     bae        ω     ω    ω      ω
                 0     0     2     2    3    7                First ele- class ωi  Wkii   Wkij
     bcd        ω     ω    ω      ⊥
                 0     1     1          3                     ment       class ωj Wkji    Wkjj
     ccc        ω1    ω1   ω1     ω1    4    8
     aac        ω0    ω0   ω1     ω1    4    9        Since we only take into account the triplets that give a solu-
                                                      tion on the class decision, all the possible situations are of one
     .  .  .     .    .     .      .    .    .
     .  .  .     .    .     .      .    .    .        of the three patterns:

Table 2: An example of classiﬁcation by analogical dissimi-   Possible patterns    First  Decision
larity. Analogical proportions whose analogical resolution on                    element    class
                                                              ω   ω     ω    ω      ω        ω
classes have no solution (represented by ⊥) are not taken into i :  i :: j :  j      i        j
                                                              ωi  ωj    ωi   ωj     ωi       ωj
account. AD is short for AD(o1,o2,o3,x).                         :    ::   :
                                                              ωi : ωi :: ωi : ωi    ωi       ωi

                                                      This remark gives us a way to compute the values Wkij from
we have the more computations we do off line and the less we the learning set.
do on line. To show its efﬁciency, we give on the SPECT data
base (Sect. 5.1) the performance of FADANA* (the naive 4.2 Learning the Weights from the Training
method would need 512 000 AD computations).                Sample
    bp     1      2    5    10   20  50 100 200       The goal is now to ﬁll the three dimensional analogical
   nAD
         83365 41753 6459 1084 404 299 283 275        weighting matrix using the learning set. We estimate Wkij
bp is the number of base prototypes, and nAD is the number by the frequency that the attribute k is in an analogical pro-
of AD computations in the online part.                portion with the ﬁrst element class ωi, and solves in class ωj.
                                                        Firstly, we tabulate the splitting of every attribute ak on the
4  Weighting the Attributes for an Analogical         classes ωi:
   Dissimilarity Decision Rule                                               . . . class ωi ...
                                                                     ak =0   ...  n0i   ...
The basic idea in weighting is that all the attributes do not
                                                                     ak =1   ...  n1i   ...
have the same importance in the classiﬁcation. The idea of
selecting interesting attributes for analogy is not new. In where ak is the attribute k and n0i (resp. n1i) is the number
[TURNEY, 2005] a discrimination is also done by keeping of objects in the class i that have the value 0 (resp. 1)forthe
the most frequent patterns in words. Therefore, one should binary attribute k. Hence,
give greater importance to the attributes that are actually dis-
                                                                         1   C
criminant. However, in an analogical classiﬁcation, there are              
                                                                               n  =  m
several ways to ﬁnd the class of the unknown element. Let                       ki
us take again the preceding two class problem example (1).              k=0 i=1
We notice that there is two ways to decide between the class m is the number of objects in the training set.
ω0 and the class ω1 (there is also a third possible conﬁgu-
                                                      Secondly, we compute Wkij by estimating the probability to
ration which is equivalent to the second by exchange of the ﬁnd a correct analogical proportion on attribute k with ﬁrst
means (Sect. 2.1)). We therefore have to take into account
                                                      element class ωi which solves in class ωj.
the equation used to ﬁnd the class. This is why we deﬁne In the following table we show all the possible ways of having
a set of weights for each attribute, according to the number
                                                      an analogical proportion on the binary attribute k. 0i (resp.
of classes. These sets are stored in an analogical weighting
                                                      1i)isthe0 (resp. 1) value of the attribute k that has class ωi.
matrix (Sect. 4.1).
                                                                      st
                                                                     1    0i : 0i :: 0j : 0j
                                                                      sd
4.1  Weighting Matrix                                                2    0i : 1i :: 0j : 1j
                                                                      rd
Deﬁnition 2 An analogical weighting matrix (W )isathree              3    0i : 0i :: 1j : 1j
                                                                      th
dimensional array. The ﬁrst dimension is for the attributes,         4    1i : 1i :: 1j : 1j
                                                                      th
the second one is for the class of the ﬁrst element in an ana-       5    1i : 0i :: 1j : 0j
                                                                      th
logical proportion and the third one is for the class of the last    6    1i : 1i :: 0j : 0j

                                                IJCAI-07
                                                   681    st
Pk(1  ) estimates the probability that the ﬁrst analogical pro- • IBk (k=10): k Nearest-Neighbor classiﬁer with distance
portion in the table above occurs.                        weighting (weight by 1/distance).
                st                    4                 •                        k
           Pk(1  )=n0in0in0jn0j/m                         IB1 (k=5)[Guide, 2004]:  Nearest-Neighbors classi-
                                                          ﬁer with attributes weighting, similarity computed as
                    .
                    .                                     weighted overlap, relevance weights computed with gain
                st            th                          ratio.
From Wkij = Pk(1  )+...+  Pk(6  ), we compute
                                         
          2    2   2    2                        4    5.2  Results
Wkij =  (n0i +n1i)(n0j +n1j)+2∗n0in0jn1in1j /(6∗m )
                                                      We have worked with the WEKA package   [WITTEN  and
The decision algorithm (Sect. 3.1) is only modiﬁed at point FRANK, 2005] and with TiMBL [Guide, 2004], choosing 7
1, which turns into Weighted Analogical Proportion Classiﬁer various classiﬁcation rules onthesamedatafromWEKA
(WAPC):                                               and the last classiﬁer from TiMBL. Some are well ﬁt to bi-
  • Given x, ﬁnd all the n triplets in S which can produce nary data, like ID3, PART, Decision Table. Others, like IB1
    a solution for the class of x. For every triplet among or Multilayer Perceptron, are more adapted to numerical and
                                                      noisy data. The results are given in Table 3. We have arbitrar-
    these n,say(a, b, c), consider the class ωi of the ﬁrst
                                                      ily taken k = 100 for our two rules. We draw the following
    element a and the class ωj of the solution. Compute the
                                x                     conclusions from this preliminary comparative study: ﬁrstly,
    analogical dissimilarity between and this triplet with                               WAPC
    the weighted AD:                                  according to the good classiﬁcation rate of in Br. and
                                                      Mu.  databases, we can say that WAPC handles the miss-
                      d                              ing values well. Secondly, WAPC seems to belong to the
        AD(a, b, c, x)=   Wkij AD(ak,bk,ck,xk)        best classiﬁers for the B.S and H.R databases, which conﬁrms
                      k=1                             that WAPC  deals well with multiclass problems. Thirdly, as
                                                      shown by the good classiﬁcation rate of WAPC in the MO.3
Otherwise, if point 1 is not modiﬁed, the method is called    WAPC
                            AP C                      problem,        handles well noisy data. Finally, the re-
Analogical Proportion Classiﬁer ( ).                  sults on MO. and B.S database are exactly the same with the
                                                      weighted decision rule WAPC than with AP C. This is due
5  Experiments and Results                            to the fact that all AD that are computed up to k = 100 are of
5.1  Experiment Protocol                              null value. But on the other data bases, the weighting is quite
                                                      effective.
We have applied the weighted analogical proportion classi-
ﬁer (WAPC) to eight classical data bases, with binary and
nominal attributes, of the UCI Repository [NEWMAN et al., 6 Discussion and FutureWork
1998].                                                Besides its original point of view, the weighted analogical
  MONK   1,2 and 3 Problems (MO.1, MO.2 and MO.3),    proportion classiﬁer seems, after these preliminary experi-
MONK3 problem has noise added. SPECT heart data (SP.). ments, to belong to the best classiﬁers on binary and nomi-
Balance-Scale (B.S)andHayes Roth (H.R) database, both nal data, at least for small training sets. The reason is that it
multiclass database. Breast-W (Br.)andMushroom (Mu.), can proﬁt from regularities in alternated values in the classes
both data sets contain missing values.                in contrast to decision trees or metric methods which directly
In order to measure the efﬁciency of WAPC,wehaveap-   correlate the attributes with the classes. This means that an
plied some standard classiﬁers to the same databases, and analogical proportion between four objects in the same class
we have also applied AP C to point out the contribution of is reinforced when taking into account a new attribute with
the weighting matrix (Sect.4.1). We give here the parameters values (0, 1, 0, 1) on the four objects but is decreased if the
used for the comparison method in Table 3:            values are (0, 1, 1, 0).
  • Decision Table: the number of non improving decision We believe that there is still ample room for progress in
    tables to consider before abandoning the search is 5. this technique. In particular, we intend to investigate a more
  •                                                   precise mode of weighting the attributes according to the type
    Id3: unpruned decision tree, no missing values allowed. of analogical proportion in which they are involved, and also
  • Part: partial C4.5 decision tree in each iteration and to test WAPC with numerical attributes. Obviously, more
    turns the ”best” leaf into a rule, One-per-value encod- work has to be done on the computational aspects. Even with
    ing.                                              the FADANA* method (Sect. 3.2), the decision process still
  • Multi layer Perceptron: back propagation training, takes too much time for realistic computation on large data
    One-per-value encoding, one hidden layer with (number sets.
    of classes+number of attributes)/2 nodes.           Another interesting question is the limits of this type of
  •                                                   classiﬁcation. We already know how to deﬁne an analogi-
    LMT  (’logistic model trees’): classiﬁcation trees with cal proportion and an analogical dissimilarity on numerical
    logistic regression functions at the leaves, One-per-value objects, and we have investigated on sequences of symbolic
    encoding.                                         or numerical objects [DELHAY and MICLET, 2005]. Still, we
  • IB1: Nearest-Neighbor classiﬁer with normalized Eu- do not know in which cases an analogical proportion between
    clidean distance.                                 classes is related to that of objects that constitute the classes

                                                IJCAI-07
                                                   682