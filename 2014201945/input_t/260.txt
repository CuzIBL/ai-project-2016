                  A Planning Algorithm for Predictive State Representations 

                                     Masoumeh T. Izadi and Doina Precup 
                                             School of Computer Science 
                                                  McGill University 
                                                   Montreal, Canada 


                        Abstract                               tions a1...ak. The prediction for a test q given prior history 
                                                               //-, denoted is the probability of seeing the sequence 
     We address the problem of optimally controlling 
                                                               of observations of q after seeing history h and taking the se•
     stochastic environments that are partially observ•
                                                               quence of actions specified by q. For any set of tests Q, its 
     able. The standard method for tackling such prob•
                                                               prediction vector is: 
     lems is to define and solve a Partially Observable 
     Markov Decision Process (POMDP). However, it 
     is well known that exactly solving POMDPs is 
                                                               A set of tests Q is a PSR if its prediction vector forms a suffi•
     very costly computationally. Recently, Littman, 
                                                               cient statistic for the dynamical system, i.e., if all tests can be 
     Sutton and Singh (2002) have proposed an alter•
                                                               predicted based on p(Q|h). Of particular interest is the case 
     native representation of partially observable en•
                                                               of linear PSRs, in which there exists a projection vector mq 
     vironments, called predictive state representations       for any test q such that 
     (PSRs). PSRs are grounded in the sequence of ac•
     tions and observations of the agent, and hence re•
     late the state representation directly to the agent's       Littman et al. also define an outcome function u map•
     experience. In this paper, we present a policy iter•      ping tests into n-dimensional vectors defined recursively by: 
     ation algorithm for finding policies using PSRs. In                  and u(aoq) = where e repre•
     preliminary experiments, our algorithm produced 
                                                               sents a null test and cn is the (1 x n) vector of all Is. Each 
     good solutions.                                           component u, (q) indicates the probability of the test q when 

                                                               its sequence of actions is applied from state st. A set of 
1 Predictive State Representation                              tests Q = = 1,2,..A;} is called linearly independent 
                                                               if the outcome vectors of its tests arc 
We assume that we are given a system consisting of a dis•
                                                               linearly independent. Using this definition, such a set Q can 
crete, finite set of n states 5, a discrete finite set of actions 
                                                               be found by a simple search algorithm in polynomial time, 
A, and a discrete finite set of observations O. The interaction 
                                                               given the POMDP model of the environment. Littman, Sut•
with the system takes place at discrete time intervals. The 
                                                               ton and Singh (2002) showed that the outcome vectors of the 
initial state of the system so is drawn from an initial probabil•
                                                               tests in Q can be linearly combined to produce the outcome 
ity distribution over states I. On every time step t, an action 
                                                               vector for any test. 
at is chosen according to some policy. Then the underlying 
state changes to and a next observation 0i+1 is gener•         2 Policy evaluation using PSRs 
ated. The system is Markovian, in the sense that for every 
action, the transition to the next state is generated according We assume that we are given a policy and that 
to a probability distribution described by an (n x n) transition the initial start state of the system, is drawn according to 
matrix Similarly, for a given observation o and action a,      the staring probability distribution I. If we consider a given 
the next observation is generated according to an (n x n) diag• horizon t, only a finite number of tests of length t are possible 
onal observation matrix where is the probability of            when starting from I. Let be this set of possible tests. 
observation o when action a is selected and state i is reached.  The value of a memoryless policy with respect to a given 
Since we are interested in optimal control, rather than predic• start state distribution I is the expected return over all possi•
tion, we also assume that there exists a set of reward vectors ble tests that can occur when the starting state is drawn form 
    for each action a, where is the reward for taking action   I and then behavior is generated according to policy  
a in underlying state i 
   PSRs are based on the notion of tests. A test is an ordered 
sequence of action-observation pairs q = The 
prediction for test q is the probability of the sequence of ob• where is the expected return for test q given that the 
servations being generated, given the sequence of ac•          initial state is drawn from / and policy is followed. 


1520                                                                                                   POSTER PAPERS    Let U be the matrix formed by concatenating 
the outcome vectors for all tests in and be its pseu-
doinverse. The columns of U define the probability of each 
test in when applied from each underlying state. Conse•
quently, IU represents the probability of the tests in when 
starting in /. 
   For each action-observation combination ao, we can define 
a projection matrix: 


 and a projection vector: 

                                                               Figure 1: Policy quality vs. number of iterations for the 4x4 
   Considering the probability distribution over tests that    grid world 
 generates and the expected return of a test q given / and 
 we have: 
                                                               Q found consists of 16 linearly independent tests: 4, 5, 6, 15, 
                                                               36, 46, 436, 446, 4436, 3336, 1336, 44336, 43336, 31336. 
                                                               We tried this problem for finite horizon case with a discount 
                                                               factor r = 0.8. Figure 1 indicates the performance of the 
                                                               algorithm on this problem. 
 This evaluation method requires a large amount of precom-
 putation, but it can be useful if a small horizon suffices to get 5 Conclusion and future work 
 a good policy.                                                The key difficulty for our planning algorithm is that the num•
                                                               ber of possible tests grows exponentially with the horizon 
 3 Policy iteration for PSRs                                   length. Hence, the algorithm cannot be used for large prob•
 Similar to POMDPs in PSRs we can define action-value func•    lems or for the infinite horizon case. However, this is not 
 tions on the level of tests. The action-value function for tak• worse than other existing exact solution methods for solving 
 ing action a after test q can be computed as:                 POMDPs. Our hope is that good approximations of the opti•
                                                               mal solution can be found efficiently. 

                                                               References 
   Then, the policy of the agent can be improved by choosing   [Cassandra et al , 1994] A. R. Cassandra, L. P. Kaelbling, 
 an action greedily with respect to this action-value function:  and M. L. Littman. Acting Optimally in Partially Observ•
                                                                 able Stochastic Domains. In Proceedings of the Twelfth 
                                                                 National Conference on Artificial Intelligence, 1994. 
   The agent must, in other words, select the best pol•        [Littman, 1996] . L. Littman. Algorithms for Sequential De•
 icy tree rooted at each decision point and each time            cision Making. PhD thesis, Brown University, Providence, 
 step. Therefore the total running time of the algorithm is       Rl, March 1996. 
                  and the complexity of the algorithm is only 
 single-exponential in the horizon time.                       [Littman et al , 2002] M. L. Littman, R. S Sutton, and S. 
                                                                  Singh. Predictive representations of state. Advances in 
 4 Experimental results                                          Neural Information Processing Systems 14. (Proceedings 
                                                                  of the 2001 conference). MIT Press. 
 We experimented with a standard gridworld navigation task 
                                                               [Parr and Russell, 1995] R. Parr, S. Russell. Approximat•
 used in the POMDP literature (Cassandra, 1994; Parr& Rus•
                                                                  ing Optimal Policies for Partially Observable Stochastic 
 sell, 1996). The environment is a (4 x 4) grid. The agent 
                                                                  Domains. In Proceedings of the Fourteenth International 
 has four actions, N, S, E, and W, which change its location 
                                                                 Joint Conference on Artificial Intelligence (IJCA1-95). 
 deterministically to one of the four neighboring states. There 
 is one goal state, in the lower right corner, which generates a 
 distinct observation and a reward of +1. All the other states 
 are perceptually aliased and generate no reward. The initial 
 probability distribution is uniform over all states except the 
 goal. Taking any action in the goal state moves the agent uni•
 formly randomly to one of the other states. 
   For this problem we have 6 possible combinations of 
 action-observation (ao) pairs: 1 -Nnothing,2-Wnothing,3-
 Enothing,4-Snothing,5-Egoal,6-Sgoal. The set of core tests 


 POSTER PAPERS                                                                                                      1521 