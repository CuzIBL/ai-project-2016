                    Efﬁcient Stochastic Local Search for MPE Solving

            Frank Hutter                    Holger H. Hoos                    Thomas Stutzle¨
      Computer Science Dept.            Computer Science Dept.            Computer Science Dept.
     Univ. of British Columbia         Univ. of British Columbia      Darmstadt Univ. of Technology
       Vancouver, BC, Canada            Vancouver, BC, Canada               Darmstadt, Germany
          hutter@cs.ubc.ca                  hoos@cs.ubc.ca          stuetzle@informatik.tu-darmstadt.de

                    Abstract                          in approximate MPE algorithms, reaching from loopy belief
                                                      propagation (BP) [Pearl, 1988] and generalized BP [Yedidia
    Finding most probable explanations (MPEs) in      et al., 2002] to stochastic local search (SLS) algorithms [Kask
    graphical models, such as Bayesian belief net-    and Dechter, 1999; Park, 2002] and specialized algorithms,
    works, is a fundamental problem in reasoning un-  such as graph cuts for certain pairwise Markov Random
    der uncertainty, and much effort has been spent on Fields (MRFs) that, for example, occur in computer vi-
    developing effective algorithms for this N P-hard sion [Boykov et al., 2001].
    problem. Stochastic local search (SLS) approaches   B&B approaches have recently been shown to be state-
    to MPE solving have previously been explored, but of-the-art methods in MPE solving, and it has been claimed
    were found to be not competitive with state-of-the- that for MPE, B&B algorithms clearly outperform GLS, the
    art branch & bound methods. In this work, we iden- best performing SLS algorithm known so far, in terms of the
    tify the shortcomings of earlier SLS algorithms for ability to ﬁnd high-quality solutions quickly [Marinescu et
    the MPE problem and demonstrate how these can     al., 2003]. This is in stark contrast to results for numerous
    be overcome, leading to an SLS algorithm that sub- other combinatorial optimisation problems, such as weighted
    stantially improves the state-of-the-art in solving MAX-SAT, where SLS algorithms clearly deﬁne the state-of-
    hard networks with many variables, large domain   the-art (see, e.g., [Hoos and Stutzle,¨ 2004]).
    sizes, high degree, and, most importantly, networks In this work, we analyse the shortcomings of previous
    with high induced width.                          SLS algorithms for MPE and demonstrate that these weak-
                                                      nesses can be overcome based on the careful consideration
1  Introduction                                       of important issues, such as the time complexity of individ-
                                                      ual search steps, search stagnation and thorough parameter
Since Pearl’s classic text [Pearl, 1988], graphical models such tuning. In particular, we introduce improvements to Park’s
as Bayesian networks have become the prime representation GLS algorithm [Park, 2002] that overcome its inferior scal-
for uncertainty in AI. This paper deals with the problem of ing behaviour with network and domain size. Our new al-
ﬁnding the Most Probable Explanation (MPE) for some ev- gorithm, GLS+, clearly outperforms current state-of-the-art
idence when reasoning under uncertainty. More speciﬁcally, MPE algorithms on various types of networks, especially for
in the light of uncertain knowledge represented as a proba- hard networks with high induced width, and hence establishes
bilistic graphical model, this problem is cast as ﬁnding the stochastic local search as a highly attractive and competitive
most probable instantiation of all the model’s variables V approach to MPE solving.
                      e          E    V
given the observed values for a subset ⊆ .              The remainder of this paper is structured as follows. We
  The MPE problem in graphical models has applications ﬁrst introduce some basic concepts and notation in Section 2;
in different ﬁelds, such as medical diagnosis [Jaakkola and next, in Section 3, we describe GLS and GLS+ and present
Jordan, 1999], fault diagnosis [Rish et al., 2002], computer some computational results illustrating the improvement of
vision [Tappen and Freeman, 2003], and prediction of side- GLS+ over GLS. In Section 4 we present empirical results
chains in protein folding [Yanover and Weiss, 2003], to name that establish GLS+ as a new state-of-the-art algorithm for
just a few. Consequently, many algorithms have been sug- ﬁnding MPEs. We close with some conclusions and a brief
gested to solve this problem, but since it is N P-hard, many outlook on future work in Section 5.
hard problem instances still cannot be solved efﬁciently. The
available algorithms for MPE solving include exact meth-
ods like variable elimination (VE) [Dechter, 1999], junc- 2 Preliminaries
tion tree (JT) [Cowell et al., 1999] and conditioning tech- A discrete Bayesian belief network (or Bayes net) B is a
niques [Pearl, 1988], but also systematic search algorithms, quadruple hV, D, G, Φi, where V is an ordered set of ran-
                                                                   D
such as branch & bound (B&B), guided by a Mini-Buckets dom variables, is an ordered set of ﬁnite domains DVi for
(MB) heuristic [Dechter and Rish, 2003]. Since in practice each Vi ∈ V, G = (V, E) is a directed acyclic graph (DAG),
many applications require efﬁcient online algorithms for net- and Φ is an ordered set of CPTs φV = P (V |pa(V )), speci-
works of high induced width, there is also much research fying the conditional probability distribution of each V ∈ Vgiven its parents pa(V ) in G. Semantically, a Bayes net spec- Algorithm 1: GLS/GLS+ for MPE
                                              V                   +
iﬁes a joint probability distribution φ over its variables in GLS and GLS differ in the procedure used to generate an ini-
                   V
factored form: φ = P ( ) = QV ∈V φV .                  tial solution, the subsidiary local search procedure, and, most
  Given a Bayes net B = hV, D, G, Φi and a set of evidence importantly, in the evaluation function g(v|Vi = vi). These dif-
variables E = e, the Most Probable Explanation (MPE)   ferences are explained in the text.
problem is to ﬁnd an instantiation V = v with maximal prob- The utilities util(φ, v) are deﬁned in the text.
        v            V    v                                               V  D             E    e
ability p( ) := Qφ∈Φ φ[ =  ] over all variable instantia- Input: Bayes net B = h , , G, Φi, evidence = , time
tions v consistent with evidence e. While all networks in     bound t, smoothing factor ρ, smoothing interval Nρ.
our experimental analysis are Bayes nets, our algorithms are Output: Variable assignment V = v with highest probability
                                                                    φ V   v  found in time t
equally applicable to other graphical models, such as MRFs     Qφ∈Φ   [ =   ]
or general factor graphs.                               //=== Initialize variable assignment v, penalties λφ, and local
  CPTs are a special case of potentials, which are functions optima counter LO.
that have non-negative entries for any assignment of their 1 v ← GenerateInitialSolution(B, e)
variables. One well-known method for solving MPE in gen- 2 foreach φ ∈ Φ and all instantiations Vφ = vφ do
eral networks is variable elimination (VE) (see, e.g., [Dechter, 3 λφ[Vφ = vφ] ← 0
1999]); it iteratively eliminates variables V by multiplying all 4 #(LO) ← 0
potentials that are deﬁned over V and then maximizing V out //=== Alternate local search and updates of the evaluation
of the product thus obtained. Once all variables are elimi-  function until termination.
nated, the best assignment can be recovered in linear time. 5 while runtime < t do
Mini-Buckets with i-bound ib (MB(ib)) [Dechter and Rish, 6 v ←  SubsidiaryLocalSearch(B, v, e)
2003] approximates VE by splitting each product into smaller //=== Local optimum, update evaluation function.
products with at most ib variables; MB-w(s) is a new variant 7 foreach φ ∈ Φ do
of MB that instead limits the number of entries in each prod- 8 if util(φ, v) = maxφ∈Φ util(φ, v) then
                                                ib
uct by s. For constant domain size d, MB(ib) and MB-w(d ) 9       λφ[V = v] ← λφ[V = v] + 1
are equivalent, but for networks with different domain sizes
MB-w performed better in our experiments. All of VE, MB,   //=== Regularly smooth penalties.
and MB-w employ a min-weight heuristic.               10   #(LO) ←  #(LO) + 1
                                                      11   if #(LO) modulo Nρ = 0 then
3  SLS for MPE: From GLS to GLS+                      12       for φ ∈ Φ and all instantiations Vφ = vφ do
                                                      13          λφ[Vφ = vφ] ← λφ[Vφ = vφ] ∗ ρ
Probably the most prominent Stochastic Local Search algo-
rithm for inference in Bayesian networks is a method called
Stochastic Simulation [Pearl, 1988; Kask and Dechter, 1999],
also known as Gibbs sampling. However, for MPE, this
method, as well as Simulated Annealing, was shown to be are dynamically updated whenever the search reaches a local
clearly outperformed by a simple algorithm called Greedy optimum.
+ Stochastic Simulation (G+StS) [Kask and Dechter, 1999], An outline of the GLS algorithm is shown in Figure 1. At
which probabilistically chooses between greedy and sam- a high level, after initialising the search and setting all penal-
pling steps.                                          ties to zero, GLS alternates between two search phases: First,
  The MPE problem is closely related to weighted MAX- a simple iterative improvement search is performed with re-
SAT [Park, 2002] (and MAX-CSP [Marinescu and Dechter, spect to the evaluation function g that takes into account the
2004]); based on this close relationship, Park adapted current penalty values. After this process has reached a lo-
two high-performance MAX-SAT   algorithms, DLM and    cal optimum o of g, certain penalty values are incremented
GLS [Mills and Tsang, 2000], to the MPE problem [Park, by one. Only penalties of solution components present in o
2002].1 His computational experiments identiﬁed GLS as the can be selected to be incremented; this selection is based on
state-of-the-art SLS algorithm for MPE solving: he showed the contribution of the respective solution component to g(o)
that GLS and DLM clearly outperform G+StS and that GLS and its current penalty value. (For details, see Algorithm 1 or
                                                      [         ]
performed better than DLM on most instances. However, as Park, 2002 .) Additionally, all penalties are regularly multi-
shown in [Marinescu et al., 2003], GLS does not reach the plied by a factor ρ ≤ 1; this smoothing mechanism prevents
performance of current B&B algorithms.                the penalty values from growing too large and is performed
  GLS has been applied successfully to many combinato- every Nρ local optima, where Nρ is a parameter.
rial problems, including TSP [Voudouris and Tsang, 1999], In Park’s GLS for MPE, the solution components are par-
SAT and weighted MAX-SAT  [Mills and Tsang, 2000]. It tial instantiations of the variables. More speciﬁcally, for each
can be classiﬁed as a dynamic local search algorithm [Hoos potential φ ∈ Φ, every instantiation Vφ = vφ of its vari-
and Stutzle,¨ 2004] and uses penalties associated with solu- ables is a solution component.2 The evaluation function to be
tion components to guide the search process; these penalties
                                                         2Note that the number of components present in any candidate
  1Although Park motivated his versions of these algorithms with solution for an MPE instance is constant; this is because every vari-
the MAX-SAT domain, his implementation works on MPE directly. able instantiation V = v is consistent with exactly one (partial)
To our best knowledge, his suggested encoding has never been im- instantiation Vφ = vφ of each potential φ, and thus the number of
plemented, and we are not aware of any implementation of GLS for solution features present in each instantiation is the total number of
weighted MAX-SAT with real-valued weights.            potentials |Φ|.                                                         2
                  0                                     10
                                                              Improving
                                                              Scores
                −100

                −200
                                                         1
                                                        10
                −300

                −400
                Log  probability of assignment GLS+
                                              GLS        0
                −500                                    10 1         2         3         4
                           0         1          2        10        10        10        10

                          10        10        10       Speedup  over previous best caching scheme
                              CPU time (sec)                   Instance size (N times average K)
              (a) Speedup due to modiﬁed evaluation function (b) Speedup due to novel caching schemes
Figure 1: (a) Effect of the modiﬁcation of the evaluation function in GLS+ compared to GLS on instance Munin2. Given are
the best, average and worst solution quality over computation time for 25 runs of GLS and GLS+. The GLS+ plot ends after
22.94 CPU seconds when all its runs have found the optimal solution quality. (b) The speedup achieved by our new caching
schemes over the previously best-performing caching scheme on a collection of randomly generated and real-world instances,
where instance size is given as the number of variables (N) times the average domain size (K).

                       v             V    v
minimized is deﬁned as g( ) = Pφ∈Φ λφ[  =  ], where   tune.” [Park, 2002], its performance can be boosted by up
λφ[V =  v] is the penalty associated with solution compo- to several orders of magnitude by simply changing Park’s de-
nent Vφ = vφ. It may be noted that this deviates from the fault smoothing value ρ = 0.8 to the constant value ρ =
standard form of the evaluation function for the general GLS 0.999. For example, for the Hailfinder network GLS
algorithm, which also captures the contribution of each solu- ﬁnds the optimal solution 10 times faster with ρ = 0.999
tion component to the optimisation objective p(v).    than with ρ = 0.8, and for several random instances this ef-
  Due to this non-standard evaluation function, the sole in- fect is much more pronounced. It is interesting to note that the
teraction of objective function and penalties in Park’s GLS very small amount of smoothing performed at ρ = 0.999 can
for MPE is via the utilities of potentials φ under the current be rather important: Without smoothing (ρ = 1), we found
assignment v, which are deﬁned as util(φ, v) = −φ[V = rare but conclusive evidence for search stagnation on random
v]/(1 + λφ[V  =  v]). An entry φ[Vφ = vφ] with high   networks.
probability is assigned low utility, and its associated penalty Secondly, the subsidiary local search procedure in GLS
           will be increased less often, driving the search uses a computationally efﬁcient ﬁrst-improvement strategy,
λφ[Vφ = vφ]                                                      +
towards the partial variable instantiation Vφ = vφ eventually, whereas GLS employs a more powerful best-improvement
but possibly only after a considerable delay.         procedure in conjunction with newly developed, powerful
  Due to this initial and persistent lack of greediness, we ex- strategies for caching and updating the effects of variable ﬂips
pected the performance of GLS for MPE to be boosted sig- on the evaluation function. While previously used caching
niﬁcantly by integrating the objective function into the search schemes only locally update the partial evaluation function
heuristic. We achieved this by a change in GLS’s evalua- value involving variables in the Markov blanket of a ﬂipped
tion function. Our improved version of GLS, which we call variable, we developed two substantial improvements. At ev-
GLS+, adds the logarithmic objective function to the appro- ery step, we maintain the score of ﬂipping each variable to
priate penalties, making the new evaluation function to be any of its values (caching scheme Scores), and on top of that
           v               V    v          V    v     the set of all variables that lead to an improvement in eval-
maximized g( ) = P  Φ log(φ[  =  ])−w  ×λφ[   =  ],
                  φ∈                                  uation function value when ﬂipped to some value (caching
where w is a weighting factor.3 Indeed, this modiﬁcation
                                                      scheme Improving); since after an initial search phase this
of the evaluation function has major consequences on search
                                                      quantity is a low constant in practice, this latter caching
behaviour and especially boosts the search in early phases
                                                      scheme enables an evaluation of the whole neighbourhood of
of the search. Figure 1(a) illustrates this for the real-world
                                                      a search position in constant time. Figure 1(b) demonstrates
network Munin2 (from the Bayesian Network Repository),
                                                      the large performance gains of our new caching schemes over
where GLS+ ﬁnds optimal-quality solutions 10 times faster
                                                      the previous state-of-the-art caching scheme.
on average.
                                                        Thirdly, GLS initializes the search randomly. However,
  GLS+  differs from GLS in a number of other components,
                                                      it has been shown that strong initial solutions, such as the
namely the parameter setting, the caching scheme, and the
                                                      ones obtained with MB, lead to much better overall solu-
initialization. All of these modiﬁcations contribute signiﬁ-
                                                      tions [Kask and Dechter, 1999]. Consequently, GLS+ initial-
cantly to GLS+’s improved performance (a detailed analy-
                                                      izes its search using the MB variant MB-w( 5 (see Section
sis of their individual importance can be found in [Hutter,                               10 )
                                                      2), which improves the quality of found solutions consider-
2004]). Firstly, a thorough experimental analysis showed
                                                      ably and for some instances speeds up the search for optimal
that, although Park states that “GLS had no parameters to
                                                      solutions by up to two orders of magnitude.
                                                                                    +
  3In order to achieve a meaningful guidance by this evaluation We also study a version of GLS that has an additional
function in areas of the search space with probability zero, we treat preprocessing stage based on VE. In this preprocessing, VE
log(φ[V = v]) as −104 for φ[V = v] = 0; we also ﬁxed the is applied until a potential with more than b entries is ob-
weighting factor w to 104 to make the penalties comparably large. tained, where b is a parameter (b = 0 results in the standard                                   Stats GLS+     GLS              BBMB               AOMB
                 Distribution
                                  W  Opt (default) (orig)   static(10) dynamic(4) static(10) dynamic(6)
                                               Random networks
         Base-line: N = 100, K = 2, P = 2 16.5 100 0.11 1.88(1/0) 0.05   1.69      0.33    1.10
            N = 200, K = 2, P = 2 34.8 0  0.70 33.97(68/0.8464) 61.95(88/0.2694) 47.66(86/0.4168) -(100/0) -(100/0)
            N = 400, K = 2, P = 2 72.6 0 10.13  -(100/0.0355) -(100/0.0020) -(100/0.0254) -(100/0) -(100/0)
            N = 100, K = 3, P = 2 16.5 100 0.34 37.15(81/0.7392) 5.51 28.90(7/0.5608) 15.38 33.72(20/0)
            N = 100, K = 4, P = 2 16.3 60 0.88  -(100/0.2225) 21.86(25/0.4856) 29.88(65/0.5071) 42.41(53/0) 59.02(91/0)
            N = 100, K = 2, P = 3 28.3 47 0.42 13.26(3/0.9659) 36.12(48/0.3650) 31.49(74/0.3884) 43.01(62/0) 53.17(62/0)
            N = 100, K = 2, P = 4 37.0 0  0.81 41.89(4/0.8538) 46.83(98/0.1079) 2.07(99/0.1502) -(100/0) -(100/0)
                                                Grid networks
            Base-line: N = 10, K = 2 11.8 100 0.01 4.55(5/0.1989) 0.05 20.23(7/0.6847) 0.13 2.56
                N = 15, K=2      22.4 0   0.90 50.00(99/0.4201) 9.75(95/0.2686) -(100/0.0880) -(100/0) -(100/0)
                N = 20, K=2      32.8 0  30.01  -(100/0.0031) -(100/0.0058) -(100/0.0015) -(100/0) -(100/0)
                N = 25, K=2      44.3 0  52.06   -(100/0)  -(100/0.0002) -(100/0) -(100/0) -(100/0)
                N=10, K = 3      12.0 100 0.34  -(100/0.4955) 0.98   47.16(97/0.3187) 1.90 43.29(53/0)
                N=10, K = 4      11.7 95  1.17  -(100/0.0496) 4.77(3/0.5950) -(100/0.1830) 15.59(4/0) 68.32(95/0)
                N=10, K = 5      11.8 84  5.05  -(100/0.0098) 6.41(9/0.5619) -(100/0.1334) 76.52(33/0) -(100/0)
                N=10, K = 6      11.9 0  17.89  -(100/0.0049) -(100/0) -(100/0.0940) -(100/0) -(100/0)

Table 1: Scaling of performance for random networks and grid networks with network size N, domain size K, and number of
parents P . For each problem distribution, there are 100 networks. W gives their average induced width, Opt the number of
networks for which our quasi-optimal solutions are provably optimal (we never found a better solution than the quasi-optimal
one). For each algorithm and problem distribution, we list the average time to ﬁnd a quasi-optimal solution. If an algorithm did
not ﬁnd the quasi-optimal solution for all instances within 100 CPU seconds, we give its average time for its solved instances,
and in parentheses its number of unsolved instances, followed by its average approximation quality for these instances.

GLS+  algorithm, and b = ∞ yields pure VE). The result- b = 0 in the experiments on random instances.
                                                +
ing reduced network can then quickly be solved with GLS , In the ﬁrst two experiments, for each network, we executed
and the eliminated variables can be instantiated optimally in each algorithm once for a maximum of 100 CPU seconds and
linear time like in regular VE.                       call the best solution found in any such run of an algorithm
                                                      quasi-optimal. For the second and third experiment, we ran
4  Experimental Results                               the B&B algorithms for a long time to obtain provably opti-
We conducted a number of computational experiments to mal solution qualities, which we found to always agree with
compare the performance and scaling behaviour for the orig- our quasi-optimal solutions. For a fair comparison, we always
inal GLS algorithm, GLS+, and the current state-of-the-art in report the times each algorithm requires for ﬁnding a quasi-
MPE solving.4 In [Marinescu et al., 2003], B&B with static optimal solution; only for the third experiment we addition-
MB heuristic (s-BBMB) and a B&B algorithm with MB tree ally report the time required for proving optimality. Finally,
elimination heuristic (BBBT) were claimed to be the state- we deﬁne the approximation quality of an algorithm run as
of-the-art for MPE solving. [Marinescu and Dechter, 2004] the ratio of the probability of the solution it found and the
then introduced B&B with dynamic MB heuristic (d-BBMB), quasi-optimal probability.
as well as versions of s-BBMB and d-BBMB that employ    Our ﬁrst experiment evaluates how the various algorithms
an AND/OR search tree; these are called s-AOMB and d- scale with important instance characteristics, such as network
AOMB.                                                 size (N), domain size of the variables (K), and network den-
  We used Radu Marinescu’s C++ implementations of s-  sity, here controlled by the number P of parents of each node.
BBMB, d-BBMB, s-AOMB, and d-AOMB.5     Furthermore,   We created instances with a random network generator pro-
we used Marinescu’s C++ implementation of GLS instead vided by Radu Marinescu (this is, e.g., described in [Mari-
of the original Java implementation by Park [Park, 2002], nescu et al., 2003]), topologically sampled all variables to
since the former was orders of magnitude faster than the lat- guarantee non-zero joint probability, and randomly picked 10
ter on all instances we tried; our GLS+ implementation is evidence variables. Table 1 shows that for small and easy net-
also written in C++. We employed a ﬁxed parameter setting works, identiﬁed as “Base-line” in the table, the original GLS
                               +                                                                +
of hρ, Nρi = h0.999, 200i for GLS and the overall best- and the B&B algorithms are competitive with GLS , but that
performing ﬁxed i-bound ib ∈ {2, 4, 6, 8, 10, 12} for each when scaling any of N, K, or P , the performance of all al-
B&B algorithm. The preprocessing stage in GLS+ only im- gorithms except GLS+ degrades rapidly. [Marinescu et al.,
proves its performance for structured networks. Thus, we set 2003] showed that s-BBMB scales much better with domain
                                                      size K than the original GLS algorithm. While this is con-
  4All experiments were carried out on compute servers each ﬁrmed by our experiments (see Table 1), our results in Fig-
equipped with dual 2GHz Intel Xeon CPUs with 512KB cache and ure 2(a) show that GLS+ substantially outperforms s-BBMB
4GB RAM  running Linux version 2.4.20, build 28.9. Our imple- for larger K, and that the relative variability in run-time in-
mentation and the benchmark instances we used are available online creases with for s-BBMB, but remains constant for GLS+.
at http://www.cs.ubc.ca/labs/beta/Projects/SLS4MPE/              K
                                                            +
  5We do not report results for BBBT, since the implementa- GLS does not only outperform the other algorithms in
tion available to us could only correctly handle networks generated terms of runtime for ﬁnding quasi-optimal solutions, but also
within the REES system (http://www.ics.uci.edu/˜radum/rees.html). in terms of solution quality found in a given ﬁxed time. This                                           0                           2
                                          10                         10

                                           −2         + 
    0                                               GLS                1
   10                                     10                         10
                                           −4
                                          10
                                                                       0
                                                                     10
                                           −6
                                          10
   GLS+ −1                                                  + 


   10                                                     GLS        CPU  time −1
                                           −8                        10
                                         s−BBMB(10) 10                                      d−BBMB(4)
                                                          N=15
                                           −10                                              s−BBMB(10)
                                 K=2      10              N=20         −2                   GLS
                                 K=3                                 10
                                                          N=25                              GLS+
    −2                           K=4
   10                                          −10 −8 −6 −4 −2 0
       −2   −1   0    1    2    3    4        10 10 10 10 10  10
      10   10   10   10   10  10   10                                   0     10     20     30     40
                 s−BBMB(10)                        Original GLS                Maximal induced width
     (a) CPU time to ﬁnd optimal solution (b) Approximation quality of GLS, (c) Scaling with induced width
                                       GLS+, and s-BBMB(10)
Figure 2: (a) CPU time required by GLS+ and s-BBMB(10) to solve random network instances to optimality. Each point is
the result of a single run on one instance. The instances are the same as summarized in Table 1. The same CPU-time for
both algorithms is indicated by the line. (b) Comparison of the quality reached by the three algorithms GLS, GLS+, and s-
BBMB(10) within 100 CPU seconds. For all instances, GLS+ found the best known solution quality, thus its approximation
quality is always 1. Each point in the ﬁgure is the result of a single run for one instance by GLS and s-BBMB(10), while the
two lines show the approximation quality of GLS+. (c) Scaling with induced width for random networks with 100 variables,
domain size 2, and maximal node degree 5. For each induced width, based on one run on each of 100 instances, we plot the
median runtime for ﬁnding the optimal solution. The errorbars are based on the quantiles q0.25 and q0.75. We do not employ
means and standard deviations since the B&B algorithms did not succeed in ﬁnding an optimal solution for every network. For
example, for the networks of induced width 40, GLS+ took 0.5 seconds in the worst case, whereas s-BBMB(10) failed to ﬁnd
the optimal solution for 18 of the 100 networks within 100 seconds.

can be seen from Table 1 for all of N, K and P , and is that the Link network could not be solved by any of the
visualized in Figure 2(b) for scaling N in random grid net- B&B algorithms, while GLS and GLS+ ﬁnd the optimal so-
works. This scatter plot compares the original GLS, GLS+, lution in one CPU second. Interestingly, MB(2) already ﬁnds
and s-BBMB all given the same maximum computation time; a tight upper bound on solution quality (in 10 CPU millisec-
this is possible since GLS+ always ﬁnds the quasi-optimal onds), but with feasible i-bounds never even comes close to a
solutions, such that its approximation quality is constantly tight lower bound. Consequently, only a combination of SLS
1. Even though s-BBMB(10) scales better than the origi- and MB can ﬁnd the optimal solution and prove its optimality
nal GLS, GLS+ shows even better scaling behaviour. Once (and this within one CPU second).
again, not only the average quality difference to GLS+ grows,
but for both GLS and s-BBMB, the relative variability also 5 Conclusion and Future Work
increases with network size.
                                                      In this work, we identiﬁed various weaknesses of GLS, the
  These scaling results are further extended in our second ex-
                                                      previously best-performing SLS algorithm for the MPE prob-
periment, where we studied the impact of the induced width
                                                      lem, and introduced GLS+, a novel variant of GLS that pays
of networks on the performance of GLS+ vs. GLS and the
                                                      more attention to such important concerns as algorithmic
B&B algorithms. For this, we used networks generated with
                                                      complexity per search step, thorough parameter tuning, and
BNGenerator,6 which allows to generate networks with a
                                                      strong guidance by the evaluation function. For a wide range
rather accurate upper bound on the induced width. The re-
                                                      of MPE instances, GLS+ widely outperforms GLS and the
sults in Figure 2(c) show that the induced width has a major
                                                      best-performing exact algorithms (all of which are B&B algo-
effect on algorithm performance, and that GLS and GLS+
                                                      rithms) that deﬁned the state-of-the-art in MPE solving. Most
scale much better with induced width than the B&B algo-
                                                      importantly, we demonstrated that the performance of GLS+
rithms (we omit s-AOMB and d-AOMB which were worse
                                                      scales much better than GLS and B&B algorithms with the
than d-BBMB). We attribute this to the B&B algorithm’s MB
                                                      network and domain size, as well as with network density
heuristic whose guidance is impaired for high induced widths.
                                                      and induced width. GLS+ also shows the best performance
  In our third experiment we studied real-world networks
                                                      for real-world instances.
from the Bayesian Network Repository.7 Here, GLS+ also
                                                        In contrast to recent claims that stochastic local search al-
performs very well, except for large networks with low in-
                                                      gorithms are not competitive for MPE solving [Marinescu et
duced width, which are easily solved with VE. As we show
                                                      al., 2003], our results establish SLS as a state-of-the-art ap-
in Table 2, a short preprocessing stage (b > 0) signiﬁcantly
                                                      proach for MPE solving that merits further investigation. In
improves the performance of GLS+ for these structured net-
                                                      particular, the anytime characteristics and excellent scaling
works, making it faster than all B&B algorithms (with op-
                                 8
timal i-bound) for all but one network. In particular, note without the preprocessing (b = 0), we performed 25 additional runs
                                                      of two hours on the instances Diabetes and Munin4. Out of
  6
   http://www.pmr.poli.usp.br/ltd/Software/BNGenerator/ these, eleven runs solved the Diabetes network, but the Munin4
  7http://www.cs.huji.ac.il/labs/compbio/Repository/  network was never solved. This highlights the importance of pre-
  8To study whether GLS+ eventually ﬁnds the optimum even processing for structured networks.