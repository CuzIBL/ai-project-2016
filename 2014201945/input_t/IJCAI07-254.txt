                 Sequence Prediction Exploiting Similarity Information
                      Istvan´ B´ıro´ and Zoltan´ Szamonek  and  Csaba Szepesvari´
          Computer and Automation Research Institute of the Hungarian Academy of Sciences
                               Kende u. 13-17, Budapest 1111, Hungary
                                       E¨otv¨os Lor´and University
                            P´azm´any P. s´et´any 1/c, Budapest 1117, Hungary
                          ibiro@sztaki.hu, zszami@elte.hu, szcsaba@sztaki.hu

                    Abstract                          class-based n-gram estimation [Brown et al., 1992], a greedy
                                                      algorithm searches the space of clustering to ﬁnd one that in-
    When data is scarce or the alphabet is large,     creases the data likelihood the most. The method is extended
    smoothing the probability estimates becomes in-                     [            ]
                           n                          to handle trigram by Ueberla, 1994 , where a heuristic ran-
    escapable when estimating -gram models. In this   domization was used to speed-up the clustering process with
    paper we propose a method that implements a form  only a slight loss in performance.
    of smoothing by exploiting similarity information
    of the alphabet elements. The idea is to view the   These early works rely  on short span relationships
    log-conditional probability function as a smooth  (such as bigram or trigram) and therefore tend to produce
    function deﬁned over the similarity graph. The al- syntactically-oriented clusters. Another approach proposed
    gorithm that we propose uses the eigenvectors of  by [Bellegarda et al., 1996] exploits large span relation-
    the similarity graph as the basis of the expansion ships between words and results in clusters that are seman-
    of the log conditional probability function whose tically oriented. The idea is to exploit that documents can
    coefﬁcients are found by solving a regularized lo- be thought of as a semantically homogenous set of sentences.
    gistic regression problem. The experimental results Hence, if a set of documents is available then this knowledge-
    demonstrate the superiority of the method when    source could be exploited to create semantically meaningful
    the similarity graph contains relevant information, word-clusters. The algorithm forms a word-document matrix
    whilst the method still remains competitive with  given the training data, performs singular-value decomposi-
    state-of-the-art smoothing methods even in the lack tion (SVD) on the resulting matrix, and then clusters over the
    of such information.                              projections according to a well-chosen measure.
                                                        Clustering is a special case of constructing similarity infor-
1  Introduction                                       mation over the alphabet elements (most often over the words
                                                      or word-forms of a language). [Dagan et al., 1999] pro-
Statistical language models (SLM) attempt to capture the
                                                      posed to use a kernel-based smoothing technique where the
regularities of a natural language using statistical methods to
                                                      kernel function is built using similarity information derived
construct estimates of distributions of various linguistic units,
                                                      from word-cooccurrence distributions. They have shown up
such as morphemes, words, phrases, sentences, or documents.
                                                      to 20% reduction in perplexity for unseen bigrams as com-
Because of the categorical nature and the large vocabulary of
                                                      pared to standard back-off schemes. [Toutanova et al., 2004]
natural languages, statistical techniques must estimate a large
                                                      investigated a Markov chain model that exploits apriorisimi-
number of parameters, and thus depend critically on the avail-
                                                      larity information, whose stationary distribution was used for
ability of a large amount of training data. Given the complex-
                                                      solving prepositional phrase attachment problems.
ity of languages, such training data is rarely available. In
the lack of a sufﬁciently large and rich dataset, simple ap- Our Contribution In this paper we propose a method that
proaches like straightforward maximum likelihood (ML) es- implements a form of smoothing by exploiting similarity in-
timation tend to produce poor quality models.         formation of the alphabet elements. The idea is to view the
  Smoothing in this context is typically used to refer to tech- log-conditional probability function as a smooth function de-
niques that combine various ML probability estimates to pro- ﬁned over the similarity graph. The algorithm that we pro-
duce more accurate models. An enormous number of tech- pose uses the eigenvectors of the similarity graph as the basis
niques have been proposed for the smoothing of n-gram mod- of the expansion of the log conditional probability function.
els, including discounting, recursively backing off to lower The coefﬁcients of the expansion are found by solving a regu-
order n-grams, linearly interpolating n-grams of different or- larized logistic regression problem. The experimental results
ders. An excellent survey of these and some other techniques demonstrate the superiority of the method when the similarity
can be found in [Chen and Goodman, 1996].             graph contains relevant information, whilst the method still
  Clustering models make use of smoothing ideas, but also remains competitive with state-of-the-art methods even in the
attempt to make use of similarities between words. In the lack of such information.

                                                IJCAI-07
                                                  15762  Sequence prediction based on similarity            spectrum of the graph Laplacian is known to have a close re-
   information                                        lationship to global properties of the graph, such as “dimen-
                                                      sion”, bottlenecks, clusters, mixing times of random walks,
Consider a word prediction task over some vocabulary V.Let
p w|h                                      w  ∈V      etc. The spectral decomposition method employed is moti-
 (   ) denote the conditional probability of word ,   vated as follows: The smoothness of a function, f : V→
where h ∈V∗  is some history. If we constrain the size of
                                                      R  on the graph G can be measured by the Sobolev-norm
histories to n−1 words we get the well known n-gram model.               2                          2
                                                      fG  =       |f(v)| dv +        (f(u) − f(v)) wuv.
For the sake of simplicity in this paper we will only deal with  v∈V             (u,v)∈E
bigrams (the methods proposed can be readily extended to The ﬁrst term here controls the size of the function, whilst
higher order n-grams). In what follows the notation p(y|x) the second controls the gradient. Using this norm, the ob-
will be used to denote the bigram probabilities.      jective to exploit similarity information can be expressed as
                                    {ϕ }              the desire to ﬁnd a loglikelihood function whose G-norm
  Given  appropriate basis functions, i ,  the log-                                          f
probabilities of the bigrams can be written in the form is small. Now, the projection of a function on the lin-
                                                     ear function space spanned by the top k eigenvectors of the
             log p(y|x) ∝   αiϕi(x, y).         (1)   Laplacian is the smoothest approximation to f with k ba-
                        i∈I                           sis functions, in the sense of the G-norm. Hence, inﬂu-
                             I   V×V                  enced by [Ding et al., 2002] we decomposed P using sin-
Clearly, the simplest choice is to let = and use the so-                               T
                            ϕ x, y   ϕ      x, y      gular value decomposition: P = USV . For practical pur-
called Euclidean basis functions, i( )= (i1,i2)( )=
δ i ,x δ i ,y       δ ·, ·                            poses, we truncated the SVD of P in such a way that the
 ( 1 ) ( 2  ),where  ( ) is the Kronecker’s delta func- P     . P         ·
tion. With this basis function set, ﬁtting this model to some k F =09 F ,where  F is the Frobenius norm, and
                                                      P     U S V T                       P
dataset is equivalent to maximum likelihood training. The k = k k k is the truncated version of where only the
main idea in the paper is that given some similarity infor- k column vectors of U and k row vectors of V corresponding
mation over V it might be possible to construct another ba- to the k largest singular values of S are kept. For proper nor-
sis function set that facilitates generalization to unseen cases. malization, the singular vectors in Uk are multiplied by the
Since the basis functions that we will construct will form an square root of the respective singular values [Dhillon, 2001;
                                                                              1/2
overcomplete representation, we will resort to a form of reg- Ding et al., 2002]: Uk = UkSk .
ularization to prevent overﬁtting.                      Now, the basis functions are obtained using the columns of
                                                      U                          ψ  ,...,ψ  ϕ     x, y
2.1  Incorporating similarity information               k: Denoting these columns by 1    k, (i1 ,i2)( )=
                                                      δ(i1,y)ψi2 (x),wherei1,x,y ∈V, i2 ∈{1,...,k}.Since
Let V = {w1,...,w|V|} be the words in the vocabulary. In
                                                      the similarity information might be unreliable we added
order to estimate the p(y|x) conditional probability distribu-                   ϕ x, y   φ      x, y
           x, y ∈W                                    the Euclidean basis functions, i( )=   (i1,i2)( )=
tion, where ( )     , we used the similarity information δ i ,x δ i ,y
hidden in the context. Assume that the similarity informa- ( 1 ) ( 2 ), to the set obtained. This way even when
                                                      the automatically constructed basis functions are useless, the
tion between words is represented as an undirected weighted
                                                      method still has a chance to recover. To handle unknown
graph G =(V,E,W),whereE       ⊂V×Vare the edges
                +                                     words, one may resort to Nystr¨om’s method (e.g. [Baker,
and W  : E →  R0  assigns non-negative weights to word-
                                                      1977]): In order to extend a singular vector ψi to a new word,
pairs. For mathematical convenience, we use W to denote z                w        x ∈V
   |V| × |V|                       i, j th      W      , it sufﬁces to know xz for all in the set of known
the         weight matrix, where the ( ) entry of                        √wxz
                                                      words. In fact, x∈V     ψi(x) can be shown to be a good
is the weight of the edge i → j. The idea is to construct                 dxdz
basis functions that respect the similarity information in W . approximation.
One way to accomplish this is to use spectral graph cluster- In the experiments below we always used a single simi-
ing which is known to yield basis functions that can be used larity graph, though the algorithm has the natural ability to
to represent any square integrable function over G [Chung, use more graphs by simply merging the set of resulting basis
1997]. In fact, spectral graph clustering methods construct functions. Just like in regression, we may use all the basis
basis functions that are natural for constructing geodesically functions, as well as all the products of two basis functions:
smooth global approximations of the target function. In other
                                                                            
words, smoothness respects the edges of the graph. In our        p y|x ∝        αaϕa  x, y
case this means that similar words (or word-pairs) will be as- log (  )           i i (  )+
                                                                           a i∈I
signed similar probabilities. It is left for future work to deter-    
                                                                            ab a       b
mine if the results presented here can be improved by using                αij ϕi (x, y)ϕj (x, y)     (2)
other techniques, such as diffusion wavelets by [Coifman and        a,b i,j∈I
Maggioni, 2006].
  In the particular method we have chosen, the basis func-
                                                                                              [         ]
tions are computed using the singular value decomposition of In fact, this is a second order ANOVA model Lin, 2000 .
               − 1   − 1                              Higher order models (products of more than two basis func-
the matrix P = D 2 WD  2 .HereD is the diagonalvalency
                                      d        w      tions) may also be used. In practice, however, they are rarely
matrix; its diagonal elements are deﬁned by i = j ij . used since second order models typically produce sufﬁciently
This operator is spectrally similar to the normalized graph
                       − 1          − 1               good results. When more than one model sets are combined,
Laplacian operator, L = D 2 (D − W )D 2 . In fact, ele- the number of parameters increases rapidly. Hence, the use
                                − 1    − 1
mentary algebra yields that I − L = D 2 WD 2 = P .The of a regularized ﬁtting method becomes of high importance.

                                                IJCAI-07
                                                  15772.2  Solving the regression problem                   Algorithm 1 Similarity Based Smoothing (SBS)
Since by assumption all of our basis functions depend only Input: similarity information between words, training data
through the Kronecker delta on y, equation (1) when raised Output: estimated probability distribution
to the exponent takes the form:                        1. Build a similarity graph between words G =(V,E,W)
                                     |V|
                                                                                                1      1
                    T                     T                                                     − 2   − 2
                1  αy β(x)               αi β(x)       2. Calculate the normalized matrix of G: P = D WD
 p(y|β(x),α)=     e     ,whereZα  =     e       (3)
               Z                                                                                  T
                α                    i=1               3. Determine the SVD decomposition of P = USV
                                      
                      α     α ,...,α    T              4. Calculate the mapping operator from the singular vectors
is the normalizing factor, =  1     |V|   is the ma-                                  1
                                     α                                                2
trix of the weight parameters to be learned , y contains the of top singular values: Φ=UkSk
weight parameters of word y,andβ(x) is the vector formed
                                                       5. Train the Logistic Regression Model weight parameters
from the basis functions evaluated at x. We shall call β(x)
                                                          using the training data
the feature vector associated with x.
  Let the training data be the sequence D =(v1,...,vN ),
vi ∈V. Assuming that the data is generated by a ﬁrst order
Markov model where the transition probabilities can be mod- IKN is considered as one of the best smoothing techniques
eled by a multinomial model (3), the data log-likelihood takes [Goodman, 2001].
the following form:                                     In order to evaluate the methods we calculated the em-
                  N                                  pirical cross-entropy of the trained model on held-out data,
                                                     w  ,...,w
   l   α|D           αT β v     −                       1      N :
    ML(    )=          vj ( j−1)                (4)
                 j=2                                                          N
                                                                            1
                                                               HN (p, pˆ)=−      log pˆ(wi|wi−1) .    (6)
                    |V|                                                 N        2
                              T                                               i=1
              −  log    exp αi β(vj−1)  +const
                     i=1                              Here pˆ(wi|wi−1) is the probability assigned to the transition
The maximum  likelihood (ML) estimate of the parameter from wi−1 to wi by the model and p denotes the true distri-
vector is the vector that maximizes this function. In order bution. (For the synthetic datasets we have wi ∼ p(·|wi−1).)
to prevent overtraining it is advisable to prefer smooth solu- Since by assumption, the held-out data has the same distribu-
tions, especially if the number of basis functions is big (in tion as the training data, by the Shannon-McMillan-Breiman
other words, if the feature vectors, β, are high dimensional). theorem we know that (under mild assumptions) HN is close
One way to enforce smoothness is to introduce a prior dis- to the cross-entropy of pˆ with respect to the true distribution
tribution over the parameters αij . Several choices are pos- p of the data. The cross-entropy of pˆ and p is the sum of the
sible for such priors. In this work we studied the behaviour entropy of p and the Kullback-Leibler divergence of pˆ and p,
of the method when it was used either with the Laplacian- a non-negative quantity. The smaller the cross-entropy, the
prior, p(αij ) ∝ λij exp(−λij |αij |), or with the Gaussian- closer the estimated model is to the true model.
                     α2
     p α ∝   1     −  ij                                For each test, we calculated the cross entropies for the
prior, ( )  λ  exp( 2λ2 ).
             ij       ij                              ML-trained bigram, IKN and the proposed similarity based
  Assuming one of the above priors, the training problem be- smoothing technique. In the case of synthetic datasets we
comes the maximization of the following objective function:
                  
                                   also estimated the entropy of the models by using the true
               N                                     transition probabilities in (6). The corresponding points in
  l     α|D        αT β v    −                        the graphs are labeled as ‘model’.
   MAP(    )=       vj ( j−1)                   (5)
               j=2
                                                     3.1  Tests on Synthetic Data
         |V|                |V| k
    −            αT β v      −        λ  |α |p        The model generating the synthetic data In order to de-
      log    exp  i  ( j−1)            ij  ij         velop a good understanding of the possible advantages and
         i=1                   i=1 j=1
                                                      limitations of the proposed method, we tested it in a con-
      p
i.e. an   penalty term is added to the ML objective function. trolled environment where data was generated using some
Here p =1or   2 depending on which prior one wishes to designated bigram model. In order to make the test interest-
use: p =1corresponds to the Laplacian prior, whilst p =2 ing, we decided to use “clustered” Markov models of the fol-
corresponds to the Gaussian prior.                    lowing form: the probability of observing y given x is com-
  We used the SMLR java implementation of [Krishnapuram puted by
et al., 2005] which implemented both the Laplacian (l1)and
               2                                               P  y|x   P  y | c y P c y | c x ,
Gaussian priors (l ), to ﬁnd the solution for equation (5). The  (   )=   (   ( ))  ( ( )  ( ))       (7)
sketch of the algorithm is shown on Algorithm 1.      where c(x) is the cluster of symbol (word) x (c : V→C
                                                      with |C| < |V|). The idea is that the next observation de-
3  Experimental Results                               pends on the past observation x only through its class, c(x)
We compared the performance of similarity based smoothing, and hence two past observations, x and x yield to identical
SBS, on synthetic and real-world data to both plain bigram es- future distributions whenever c(x)=c(x). Note that when
timates (BI) and interpolated Kneser-Ney smoothing (IKN). P (y|c)=δ(c(y),c) then the model can be thought of as a

                                                IJCAI-07
                                                  1578Hidden Markov Model (HMM) with state transitions deﬁned 3.2 Results
    C
over , whilst when the observation probabilities are uncon- We performed a sensitivity analysis to test the effect of how
                                |C|
strained then in general no HMM with states is equivalent the various parameters inﬂuence the results. In particular, we
to this model. In any way, the model can be speciﬁed by two studied the performance of SBS as a function of the observa-
         A, B       A        P c |c   A ∈  R|C|×|C|
matrices, (  ), with c1,c2 =  ( 2  1) (           )   tion noise, γ, that masks the identiﬁability of the block struc-
                       |V|×|C|
and By,c = P (y|c) (B ∈ R   ). Let us now describe how ture, the noise in the similarity matrix () that gradually de-
these matrices were generated.                        creases the quality of the similarity information available for
  For computing the matrix A, we start with a permutation SBS, the number of training sentences (Ntr) and the cluster
matrix of size |C| × |C| and perturb it with random noise so structure. These parameters were changed one by one, whilst
that (i) all transition probabilities are nonzero and (ii) for each the others are kept at their default values which were γ =0.2,
state the number of “signiﬁcant” transitions lies between 1  =0.1, Ntr = 300. The default cluster structure was to have
    |C|                                                                            ,  ,  , ,
and 4 .                                               6 clusters, having observations 30 20 10 5 5 and 5, respec-
  For computing B, we start from the idealized block- tively (so that some clusters were bigger, some were smaller).
                                                          |V|
structured matrix with Byc ∝ δ(c(y),c) and then perturb it Thus , was kept at 75. We present the results for both the
according to                                          Laplacian and Gaussian priors, the corresponding results are
                                                    labeled as ‘SBS Gauss’ and ‘SBS Lapl’ in the graphs.
              Byc ∝ Byc + δ(1 + γzyc),
where the elements of z are independent random variables Sensitivity to noise masking the block-structure When
                    yc                                γ
uniformly distributed in [−0.5, 0.5].Ifδ =0then the block =0, the observations can be used directly to infer the un-
structure of the source is clearly identiﬁable based on the out- derlying classes and the estimation problem is easier. When
puts: Given an observation y and knowing C we can infer the block-structure masking noise is increased the problem
with probability one that the hidden state is c(y) and as noted becomes harder.
before the model collapses to a hidden Markov model with
C            δ                                                5.6
  states. When is non-zero this structure is blurred. In the                               ikn
experiments we used δ =0.1 whilst we let γ change in [0, 1].                            sbs lapl
        γ                                                     5.5                     sbs gauss
Note that =1roughly corresponds to a noise level of 5%                                   model
                                                                                        bigram
in the observations.                                          5.4

Similarity information provided for SBS We controlled         5.3
how well the similarity information provided for SBS reﬂects
the actual block structure of the data. The perfect similar- cross-entropy  5.2
ity information assigns 0 to observations (words) in different
clusters, whilst it assigns the same positive value, say 1,to  5.1
observations in the same cluster. The corresponding matrix is
                 |V|×|V|                                       5
denoted by S (S ∈ R    ): Sxy = δ(c(x),c(y)).                     0     0.2    0.4   0.6   0.8    1
  We then disturbed S by adding noise to it. For this a ran-            block-structure masking noise
dom |V| × |V| matrix (Z) was created whose entries for inde-
pendent, uniformly distributed random variables taking val- Figure 1: The cross-entropy of models built with various al-
ues in [0, 1]. Given a noise parameter,  ≥ 0, the perturbed gorithms as a function of the block-structure masking noise
matrix is computed by                                 parameter (γ)

       S = S +  ((−S   Z)+((1 −  S)   Z)).
                                                        Figure 1 shows the results of the measurements. It can
Here U   V denotes the component wise product of matrices be observed that the proposed method performs signiﬁcantly
U and V , 1 denotes the matrix with all of its entries being better over the considered spectrum of γ than its peers: In
1. In words, increasing  reduces the inter-cluster similarity fact, as the masking noise gets bigger, IKN’s performance
and increases between-cluster similarity. In the extreme case converges to that of the bigram model. On the other hand,
when  =1all similarity information is lost.          SBS was able to maintain its estimation accuracy for the
                                                      whole range of γ. In all of these experiments SBS with the
Training and test datasets A training dataset normally Gaussian parameter-prior performed slightly better than SBS
contains Ntr = 300 observation sequences (except when we with the Laplace prior. We believe that this is because the
experiment by changing this parameter), each having a ran- Laplacian prior prefers sparse solutions and the number of
dom length that was generated by drawing a random number basis functions is not high enough for making sparse solu-
L from a normal distribution with mean 11 and variance 6 and tions preferable.
then setting the length to max(2,L). The test dataset (sepa-
rate from the training dataset) had 5000 sequences which was Robustness against the degradation of the similarity in-
generated using the same procedure. All measurements were formation In the next set of experiments we investigated
repeated 9 times and the average values are presented. the sensitivity of the method to the quality of the similarity

                                                IJCAI-07
                                                  1579information. For this we gradually increased the similarity-  6.1
                                                                                          ikn
information noise parameter, , from 0 to 1.Aswehave            6                        sbs lapl
discussed, when  =0the similarity information is perfect,    5.9                     sbs gauss
                                                                                        model
whilst when =1all similarity information is lost.             5.8                       bigram
                                                              5.7
                                                              5.6
        5.7
                                    ikn                       5.5
                                 sbs lapl
        5.6                                                   5.4
                                sbs gauss                  cross-entropy
        5.5                       model                       5.3
                                  bigram
                                                              5.2
        5.4
                                                              5.1
        5.3                                                    5
                                                                 0     200   400    600    800   1000
        5.2

     cross-entropy                                                    no. of sentences in the training corpus
        5.1

         5                                            Figure 3: The cross-entropy of models built with various al-
                                                      gorithms as a function of the number of sentences in the train-
        4.9                                                   N
            0     0.2   0.4   0.6    0.8    1         ing data ( tr).
                 noise in the similarity information
                                                                             H  p, p   − H  p, p
                                                            clusters           (  ˆikn)    (  ˆsbs)
Figure 2: The cross-entropy of models built with various al-                  average       stddev
gorithms as a function of the noise parameter, , governing 1,1,1,1,1,1,1      -0.0112     0.0036
the quality of the similarity information                   10                  0.1776     0.0406
                                                            10x2                0.4808     0.0246
  As expected, when the similarity information gets weaker, 10x3                0.8679     0.0633
the performance of SBS degrades and converges to that of    10x5                1.5223     0.0708
the ML-estimated bigram model. It is notable that even when 10,5                0.3001     0.0361
 =0.7 (when the similarity information is already very poor) 10,7,5            0.5150     0.0499
SBS performs as well as IKN. The reason is that although in 10,7,5,3            0.6296     0.0311
these experiments we did not add the Euclidean basis func-  30,20,10            1.7790     0.1691
tions to the set of basis functions, we can expect the spectrum 30,20,10,5x3    1.3352     0.2011
of a high-noise similar matrix to be uniform, hence covering 30,20,10,5x3,1x3   1.0971     0.2303
90% of the spectrum will add almost all singular vectors to 30,20,10,5x3,1x9    0.6537     0.1141
the basis and thus the model automatically retains its power.
                                                      Table 1: The cross-entropy decrease of SBS as compared with
                                                      IKN for a number of different cluster structures
Sparsity of training data The main promise of SBS is that
by using the similarity information it is capable of achieving
better performance even when the available training data is 3.3 Tests on Real Data
limited. In order to validate this, we performed a set of ex- The task considered is to predict POS sequences on the Wall
periments when the number of sentences in the training data Street Journal (WSJ) corpus. We extracted a weighted di-
was varied. The results are shown in Figure 3. Both with the rected sub graph based on all senses in all syntactic categories
Gaussian and the Laplacian priors, SBS outperformed IKN from WordNet [Fellbaum, 1989] for 50, 000 words. Based on
for a wide range of values of Ntr. It is interesting to note this information, a POS similarity graph (PG) was built as
that the Gaussian prior performed much better than any of follows: using the Brown corpus1, we ﬁrst collected the pos-
the other methods when the number of sentences was very sible POS categories for every word. Next we created a prior
small.                                                POS-tag distribution for all the words. Then for every unique
                                                      word in the Brown corpus, we added the out-links to the POS
                                                      graph according to the WordNet graph, using the weight of
Cluster structure We tested SBS with large variety of clus- the link in the WordNet graph and the weight of the POS tags
ter setups, ranging from 1 to 15 clusters, and with vocabulary in both sides of the link. For example, if we arrive at the
sizes between 7 and 110. The results are summarized on Ta- word live and if the link live → bouncy in the WordNet graph
ble 1.                                                has weight w then for all possible POS tag combinations of
  It is clear that if the number of clusters is small or when all live and bouncy, the link between the POS-pairs (x, y) is in-
the words are in different clusters, then there is no signiﬁcant creased by p(x|live) · p(y|bouncy) · w. In the next step, we
structural information in the similarity. It is notable that in calculated the basis functions as outlined beforehand.
such cases SBS was still able to perform as well as IKN. On
the other hand, if there is a signiﬁcant hidden structure in the 1The Penn Treebank Project: http://www.cis.upenn.edu/ tree-
model, SBS greatly outperforms IKN.                   bank/

                                                IJCAI-07
                                                  1580