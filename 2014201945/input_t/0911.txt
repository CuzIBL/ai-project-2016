      Semantic    Argument      Classiﬁcation     Exploiting    Argument      Interdependence

                       Zheng  Ping  Jiang 1 and   Jia Li 2 and  Hwee   Tou  Ng 1,2
               1. Singapore-MIT  Alliance, E4-04-10, 4 Engineering Drive 3, Singapore 117576
                    2. Department of Computer Science, National University of Singapore,
                                   3 Science Drive 2, Singapore 117543
                          {jiangzp,    lijia1,    nght}@comp.nus.edu.sg

                    Abstract                          low parse, or a constituent node in a full parse tree. Seman-
                                                      tic argument classiﬁcation involves classifying each seman-
    This paper describes our research on automatic se- tic argument identiﬁed into a speciﬁc semantic role, such as
    mantic argument classiﬁcation, using the PropBank ARG0,  ARG1,  etc.
    data [Kingsbury et al., 2002]. Previous research    Various features based on either a shallow parse or full
    employed features that were based either on a full parse of a sentence have been proposed. These features are
    parse or shallow parse of a sentence. These fea-  then used by some machine learning algorithm to build a clas-
    tures were mostly based on an individual seman-   siﬁer for semantic argument classiﬁcation. While these fea-
    tic argument and the relation between the predi-  tures capture the syntactic environment of the semantic argu-
    cate and a semantic argument, but they did not cap- ment currently being classiﬁed, they overlook the semantic
    ture the interdependence among all arguments of a context of the argument.
    predicate. In this paper, we propose the use of the We propose a notion of semantic context, which consists of
    neighboring semantic arguments of a predicate as  the already identiﬁed or role-classiﬁed semantic arguments in
    additional features in determining the class of the the context of an argument that is currently being classiﬁed.
    current semantic argument. Our experimental re-   Semantic context features are deﬁned as features extracted
    sults show signiﬁcant improvement in the accuracy from the neighboring semantic arguments, and used in clas-
    of semantic argument classiﬁcation after exploit- sifying the current semantic argument. The use of these fea-
    ing argument interdependence. Argument classiﬁ-   tures explicitly captures the interdependence among the se-
    cation accuracy on the standard Section 23 test set mantic arguments of a predicate.
    improves to 90.50%, representing a relative error
                                                        In this paper, we focus on the semantic argument classiﬁ-
    reduction of 18%.
                                                      cation task using PropBank[Kingsbury et al., 2002]. Our se-
                                                      mantic context features are derived from full parse trees. Our
1  Introduction                                       experimental results show signiﬁcant improvement in the ac-
                                                      curacy of semantic argument classiﬁcation after adding the
Deriving the semantic representation of a sentence is im- semantic context features, when compared to using only fea-
portant in many natural language processing tasks, such as tures of the current semantic argument.
information extraction and question answering. The re-
                                                        The rest of this paper is organized as follows: Section 2
cent availability of semantically annotated corpora, such as
                                                      explains in detail the application of semantic context features
FrameNet [Baker et al., 1998]1 and PropBank [Kingsbury
                                                      to semantic argument classiﬁcation; Section 3 gives a spe-
et al., 2002]2, prompted research in automatically producing
                                                      ciﬁc example to illustrate how semantic context features can
the semantic representations of English sentences. For Prop-
                                                      improve semantic argument classiﬁcation; Section 4 presents
Bank, the semantic representation annotated is in the form of
                                                      our experimental results; Section 5 reviews related work; and
semantic roles, such as ARG0, ARG1, etc. of each predicate
                                                      Section 6 concludes the paper.
in a sentence, and the process of determining these semantic
roles is known as semantic role labeling.
  Most previous research treats the semantic role labeling 2 Semantic Argument  Classiﬁcation  with
task as a classiﬁcation problem, and divides the task into two Semantic Context Features
subtasks: semantic argument identiﬁcation and semantic ar-
gument classiﬁcation. Semantic argument identiﬁcation in- Similar to [Pradhan et al., 2005], we divide the task of se-
volves classifying each syntactic element in a sentence into mantic role labeling into two sequential subtasks: semantic
either a semantic argument or a non-argument. A syntac- argument identiﬁcation and semantic argument classiﬁcation,
tic element can be a word in a sentence, a chunk in a shal- and treat each subtask as a separate classiﬁcation problem.
                                                      Our investigation focuses on semantic argument classiﬁca-
  1http://framenet.icsi.berkeley.edu/                 tion with the assumption that the semantic arguments have
  2http://www.cis.upenn.edu/˜ace                      been correctly identiﬁed by the semantic argument identiﬁca-                                                                                  S
Feature      Description                                                         ¨H
                                                                              ¨¨   HH
Sentence level features                                                     ¨¨       HH
                                                                          ¨¨            HH
predicate (Pr) predicate lemma in the predicate-argument struc-         ¨¨                HH
                                                                     ¨¨                     HH
                                                                   ¨¨                         HH
             ture                                                 ¨                              H
                                                                ¨¨                                HH
voice (Vo)   grammatical voice of the predicate, either active ¨                                    H
                                                              NP                 VP                  .
             or passive                                                         PP
                                                             (ARG1)              @@ PP
                                                             PP                    P              .
                                                              PP               @   PP
subcat (Sc)  grammar rule that expands the predicate’s parent    P                    PP
                                                          The Nasdaq index         @       P
             node in the parse tree                                   VBD    NP      PP     PP
                                                                     (predicate) (ARG2) (ARPG4) (ARGM-ADP V)
Argument-speciﬁc features                                                           PP    PP
phrase type (Pt) syntactic category of the argument constituent      added   1.01  to 456.64 on . . . shares
head word (Hw) head word of the argument constituent
                                                       Figure 1: Semantically labeled parse tree, from dev set 00
Argument-predicate relational features
path (Pa)    syntactic path through the parse tree from the ar-
             gument constituent to the predicate node   Pr Vo       Sc      Pt Hw      Pa    Po    Ar
position (Po) relative position of the argument consitituent with
             respect to the predicate node, either left or right add active VP:VBD NP PP PP NP index NP∧S VP VBD L ARG1
                                                        add active VP:VBD NP PP PP NP 1.01 NP∧VP VBD R ARG2
                                                        add active VP:VBD NP PP PP PP to PP∧VP VBD R ARG4
                                                        add active VP:VBD NP PP PP PP on PP∧VP VBD R ARGM-ADV
              Table 1: Baseline features

tion module in the ﬁrst step.                            Table 2: Semantic context features based on Figure 1
2.1  Baseline Features
By treating the semantic argument classiﬁcation task as a Each row of Table 2 contains the baseline features (as deﬁned
classiﬁcation problem using machine learning, one of the in Table 1) extracted for each semantic argument.
most important steps in building an accurate classiﬁer is the In Figure 1, suppose the semantic argument identiﬁcation
choice of appropriate features. Most features used in prior module has correctly identiﬁed the constituents “The Nasdaq
research, such as [Gildea and Jurafsky, 2002; Pradhan et al., index”, “1.01”, “to 456.64”, and “on . . . shares” as semantic
2005; Bejan et al., 2004], can be categorized into three types: arguments of the predicate “added”. Assume further that the
                                                      semantic argument “The Nasdaq index” has been assigned the
  • Sentence level features, such as predicate, voice, and semantic role ARG1. Now consider the task of assigning a
    predicate subcategorization.                      semantic role to the semantic argument “1.01”. Without using
  • Argument-speciﬁc features, such as phrase type, head semantic context features, the baseline features used for the
    word, content word, head word’s part of speech, and semantic argument “1.01” appear in row 2 of Table 2.
    named entity class of the argument.                 If we augment with the semantic context features of all
                                                      neighboring arguments, then the features P t, Hw, P a,
  • Argument-predicate relational features, such as an ar- and P o of the neighboring arguments ARG1, ARG4, and
    gument’s position with respect to the predicate, and its ARGM-ADV are added as additional features. Also, since
    syntactic path to the predicate.                  “The Nasdaq index” has been assigned a semantic role, its
  The above features attempt to capture the syntactic envi- argument class ARG1 is also used as an additional semantic
ronment of the semantic argument currently being classiﬁed. context feature Ar.
They are entirely determined by the underlying shallow parse
or full parse of the sentence. They carry no information about 2.3 Various Ways of Incorporating Semantic
the semantic arguments that have already been identiﬁed or Context Features
classiﬁed. As such, the classiﬁcation of each semantic argu- There are many possible subsets of the semantic context fea-
ment is done independently from other semantic arguments. tures in Table 2 that one can choose to add to the baseline
It assumes that the semantic arguments of the same predicate features of the current semantic argument being classiﬁed. In
do not inﬂuence each other.                           addition, since the semantic role labels of arguments that are
  We use the baseline features in [Pradhan et al., 2005] as already classiﬁed (like ARG1 in the previous example) are
our baseline. These features are explained in Table 1. available as additional context features, the order in which
                                                      we process the semantic arguments during semantic argument
2.2  Semantic Context  Features                       classiﬁcation can affect the accuracy of the semantic role la-
For a semantic argument, its semantic context features are de- bels assigned. A good classiﬁer should incorporate an infor-
ﬁned as the features of its neighboring semantic arguments. mative set of semantic context features, as well as adopt an
The combination of the features of the current semantic ar- argument ordering that helps to give the best overall accuracy
gument and its neighboring semantic arguments encodes the for semantic argument classiﬁcation.
interdependence among the semantic arguments.           We use context feature acronyms with subscripts to denote
  To illustrate, the parse tree in Figure 1 is annotated with the particular types of context features at speciﬁc locations rel-
semantic arguments of the predicate “added”. These semantic ative to the current argument being classiﬁed. For example,
arguments are ARG1, ARG2, ARG4,  and ARGM-ADV     .   Hw1  refers to the head word feature of the argument imme- Feature Description             Examples                 Pr Vo       Sc      Pt Hw      Pa    Po  Ar
                                                                                 nil
 P ti                            P t 1    P t1
         the syntactic category of the − =NP ; =PP        add active VP:VBD NP PP PP NP index NP∧S VP VBD L ARG1
         ith context semantic argu-                       add active VP:VBD NP PP PP NP 1.01 NP∧VP VBD R *
         ment                                             add active VP:VBD NP PP PP PP to PP∧VP VBD R
                                                          add active VP:VBD NP PP PP PP on PP∧VP VBD R
 Hwi     the head word of the ith con- Hw−1=index ;
                                                                                 nil
         text semantic argument  Hw1=to
 P ai    the path of the ith context P a−1=NP∧S VP VBD
                                  P a
         semantic argument       ;  1=PP∧VP  VBD        Table 5: Adding one type of context features Hw{−2..2}
 P oi    the position of the ith con- P o−1=L ; P o1=R
         text semantic argument
 Ari     the semantic role of the ith Ar−1=ARG1       is well-deﬁned. Using a linear ordering, the semantic argu-
         previous semantic argument                   ments in Figure 1 are processed in the order ARG1, ARG2,
                                                      ARG4,  and ARGM-ADV    .
                                                                              [              ]
Table 3: Examples of semantic context features. The current Experiments reported in Lim et al., 2004 indicate that
semantic argument being classiﬁed is “1.01” as shown in Fig- semantic arguments spanned by the parent of the predicate
ure 1.                                                node (the immediate clause) can be more accurately classi-
                                                      ﬁed. Hence, if we process these semantic arguments ﬁrst,
                                                      their semantic role labels Ari may be more accurately de-
diately to the right of the current argument being classiﬁed. termined and so may help in the subsequent classiﬁcation of
More examples are given in Table 3. We also use set notation the remaining semantic arguments. Inspired by [Lim et al.,
{−i..i} to denote the set of context features with subscript 2004], we view a parse tree as containing subtrees of increas-
index j ∈ {−i, . . . , i}. For example, Hw{−1..1} denotes the ing size, with each subtree rooted at each successive ances-
context features Hw−1 and Hw1.                        tor node of the predicate node. In subtree ordering, the se-
                                                      mantic arguments spanned by a smaller subtree are processed
Adding All Types of Context Features
                                                      ﬁrst before the arguments of the next larger subtree. Using
We could choose to add all types of context features within a subtree ordering, the semantic arguments in Figure 1 are
a certain window size. To illustrate with the example in Fig- processed in the order ARG2, ARG4, ARGM-ADV , and
ure 1, suppose the current semantic argument being classiﬁed ARG1.
is “1.01”. If we add all types of context features from the
     1  1
set {− .. } (i.e., within a window size of one), then the ad- 3 An Illustrating Example of the Utility of
ditional context features added are those in Table 4 that are
darkly shaded. The baseline features for the current argument Semantic Context Features
“1.01” is lightly shaded in Table 4.                  In this section, we shall motivate the utility of semantic con-
                                                      text features by considering the example in Figure 1. The
    Pr Vo       Sc      Pt Hw     Pa     Po Ar        predicate “add” has three rolesets deﬁned in PropBank, as

   add active VP:VBD NP PP PP NP index NP∧S VP VBD L ARG1 shown in Table 6. Each roleset corresponds to a different ar-
   add active VP:VBD NP PP PP NP 1.01 NP∧VP VBD R *   gument structure. Depending on the particular roleset that
   add active VP:VBD NP PP PP PP to PP∧VP VBD R       a speciﬁc instance of “add” belongs to, different argument
   add active VP:VBD NP PP PP PP on PP∧VP VBD R       classes are assigned to its semantic arguments.

                                                            Roleset Arguments
Table 4: Adding all types of context features from the set  add.01 ARG0(speaker), ARG1(utterance)
  1  1
{− .. }                                                     add.02 ARG0(adder), ARG1(thing being added),
                                                                  ARG2(thing being added to)
                                                            add.03 ARG1(logical subject, patient, thing
Adding a Single Type of Context Features                          rising / gaining / being added to),
Alternatively, we could add only a speciﬁc type of context        ARG2(amount  risen), ARG4(end point),
features within a certain window size, since it may be the case   ARGM-LOC(medium)
that certain types of context features are more effective than
others. To illustrate again with the same example. If we add Table 6: The rolesets of “add”, as deﬁned in PropBank
the head word features Hw{−2..2} (i.e., within a window size
of two), then the additional context features added are those The roleset for “add” in our example is “add.03”, where
in Table 5 that are darkly shaded. The baseline features for the ﬁrst argument to the left of “added”, “The Nasdaq index”,
the current argument “1.01” is lightly shaded in Table 5. is the “logical subject, patient, thing rising / gaining / being
                                                      added to”. This contrasts with the second roleset “add.02”,
Different Argument Ordering                           such as “added” in the sentence “Judge Curry added an addi-
So far we have implicitly assumed that the semantic argu- tional $55 million to the commission’s calculations.”, which
ments are processed in a linear ordering, i.e., according to is an illustrating example used in PropBank’s description of
the natural textual order in which the semantic arguments ap- the roleset “add.02”.
pear in a sentence. In PropBank, the semantic arguments of a During classiﬁcation of the semantic arguments “1.01” and
single predicate in a sentence do not overlap, so this ordering “to 456.64” in this example, using only the baseline fea-tures of an individual semantic argument results in “1.01” be- of the current argument are used together. Context features to
ing misclassiﬁed as ARG1 and “to 456.64” being misclassi- the left of the current argument are not effective when used
ﬁed as ARG2, when their correct semantic argument classes alone.
should be ARG2 and ARG4 respectively. Since the two role-
sets “add.02” and “add.03” have similar argument structures       Accuracy of increasing context window size
                                                                   1..1    2..2   3..3   4..4    5..5
(add something to something), and since “add.02” occurs   feature {−  } {−    } {−   } {−   } {−    }
                                                          all     89.27* 89.32*  88.91  88.52  88.20
more frequently than “add.03”, the semantic argument clas-         1..0    2..0   3..0   4..0    5..0
siﬁcation module has assigned the semantic argument classes feature {− } {−   } {−   } {−   } {−    }
                                                          all     87.88  87.41   87.14  86.84  86.70
of the roleset “add.02” to “1.01” and “to 456.64”, which is
                                                          feature {0..1} {0..2} {0..3}  {0..4} {0..5}
incorrect.                                                all     88.74  88.82   88.48  88.32  88.08
  However, our experiments in section 4 show that a clas-
siﬁer augmented with the context features Hw{−2..2} as-
signs the correct semantic argument classes to “1.01” and Table 7: Accuracy based on adding all types of semantic con-
“to 456.64”. This is because in the sentence “index added text features, with increasing window size, assuming correct
something to something”, that “index” is the head word of argument identiﬁcation and linear ordering of processing ar-
the ﬁrst argument gives a strong clue that the correct roleset guments
is “add.03”, since “index” cannot ﬁll the adder (agent) role of
“add.02”. In fact, occurrence counts from PropBank section
                                                      Results Using a Single Type of Context Features
02-21 indicate that of the three occurrences of “index” as the
head word of the ﬁrst argument of predicate “add”, all three Accuracy scores of semantic argument classiﬁcation after
                                                      augmenting baseline features with a single type of context
appear in the roleset “add.03”. This example illustrates the
                                                      features are shown in Table 8. The best improvement results
importance of neighboring semantic arguments in the context
                                                                                    Hw
of the current semantic argument in determining its correct from the use of head word features . The use of features
                                                      Ar reduces accuracy. This is likely due to semantic argument
semantic argument class.
                                                      classes not being accurately determined, and errors commit-
                                                      ted in a position in linear ordering might affect classiﬁcation
4  Experimental    Results                            accuracy at subsequent positions. However, a better argument
Our semantic argument classiﬁcation module uses sup-  ordering may improve the effectiveness of Ar.
                                              3
port vector learning and is implemented with yamcha and 4.2 Results of Subtree Ordering
tinysvm4 [Kudo and Matsumoto, 2001]. Polynomial kernel
with degree 2 is used in a one-vs-all classiﬁer. The cost per We repeat the experiments of the last subsection, but this time
unit violation of the margin C = 1, and the tolerance of the with the use of subtree ordering. The accuracy scores are
termination criterion e = 0.001.                      given in Table 9 and Table 10. The most prominent differ-
  The training, development, and test data sections follow ence from the results of linear ordering is the better accuracy
the conventional split of Section 02-21, 00, and 23 of Prop- scores with the use of the Ar features, as shown in Table 10
Bank release I respectively. In Section 4.1 to Section 4.4, we compared with Table 8. The improvement observed is consis-
present the accuracy scores when evaluated on development tent with the ﬁndings of [Lim et al., 2004], that the argument
section 00. In Section 4.5, we present the accuracy scores on class of the semantic arguments spanned by the parent of the
test section 23.                                      predicate node can be more accurately determined.
  The semantic argument classiﬁcation accuracy when eval- 4.3 More Experiments with the Ar Feature
uated on section 00 using baseline features is 88.16%, assum-
ing that semantic argument identiﬁcation is done correctly Unlike other semantic context features that are completely de-
(i.e., “gold argument identiﬁcation”). In Section 4.1 to Sec- termined once argument identiﬁcation is completed, the Ar
tion 4.4, accuracy scores that show a statistically signiﬁcant feature is dynamically determined during argument classiﬁ-
improvement (χ2 test with p = 0.05) over the baseline score cation. It may be possible to improve the overall accuracy of
of 88.16% are marked by “*”. The highest score of each row
in the score tables is boldfaced.                                 Accuracy of increasing context window size
                                                          feature {−1..1} {−2..2} {−3..3} {−4..4} {−5..5}
4.1  Results of Linear Ordering                           Pt      89.01* 89.06*  88.89  88.81  88.55
                                                          Hw      89.49* 89.82* 89.56* 89.42*  89.32*
Results Using All Types of Context Features               Pa      89.17* 89.04*  88.89  88.66  88.55
Table 7 contains accuracy scores after augmenting baseline Po     88.63  88.74   88.58  88.54  88.24
features with all types of semantic context features. Ar fea- feature {−1..0} {−2..0} {−3..0} {−4..0} {−5..0}
ture is only present within context feature sets that contain Ar  87.98  87.72   87.73  87.78  87.75
neighboring arguments to the left of the current argument.
We observe that the accuracy scores have a signiﬁcant im-
provement from the baseline if context features on both sides Table 8: Accuracy based on adding a single type of semantic
                                                      context features, with increasing window size, assuming cor-
  3http://chasen.org/˜taku/software/yamcha/           rect argument identiﬁcation and linear ordering of processing
  4http://chasen.org/˜taku/software/TinySVM/          arguments            Accuracy of increasing context window size               Accuracy of increasing context window size
    feature {−1..1} {−2..2} {−3..3} {−4..4} {−5..5}     Ar with    {−1..0} {−2..0} {−3..0} {−4..0} {−5..0}
    all    88.99* 88.96*  88.62   88.43  88.33          linear      87.98   87.72  87.73  87.78   87.75
    feature {−1..0} {−2..0} {−3..0} {−4..0} {−5..0}     linear beam 88.83   88.46  88.64  88.66   88.67
    all     88.92  88.83  88.87   88.66  88.40          linear gold 89.37* 89.54*  89.59* 89.60* 89.58*
    feature {0..1} {0..2} {0..3} {0..4}  {0..5}         subtree     88.85   88.84  88.90  88.87   88.83
    all     88.41  88.38  88.08   87.80  87.63          subtree beam 89.13* 89.20* 89.18* 89.19* 89.20*
                                                        subtree gold 89.87* 89.84* 90.09* 90.07* 90.11*
Table 9: Accuracy based on adding all types of semantic con-
text features, with increasing window size, assuming correct Table 11: More experiments with the Ar feature, using beam
argument identiﬁcation and subtree ordering of processing ar- search and gold semantic argument class history
guments

            Accuracy of increasing context window size 4.4 Combining   Multiple Semantic Context
    feature {−1..1} {−2..2} {−3..3} {−4..4} {−5..5}        Features
    Pt     89.03* 88.98*  88.83   88.77  88.46        The best accuracy score we have obtained so far is 89.82%,
    Hw     89.29* 89.60*  89.29* 89.20* 89.08*
                                                      given by Hw−2..2 in Table 8. This score is a statistically sig-
    Pa     89.25* 89.25*  89.14*  88.95  88.74        niﬁcant improvement over the baseline score of 88.16% ob-
    Po      88.77 88.96*  88.72   88.37  88.21        tained with only baseline features. We want to further lever-
    feature {−1..0} {−2..0} {−3..0} {−4..0} {−5..0}   age on the other types of semantic context features. Error
    Ar      88.85  88.84  88.90   88.87  88.83        analysis of experiments in Table 8 and 10 on development
                                                      section 00 shows that classiﬁers using different semantic con-
Table 10: Accuracy based on adding a single type of seman- text features make different classiﬁcation mistakes. Thus we
tic context features, with increasing window size, assuming would like to explore the combination of multiple semantic
correct argument identiﬁcation and subtree ordering of pro- context features. This provides a basis for combining multi-
cessing arguments                                     ple semantic context features. Here we more carefully select
                                                      the context features than simply using all available features
                                                      as in Table 7 and 9.
assigning argument classes to all the semantic arguments of a
predicate if we use a beam search algorithm to keep the best A Single Classiﬁer with Multiple Context Features
k combinations of argument classes assigned to all semantic The best context features from each row of Table 8,
arguments processed so far.                           P t{−2..2}, Hw{−2..2}, P a{−1..1}, P o{−2..2}, and Ar{−1..0}
                                                      are combined to form a new classiﬁer based on linear order-
Ar Feature with Gold Semantic Argument Class
                                                      ing. Similarly P t{−1..1}, Hw{−2..2}, P a{−2..2}, P o{−2..2},
To determine the maximum accuracy improvement possible and Ar{−3..0} from Table 10 are combined to form a new
with the use of the Ar feature, we assume that the argument classiﬁer based on subtree ordering. Evaluation on develop-
classes of all previous semantic arguments have been accu- ment section 00 gives accuracy scores of 89.54% and 89.19%,
rately determined and used as semantic context features. The for linear and subtree ordering, respectively. The lack of ac-
experimental results are presented in Table 11, titled “linear curacy improvement over that of Hw−2..2 shows that such a
gold” and “subtree gold” respectively for linear and subtree simple combination might accumulate the errors introduced
ordering.                                             by each additional context feature.
Ar Feature with Beam Search                           Voting with Multiple Classiﬁers
Tinysvm’s output scores are converted to conﬁdence scores Instead of building a new classiﬁer with multiple semantic
between 0 and 1, by applying the sigmoid function y = context features, one can combine the classiﬁcation results
  1
1+e−x . We implemented a beam search algorithm to ﬁnd the of multiple classiﬁers and hope to achieve better accuracy
overall best sequence of argument classes to be assigned to through consensus and mutual error correction. The voting
all semantic arguments. At each iteration, the beam search process combines k different classiﬁers each belonging to
algorithm processes one semantic argument and determines one row in Table 8 or Table 10. Currently only classiﬁers
the top 3 argument classes with the highest conﬁdence scores based on the same ordering are combined. Classiﬁers are
for each semantic argument. It then ﬁnds and keeps the best chosen from different context feature types, but can be of
k (k = 10 in our implementation) argument class sequences the same context feature window size. For a particular se-
overall. Each argument class sequence is assigned a conﬁ- mantic argument A, each candidate argument class will accu-
dence score, which is the product of the conﬁdence scores of mulate its conﬁdence score assigned by all the voting classi-
all arguments in the sequence. Finally, the sequence of argu- ﬁers. The candidate argument class with the highest aggre-
ment classes for all semantic arguments with the highest score gate score will be output as the argument class for A. Ex-
is chosen. Experimental results are presented under “linear haustive experiments are carried out with k = 2, 3, 4, 5. On
beam” and “subtree beam” in Table 11, and show improve- the development section 00, the best voting classiﬁer in linear
ments after using beam search with Ar feature for subtree ordering is {P t{−1..1}, Hw{−4..4}, P a{−2..2}, Ar{−1..0}},
ordering.                                             with accuracy 90.32%. The best for subtree ordering is