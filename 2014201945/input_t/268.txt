                               A Portfolio Approach to Algorithm Select 

Kevin Leyton-Brown and Eugene Nudelman and Galen Andrew and Jim McFadden and Yoav Shoham 
             {kevinlb;eugnud;galand;jmcf;shoham}@cs.stanford.edu, Stanford University, Stanford CA 94305 

     1 Introduction 
    Although some algorithms are better than others on average, 
    there is rarely a best algorithm for a given problem. Instead, it 
    is often the case that different algorithms perform well on dif•
    ferent problem instances. Not surprisingly, this phenomenon 
    is most pronounced among algorithms for solving AA'P-Hard 
    problems, because runtimes for these algorithms are often 
    highly variable from instance to instance. When algorithms 
    exhibit high runtime variance, one is faced with the problem 
    of deciding which algorithm to use; in 1976 Rice dubbed this 
    the "algorithm selection problem" [8]. In the nearly three          Figure 1: Algorithm and Portfolio Runtimes 
    decades that have followed, the issue of algorithm selection 
    has failed to receive widespread attention, though of course 
    some excellent work does exist. By far, the most common 
    approach to algorithm selection has been to measure different 
    algorithms' performance on a given problem distribution, and 
    then to use only the algorithm having the lowest average run•
    time. This "winner-take-aH" approach has driven recent ad•
    vances in algorithm design and refinement, but has resulted in 
    the neglect of many algorithms that, while uncompetitive on 
    average, may offer excellent performance on particular prob•      Figure 2: Optimal Figure 3: Selected 
    lem instances. Our consideration of the algorithm selection 
    literature, and our dissatisfaction with the winner-take-all ap• 1. Use domain knowledge to select features of problem in•
    proach, has led us to ask the following two questions. First,   stances that might be indicative of runtime. 
    what general techniques can we use to perform per-instance 
    (rather than per-distribution) algorithm selection? Second,   2. Generate a set of problem instances from the given dis•
    once we have rejected the notion of winner-take-all algorithm   tribution, and collect runtime data for the algorithm on 
    evaluation, how ought novel algorithms to be evaluated? We      each instance. 
    offer the following answers:                                  3. Use regression to learn a real-valued function of the fea•
                                                                    tures that predicts runtime. 
      1. Algorithms with high average running times can be com•
         bined to form an algorithm portfolio with low average    Given this existing technique for predicting runtime, we 
         running time when the algorithms' easy inputs are suffi• now propose building portfolios of multiple algorithms as fol•
         ciently uncorrected.                                   lows: 
      2. New algorithm design should focus on problems on         1. Train a model for each algorithm, as described above. 
         which the current algorithm portfolio performs poorly.   2. Given an instance: 
      Readers familiar with the boosting paradigm in machine         (a) Compute feature values 
    learning [9] will recognize that boosting uses similar ideas:    (b) Predict each algorithm's running time 
    combining weak classifiers into a much stronger ensemble 
                                                                     (c) Run the algorithm predicted to be fastest 
    by iterativcly training new classifiers on data on which the 
    ensemble performs poorly.                                   2.1 WDP Case Study: Past Work 
                                                                Our case study is based on the data collected in our previous 
    2 Algorithm Portfolios                                      work [6]. In that work we have constructed models for pre•
    In our previous work [6] we demonstrated that statistical re• dicting the running time of the CPLEX solver on the winner 
    gression can be used to learn surprisingly accurate algorithm- determination problem (WDP), which is an NP-Complete 
    specific models of the empirical hardness of given distribu• combinatorial optimization problem formally equivalent to 
    tions of problem instances. In short, the method proposed in weighted set-packing. We have also created models for two 
    that work is:                                               other WDP algorithms: GL (Gonen-Lehmann) [3], a simple 


    1542 branch-and-bound algorithm with CPLEX's LP solver as its 
heuristic, and CASS [1], a more complex branch-and-bound 
algorithm with a non-LP heuristic. The data set consists of 
4500 instances of a fixed size (256 goods and 1000 non-
dominated bids), sampled uniformly from CATS [7] instance 
generator. Since our methodology relies on machine learning, 
we split the data into training, validation, and test sets. We re•
port our portfolio runtimes only on the test set that was never 
used to train or evaluate models. Runtime data was collected 
on 550 MHz Pentium Xeons, running Linux 2.2.                            4 Conclusions 
                                                                        Learned runtime models may be used to create algorithm 
2.2 WDP Case Study: Portfolios 
                                                                        portfolios that outperform their constituent algorithms. These 
                                                                        models can also be used to induce harder benchmark distri•
We now describe new results. Fig. 1 compares the average 
                                                                        butions for use in the development and evaluation of new al•
runtimes of our three algorithms to that of the portfolio. Note 
                                                                        gorithms. Our case study on combinatorial auctions demon•
that CPLEX would be chosen under winner-take-all algo•
                                                                        strates that a portfolio composed of CPLEX and two older-
rithm selection. The "optimal" bar shows the performance of 
                                                                        and generally much slower—algorithms outperforms CPLEX 
an ideal portfolio where algorithm selection is performed per•
                                                                        alone by about a factor of 3. In the full version of this paper 
fectly and with no overhead. The portfolio bar shows the time 
                                                                        we describe our methodology in more detail, and also: 
taken to compute features (light portion) and the time taken 
to run the selected algorithm (dark portion). Despite the fact             • Show how to reduce the time spent computing features 
that CASS and GL are much slower than CPLEX on average,                       without substantially degrading portfolio performance; 
the portfolio outperforms CPLEX by more than a factor of 3.                • Demonstrate ways of using response variable transfor•
Further, neglecting the cost of computing features, our port•                 mations to focus portfolios on metrics other than average 
folio's selections take only 5% longer to run than the optimal                running time; 
selections. Figs. 2 and 3 show the frequency with which each 
algorithm is selected in the ideal portfolio and in our portfo•            • Explain how to induce distributions with characteristics 
lio. They illustrate the quality of our algorithm selection and               other than hardness (e.g. realism); 
the relative value of the three algorithms. While our portfo•              • Compare our approach to previous work that executes 
lio does not always make the right choice, most of the mis•                   algorithms in parallel [2]; uses classification instead of 
takes occur when algorithms have very similar running times.                  regression [4]; or considers the problem as an MDP [5]. 
These mistakes are not very costly, which explains why our 
portfolio's choices have a running time so close to optimal.            References 
These results show that our portfolio methodology can work 
                                                                        [1] Y. Fujishima, K. Leyton-Brown, and Y. Shoham. Taming the 
very well even with a small number of algorithms, and even 
                                                                             computational complexity of combinatorial auctions: Optimal 
when one algorithm has superior average performance.                         and approximate approaches. In IJCAI, 1999. 
                                                                        [2] C. Gomes and B. Selman. Algorithm portfolios. Artificial Intel•
3 Focused Algorithm Design                                                   ligence, \26(]-2):43 62,2001. 
                                                                        [3] R. Gonen and D. Lehmann. Linear programming helps solv•
Once it has been decided to select algorithms from a portfo•                 ing large multi-unit combinatorial auctions. Technical Report 
                                                                            TR-2001-8, Leibniz Center for Research in Computer Science, 
lio on a per-instance basis, it is necessary to reexamine the 
                                                                             April 2001. 
way we design and evaluate algorithms. Since the purpose 
of designing new algorithms is to reduce the time that it will          |4] E. Horvitz, Y. Ruan, C. Gomes, H. Kautz, B. Selman, and 
take to solve problems, designers should aim to produce new                 M. Chickering. A Bayesian approach to tackling hard computa•
                                                                             tional problems. In UAI, 2001. 
algorithms that complement an existing portfolio given a dis•
tribution D reflecting problems that will be encountered in             [5] M. Lagoudakis and M. Littman. Algorithm selection using re•
practice. The instances with the greatest potential for im•                  inforcement learning. In ICML, 2000. 
provement will be hard for the portfolio, common in D, or               [6] K. Leyton-Brown, E. Nudelman, and Y. Shoham. Learning the 
both. The full version of this paper describes a technique for              empirical hardness of optimization problems: The case of com•
using rejection sampling to automatically generate such in•                 binatorial auctions. In CP, 2002. 
stances. In Figures 4 and 5 we show how our techniques are              [7] K. Leyton-Brown, M. Pearson, and Y. Shoham. Towards a uni•
able to automatically skew two of the easiest CATS instance                 versal test suite for combinatorial auction algorithms. In ACM 
distributions towards harder regions. In fact, for these two                EC, 2000. 
distributions we generated instances that were (respectively)           [8] J. R. Rice. The algorithm selection problem. Advances in Com-
100 and 50 times harder than anything we had previously                     puters, 15:65-118,1976. 
seen! Moreover, the average runtime for the new distribu•
                                                                        [9] R. Schapire. The strength of weak learnability. Machine Learn-
tions was greater than the observed maximum running time                    wg, 5:197 227, 1990. 
on the original distribution. 


POSTER PAPERS                                                                                                                         1543 