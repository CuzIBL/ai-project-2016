                  Case Base Adaptation Using Solution-Space Metrics* 

             Brian Knight                                                  Fei Ling Woon 
        University of Greenwich                                     Tunku Abdul Rahman College 
     School of Computing & Mathematical Sciences                        School of Arts & Science 
      London, SE10 9LS, UK                                             Kuala Lumpur, Malaysia 
           b.knight@gre.ac.uk                                              f.woon@gre.ac.uk 

                    Abstract 
    In this paper we propose a generalisation of the 2 An Error Function 
    k-nearest neighbour (k-NN) retrieval method      The GSNN method will be applied to a general class of 
    based on an error function using distance metrics problem and solution domains. A distance metric 
    in the solution and problem space. It is an inter•       is here defined on the problem domain X and 
    polate method which is proposed to be effec•              on the solution domain Y. For the problem 
    tive for sparse case bases. The method applies   space, the term in the Shepard's method [1968] 
    equally to nominal, continuous and mixed do•     is generalised to over X. For the solution space 
    mains, and does not depend upon an embedding               is used where is the value of y which 
    n-dimensional space. In continuous Euclidean     minimizes the error function: 
    problem domains, the method is shown to be a 
    generalisation of the Shepard's Interpolation 
    method. We term the retrieval algorithm the 
    Generalised Shepurd Nearest Neighbour 
                                                       That this is a minimum follows from the positive definite 
    (GSNN) method. A novel aspect of GSNN is 
    that it provides a general method for interpola• form:  
    tion over nominal solution domains. The per•       The function depends only upon the Euclidean dis•
    formance of the retrieval method is examined     tance over Y = R and X = In order to generalise the 
    with reference to the Iris classification problem, method completely, we propose the error function: 
    and to a simulated sparse nominal value test                                                  a) 
    problem. The introduction of a solution-space 
    metric is shown to out-perform conventional        Here, the set are the k nearest neighbours in 
    nearest neighbours methods on sparse case        the problem space to the point x. and are 
    bases.                                           distance on domains The retrieved value y is 
                                                     the value which minimizes the error function /. The 
1 Introduction                                       GSNN algorithm is given as follows: 
We present in this study a Case-Based Reasoning (CBR) 
retrieval method that utilises a distance metric imposed 
on solution space. The motivation for such a method is to 
extend a powerful interpolative method, already proven 3 Illustrative Example 
in the real domain, so that it applies equally in the do•
main of nominal values. Interpolative methods are well In this example we illustrate in detail how the method 
studied in the real domain, and can give good results works. We choose the Iris data set [Fisher, 1936]. The 
from relatively sparse datasets. However, no general in• problem space is X and x = is a point in X. 
terpolative method exists for nominal (discrete) solution The solution space Y= {setosa, versicolour, virginica}. 
domains.                                             For the problem space we define distance according to a 
                                                     weighted sum of attributes. For the Y space, we define 
                                                            by using the distances between cluster centres to 
                                                     represent the distance between the classes. These dis•
                                                     tances are shown in the following matrix: 

    * The support by the University of Greenwich and Tunku Ab•
    dul Rahman College (TARC) is acknowledged. 


POSTER PAPERS                                                                                         1347                     setosa versicolour virginica             regular case bases of equivalent size, whatever the value of 
           setosa 0 .35 .49                                  k. Once again, the results show that GSNN out-performed 
         versicolour .35 0 .18                               the other nearest neighbour methods. 
          virginica .49 .18 0                                [ Size   Methods     k=l      k=2       k=3      k=4 
We take two cases, one from setosa and one from virginica:    100     GSNN        734      663       653      678 
    Xj= (44 2.9, 1.4, 0.2), y,= setosa                                k-NN                 772       843      843 
    x2 = (7.2, 3.2, 6, 1.8), y2 = virginica                           DWNN                 733       737      739 
We take as target the versicolour iris:                       400     GSNN        573      506       511      492 
    x = (5.5, 2.3, 4, 1.3),y = ?                                      k-NN                 643       695      708 
                                                                      DWNN                 573       583      591 
Taking p=l and k = 2, the function l(y) is: 
                                                              900     GSNN        421      356       359      344 
                                                                      k-NN                 486       547      548 
                                                                      DWNN                 422       435      432 

   Since I (versicolour) is minimum, we take                Table 2. Errors in estimating a test set of 1000 targets, for 
as the estimated value. This example shows an advantage of   random case bases. 
the interpolation method in situations where cases are 
sparse, in that it can correctly predict nominal values not  5 Conclusion 
represented in the case base itself. 
                                                            In this paper, we have proposed a method for interpolation 
4 Test on a Simulated Case Base                             over nominal values. The method generalises the Shepard's 
                                                            interpolation method by expressing it in terms of the mini•
To examine how the GSNN method might work on real           mization of a function I(y). This function relies only on dis•
case bases, we simulated case bases of varying density and  tance metrics defined over problem and solution spaces. 
structure, and used the method to estimate simulated target The method has an advantage for CBR in that it is applica•
sets. As a basis for the simulation, we adapted the function ble to case bases with nominal values in the problem and 
used by Ramos and Enright [2001] (i.e. solution domain where no natural ordering exists. The ex•
                              to give 21 nominal values,    amples studied indicate that GSNN could be useful in CBR 
                                                            with a sparse set of cases, and particularly where the cases 
yi,...,y2i. These 21 nominal values inherited a distance met•
ric from the numeric values:                                can be organised. Tests show that GSNN is more efficient 
                                                            as a retrieval engine than other nearest neighbour methods. 
  Test 6.1 uses regularly spaced cases at various case densi•
                                                            The inclusion of a solution space metric in the GSNN tech•
ties. This might represent a well organised case base. Test nique could be useful in two areas of CBR: (i) The selection 
6.2 uses randomly selected cases, and is intended to repre• of an optimum case base, (ii) Case based model building, 
sent disorganised sparse case bases. Cases are con•         from experimental or numerical modeling exercises. Inves•
structed in the domain: over a regular square               tigations using numerical models indicate at GSNN would 
lattice, with 102,202,302 points.                           appear to be a promising approach for the construction of 
                                                            efficient case-based models. 
[ Size    Methods   k=l       k=2       k=3      k=4 
 100      GSNN      709       501       453      539        References 
          k-NN                769       799      786 
          DWNN                710       706      709        [Fisher, 1936] R. A. Fisher. The Use of Multiple Meas•
 400      GSNN      501       308       188      215        urements in Taxonomic Problems. Annual Eugenics, 7, 
          k-NN                616      684       652        Part II, 179-188(1936). 
          DWNN                499      478       488 
 900      GSNN      360       251       181      224         [Mitchell, 1997] T. Mitchell. Machine Learning. 
          k-NN                450       471      456        McGraw-Hill Series in Computer Science, 
          DWNN                360       344      344        WCB/McGraw-Hill, 230 - 247, USA 1997. 

Table 1. Errors in estimating a test set of 1000 targets, for [Ramos and Enright, 2001] G. A. Ramos and W. Enright. 
regular case bases. 
                                                            Interpolation of Surfaces over Scattered Data. Visualiza•
                                                            tion, Imaging and Image Processing Conference, VIIP02. 
Table 1 shows the result of Test 6.1. These results confirm 2001 IASTED 
that GSNN with k > 1 can out-perform both k-NN and 
DWNN [Mitchell, 1997] for case bases with regular struc•    [Shepard, 1968] D. Shepard. A Two-Dimensional Inter•
ture. Table 2 shows the results of Test 6.2. The results show polation Function for Irregularly Spaced Data. Proceed•
that more errors are recorded for random case bases than for ing of the 23rd National Conference,ACM,517-523, 1968. 


 1348                                                                                                 POSTER PAPERS 