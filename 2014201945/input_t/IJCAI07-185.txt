          Managing Domain Knowledge and Multiple Models with Boosting

                        Peng Zang                                  Charles Isbell
                   College of Computing                         College of Computing
                       Georgia Tech                                 Georgia Tech
                    Atlanta, GA 30332                            Atlanta, GA 30332
                pengzang@cc.gatech.edu                        isbell@cc.gatech.edu

                    Abstract                          2   MBoost
                                                      AdaBoost [Schapire and Singer, 1999; Freund and Schapire,
    We present MBoost, a novel extension to AdaBoost  1995] is an ensemble learning technique that iteratively con-
    that extends boosting to use multiple weak learners structs an ensemble of hypotheses by applying a weak learner
    explicitly, and provides robustness to learning mod- repeatedly on different distributions of data. Distributions are
    els that overﬁt or are poorly matched to data. We chosen to focus on the “hard” parts of the data space, that is,
    demonstrate MBoost on a variety of problems and   where the hypotheses generated thus far perform poorly. If
    compare it to cross validation for model selection.
                                                      h1,h2,...,hT is a set of hypotheses generated by AdaBoost,
                                                      the ﬁnal boosted hypothesis is then: H(x)=sign( f (x)) =
                                                      sign(∑T  α h (x)),whereα  denotes the weighting coefﬁ-
1  Introduction                                             t=1 t t           t
                                                      cient for ht.
Recent efforts in ensemble learning methods have shown em- Although described as a single learner, the weak learner
pirically that the use of many methods and models is more could be a “bag” of several learning models. For example,
powerful than using any single method alone. For example, rather than choosing between a decision tree and a perceptron
Caruana [2004] shows how constructing an ensemble classi- as the weak learner, one could use a learner that contained
ﬁer from a library of around 2000 models ranging from de- both, presumably choosing the better for any given iteration.
cision trees to SVMs produces classiﬁers that outperform the Using such a composite weak learner requires some atten-
best of any single model. Oddly, Boosting [Schapire, 1990]— tion to detail. Whether the bag chooses the best model or
a particularly popular ensemble technique with strong theo- combines the models in some way, it introduces additional
retical and empirical support—is rarely used in such a fash- inductive bias: one is no longer boosting over a decision tree
ion. In principle, Boosting should be able to handle multi- and a perceptron, but over a potentially complex ensemble
ple learning methods automatically simply by using a learner learner that happens to include a decision tree and perceptron.
that itself chooses among multiple weak learners. On the To ensure that no additional inductive bias is introduced, we
other hand, such a learner introduces additional complexity propose that the boosting algorithm itself should be the arbi-
and may compound overﬁtting effects.                  trator, acting as the mechanism for choosing which model to
  In this paper, we present a new boosting algorithm, use.
MBoost, that incorporates multiple models into Boosting ex- The MBoost algorithm explicitly supports multiple weak
plicitly. In particular, we extend AdaBoost [Schapire and learners and formalizes the notion of boosting as the arbitra-
Singer, 1999; Freund and Schapire, 1995] to act as an arbi- tor. In each round, each weak learner proposes a hypothesis
trator between multiple models. Our contributions are: and MBoost selects the “best” one. Here, “best” is deter-
                                                      mined by how boosting would reweight the distribution from
  • A principled technique for extending Boosting to enable round to round, allowing MBoost to arbitrate among the weak
    explicit use of multiple weak learners.           learners without introducing any inductive bias not already
  • A technique for controlling weak learners that overﬁt or intrinsic to the boosting framework.
    are poorly matched to the data (e.g., embody poor do- Boosting is known to be susceptible to overﬁtting when its
    main knowledge).                                  weak learner overﬁts. Imagine for example, that the weak
                                                      learner is a rote learner such as a hash table. The training er-
In the sections that follow we ﬁrst present MBoost itself, ror will be zero, but there will also be no generalization. No
comparing it to AdaBoost. We analyze the theoretical and matter how many rounds are run, the rote learner will gen-
practical consequences of MBoost, and validate our analysis erate the same hypothesis (assuming there is no noise in the
by presenting several empirical results, including a compar- labels). As a result, the ﬁnal Boosting classiﬁer would show
ison of MBoost to model selection via cross validation. Fi- the same (lack of) generalization capabilities. Using multiple
nally, we situate our work in the literature and conclude with weak learners can only compound this problem.
some discussion.                                        Boosting’s susceptibility to this kind of overﬁtting lies in

                                                IJCAI-07
                                                  1144Algorithm 1 MBoost                                    is that given the coordinate, AdaBoost is taking the largest
                                                      possible loss-minimizing step along that coordinate.
Require: Weak learners b1,...,bm
   Data (x ,y ),...,(x ,y ) where x ∈ χ,y ∈{−1,+1}      When Boosting is extended to multiple weak learners, not
         1  1      n  n        i    i                 only must the optimal weight be chosen, but the optimal hy-
 1. Initialize D0(i)=1/n
 2. for t = 1,...,T − 1 do                            pothesis (among a set of proposed hypotheses) must be cho-
 3.  Split the data randomly into two parts Dtrain and sen as well. In other words, we must ﬁnd the best coordinate
                                              t       (the one with the largest component in the direction of the
     Dvalt.
                                                      gradient) in addition to optimizing the loss-minimizing step.
 4.  Train learners b1 ...bn with Dtraint, generating hy-
                                                        Because we do not know our learning models apriori,we
     potheses h1 ...hn.
                                                      are precluded from analytical optimizations that ﬁnd the best
 5.  Choose hypothesis ht = argminZ (hi,...,hn)
                                t                     hypothesis. Instead, we perform an exhaustive search. For
 6.  Choose αt for ht per usual
                                −α   ( )              each hypothesis proposed by the different models, we ﬁnd its
 7.  Update: Dval + (i)=Dval (i)e t yiht xi
                 t 1        t                         optimal weight and calculate the resulting loss. The hypoth-
 8.              + =   + /           ∑   + ( )=
     Normalize Dt 1  Dt 1 Zt such that Dt 1 i   1     esis with the lowest loss is chosen. Because the number of
                                     i
 9. end for                                           weak learners is typically small and computation of the loss
                                     T                requires only hypothesis predictions and not training, the cost
10. return Final hypothesis: H(x)=sign(∑ αt ht (x))   of this step is low.
                                    t=1                 Applying this technique to AdaBoost, we see that the
      χ                                                                                            −  ( )
Where   is the input space and Zt is the normalization con-                             ( )=∑N      yi f xi
                                                      best hypothesis is the one minimizing G f i=1 e   ,
stant.                                                                           ( )=  T  α   ( )
                                                      where yi is the true label and f x ∑t=1 t ht x is the hy-
                                                      potheses ensemble classiﬁer. The cost of this minimization
part in its use of training error for hypothesis evaluation. grows linearly with the number of rounds run. Over many
Training error can be an overly-optimistic and poor measure rounds, it can become expensive. Fortunately, minimizing
of true performance. Rather than avoid “stronger” learners, G( f ) is equivalent to minimizing Zt , the normalization con-
MBoost divides the data every round into training and vali- stant in AdaBoost, in that round.
dation sets, using the latter to evaluate generated hypotheses.
This approach yields a more accurate measure of the hypothe- Proof. In any particular round t, the update rule gives us:
ses’ performance which in turn is used to choose the best hy-                    −  α ( )
                                                                       ( )=   ( ) yi t ht xi /
pothesis and its weight. Additionally, MBoost only reweights       Dt+1 i   Dt i e        Zt          (1)
                                                                             ∑ − α  ( )
the distribution on data from the validation set. Points from               e t yi t ht xi
the training set are suspect as their prediction may be due to            =                           (2)
                                                                              m∏t  Zt
overﬁtting causes. Reweighting these points may incorrectly                  −  ( )
                                                                            e yi f xi
imply that they have been learned. Reweighting only the val-              =                           (3)
idation set avoids making such a mistake. Note that the con-                 m∏t Zt
vergence of the algorithm does not change. All points will
                                                                                   ( )=
eventually be reweighted since points in the training set of Because Zt must normalize Dt+1 i 1, we have:
one round may very well be in the validation set of the next.                     − α  ( )
                                                                        =     ( )  yi tht xi
  The combination of these two techniques means MBoost                Zt  ∑Dt  i e                    (4)
has a much more accurate measure of the hypotheses’ gener-                 i
                                                                        =       ( )
alization errors. Further, by being “inside” the bag of compet-           ∑Dt+1  i Zt                 (5)
ing models, MBoost can choose the hypothesis that directly                 i
                                                                              −   ( )
minimizes the loss function. The net effect is that users can                 e yi f xi
                                                                        = ∑                           (6)
insert a variety of models into boosting to take advantage of                   t−1
                                                                             m∏  = Z j
the empirical power of using multiple models while mitigat-                i    j 1
ing the effects of poor weak learners that either overﬁt or em- Thus
body incorrect domain knowledge.
                                                                                 N
  Algorithm 1 shows pseudo-code for MBoost. We note that                            −  ( )
                                                           argmin(G( f )) = argmin(∑ e yi f xi )      (7)
while MBoost as shown is derived from AdaBoost, any boost-
                                                                                 i=1
ing scheme can be similarly adapted.
                                                                                     1       −  ( )
                                                                        = argmin(         ∑e  yi f xi ) (8)
                                                                                   ∏t−1
3  Analysis and Implications                                                     m   j=1 Z j i
                                                                        =       ( )
3.1  Using multiple weak learners                                         argmin Zt                   (9)
Boosting can be viewed as a coordinate gradient descent
method that searches for the set of hypotheses which, when
combined with their weights, minimizes some loss. In each We note that in the MBoost formulation, Zt is deﬁned
round of AdaBoost, a hypothesis ht (the coordinate) is given, somewhat differently than in AdaBoost. This is because
and AdaBoost optimizes the weight αt for that coordinate to MBoost’s use of a validation set changes the loss function;
minimize the loss function. An interpretation of this process however, the difference is slight and the proof still applies.

                                                IJCAI-07
                                                  1145  Finally, it is worth noting that because MBoost is choosing and validation sets are chosen randomly. To determine that
among multiple hypotheses, the weak learner requirement (in the learners are indeed “exhausted”, many consecutive rounds
the PAC sense) can be relaxed. The ensemble of learners must in which no hypothesis beats random is required.
act as a weak learner but no individual learner must.   We note that in these rounds, the distribution is static.
                                                      MBoost is essentially performing an internal Monte-Carlo
3.2  Using an internal validation set                 cross-validation on each of its weak learners. As we run more
MBoost needs an accurate error estimate for candidate hy- rounds, we acquire tighter bounds on each learner’s true error.
potheses each round. Following [Langford, 2005], we model Eventually, this will either show that at least one of the weak
hypothesis error as a biased coin ﬂip where the bias is the learners does better than random, in which case MBoost will
true error, the number of tails is the number of correct pre- continue, or that none of the weak learners can do better than
dictions and the number of heads is the number of incorrect random (with some conﬁdence), in which case MBoost halts.
predictions.
  The probability that one makes k or fewer mistakes in n 3.4 Error bounds
predictions given that the true error rate is ε is given by the MBoost can be approximately reduced to AdaBoost. To see
binomial  cumulative distribution function: CDF(k,n,ε)= this, consider the weak learner L which when trained on data
∑k   n εi( − ε)n−i                                    set D in round t does the following:
 i=0 i   1      .                                         t
  The largest true error rate r such that the probability of • Splits the data set into a training set and a validation set,
having k or fewer mistakes in those n predictions is at least Dtrain and Dval .
δ            (       ( , , ) ≥ δ)                               t        t
 , is then: maxr r : CDF k n r  . We will refer to this •                                 ...
                                    ( , ,δ)               Trains each of its member learners b1 bm on Dtraint,
as the maximum reasonable true error: mrte k n . A bound                       ...
on the true error can then be formed as: P(ε ≤ mrte(k,n,δ)) ≥ generating hypotheses h1 hm.
1 − δ.                                                  • Simulates for each hypothesis the boosting reweighting
  In MBoost, a hypothesis is only used if its mrte(k,n,δ) is on Dvalt and chooses the hypothesis hbest,t with the low-
less than 0.5. That is, a hypothesis is only used if the upper est hypothetical normalization constant Z.
bound of its true error is less than 0.5, suggesting with high •   ( )=     ( ) ( ∈/      )        ( ) ∈
                                                          Returns ht x   hbest,t x I x Dtraint ,whereI p
conﬁdence that the hypothesis is better than random.      0,1 is the indicator function that returns 1 when the pred-
  Recall that we can view Boosting as a coordinate gradient icate p is true, and 0 otherwise. In other words, return
descent method optimizing a particular loss function. Use of ( )              ( )
                                                          ht x as a copy of hbest,t x except that when asked to
the validation set for hypothesis selection, weighting and dis- predict on data it has been trained on it abstains.
tribution reweighting essentially amounts to a different loss
             −  ∑  ( ) ( ∈/  )                          Applying AdaBoost to the weak learner L, we con-
function: ∑N e yi t ht x I xi Dtraint . MBoost’s loss function
          i=1                                         struct a learner L . This is essentially MBoost. The
reﬂects our understanding that the real goal of interest is the       ada
                                                      sole difference is that MBoost’s ﬁnal hypothesis is H(x)=
ensemble’s generalization error. We want to avoid being mis- ( T α   ( ))          ( )=    (  T α  ( ))
                                                      sign ∑t=1 t hbest,t x instead of H x sign ∑t=1 t ht x ;
led by atypical results on training data. Use of the test set for ( )        ( )
evaluation allows MBoost to effectively apply weak learners however, ht x and hbest,t x differ only in predictions
prone to overﬁtting (as a “strong” learner may do).   on a randomly-drawn ﬁnite set, namely, Dtraint. Thus,
                                                      H(x) and H(x) can differ only on points in the union of
  One could easily imagine extending the use of a single in- ...
ternal validation set to perform full 10-fold cross-validation Dtrain1 DtrainT , a subset of the overall training data D.In
per round of boosting. We note however, that such a treat- the limit, as the size of the instance space approaches inﬁn-
ment aims towards ﬁnding the best weak learner, not the best ity, the probability of predicting on a data point seen in train-
                                                      ing is vanishingly small. More formally, lim Pr(x ∈ D)=0
hypothesis. In building an ensemble of hypotheses, our goal                             |χ|→∞
is to ﬁnd the best hypothesis per round.              and therefore lim |H(x)−H(x)| = 0whereχ is the instance
  Finally, we note that internal validation is also mildly help-  |χ|→∞
ful when overﬁtting is due to noise. Such error is a side effect space and x is a point randomly drawn from χ.Further:
of boosting’s emphasis on “hard” examples, or in noisy situa- • The sum of all errors is bounded by the size of the train-
                                                                      
tions, noise. If noise is non-systematic, MBoost should deter- ing set: ∑ |H (x) − H(x)| < |D|.
mine that the generated hypotheses cannot predict the noise      x∈χ
better than random. As we note in the next section, this will • ( ) ( )
cause MBoost to halt.                                     H  x  and H x differ only on a randomly drawn, ﬁnite
                                                          set, and the errors |H(x) − H(x)|, are not systematic.
                                                                                            
3.3  Automatic stopping condition                         In other words, ∑|H (x) − H(x)|≈0. H (x) and H(x)
It is natural for MBoost to stop when none of its weak learners represent the same decision boundary.
                                                          
perform better than random. Standard boosting uses training H (x) ≈ H(x) and thus, Lada is approximately MBoost.
data for evaluation so it may be easier for weak learners to This reduction now allows us to inherit the generalization
perform better than random than in MBoost. In running our error bounds that have been derived for AdaBoost. In par-
experiments, MBoost usually stops due to this condition. ticular, both the VC-theory based generalization error bound
  For MBoost to determine that none of its weak learners found in [Freund and Schapire, 1995],aswellasthemargin-
can perform better than random, a single round in which no based generalization bounds in [Schapire et al., 1997] and
generated hypotheses perform is insufﬁcient as the training [Schapire and Singer, 1999] can be applied straightforwardly.

                                                IJCAI-07
                                                  11464  Empirical evaluation
                                                           Table 1: CV Accuracy for MBoost and BestAda
We performed four sets of experiments to examine MBoost’s Dataset/Learner  MBoost10   BestAda  p-value
performance.
                                                       ADULT               0.837      0.765    0.0000
4.1  Effects of domain knowledge                       BREAST-CANCER       0.751      0.706    2.12e-04
                                                       CRX                 0.874      0.809    0.0000
This set of experiments focus on exploring MBoost’s basic
                                                       HORSE-COLIC         0.816      0.777    0.0016
ability to exploit multiple domain knowledge sets, as encoded
                                                       IONOSPHERE          0.947      0.883    2.89e-12
by the weak learners; the validation set is not used. We used
a 2D synthetic dataset composed of three Gaussians whose
points are labeled according to three different lines, one for
each Gaussian. Two weak learners were used. The ﬁrst is a Table 2: CV Accuracy for MBoost10 and BestCV-Ada
simple perceptron. The second is a Gaussian estimator com- Dataset/Learner MBoost10 BestCV-Ada p-value
bined with a perceptron. This represents the accurate domain ADULT      0.837      0.812       0.0038
knowledge case.                                        B-CANCER   2     0.751      0.731       0.0213
  As we can see in Figures 1a and 1b, this is an easy dataset CRX 2     0.874      0.861       0.0323
and both AdaBoost and MBoost are able to approach perfect HORSE-COLIC   0.816      0.818       0.9601
classiﬁcation. MBoost, however, by leveraging the additional IONOSPHERE 0.947      0.946       0.6145
domain knowledge in the second weak learner, is able to do
so ﬁve times faster.
  To explore the effect of inaccurate domain knowledge, we each of the weak learners in turn (for a total of twenty-ﬁve
replace the second weak learner with one that learns vertical runs). The best AdaBoosted model (according to their re-
bands of width 0.1. This hypothesis class can yield no help- ported training error) is chosen and used. We call this second
ful hypotheses, simulating poor domain knowledge. In this learner BestAda. Note that the computational complexity of
case, MBoost discards the hypotheses generated by the sec- the two learners are equivalent, they are O(nm) where n is the
ond weak learner. MBoost and AdaBoost perform identically number of rounds and m is the number of weak learners.
from round to round.                                    Table 1 shows the accuracy results on the ﬁve datasets. All
                                                      reported accuracy scores use subsample cross-validation 50
4.2  MBoost versus AdaBoost on individual             times; that is, in each cross validation round we randomly
     learners                                         split the data into training and validation sets with 90% in the
In this set of experiments, we explore the effect of boost- training set. We use this variation of cross validation through-
ing multiple models together, versus boosting each model in- out our evaluation because it can be performed any number of
dividually. We want to determine if heterogeneous ensem- times. We ﬁnd that 50 rounds typically produce enough sam-
bles are superior to homogeneous ones. We performed the ples for our signiﬁcance tests.
experiments on ﬁve of the largest binary classiﬁcation UCI We used one-way ANOVA  tests to determine if the
                                                                                                       =
datasets1. A set of twenty-ﬁve weak learners were used: reported accuracies are statistically signiﬁcant, pvalue
                                                      F(1,98). All datasets show MBoost performing statistically
  • Five Naive Bayes learners. One uses empirical probabil- signiﬁcantly better.
    ities, the other four use m-estimates with m at 4, 16, 64, These results are quite surprising. Detailed analysis shows
    and 256. We used the Naive Bayes learner provided in that BestAda suffered from signiﬁcant overﬁtting. For exam-
    the Orange data mining package.                   ple, AdaBoosted SVM would report a higher accuracy rate
  • Five kNN learners with k at 1, 4, 16, 64, and 256. We than AdaBoosted C4.5 when in fact AdaBoosted C4.5 gener-
    used the kNN learner provided in the Orange data min- alized better.
    ing package.                                        One might imagine using cross-validation to avoid this
                                                      problem. To this end, we create a third learner, BestCV-Ada,
  • Five C4.5 decision tree learners with minOb js, the min-
                                                      that is the same as BestAda but uses ten-fold cross validation
    imum number of examples in a leaf, set at 1, 4, 16, 64
                                                      to select the best AdaBoosted weak learner. Note that this in-
    and 256. All other options were set to defaults. We used
                                                      creases the computational cost of BestCV-Ada by a factor 10.
    R8 of Quinlan’s implementation.
                                                      We present results comparing BestCV-Ada and MBoost10 in
  • Ten SVM learners with C, the parameter for the cost of Table 2.
    misclassiﬁcation, set at 0.125, 0.5, 2, 8, 32, 128, 512, These results are in line with expectations. As commonly
    2048, 8192, and 32768. The SVMs used a RFB kernel. reported in the literature, the best single-model booster is
    All other options were set to defaults. We used the lib- quite effective. Nevertheless, we see that MBoost, using one
    svm library by Chih-Chung Chang and Chih-Jen Lin. tenth of the computational cost, can perform better on three
We compared two learners. The ﬁrst, MBoost10, is given all of the ﬁve data sets and as good on the rest. Heterogeneous
twenty-ﬁve weak learners and set to run for ten rounds. The ensembles appear to perform better than homogeneous ones.
second is a learner that runs AdaBoost for 10 rounds with MBoost is able to take advantage of this effect and combine
  1The full ADULT data set is quite large, we used a subsample of 2Additional CV rounds were performed to ascertain statistical
1000 examples.                                        signiﬁcance.

                                                IJCAI-07
                                                  1147                            (a)                                           (b)

Figure 1: Comparing AdaBoost and MBoost on Gaussian dataset. Left shows training accuracy and theoretical bound, right
shows training and testing accuracy. Averaged over 50 runs


    Table 3: CV Accuracy for MBoost and MBoostAcc        Table 4: CV Accuracy for MBoost-10 and BestCV-Ind
 Dataset/Learner MBoost   MBoostAcc   p-value          Dataset/Learner  MBoost10   BestCV-Ind  p-value
 ADULT           0.837    0.836       0.8685           ADULT            0.837      0.833       0.5162
 B-CANCER        0.751    0.739       0.1752           B-CANCER         0.751      0.745       0.5233
 CRX             0.874    0.876       0.8494           CRX  2           0.874      0.862       0.0326
 HORSE-COLIC     0.816    0.815       0.8867           HORSE-COLIC      0.816      0.813       0.7761
 IONOSPHERE      0.947    0.945       0.7399           IONOSPHERE       0.947      0.947       0.9294

the ensembles appropriately.                             Table 5: CV Accuracy for MBoost10 and MBoostAuto
                                                       Dataset/Learner  MBoost10   MBoostAuto  p-value
4.3  MBoost as weak learner arbitrator                 ADULT            0.837      0.838       0.8142
Here, we perform a set of experiments to explore the effects B-CANCER   0.751      0.750       0.9362
of using MBoost’s loss function as a weak learner arbitrator. CRX       0.874      0.876       0.7147
We compare two versions of MBoost. The ﬁrst is standard HORSE-COLIC     0.816      0.818       0.8454
MBoost using its exponential loss function as the arbitrator, IONOSPHERE 0.947     0.949       0.7518
the second, MBoostAcc using validation accuracy of the weak
learners. Both boosters are run for 10 rounds.
  Table 3 shows that the two versions’ performance are quite individual model via 10-fold cross validation. MBoost-Auto
similar. Only one dataset shows any difference that might be typically requires two to ﬁve times the computational cost.
statistically signiﬁcant with MBoost outperforming MBoost- Table 4 shows MBoost10 performing better on one of the
Acc. In further exploration, we ran the same experiment but ﬁve datasets and equivalently on the rest. Further, as we can
allowed both versions to run until exhaustion. In this experi- see in Table 5, the accuracy scores do not degrade even as
ment, all performance differences disappeared. This is unsur- we run MBoost to exhaustion. MBoost is quite resistant to
prising as the performance difference should be in terms of overﬁtting. These factors combined with MBoost’s low com-
speed of convergence, not ﬁnal asymptomatic performance. putational complexity lead us to suggest it as an alternative to
The exponential loss function is more aggressive, so it is rea- cross-validation for model selection. Instead of trying many
sonable that it might show some early gains.          models and ﬁnding the best, we suggest Boosting the models
                                                      together and synthesizing an ensemble whole.
4.4  MBoost versus the best individual learners
Here we perform a series of experiments comparing MBoost 5 Related Work
with the best individual model as selected via 10 fold cross MBoost’s use of multiple weak learners is similar to a boost-
validation (BestCV-Ind). We ran two versions of MBoost, ing extension known as localized or dynamic boosting ([Mo-
one running for 10 rounds and one with the automatic stop- erland and Mayoraz, 1999; Meir et al., 2000; Avnimelech and
ping condition, though never more than 50 rounds. MBoost10 Intrator, 1999; Maclin, 1998])whereαt —the weight on the
has the same computational complexity as selecting the best hypothesis in round t–is generalized to be a function depen-

                                                IJCAI-07
                                                  1148