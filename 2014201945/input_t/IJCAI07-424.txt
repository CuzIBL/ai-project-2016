       Towards Efﬁcient Computation of Error Bounded Solutions in POMDPs:
            Expected Value Approximation and Dynamic Disjunctive Beliefs
        Pradeep Varakantham†, Rajiv T. Maheswaran‡, Tapana Gupta†           and Milind Tambe†
         † {varakant, tapanagu, tambe}@usc.edu, Institute for Robotic and Intelligent Systems,
                                    University of Southern California
        ‡ {maheswar}@isi.edu, Information Sciences Institute, University of Southern California

                    Abstract                            Previous work in POMDPs has made encouraging progress
                                                      using two approaches: (1) Exact algorithms compute the op-
    While POMDPs (partially observable markov deci-   timal solution and thus avoid problems with error bounds
    sion problems) are a popular computational model  [Cassandra et al., 1997; Feng and Zilberstein, 2004; 2005],
    with wide-ranging applications, the computational however, they do not scale. (2) Approximate algorithms, a
    cost for optimal policy generation is prohibitive. currently popular approach, address the signiﬁcant computa-
    Researchers are investigating ever-more efﬁcient  tional complexity in POMDP policy generation by sacriﬁc-
    algorithms, yet many applications demand such al- ing solution quality for speed [Pineau et al., 2003; Smith and
    gorithms bound any loss in policy quality when    Simmons, 2005; Poupart and Boutilier, 2004]. While some
    chasing efﬁciency. To address this challenge, we  approximate algorithms (point-based approaches [Pineau et
    present two new techniques. The ﬁrst approximates al., 2003; Smith and Simmons, 2005]) provide expressions
    in the value space to obtain solutions efﬁciently for for error bounds, they have many drawbacks: (a) The bounds
    a pre-speciﬁed error bound. Unlike existing tech- in these point-based approximation algorithms are based on
    niques, our technique guarantees the resulting pol- maximum and minimum possible single-stage reward, which
    icy will meet this bound. Furthermore, it does not can take extreme values, leading to a very loose bound which
    require costly computations to determine the qual- is not useful in many domains, especially in those that have
    ity loss of the policy. Our second technique prunes penalties (e.g., Rmin  0). (b) The computation of the
    large tracts of belief space that are unreachable, al- bound for a particular policy is itself a potentially signiﬁcant
    lowing faster policy computation without any sacri- expense (e.g., requiring a non-linear program). (c) The algo-
    ﬁce in optimality. The combination of the two tech- rithms cannot guarantee that they will yield a policy that can
    niques, which are complementary to existing op-   achieve a pre-speciﬁed error bound.
    timal policy generation algorithms, provides solu-
                                                        To address these challenges, we present two new tech-
    tions with tight error bounds efﬁciently in domains
                                                      niques that are complementary to existing optimal policy
    where competing algorithms fail to provide such
                                                      generation algorithms. In particular, both are presented as
    tight bounds.1
                                                      wrappers to Generalized Incremental Pruning (GIP) [Cas-
                                                      sandra et al., 1997] and Region-Based Incremental Pruning
1  Introduction                                       (RBIP) [Feng and Zilberstein, 2004; 2005]. Our ﬁrst tech-
                                                      nique EVA (Expected Value Approximation) is an approxi-
Partially observable markov decision problems (POMDPs) mate one that sacriﬁces quality for speed, but operates within
are a popular model with potential applications in health a pre-speciﬁed error bound. EVA differs from existing ap-
care, disaster rescue and space exploration [Hauskrecht and proaches (primarily point-based approaches [Pineau et al.,
Fraser, 2000; Pineau et al., 2003; Poupart and Boutilier, 2004; 2003; Smith and Simmons, 2005]), by approximating directly
Nair et al., 2003]. Unfortunately, the computational cost in the value space as opposed to sampling in the belief space
for optimal policy generation in POMDPs is prohibitive, re- for an indirect value approximation. EVA alleviates the three
quiring increasingly efﬁcient algorithms to address problems key problems mentioned earlier: (a) EVA provides an error
in these large-scale domains. Furthermore, as we apply bound that does not depend on the maximum and minimum
POMDPs in domains where property and human safety are possible reward. (b) The computation of the error bound in
of concern, e.g. disaster rescue, patient care and space ex- itself imposes no overhead when computing policies in EVA.
ploration, error bounds (limiting POMDP policy quality loss) (c) Given any pre-speciﬁed error bound, EVA will generate a
become critical.                                      policy whose quality meets or exceeds that bound.
  1This material is based upon work supported by the Defense Ad- While results from EVA are promising, in domains that re-
vanced Research Projects Agency (DARPA), through the Depart- quire tight error bounds, EVA alone may not be sufﬁciently
ment of the Interior, NBC, Acquisition Services Division, under fast. Our second technique, DDB (Dynamic Disjunctive Be-
Contract No. NBCHD030010.                             liefs), complements EVA. DDB exploits domain structure to

                                                IJCAI-07
                                                  2638prune large tracts of belief space that are unreachable. Be- over the entire belief space, and the heuristics employed
cause DDB does not sacriﬁce quality, combining it with EVA for belief set expansion differentiate PBVI and HSVI2. We
yields the same error bounds as the original EVA. We il- compare EVA against PBVI, as it is known to be one of
lustrate how DDB combined with EVA provides signiﬁcant the fastest approximate algorithms. The error bound pro-
                                                                                                2
performance improvements over EVA alone, and over earlier vided by PBVI is: (Rmax − Rmin) ∗ b/(1 − γ) , with b
                                                                             
techniques that prune belief spaces, such as [Varakantham et = maxb∈Δminb∈Bb − b 1, where Δ is the entire belief
al., 2005].                                           space; B is the set of expanded belief points; γ is the dis-
                                                      count factor; Rmax and Rmin are the extremal single stage
2  Background : POMDPs                                rewards. Computing b requires solving a Non-Linear Pro-
                                                               2
A   POMDP    can  be   represented using the  tuple   gram, NLP (shown in Algorithm 1). While we provide exper-
S, A, P, Ω,O,R, where S is the set of states; A is  imental comparisons with only PBVI, the drawbacks of PBVI
the set of actions; Ω is the set of observations; P (s, a, s) is with regard to calculating and achieving bounds are equally
the transition function; O(s,a,ω) denotes the observation applicable to HSVI2 due to their similarity.
model; R(s, a) is the reward function. As the state is
partially observable, POMDP policies are expressed as Algorithm 1 Non-Linear Program to obtain b
belief-state-to-action mappings, where a belief state b is a Maximize b
probability distribution over the set of states S. The goal is subject to the constraints
to compute the policy with maximum expected reward.          Σ1≤i≤|S|b[i]=1
  Currently, the most efﬁcient exact algorithms for POMDPs   b[i] ≥ 0 and b[i] ≤ 1, ∀i ∈{1,...,|S|}
are value-iteration algorithms, speciﬁcally GIP [Cassandra et b < Σ1≤i≤|S| |b[i] − bk[i]| , ∀bk ∈B
al., 1997] and RBIP [Feng and Zilberstein, 2004]. These
are dynamic-programming algorithms, where at each itera-
tion the value function and optimal policies are represented
by a minimal set of vectors called the parsimonious set. De- 3 Expected Value Approximation (EVA)
termining the parsimonious set involves pruning out domi- The value function in a POMDP is piecewise linear and can
nated vectors. This requires solving a linear program (LP) be expressed by a set of vectors. Approximate algorithms
for all vectors in the original set. Thus, pruning is recognized generate fewer vector sets than the optimal algorithms. Ex-
to be computationally expensive.                      isting approaches generate these reduced vector sets by sam-
  DB : The performance of these dynamic programming al- pling the belief space and ﬁnding the vectors that apply only
gorithms can be improved considerably by exploiting the dy-
namics of the domain [Varakantham et al., 2005]. Dynamic at these points. In our approach, Expected Value Approxima-
Beliefs, DB (implemented as a wrapper to GIP) eliminates tion (EVA), we choose a reduced set of vectors by approx-
unreachable belief spaces, given information about the start- imating the value space with a subset of vectors whose ex-
ing belief space, B0. It computes belief bounds over states pected reward will be within a desired bound of the optimal
at future time steps by solving maximization and minimiza- reward.
tion problems on the equation for belief probability updates. Using an approximation (subset) of the optimal parsimo-
Given an action a and observation ω, the maximization prob- nious set will lead to lower expected quality at some set of
                            s
lem for belief probability of state t is:             belief points. Let  denote the maximum loss in quality we

           Ot(st,a,ω)Σst−1 Pt−1(st−1,a,st)bt−1(st−1)  will allow at any belief point. We henceforth refer to any
   max   P                                                                  
 bt−1∈Bt−1   Ot(st,a,ω)Σs  Pt−1(st−1,a,st)bt−1(st−1)  vector set that is at most away from the optimal value at
           st            t−1
                                                      all points in the belief space (as illustated in Fig 1) as an -
Lagrangian techniques are used to solve these maximization
                                                      parsimonious set. The key problem in EVA is to determine
problems efﬁciently in polynomial time. These bounds can
                                                      this -parsimonious set efﬁciently.
signiﬁcantly improve LP run-times, as the pruning via LPs
                                                        To that end, we employ a heuristic that extends the prun-
happens over a restricted belief space. The restricted belief
                                                      ing strategies presented in GIP. In GIP, a parsimonious set V
space also leads to fewer vectors in the parsimonious set for
                                                      corresponding to a set of vectors, U is obtained in three steps:
a particular interation. This cascades through the steps of the
dynamic programming leading to fewer LP calls and conse- 1. Initialize the parsimonious set V with the dominant vec-
quently, signiﬁcant speedups. DB can lead to orders of mag- tors at the simplex points.
nitude improvement over existing algorithms [Varakantham 2. For some chosen vector u ∈U, execute a LP to compute
et al., 2005].                                            the belief point b where u dominates the current parsi-
  Point-Based Approaches : Two leading approximate al-    monious set V.
gorithms for POMDPs that provide error bounds are Point-                   u
                           [                  ]         3. Compute the vector with highest expected value in the
Based Value Iteration (PBVI) Pineau et al., 2003 and         U                 b             u     U
                                    [                     set  at the belief point, ; remove vector from and
Heuristic-Search Value Iteration (HSVI2) Smith and Sim-           V
mons, 2005]. In these algorithms, a policy computed for a add it to .
sampled set of belief points is extrapolated to the entire belief 2An LP formulation with non-convex feasible region is possible,
space. PBVI/HSVI2 are anytime algorithms where the sam- but it will have O(2|S|) number of linear constraints. The smallest of
pled set of belief points is expanded over time. The expan- our problems has 60 states (∼ 1018 constraints), making it infeasible
sion ensures that the belief points are uniformly distributed to solve such an LP.

                                                IJCAI-07
                                                  2639                                                           ∗                 ∗                    ∗
  EVA  modiﬁes the ﬁrst two steps, to obtain the -     2. vb ·ˆb ≥ vˆˆb ·ˆb +  and vb ·ˆb ≥ uˆ ·ˆb: This means vb would
                                                                                                  ∗    
parsimonious set:                                         have been included in the -parsimonious set, vb ∈V,
 1. Since we are interested in representing the optimal par- which is a contradiction.
                                                           ∗                  ∗
    simonious set with as few vectors as possible, the initial- 3. vb · ˆb ≥ vˆˆb · ˆb +  and vb · ˆb<uˆ · ˆb: uˆ will be included
                                                                   ∗
    ization process only selects one vector over the beliefs in in V and vb is returned to U to be considered again until
    the simplex extrema. We choose a vector with the high- one of previous two terminal conditions occur. 
    est expected value at the most number of simplex belief
    points, choosing randomly to break ties.
                                                      V(b)
 2. The LP is modiﬁed to check for -dominance, i.e., dom-                                ∗
    inating all other vectors by  at some belief point. Algo-                         V (b)
                                   max      min
    rithm 2 provides a modiﬁed LP with bt and bt .                                 ε
                                                                                       VECTOR OF OPTIMAL SET

                               max   min                                                 ε
Algorithm 2 LP-DOMINATE(w, U, bt   ,bt  ,)                                            V (b)
 1: variables: d, b(st) ∀st ∈ St
                                                                                        VECTOR OF APPROXIMATE SET
 2: maximize d                                                                   b
 3: subject to the constraints
 4:  b · (w − u) ≥ d + , ∀u ∈ U
           b s      b s  ≤ bmax s  b s  ≥ bmin s                                      
 5:  Σst∈St ( t)=1,  ( t)   t  ( t), ( t)  t  ( t)         Figure 1: EVA: An example of an -parsimonious set
 6: if d ≥ 0 return b else return nil
                                                      Proposition 2 The error introduced by EVA at each stage
  The key difference between the LP used in GIP and the of the policy computation, is bounded by 2|Ω| for GIP-type
                      
one in Algorithm 2 is the in RHS of line 4 which checks cross-sum pruning.
for expected value dominance of the given vector w over a
vector u ∈ U. Including  as part of the RHS constrains w to Proof. The EVA algorithm introduces an error of  in a par-
dominate other vectors by at least . In the following propo- simonious set whenever a pruning operation (PRUNE) is per-
sitions, we prove the correctness of the EVA algorithm and formed, due to Proposition 1. In GIP, there are three pruning
the error bound provided by EVA. Let V and V∗ denote the steps at each stage of policy computation.

 -parsimonious and optimal parsimonious set, respectively. 1. Va,o = PRUNE(Va,o,i): After this step, each Va,o is
                                               
Proposition 1 ∀b ∈ Δ, the entire belief space, if vb =    at most  away from optimal ∀a, ∀o.
                    ∗                ∗         
           v · b  v            ∗  ∗ v · b   v  · b
arg maxv ∈V     and  b =argmaxv   ∈V     , then b           a                           a,o1    a,o2
      ∗                                                 2. V  =  PRUNE(···(PRUNE(V          ⊕V     ) ··· ⊕
+  ≥ v · b.                                                a,o                 a,o1
      b                                                   V   |Ω| ): We begin with V which is away from opti-
Proof. We prove this by contradiction. Assume ∃b ∈ Δ such mal by at most . Each pruning operation adds 2 to the
             ∗                  ∗      ∗                                      a,oi
that vb ·b+<vb ·b. This implies vb = vb , and vb ∈V/ .We bound ( for the new term V and  for the PRUNE).
                              ∗                                                                     a,o
now consider the situation(s) when vb is considered by EVA. There are |Ω|−1 prune operations. Thus, each V is
At these instants, there will be a current parsimonious set V away from the optimal by at most 2(|Ω|−1) + .
                                                                                              
and a set of vectors still to be considered U. Let                             a                     a
                                                        3. V =  PRUNE(     a∈A V ): The error of a∈A V is
                          ∗                                                     a
         ˆb =argmax{min(vb · ˜b − v · ˜b)}                bounded by the error of V . The PRUNE adds , lead-
                ˜    v∈V
                b∈Δ                                       ing to a total one-stage error bound of 2|Ω|. 
be the belief point at which v∗ is best w.r.t. V. Let
                        b                             Proposition 3 The total error introduced by EVA (for GIP-
                                                      type cross-sum pruning) is bounded by | |T for a T -
               vˆˆb =argmaxv · ˆb                                                         2 Ω
                       v∈V                            horizon problem.
             V              ˆb                                            ∗
be the vector in which is best at . Let               Proof. Let Vt (b) and Vt (b) denote the EVA-policy and op-
                                                                                           t    A  ⊆  A,
               uˆˆ =argmaxu  · ˆb                     timal value function, respectively, at time .If t
                b      u∈U                            is the set of actions at the roots of all policy-trees asso-
                                                                  
                                                      ciated with Vt , the EVA vector set for time t and et =
be the vector in U which is best at ˆb. There are three possibil- ∗               
                                                      maxb∈B{V   (b) − V (b)}, then, V (b)=
ities:                                                          t      t           t−1
     ∗                           ∗                                                       
    v · ˆb<v  · ˆb             v · ˆb − v · ˆb<.      max     {R b, a      P  b |b, a V b }
 1.  b     ˆˆb  +  : This implies b    ˆˆb      By    =     a∈At   (  )+Σb   ∈B  (     ) t ( )
                                                                                        ∗  
    the deﬁnition of ˆb,wehave                        ≥max      {R b, a      P  b |b, a V b }
                                                            a∈At   (  )+Σb   ∈B  (     ) t ( )
            ∗             ∗                                        
           vb · b − vˆb · b<vb · ˆb − vˆˆb · ˆb<       − Σb∈BP (b |b, a)et
                                                                                        ∗  
         v               v · b.                         max     {R b, a      P  b |b, a V b }−e
    where ˆb =argmaxv∈V      This implies             =     a∈At   (  )+Σb   ∈B  (     ) t ( )    t
             ∗                                                                        ∗  
            vb · b<vˆb · b +  ≤ vb · b + ,          ≥maxa∈A{R(b, a)+Σb∈BP    (b |b, a)Vt (b )}−2|Ω|−et
                                                        V ∗  b −  | |−e
    which is a contradiction.                         =  t−1( )  2 Ω     t

                                                IJCAI-07
                                                  2640                                                                 a,ω,max       a,ω,min
The last inequality is due to Proposition 2, and the other re- 1. Obtain bt (s) and bt (s), ∀a ∈ A, ω ∈ Ω
                                            e
lations are by deﬁnition. The above implies that t−1 =  2. Create multiple belief polytopes, one for each action-
et +2|Ω| and with eT =2|Ω|, we have the error for the                    ai,ωi
                                                          observation pair: Bt =
EVA-policy at the beginning of execution, e1 =2|Ω|T (the
                                                                   ai,ωi,min   ai,ωi,max
total error bound for EVA).                                        [bt     (s1) bt     (s1)] ×···
                                 γ
  Similarly, it can be proved that for -discounted inﬁnite             ai,ωi,min     ai,ωi,max
                                   2|Ω|                         ···×[bt       (s|S|) bt    (s|S|)].
horizon problems, the total error bound is 1−γ .
                                                                                  DDB           a,ω
                                                        The feasible belief space is Bt = ∪a,ωBt  .How-
4  Dynamic Disjunctive Beliefs (DDB)                  ever, this is disjunctive and cannot be expressed in the LP
                                                      that is used for pruning. Instead, the prune occurs over each
                                                      Ba,ω
          b(s j)                                        t  and we take the union of dominant vectors for these be-
                                                      lief spaces. This increases the number of LP calls for a ﬁxed
                                a1,ω1
                               Bt                     epoch but the LPs cover smaller spaces and will yield fewer
                 a1,ω2
                Bt                                    vectors at the end. The cascading effects of the reduced vector

                    a2,ω1
                   Bt                                 sets (as in EVA) over the dynamic programming process can

                             ,ω
                            a2 2 DB                   lead to fewer policies at the next stage, resulting in signiﬁcant
                           Bt   Bt
                                                      computational savings.
                                      b(si)
                                                      5   Experiments
        Figure 2: DDB: An example of DDB vs. DB
                                                      Our experiments3 show how EVA  and DDB address key
  By eliminating reasoning about optimal policies at un- shortcomings of current POMDP algorithms when a policy
reachable beliefs, it is possible to improve policy computation that meets a pre-speciﬁed error bound is desired. Three key
time greatly without sacriﬁcing any solution quality. Our key results provided in this section relevant to EVA are: (1) EVA
insight is that the reachable belief space is composed of small can guarantee the generation of a policy that meets any pre-
belief polytopes; hence we introduce an efﬁcient technique speciﬁed error bound where point-based algorithms cannot
for exploiting these smaller polytopes. This contrasts with (Section 5.1). (2) The cost of calculating the error bound of
the previous work on Dynamic Beliefs, DB by [Varakantham a policy, which EVA avoids, can signiﬁcantly impact perfor-
et al., 2005], which assumed a single large belief polytope. In mance of point based approaches(Section 5.2). (3) There are
particular, DB obtains the maximum belief probability for a domains, where point-based algorithms cannot provide func-
                                a,ω,max               tional bounds (Section 5.3). For DDB, we show an order-
state given an action and observation, bt (s) by solving
a constrained optimization problem using Lagrangian tech- of-magnitude speedup over the previously best technique for
niques. Similarly minimum possible belief probability is also belief space pruning (Section 5.4).
computed. The dynamic belief polytope at epoch t was cre-
ated as follows:
 1. Find the maximum  and minimum  possible belief for
                                          max
    each state over all actions and observations: bt (s)=
            a,ω,max   min             a,ω,min
    maxa,ω bt     (s), bt (s)=maxa,ω bt     (s).
 2. Create a belief polytope that combines these bounds over
              DB
    all states: Bt =
       min     max             min      min
     [bt  (s1) bt (s1)] ×···×[bt  (s|S|) bt (s|S|)].
  While this is an appropriate bound in that any belief out-
    BDB                      t
side t   is not possible at time , in many domains there    Figure 3: Error bounds for Hallway and Hallway2
is not a single belief polytope, but a collection of smaller
belief polytopes - because future beliefs depend on partic-
ular action-observation pairs. Unfortunately, DB insists on 5.1 Bound Achievability
a single belief polytope, thus including many unreachable
beliefs. Figure 2 illustrates this for a two-observation two- In the general case, point-based approaches can always en-
                                                DB    counter problems where they cannot generate a policy that
action system over a two-dimensional support. While Bt
is smaller than the entire space of potential beliefs, it is larger meets a pre-speciﬁed error bound. If the span of belief sup-
than necessary; it is not possible to believe anything outside port, chosen independently from the value function, is smaller
  ∪         Bai,ωj
of  i=1,2;j=1,2 t . In contrast, our new method for ex-  3Hallway, Hallway2, Tiger-Grid, and Aircraft are benchmark
pressing feasible beliefs is referred to as Dynamic Disjunctive problems available from Anthony Cassandra’s POMDP website.
Belief (DDB) bounds. The DDB method for computing fea- Scotland Yard is a 216-state, 16-act, 6-obs problem motivated by the
sible belief spaces is to eliminate conditioning on action and Ravensburger Scotland Yard game. Tag is a 870-state, 5-act, 30-obs
observations (eliminate taking max over a, ω in (1) above): problem from [Pineau et al., 2003].

                                                IJCAI-07
                                                  2641than the feasible belief space, there exists a value function We note that: (1) The same-bound EVA run-times are less
for which the algorithm will have a minimum error, i.e., the than NLP run-times, i.e. just the bound computation ove-
solution quality cannot exceed some threshold. If the pre- head of PBVI is a signiﬁcant burden. (2) Even without the
speciﬁed error bound is smaller than this minimum error, the NLP, EVA run-times are better than PBVI run-times in all
algorithm will never be able to provide a satisfactory policy. cases. (3) The half-bound EVA run-times are better than NLP
Current point-based approaches exacerbate this problem by run-times in all but one case. We see that a hampered EVA
(a) starting with one belief point and (b) only considering outperformed an ameliorated PBVI. HSVI2 [Smith and Sim-
points that are reachable from this initial point. Thus, they mons, 2005] outperforms PBVI in run-time but suffers from
potentially restrict the span of their belief support severely the same problems i.e. the time to compute the bound and
which implies a signiﬁcant minimum error. Figure 3 presents the unattainability of the bound. Beyond the experimental
the PBVI error bound for the Hallway and Hallway2 prob- results, point-based approaches will suffer as their belief sup-
lems as the belief set is expanded over time. The x-axis repre- ports get larger because the NLP cost increases exponentially.
sents the number of belief set expansions, and the y-axis rep-
resents the error bound (by solving an NLP for each belief set
expansion). The error bound remains constant for both prob-
lems after the ﬁfth expansion (around 30 belief points in the
set). After the tenth expansion (around 1000 belief points) the
NLP becomes too expensive to calculate. Thus, for instance,
for Hallway2, it is not veriﬁable if PBVI would be able to
reach an error bound of 1.8. However with EVA, by using
an  (in Algorithm 2) corresponding to the pre-speciﬁed error
bound (obtained by equating error bound to 2 ∗  ∗|Ω|∗T ),
any error bound is achievable.

5.2  Bound Calculation Cost
                                                            Figure 4: Run time comparison of EVA and PBVI
Now let us consider the case where a given point-based ap-
proach can generate a policy whose quality is within a pre-
speciﬁed error bound. These algorithms still do not know the 5.3 Bound Functionality
quality loss in their policy as they expand their belief support.
As indicated earlier, they require an expensive NLP to calcu- Even if the NLP cost vanished, there are domains where the
late the error bound 4. We ran both EVA and PBVI for the expression for error bound in point-based approaches yields
Tiger-Grid, Hallway, Hallway2, Aircraft, Scotland Yard and a value that is not functional. Consider the Task Manage-
                                                      ment Problem (TMP) [Varakantham et al., 2005] in software
Tag problems. As PBVI depends on the starting belief point,                                   R     
for each problem we obtained the bound for 10 randomly se- personal assitant domains. There is a penalty ( min 0)
lected initial belief points. For each given initial belief point, associated with certain state-action pairs to control agent au-
we ran PBVI for a ﬁxed number of belief set expansions and tonomy. In this case, the output of the NLP yields an error
computed the bound of the resulting policy with an NLP. The bound so loose with respect to the actual solution quality that
deviations in the obtained bounds were extremely small. This it renders the bound useless. There is no method for a point-
gives an advantage to PBVI by assuming that it has the cor- based approach to determine when to stop expanding the set
rect belief support for the obtained bound immediately. Ad- of belief sets in TMPs.
ditionally, we only counted the average run-time of the ﬁnal 5.4 DDB
expansion.
  We then ran EVA with the equivalent error bound, forcing As shown in Figure 5, the TMP domain also illustrates the
EVA to obtain a policy as good or better (bound calculation beneﬁts of DDB . GIP and RBIP did not terminate for TMP
for EVA is trivial, i.e., constant-time computation). Further- within the pre-speciﬁed time limit. With a sufﬁciently large
more, EVA was also run with a tighter bound (half the value bound, EVA produced a policy within our time limits. To
obtained for PBVI), to address the possibility that an alter- improve performance, one option was to use EVA with a
nate belief expansion heuristic in PBVI led to a better bound. larger bound, but this would have meant an additional sac-
The results are shown in Figure 4. The NLP bar denotes the riﬁce in solution quality. EVA+DB reduced run-time without
time taken for computing the error bound in PBVI, while sacriﬁcing solution quality. We see that EVA+DDB (combin-
the PBVI bar shows the run-time for computing the policy ing our two new techniques) provided an order-of-magnitude
corresponding to the ﬁnal expansion (not including the NLP speedup beyond EVA+DB, also without losing solution qual-
run-time). The bars for EVA (same bound) and EVA (half ity. Runtimes for EVA+DDB and EVA+DB included the
bound)indicate the time taken by EVA when provided with a overhead for computation of the belief bounds.
bound equal to and half of that used by PBVI respectively. 5.5 Experiment Summary
  4We employed an industrial optimization software called Lingo We demonstrated the drawbacks of current point-based ap-
8.0 for solving NLPs. Machine specs for all experiments: Intel Xeon proaches when tasked with generating policies with pre-
3.6 GHZ processor, 2GB RAM                            speciﬁed error bounds. These limitations include the inability

                                                IJCAI-07
                                                  2642