                     A Visual-Sensor Model for Mobile Robot Localisation 

                                     Matthias Fichtner Axel Grofimann 
                                         Department of Computer Science 
                                          Technische Universitat Dresden 
                                                 Dresden, Germany 


1 Introduction 
Due to recent advances in robot hardware, there is a 
great demand for vision-based robot localisation techniques 
[DeSouza and Kak, 2002]. We present a probabilistic sen•
sor model for camera-pose estimation in hallways and other 
known structured environments. Given a 3D geometrical map 
of the environment, we want to find an approximate mea•
sure of the probability that a given camera image has been 
obtained at a certain place in the robot's operating environ•    Figure 1: Processing steps of the visual-sensor model. 
ment. Our sensor model is based on feature matching tech•
niques that are simpler than state-of-the-art photogrammet-     Similarity measures. The correspondence of image and 
ric approaches. This allows the model to be used in proba•    model features is evaluated by a similarity measure. In gen•
bilistic robot localisation methods, such as Monte Carlo lo•  eral, such a measure may take into account the differences 
calisation (MCL) [Dellaert et al, 1999]. We have combined     in orientation between corresponding line segments in im•
photogrammetric techniques for feature projection with the    age and model, or their distance and difference in length. In 
flexibility and robustness of MCL. Moreover, our approach is  the following, we present two simple and efficient similarity 
sufficiently fast to allow for sensor fusion. That is, by using measures. The first is solely based on the distance of line seg•
distance measurements from sonars and laser in addition to    ments in the Hough space. We consider only those image fea•
the visual input, we may be able to improve localisation ac•  tures as possible matches that lie within a rectangular cell in 
curacy. We have used our sensor model with MCL to track the   the Hough space centred at the model feature. The matches 
position of a Pioneer 2 robot navigating in a hallway. Possi• are counted and the resulting sum is normalised. The map•
bly, our approach can be used also for localisation in cluttered ping from the expectation (model features) to the measure•
environments and for shape-based object detection.            ment (image features) accounts for the fact that the measure 
  In the next section, we briefly describe the components of  should be invariant with respect to objects not modelled in 
the visual-sensor model. We conclude with a discussion of     the provided map or unexpected changes in the operating en•
experimental results.                                         vironment. Invariance of the number of visible features is 
                                                              obtained by normalisation. Specifically, this centred match 
2 Sensor Model                                                count (CMC) measure SCMC is defined as: 
A visual-sensor model describes the probability of obtaining 
a particular camera image given the camera's pose and a ge•
ometrical map of the environment. Rather than comparing 
camera image and the expected view at the pixel level, we     where the predicate m defines a valid match using the dis•
gained improved robustness using image features. We de•       tance parameters and the operator # counts the num•
cided to use line segments because they can be detected com•  ber of matches. Generally speaking, this similarity measure 
paratively reliably under changing illumination conditions.   computes the proportion of expected model Hough points 
As world model, we use a wire-frame model of the operating            that are confirmed by at least one measured image 
environment, represented in VRML. The individual process•     Hough point falling within tolerance Note 
ing steps are depicted in Figure 1.                           that neither endpoint coordinates nor lengths are considered 
  Representation. Each straight-line feature is represented   here. The second measure is based on a comparison of the 
as a single point in the 2D Hough space given by              total length values of groups of lines, which we name grid 
                       where end-points are neglected. In     length match (GLM). Split lines in the image are grouped 
this representation, truncated or split lines have similar coor• together using a uniform discretisation of the Hough space. 
dinates.                                                      This method is similar to the Hough transform for straight 


POSTER PAPERS                                                                                                     1555 1556 

                                                                                          POSTER PAPERS 