               A Predictive Approach to Help-desk Response Generation

                                 Yuval Marom and Ingrid Zukerman
                         Faculty of Information Technology, Monash University
                                 Clayton, Victoria 3800, AUSTRALIA
                           {yuvalm,ingrid}@csse.monash.edu.au


                    Abstract                           Is there a way to disable the NAT ﬁrewall on the CP-2W so I
    We are developing a corpus-based approach for      don’t get a private ip address through the wireless network?
    the prediction of help-desk responses from features Unfortunately, you have reached the incorrect eResponse queue
                                                       for your unit. Your device is supported at the following link, or
    in customers’ emails, where responses are repre-
                                                       at 888-phone-number. We apologize for the inconvenience.
    sented at two levels of granularity: document and
    sentence. We present an automatic and human-              Figure 1: A sample request-response pair.
    based evaluation of our system’s responses. The
    automatic evaluation involves textual comparisons this information, while other times they elicit answers that do
    between generated responses and responses com-    not refer to the query terms, but contain generic information,
    posed by help-desk operators. Our results show that as seen in the example in Figure 1.2 Our previous experiments
    both levels of granularity produce good responses, show that a standard document retrieval approach, where a
    addressing inquiries of different kinds. The human- new request is matched in its entirety with previous requests
    based evaluation measures response informative-   or responses, is successful only in very few cases [Zukerman
    ness, and conﬁrms our conclusion that both levels and Marom, 2006]. We posit that this is because (1) many re-
    of granularity produce useful responses.          quests raise multiple issues, and hence do not match well any
                                                      one document; (2) the language variability in the requests is
1  Introduction                                       very high; and (3) as seen in Figure 1, the replies to many
Email inquiries sent to help desks often “revolve around a technical requests are largely non-technical, and hence do not
small set of common questions and issues”.1 This means match technical terms in the requests.
that help-desk operators spend most of their time dealing with These observations lead us to consider a predictive ap-
problems that have been previously addressed. Further, a sig- proach, which uses correlations between features of requests
niﬁcant proportion of help-desk responses contain a low level and responses to guide response generation. In principle, cor-
of technical content, corresponding, for example, to inquiries relations could be modelled directly between terms in the re-
addressed to the wrong group, or insufﬁcient detail provided quests and responses [Berger and Mittal, 2000].However,we
by the customer. Organizations and clients would beneﬁt if have observed that responses in our corpus exhibit strong reg-
the efforts of human operators were focused on difﬁcult, atyp- ularities, mainly due to the fact that operators are equipped
ical problems, and an automated process was employed to with in-house manuals containing prescribed answers. For
deal with the easier problems.                        example, Figure 2 shows two responses that contain parts that
  In this paper, we report on our experiments with corpus- are almost identical (the two italicized sentences). The exis-
based approaches for the automation of help-desk responses. tence of these regularities motivates us to generate abstrac-
Our study was based on a log of 30,000 email dialogues tions of responses, rather than deal with low-level response
between users and help-desk operators at Hewlett-Packard. terms. In contrast, similar regularities do not exist in the re-
However, to focus our work, we used a sub-corpus of 6659 quests so we choose to represent them at a term-based level.
email dialogues, which consisted of two-turn dialogues where The request-response correlations are then modeled at these
the answers were reasonably concise (15 lines at most). These two levels. As seen in the examples in Figures 1 and 2, the de-
dialogues deal with a variety of user requests, which include sirable granularity for representing responses can vary: it can
requests for technical assistance, inquiries about products, be as ﬁne as sentences (Figure 2), or as coarse as complete
and queries about how to return faulty products or parts. documents (Figure 1). The investigations reported here in-
  Analysis of our corpus reveals that requests containing pre- volve these two levels of granularity, leading to the Sent-Pred
cise information, such as product names or part speciﬁca- and Doc-Pred methods respectively (Section 2).
tions, sometimes elicit helpful, precise answers referring to
                                                         2Sample requests and responses are reproduced verbatim from
  1
   http://customercare.telephonyonline.com/ar/telecom_ the corpus (except for URLs and phone numbers, which have been
next_generation_customer.                             disguised by us), and some have customer or operator errors.

                                                IJCAI-07
                                                  1665                                                      request features, and selects the response that is most repre-
  If you are able to see the Internet then it sounds like it is work- sentative of the cluster (closest to the centroid). This method
 ing, you may want to get in touch with your IT department to
 see if you need to make any changes to your settings to get it to predicts a group of responses similar to the response in Fig-
 work. Try performing a soft reset, by pressing the stylus pen in ure 1 from the input term “CP-2W”.
 the small hole on the bottom left hand side of the Ipaq and then The clustering is performed in advance of the prediction
 release.                                             process by the clustering program Snob, which performs mix-
                                                      ture modelling combined with model selection based on the
  I would recommend doing a soft reset by pressing the stylus Minimum Message Length criterion [Wallace and Boulton,
 pen in the small hole on the left hand side of the Ipaq and then 1968]. We chose this program because one does not have to
 release. Then charge the unit overnight to make sure it has been specify in advance the number of clusters. We use a binary
 long enough and then see what happens. If the battery is not representation whereby the lemmatized content words in the
 charging then the unit will need to be sent in for repair. corpus make up the components of an input vector, and its
                                                      values correspond to the absence or presence of each word in
        Figure 2: Responses that share a sentence.    the response (this representation is known as bag-of-words).
                                                        The predictive model is a Decision Graph [Oliver, 1993]
  As for any learning task, building prediction models for trained on (1) input features: unigram and bigram lemmas
responses at an abstracted level of representation has advan- in the request,3 and (2) target feature: the identiﬁer of the
tages and drawbacks. The advantages are that the learn- response cluster that contains the actual response for the re-
ing is more focused, and it deals with data of reduced spar- quest. The model provides a prediction of which response
sity. The drawback is that there is some loss of informa- cluster is most suitable for a given request, as well as a level
tion when somewhat dissimilar response units (sentences or of conﬁdence in this prediction. If the conﬁdence is not sufﬁ-
documents) are grouped together. In order to overcome this ciently high, we do not attempt to produce a response.
disadvantage, we have developed a prediction-retrieval hy-
brid approach, which predicts groups of responses, and then 2.2 Sentence Prediction (Sent-Pred)
selects between dissimilar response units by matching them As for the Doc-Pred method, the Sent-Pred method starts
with request terms. We investigate this approach at the sen- by abstracting the responses. It uses the same clustering
tence level, leading to the Sent-Hybrid method (Section 2). program, Snob, to cluster sentences into Sentence Clusters
  Note that the Doc-Pred method essentially re-uses an ex- (SCs), using the representation used for Doc-Pred.4 Unlike
isting response in the corpus to address a new request. In the Doc-Pred method, where only a single response cluster
contrast, the two sentence-based methods combine sentences is predicted, resulting in a single response document being
from multiple response documents to produce a new re- selected, in the Sent-Pred method several SCs are predicted.
sponse, as is done in multi-document summarization [Fila- This is because here we are trying to collate multiple sen-
tova and Hatzivassiloglou, 2004]. Hence, unlike Doc-Pred, tences into one response. Each request is used to predict
Sent-Pred and Sent-Hybrid may produce partial responses. In promising SCs, and a response is composed by extracting one
this paper, we investigate when the different methods are ap- sentence from each such SC. Because the sentences in each
plicable, and whether individual methods are uniquely suc- SC originate from different response documents, the process
cessful in certain situations. Speciﬁcally, we consider the of selecting them for a new response corresponds to multi-
trade off between partial, high-precision responses, and com- document summarization. In fact, our selection mechanism is
plete responses that may contain irrelevant information. based on a multi-document summarization formulation pro-
  The rest of this paper is organized as follows. In the next posed by Filatova and Hatzivassiloglou [2004].
section, we describe our three prediction methods, followed To illustrate these ideas, consider the ﬁctitious example
by the evaluation of their results in Section 3. In Section 4, in Figure 3. Three small SCs are shown in the example
we discuss related research, and then present our conclusions (in practice the SCs can have tens and hundreds of sen-
and plans for future work in Section 5.               tences). The thick arrows correspond to high-conﬁdence pre-
                                                      dictions, while the thin arrows correspond to sentence selec-
2  Methods                                            tion. The other components of the diagram demonstrate the
In this section, we present the implementation details of our Sent-Hybrid approach (Section 2.3). In this example, three
three prediction methods. Note that some of the methods of the request terms – “repair”, “faulty” and “monitor” – re-
were implemented independently of each other at different sult in a conﬁdent prediction of two SCs: SC1 and SC2.The
stages of our project. Hence, there are minor implementa- sentences in SC1 are identical, so we can arbitrarily select
tional variations, such as choice of machine learning algo- a sentence to include in the generated response. In contrast,
rithms and some discrepancies regarding features. We plan to although the sentences in SC2 are rather similar, we are less
bridge over these differences in the near future, but are con- conﬁdent in arbitrarily selecting a sentence from it.
ﬁdent that they do not impede the aim of the current study: The predictive model is a Support Vector Machine (SVM).
evaluating a predictive approach to response generation. A separate SVM is trained for each SC, with unigram and bi-
2.1  Document Prediction (Doc-Pred)
                                                         3Signiﬁcant bigrams are obtained using the NSP package (http:
This prediction method ﬁrst groups similar response docu- //www.d.umn.edu/˜tpederse/nsp.html).
ments (emails) in the corpus into response clusters. For each 4We have also experimented with syntactic features of sentences,
request it then predicts a response cluster on the basis of the but the simple bag-of-words representation was as competitive.

                                                IJCAI-07
                                                  1666                                                         SC1
                                   Forhardwarerepairspleasecontact
                                   ourSupportTeamon1800SUPPORT.    Generatedresponse
                                      Forhardwarerepairspleasecontact
  Request                             ourSupportTeamon1800SUPPORT.
                                   Forhardwarerepairspleasecontact Forhardwarerepairspleasecontact
   Needtorepairfaultymonitor.      ourSupportTeamon1800SUPPORT.    ourSupportTeamon1800SUPPORT.
                                                        SC2
                                                                   Theywillarrangeaserviceforyourmonitor.
   ItisaT20modelwhichI’vehadfor     Theywillarrangeaserviceforyournotebook.
                                     Theywillarrangeaserviceforyourprinter.
   over3yearswithoutanyproblems.   Theywillarrangeaserviceforyourmonitor.
                                   Theywillarrangeaserviceforyourprinter. TheT20modelhasknownproblemsthatoccur
                                                                   23yearsafterpurchase.
                                                       SC3

                                   TheT20modelhasknownproblemsthatoccur
                                   23yearsafterpurchase.
                                 TheT20monitorsshouldbeservicedevery23years.
                                   TheT20serieshasaservicecycleof23years.

                 Figure 3: A ﬁctitious example demonstrating the Sent-Pred and Sent-Hybrid methods.

gram lemmas in a request as input features, and a binary target hesive for a conﬁdent selection of a representative sentence.
feature specifying whether the SC contains a sentence from However, sometimes the ambiguity can be resolved through
the response to this request. During the prediction stage, the cues in the request. In this example, one of the sentences
SVMs predict zero or more SCs for each request, as shown matches the request terms better than the other sentences, as it
in Figure 3. The sentence closest to the centroid is then ex- contains the word “monitor”. The Sent-Hybrid method com-
tracted from each highly cohesive SC predicted with high plements the prediction component of Sent-Pred with a re-
conﬁdence. A high-conﬁdence prediction indicates that the trieval component, and thus forms a hybrid.
sentence is relevant to many requests that share certain regu- This retrieval component implements the traditional Infor-
larities. A cluster is cohesive if the sentences in it are similar mation Retrieval paradigm [Salton and McGill, 1983],where
to each other, which means that it is possible to obtain a sen- a “query” is represented by its content terms, and the system
tence that represents the cluster adequately. These stringent retrieves a set of documents that best matches this query. In
requirements placed on conﬁdence and cohesion mean that the help-desk domain, a good candidate for sentence retrieval
the Sent-Pred method often yields partial responses.  contains terms that appear in the request, but also contains
  The cohesion of an SC is calculated as              other terms that hopefully provide the requested information
   1 N                                               (in the example in Figure 3, the “best” sentence in SC2 shares
        [ (w  ∈ SC)  ≤ α ∨   (w  ∈ SC)  ≥ 1−α]
  N     Pr   k             Pr   k                     only one term with the request). Therefore, we perform a
     k=1                                              recall-based retrieval, where we ﬁnd sentences that match as
where N is the number of content lemmas, Pr(wk ∈ SC)  many terms as possible in the request, but are allowed to con-
is the probability that lemma wk is used in the SC (obtained tain additional terms. Recall is calculated as
from the centroid), and α is a parameter that represents how    Σ
                                                             =    TF.IDF of lemmas in request sent & response sent
strict we are when judging the similarity between sentences. recall
                                                                    Σ TF.IDF of lemmas in request sentence
We have used this parameter, instead of raw probabilities, be-
cause strictness in judging sentence similarity is a subjective We have decided to treat the individual sentences in a re-
matter that should be decided by the users of the system. The quest email as separate “queries”, rather than treat the com-
sensitivity analysis we performed for this parameter is dis- plete email as a single query, because a response sentence is
cussed in [Marom and Zukerman, 2005].                 more likely to have a high recall when matched against a sin-
  Our formula implements the idea that a cohesive group of gle request sentence as opposed to a whole document.
sentences should agree on both the words that are included For highly cohesive SCs predicted with high conﬁdence,
in these sentences and the words that are omitted. For in- we select a representative sentence as before.
stance, the italicized sentences in Figure 2 belong to an SC For SCs with medium cohesion predicted with high conﬁ-
with cohesion 0.93. For values of α close to zero, our for- dence, we attempt to match its sentences with a request sen-
mula behaves like entropy, favouring very strong agreement tence. Here we use a liberal (low) recall threshold, because
on word usage and omission. The latter is necessary because the high prediction conﬁdence guarantees that the sentences
just knowing that certain words appear in a high percentage in the cluster are suitable for the request. The role of retrieval
of the sentences in a cluster is not sufﬁcient to determine how in this situation is to select the sentence whose content lem-
similar are these sentences. One also must know that these mas best match the request, regardless of how well they match
sentences exclude most other words.                   (the non-content lemmas, also known as function words or
                                                      stop words, are excluded from this account).
2.3  Sentence Prediction-Retrieval (Sent-Hybrid)        For uncohesive clusters or clusters predicted with low con-
As we can see in cluster SC2 in Figure 3, it is possible for ﬁdence, we can rely only on retrieval. Now we must use a
an SC to be strongly predicted without being sufﬁciently co- more conservative recall threshold to ensure that only very

                                                IJCAI-07
                                                  1667highly-matching sentences are included in the response. SC3  Table 1: Results of the automatic evaluation
in Figure 3 is an example of an SC for which there is insuf- Method   Coverage          Quality
ﬁcient evidence to form strong correlations between it and                          Average (Stdev.)
request terms. However, we can see that one of its sentences                      Precision   F-score
matches very well the second sentence in the request. In fact, Doc-Pred 29%      0.82 (0.21) 0.82 (0.24)
all the content words in that request sentence are matched, Sent-Pred   34%      0.94 (0.13) 0.78 (0.18)
resulting in a perfect recall score of 1.0.              Sent-Hybrid    43%      0.81 (0.29) 0.66 (0.25)
  Once we have the set of candidate response sentences that
satisfy the appropriate recall thresholds, we remove redun- and McGill, 1983]: precision approximates correctness, and
dant sentences. Since sentences that belong to the same F-score approximates completeness and correctness in com-
medium-cohesive SC are quite similar to each other, it is sufﬁ- bination. Precision and F-score are calculated as follows us-
cient to select a single sentence – that with the highest recall. ing a word-by-word comparison (stop-words are excluded).5
Sentences from uncohesive clusters are deemed sufﬁciently
                                                                   # words in both model and generated response
different to each other, so they can all be retained. All the Precision =
retained sentences will appear in the generated response (at            # of words in generated response
present, these sentences are treated as a set, and are not orga-  # words in both model and generated response
nized into a coherent reply).                             Recall =
                                                                        # of words in model response
                                                                           2 × Precision × Recall
3  Evaluation                                                     F-score =
In this section, we examine the performance of our three pre-               Precision + Recall
diction methods. Our experimental setup involves a standard 3.2 Results
10-fold validation, where we repeatedly train on 90% of a
dataset and test on the remaining 10%. We present each test The combined coverage of the three methods is 48%. This
split as the set of new cases to be addressed by the system. means that for 48% of the requests, a partial or complete re-
                                                      sponse can be generated by at least one of the methods. Ta-
3.1  Performance Measures                             ble 1 shows the individual results obtained by the different
We are interested in two performance indicators: coverage methods. The Doc-Pred method can address 29% of the re-
and quality.                                          quests. Only an overall 4% of the requests are uniquely ad-
                                                      dressed by this method, but the generated responses are of a
  Coverage is the proportion of requests for which a re-
                                                      fairly high quality, with an average precision and F-score of
sponse can be generated. We wish to determine whether the
                                                      0.82. Notice the large standard deviation of these averages,
three methods presented in the previous section are applica-
                                                      suggesting a somewhat inconsistent behaviour. This is due
ble in different situations, i.e., how exclusively they address
                                                      to the fact that this method gives good results only when it
or “cover” different requests. Each of these methods speci-
                                                      predicts a complete generic response that is very similar to
ﬁes a requisite level of conﬁdence for the generation of a re-
                                                      the model response. However, when this is not the case, per-
sponse. If the response planned by a method fails to meet this
                                                      formance degrades substantially. This means that Doc-Pred
conﬁdence level for a request, then the request is not covered
                                                      is suitable when requests that share some regularity receive a
by this method. Since the sentence-based methods generate
                                                      complete template response.
partial responses, we say that their responses cover a request
                                                        The Sent-Pred method can ﬁnd regularities at the sub-
if they contain at least one sentence generated with high con-
                                                      document level, and therefore deal with cases where partial
ﬁdence. Non-informative sentences, such as “Thank you for
                                                      responses can be generated. It produces responses for 34% of
contacting HP”, which are often produced by these methods,
                                                      the requests, and does so with a consistently high precision
have been excluded from our calculations, in order to have a
                                                      (average 0.94, standard deviation 0.13). Only an overall 1.6%
useful comparison between our methods.
                                                      of the requests are uniquely addressed by this method. How-
  Quality is a subjective measure, best judged by users of a
                                                      ever, for the cases that are shared between this method and
deployed system. Here we approximate a quality assessment
                                                      other ones, it is useful to compare the actual quality of the
by means of two experiments: a preliminary human-based
                                                      generated responses. For example, with respect to Doc-Pred,
study where people evaluate a small subset of the responses
                                                      for 10.5% of the cases Sent-Pred either uniquely addresses re-
generated by our system (Section 3.3); and a comprehensive
                                                      quests, or jointly addresses requests but has a higher F-score.
automatic evaluation that treats the responses generated by
                                                      This means that in some cases a partial response has a higher
the help-desk operators as model responses, and performs
                                                      quality than a complete one. Note that since Sent-Pred has
text-based comparisons between these responses and the gen-
                                                      a higher average precision than Doc-Pred, its lower average
erated ones (Section 3.2). We are interested in the correct-
                                                      F-Score must be due to a lower average recall. This conﬁrms
ness and completeness of a generated response. The former
                                                      that Sent-Pred produces partial responses.
measures how much of its information is correct, and the lat-
                                                        The Sent-Hybrid method extends the Sent-Pred method by
ter measures its overall similarity with the model response.
                                                      employing sentence retrieval, and thus has a higher coverage
We consider correctness separately because it does not pe-
nalize missing information, enabling us to better assess our 5We have also employed sequence-based measures using the
sentence-based methods. These measures are approximated ROUGE tool set [Lin and Hovy, 2003], with similar results to those
by means of two measures from Information Retrieval [Salton obtained with the word-by-word measures.

                                                IJCAI-07
                                                  1668 My screen is coming up reversed (mirrored). There must be        Doc−Pred
 something loose electronically because if I put the stylus in it’s 0.6 Sent−Hybrid
 hole and move it back and forth, I can get the screen to display
                                                            0.4
 properly momentarily. Please advise where to send for repairs.

  To get the iPAQ serviced, you can call 1-800-phone-number, Proportion 0.2
 options 3, 1 (enter a 10 digit phone number), 2. Enter your  0
 phone number twice and then wait for the routing center to put    0        1        2         3
 you through to a technician with Technical Support. They can                 Judgement
 get the unit picked up and brought to our service center.
                                                                  Sent−Pred
                                                            0.6   Sent−Hybrid
   To get the iPAQ repaired (battery, stylus lock and screen), 0.4
 please call 1-800-phone-number, options 3, 1 (enter a 10 digit

 phone number), 2.                                          Proportion 0.2
                                                              0
                                                                   0        1        2         3
 Figure 4: Example demonstrating the Sent-Hybrid method.                      Judgement
                                                           Figure 5: Human judgements of informativeness.
(43%). This is because the retrieval component can often in-
clude sentences from SCs with medium and low cohesion, than a response composed by an operator.
which might otherwise be excluded. However, this is at the We maximized the coverage of this study by allocating dif-
expense of precision. Retrieval selects sentences that match ferent cases to each judge, thus avoiding a situation where
closely a given request, but this selection can differ from the a particularly good or bad set of cases is evaluated by all
“selections” made by the operator in the model response. Pre- the judges. Since the judges do not evaluate the same cases,
cision (and hence F-score) penalizes such sentences, even we cannot employ standard inter-tagger agreement measures.
when they are more appropriate than those in the model re- Still, it is necessary to have some measure of agreement be-
sponse. For example, consider the request-response pair at tween judges, and control for bias from speciﬁc judges or
the top of Figure 4. The response is quite generic, and is used speciﬁc cases. We do this separately for each prediction
almost identically for several other requests. The Sent-Hybrid method by performing pairwise signiﬁcance testing (using the
method almost reproduces this response, replacing the ﬁrst Wilcoxon rank sum test for equal medians), where the data
sentence with the one shown at the bottom of Figure 4. This from two judges are treated as independent samples. We then
sentence, which matches more request words than the ﬁrst remove the data from a particular judge if he or she has a sig-
sentence in the model response, was selected from an SC that niﬁcant disagreement with other judges. This happened with
is not highly cohesive, and contains sentences that describe one judge, who was signiﬁcantly more lenient than the oth-
different reasons for setting up a repair (the matching word ers on the Sent-Pred method. Since there are four judges, we
is “screen”). Overall, the Sent-Hybrid method outperforms have an overall maximum of 80 cases in each evaluation set.
the other methods in about 12% of the cases, where it either Figure 5 shows the results for the two evaluation sets. The
uniquely addresses requests, or addresses them jointly with top part, which is for the ﬁrst set, shows that when both Doc-
other methods but produces responses with a higher F-score. Pred and Sent-Hybrid are applicable, the former receives an
                                                      overall preference, rarely receiving a zero informativeness
3.3  Human judgements                                 judgement. Since the two methods are evaluated together
The purpose of this part of the evaluation is twofold. First, for the same set of cases, we can perform a paired signiﬁ-
we want to compare the quality of responses generated at cance test for differences between them. Using a Wilcoxon
different levels of granularity. Second, we want to evalu- signed rank test for a zero median difference, we obtain a
ate cases where only the sentence-based methods can pro- p-value 0.01, indicating that the differences in judgements
duce a response, and therefore establish whether such re- between the two methods are statistically signiﬁcant.
sponses, which are often partial, provide a good alternative to The bottom part of Figure 5 is for the second evaluation
a non-response. Hence, we constructed two evaluation sets: set, comparing the two sentence-based methods. Recall that
one containing responses generated by Doc-Pred and Sent- this evaluation set comprises cases that were addressed only
Hybrid, and one containing responses generated by Sent-Pred by these two methods. Thus, the ﬁrst important observation
and Sent-Hybrid. The latter evaluation set enables us to fur- from this chart is that when a complete response cannot be re-
ther examine the contribution of the retrieval component of used, a response collated from individual sentences is often
the hybrid approach. Each evaluation set comprises 20 cases, judged to contain some level of informativeness. The second
and each case contains a request email, the model response observation from this chart is that there does not seem to be
email, and the two system-generated responses. We asked a difference between the two methods. In fact, the Wilcoxon
four judges to rate the generated responses on several criteria. signed rank test produces a p-value of 0.13 for the second
Owing to space limitations, we report here only on one cri- evaluation set, thus conﬁrming that the differences are not sta-
terion: informativeness. We used a scale from 0 to 3, where tistically signiﬁcant. It is encouraging that the performance
0 corresponds to “not at all informative” and 3 corresponds of Sent-Hybrid is at least as good as that of Sent-Pred,be-
to “very informative”. The judges were instructed to position cause we saw in the automatic evaluation that Sent-Hybrid
themselves as users of the system, who know that they are re- has a higher coverage. However, it is somewhat surprising
ceiving an automated response, which is likely to arrive faster that Sent-Hybrid did not perform better than Sent-Pred.Itis

                                                IJCAI-07
                                                  1669