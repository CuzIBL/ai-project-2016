           Web Page Cleaning for Web Mining through Feature Weighting 

                         LanYi                                         Bing Liu 
                  School of Computing                     Department of Computer Science 
           National University of Singapore                University of Illinois at Chicago 
                    3 Science Drive 2                           851 S. Morgan Street 
                   Singapore 117543                               Chicago, IL 60607 
                yilan@comp.nus.edu.sg                              liub@cs.uic.edu 

                     Abstract                           with the main content of the page. Such noise includes 
                                                        banner ads, navigational guides, decoration pictures, etc. 
    Unlike conventional data or text, Web pages 
                                                       This work focuses on dealing with local noise in Web 
    typically contain a large amount of information 
                                                       pages. We call it Web page cleaning. The work is chal•
    that is not part of the main contents of the pages, 
                                                       lenging because it is a non-trivial task to decide which part 
    e.g., banner ads, navigation bars, and copyright 
                                                       of a Web page is meaningful and which is noisy. Despite 
    notices. Such irrelevant information (which we 
                                                       its importance, relatively little research has been done on 
    call Web page noise) in Web pages can seriously 
                                                       Web page cleaning so far (see Section 2). In this paper, we 
    harm Web mining, e.g., clustering and classifi•
                                                       propose an effective feature weighting technique to clean 
    cation. In this paper, we propose a novel feature 
                                                       Web pages with the purpose of improving Web mining. 
    weighting technique to deal with Web page noise 
                                                       Our method does not decide and physically remove those 
    to enhance Web mining. This method first builds 
                                                       noisy blocks of a page. Instead, it assigns low weights to 
    a compressed structure tree to capture the com•
                                                       the features in those possibly noisy blocks. The advantage 
    mon structure and comparable blocks in a set of 
                                                       of our method is that there is no need to use a threshold to 
    Web pages. It then uses an information based 
                                                       determine whether a block is noisy or not. Setting a suit•
    measure to evaluate the importance of each node 
                                                       able threshold is very hard in practice. 
    in the compressed structure tree. Based on the tree 
                                                         Our cleaning technique is based on the following ob•
    and its node importance values, our method as•
                                                       servation. In a typical commercial Web site, Web pages 
    signs a weight to each word feature in its content 
                                                       tend to follow some fixed layouts and presentation styles 
    block. The resulting weights are used in Web 
                                                       as most pages are automatically generated. Those parts of 
    mining. We evaluated the proposed technique 
                                                       a page that also appear in many other pages in the site are 
    with two Web mining tasks, Web page clustering 
                                                       more likely to be the noise, and those parts of the page that 
    and Web page classification. Experimental results 
                                                       are quite different from other pages are usually the main 
    show that our weighting method is able to dra•
                                                       content of the page. To capture the common structure and 
    matically improve the mining results. 
                                                       comparable blocks among Web pages, we introduce the 
                                                       compressed structure tree to compress or to merge a set of 
1 Introduction                                         Web pages from a Web site. We then propose a measure 
The rapid expansion of the Internet has made Web a     that is based on information theory [Shannon, 1948] to 
popular place for disseminating and collecting information. evaluate each element node in the compressed structure 
However, useful information on the Web is often ac•    tree to determine the importance of the node. Based on the 
companied by a large amount of noise such as banner ads, tree and node importance values, our method assigns a 
navigation bars, copyright notices, etc. Although such weight to each word feature in its content block. These 
information items are functionally useful for human    feature weights are then directly used in Web mining. 
viewers and necessary for Web site owners, they can se•  Our experimental results based on two popular Web 
riously harm automated information collection and mining mining tasks, i.e., Web page clustering and classification, 
on the Web, e.g., Web page clustering, Web page        demonstrate that the proposed cleaning technique is able 
classification, and information retrieval. Web noise can be to boost the mining accuracy significantly. For example, in 
grouped into two categories based on their granularities: classification, the average classification accuracy over all 
Global noise: It refers to redundant objects with large our datasets increases from 0.785 before cleaning to 0.951 
  granularities, which are no smaller than individual pages. after cleaning. 
  Global noise includes mirror sites, duplicated Web pages 
 and old versioned Web pages to be deleted, etc.       2 Related Work 
Local (intra-page) noise: It refers to irrelevant items 
                                                       Although Web page cleaning is an important problem, 
 within a Web page. Local noise is usually incoherent 


Al AND THE INTERNET                                                                                     43  relatively little work has been done to deal with it. In [Lin 1999]. The former identifies coherent blocks of text with 
 and Ho, 2002], a method is proposed to detect informative similar vocabulary. The latter combines lexical cohesion 
 blocks in Web pages. Their concept of informative blocks with other indicators of topic shift, such as relative per•
 is similar to our concept of main contents of a page. formance of two statistical language models and cue 
 However, the work in [Lin and Ho, 2002] is limited by the words. In [Hearst and Plaunt, 1993], Hearst and Plaunt 
following two assumptions: (1) the system knows a priori discussed the merits of imposing structure on full-length 
 how a Web page can be partitioned into coherent content text documents and reported good results of using local 
 blocks; and (2) the system knows a priori which blocks are structures for information retrieval. Instead of using pure 
 the same blocks in different Web pages. As we will see, text, which is unstructured, we process semi-structured 
 partitioning a Web page and identifying corresponding data. We can make use of the semi-structures present in 
blocks in different pages are actually two critical problems Web pages to help our cleaning task. 
 in Web page cleaning. Our system is able to perform these Finally, our feature weighting method is different from 
tasks automatically. Besides, their work views a Web page feature weighting methods used in information retrieval. 
as a flat collection of blocks, and each block is viewed as a One of the popular methods used in text information re•
collection of words. These assumptions are often true in trieval for feature weighting is the TFIDF scheme [Salton 
news Web pages, which is the domain of their applications. and McGill, 1983; Baeza-Yates and Bibeiro-Neto, 1999]. 
In general, these assumptions are too strong.          This scheme is based on individual word (feature) counts 
  In [Bar-Yossef and Rajagopalan, 2002], Web page      within a page and among all the pages. It is, however, not 
cleaning is defined as a frequent template detection   suitable for Web pages because it does not consider Web 
problem. They propose a frequency based algorithm to   page structures in determining the importance of each 
detect templates or patterns. Their work is not concerned content block and consequently the importance of each 
with the context of a Web site, which can give useful clues word feature in the block. For example, a word in a 
for page cleaning. Moreover, in their work, the partition• navigation bar is usually noisy, while the same word oc•
ing of a Web page is pre-fixed, which is not suitable for a curring in the main part of the page can be very important. 
Web site because a Web site typically has common layouts 
and presentation patterns. We can exploit these common 3 The Proposed Technique 
patterns to partition Web pages and to clean them. 
  Other related work includes data cleaning for data   The proposed cleaning technique is based on analysis of 
mining and data warehousing (e.g., [Lee, et al, 2000]). both layouts (structures) and contents of the Web pages in 
However, they are only concerned with structured data. a given Web site. Thus, our first task is to find a suitable 
Web page data are semi-structured, and thus require dif• data structure to capture and to represent common layouts 
ferent cleaning techniques.                            or presentation styles in a set of pages of the Web site. We 
  Web page cleaning is also related to feature selection in propose the compressed structure tree (CST) for this pur•
traditional text learning (see [Yang and Pedersen, 1997] pose. Below, we start by giving an overview of the DOM 
for a survey of feature selection techniques). In feature (Document Object Model)1 tree and showing that it is 
selection, features are individual words or attributes. insufficient for our task. We then present the compressed 
However, items in Web pages have some structures, which structure tree, which is followed by our entropy measures 
are reflected by their nested HTML tags. Hence, different for evaluating the nodes in the compressed structure tree 
methods are needed in the context of the Web.          for noise detection. 
  [Kushmerick, 1999] proposes some learning mecha•
                                                       3.1 DOM tree 
nisms to recognize banner ads, redundant and irrelevant 
links of Web pages. However, these techniques are not  Each HTML page corresponds to a DOM tree where tags 
automatic. They require a large set of manually labeled are internal nodes and the actual text, images or hyperlinks 
training data and also domain knowledge to generate    are the leaf nodes. Figure 1 shows a segment of HTML 
classification rules.                                  codes and its corresponding DOM tree. In the DOM tree, 
  [Kao et al., 2002] enhances the HITS algorithm of    each solid rectangle is a tag node. The shaded box is the 
[Kleinberg, 1998] by using the entropy of anchor text to actual content of the node, e.g., for the tag IMG, the actual 
evaluate the importance of links. It focuses on improving content is "src=image.gif'' The order of child tag nodes is 
HITS in order to find informative structures in Web sites. from left to right. Notice that our study of HTML Web 
Although it segments Web pages into content blocks to pages begins from the BODY tag nodes since all the 
avoid unnecessary authority and hub propagations, it does viewable parts are within the scope of BODY. Each tag 
not detect or eliminate noisy contents in Web pages.  node is also attached with the display properties of the tag. 
  Our work is also related to segmentation of text docu• Display properties defined by CSS (Cascading Style Sheet) 
ments, which has been studied extensively in information 2 are treated as normal display properties if CSS is avail•
retrieval. Existing techniques roughly fall into two cate• able in the page source (i.e., if CSS is not external). For 
gories: lexical cohesion methods [Beeferman et a/., 1997; convenience of analysis, we add a virtual root node 
Eichman et al., 1999; Kaufmann, 1999; Reynar, 1999] and 

multi-source methods [Allan et al., 1998; Beeferman et al., 1 http://www.w3.org/DOM/ 
                                                      2 http://www.w3.org/Stvle/CSS/ 


44                                                                                   Al AND THE INTERNET  without any attribute as the parent tag node of BODY tag        • STYLEs is the set of presentation styles merged into 
node in each DOM tree.                                              E. 
                                                                 • CHILDs is the set of pointers pointing to the child 
                                                                    element nodes of E in CST. 


   Figure 1: A DOM tree example (lower level tags are omitted)  Figure 2: DOM trees and the compressed structure tree (lower 
Although a DOM tree is sufficient for representing the                          levels of trees are omitted) 
structure of one HTML page and it has been used in many        An example of compressed structure tree is given in Figure 
applications (e.g., [Chakrabarti et al., 2001]), it cannot 
                                                               2, which compresses the DOM trees d1 and d2- We observe 
represent the common structure of a set of Web pages. We       that, except for the tag node SPAN, all the tag nodes in d\ 
aim to compress individual DOM trees of Web pages into 
                                                               are merged with corresponding tag nodes in d2. So, an 
a single tree (compressed structure tree) which captures       element node in CST actually denotes a set of merged tag 
the commonalities of the pages in a Web site. Based on         nodes, which represents a set of logically comparable 
this tree, we can easily study some statistical properties of  blocks in different DOM trees. 
the structures and the contents of the Web pages.                Tag nodes in different DOM trees cannot be merged 
                                                               randomly. We need to ensure that the merged tag nodes are 
3.2 Compressed structure tree (CST) 
                                                               the same logical blocks in different Web pages. We build a 
Before defining the compressed structure tree, we first        CST of a set of Web pages from a Web site by merging 
define the presentation style of a tag node in a DOM tree.     their DOM trees from top to bottom in following steps: 
                                                               1. All root tag nodes of the DOM trees are merged to form 
                                                                  the first (the top most) element node of the CST. We 
                                                                  have Tag = root, Attr- {}, and TAGs being the set of all 
                                                                  the root tag nodes in the DOM trees of the Web pages. 
                                                               2. We compute STYLEs of the element node E passed 
                                                                  from the previous step. E.STYLEs is the set of presen•
                                                                  tation styles of all the tag nodes in the DOM trees 
                                                                  covered by E. Note that common presentation styles are 
                                                                  combined. 


   An element node (the basic information unit) of a 
compressed structure tree (CST) is defined as follows: 
Definition (element node): An element node E represents 
  a set of merged tag nodes in the DOM tree. It has 5 
  components, denoted by (Tag, Attr, TAGs, STYLEs, 
  CHILDs), where 
  • Tag is the tag name; 
  • Attr is the set of display attributes of Tag. 
  • TAGs is the set of actual tag nodes in the original 
     DOM trees that are compressed (or merged) in E. 


Al AND THE INTERNET                                                                                                    45                                                                Definition: For an element node E in a CST, the path 
 4. If no child element node is created in step 3, stop; else    importance of E, denoted by Pathlmp(E), is: 
    for each child element node from step 3, goto step 2. 
 After the CST is built, it is used to compute a weight for 
 each word feature in each block of a Web page. 

 3.3 Weighting policy                                          Our weighting policy considers both the structure and 
 Intuitively, if an element node contains many different       content context of the features. The weight of each feature 
 presentation styles, it is more likely to be important and    in a particular block (or under a particular tag) of the Web 
 hence should be assigned a high weight. Otherwise, it will    page is computed as follow. 
 be assigned a low weight, i.e., it is more likely to be noisy. 
   We use the entropy of presentation styles to encode the 
 importance of an element node E in the CST. 
 Definition: For an internal element node E in CST, let / = 
  \E.STYLEs\ and m = IE.TAGs\. The importance of E, 
  denoted by Nodelmp(E), is 

                                                               In our implementation, we do not use the actual leaf tag 
                                                               nodes in a page as they overly fragment the page. Instead, 
                                                               we use the grandparent tag nodes of the actual leaf tag 
  where p, is the probability that a tag node in E. TAGs uses 
                                                               nodes in the DOM tree as the (virtual) leaf tag nodes in 
  the ith presentation style. 
                                                               building CST, and in computing feature weights. 
                                                                 After a weight to each feature in every block of a Web 
                                                               page is assigned, we add the weights of the same feature in 
                                                               the page. All the feature weights of the page together form 
                                                               a feature vector of the page, which is used as input to Web 
                                                               mining, e.g., clustering and classification. 

                                                               4 Empirical Evaluation 
                                                               This section evaluates the proposed cleaning technique. 
                                                               Since the purpose of our data cleaning is to improve Web 
                                                               mining, we performed two mining tasks, i.e., clustering 
                                                               and classification, to test our system. By comparing the 
                                                               mining results before cleaning and after cleaning, we show 
                                                               that our Web cleaning technique is able to boost mining 
                                                               accuracy dramatically. 
                                                                 Below, we first describe the datasets used in our ex•
                                                               periments and the evaluation measures. We then present 
                                                               our experimental results of clustering and classification. 
                                                               4.1 Datasets and evaluation measures 


Nodelmp(E) only evaluates local importance of E. In order 
to weigh a feature contained in a leaf node of a CST, we 
use the cumulative importance of the path from root to the 
node containing the feature. We call this cumulative im•
portance of E the path importance, denoted by Pathlmp(E). 
PathImp(E) measures the importance of the structure from 
root to E in CST. 
   Since the path importance is a cumulative importance 
value, it should meet the following requirement: 

• For any two element nodes E\ and E2 in CST, if E\ is an 


46                                                                                                Al AND THE INTERNET                                        Figure 3: The distribution of F scores of clustering 
suits (in both clustering and classification).                [Anderberg, 1973]. We put all the 5 categories of Web 
  All the five Web sites contain Web pages of many            pages into a big set, and use the K-means clustering algo•
categories or classes of products. We choose the Web          rithm to cluster them into 5 clusters. Since the K-means 
pages that focus on the following categories of products:     algorithm selects the initial seeds randomly, we performed 
Digital Camera, Notebook, TV, Printer and Mobile Phone.       a large number of experiments (800) to show the behaviors 
Table 1 lists the number of documents that we downloaded      of K-means before and after page cleaning. The F score for 
                                                              each of the 800 runs is drawn in Figure 3, where X-axis 
                                                              shows the experiment number and Y-axis shows the F 
                                                              score. F score for each run is the average value of the 5 
                                                              classes, which is computed as follow: By comparing the 
                                                              pages' original classes and the clustering results, we find 
                                                              the optimal assignment of class to each cluster that gives 
                                                              the best average F score for the 5 classes. 
                                                                 From Figure 3, we can clearly observe that after clean•
                                                              ing the results are drastically better. Over the 800 runs, the 
    Table 1: Web pages and their classes from the 5 sites 
                                                              average F score for the noise case is 0.506, while the av•
from each Web site, and their corresponding classes.          erage F score for the clean case is 0.756, which is a 
  Since we test our system using clustering and classifi•     remarkable improvement. 
cation, we use the popular F score measure in information        After cleaning, 84.38% of the 800 results have F scores 
retrieval to evaluate the results before and after cleaning. F higher than 0.7, and only 4.63% lower than 0.5. Before 
score is defined as follows:                                  cleaning, only 0.5% of the 800 results (4 out of 800) have 
                                                              F scores higher than 0.7, and 47.63% lower than 0.5. Thus, 
where P is the precision and r is the recall. F score meas-   we can safely conclude that our feature weighting tech•
ures the performance of a system on a particular class, and   nique is highly effective for clustering. 
it reflects the average effect of both precision and recall.  Classification 
We will also include the accuracy results for classification. 
                                                              For classification, we use Support Vector Machine 
4.2 Experimental results                                      (SVM), which has been shown to perform very well for 
We now report our experimental results before and after       text classification by many researchers (e.g., [Joachims 
cleaning. Cleaning is done on all the Web pages from each      1997]). We used the SVMlight package [Joachims, 1999] 
Web site separately using the proposed technique.             for all our experiments. 
                                                                 In each experiment, we build classifiers based on two 
Clustering                                                    classes of pages from different Web sites. For example, the 
For clustering, we use the popular k-means algorithm          two classes can be camera pages from Amazon and 


Al AND THE INTERNET                                                                                                   47 