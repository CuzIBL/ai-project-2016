                 Optimal Time-Space Tradeoff in Probabilistic Inference 

                                   David Allen and Adnan Darwiche 
                                         University of California 
                                         Los Angeles, CA 90095 
                                     {dlallen,darwiche}@cs.ucla.edu 

                     Abstract 

    Recursive Conditioning, RC, is an any-space al•
    gorithm lor exact inference in Bayesian networks, 
    which can trade space for time in increments of the 
    size of a floating point number. This smooth trade•
    off' is possible by varying the algorithm's cache 
    size. When RC is run with a constrained cache 
    size, an important problem arises: Which specific 
    results should be cached in order to minimize the 
    running time of the algorithm? RC is driven by a 
    structure known as a dtree, and many such dtrees 
    exist for a given Bayesian network. In this paper, 
    we examine the problem of searching for an opti•
    mal caching scheme for a given dtree, and present 
    some optimal time-space tradeoff curves for given               Figure 1: An example dtree. 
    dtrees of several published Bayesian networks. We 
    also compare these curves to the memory require•     We approach this problem by formulating it as a system•
    ments of state-of-the-art algorithms based on join- atic search problem. We then use the developed method 
    trees. Our results show that the memory require•   to construct time-space tradeoff curves for some real-world 
    ments of these networks can be significantly re•   Bayesian networks, and put these curves in perspective by 
    duced with only a minimal cost in time, allowing   comparing them to the memory requirements of state-of-
    for exact inference in situations previously imprac• the-art methods based on jointrees [Jensen et a/., 1990; 
    tical. They also show that probabilistic reasoning Shafer and Shenoy, 1990]. The curves produced illustrate that 
    systems can be efficiently designed to run under   a significant amount of memory can be reduced with only a 
    varying amounts of memory.                         minimal cost in time. In fact, for much of their domains, the 
                                                       time-space curves we produce appear close to linear, with ex•
                                                       ponential behavior appearing only near the extreme case of no 
1 Introduction                                         caching. This dramatic space reduction, without a significant 
                                                       time penalty, allows one to practically reason with Bayesian 
Recursive Conditioning, RC, was recently proposed as an networks that would otherwise be impractical to handle. 
any-space algorithm for exact inference in Bayesian net•
                                                         This paper is structured as follows. We start in Section 2 
works [Darwiche, 2001]. The algorithm works by using con•
                                                       by providing some background on recursive conditioning and 
ditioning to decompose a network into smaller subnetworks 
                                                       the cache allocation problem. We then formulate this prob•
that are then solved independently and recursively using RC. 
                                                       lem in Section 3 as a systematic search problem. Time-space 
It turns out that many of the subnetworks generated by this 
                                                       tradeoff curves for several published Bayesian networks are 
decomposition process need to be solved multiple times re•
                                                       then presented in Section 4. Finally, in Section 5, we provide 
dundantly, allowing the results to be stored in a cache after 
                                                       some concluding remarks and discuss some future work. 
the first computation and then subsequently fetched during 
further computations. This gives the algorithm its any-space 
behavior since any number of results may be cached. This 2 Any-Space Inference 
also leads to an important question, which is the subject of The RC algorithm for exact inference in Bayesian networks 
this paper: "Given a limited amount of memory, which re• works by using conditioning and case analysis to decompose 
sults should be cached in order to minimize the running time a network into smaller subnetworks that are solved indepen•
of the recursive conditioning algorithm?"              dently and recursively. The algorithm is driven by a structure 


PROBABILISTIC INFERENCE                                                                               969  known as a decomposition tree (dtree), which controls the de•
 composition process at each level of the recursion. We will 
 first review the dtree structure and then discuss RC. 
 2.1 Dtrees 
 Definition 1 [Darwiche, 2001] A dtree for a Bayesian net•
 work is a full binary tree, the leaves of which correspond to 
the network conditional probability tables (CPTs). If a leaf 
node t corresponds to a CPT then vars(t) is defined as the 
variables appearing in CPT 
   Figure 1 depicts a simple dtree. The root node t of the dtree 
represents the entire network. To decompose this network, 
the dtree instructs us to condition on variable B, called the 
cutset of root node t. Conditioning on a set of variables leads 
to removing edges outgoing from these variables, which for a 
cutset is guaranteed to disconnect the network into two sub•
networks, one corresponding to the left child of node and 
another corresponding to the right child of node see Fig•
ure 1. This decomposition process continues until a boundary 
condition is reached, which is a subnetwork that has a single 
variable. 
  We will now present some notation needed to define ad•
ditional concepts with regard to a dtree. The notation and 
   will be used for the left child and right child of node 
and the function vars will be extended to internal nodes 
vars(t) vars Each node in a dtree has three            or directly by using the hMeTiS [Karypis and Kumar, 1998] 
more sets of variables associated with it. The first two of these hypergraph partitioning program. 
sets are used by the RC algorithm, while the third set is used 
to analyze the complexity of the algorithm.            2.2 Recursive Conditioning 
                                                         Given a Bayesian network and a corresponding dtree with 
                                                       root t, the RC algorithm given in Algorithms 1 and 2 can be 
                                                       used to compute the probability of evidence c by first "record•
                                                       ing" the instantiation e and then calling RC(/), which returns 
                                                       the probability of e. 
                                                         Our main concern here is with Line 5 and Line 13 of the 
                                                       algorithm. On Line 5, the algorithm checks whether it has 
                                                       performed and cached this computation with respect to the 
The width of a dtree is the size of its maximal cluster —1. subnetwork associated with node /,. A computation is charac•
  The cutset of a dtree node t is used to decompose the net• terized by the instantiation of Vs context, which also serves 
work associated with node /, into the smaller networks asso• as an index into the cache attached to node /. If the com•
ciated with the children of t. That is, by conditioning on vari• putation has been performed and cached before, its result is 
ables in cutset(f), one is guaranteed to disconnect the net• simply fetched. Otherwise, the computation is performed and 
work associated with node t. The context of dtree node t its result is possibly cached on Line 13. 
is used to cache results: Any two computations on the net• When every computation is cached, RC uses 0(n exp(w)) 
work associated with node t will yield the same result if these space and 0(n exp(iu)) time, where n is the number of nodes 
computations occur under the same instantiation of variables in the network and w is the width of the dtree. This cor•
in context(f). Hence, a cache is associated with each dtree responds to the complexity of jointree algorithm, assuming 
node t, which stores the results of such computations (proba• that the dtree is generated from a jointree [Danviche, 2001]. 
bilities) indexed by instantiations of context(f). This means When no computations are cached, the memory requirement 
that the size of a cache associated with dtree node t can grow of RC is reduced to 0{n), in which case the time requirement 
as large as the number of instantiations of context(t). increases to 0(nexp(w logn)). Any amount of memory be•
  For a given Bayesian network, many different dtrees exist tween these two extremes can also be used in increments of 
and the quality of the dtree significantly affects the resource the size of a floating point number, a cache value. 
requirements of RC. The width is one important measure of Suppose now that the available memory is limited and we 
this, as RCs time complexity is exponential in this value. can only cache a subset of the computations performed by 
The construction of dtrees is beyond the scope of this paper, RC. The specific subset that we cache can have a dramatic ef•
but in [Danviche, 2001; Danviche and Hopkins, 2001] it was fect on the algorithm's running time. A key question is then to 
shown how to create them from elimination orders, jointrees, choose that subset which minimizes the running time, which 


970                                                                             PROBABILISTIC INFERENCE              Figure 2: An example dgraph.               Figure 3: Search tree for a dgraph with 3 internal nodes. 

                                                         One can count the number of recursive calls made by RC 
is the main objective of this paper. We refer to this as the 
                                                       (and, hence, compute its running time) given any discrete 
secondary optimization problem, with the first optimization 
                                                       cache factor. Specifically, if denotes a parent of node f 
problem being that of constructing an optimal dtree. 
                                                       in a dgraph, and denotes the number of instantiations of 
  Most of our results in this paper are based on a version of 
                                                       variables S, the number of recursive calls made to node t is 
RC which not only computes the probability of evidence e, 
                                                       LDarwiche, 2001; 2002]: 
but also posterior marginals over families and, hence, poste•
rior marginals over individual variables. This version of RC 
uses a decomposition graph (dgraph), which is basically a set 
of dtrees that share structure. 
  An example dgraph for a network with four variables can 
be seen in Figure 2. It should be noted that each of the four If the cache factor is not discrete, the above formula gives the 
root nodes corresponds to a valid dtree, so this dgraph ac• average number of recursive calls, since the actual number of 
tually contains four dtrees which share a significant portion calls will depend on the specific computations cached. This 
of their structure. Creation of dgraphs is discussed in LDar- equation is significant as it can be used to predict the expected 
wiche, 2002].                                          time requirement of RC under a given caching scheme. 
  The code in Algorithms 1 and 2 is also used in this ver• We focus in this paper on searching for an optimal dis•
sion of RC, where RC(r) is called once on each root f of the crete cache factor, given a limited amount of memory, where 
dgraph (the posterior marginal of each family is computed optimality is with respect to minimizing the number of recur•
as a side effect of each of these calls). This version of RC sive calls. To this end, we will first define a search problem 
uses more memory as it maintains more caches. But it is for finding an optimal discrete cache factor and then develop 
more meaningful when it comes to comparing our time -space a depth-first branch-and-bound search algorithm. We will 
tradeoff curves with the memory requirements of jointree al• also use the developed algorithm to construct the time-space 
gorithms, as this version of RC is equally powerful to these tradeoff curves for some published Bayesian networks from 
algorithms.                                            various domains, and compare these curves to the memory 
                                                       demands and running times of jointree algorithms. 
3 The Cache Allocation Problem                         3.1 Cache Allocation as a Search Problem 
The total number of computations that a dgraph (or dtree) The cache allocation problem can be phrased as a search 
node t needs to cache equals the number of instantiations of problem in which states in the search space correspond to 
context(t). Given a memory constraint, however, one may partial cache factors that do not violate the given memory 
not be able to cache all these computations, and we need a constraint, and where an operator extends a partial cache fac•
way to specify which results in particular to cache. A cache tor by making a caching decision on one more dgraph node. 
factor cf for a dgraph is a function which maps each internal The initial state in this problem is the empty cache factor, in 
node t in the dgraph into a number cf (t) between 0 and 1. which no caching decisions have been made for any nodes 
Hence, if cf(t) = .75, then node t can only cache 75% of in the dgraph. The goal states correspond to complete cache 
these total computations. A discrete cache factor is one which factors, where a caching decision has been made for every 
maps every internal dgraph node into either 1 or 0: all of the dgraph node, without violating the given memory constraint. 
node's computations are cached, or none are cached. The RC Suppose for example that we have a dgraph with three inter•
code in Algorithms 1 and 2 assumes a discrete cache factor, nal nodes This will then lead to the search tree in 
which is captured by the flag cache?(£), indicating whether Figure 3. In this figure, each node n in the search tree repre•
caching will take place at dgraph node t.              sents a partial cache factor For example, the node in bold 


PROBABILISTIC INFERENCE                                                                                971  corresponds to the partial cache factor               unmanageable very quickly. Hence the search algorithm must 
 and Moreover, if node is labeled with a dgraph        eliminate portions of the search space while still being able to 
 node i;, then the children of represent two possible exten• guarantee an optimal result. One of the key methods of doing 
 sions of the cache factor one in which dgraph node    this is by pruning parts of the search tree which are known to 
 will cache all computations (1—child), and another in which contain non-optimal results. The DFBnB algorithm does this 
 dgraph node will cache no computations (0—child).     by pruning search tree nodes when the cost function is 
   According to the search tree in Figure 3, one always makes larger than or equal to the current best solution. Hence, more 
a decision on dgraph node followed by a decision on    accurate cost functions will allow more pruning. Another ma•
dgraph node to, and then node £3. A fixed ordering of dgraph jor source of pruning is the given constraint on total memory. 
nodes is not necessary, however, as long as the following con• This is accomplished by pruning a search tree node and all 
dition is met: A decision should be made on a dgraph node its descendants once it attempts to assign more memory to 
only after decisions have been made on all its ancestors in the caches than is permitted by the memory constraint. 
dgraph. We will explain the reason for this constraint later on 
when we discuss cost functions.                        3.4 Search Decisions 
  In the search tree depicted in Figure 3, the leftmost leaf Now that we have chosen a cost function, we are still left with 
represents no caching, while the rightmost leaf represents full two important choices in our search algorithm: (1) which 
caching. The search trees for this problem have a maximum child of a search tree node to expanded first, and (2) in what 
depth of c/, where d is the number of internal nodes in the order to visit dgraph nodes during search. Expanding the 1-
dgraph. Given this property, depth-first branch-and-bound child first is a greedy approach, as it attempts to fully cache 
search is a good choice given its optimality and linear space at a dgraph node whenever possible. Results on many dif•
complexity [Papadimitriou and Steiglitz 1998]. It is also an ferent networks have shown that in many cases, expanding 
anytime algorithm, meaning that it can always return its best the 1—child before the 0-child appears to be equal to or bet•
result so far if interrupted, and if run to completion will return ter than the opposite, and it is this choice that we adopt in 
the optimal solution. Hence, we will focus on developing a our experiments. The specific order in which we visit dgraph 
depth-first branch-and-bound search algorithm.         nodes in the search tree turns out to have an even more dra•
                                                       matic effect on the efficiency of search. Even though wc make 
3.2 Cost Functions 
                                                       caching decisions on parent dgraph nodes before their chil•
The depth-first branch-and-bound (DFBnB) algorithm re• dren, there is still a lot of flexibility. Our experimentation on 
quires a cost function / which assigns a cost to every many networks has shown that choosing the dgraph node t 
node in the search tree. The function estimates the    with the largest context(t) is orders of magnitude more ef•
cost of an optimal solution that passes through n. The key ficient than some other basic ordering heuristics [Allen and 
here is that must not overestimate that cost; otherwise, Darwiche, 2002]. This choice corresponds to choosing the 
one loses the optimality guarantee offered by the search algo• dgraph node with the largest cache, and it is the one we use 
rithm. We will now develop such a cost function based in our search algorithm. 
on the following observations. Since each node represents a 
partial cache factor function must estimate the num•
ber of recursive calls made to RC based on an optimal com• 4 Time-Space Tradeoff 
pletion of cache factor Consider now the completion    The main goal of this section is to present time-space tradeoff 
of in which we decide to cache at each dgraph node that curves for a number of benchmark Bayesian networks, some 
   did not make a decision on. This cache factor is the of which are obtained from [Bayesian Network Repository, 
best completion of from the viewpoint of running time, URL] and others are included in the distributions of [Hugin 
but it may violate the constraint given on total memory. Yet, Expert, URL; GeNle, URL]. The main points to observe with 
we will use it to compute as it guarantees that ) will respect to each curve is the slope of the curve, which pro•
never overestimate the cost of an optimal completion of vides information on the time penalty one pays when reduc•
  One important observation in this regard is that once the ing space in probabilistic inference. The second main point 
caching decision is made on the ancestors of dgraph node is to compare the produced curves with the time and space 
we can compute exactly the number of recursive calls that requirement of jointree methods, as the version of RC we 
will be made to dgraph node (see Equation 1). Therefore, are using provides the same functionality as these algorithms 
when extending a partial cache factor, we will always insist (that is, probability of evidence and posterior marginals over 
on making a decision regarding a dgraph node t for which families and variables). This baseline comparison is impor•
decisions have been made on all its ancestors. This improves tant as it places our results in the context of state-of-the-art 
the quality of the estimate as gets deeper in the tree. It inference systems. 
also allows us to incrementally compute this estimate based Time-space tradeoff curves. Figures 4, 5, and 6 depict 
on the estimate of 's parent in the search tree.       optimal discrete time-space tradeoff curves for three net•
                                                       works. These curves were generated as follows. A join-
3.3 Pruning 
                                                      tree was first generated for the network using Hugin.1 The 
As depicted by the search tree in Figure 3, there is potentially 
an exponential number of goal nodes in the search tree and 'We used Hugin's default setting: the minimum fill-in weight 
the combinatorial explosion of exhaustive search can become heuristic in conjunction with prime component analysis. 


972                                                                             PROBABILISTIC INFERENCE                                                                Figure 6: Time-space tradeoff on Water. 

                                                       A number of observations are in order here. First, these time-
                                                       space tradeoff curves show that the amount of memory used 
                                                       by RC under full caching is very close to that required by 
                                                       the Shenoy-Shafer architecture (the Hugin architecture takes 
                                                       much more space). Second, the curves show that a signifi•
                                                       cant amount of memory can sometimes be reduced from full 
                                                       caching with only a limited increase in the time required; in 
                                                       fact, the exponential growth appears to be occurring only near 
                                                       the lower extreme of no caching. The space requirement for 
                                                       Water (Figure 6), for example, can be reduced to 30% while 
                                                       only increasing the running time by a factor of 6. Moreover, 
                                                       the space requirements for B (Figure 5) can be reduced to 
                                                       about 2.5%; while increasing the running time by a factor of 
                                                       20. Finally, we note that each optimal search for the Water 
                                                       and B networks took less than two seconds, and each opti•
          Figure 5: Time-space tradeoff on B.          mal search for Alarm took less than two minutes. We stress 
                                                       though that such searches need to be done only once for a 
                                                       network, and their results can then be used for many further 
jointree was then converted into a dtree as described in [Dar- queries. 
wiche, 2001]. The dtree was finally converted into a dgraph 
                                                         Non-optimal tradeoffs. On some networks, the search 
as described in the full paper. Two sets of results were then 
                                                       space is too large to solve optimally using our search algo•
generated: 
                                                       rithm, but the anytime nature of the algorithm allows us to 
    We computed the space requirements for jointree algo• interrupt the search at any point and ask for the best result 
    rithms, using both the Hugin [Jensen et al., 1990] and obtained thus far. Figures 7 and 8 were generated by allow•
    Shenoy-Shafer iShaferand Shenoy, 1990] architectures. ing the search to run for an hour. Even though these curves 
    For the first architecture, we assumed one table for each are not optimal, they are useful practically. For example, ac•
    clique and one table for each separator. For the sec• cording to these curves, the memory requirement of Barley 
    ond, we assumed two tables for each separator (no ta• can be reduced from about 22 MB to about 6.5 MB while 
    bles for cliques). We also performed propagation on the only increasing the running time from about 1 to 4 minutes. 
    jointree using Netica [Norsys Software Corp., URL] and Moreover, the space requirement of Munin 1 can be reduced 
    recorded the running time.                         from about 370 MB to 150 MB, while increasing the running 
  • We then ran our search algorithm to find an optimal time from about 22 minutes to about 6 hours. Encouraged 
    cache factor under different memory constraints, where by such results, we are planning to investigate other (non-
    we generated 100 data points for each curve. For each optimal) search methods, such as local search. 
    caching factor that we identified, we computed the num• Dtrees vs dgraphs. Running RC on a dtree takes less 
    ber of recursive calls that will be made by RC under that space than running it on a dgraph, but produces much less 
    factor and converted the calls to seconds.2 
                                                       computer with 256 MB of RAM, makes an average number of three 
   2Our Java implementation of RC on a Sun Ultra 10, 440 MHz million recursive calls per second. 


PROBABILISTIC INFERENCE                                                                               973 