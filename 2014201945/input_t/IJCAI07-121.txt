                Learning Classiﬁers When The Training Data Is Not IID

                   Murat Dundar, Balaji Krishnapuram, Jinbo Bi, R. Bharat Rao
               Computer Aided Diagnosis & Therapy Group, Siemens Medical Solutions,
                          51 Valley Stream Parkway, Malvern, PA-19355, USA
  {murat.dundar, balaji.krishnapuram, jinbo.bi, bharat.rao}@siemens.com


                    Abstract                          we do not appreciate the beneﬁts of modeling these corre-
                                                      lations, and the ease with which this can be accomplished
    Most methods for classiﬁer design assume that the algorithmically. For motivation, consider computer aided di-
    training samples are drawn independently and iden- agnosis (CAD) applications where the goal is to detect struc-
    tically from an unknown data generating distribu- tures of interest to physicians in medical images: e.g. to iden-
    tion, although this assumption is violated in several tify potentially malignant tumors in CT scans, X-ray images,
    real life problems. Relaxing this i.i.d. assumption, etc. In an almost universal paradigm for CAD algorithms,
    we consider algorithms from the statistics litera- this problem is addressed by a 3 stage system: identiﬁcation
    ture for the more realistic situation where batches of potentially unhealthy candidate regions of interest (ROI)
    or sub-groups of training samples may have inter- from a medical image, computation of descriptive features
    nal correlations, although the samples from dif-  for each candidate, and classiﬁcation of each candidate (e.g.
    ferent batches may be considered to be uncorre-   normal or diseased) based on its features. Often many candi-
    lated. Next, we propose simpler (more efﬁcient)   date ROI point to the same underlying anatomical structure at
    variants that scale well to large datasets; theoretical slightly different spatial locations.
    results from the literature are provided to support Under this paradigm, correlations clearly exist among both
    their validity. Experimental results from real-life the features and the labels of candidates that refer to the same
    computer aided diagnosis (CAD) problems indi-     underlying structure, image, patient, imaging system, doc-
    cate that relaxing the i.i.d. assumption leads to sta- tor/nurse, hospital etc. Clearly, the candidate ROI acquired
    tistically signiﬁcant improvements in the accuracy from a set of patient images cannot be assumed to be IID.
    of the learned classiﬁer. Surprisingly, the simpler Multiple levels of hierarchical correlations are commonly ob-
    algorithm proposed here is experimentally found to served in most real world datasets (see Figure 1).
    be even more accurate than the original version.
                                                      1.2  Relationship to Previous Work
1  Introduction                                       Largely ignored in the machine learning and data mining lit-
Most classiﬁer-learning algorithms assume that the training erature, the statistics and epidemiology communities have de-
data is independently and identically distributed. For ex- veloped a rich literature to account for the effect of correlated
ample, support vector machine (SVM), back-propagation for samples. Perhaps the most well known and relevant mod-
                                                                                                      [ ]
Neural Networks, and many other common algorithms im- els for our purposes are the random effects model (REM) 3 ,
                                                                                                      [ ]
plicitly make this assumption as part of their derivation. Nev- and the generalized linear mixed effects models (GLMM) 7 .
ertheless, this assumption is commonly violated in many real- These models have been mainly studied from the point of
life problems where sub-groups of samples exhibit a high de- view of explanatory data analysis (e.g. what is the effect of
gree of correlation amongst both features and labels. smoking on the risk of lung cancer?) not from the point of
  In this paper we: (a) experimentally demonstrate that ac- view of predictive modeling, i.e. accurate classiﬁcation of un-
counting for the correlations in real-world training data leads seen test samples. Further, these algorithms tend to be com-
to statistically signiﬁcant improvements in accuracy; (b) pro- putationally impractical for large scale datasets that are com-
pose simpler algorithms that are computationally faster than monly encountered in commercial data mining applications.
previous statistical methods and (c) provide links to theoreti- In this paper, we propose a simple modiﬁcation of existing al-
cal analysis to establish the validity of our algorithm. gorithms that is computationally cheap, yet our experiments
                                                      indicate that our approach is as effective as GLMMs in terms
1.1  Motivating example: CAD                          of improving classiﬁcation accuracy.
Although overlooked because of the dominance of algorithms
that learn from i.i.d. data, sample correlations are ubiqui- 2 Intuition: Impact of Sample Correlations
tous in the real world. The machine learning community fre- Simpliﬁed thought experiment Consider the estimation of
quently ignores the non-i.i.d. nature of data, simply because the odds of heads for a biased coin, based on a set of ob-

                                                IJCAI-07
                                                   756                        Figure 1: A Mixed effects model showing random and ﬁxed effects

servations. If every observation is an independent ﬂip of a dates that are spatially close in the image; i.e., all these can-
coin, then this corresponds to an i.i.d. assumption for the data; didates will refer to the same underlying physiological region
based on this assumption one can easily build estimators for (e.g. the same lung nodule). Moreover, some other types of
our binomial data. This would work ﬁne so long as the obser- structures (e.g. non-wall attached nodules) may be associated
vations reported to the statistician are really true to the under- with only one or two candidates, again due to the fundamen-
lying coin ﬂips, and provided the underlying data generating tal properties and biases of the candidate generation (CG) al-
mechanism was truly binomial (and not from some other dis- gorithm. The occurrence frequency & other statistical prop-
tribution that generates “outliers” as per the binomial model). erties of the underlying structural causes of the disease are
  However, suppose that the experimenter performing the ex- systematically altered if we naively treated all data as being
periment reports something else to the statistician. After cer- produced from i.i.d. data sources. As a result, a systematic
tain coin ﬂips, the experimenter reports the result of one coin bias is introduced into the statistical classiﬁer estimation al-
ﬂip observation as if it occurred many times (but only if he gorithm that learns to diagnose diseases, and this bias remains
observed “heads” on those occasions). For other observations even when we have large amounts of training data.
he reports the occurrences exactly once. Then the simple bi- When does the violation of i.i.d. assumptions not matter?
nomial estimator (designed using the i.i.d. assumption) would Clearly, if the correlations between samples is very weak,
not be appropriate. On the other hand, if every occurrence of we can effectively ignore them, treating the data as i.i.d. .
an i.i.d. sample is repeated the same number of times, then Further, even if the correlations are not weak, if we do not
we are essentially immune to this effect. Although this exam- much care for outlier immunity, and if each sub-type or sub-
ple may seem simplistic, one should bear in mind that logistic population occurs with similar or almost identical frequency,
regression, Gaussian Processes and most other classiﬁers es- then we should be able to ignore these effects.
sentially rely on the same binomial distribution to derive the
likelihood that they maximize in the training step.   3   Random & Mixed Effects Models
  Implications for Classiﬁer Learning The implicit as- Consider the problem shown in Figure 1. We are given a
                                                                                                   d
sumption in the machine learning community seems to be that training dataset D = {(xijk ,yijk)},wherexijk ∈ is the
even if IID assumptions are violated, the algorithms would feature vector for the ith candidate in the jth patient of the
                                                       th
work well in practice. When would this not be the case? k hospital and yijk ∈{−1, 1} are class labels. Let us also
  The ﬁrst intuition is that outliers (e.g. mis-labeled sam- assume without loss of generality that the indexing variable
ples), do not systematically bias the estimation of the clas- follow i = {1,...,j},j= {1,...,pk},k= {1,...,K}.
siﬁer during training, provided they are truly i.i.d. and drawn In generalized linear models (GLM) [7] such as logistic or
from a fairly symmetric distribution. These outliers introduce probit regression, one uses a nonlinear link function—e.g. the
a larger variance in the estimation process, but this can be logistic sigmoid function σ(r)=1/(1 + exp(−r)))—to ex-
largely overcome simply by increasing the sample sizes of the press the posterior probability of class membership as:
training set. On the other hand, if outliers (and samples) are P y      |x   ,α   σ αx     α   .
systematically correlated, they do introduce a systemic bias    ( ijk =1  ijk  )=   (   ijk + 0)      (1)
in the estimation process, and their effect remains even if we However, this mathematically expresses conditional inde-
have a large amount of training data.                 pendence between the classiﬁcation of samples and ignores
  For explaining the second intuition, let us ﬁrst consider a the correlations between samples from the same patient (j, k)
practical situation occurring in CAD problems. Due to the or from the same hospital k. This limitation is overcome in a
way the candidate generation (CG) algorithms for identify- generalized linear mixed effects model (GLMM) [7] by postu-
ing ROI are designed, some diseased structures (e.g. wall at- lating the existence of a pair of random variables that explain
tached nodules in a lung) may be identiﬁed by many candi- the patient speciﬁc effect δj,k and a hospital speciﬁc effect

                                                IJCAI-07
                                                   757δk. During training, one not only estimates the ﬁxed effect First we deﬁne some convenient notation (which will use
parameters of the classiﬁer α, but also the distribution of the bold font to distinguish it from the rest of the paper). Let us
random effects p(δk,δj,k|D). The class prediction becomes: denote the vector combining α, α0 and all the random effect
                                                      variables as
  P (yijk =1|xijk,α)=                                                                           
                                                           w  =[α ,α0,δ1,δ2,...,δK ,δ11,...,δpK ,K ] .
      σ αx     α    δ    δ  p δ ,δ   dδ dδ  .
       (   ijk +  0 + k + j,k) ( k j,k) k  j,k  (2)   Let us deﬁne the augmented vector combining the fea-
                                                      ture vector x, the unit scalar 1 and a vector 0 of zeros—
  In Bayesian terms, classiﬁcation of a new test sample x whose length corresponds to the number of random effect
with a previously trained GLM involves marginalization over variables—as x =[x, 1, 0]. Next, observe that if one
the posterior of the classiﬁer’s ﬁxed effect parameters: only requires a hard classiﬁcation (as in the SVM literature)
                                                     the sign(•) function is to be used for the link. Finally, note
                       
  E[P (y =1|x)] =  σ(α x + α0)p(α, α0|D)dαdα0.  (3)   that the classiﬁcation decision in (5) can be expressed as:
                                                      E[P (y =1|x)] =  sign(wx)p(w|D)dw.
The extension to ﬁnd the classiﬁcation decision on a test sam- Lemma 1 For any sample x, the hard classiﬁcation decision
                                                                        f  x, w        w x
ple in a GLMM is quite straight-forward:              of a Point Classiﬁer PC(  )=sign(     ) is identical to
                                                      that of a Bayesian Voting Classiﬁcation
                                                                                              
  E[P (yijk =1|xijk)] =
                                                            f    x,q                wx q w  dw
                                                             BVC(   )=sign     sign(   ) (  )        (6)
       σ(α xijk + α0 + δk + δj,k)p(α, α0,δk,δj,k|D)
                                                      if q(w)=q(w|w ) is a symmetric distribution with respect to
                                dαdα0dδkdδj,k.  (4)
                                                      w , that is, if q(w|w )=q(w |w ),wherew ≡ 2 w − w is the
Notice that the classiﬁcation predictions of sets of samples symmetric reﬂection of w about w .
                    j, k                      k
from the same patient ( ) or from the same hospital are Proof: For every w in the domain of the integral, w is also
no longer independent. During the testing phase, if one wants in the domain of the integral. Since w =2w − w,wesee
                                                                        
to classify samples from a new patient or a new hospital not that wx + w x =2w x. Three cases have to be considered:
seen during training (so the appropriate random effects are             wx       −     w x     wx
not available), one may still use the GLM approach, but rely Case 1: When sign( )=  sign(   ),or       =
                                                          w x                            w     w
on a marginalized version of the posterior distributions learnt =0, the total contribution from and to the
for the GLMM:                                             integral is 0,since
                                                                                     
                                                              sign(w x)q(w|w )+sign(w x)q(w |w )=0.
                       
  E P y    |x      σ  α x  α0 p α, α0,δk,δj,k|D
   [ (  =1   )] =    (   +   ) (              )                                                     
                                                      Case 2: When sign(wx)=sign(w  x),sincewx+w x =
                                                                                    
                                dαdα0dδkdδj,k.  (5)       2w x, it follows that sign(w x)=sign(wx)=
                                                               w x
A note of explanation may be useful to explain the above  sign(   ). Thus,
                                δ      δ                                                           
equation. The integral in (5) is over k and j,k, for all j            wx q w|w        w x q w |w
and k, i.e. we are marginalizing over all the random effects in sign sign( ) (  )+sign(     ) (    )
                                                                                                   
order to obtain p(α, α0|D) and then relying on (2). Clearly,                               =sign(w x).
training a GLMM amounts to the estimation of the posterior
                                                                                               
distribution, p(α, α0,δk,δj,k|D).                     Case 3: When w  x =0, w x =0 or w x =0 , w x =0,
                                                                                          
                                                          we have again from wx + w x =2w x that
3.1  Avoiding Bayesian integrals                                                                   
                                                                      wx q w|w        w x q w |w
Since the exact posterior can not be determined in analytic  sign sign(   ) (   )+sign(     ) (    )
                                                                                                   
form, the calculation of the above integral for classifying ev-                            =sign(w x).
ery test sample can prove computationally difﬁcult. In prac-
tice, we can use Markov chain monte carlo (MCMC) meth-  Unless w =0(in which case the classiﬁer in undeﬁned),
ods but they tend to be too slow for data mining applica- case 2 or 3 will occur at least some times. Hence proved. 
tions. We can also use approximate strategies for computing Assumptions & Limitations of this approach: This Lemma
Bayesian posteriors such as the Laplace approximation, vari- suggests that one may simply use point classiﬁers and avoid
ational methods or expectation propagation (EP). However, Bayesian integration for linear hyper-plane classiﬁers. How-
as we see using the following lemma (adapted from a differ- ever, this is only true if our objective is only to obtain the
ent context [5]), if this posterior is approximated by a sym- classiﬁcation: if we used some other link function to obtain a
metric distribution(e.g. Gaussian) and we are only interested soft-classiﬁcation then the above lemma can not be extended
in the strict classiﬁcation decision rather than the posterior for our purposes exactly (it may still approximate the result).
class membership probability, then a remarkable result holds: Secondly, the lemma only holds for symmetric posterior
The GLMM   classiﬁcation involving a Bayesian integration distributions p(α, α0,δk,δj,k|D). However, in practice, most
can be replaced by a point classiﬁer.                 approximate Bayesian methods would also use a Gaussian

                                                IJCAI-07
                                                   758approximation in any case, so it is not a huge limitation. Fi- the within class scattering whereas the second term penalizes
nally, although any symmetric distribution may be approxi- models α that are aprioriunlikely.
                                                                           2               2
mated by a point classiﬁer at its mean, the estimation of the Setting L(ξ|cξ)=ξ , L(α|cα)=α , i.e. assuming
mean is still required. However, for computing a Laplace ap- a zero mean Gaussian density model with unit variance for
proximation to the posterior, one simply chooses to approxi- p(ξ|cξ) and p(α|cξ) and using the augmented feature vectors
mate the mean by the mode of the posterior distribution: the we obtain the following optimization problem. Fisher’s Dis-
mode can be obtained simply by maximizing the posterior criminant with Augmented Feature Vectors (FD-AFV):
during maximum a posteriori (MAP) estimation. This is com-            +  2        − 2        2        2
                                                         min     c¯ξ+ ξ  +¯cξ− ξ  +¯cα α +¯cδ δ
putationally expedient & works reasonably well in practice. α, α0,ξ,δ,γ
                                                                                         
                                                          s.t.       ξijk + yijk = α xijk + δ x˜ijk + α0
4  Proposed Algorithm                                                       eξC =0,C∈{±}
4.1  Augmenting Feature Vectors (AFV)                 where for the ﬁrst set of constraints, k runs from 1 to K, j
In the previous section we showed that full Bayesian inte- from 1 to pk for each k, i from 1 to jk for each pair of (j, k),
                                                       C
gration is not required and a suitably chosen point classiﬁer ξ is a vector of ξijk corresponding to class C and δ is the
would achieve an identical classiﬁcation decision. In this sec- vector of model parameters for the auxiliary features.
tion, we make practical recommendations about how to ob-
tain these equivalent point classiﬁers with very little effort, 4.3 Parameter Estimation
using already existing algorithms in an original way. Depending on the sensitivity of the candidate generation
  We propose the following strategy for building linear clas- mechanism, a representative training dataset might have on
siﬁers in a way that uses the known (or postulated) correla- the order of few thousand candidates of which only few are
tions between the samples. First, for all the training samples, positive making the training data very large and unbalanced
construct augmented feature vectors with additional features between classes. To account for the unbalanced nature of the
corresponding to the indicator functions for the patient & hos- data we used different tuning parameters, c¯ξ+ , c¯ξ− for the
pital identity. In other words, to the original feature vector of positive and negative classes. To estimate these and c¯α, c¯δ,
asamplex, append a vector that is composed of zero in all we ﬁrst coarsely tune each parameter independently and de-
locations except the location of the patient-id, and another termine a range of values for that parameter. Then for each
corresponding to the location of the hospital-id, and augment parameter we consider a discrete set of three values. We use
the feature vector with the auxiliary features by, x¯ =[x x˜]. 10-fold cross validation on the training set as a performance
  Next use this augmented feature vector to train a classi- measure to ﬁnd the optimum set of parameters.
ﬁer using any standard training algorithm such as Fisher’s
Discriminant or SVMs. This classiﬁer will classify samples 5 Experimental Studies
based not only on their features, but also based on the ID of For the experiments in this section, we compare three tech-
the hospital and the patient. Viewed differently, the classiﬁer niques: naive Fisher’s Discriminant (FD), FD-AFV, and
in this augmented feature space not only attempts to explain GLMM (implemented using approximate expectation propa-
how the original features predict the class membership, but gation inference). We compare FD-AFV and GLMM against
it also simultaneously assigns a patient and hospital speciﬁc FD to see if these algorithms yield statistically signiﬁcant im-
“random-effect” explanation to de-correlate the training data. provements in the accuracy of the classiﬁer. We also study if
  During the test phase, for new patients/hospitals, the ran- the computationally much less expensive FD-AFV is compa-
dom effects may simply be ignored (implicitly this is what we rable to GLMM in terms of classiﬁer sensitivity.
used in the lemma in the previous section). In this paper we For most CAD problems, it is important to keep the num-
implement the proposed approach for Fisher’s Discriminant. ber of false positives per volume at a reasonable level, since
4.2  Fisher’s Discriminant                            each candidate that is marked as positive by the classiﬁer will
                                                      then be visually inspected by a physician. For the projects
In this section we adopt the convex programming formulation described below, we focus our attention on the region of the
of Fisher’s Discriminant(FD) presented in [8],        Receiver Operating Characteristics (ROC) curve with fewer
                                                      than 5 false positives per volume. This roughly corresponds
         min   L(ξ|cξ)+L(α|cα)
        α, α0,ξ
                                                     to 90% speciﬁcity for both datasets.
         s.t.   ξi + yi =   α xi + α0           (7)
                  eξC  =0,C∈{±}                      5.1  Experiment 1: Colon Cancer
                                                      Problem Description: Colorectal cancer is the third most
where L(z) ≡−log   p(z) be the negative log likelihood common cancer in both men and women. It is estimated that
associated with the probability density p. The constraint
                                                     in 2004, nearly 147,000 cases of colon and rectal cancer will
ξi + yi = α xi + α0 ∀i, i =(1, 2,...,) where  is the total be diagnosed in the US, and more than 56,730 people would
number of labeled samples, pulls the output for each sample die from colon cancer [4]. While there is wide consensus that
to its class label while the constraints eξC =0,C∈{±} screening patients is effective in decreasing advanced disease,
ensure that the average output for each class is the label, i.e. only 44% of the eligible population undergoes any colorectal
without loss of generality the between class scattering is ﬁxed cancer screening. There are many factors for this, key being:
to be two. The ﬁrst term in the objective function minimizes patient comfort, bowel preparation and cost.

                                                IJCAI-07
                                                   759  Non-invasive virtual colonoscopy derived from computer
                                                           0.9
tomographic (CT) images of the colon holds great promise                                      FD−AFV
as a screening method for colorectal cancer, particularly if 0.8                              FD
                                                                                              GLMM
CAD tools are developed to facilitate the efﬁciency of radiol-
                                                           0.7
ogists’ efforts in detecting lesions. In over 90% of the cases
colon cancer progressed rapidly is from local (polyp adeno- 0.6

mas) to advanced stages (colorectal cancer), which has very 0.5
poor survival rates. However, identifying (and removing) le-
sions (polyp) when still in a local stage of the disease, has 0.4

very high survival rates [1], hence early diagnosis is critical. 0.3
  This is a challenging learning problem that requires the
use of a random effects model due to two reasons. First,   0.2
the sizes of polyps (positive examples) vary from 1 mm to  0.1
all the way up to 60 mm: as the size of a polyp gets larger,
                                                            0
the number of candidates identifying it increases (these can- 0.9   0.92   0.94    0.96    0.98    1
didates are highly correlated). Second, the data is collected
from 152 patients across seven different sites. Factors such
as patient anatomy/preparation, physician practice and scan- Figure 2: Comparing FD, FD-AFV on the Colon Testing data,
                                                       FD               FD            GLMM
ner type vary across different patients and hospitals—the data pFD−AF V =0.01, pGLMM =0.07, pFD−AF V =0.18.
from the same patient/hospital is correlated.
  Dataset: The database of high-resolution CT images used
in this study were obtained from NYU Medical Center, Cleve-
land Clinic Foundation, and ﬁve EU sites in Vienna, Belgium, Table 1: Comparison of training times in seconds for FD, FD-
Notre Dame, Muenster and Rome. The 275 patients were  AFV and GLMM for PE and Colon training data
randomly partitioned into training (n=152 patients, with 126    Algorithm Time (Colon) Time (PE)
polyps among 15596 candidates) and test (n=123 patients,        FD             5          26
with 104 polyps among 12984 candidates) groups. The test        FD-AFV         7          28
group was sequestered and only used to evaluate the perfor-     GLMM          518        329
mance of the ﬁnal system. A combined total of 48 features
are extracted for each candidate.
  Experimental Results: The ROC curves obtained for the helping them detect and characterize emboli in an accurate,
three techniques on the test data are shown in Figure 2. To efﬁcient and reproducible way [9], [11].
better visualize the differences among the curves, the en- An embolus forms with complex shape characteristics in
larged views corresponding to regions of clinical signiﬁcance the lung making the automated detection very challenging.
are plotted. We performed pair-wise analysis of the three The candidate generation (CG) algorithm searches for inten-
curves to see if the difference between each pair for the area sity minima. Since each embolus is usually broken into sev-
under the ROC-curve is statistically signiﬁcant (p values com- eral smaller units, CG picks up several points of interest in
puted using the technique described in [2]). Statistical anal- the close neighborhood of an embolus from which features
ysis indicates that FD-AFV is more accurate than FD with a are extracted. Thus multiple candidates are generated while
p-value of 0.01, GLMM will be more accurate than FD with characterizing an embolus by the CG algorithm.
a p-value of 0.07 and FD-AFV will be more accurate than In addition to usual patient and hospital level random ef-
GLMM with a p-value of 0.18. The run times for each algo- fects, here we observe a more concrete example of random
rithm are shown in Table 1.                           effects in CAD; the samples within the close neighborhood
                                                      of an embolus in a patient are more strongly correlated than
5.2  Experiment 2: Pulmonary Embolism                 those far from the embolus. This constitutes a special case of
                                                      patient-level random effects. All the samples pointing to the
Data Sources and Domain Description
                                                      same PE are labeled as positive in the ground truth and are as-
Pulmonary embolism (PE), a potentially life-threatening con- signed the same PE id. We have collected 68 cases with 208
dition, is a result of underlying venous thromboembolic dis- PEs marked by expert chest radiologists at two different in-
ease. An early and accurate diagnosis is the key to survival. stitutions. They are randomly divided into two sets: training
Computed tomography angiography (CTA) has merged as an (45 cases with 142 clots generating a total of 3017 candidates)
accurate diagnostic tool for PE. However, there are hundreds and testing (23 cases with 66 clots generating a total of 1391
of CT slices in each CTA study. Manual reading is laborious, candidates). The test group was sequestered and only used
time consuming and complicated by various PE look-alikes to evaluate the performance of the ﬁnal system. A combined
(false positives) including respiratory motion artifact, ﬂow- total of 115 features are extracted for each candidate.
related artifact, streak artifact, partial volume artifact, stair
step artifact, lymph nodes, vascular bifurcation among many Experimental Design and Results:
others [6], [10]. Several Computer-Aided Detection(CAD) The ROC curves obtained for the three techniques on the test
systems are developed to assist radiologists in this process by data are shown in Figure 3. The statistical analysis indicate

                                                IJCAI-07
                                                   760