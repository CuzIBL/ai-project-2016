                     Phase Transitions within Grammatical Inference

                        Nicolas Pernot, Antoine Cornuejols´   & Michele`  Sebag
                     Laboratoire de Recherche en Informatique, CNRS UMR 8623
                                Bat.490,ˆ Universite´ de Paris-Sud, Orsay
                                      91405 Orsay Cedex (France)
                                          antoine,sebag@lri.fr

                    Abstract                          phase transition framework (PT) [Hogg et al., 1996], which
                                                      considers the satisﬁability and the resolution complexity of
    It is now well-known that the feasibility of induc- CSP instances as random variables depending on order pa-
    tive learning is ruled by statistical properties link- rameters of the problem instance (e.g. constraint density and
    ing the empirical risk minimization principle and tightness). This framework unveiled an interesting structure
    the “capacity” of the hypothesis space. The dis-  of the CSP landscape. Speciﬁcally, the landscape is divided
    covery, a few years ago, of a phase transition phe- into three regions: the YES region, corresponding to under-
    nomenon in inductive logic programming proves     constrained problems, where the satisﬁability probability is
    that other fundamental characteristics of the learn- close to 1 and the average complexity is low; the NO region,
    ing problems may similarly affect the very possi- corresponding to overconstrained problems, where the satis-
    bility of learning under very general conditions. ﬁability probability is close to 0 and the average complexity
    Our work examines the case of grammatical in-     is low too; last, a narrow region separating the YES and NO
    ference. We show that while there is no phase     regions, referred to as phase transition region, where the sat-
    transition when considering the whole hypothesis  isﬁability probability abruptly drops from 1 to 0 and which
    space, there is a much more severe “gap” phe-     concentrates on average the computationally heaviest CSP in-
    nomenon affecting the effective search space of   stances.
    standard grammatical induction algorithms for de-   The phase transition paradigm has been transported to re-
    terministic ﬁnite automata (DFA). Focusing on the lational machine learning and inductive logic programming
    search heuristics of the RPNI and RED-BLUE algo-  (ILP) by [Giordana and Saitta, 2000], motivated by the fact
    rithms, we show that they overcome this problem   that the covering test most used in ILP [Muggleton and Raedt,
    to some extent, but that they are subject to over- 1994] is equivalent to a CSP. As anticipated, a phase transi-
    generalization. The paper last suggests some direc- tion phenomenon appears in the framework of ILP: a wide
    tions for new generalization operators, suited to this YES (respectively NO) region includes all hypotheses which
    Phase Transition phenomenon.                      cover (resp. reject) all examples, and the hypotheses that can
                                                      discriminate the examples lie in the narrow PT, where the av-
                                                      erage computational complexity of the covering test reaches
1  Introduction                                       its maximum.
It is now well-known that the feasibility of inductive learn- Besides computational complexity, the PT phenomenon
ing is ruled by statistical properties linking the empirical risk has far-reaching effects on the success of relational learning
minimization principle and the “capacity” of the hypothesis [Botta et al., 2003]. For instance, a wide Failure Region is ob-
space [Vapnik, 1995]. While this powerful framework leads served: for all target concepts/training sets in this region, no
to a much deeper understanding of machine learning and to learning algorithms among the prominent ILP ones could ﬁnd
many theoretical and applicative breakthroughs, it basically hypotheses better than random guessing [Botta et al., 2003].
involves only statistical information on the learning search These negative results lead to a better understanding of the
space, e.g. the so-called VC-dimension. The dynamics of the intrinsic limits of the existing ILP algorithms and search bi-
learning search is not considered.                    ases. Formally, consider a greedy specialization (top-down)
  Independently, a new combinatoric paradigm has been search strategy: starting its exploration in the YES region,
studied in the Constraint Satisfaction community since the the system is almost bound to make random specialization
early 90s, motivated by computational complexity concerns choices, for all hypotheses in this region cover every exam-
[Cheeseman et al., 1991]: where are the really hard prob- ple on average. The YES region constitutes a rugged plateau
lems? Indeed, the worst case complexity analysis poorly from a search perspective, and there is little chance that the
accounts for the fact that, despite an exponential worst-case algorithm ends in the right part of the PT region, where good
complexity, empirically, the complexity is low for most CSP hypotheses lie. A similar reasoning goes for algorithms that
instances. These remarks led to developing the so-called follow a greedy generalization strategy.  The phase transition paradigm thus provides another per- by Chomsky, yet are sufﬁciently rich to express many inter-
spective on the pitfalls facing machine learning, focusing on esting sequential structures. Their identiﬁcation in the limit
the combinatoric search aspects while statistical learning fo- from positive examples only is known to be impossible, while
cuses on the statistical aspects.                     it is feasible with a complete set of examples [Gold, 1967].
  The main question studied in this paper is whether the PT It is known that any regular language can be produced by
phenomenon is limited to relational learning, or threatens the a ﬁnite-state automaton (FSA), and that any FSA generates
feasibility and tractability of other learning settings as well. a regular language. In the remaining of the paper, we will
  A learning setting with intermediate complexity between mostly use the terminology of ﬁnite-state automata. A FSA
full relational learning and propositional learning is thus is a 5-tuple A = hΣ, Q, Q0, F, δi where Σ is a ﬁnite alphabet,
considered, that of grammatical inference (GI) [Pitt, 1989; Q is a ﬁnite set of states, Q0 ⊆ Q is the set of initial states,
Sakakibara, 1997]. Only the case of Finite-State Automata F ⊆ Q is the set of ﬁnal states, δ is the transition function
(FSA, section 2), will be considered through the paper. deﬁned from Q × Σ to 2Q.
Speciﬁcally, the phase transition phenomenon will be inves- A positive example of a FSA is a string on Σ, produced by
tigated with respect to three distributions on the FSA space, following any path in the graph linking one initial state q0 to
incorporating gradually increasing knowledge on the syntac- any accepting state.
tical and search biases of GI algorithms.               A ﬁnite state-automaton (FSA) is deterministic (DFA) if
  The ﬁrst distribution incorporates no information on the al- Q0 contains exactly one element q0 and if ∀q ∈ Q, ∀x ∈
gorithm and considers the whole space of FSA. Using a set of Σ, Card(δ(q, x)) ≤ 1. Otherwise it is non-deterministic
order parameters, the average coverage of automata is studied (NFA). Every NFA can be translated into an equivalent DFA,
analytically and empirically.                         but at the price of being possibly exponentially more complex
  The second one reﬂects the bias introduced by the gener- in terms of number of states. Given any FSA A’, there exists
alization relations deﬁned on the FSA space and exploited a minimum state DFA (also called canonical DFA) A such
by GI algorithms. The vast majority of these algorithms ﬁrst that L(A) = L(A0) (where L(A) denotes the set of strings
construct a least general generalization of the positive exam- accepted by A). Without loss of generality, it can be assumed
ples, or Preﬁx Tree Acceptor (PTA), and restrict the search to that the target automaton being learned is a canonical DFA.
the generalizations of the PTA, or generalization cone1. A set S+ is said to be structurally complete with respect
  The third distribution takes into account the heuristics used to a DFA A if S+ covers each transition of A and uses every
by GI algorithms, guiding the search trajectory in the general- element of the set of ﬁnal states of A as an accepting state.
ization cone. Due to space limitations, the study is restricted Clearly, L(P T A(S+)) = S+.
to two prominent GI algorithms, namely RPNI [Oncina and Given a FSA A and a partition π on the set of states Q of
Garcia, 1992] and RED-BLUE [Lang et al., 1998].       A, the quotient automaton is obtained by merging the states
  This paper is organized as follows. Section 2 brieﬂy in- of A that belong to the same block in partition π (see [Dupont
troduces the domain of Grammatical Inference, the princi- et al., 1994] for more details). Note that a quotient automa-
ples of the inference algorithms and deﬁnes the order param- ton of a DFA might be a NFA and vice versa. The set of
eters used in the rest of the paper. Section 3 investigates the all quotient automata obtained by systematically merging the
existence and potential implications of phase transition phe- states of a DFA A represents a lattice of FSAs. This lattice
nomenons in the whole FSA space (section 3.1) and in the is ordered by the grammar cover relation . The transitive

generalization cone (section 3.2). Section 4 focuses on the closure of  is denoted by . We say that Aπi  Aπj iff
actual landscape explored by GI algorithms, speciﬁcally con-                                           +
                                                      L(Aπi ) ⊆ L(Aπj ). Given a canonical DFA A and a set S
sidering the search trajectories of RPNI and RED-BLUE. Sec- that is structurally complete with respect to A, the lattice de-
tion 5 discusses the scope of the presented study and lays out rived from P T A(S+) is guaranteed to contain A.
some perspectives for future research.                  From  these assumptions, follows the paradigmatic ap-
                                                      proach of most grammatical inference algorithms (see, e.g.,
2  Grammatical inference                              [Coste, 1999; Dupont et al., 1994; Pitt, 1989; Sakakibara,
                                                      1997]), which equates generalization with state merging op-
After introducing general notations and deﬁnitions, this sec- erations starting from the PTA.
tion brieﬂy discusses the state of the art and introduces the
order parameters used in the rest of the paper.       2.2  Learning biases in grammatical inference
2.1  Notations and deﬁnitions                         The core task of GI algorithms is thus to select iteratively
Grammatical inference is concerned with inferring grammars a pair of states to be merged. The differences among al-
from positive (and possibly negative) examples. Only reg- gorithms is related to the choice of: (i) the search criterion
ular grammars are considered in this paper. They form the (which merge is the best one); (ii) the search strategy (how is
bottom class of the hierarchy of formal grammars as deﬁned the search space explored); and (iii) the stopping criterion.
                                                        We shall consider here the setting of learning FSAs from
  1More precisely, the Preﬁx Tree Acceptor is obtained by merging positive and negative examples, and describe the algorithms
the states that share the same preﬁx in the Maximal Canonical Au- studied in section 3. In this setting, the stopping criterion is
tomaton (MCA), which represents the whole positive learning set as determined from the negative examples: generalization pro-
an automaton. A PTA is therefore a DFA with a tree-like structure. ceeds as long as the candidate solutions remain correct, notcovering any negative example2                          • for every state q, (i) B output edges (q, q0) are cre-
  The RPNI algorithm [Oncina and Garcia, 1992] uses a     ated, where q0 is uniformly selected with no replacement
depth ﬁrst search strategy with some backtracking ability, fa- among the Q states; (ii) L × B distinct letters are uni-
voring the pair of states which is closest to the start state, formly selected in Σ; and (iii) these letters are evenly
such that their generalization (FSA obtained by merging the distributed among the B edges above.
two states and subsequently applying the determinisation op- • every state q is turned into an accepting state with prob-
erator) does not cover any negative example.              ability a.
  The RED-BLUE  algorithm (also known as BLUE-FRINGE)
[Lang et al., 1998] uses a beam search from a candidate list, The sampling mechanism for NFA differs from the above in
selecting the pair of states after the Evidence-Driven State a single respect: two edges with same origin state are not
Merging (EDSM) criterion, i.e. such that their generalization required to carry distinct letters.
involves a minimal number of ﬁnal states. RED-BLUE thus For each setting of the order parameters, 100 independent
also performs a search with limited backtracking, based on a problem instances are constructed. For each considered FSA
more complex criterion and a wider search width than RPNI. (the sampling mechanisms are detailed below), the cover-
                                                      age rate is measured as the percentage of covered examples
2.3  Order Parameters                                 among 1,000 examples (strings of length `) uniformly sam-
Following the methodology introduced in [Giordana and pled.
                                                                                           (a, B)
Saitta, 2000], the PT phenomenon is investigated along so- Fig. 1 shows the average coverage in the plane, for
                                                      |Σ| = 2 L = 1    ` = 10                     a
called order parameters chosen in accordance with the pa-    ,     and       , where the accepting rate varies
                                                         [0, 1]                    B          {   }
rameters used in the Abbaddingo challenge [Lang et al., in   and the branching factor varies in 1,2 . Each
                                                                                                    s
1998]:                                                point reports the average coverage of a sample string by a
                                                      FSA (averaged over 100 FSA drawn with accepting rate a and
  • The number Q of states in the DFA.                branching factor B, tested on 1, 000 strings s of length `).
  • The number B of output edges on each state.         These empirical results are analytically explained from the
                                                      simple equations below, giving the probability that a string of
  • The number L of letters on each edge.             length ` be accepted by a FSA deﬁned on an alphabet of size
  • The fraction a of accepting states, taken in [0,1]. |Σ|, with a branching factor B and L letters on each edge, in
  • The size |Σ| of the alphabet considered.          the DFA and NFA cases (the number of states Q is irrelevant
                                                      here).
  • The length ` of the test examples. Also the maximal
    length ` of the learning examples in S+ (as explained
                                                                      (     B·L `
    below).                                                             a · ( |Σ| )        for a DFA
                                                          P (accept) =              L  B `
  The study ﬁrst focuses on the intrinsic properties of the             a · [1 − (1 − |Σ| ) ] for a NFA
search space (section 3) using a a random sampling strategy
(all ` letters in the string being independently and uniformly The coverage of the FSA decreases as a and B decrease.
drawn in Σ). In section 4, we examine the capacity of the The slope is more abrupt in the DFA case than in the NFA
studied learning algorithms to approximate a target automa- case; still, there is clearly no phase transition here.
ton, based on positive sampling, where each training string is
produced by following a path in the graph, randomly select-
ing an output edge in each step3.

3  Phase Transitions: the FSA space and the
   generalization cone
This section investigates the percentage of coverage of deter-
ministic and non-deterministic Finite-State Automata, either
uniformly selected (section 3.1), or selected in the subspace
actually investigated by grammatical inference algorithms,
that is, the generalization cone (section 3.2).
                                                      Figure 1: Coverage landscapes for Deterministic and Non-
3.1  Phase Transition in the whole FSA space          Deterministic FSA, for |Σ|=2, L=1 and `=10. The density
The sampling mechanism on the whole deterministic FSA of accepting states a and the branching factor B respectively
space (DFA) is deﬁned as follows. Given the order param- vary in [0, 1] and {1, 2}.
eter values (Q, B, L, a, |Σ|):

  2
   In this paper, after the standard Machine Learning terminology, 3.2 PT in the Generalization Cone
a string is said to be covered by a FSA iff it belongs to the language
thereof.                                              The coverage landscape displayed in Fig. 1 might suggest
  3The string is cut at the last accepting state met before arriving that grammatical inference takes place in a well-behaved
at length `, if any; otherwise it is rejected.        search space. However, grammatical inference algorithms donot explore the whole FSA space. Rather, as stated in section
2.1, the search is restricted to the generalization cone, the set
of generalizations of the PTA formed from the set S+ of the
positive examples. The next step is thus to consider the search
space actually explored by GI algorithms.
  A new sampling mechanism is deﬁned to explore the DFA
generalization cone:
 1. |S+| (= 200 in the experiments) examples of length
    ` are uniformly and independently sampled within the
    space of all strings of length < `, and the corresponding
    PTA is constructed;
 2. N  (= 50 in the experiments) PTAs are constructed in
    that way.
                                                      Figure 2: Coverage landscape in the DFA generalization cone
 3. K (= 20 in the experiments) generalization paths, lead-
                                                      (|Σ| = 4, ` = 8). At the far right stand the 50 PTA sam-
    ing from each PTA to the most general FSA or Universal
                                                      pled, with circa 1150 states each. The generalization cone of
    Acceptor (UA), are constructed;
                                                      each PTA includes 1,000 generalization paths, leading from
    In each generalization path (A = P T A, A , . . . , A =
                             0          1      t      the PTA to the Universal Acceptor. Each point reports the
    UA), the i-th FSA A is constructed from A by merg-
                     i                  i−1           coverage of a DFA, evaluated over a sample of 1,000 strings.
    ing two uniformly selected states in A , and subse-
                                     i−1              This graph shows the existence of a large gap regarding both
    quently applying the determinisation operator.
                                                      the number of states and the coverage of the DFAs that can
 4. The generalization cone sample is made of all the FSAs be reached by generalization.
    in all generalization paths (circa 270,000 FSAs in the
    experiments).
  The sampling mechanism on the non-deterministic gener-
alisation cone differs from the above in a single respect: the
determinisation operator is never applied.
  Fig. 2 shows the behaviour of the coverage in the DFA


generalisation cone for |Σ| = 4 and ` = 8. Each DFA A is             C
                                                                     o

depicted as a point with coordinates (Q, c), where Q is the          v
                                                                     e
                                                                     r

number of states of A and c is its coverage (measured as in          a
                                                                     g

section 3). The coverage rate for each FSA in the sample is          e
                                                                     r
evaluated from the coverage rate on 1000 test strings of length      a
                                                                     t
`.                                                                   e
  Fig. 3 similarly shows the behaviour of the coverage in the
NFA generalisation cone, with |Σ| = 4 and ` = 16.                          Number of states
  Fig 2, typical of all experimental results in the range of ob-
servation (|Σ| = 2, 4, 8, 16, and ` = 2, 4, 6, 8, 16, 17), shows
a clear-cut phase transition. Speciﬁcally, here, the coverage Figure 3: Coverage landscape in the NFA generalization
abruptly jumps from circa 13% to 54%; and this jump co- cone, with same order parameters as in ﬁg. 2.
incides with a gap in the number of states of the DFAs in
the generalization cone: no DFA with a number of states in
[180, 420] was found. The gap is even more dramatic as the
length of the training and test sequences ` is increased. in between a large interval (typically between less than 20%
  Interestingly, a much smoother picture appears in the non- to approximately 60%) falling abruptly. Therefore, a random
deterministic case (ﬁg. 3); although the coverage rapidly in- exploration of the generalization cone would face severe difﬁ-
creases when the number of states decreases from 300 to 200, culties in ﬁnding a hypothesis in this region and would likely
no gap can be seen, neither in the number of states nor in the return hypotheses of poor performance if the target concept
coverage rate itself4.                                had a coverage rate in this “no man’s land” interval.
  In the following, we focus on the induction of DFAs.  It is consequently of utmost importance to examine the
                                                      search heuristics that are used in the classical grammatical
4  Phase transition and search trajectories           inference systems. First, are they able to thwart the a pri-
The coverage landscape, for the DFAs, shows a hole in the ori very low density of hypotheses in the gap? Second, are
generalization cone, with a density of hypotheses of coverage they able to guide the search toward hypotheses of appropri-
                                                      ate coverage rate, specially if this coverage falls in the gap?
  4The difference with the DFA case is due to the determinisation
process that forces further states merging when needed. A diffusion The study will thus focus on two standard algorithms in
like analytical model was devised, that predicts the observed start of grammatical inference, namely the RPNI and the RED-BLUE
the gap with 15% precision.                           algorithms [Oncina and Garcia, 1992; Lang et al., 1998].                                                       C
                                                       o
                                                       v
                                                       e
                                                       r
                                                       a
                                                       g
                                                       e
                                                       R
                                                       a
                                                       t
                                                       e


4.1  Experimental setting
                                                                100

Previous experiments considered training sets made of pos-      90

itive randomly drawn strings sequences only. However, in        80
order to assess the performance of learning algorithms, the
                                                                70
hypothesis learned must now be compared to the target au-
                                                                60
tomaton. Therefore, another experimental setting is used in
                                                                50
this section, with the sampling of target automata, and the
                                                                40
construction of training and test sets. These data sets include
                                                                30
positive and negative examples as most GI algorithms (and
                                                                20
speciﬁcally RPNI and RED-BLUE) use negative examples in
order to stop the generalization process.                       10
                                                                0
  In our ﬁrst experiments, we tested whether heuristically       0  50  100 150 200 250 300 350 400 450

                                                       C                    Number of States

guided inference algorithms can ﬁnd good approximations of o
                                                       v
                                                       e
                                                       r


the target automata considering target automata with approx- a
                                                       g
imately (i) 50% coverage rate (as considered in the inﬂuential e
                                                      FigureR 4: Three RPNI learning trajectories for a target concept
                                                       a


Abbadingo challenge, and in the middle of the “gap”), and t
                                                      ofe coverage=56%. Their extremity is outlined in the oval on
(ii) 5% coverage rate.                                the left. The doted horizontal line corresponds to the cover-
  For each target coverage rate, we used the experimental age of the target concept. The cloud of points corresponds to
setting described in [Lang et al., 1998] in order to retain a cer- random trajectories.
tain number of target automata with a mean size of Q states
(Q = 50, in our experiments). For each automaton then, we
generated N (=20) training sets of size |S| (= 100) labeled
according to the target automaton, with an equal number of      100
positive and negative instances (|S+| = |S−| = 50) of length    90
` = 14. The coverage rate was computed as before on 1000        80
uniformly drawn strings (with no intersection with the train-   70
ing set).                                                       60
  In a second set of experiments, we analyzed the learning      50
performances of the algorithms with respect to test errors,     40
both false positive and false negative.                         30
  In these experiments, we chose the type of target automata    20
by setting the number of states Q and some predetermined        10
                5                                               0
structural properties .                                          0  50  100 150 200 250 300 350 400 450
                                                                            Number of States
4.2  The heuristically guided search space
Due to space limitation, only the graph obtained for the RPNI Figure 5: Same as in ﬁgure 4, except for the coverage of the
algorithm is reported (see ﬁgure 4), with three typical learn- target concept, here 3%.
ing trajectories. Similar results were obtained with the RED-
BLUE algorithm.
  One immediate result is that both the RPNI and the EDSM 4.3 Generalization error
heuristics manage to densely probe the “gap”. This can ex-
                                                      Table 1, obtained for different sizes of the target automata
plain why the gap phenomenon was not discovered before,
                                                      and for training sets of structural completeness above 40%,
and why the RED-BLUE algorithm for instance could solve
                                                      conﬁrms that both RPNI and RED-BLUE return overgener-
some cases of the Abbadingo challenge where the target con-
                                                      alized hypotheses. On one hand, their average coverage is
cepts have a coverage rate of approximately 50%. How-
                                                      vastly greater than the coverage of the target automata, on the
ever, where RPNI tends to overspecialize the target automa-
                                                      other hand, they tend to cover only part of the positive test
ton, RED-BLUE tends to overgeneralize it by 5% to 10%.
                                                      instances, while they cover a large proportion of the nega-
  In order to test the capacity of the algorithms to return au- tive test instances. This shows that the heuristics used in both
tomata with a coverage rate close to the target coverage, we
                                                      RPNI and RED-BLUE  may be inadequate for target concepts
repeated these experiments with target automata of coverage of low coverage.
rate of approximately 3%. The results (ﬁgure 5) shows that,
in this case, RPNI ends up with automata of coverage 4 to
6 times greater than the target coverage. The effect is even 5 Conclusion
more pronounced with RED-BLUE which returns automata  This research has extended the Phase Transition-based
of average coverage rate around 30%!                  methodology [Botta et al., 2003] to the Grammatical Infer-
                                                      ence framework. Ample empirical evidence shows that the
  5The datasets and more detail are available at http://www. search landscape presents signiﬁcant differences depending
lri.fr/∼antoine/www/pt-gi/                            on the search operators that are considered.