                             Marginalized Multi-Instance Kernels

                                James T. Kwok        Pak-Ming Cheung
                           Department of Computer Science and Engineering
                     Hong Kong University of Science and Technology, Hong Kong
                                     {jamesk,pakming}@cse.ust.hk


                    Abstract                          trieval [Chen and Wang, 2004], where each image is a bag
                                                      and each local image patch an instance.
    Support vector machines (SVM) have been highly
    successful in many machine learning problems.       Following Dietterich et al. ’s seminal work, a number of
    Recently, it is also used for multi-instance (MI) new MI learning methods, such as the Diverse Density (DD)
                                                               [                           ]
    learning by employing a kernel that is deﬁned di- algorithm Maron and Lozano-Perez, 1998 ,haveemerged.
    rectly on the bags. As only the bags (but not the in- Here, we will focus on methods based on the support vec-
    stances) have known labels, this MI kernel implic- tor machines (SVM), which have been highly successful in
    itly assumes all instances in the bag to be equally many machine learning problems. There are two main ap-
    important. However, a fundamental property of MI  proaches to extend the standard SVM for MI data. One is to
                                                                                     [
    learning is that not all instances in a positive bag modify the SVM formulation, as in Andrews et al., 2003;
                                                                            ]
    necessarily belong to the positive class, and thus Cheung and Kwok, 2006 . However, unlike the standard
    different instances in the same bag should have dif- SVM, they lead to non-convex optimization problems which
    ferent contributions to the kernel. In this paper, suffer from local minima. Another approach is to design ker-
                                                                            [                 ]
    we address this instance label ambiguity by using nels directly on the bags G¨artner et al., 2002 . These so-
    the method of marginalized kernels. It ﬁrst as-   called MI kernels can then be used in a standard SVM. As the
    sumes that all the instance labels are available and instance labels are unavailable, the MI kernel implicitly as-
    deﬁnes a label-dependent kernel on the instances. sumes all instances in the bag to be equally important. How-
    By integrating out the unknown instance labels, a ever, a central assumption in MI learning is that not all the in-
    marginalized kernel deﬁned on the bags can then   stances in a positive bag are necessarily positive. Thus, many
    be obtained. A desirable property is that this ker- of these instances should have small contributions and the as-
    nel weights the instance pairs by the consistencies sumption made by the MI kernel is very crude.
    of their probabilistic instance labels. Experiments Recall that the major difference between MI learning and
    on both classiﬁcation and regression data sets show traditional single-instance learning lies in the ambiguity of
    that this marginalized MI kernel, when used in a  the instances labels. If the instance labels were available,
    standard SVM, performs consistently better than   then the MI problem could be solved much more easily. A
    the original MI kernel. It also outperforms a num- related idea is explored in the EM-DD algorithm [Zhang and
    ber of traditional MI learning methods.           Goldman, 2002]. In each bag, the instance that is most con-
                                                      sistent with the current hypothesis is selected as the represen-
                                                      tative of the whole bag. This converts the multiple-instance
1  Introduction                                       data to single-instance data and leads to improved speed and
In supervised learning, each training pattern has a known accuracy over the DD algorithm. However, it implicitly as-
class label. However, many applications only have weak la- sumes that there is only one positive instance in each pos-
bel information and thus cannot be formulated as supervised itive bag. In practice, both DD and EM-DD are often in-
learning problems. A classic example as discussed by [Diet- ferior to kernel-based MI algorithms [Andrews et al., 2003;
terich et al., 1997] is drug activity prediction, where the task Cheung and Kwok, 2006].
is to predict whether a drug molecule can bind to the targets In this paper, we address the ambiguity of instance labels
(enzymes or cell-surface receptors). A molecule is considered in the design of MI kernels by using the method of marginal-
useful as a drug if one of its conformations can bind to the ized kernels [Tsuda et al., 2002]. Similar to the Expectation-
targets. However, biochemical data can only tell the binding Maximization (EM) algorithm, it transforms an incomplete
capability of a molecule, but not a particular conformation. In data problem (with only the observed data) to a complete data
other words, we only have class labels associated with sets of problem (with both the observed and hidden data), which is
patterns (bags), instead of the individual patterns (instances). then often easier to solve. This method has been successfully
Dietterich et al. called this multi-instance (MI) learning. An- used to deﬁne marginalized kernels for strings, [Tsuda et al.,
other well-known MI application is content-based image re- 2002], trees and graphs [Mah´e et al., 2004].

                                                IJCAI-07
                                                   901  The rest of this paper is organized as follows. Section 2 3.2 Marginalizing the Joint Kernel
ﬁrst gives a brief introduction to the marginalized kernel. Sec- To obtain the marginalized kernel, we take expectation of the
tion 3 then describes the proposed marginalized kernel for MI joint kernel in (2) w.r.t. the hidden variables c1 and c2,as:
classiﬁcation, which is followed by an extension to MI regres-       
                                                           (     )=        (c |  ) (c |  )  (z  z )
sion in Section 4. Experimental results on a number of clas- k B1,B2     P   1 B1 P  2 B2 kz  1, 2 .  (4)
siﬁcation and regression data sets are presented in Section 5,       c1,c2
and the last section gives some concluding remarks.   Computation of the conditional probability P (ci|Bi) will be
                                                      postponed to Section 3.3. Note that even when P (ci|Bi) is
2  Marginalized Kernels                               known, a direct computation of (4) is still computationally
                                                                           n +n                   n
Like the EM algorithm, the data are assumed to be generated infeasible (and takes Ω(2 1 2 ) time) as ci ∈{0, 1} i .
by a latent variable model, with observed variable x and hid- With the joint kernel deﬁned in (2), k(B1,B2) in (4) can
den variable θ. The task is to deﬁne a (marginalized) kernel be simpliﬁed as:
between two observed variables x1 and x2. With the help  X                  Xn1 Xn2
of the hidden variables, one can ﬁrst deﬁne a joint kernel   P (c1|B1)P (c2|B2)    kc(c1i,c2j )kx(x1i, x2j )
kz(z1,z2) over the pairs z1 =(x1,θ1) and z2 =(x2,θ2).    c1,c2               i=1 j=1
As the hidden information is indeed unobserved, the poste-   Xn1 Xn2          X
rior distribution of θ1 ad θ2 are obtained by some probabilistic = kx(x1i, x2j )  {kc(c1i,c2j )
       ( | )
model p θ x , which in turn is estimated from the data. The  i=1 j=1         c1i,c2j
marginalized kernel is then obtained by taking expectation of  X                  X
                                                              ·    P (c , c \ c |B ) P (c , c \ c |B )},
the joint kernelX w.r.t. the hidden variables, as:                    1i 1  1i  1       2j  2  2j  2
                                                               c1\c1i            c2\c2j
 k(x1,x2)=       P (θ1|x1)P (θ2|x2)kz(z1,z2)dθ1 dθ2, (1)
                                                                                                 
           θ ,θ ∈Θ                                          c \     =[                           ]
           1 2                                        where i  cij    ci1,...,ci,j−1,ci,j+1,...,cini .Using
      Θ                                                        ( ij ci \ ij | i)= ( ij | i)
where    is the domain of the hidden variables. When the ci\cij P c ,  c  B     P c  B   and the conditional
hidden variable is continuous, the summation is replaced by independence assumption that P (cij |Bi)=P (cij |xij ),
an integration. Note that computing (1) may be intractable k(B1,B2) can be reduced to
     Θ
when   is large, and so this is an important issue in designing Xn1 Xn2 X
marginalized kernels.                                            kx(x1i, x2j )kc(c1i,c2j )P (c1i|x1i)P (c2j |x2j ). (5)
                                                      i=1 j=1 c1i,c2j
3  Marginalized Kernels for MI Classiﬁcation           As will be shown in Section 3.4, this can now be computed
In MI classiﬁcation, we are given a set of training bags in polynomial time. In particular, on using (3) as the instance
{(     )    (       )}          =  {x       x  }
  B1,y1 ,..., Bm,ym   ,whereBi       i1,..., ini is   label kernel, k(B1,B2) is simply
the ith bag containing instances xij ’s, and yi ∈{0, 1}.For n1 n2
each bag Bi, the observed information is then the associated     P (c1i =1|x1i)P (c2j =1|x2j)kx(x1i, x2j)
xij ’s while the hidden information is the unknown instance i=1 j=1
     c =[            ]        ∈{0  1}
labels i  ci1,...,cini ,wherecij   ,  .                      n1 n2
                                                           +         (   =0|x  )  (   =0|x   ) (x   x  )
3.1  The Joint Kernel                                              P  c1i     1i P c2j     2j kx  1i, 2j ,
                                                             i=1 j=1
The joint kernel is deﬁned on two combined variables z1 =
(B1, c1) and z2 =(B2, c2). It can thus utilize information which is very intuitive. Finally, to avoid undesirable scaling
on both the input (xij ’s) and instance labels (ci). Note that problems, we normalize the kernel as in [G¨artner et al., 2002]:
                                                                       k(Bi,Bj )
                                                       (  i  j) ← √       √
while traditional kernels (such as the polynomial and Gaus- k B ,B  k(B ,B ) k(B ,B ) .
sian kernels) are deﬁned on the input part only, recent results        i i     j j
                                                      Note that (5) reduces to the MI kernel k(Bi,Bj)=
show that the use of label information can lead to better ker- n1 n2
                                                                 kx(x1i, x2j) in [G¨artner et al., 2002] if kc(·, ·)
nels (e.g, [Cristianini et al., 2002]).                 i=1   j=1
  In this paper, we deﬁne the joint kernel as:        is constant. Thus, while [G¨artner et al., 2002] assumes that
                 n1 n2                              all the instance pairs between Bi and Bj are equally impor-
      kz(z1, z2)=       kc(c1i,c2j)kx(x1i, x2j ), (2) tant, kc in (5) weights them differently according to the con-
                 i=1 j=1                              sistency of their probabilistic labels. Moreover, compared to
where kx(·, ·) and kc(·, ·) are kernels deﬁned on the input and EM-DD which chooses only one representative instance from
label parts of the instances, respectively. A simple and rea- each bag during inference, here we perform marginalization
sonable deﬁnition of kc is1                           over all possible instance labels and is thus more consistent
             kc(c1i,c2j)=I(c1i = c2j),          (3)   with the Bayesian framework.
where I(·) returns 1 when the predicate is true, and 0 other- 3.3 Deﬁning the Conditional Probabilities
wise. In this case, (2) is also equal to the alignment between In this section, we consider how to obtain the conditional
kernel kx and the instance labels [Cristianini et al., 2002].A
                                                      probability P (ci|Bi). As mentioned in [Tsuda et al., 2002],
high alignment thus implies a high kernel value (or similarity) an advantage of the marginalized kernel approach is that the
between B1 and B2.                                    deﬁnitions of the joint kernel and probabilistic model are
  1Other deﬁnitions are also possible, e.g., one can have completely separated. Thus, one has a lot of freedom in pick-
kc(c1i,c2j )=1if c1i = c2j =1;and0otherwise.          ing a good probabilistic model.

                                                IJCAI-07
                                                   902Using the Probabilistic Model for Diverse Density     Algorithm 1 Marginalized MI kernel for classiﬁcation.
Motivated by the success of the DD algorithm in MI learning Input: Training set D; A pair of bags B1 and B2
[                          ]
Maron and Lozano-Perez, 1998 , we ﬁrst consider using its Output: k(B1,B2)
probabilistic model for estimating P (cij |xij ). The DD algo-
                                               2 X     1: Run the DD algorithm with different initializations and
rithm ﬁnds a hypothesis h over the whole instance space                                      H
by maximizing the following DD function:                  store the hypotheses obtained in the array .
          0                   1                        2: for all h ∈Hdo
                “            ”       “            ”
        Y     Y             2  Y  Y              2     3:   Compute P (h|D) using (6) or (11) and store the value
          @          −h−xij  A          −h−xij 
DD(h)=     1−    1−e                  1 − e         .       in array Q.
        B+    xij              B− xij
         i                      i                      4: end for
       +      −                                        5: for all xij ∈ B1 ∪ B2 do
Here, Bi and Bi  index all the positive and negative bags
                                                       6:   Compute P (cij |xij ) using (7) and (8).
in the training set, respectively. Intuitively, h has a high DD
                                                       7: end for
value if all positive bags have instances close to h, while neg-
                                                       8: Compute kx(x1i, x2j) for all instance pairs in B1 × B2.
ative instances from all the negative bags are far away from
                                                       9: Compute k(B1,B2)  as in (9), using the pre-computed
h. Under this model, we can then deﬁne
                                                          values in the array Q, kx and P (cij |xij ).
                P (h|D)=DD(h)/Z,                (6)
                                   
where Z is a normalizing factor such that P (h|D)=1,
                                     h                Using the Training Accuracy to Improve P (h|D)
                                     2
                             −xij −h
             P (cij =1|xij )=e                  (7)   As mentioned earlier, the DD algorithm has been very suc-
                                                      cessful. Note that one only has to use the hypotheses, but not
and P (cij =0|xij )=1− P (cij =1|xij ).
                                                      their DD values, on classiﬁcation. Recall that the obtained
  However, the DD function is highly nonlinear with many
                                                      hypotheses are local minima of the DD function. While many
local minima. Hence, instead of using only one h obtained
                                                      of them may have comparable classiﬁcation accuracies, as
from the optimization process, it is often advantageous to use
                                                      will be demonstrated in Section 5, a few hypotheses can have
multiple hypotheses [Chen and Wang, 2004].Wethenhave3
                                                     DD values that are much higher than the others. This is not
       P (cij |xij )=    P (h|xij )P (cij |xij ,h)    surprising as DD value may drop signiﬁcantly even if the hy-
                     h                               pothesis is close to only one negative instance. Consequently,
                  =      P (h|D)P (cij |xij ,h), (8)  the summation in (8) is often dominated by a few hypotheses.
                      h                                 To alleviate this problem while still retaining the merits of
                                                      DD, we will instead deﬁne P (h|D) of each DD hypothesis
where the summation is over the set of hypotheses obtained.
                                                      to be proportional to its training accuracy. Let pred (Bi)
Note that (8) automatically weights each hypothesis by its                                          h
                                                      be the label predicted by h on Bi, which is equal to 1 if
likelihood. On the contrary, the DD-SVM in [Chen and                    2
                                                      max       −xij −h ≥ 0 5
Wang, 2004] has no such weighting and has to rely on addi- xij∈Bi e          . , and 0 otherwise. Then,
                                                                          m
tional heuristics to ﬁlter away the less important hypotheses.           
  Substituting all these equations into (5), we ﬁnally have     P (h|D)=    I(predh(Bi)=yi)/Z,       (11)
           Xn1 Xn2 X  X                                                  i=1
 k(B ,B )=                k (x , x )k (c ,c )
    1  2                   x  1i 2j  c 1i 2j          where m is the number of training bags and Z is again for
           i=1 j=1 h ,h c ,c
                  1 2 1i 2j                           normalization. Its superiority over the deﬁnition in (6) will
           P (h1|D)P (h2|D)P (c1i|x1i,h1)P (c2j |x2j ,h2). (9) be experimentally veriﬁed in Section 5.
The complete algorithm is shown in Algorithm 1.         Because of the modular design of the marginalized kernel,
  Very recently, [Rahmani and Goldman, 2006] proposed a we can easily plug in other good estimators of P (cij |xij ).
graph-based MI semi-supervised learning method, where the For example, an EM-based approach may also be used.
edge weight between bags B1 and B2 is roughly equal to4
                                                      3.4  Time Complexity
        Xn1 Xn2
              DD(x1i)+DD(x2j   )
                                k (x , x ).           Let Nh be the number of DD hypotheses. In Algorithm 1,
                       2         x  1i 2j      (10)
        i=1 j=1                                       computing all the P (cij |xij )’s at Step 6 takes O((n1 +
                                                        )    )                    ( |D)
This is quite similar to (9). However, in general, (10) is not n2 Nhd time (assuming that P h ’s can be obtained in
positive semi-deﬁnite and so not a valid kernel function. constant time or have been pre-computed), where d is the data
                                                      dimensionality. Assuming that each evaluation of the kernel
  2Searching over a large X can be prohibitive. To be computa- kx takes O(d) time, then computing all the kx(·, ·)’s in Step 8
tionally efﬁcient, one often performs a gradient-based optimization takes O(n1n2d) time. The summation involved in k(B1,B2)
                                                                        2
of the DD function by initializing from every instance in every pos- of Step 9 takes O(Nh n1n2) time (as cij only takes 0 or 1).
itive bag [Chen and Wang, 2004; Maron and Lozano-Perez, 1998]. Thus, the total time complexity for computing k(B1,B2) is
  3                                                        2
   Note that all the probabilities here are implicitly conditioned O((Nh + d)n1n2 +(n1 + n2)Nhd), which is polynomial.
on the training data D. For clarity, in (8) we write P (h|D) (the
                  h                    P (h)
posterior probability of given the data), instead of . 4  Marginalized Kernels for MI Regression
  4In [Rahmani and Goldman, 2006], the DD values in (10) are
ﬁrst normalized, nonlinearly transformed and then ﬁltered. More- In regression, we assume that each bag output yi is normal-
over, this weight is deﬁned on positive bags only.    ized to the range [0, 1]. Subsequently, each (hidden) instance

                                                IJCAI-07
                                                   903 label cij is also in [0, 1]. In this real-valued setting, we follow When multiple hypotheses are used, k(B1,B2) can be sim-
 the extended DD model in [Amar et al., 2001], and have: ilarly generalized to:
                                                                                                       2
                                  2                                                   −h −x 2  −h −x 
                           −h−xij                      X  P (h |D)P (h |D)k(x , x )G(e 1  1i ,e   2  2j )
    P (cij |xij )= 1 −|cij − e     | /Z(xij ),  (12)           1      2      1i 2j                         .
                                                                              Z(x1i)Z(x2j )
                                                      i,j,h1,h2
        (x  )            1  (  |x )    =1
 where Z  ij ensures that 0 P cij ij dcij  . It can be                      ( |D)
                                −h−x  2      2        As for the probability P h , again one can follow the ex-
 easily shown that Z(xij )=0.75 − (e  ij − 0.5) .      tended DD model and obtain:
   Proceeding as in Section 3, we employ the same joint ker-            
                        
                                                                                                2
                                                                                        −h−xij 
 nel in (2) (but with summations of c1i and c2j replaced by in- P (h|D)= 1 −|yi − max  e         | /Z.
             [0 1]                        (     )                                 xij ∈Bi
 tegrations over , ) in the marginalized kernel k B1,B2 of           Bi
 (5). Again, this can then avoid integrating over an (n1 + n2)-
 dimensional space.                                    Alternatively, as in (11), we can also use the train-
                                                       ing accuracy of each hypothesis. Following [Dooly et
   The probability that two instances share the same real-                                   2
                                                                           max       −h−xij 
 valued label is zero, and so (3) is a poor measure of in- al., 2002],weuse   xij ∈Bi e        as h’s pre-
                                                       dicted output of bag Bi. The error of h is then eh =
 stance label similarity. Intuitively, the larger the difference                    2
                                                       m                           2
 between c1i and c2j, the smaller is kc(c1i,c2j). Noting that                −h−xij 
                                                              yi − maxxij ∈Bi e        , and we can deﬁne
 |  −    |∈[0  1]                                        i=1
 c1i  c2j     , , we have two natural choices for kc (Fig-           
                       
      5
 ure 1) :                                                                    eh − minh eh
                                                           P (h|D)=   1 −                      /Z.    (17)
                                                                          max     − min  
         (linear) kc(c1i,c2j)=(1−|c1i − c2j|),  (13)                          h eh      h eh
                                           
                                           2
   (Gaussian) kc(c1i,c2j)=exp  −β(c1i − c2j) .  (14)   5   Experiments
                                                       In this section, we compare the performance of the proposed

                                β   2                  marginalized kernels with the MI kernel in [G¨artner et al.,
               1             exp(- (c -c ) )  
                                 1i 2j                 2002] on a number of MI data sets. As for the kernel deﬁned
                             1-|c -c |
                               1i 2j
              0.8                                      on the input parts, we use the Gaussian kernel kx(x1i, x2j)=
                                                                         2
                                                       exp (−γ x1i − x2j  ),whereγ is the width parameter.
              ) 0.6
              2j
              ,c
              1i
              (c
              c                                        5.1  MI Classiﬁcation
              k 0.4
                                                       For simplicity, the marginalized kernels deﬁned in (6) and
              0.2                                      (11) are denoted by MG-DDV and MG-ACC, respectively.

               0  
                0   0.2  0.4 0.6  0.8  1               Drug Activity Prediction
                          |c -c |
                           1i 2j                       The ﬁrst experiment is performed on the popular Musk1 and
 Figure 1: Two possible deﬁnitions of the label kernel Musk2 data sets6. The task is to predict if a drug is musky.
   (     )
 kc c1i,c2j in the regression setting.                 As mentioned in Section 1, each drug is considered a bag,
                                                       and each of its low-energy conformations an instance. We
   Putting all these together, and after long but straightfor- use 10-fold cross-validation. In each fold, the DD algorithm
 ward calculations, it can be shown that the marginalized MI is applied only on the training data and so the DD hypotheses
 kernel between bags B1 and B2 is given by:            cannot capture any information in the test set. For compari-
             n1 n2                      2         2    son, the DD-SVM [Chen and Wang, 2004] is also run.
                  (x   x  ) ( −h−x1i  −h−x2j  ))
  (      )=        k  1i, 2j G e        ,e               As can be seen from Table 1, the MG-ACC kernel always
 k B1,B2                       (x  ) (x  )           ,
            i=1 j=1           Z  1i Z  2j              performs better than the MI kernel. It also outperforms the
                                                       DD-SVM, which is also using multiple DD hypotheses that
 where G(u, v) is equal to                             are assumed to be equally important. This thus demonstrates
 Z 1Z 1                                                the importance of weighting instance pairs based on the con-
      (1 −|c1i − c2j |)(1−|c1i − u|)(1 −|c2j − v|)dc1i dc2j , (15) sistency of their underlying labels (Section 3.2).
  0 0
when (13) is used; and is equal to                        Table 1: Testing accuracies (%) on the Musk data sets.
 Z Z                                                                              Musk1   Musk2
   1 1  `           ´
                   2
    exp  −β(c1i −c2j ) (1 −|c1i − u|)(1−|c2j − v|)dc1i dc2j , (16)  DD-SVM         89.0    87.9
  0 0                                                               MI kernel      89.3    87.5
 when (14) is used. Closed form expressions for these two        MG-DDV kernel     87.8    88.6
 integrals can be found in the Appendix. Moreover, as in clas-   MG-ACC kernel     90.1    90.4
 siﬁcation, both deﬁnitions of k(B1,B2) can be computed in
 polynomial time.                                        Moreover, the MG-ACC kernel is better than the MG-DDV
                                                       kernel. Figure 2(a) compares their corresponding P (h|D)
   5
    In the experiments, we set β such that kc(c1i,c2j )=0.0001
                                                          6
 when |c1i − c2j | =1.                                    ftp://ftp.ics.uci.edu/pub/machine-learning-databases/musk/

                                                 IJCAI-07
                                                    904values ((6) for MG-DDV and (11) for MG-ACC) for all the
DD hypotheses obtained from a typical fold of the Musk1  Table 2: Testing accuracies (%) on the image data set.
data. As can be seen, for the MG-DDV kernel, a small subset                 5 repetitions 50 repetitions
                                                               DD-SVM       81.50 ± 3.00     -
of P (h|D) values are very high, and so the summation in (8)
                                                               Hist-SVM     66.70 ± 2.20     -
is dominated by these few hypotheses. On the other hand, for                    ±
                      ( |D)                                    MI-SVM       74.70 0.60       -
the MG-ACC kernel, the P h  values vary much gradually         MI kernel    84.12 ± 0.90 84.12 ± 1.22
(Figure 2(b)), and hence more hypotheses can be utilized in MG-DDV kernel  76.52 ± 17.29     -
           (  |x )                    ( |D)
computing P cij ij . Moreover, recall that P h in (6) is    MG-ACC kernel   84.64 ± 1.28 84.54 ± 1.21
proportional to h’s DD value, while the one in (11) is propor-
tional to h’s training accuracy. Hence, the almost linear trend
in Figure 2(b) shows that the higher the DD value, the higher website8, and three more9 (LJ-16.50.2, LJ-80.206.1 and LJ-
is the training accuracy of the hypothesis. It thus provides 160.566.1) used in a recent study [Cheung and Kwok, 2006].
another evidence for the success of the DD algorithm. The latter were obtained by adding irrelevant features to the
                                                      former data sets, while keeping its real-valued outputs intact.

                             -3
                            x 10
                    MG-DDV                     1        As demonstrated in Section 5.1, the MG-ACC kernel is su-
   0.2              MG-ACC                            perior than the MG-DDV kernel. Hence, we will only exper-
                           6                   0.9
  0.15                                                iment with the MG-ACC kernel deﬁned with (17) here. As
                                               0.8    in [Dooly et al., 2002], we report both the percentage error


  P(h|D) 0.1
                           P(h|D)                           10
                                               0.7    (%err)  and mean squared error (MSE).
                                                training  accuracy
  0.05
                           4                   0.6      Table 3 shows the results, along with those of DD, EM-
                                                      DD and citation-kNN [Wang and Zucker, 2000] as reported
    0  
    0    50   100  150      0   50   100 150  200
           DD hypothesis           DD hypothesis      in [Cheung and Kwok, 2006]. As can be seen, the proposed
    P (h|D)
 (a)        values for the (b) A closer look on the   MG-ACC kernel with the Gaussian instance label kernel c in
                          P (h|D)                                                                    k
 MG-DDV and MG-ACC ker-          values for the MG-   (14) consistently outperforms the others. Its superiority over
 nels.                    ACC kernel.                 the MG-ACC kernel (with a linearly decaying instance label
Figure 2: P (h|D) values obtained from a typical fold of the kernel) indicates that a much smaller weight should be as-
Musk1 data. The x-axis shows the different hypotheses ob- signed to instance pairs whose labels are quite different (Fig-
tained by the DD algorithm, sorted in increasing P (h|D) val- ure 1). This also explains the relatively inferior performance
ues as obtained for the MG-ACC kernel.                of the MI kernel, which weights all instance pairs equally.

Image Categorization                                  6Conclusion
The second experiment is on an image categorization task us- In this paper, we show how to design marginalized kernels in
ing the data set7 in [Chen and Wang, 2004].Thereare10 multiple-instance learning. First, we pretend that all the hid-
classes (beach, ﬂowers, horses, etc.), with each class contain- den instance labels are known, and deﬁne a joint kernel using
ing 100 images. Each image is regarded as a bag, and each both the instance inputs and labels. By integrating out the
segment an instance. We follow exactly the same setup as hidden data, the marginalized kernel is obtained. This kernel
[Chen and Wang, 2004]. The data is randomly divided into a differs from the existing MI kernel in that different instance
training and test set, each containing 50 images of each cat- pairs are weighted by the consistency of their probabilistic
egory. Since this is a multi-class classiﬁcation problem, we labels. Experimentally, this marginalized MI kernel always
employ the standard one-vs-rest approach to convert it to a performs better than the MI kernel and also often outperforms
number of binary classiﬁcation problems. The experiment is other traditional MI learning methods.
repeated 5 times, and the average testing accuracy reported. Note that the proposed kernel can be straightforwardly
  Table 2 shows the results, along with those of the DD- used in the regularization framework recently proposed in
SVM, Hist-SVM  [Chapelle et al., 1999] and MI-SVM as re- [Cheung and Kwok, 2006]. Moreover, because of the mod-
ported in [Chen and Wang, 2004]. As can be seen, the MG- ularity of the marginalized kernel, we will also explore even
ACC kernel is again superior to the MI kernel and others. To better deﬁnitions of the joint kernel and probabilistic model.
ensure that the improvement over the MI kernel is statisti-
cally signiﬁcant, we repeat the experiment 50 times (instead Acknowledgments
of only 5). The difference is conﬁrmed to be signiﬁcant at the
0.05 level of signiﬁcance by using the paired t-test. This research has been partially supported by the Research
                                                      Grants Council of the Hong Kong Special Administrative Re-
5.2  MI Regression: Synthetic Musk Molecules          gion.
                                                         8
We perform MI regression on the data set used in [Dooly et http://www.cs.wustl.edu/∼sg/multi-inst-data. There are four more data
al., 2002], where the goal is to predict the binding energies sets available. However, they are easier variations of the three that
of the musk molecules. We use three data sets (LJ-16.30.2, we used, and so are dropped in this study.
LJ-80.166.1 and LJ-160.166.1) downloaded from the author’s 9http://www.cs.ust.hk/∼jamesk/papers/icml06 data.zip
                                                        10%err is the classiﬁcation error obtained by thresholding both the
  7
   http://www.cs.uno.edu/∼yixin/ddsvm.html            target output and the predicted output at 0.5.

                                                IJCAI-07
                                                   905