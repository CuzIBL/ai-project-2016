    A Weighted Polynomial Information Gain Kernel for Resolving Prepositional 
             Phrase Attachment Ambiguities with Support Vector Machines 

                          Bram Vanschoenwinkel* Bernard Manderick 
       Vrije Universiteit Brussel, Department of Computer Science, Computational Modeling Lab 
                                   Pleinlaan 2, 1050 Brussel, Belgium 
                                 {bvschoen® - bernard@arti}.vub.ac.be 

                     Abstract                          and Nagao, 1997]. A computer program usually cannot rely 
                                                       on that kind of knowledge. 
    We introduce a new kernel for Support Vector Ma•     This problem has already been tackled using memory-
    chine learning in a natural language setting. As a based learning like for example K-nearest neighbours. Here, 
    case study to incorporate domain knowledge into    the training examples are first stored in memory and classi•
    a kernel, we consider the problem of resolving     fication of a new example is done based on the closest ex•
    Prepositional Phrase attachment ambiguities. The   ample stored in memory. Therefore, one needs a function 
    new kernel is derived from a distance function     that expresses the distance or similarity between examples. 
    that proved to be succesful in memory-based learn• There already exist several dedicated distance functions to 
    ing. We start with the Simple Overlap Metric from  solve all kind of natural language problems using memory-
    which we derive a Simple Overlap Kernel and ex•    based learning [Veenstra et al, 2000; Zavrel et al, 1997; 
    tend it with Information Gain Weighting. Finally,  Daelemans et al, 2002]. 
    we combine it with a polynomial kernel to increase   We will use a Support Vector Machine (SVM) to tackle the 
    the dimensionality of the feature space. The clo•  problem of PP attachment disambiguation. Central to SVM 
    sure properties of kernels guarantee that the result learning is the kernel function K : .X x X -> R where X 
    is again a kernel. This kernel achieves high clas• contains the examples and the kernel K calculates an inner 
    sification accuracy and is efficient in both time and product in a second space, the feature space F. This product 
    space usage. We compare our results with those ob• expresses how similar examples are. 
    tained by memory-based and other learning meth•      Our goal is to combine the power of SVMs with the dis•
    ods. They make clear that the proposed kernel      tance functions that arc well-suited for the probem for which 
    achieves a higher classification accuracy.         they were designed. Deriving a distance from a kernel is 
                                                       straightforward, see Section 2.1. However, deriving a kernel 
1 Introduction                                         from a distance is not trivial since kernels must satisfy some 
                                                       extra conditions, i.e. being a kernel is a much stronger con•
An important issue in natural language analysis is the resolu• dition than being a distance. In this paper we will describe 
tion of structural ambiguity. A sentence is said to be struc• a method that shows how such dedicated distance functions 
turally ambiguous when it can be assigned to more than one can be used as a basis for designing kernels that sequentially 
syntactic structure [Zavrel et al. , 1997]. In Prepositional can be used in SVM learning. 
Phrase (PP) attachment one wants to disambiguate between We use the PP attachment problem as a case study to illus•
cases where it is uncertain whether the PP attaches to the verb trate our approach. As a starting point we take the Overlap 
or to the noun.                                        Metric that has been succesfully used in memory-based learn•
Example 1 Consider the following two sentences:        ing for the same problem [Zavrel et al, 1997]. 
                                                         Section 2 will give a short overview of the theory of SVMs 
  1. I bought the shirt with pockets.                  together with some theorems and definitions that are needed 
  2. 1 washed the shirt with soap.                     in Section 4. Based on [Zavrel et al, 1997], section 3 gives 
                                                       an overview of metrics developed for memory-based learning 
  In sentence 1, with modifies the noun shirt because with 
                                                       applied to the PP attachment problem. In Section 4 the new 
pockets (PP) describes the shirt. In sentence 2 however, with 
                                                       kernels will be introduced. Finally Sections 5 and 6 give some 
modifies the verb washed because with soap (PP) describes 
                                                       experimental results and a conclusion of this work. 
how the shirt is washed [Ratnaparkhi, 1998]. 
  This type of attachment ambiguity is easy for people to 2 Support Vector Machines 
resolve because they can use their world knowledge [Stetina 
                                                       For simplicity, in our explanation we will consider the case 
  * Author funded by a doctoral grant of the institute for advance• of binary classification only, i.e. we consider an input space 
ment of scientific technological research in Flanders (1WT). X with input vectors x and a target space D = {1, -1}. The 


CASE-BASED REASONING                                                                                   133  goal of the SVM is to assign every x to one of two classes 
 D = {1,-1}. The decision boundary that separates the in•
 put vectors belonging to different classes is usually an arbi•
 trary n — 1-dimensional manifold if the input space X is n-
 dimensional. 

                                                               2.1 Some Properties of Kernels 


                                                               even if we don't know the exact form of the features that are 
                                                               used in F. Moreover, the kernel expresses prior knowledge 
                                                               about the patterns being modelled, encoded as a similarity 
                                                               measure between two vectors [Brown et al, 1999]. 
                                                                 But not all maps over X x X are kernels. Since a kernel K 
                                                               is related to an inner product, cfr. the definition above, it has 
                                                               to satisfy some conditions that arise naturally from the defini•
                                                               tion of an inner product and are given by Mercer's theorem: 
                                                               the map must be continuous and positive definite I Vapnik, 
                                                               1998]. 
                                                                 In this paper we will use following methods to construct 
                                                               kernels iCristianini and Shawe-Taylor, 2000]: 
   SVMs sidestep both difficulties [Vapnik, 1998]. First, 
                                                               M1 Making kernels from kernels: Based on the fact that ker•
overfitting is avoided by choosing the unique maximum mar•
                                                                    nels satisfy a number of closure properties. In this case, 
gin hyperplane among all possible hyperplanes that can sep•
                                                                    the Mercer conditions follow naturally from the closure 
arate the data in F. This hyperplane maximizes the distance 
                                                                    properties of kernels. 
to the closest data points. 
                                                               M2 Making kernels from features: Start from the features 
                                                                    of the input vectors and obtain a kernel by working out 
                                                                    their inner product. A feature is a component of the in•
                                                                    put vector. In this case, the Mercer conditions follow 
                                                                    naturally from the definition of an inner product. 


   7b be more precise, once we have chosen a kernel K we 
can represent the maximal margin hyperplane (or decision 
boundary) by a linear equation in x 


convex quadratic objective function with linear constraints. 
Moreover, most of the cti prove to be zero. By definition the  3 Metrics for Memory-based Learning 
vectors xi corresponding with non-zero a, are called the sup•
port vectors SV and this set consists of those data points that In this section, we will focus on the distance functions [Zavrel 
lie closest to the hyperplane and thus are the most difficult to et al, 1997; Cost and Salzberg, 1993] used for memory-based 
classify.                                                      learning with symbolic values. First, we will have a look at 
                                                               the Simple Overlap Metric (SOM) and next we will discuss 
   In order to classify a new point xnew, one determines the 
sign of                                                        Information Gain Weighting (1GW). Memory-based learning 
                                                               is a class of machine learning techniques where training in•
                                                               stances are stored in memory first and classification of new 
                                                               instances later on is based on the distance (or similarity) be•

If this sign is positive xnew, belongs to class 1, if negative to tween the new instance and the closest training instances that 
class -1, if zero xnew lies on the decision boundary. Note     have already been stored in memory. A well-known example 
that we have now restricted the summation to the set SV of     of memory-based learning is k-nearest neighbours classifica•
support vectors because the other a, are zero anyway.          tion. We will not go into further detail, the literature offers 


134                                                                                           CASE-BASED REASONING  many books and articles that provide a comprehensive intro•       A Kernel for Natural Language settings 
 duction to memory-based learning, e.g. [Mitchell, 1997]. For 
 our purpose, the important thing to remember is that we will 
 be working with symbolic values like strings over an alphabet 
 of characters. Instances are n-dimensional vectors a and b in 


 3.1 Simple Overlap Metric 
 The most basic metric for vectors with symbolic values is the 
 Simple Overlap Metric(SOM) [Zavrel et a/., 1997]: 


 Here dSOM (a,b) is the distance between the vectors a and 
b, represented by n features and S is the distance per feature. 
The k;-ncarest neighbour algorithm equipped with this metric 
 is called IB1 [Aha et al, 1991]. The IB1 algorithm simply 
counts the number of (mis)matching feature values in both 
vectors. This is a reasonable choice if we have no information 
about the importance of the different features. But if we do 
have information about feature importance then we can add 
linguistic bias to weight the different features. 
3.2 Information Gain Weighting 
Information Gain Weighting (1GW) measures for every fea•
ture i separately how much information it contributes to our 
knowledge of the correct class label. The Information Gain 
(IG) of a feature i is measured by calculating the entropy be•
tween the cases with and without knowledge of the value of 
that feature: 
                                                              We don't have to proof that the kernel ksok satisfies the Mer•
                                                              cer conditions because this follows naturally from the defini•
                                                              tion of the inner product. We started from the features, de-

                                                              inner product between them, see M2 from Section 2.1. How•
                                                              ever, to show that the kernel really corresponds to the distance 
                                                              function given in Equations 3 and 4 we have to verify the dis•
                                                              tance formula for kernels given in Section 2.1: 


weights Wi can be used to extend Equation 2 with weights 
(see Equation 6). The k-nearest neigbour algorithm equipped 
with this metric is called 1B1-1G [Zavrel et al, 1997] and the 
corresponding distance called dIG: 


CASE-BASED REASONING                                                                                                135 4.2 Extending the SOK to n Dimensions 


                                                              4.4 A Polynomial Information Gain Kernel 
                                                              In this section, we will increase the dimensionality of the fea•
                                                              ture space F by making use of a polynomial kernel Kpoly. 
                                                              We will start this section by giving an example [Cristianini 
                                                              and Shawe-Taylor, 2000]: 
We don't have to proof that the kernel KSOK is a valid 
kernel as this follows naturally from the definition of 
the inner product. However, we again have to show that 
it is a kernel that corresponds to the distance function 
dSOM from Equation 2. In the following we will show 


However, this does not impose any problems for the kernel 
we are aiming to develop. 
4.3 Adding Information Gain to the SOK 


136                                                                                          CASE-BASED REASONING                                                                         5.2 Experimental Setup 
                                                                        The experiments have been done with LIBSVM, a C/C++ and 
                                                                        Java library for SVMs [Chih-Chung and Chi-Jen, 2002]. The 
                                                                        machine we have used is a Pentium III with 256MB RAM 
                                                                        memory, running Windows XP. We choose to implement the 

                                                                        kernels KIG and KPIG in Java. 
From the closure properties of kernels, it follows naturally            The type of SVM learning we have used is C-SVM [Chih-
that KPIG indeed is a valid kernel which calculates the in•             Chung and Chi-Jen, 2002]. The parameter C controls the 
ner product of two vectors transformed by a feature mapping             model complexity versus the model accuracy and has to be 
                                                                        chosen based on the complexity of the problem. 
                                                                           The experiments in this section are conducted on a simpli•
                                                                        fied version of the full PP attachment problem (see Example 
                                                                        3). The data consists of four-tuples of words, extracted from 
                                                                        the Wall Street Journal Treebank [Marcus et al., 1993] by a 

                                                                        group at IBM [Ratnaparkhi et al. , 1994]1. 
                                                                           The data set contains 20801 training patterns, 3097 test 
                                                                        patterns and an independant validation set of 4093 patterns 
                                                                        for parameter optimization. All of the models described be-
5 Experimental Results                                                  low were trained on all training examples and the results are 
                                                                        given for the 3097 test patterns. For the benchmark compari•
                                                                        son with other models from the literature, we use only results 
                                                                        for which all parameters have been optimized on the valida•
                                                                        tion set. For more details concerning the data set we refer 
                                                                        to [Zavrel et al. , 1997]. 
                                                                        5.3 Results 

5.1 PP attachment disambiguation problem 
If it is uncertain in a sentence whether the preposition attaches 
to the verb or to the noun then we have a prepositional phrase 
(PP) attachment problem. For example, in sentence 1 of Ex•
ample 1, with modifies the noun shirt because with pockets 
(PP) describes the shirt. In contrast, in sentence 2, with mod•
ifies the verb washed because with soap (PP) describes how 
the shirt is washed [Ratnaparkhi, 1998]. 


  In fact, we only keep those words that are of 
any importance to the PP attachment problem, i.e. 
(V(erb),N(oun),P(reposition),N(oun)). 

  In the case where sentences are reduced to quadruples as 
illustrated in Example 3, the human performance is approx•
imately 88.2% [Ratnaparkhi et al, 1994]. This performance 
                                                                           The fact that the kernel KIG performs worse than IB1-IG, 
rate gives us an acceptable upper limit for the maximum                 although it is equipped with the very same distance metric, 
performance of a computer because it seems unreasonable                 may seem somehow surprising. We believe this is because 
to expect an algorithm to perform much better than a hu•                SVMs perform linear separation in the feature space F. The 
man. As we will show in our experimental results the kernel             decision boundary of IB1-IG on the other hand is non-linear. 
KIG achieves a classification accuracy up to 82,9%, see Sec•            Due to the linearity of the decision boundary of the SVM, 
tion 5.3. However, in [Zavrel et al. , 1997] the 1B1-1G attains         some points get misclassified. The number of misclassifica-
a maximum classification accuracy of 84.1%, this is a good              tions is controlled by the parameter C. Choosing a larger 
indication of the classification accuracy that should be possi•
ble to obtain with a kernel based on the distance defined in 
Equation 6. 


CASE-BASED REASONING                                                                                                                   137 