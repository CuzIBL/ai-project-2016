                       Identifying Expressions of Opinion in Context∗

                            Eric Breck and Yejin Choi and Claire Cardie
                                    Department of Computer Science
                                           Cornell University
                          {ebreck,ychoi,cardie}@cs.cornell.edu


                    Abstract                            Consider the following sentences, in which we denote two
                                                      kinds of opinion expression in boldface and italic (described
    While traditional information extraction systems
                                                      below).
    have been built to answer questions about facts,
    subjective information extraction systems will an- 1: Minister Vedrine criticized the White House reaction.
    swer questions about feelings and opinions. A cru- 2: 17 persons were killed by sharpshooters faithful to the
    cial step towards this goal is identifying the words  president.
    and phrases that express opinions in text. Indeed, 3: Tsvangirai said the election result was “illegitimate”and
    although much previous work has relied on the         a clear case of “highway robbery”.
    identiﬁcation of opinion expressions for a variety
    of sentiment-based NLP tasks, none has focused    4: Criminals have been preying on Korean travellers in
    directly on this important supporting task. More-     China.
    over, none of the proposed methods for identiﬁca- To understand, extract, or answer questions about the opin-
    tion of opinion expressions has been evaluated at ions in these sentences a system must (minimally) determine
    the task that they were designed to perform. We   the basic attributes of the opinion: Is its polarity positive, neg-
    present an approach for identifying opinion expres- ative, or neutral? With what strength or intensity is the opin-
    sions that uses conditional random ﬁelds and we   ion expressed: mild, medium, strong or extreme? Who or
    evaluate the approach at the expression-level us- what is the source, or holder, of the opinion? What is its tar-
    ing a standard sentiment corpus. Our approach     get, i.e. what is the opinion about? The opinion expressions
    achieves expression-level performance within 5%   marked in the sentences above are the key to answering these
    of the human interannotator agreement.            questions. In particular, the marked phrases denote the po-
                                                      larity of the opinion: for example, “criticized” and “faithful
1  Introduction                                       to” (examples 1 and 2) denote negative and positive attitudes,
                                                      respectively. The opinion expressions also often provide lin-
Traditional information extraction tasks can be viewed as be- guistic anchors for the automatic extraction of the source and
ginning with a set of questions about facts, such as who?, target of the opinion. The predicate “criticized”, for exam-
where?,orhow many?. Researchers then build systems to ex- ple, organizes the semantic roles that denote the source of the
tract the answers to these questions from text [MUC7, 1998; opinion (the agent role = “Minister Vedrine”) and the target
NIS, 2005; DUC2005, 2005]. More recently, researchers of the opinion (the object/theme role = “White House reac-
have investigated the construction of language-processing tion”). Similarly, the opinion expression “faithful to” orga-
systems that extract answers to subjective questions, such as nizes the semantic roles associated with the source (the agent
how does X feel about Y? (see Section 4). Intelligence ana- role = “sharpshooters”) and the target (the object/theme role
lysts, for example, could use such opinion-oriented systems = “the president”) of the opinion in example 2.
to monitor trouble spots around the world while marketing re- Wiebe et al. [2005] distinguish two types of opinion ex-
searchers could use them to understand public opinion about pressions, and we follow their deﬁnitions here. Direct subjec-
their product.                                        tive expressions (DSEs), shown in boldface, are spans of text
  As with factual information extraction and question an- that explicitly express an attitude or opinion. “Criticized” and
swering, subjective information extraction and question an- “faithful to” (examples 1 and 2), for example, directly denote
swering will require techniques to analyze text below the sen- negative and positive attitudes towards the “White House re-
tence level. For example, Stoyanov et al. [2005] show that action” and “the president”, respectively. Speech events like
identifying opinion expressions is helpful in localizing the an- “said” in example 3 can be DSEs if they express subjectivity.
swers to opinion questions.                           In contrast, expressive subjective elements (ESEs), shown in
  ∗This work was supported by the Advanced Research and De- italics, are spans of text that indicate, merely by the speciﬁc
velopment Activity (ARDA), by NSF Grants IIS-0535099 and IIS- choice of words, a degree of subjectivity on the part of the
0208028, and by gifts from Google and the Xerox Foundation. speaker. The phrases “illegitimate” and “highway robbery”,

                                                IJCAI-07
                                                  2683 IOB   ...faithful/B to/I the/O president/O ./O       2.1  The class variable
 IO    ...faithful/I to/I the/O president/O ./O
                                                      A common encoding for extent-identiﬁcation tasks such as
Figure 1: How to encode the class variable: The IOB method named entity recognition is the so-called IOB encoding. Us-
and the IO method. IO is used in this paper.          ing IOB, each word is tagged as either Beginning an entity,
                                                      being In an entity (i.e. an opinion expression), or being Out-
                                                      side of an entity (see Figure 1). While we initially used this
for example, indirectly relay Tsvangirai’s negative opinion of encoding, preliminary investigation on separate development
“the election result” (example 3), and the use of “preying on” data revealed that a simpler binary encoding produces better
(instead of, say, “mugging”) indicates the writer’s sympathy results. We suspect this is because it is rarely the case in our
for the Korean travellers in example 4.               data that two entities are adjacent, and so the simpler model
  While some previous work identiﬁes opinion expressions is easier to ﬁt. Thus, we tag each token as being either In an
in support of sentence- or clause-level subjectivity classiﬁ- entity or Outside of an entity. When predicting, a sequence of
cation [Riloff and Wiebe, 2003; Wiebe and Wilson, 2002], consecutive tokens tagged as In constitutes a single predicted
none has directly tackled the problem of opinion expression entity.
identiﬁcation, developed methods for the task, and evaluated
performance at the expression level. Instead, previous work 2.2 Features
in this area focuses its evaluation on the sentence-level sub- In this section, we describe the features used in our model.
jectivity classiﬁcation task.                         We include features to allow the model to learn at various lev-
  In this work, we treat the identiﬁcation of opinion expres- els of generality. We include lexical features to capture spe-
sions as a tagging task, and use conditional random ﬁelds to ciﬁc phrases, local syntactic features to learn syntactic con-
address this problem. For comparison, we chose two lists text, and dictionary-based features to capture both more gen-
of clues to subjectivity from previous work [Wilson et al., eral patterns and expressions already known to be opinion-
2005b; Wiebe and Riloff, 2005]. These clues have previously related. The same feature set is used for identifying both
been evaluated only for their utility in clause- or sentence- types of subjective expression. For pedagogical reasons, we
level classiﬁcation. Here we interpret the clues as expressions present the features as categorically valued, but in our model
of opinion and compare them to our results. We achieve F- we encode all features in binary (i.e. a feature (f,v) is 1 for
measures of 63.4% for ESEs and 70.6% for DSEs, within 5% atokent if f(t)=v,and0otherwise).
of the human interannotator agreement for DSEs and within
10% for ESEs.
                                                                                       lex
  The rest of this paper proceeds as follows. In Section 2, we Lexical features We include features i, deﬁned to be the
                                                                     i
discuss our approach to identifying direct subjective expres- word at position relative to the current token. We include
                                                      lex  ,lex  ,...lex
sions and expressive subjective elements. In Section 3, we −4  −3       4. These are encoded into about 18,000
present experimental results. In Section 4, we present related binary features per position (i.e. the vocabulary size).
work and in Section 5, we conclude and discuss future work.
                                                      Syntactic features We include a feature part-of-speech,de-
2  The approach                                       ﬁned to be the part of speech of the current token according
                                                      to the GATE part-of-speech tagger [Cunningham et al., 2002]
As the example sentences in Section 1 suggest, identifying (encoded into 45 binary features). We also include three fea-
subjective expressions is a difﬁcult task. The expressions can tures prev, cur, and next, deﬁned to be the previous, current,
vary in length from one word to over twenty words. They or following constituent type, respectively, according to the
may be verb phrases, noun phrases, or strings of words that CASS partial parser [Abney, 1996]2. These are encoded into
do not correspond to any linguistic constituent. Subjectivity about 100 binary features each.
is a realm of expression where writers get quite creative, so no
short ﬁxed list can capture all expressions of interest. Also,
an expression which is subjective in one context is not al- Dictionary-based features We include features from four
ways subjective in another context [Wiebe and Wilson, 2002]. sources. We include a feature WordNet,deﬁnedtobeall
Therefore, we present in this section our machine learning ap- synsets which are hypernyms of the current token in the
proach for the identiﬁcation of direct subjective expressions WordNet hierarchy [Miller, 1995]. This is encoded into
and expressive-subjective elements. We treat the task as a tag- 29,989 binary features, many of which may be 1 for a given
ging problem, and use conditional random ﬁelds [Lafferty et token. We include a feature Levin, deﬁned to be the section
al., 2001]1. Below we discuss the encoding of the class vari- of Levin’s [1993] categorization of English verbs in which a
able (Section 2.1), the features (Section 2.2), and the learning given verb appears, and a feature Framenet,deﬁnedtobethe
method (Section 2.3).                                 category of a word in the categorization of nouns and verbs
                                                      in Framenet3. Finally, we include a feature that speciﬁcally
  1CRFs allow the use of a large, diverse set of features, while targets subjective expressions. Wilson et al. [2005b] identify
offering the choice of modeling individual tokens or sequences.
Margin-based methods would be another natural option, but pilot ex- 2Cass is available for download at http://www.vinartus.
periments on development data suggested the performance of SVMs net/spa/.
was inferior to CRFs for this task.                      3http://www.icsi.berkeley.edu/˜framenet/

                                                IJCAI-07
                                                  2684                                                            a    |{p|p ∈ P ∧∃c ∈ Cs.t.a(c,p)}|
         number of sentences   8297                   as SP   =             |P |          and soft recall as
         number of DSEs        6712                   SRa   =  |{c|c ∈ C ∧∃p ∈ Ps.t.a(c,p)}|   a(c, p)
         number of ESEs        8640                                       |C|          ,where        is a
                                                                                     c
         average length of DSEs 1.86 words            predicate true just when expression “aligns” to expression
                                                      p                   a
         average length of ESEs 3.33 words              in a sense deﬁned by . We report results according to two
                                                      predicates: exact and overlap. exact(c, p) is true just when
                                                      c    p
            Table 1: Statistics for test data           and  are the same spans - this yields the usual notions of
                                                      precision and recall. A softer notion is produced by the pred-
                                                      icate overlap(c, p), which is true when the spans of c and p
a set of clues as being either strong or weak cues to the sub- overlap5.
jectivity of a clause or sentence. We identify any sequence
of tokens included on this list, and then deﬁne a feature Wil- 3.3 Baselines
son that returns the value ‘-’ if the current token is not in any
recognized clue, or strong or weak if the current token is in For baselines, we compare to two dictionaries of subjectiv-
a recognized clue of that strength. This clue is encoded into ity clues identiﬁed by previous work [Wilson et al., 2005b;
two binary features (the ‘-’ case is not encoded).    Wiebe and Riloff, 2005]. These clues were collected to help
                                                      recognize subjectivity at the sentence or clause level, not at
2.3  The learning method                              the expression level, but the clues often correspond to subjec-
We chose to use a linear-chain conditional random ﬁeld tive expressions. Each clue is one to six consecutive tokens,
model for all of our experiments, using the MALLET toolkit possibly allowing for a gap, and matching either stemmed or
[McCallum, 2002]. This discriminatively-trained sequence unstemmed tokens, possibly of a ﬁxed part of speech. In the
model has been found to perform extremely well on tagging following experiments, we report results of the Wiebe base-
tasks such as ours [Lafferty et al., 2001]. Based on pilot ex- line, which predicts any sequence of tokens matching a clue
periments on development data, we chose a Gaussian prior of from Wiebe and Riloff [2005] to be a subjective expression,
0.25.                                                 and the Wilson baseline, using similar predictions based on
                                                      clues from Wilson et al. [2005b]. When predicting DSEs, we
                                                      remove all clues from the list which never match a DSE in the
3  Experiments                                        test data, to make the baseline’s precision as high as possible
In this section, we describe the data and evaluations used in (although since many potentially subjective expressions are
our experiments, describe the baselines we compare to, and often not subjective, the precision is still quite low). We sim-
present our results.                                  ilarly trim the lists when predicting the other targets below.
                                                      Apart from this trimming, the lists were not derived from the
3.1  Data                                             MPQA corpus. Note that the higher-performing of these two
The MPQA corpus  [Wiebe et al., 2005]4 consists of 535 baselines, from Wilson et al. [2005b], was incorporated into
newswire documents annotated with a variety of annotations the feature set used in our CRF models6.
of interest for subjectivity research. In particular, all DSEs
and ESEs in the documents have been manually identiﬁed. In 3.4 Results
this work, we used 135 documents for development of our Tables 2 and 3 present experimental results on identifying
features and determination of parameters, and kept the re- direct subjective expressions and expressive subjective ele-
maining 400 blind for evaluation. All of our evaluation results ments in several settings, as well as presenting the two base-
use 10-fold cross-validation on the 400 documents. Table 1 lines for comparison purposes. We experiment with two vari-
presents some statistics on these 400 documents.      ants of conditional random ﬁelds, one with potentials (fea-
3.2  Evaluation                                       tures) for Markov order 1+0 (similar to the features in a hid-
                                                      den Markov model, labeled crf-1 in the tables), and one with
As with other information extraction tasks, we use preci- features only for order 0 (equivalent to a maximum entropy
sion, recall and F-measure to evaluate our method’s per-
                     |C∩P |            |C∩P |            5
formance. Precision is |P | and recall is |C| ,where     A potential issue with soft precision and recall is that the mea-
C and P  are the sets of correct and predicted expression sures may drastically overestimate the system’s performance. A sys-
spans, respectively. F is the harmonic mean of precision tem predicting a single entity overlapping with every token of a doc-
          2PR                                         ument would achieve 100% soft precision and recall with the over-
and recall, P +R . Our method often identiﬁes expressions lap predicate. We can ensure this does not happen by measuring the
that are close to, but not precisely the same as, the man- average number of expressions to which each correct or predicted
ually identiﬁed expressions. For example, for the expres- expression is aligned (excluding expressions not aligned at all). In
sion “roundly criticized,” our method might only identify our data, this does not exceed 1.13, so we can conclude these evalu-
“criticized.” We therefore introduce softened variants of ation measures are behaving reasonably.
precision and recall as follows. We deﬁne soft precision 6The CRF features based on the Wilson dictionary were based
                                                      on the entire dictionary, including clues not relevant for the partic-
  4Available at http://www.cs.pitt.edu/mpqa/.Weuse    ular problem being tested. Also, the choice to use only the Wilson
version 1.1 of the corpus. Code and data used in our experiments dictionary and not the Wiebe for features was made during develop-
areavailableathttp://www.cs.cornell.edu/˜ebreck/      ment of the model on a separate development dataset. So the model
breck07identifying.                                   tested was in no way developed using the test data.

                                                IJCAI-07
                                                  2685                                          overlap                         exact
              method           recall    precision F          recall    precision F
              Wiebe baseline   45.692.4  31.102.5  36.972.3   21.521.8  13.911.4  16.871.4
              Wilson baseline  55.152.2  30.731.9  39.441.9   25.651.7  13.321.0  17.521.2
              crf-1-DSE        60.221.8  79.343.2  68.442.0   42.652.9  57.652.8  49.012.8
              crf-1-DSE&ESE    62.732.3  77.993.1  69.512.4   43.232.9  55.382.8  48.542.8
              crf-0-DSE        65.482.0  74.853.5  69.832.4   39.952.4  44.522.2  42.102.2
              crf-0-DSE&ESE    69.221.8  72.163.2  70.652.4   42.132.3  42.692.5  42.402.3

       Table 2: Results for identifying direct subjective expressions. Superscripts designate one standard deviation.


                                          overlap                         exact
              method           recall    precision F          recall    precision F
              Wiebe baseline   56.362.1  43.034.5  48.663.3   15.091.1  9.911.6   11.921.4
              Wilson baseline  66.102.6  40.944.7  50.384.0   17.231.9  8.761.5   11.561.6
              crf-1-ESE        46.364.1  75.216.6  57.143.6   15.111.7  27.282.3  19.351.5
              crf-1-DSE&ESE    48.793.2  74.096.7  58.703.7   15.581.1  26.182.1  19.460.8
              crf-0-ESE        61.223.4  64.845.4  62.823.3   18.311.7  17.113.0  17.612.2
              crf-0-DSE&ESE    63.463.3  63.765.7  63.433.3   18.961.4  16.792.5  17.741.8

       Table 3: Results for identifying expressive subjective elements. Superscripts designate one standard deviation.


                                          overlap                         exact
              method           recall    precision F          recall    precision F
              Wiebe baseline   51.592.0  61.354.6  55.992.8   17.700.8  19.612.0  18.581.2
              Wilson baseline  61.232.1  58.484.7  59.733.1   20.611.4  17.681.5  19.001.3
              crf-1-DSE+ESE    64.772.2  81.334.4  72.032.2   26.682.7  39.232.6  31.702.4
              crf-1-DSE&ESE    62.362.1  81.904.1  70.742.2   28.242.7  42.641.9  33.922.3
              crf-0-DSE+ESE    74.702.5  71.644.5  73.052.8   30.932.5  28.202.3  29.442.0
              crf-0-DSE&ESE    71.912.2  74.044.5  72.882.6   30.302.2  29.642.3  29.911.8

Table 4: Results for identifying expressions which are either DSEs or ESEs. Superscripts designate one standard deviation.
DSE&ESE indicates a model trained to make a three-way distinction among DSEs, ESEs, and other tokens, while DSE+ESE
indicates a model trained to make a two-way distinction between DSEs or ESEs and all other tokens.


                                                   overlap                         exact
     feature set                        recall    precision F          recall    precision F
     base                               47.142.6  70.914.4  56.603.0   30.552.7  45.123.1  36.412.8
     base + Levin/FN                    50.573.1  70.514.1  58.863.3   32.203.1  44.113.3  37.203.1
     base + Wilson                      54.922.4  70.734.0  61.812.9   34.612.5  43.602.9  38.572.5
     base + Wilson + Levin/FN           57.212.6  70.794.1  63.263.0   35.772.4  43.422.8  39.212.5
     base + WordNet                     68.292.4  71.823.5  70.002.8   41.802.5  42.712.5  42.242.4
     base + Wilson + WordNet            68.932.1  72.063.3  70.452.6   42.102.5  42.712.6  42.402.5
     base + Levin/FN + WordNet          68.482.4  71.873.3  70.132.8   41.922.2  42.802.5  42.342.3
     base + Levin/FN + WordNet + Wilson 69.221.8  72.163.2  70.652.4   42.132.3  42.692.5  42.402.3

Table 5: Results for feature ablation for identifying DSEs. FN is the FrameNet dictionary features. “base” indicates the lexical
features and the syntactic features. The bottom line represents the same model as CRF-0-DSE&ESE in Table 2.


                                                IJCAI-07
                                                  2686model, labeled crf-0 in the tables). Orthogonally, we com- Finally, we note that the interannotator agreement results
pare models trained separately on each task (classifying each for these tasks are relatively low; 0.75 for DSEs and 0.72 for
token as in a DSE versus not or in an ESE versus not, labeled ESEs, according to a metric very close to overlap F-measure8.
just DSE or ESE in the tables) to models trained to do both Our results are thus quite close to the human performance
tasks at once (classifying each token into one of three classes: level for this task.
in a DSE, in an ESE, or neither7, labeled DSE&ESE in the ta-
bles).
  Because the baselines were not designed to distinguish be- 4 Related work
tween DSEs and ESEs, we run another set of experiments Subjectivity research has been quite popular in recent years.
where the two categories are lumped together. The rows la- While ultimately research in lexicon building [Hatzivas-
beled DSE&ESE use the models trained previously to dis- siloglou and McKeown, 1997, e.g.], and classiﬁcation [Dave
tinguish three categories, but are here evaluated only on the et al., 2003, e.g.] may be relevant, we focus here on work in
binary decision of opinion expression or not. The rows la- extracting sub-sentential structure relevant to subjectivity.
beled DSE+ESE are trained to classify a token as I if it is in Bethard et al. [2004] address the task of extracting propo-
either a DSE or ESE, or O otherwise. The results of these sitional opinions and their holders. They deﬁne an opinion
experiments are reported in Table 4.                  as “a sentence, or part of a sentence that would answer the
  Finally, to determine the effect of the various dictionar- question ‘How does X feel about Y?’ ” A propositional opin-
ies, we examine all combinations of the various dictionaries - ion is an opinion “localized in the propositional argument”
WordNet, Framenet, Levin, and the clues from Wilson et al. of certain verbs, such as “believe” or “realize”. Their task
[2005b] (to save space, we combine the two smallest dictio- then corresponds to identifying a DSE, its associated direct
naries, Framenet and Levin, into one). These results, on the source, and the content of the private state. However, in each
DSE task, are reported in Table 5.                    sentence, they seek only a single verb with a propositional
3.5  Discussion                                       argument, whereas we may identify multiple DSEs per sen-
                                                      tence, which may be multi-word expressions of a variety of
We note that the order-0 models outperform the order-1 mod- parts of speech.
els according to overlap F-measure and recall, but by exact Another group of related work looks at identifying a class
F-measure and either precision metric, the order-1 models are of expressions similar to the DSEs we identify [Wiebe et al.,
superior. The creators of the dataset state “we did not attempt 2003; Munson et al., 2005; Wilson et al., 2005a] 9. We can-
to deﬁne rules for boundary agreement in the annotation in- not compare our results to theirs because this previous work
structions, nor was boundary agreement stressed during train- does not distinguish between DSEs and objective speech ex-
ing.” [Wiebe et al., 2005, page 35]. For example, whether a pressions, and because the prior results only address ﬁnding
DSE ought to be annotated as “ﬁrmly said” or just “said” is single word expressions.
left up to the annotator. Therefore, we hypothesize that the Another area of related work is reputation or sentiment
model with greater capacity (the order 0+1) may overﬁt to analysis [Morinaga et al., 2002; Nasukawa and Yi, 2003].
the somewhat inconsistent training data.              This work is in the context of marketing research, and in-
  The ablation results (in Table 5) indicate that the Word- volves identifying polarity and sentiment terminology with
Net features are by far the most helpful. The other two dic- respect to particular products or product categories. Their no-
tionary sets are individually useful (with the Wilson features tions of sentiment terms are related, but not identical, to ours.
being more useful than the Levin/Framenet ones), but above However, they do provide evidence that working at the ex-
the WordNet features the others make only a small difference. pression level is of interest to consumers of opinion-oriented
This is interesting, especially since the WordNet dictionary is information extraction.
entirely general, and the Wilson dictionary was built speciﬁ-
cally for the task of recognizing subjectivity. Ablation tables
for the other two targets (ESEs and DSE&ESE) look similar 5 Conclusions and Future Work
and are omitted.
  In looking at errors on the development data, we found Extracting information about subjectivity is an area of great
several causes which we could potentially ﬁx to yield higher interest to a variety of public and private interests. We have
performance. The category of DSEs includes speech events argued that successfully pursuing this research will require
like “said” or “a statement,” but not all occurrences of speech the same expression-level identiﬁcation as in factual infor-
events are DSEs, since some are simply statements of objec- mation extraction. Our method is the ﬁrst to directly ap-
tive fact. Adding features to help the model make this distinc- proach the task of extracting these expressions, achieving per-
tion should help performance. Also, as others have observed, formance within 5% of human interannotator agreement. In
expressions of subjectivity tend to cluster, so incorporating the future, we hope to improve performance even further by

features based on the density of expressions might help as 8
well [Wiebe and Wilson, 2002].                           Using the agr statistic, the interannotator agreement for ESEs
                                                      ontheMPQAdatais0.72[Wiebe  et al., 2005, page 36],andfor
  7A small number of tokens are manually annotated as being part DSEs is 0.75 (Theresa Wilson, personal communication). agr is the
of both a DSE and an ESE. For training, we label these tokens as arithmetic (rather than harmonic) mean of overlap recall and preci-
DSEs, while for testing, we (impossibly) require the model to anno- sion between two annotators.
tate both entities.                                      9The category is referred to as an on or private state frame.

                                                IJCAI-07
                                                  2687