                       Representations for Action Selection Learning
                       from Real-Time Observation of Task Experts
                                Mark A. Wood and Joanna J. Bryson
                            Artiﬁcial models of natural Intelligence (AmonI)
                                    Department of Computer Science
                                         University of Bath, UK
                                 {cspmaw,jjb}@cs.bath.ac.uk

                    Abstract                          the other apes. Understanding the complexity characteristics
                                                      of this task is key to both understanding human intelligence
    The association of perception and action is key to and harnessing the approach for AI.
    learning by observation in general, and to program-
    level task imitation in particular. The question is 1.1 System Overview
    how to structure this information such that learn-
    ing is tractable for resource-bounded agents. By  COIL is an adaption to generic imitation learning of Roy’s
    introducing a combination of symbolic represen-   language learning system, CELL. CELL is one of the most
    tation with Bayesian reasoning, we demonstrate    convincing examples to date of real-time interactive social
    both theoretical and empirical improvements to a  learning. It enables a robot to learn the names for various toys
    general-purpose imitation system originally based using the same sorts of input as an infant. Both CELL and
    on a model of infant social learning. We also     COIL are detailed elsewhere [Roy, 1999; Wood and Bryson,
    show how prior task knowledge and selective at-   2007]; here we give a skeletal overview of COIL, to clarify
    tention can be rigorously incorporated via loss ma- those parts most relevant to the extentions described later.
    trices and Automatic Relevance Determination re-
    spectively.
                                                        RawSensorData

1  Introduction                                                                  FeatureExtraction
                     [                     ]
Program-level imitation Byrne and Russon, 1998 ,orthe    Achannels
acquisition of behavioural structure from observation, is an
                                                         Pchannels
under-researched ﬁeld in AI. In robot AI, much effort has
rightly been directed toward action-level imitation; the repro-                  EventSegmentation
duction of movements involving a ﬁne degree of motor con-
trol. Indeed, this highly complex and difﬁcult task needs to be Aevents
solved by a robot before it is able to acquire structural data. Pevents
However, by using ‘intelligent’ virtual environments which                       CooccurenceFiltering
implicitly deal with low-level actions, we can gain access to a
rich class of higher-level problems. Unreal Tournament [Dig-
                                                        APevents
ital Extremes, 1999] is an example of such an environment,
and it is popular with those few looking at this problem [Thu-
rau et al., 2004; Le Hy et al., 2004]. It is also the domain of                 RecurrenceFiltering
choice for the system that underpins this paper: COIL [Wood
and Bryson, 2007].                                       MEcandidates
  In this paper we demonstrate how a formal Bayesian
framework can be incorporated into a complex modular learn-                      MutualInformationFiltering
ing system. Through this we widen its potential applicability
in theory, signiﬁcantly improve its learning performance in MEitems
practise, and add natural extensibility to the system via tried
and tested Bayesian methodology. Our experiments also en-
able us to examine the broader issue of the combinatorial Figure 1: The inputs and outputs of each stage of COIL
complexity of social learning. Social learning is an important [Wood and Bryson, 2007].
mechanism for acquiring intelligent behaviour — arguably it
accounts for the massive accumulation of culture and artifacts The learning part of COIL has ﬁve constituent stages of
seen in humans compared to our nearest biological relatives, processing (see Figure 1). It is designed to function embed-

                                                IJCAI-07
                                                   641dedinanimitator agent observing a conspeciﬁc expert agent actions1 required for an imitation task lie in a discrete space
executing a given task in a shared environment. The input to where ‘nearness’ is not only hard to deﬁne but not a partic-
the ﬁrst stage consists of the incoming raw data from the im- ularly useful concept. For example, how far is jump from
itator’s sensors. During Feature Extraction, these data are turn right, and what use would that information be any-
directed into separate channels which are one of two types: way? Perceptual channels are similarly unrelated since we
Action channels receive data pertaining to the actions taken are not simply learning to respond to different shapes, but
by the expert, and Perception channels receive data pertain- are interested in different proprioceptive / physical position-
ing (but not identical) to the perceptual state of the expert. ing as well as more thoroughly categorised ‘visual’ stimuli
Once in channels the data are segmented into A- and P-events (e.g. identifying an object vs. an agent). These comparisons
respectively, and then further into A- and P-subevents accord- point toward using a more thoroughly discrete representation
ing to a set of pre-programmed triggers; this is Event Seg- for both actions and perceptions throughout COIL.
mentation.TheCo-Occurrence Filter then simply binds co- An obvious solution might appear to be to reduce the level
occurring A- and P-events together as AP-events and shunts of abstraction at which the learning and action selection oc-
them to a queue called STM (Short Term Memory). When- curs. For example, by descending into ﬁner-grained mus-
ever a new AP-event arrives in STM, it is compared with cle and motor position spaces we would reclaim continuity.
each of those already there in turn. The Recurrence Filter There are two problems here: ﬁrstly, both of these spaces
scans for co-occurring A- and P-subevent pairs and, if any have unusual discontinuities which lead to the problems of
are found, binds them as a Motivation-Expectation or M-E inverse kinematics which ensnare traditional robotics. Sec-
Candidate and stores them in MTM (Mid Term Memory).   ondly, the complexity inherent in the remaining imitation
Finally, the Mutual Information Filter calculates a mutual (and subsequent action selection) process demands a reduced
information score for each M-E Candidate using a some- space of more generic operators. Thus rather than learn-
what complex algorithm not described here (see [Wood and ing 180 different turn-right commands, each varying by
Bryson, 2007] for details). Those candidates whose scores only one degree of turn arc, we simply learn to initiate turn-
exceed a pre-determined threshold are saved in LTM (Long right, and then check for stopping criteria. Note that this
Term Memory as M-E Items.                             solution is more robust to uncertain motion and sensing, as-
  The M-E Items themselves represent observed perception- suming the stopping or homing criteria are perceivable within
action pairs (though note the original agent may not have had some range of degrees during the turn.
this model [Bryson and Wood, 2005]), and can consequently
be used to create a reactive imitative behaviour. The imita- Algorithms
tor’s sensors deﬁne a perception space which it will move
through as the task surroundings change. M-E Items can be The performance of COIL is further affected by certain char-
used to create maps from regions of this space onto the im- acteristics inherited from the CELL algorithms. Firstly, the
itator’s action repertoire. In practise, the imitator searches Recurrence Filter is designed to search for events recurring
the perception chunks stored in LTM for those that match its only within a short history of observations. This is appropri-
current perceptual state. If matches are found, the highest pri- ate for aspects of infant learning that assume frequent rep-
ority match is selected (ranked by mutual information score), etition, e.g. of word/object pairs, but task learning in gen-
and then the associated action chunk is returned and executed. eral may have arbitrarily long gaps between recurring per-
                                                      ception/action pairs. The problem with simply increasing
                                                      the size of the Recurrence Filter window is that every arriv-
1.2  System Shortcomings                              ing AP-event is compared to all those already present, which
                                                      clearly results in combinatorial problems as the size of the
Although COIL has had some previously-reported success in queue increases. The Mutual Information Filter has a simi-
performing imitation tasks, we have discovered a number of lar scaling problem. It has cubic complexity in the number
ﬂaws which prevent it from scaling to more difﬁcult prob- of elements in MTM (which in general could grow without
lems.                                                 bound) and exponential in the number of monitored channels
                                                      (which grows with the complexity of the task domain). Addi-
Representations                                       tionally, the probabilities used in the calculation of the mutual
COIL’s primary weakness results from trying to represent information are frequentist (as opposed to Bayesian) approx-
general action and perception with the same system CELL imations, and are consequently very sensitive to noise caused
uses for speech and vision. CELL receives continuous mi- by small frequency counts (ie. rarely observed events). Roy
crophone data, later converted into discrete phonemes during tackles this by interpolating these probabilities with priors,
Event Segmentation. COIL receives continuous action data but the choice of prior mass parameter required by this tech-
which is parsed into discrete action segments. The crucial nique can have signiﬁcant effects on the resulting probabili-
difference is between the spaces in which these discrete ob- ties, particularly if many of the events in question are infre-
jects lie. We omit the details of Roy’s metric here (see [Roy, quent. This may well be the case for our applications, so we
1999, p. 106]), but intuitively both phonemes and shapes can desire a more robust method.
have inﬁnite variation and can be mapped into a continuous
space using histograms where the notion of ‘nearness’ is rel- 1Limited in that relatively few actions can be initiated at any
atively well-deﬁned. In contrast, the limited set of executable given time.

                                                IJCAI-07
                                                   6422Model                                                probabilities of action class membership for a given per-
                                                      ceptual state. This is achievable using a softmax activation
We therefore wish to implement a learning algorithm which
                                                      function for the network output units [Bridle, 1990] and
can operate within the COIL framework and minimise the po-
                                                      minimising a cross-entropy error function for the network
tential problems outlined above. Speciﬁcally, it should be:
                                                      weights [Bishop, 1995, p. 237]. After some empirical testing,
Scalable - both in terms of memory requirements and learn- we chose to include three hidden units in the network,
ing complexity.                                       although the results were not particularly sensitive to this
Incremental - so that all observations are used and knowl- choice. We are currently looking into Bayesian model
edge is consolidated in a natural way.                selection techniques for selecting the number of hidden units
Rigorous - having output that is interpretable and justiﬁable (see Section 4). Given the above node structure, the network
through established mathematical argument.            we used was fully connected with a simple feedforward
                                                      structure, as shown in Figure 2.
Robust - not prone to failure when processing unusual, un-
forseen or extreme events.
                                                               outputscorrespondtoactionclassprobabilities
  It would also be preferable for this algorithm to be sufﬁ-
                                                                           A1 A2 A3 A4 A5
ciently general-purpose to be applied to other similar learn-
ing problems in this ﬁeld. The Bayesian framework would
seem a good place to begin, as it allows each observed event
to update prior belief in an efﬁcient and well-deﬁned manner
[Bishop, 1995, p. 17]. However, there are many algorithms
which make use of it, so the question becomes which one to
use in order to obtain the desired posterior beliefs. We chose biases
a multi-layer perceptron (MLP) architecture which, given a
certain set of constraints, provides Bayesian posterior proba-
bilities as its outputs. We describe this speciﬁc conﬁguration
in the next section.                                           binaryinputsgroupedbyPerceptionchannel

2.1  Network Architecture                             Figure 2: Diagram of MLP architecture. The binary input
The parts of COIL most prone to the kinds of problems de- vector is a concatenation of 1-of-c encoded symbols for each
scribed above are the Recurrence and Mutual Information ﬁl- Perception channel. There are three hidden units with soft-
ters. To replace these, we therefore require that the new algo- max activation to the outputs, which consequently correspond
rithm receive perception-action data from the Co-occurrence to posterior probabilities of action class membership. Arrow
ﬁlter and output a behavioural map. In MLP terms, this map shows direction of forward propagation; biases are shown ex-
looks like a classiﬁer network, which receives perceptual data plicitly for clarity.
and assigns it to the appropriate action class. To allow an
MLP to learn such a classiﬁcation, we must translate the ob- The network training scheme uses Bayesian regularisa-
served perception-action pairs into an appropriate set of train- tion with separate hyperparameters for each of four weight
ing examples. Each example should consist of a set of input groups: ﬁrst-layer weights, ﬁrst-layer biases, second-layer
variables and a set of target variables. In this case, the input weights and second-layer biases [Bishop, 1995, p. 408].
variables should correspond to the observed perceptual state Training was by back-propagation for up to 100 cycles of
of the expert, and the target variables should correspond to scaled conjugate gradient search (fewer if convergence oc-
the observed action. The question is, what encoding to use curred beforehand), followed by re-estimation of the hy-
for these variable sets?                              perparameters using the evidence approximation technique
  As  explained above,  we  are  assuming for  now    [MacKay, 1992b]. This cycle of re-estimation and re-training
that perceptual categories (such as item left  and    was carried out 8 times. The test data for the network con-
no item  visible) have no implicit relationship to each sisted of querying all possible combinations of perceptual
other. They do not lie in a metric space and so cannot be state, to evaluate the most probable action assigned to that
represented by ordinal variables. We therefore use a purely state by the classiﬁer. Finally, these posterior probabilities
categorical 1-of-c encoding for the input [Bishop, 1995, were marginalised according to the observed data [MacKay,
p. 300]. Suppose a given Perception channel has n possible 1992a]. Although this last step does not affect the most prob-
symbolic states. Then symbol i can be represented by a able action class, it can have signiﬁcant effects if loss matrices
vector of n bits where only the ith bit is set (1 <= i<= n). are added (see Section 3.3 below).
If there are m Perception channels then the concatenation of Empirical evidence showing the increased learning perfor-
m such vectors produces the complete required binary input mance of the new algorithm can be found in the next section.
vector of length n1 + ... + nm.Iftherearek observable Before we examine this however, we review the theoretical
actions, then this is equivalent to a classiﬁcation problem criteria set out at the beginning of this section and ask if they
with k target classes, and we can create one output node for are satisﬁed.
each class. We have already stated that it is desirable for Scalable - as far as learning complexity is concerned, net-
these outputs to have a Bayesian interpretation as posterior work training time increases only linearly with the number

                                                IJCAI-07
                                                   643of observed events, as compared to the combinatorics of the 3.1 Task 1
original algorithms (see Section 1.2). Also, the MLP is a The ﬁrst task involved collecting health vials, one of many
function which requires storage equal to the number of net- so-called ‘pickups’ available in UT, from various locations
work weights as opposed to a potentially boundless number within a game map. The data was originally received and
of stored M-E Items.                                  processed by a COIL system embedded in an AI-controlled
Incremental - due to this increase in efﬁciency and the be- bot, programmed to observe from within the environment a
lief accumulation property of the Bayesian framework, every human-controlled bot carrying out the task. We used three
observation can be taken into account and consolidated with different ‘tactics’ during our demonstrations:
prior knowledge.                                      CW   Tend to turn clockwise if no vials are visible.
Rigorous - the fact that we can interpret the MLP outputs as ACW Tend to turn anticlockwise.
posterior probabilities is a well-proven property of this type
of network and totally independent of the domain in which Mix No ﬁxed tendency.
we’re working.                                        Ten trials for each tactic were carried out, for a total of 30 tri-
Robust - the parametric re-estimation carried out after each als each lasting approximately 60 seconds. During the origi-
network training cycle serves to minimise any problems nal experimental runs, the data arriving in channels (ie. post
caused by local minima relating to, say, weight initialisation. Feature Extraction) were saved prior to further processing.
                                                      This allowed us to compare the new learning algorithms on
                                                      the same data sets. The MLP replaces only the recurrence
3  Experiments                                        and mutual information ﬁltering stages of COIL, with the ﬁrst
                                                      three stages remaining unchanged. For further performance
To evaluate our new model we tested it against the same comparison, we also fed this data into a decision tree algo-
data collected for the original COIL experiments. These data rithm, C4.5 [Quinlan, 1992].
were gathered in Unreal Tournament (UT), a commercially Our performance metric derives from our representation of
released, multi-player ‘First Person Shooter’ (game) [Digi- behaviour as a mapping from discrete regions of perceptual
tal Extremes, 1999]. As the term suggests, the user has an space to discrete actions. We deﬁned the behaviour on which
agent’s-eye view of the game and direct, real-time control we based our demonstrations as the ‘ideal’ map from percep-
of an avatar’s actions. UT also supports remote control of tion to action for this task. The proportion of the learned be-
agents by sending commands to the game server over a net- haviour which matches the ideal allowed us to assign a ‘per-
work. This provides a framework for allowing external pro- centage correct behaviour’ score to each trial. The results
grams to direct an agent’s actions. As such, UT provides a comparing the three techniques are shown in Figure 3(a). As
viable platform for testing strong AI, since humans and AI can be seen from the ﬁgure, the MLP (grey bars) generated
can compete and interact in a real-time, spatially situated do- universally perfect behaviour for this task, correcting all er-
main. The game server sends two categories of sensor data rors made by COIL’s native algorithms (black bars). Interest-
back to the client. The ﬁrst is synchronous: at regular inter- ingly though, C4.5 (white bars) also performs a perfect clas-
vals the client is informed of the agent’s status (e.g. health, siﬁcation
ammo, current weapon, etc). The second is asynchronous:
for example whenever a wall is bumped, a footstep is heard 3.2 Task 2
or damage is taken.                                   The second task required the expert to locate and destroy en-
  These data when viewed as a whole are a highly dynamic, emy bots in an environment which also contained an equal
high-dimensional mixture of categorical and continuous vari- number of friendly bots. Each trial lasted as long as it took
ables, akin to sensory data acquired in the real world. Other for this task to be completed; typically around 60 seconds.
similarities include physics simulation, such as collisions, Tactics CW, ACW and Mix were used analogously to Task 1,
gravity, and sound and light transmission. AI agents’ (bots’) again with ten trials each for a total of 30. All algorithms and
sensor data are incomplete in the sense that only a reduced training methods remained the same for this task as for the
subset of the game variables are observable; the bots have previous one. Results are summarised in Figure 3(b). The
limited virtual sensors. For example, the imitator cannot MLP (left-hand grey bar in each group) provides a small but
know the health state of the expert, although this may well not signifcant (t-test, p =0.05) increase in performance
affect the expert’s choices. The environment contains many from both COIL (black bars) and C4.5 (white bar), which for
independent features, each of which could be represented in this task performs no better than COIL. Upon inspection of
a number ways. Thus the problem of assimilating the be- the data it is clear that for a majority of the trials, the asso-
haviour of another agent via observation is far from simple. ciations that would be necessary to form a fully correct be-
The ﬁrst three stages of COIL each serve to reduce the com- haviour are never observed. Speciﬁcally, most of the mis-
plexity of this problem (see Section 1.1) so that it arrives at classiﬁcations are made for turning toward an enemy; in the
the inputs of our new algorithm in a tractable state. absence of such associations the imitator tended to adopt the
  In short, UT agents deal with real-world temporally- dominant turning direction observed and consequently err in
bounded cognitive constraints, not least the combinatorial either the enemy left or enemy right state. The other
complexity of learning, which makes them ideal test subjects common mistake was to ﬁre before the enemy was centred
for our research.                                     in sights; both were made less by the network than the other

                                                IJCAI-07
                                                   644             (a) Task 1                         (b) Task 2                         (c) ARD

Figure 3: Comparative performance of the different learning algorithms over a variety of tasks. The black bars correspond to
the original COIL algorithms, the white bars correspond to C4.5 and the grey bars correspond to the new MLP classiﬁcations.
In the second study, the right-hand grey bar corresponds to the system moderated by a loss matrix (see text for details). The
third study shows the automatically determined relative importance of two Perception channel input sets. Error bars show the
standard error of the means.

algorithms. Performance is further improved, this time sig- a penalty than any other misclassiﬁcation2. Informally, one
niﬁcantly (t-test, p =0.05), by introducing a loss matrix to would expect this to be equivalent to giving the imitator the
represent prior task knowledge (right-hand grey bars in each instruction “only shoot if you’re sure”, prior to acting. The
group); one of the extentions we go on to talk about in the results of applying this matrix to the Task 2 network outputs
next section.                                         are shown in Figure 3(b) (right-hand grey bars in each group).
                                                      The improvement, as expected, is due to fewer cases of ﬁring
3.3  Bayesian Extentions                              before the enemy is in position. Although this is a relatively
As discussed in Section 2, the probabilistic interpretation of simple example of the application of this technique, it does
results possible from the network model is highly desirable. demonstrate the ease at which prior knowledge can be for-
This also allows other Bayesian techniques to be applied to mally incorporated into the model, and how it could be sys-
the network and its outputs. We now discuss two such tech- tematically altered to test the effect on output behaviour.
niques and show preliminary results.
                                                      Selective Attention
Loss Matrices                                         It is likely that for a given task, only a small subset of the
In general decision theoretic terms, a loss matrix describes full available perceptual state will be required for good per-
the relative penalties associated with misclassifying a given formance. So far in this paper, this subset has been chosen by
input [Bishop, 1995, p.27]. In this case we can describe the hand, but the MLP model can enable us to make this selec-
matrix as having elements Lkj representing the loss resulting tion automatically, within a sound Bayesian framework. This
from assigning an action Aj to a given perceptual state when Automatic Relevance Determination [Neal, 1996, p. 15] is
in fact Ak should be assigned. Decisions are then made by achieved by using a different hyperprior. Instead of grouping
minimising the risk, or equivalently by using the following the weights such that there are four independent regularisa-
discriminant to classify a given perceptual state x:  tion hyperparameters, the weights associated with each input
                                                      have their own hyperparameter. These coefﬁcients vary in
   c               c
       L  P (A |x) <    L  P (A |x)   ∀ i = j        proportion to the inverse posterior variance of the weight dis-
        kj    k          ki    k                (1)   tribution for that input. Thus if a given input has a high coef-
   k=1              k=1                               ﬁcient value, the weight distribution for that input has a low
where P (Ak|x) can be obtained from the (marginalised) net- variance and the input has little bearing on the ultimate classi-
work output probabilities. To demonstrate this we applied a ﬁcation: the input has low relevance. Using a similar training
simple loss matrix to the networks generated during Task 2: and re-estimation scheme as described earlier, these hyperpa-
                                                    rameters can be used to determine the relative relevance of
                         055                          the different network inputs, which in this case correspond to
               (Lkj )=   101                    (2)   different aspects of the environment. Thus we have a method
                         110                          for automatic attention selection within a broader set of chan-

where A1 is the fire action, A2 is turn left and A3 is   2Note a loss matrix with zeros on the main diagonal and ones ev-
turn right. This matrix speciﬁes that ‘accidentally’ ﬁring erywhere else describes a discriminant equivalent to simply choos-
instead of correctly turning should incur ﬁve times greater ing the class with the greatest posterior probability.

                                                IJCAI-07
                                                   645