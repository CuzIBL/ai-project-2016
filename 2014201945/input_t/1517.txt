  Dynamically Constructed Bayes Nets for Multi-Domain Sketch Understanding

                               Christine Alvarado∗   and Randall Davis†
                                              MIT CSAIL
                                         Cambridge, MA 02139
                                    {calvarad,davis}@csail.mit.edu


                    Abstract                          the user will often say the name of the symbol when drawing
                                                      it. The approach in [Kara and Stahovich, 2004] assumes that
    This paper presents a novel form of dynamically   the diagram (a feedback control system) consists of shapes
    constructed Bayes net, developed for multi-domain linked by arrows, which is not true in other domains.
    sketch recognition. Our sketch recognition engine   While these systems have proven useful for their respective
    integrates shape information and domain knowl-    tasks, we aimed to create a more general system, independent
    edge to improve recognition accuracy across a vari- of drawing assumptions in any one domain. Our system is
    ety of domains using an extendible, hierarchical ap- designed, instead, to be applied to a number of symbolic do-
    proach. Our Bayes net framework integrates the in- mains by giving it descriptions of shapes and commonly oc-
    ﬂuence of stroke data and domain-speciﬁc context  curring combinations of shapes. We use these descriptions in
    in recognition, enabling our recognition engine to a three-stage, constraint-based approach to recognition. Our
    handle noisy input. We illustrate this behavior with system ﬁrst relies on a rough processing of the user’s strokes
    qualitative and quantitative results in two domains: to generate a number of interpretation hypotheses. In the sec-
    hand-drawn family trees and circuits.             ond stage, the system uses a novel form of dynamically con-
                                                      structed Bayes net to determine how well each hypothesis ﬁts
1  Introduction                                       the data. In the third stage, it uses this evaluation to guide fur-
                                                      ther hypothesis generation. This paper focuses speciﬁcally on
There is a gap between how people naturally express ideas the second stage, exploring hypothesis evaluation; the other
and the ability of computers to use that information. For ex- two stages are described in [Alvarado, 2004].
ample, while sketching provides a natural way to record de- Using Bayes nets for hypothesis evaluation offers two
sign ideas in many domains, sketches are, unavoidably, static advantages over previous constraint-based recognition ap-
pictures. Computer aided design tools, on the other hand, proaches (e.g., [Futrelle and Nikolakis, 1995; Hammond and
offer powerful capabilities, but require designers to interact Davis, 2004]). First, our system can interpret drawings as
through buttons and menus. The hardware to draw on the they develop, identifying shapes before they are complete.
computer exists; the missing element is the computer’s abil- Second, the system’s belief in a hypothesis can be inﬂuenced
ity to interpret hand-drawn symbols in a domain. To address by both the strokes and the context in which the shape ap-
this problem, we are constructing a general sketch recognition pears, allowing the system to cope with noise in the drawing.
architecture applicable to a number of domains, and capable We constructed a sketch recognition engine and used it in
of parsing freely-drawn strokes in real time and interpreting two domains: family tree diagrams and circuit diagrams. We
them as depicting objects in the domain of interest.  show that the Bayes net successfully allows domain-speciﬁc
  Sketch recognition involves two related subproblems: context to help the system overcome noise in the stroke data,
stroke segmentation and symbol recognition. Segmentation reducing interpretation errors compared to a baseline system.
determines which strokes should be grouped to form a single
symbol. Symbol recognition determines what symbol a given 2 Dynamically Constructed Graphical Models
set of strokes represents.
  The difﬁculty of doing segmentation and recognition si- While time-based graphical models (e.g., Dynamic Bayes
multaneously has led previous approaches to place con- Nets) have been widely used, they are not suitable for sketch
straints on the user’s drawing style, or focus on tasks where understanding, for two reasons. First, we must model shapes
assumptions can greatly reduce segmentation complexity. For based on two-dimensional constraints (e.g., touches) rather
example, the multimodal approach in [Wu et al., 1999] as- than on temporal constraints (i.e., follows). Second, our mod-
sumes that each symbol will be drawn independently, and that els cannot simply unroll in time as data arrives: we cannot
                                                      necessarily predict the order in which the user will draw the
  ∗
   Currently at Harvey Mudd College, Claremont, CA, 91711 strokes, and things drawn previously can be changed.
  †This work was funded by the MIT iCampus project supported While it is not difﬁcult to use Bayes nets to model spatial
by Microsoft and MIT Project Oxygen.                  relationships, static Bayes nets are not suitable for our task               Primitive hypotheses                   language is a shape, which we use to mean a pattern recog-
         s1
    s4         E = ellipse-fit(s )                    nizable in a given domain. Shapes may be compound, i.e.,
  s             1               1
   2           L = line-fit(s )                       composed of subshapes ﬁt together according to constraints.
                1           2                         These subshapes also may be compound, but all shapes must
    s          L2 = line-fit(s3)
     3                                                be non-recursive. A shape that cannot be decomposed into
               L3 = line-fit(s4)
                                                      subshapes (e.g., a line), is called a primitive shape. Primi-
  Arrow hypothesis A1
  Subshapes:                                          tive shapes may have named subcomponents that can be used
                                                      when describing other shapes, e.g., the endpoints of a line,
  line hypotheses L1,L2, L3
  Constraints:                                        “p1” and “p2”, used in Figure 1.
  C2 = (coincident line-fit(s2).p1 line-fit(s3).p1)     We refer to each shape description as a template with one
  C3 = (coincident line-fit(s2).p1 line-fit(s4).p1)   slot for each subpart. A shape hypothesis is a template with
  C4 = (equalLength line-fit(s3) line-fit(s4))        an associated mapping between slots and strokes, generated
  C5 = (shorter line-fit(s3) line-fit(s2))            during the recognition process. Similarly, a constraint hy-
  C6 = (acuteAngle line-fit(s2), line-fit(s3))        pothesis is a proposed constraint on one or more of the user’s
  C7 = (acuteAngle line-fit(s2), line-fit(s4))        strokes. A partial hypothesis is a hypothesis in which one or
                                                      more slots are not bound to strokes.
  CS hypothesis CS
               1                                        To introduce our Bayes net model, we begin by considering
  Subshapes:
                                                      how to model a current source hypothesis (CS1) for the stokes
   arrow hypothesis A1, ellipse hypothesis E1
  Constraints:                                        in Figure 1. A current source is a compound shape, so CS1 in-
                                                      volves two lower-level shape hypotheses—E1, an ellipse hy-
  C1 = (contains ellipse-fit(s1) shape-fit(s2,s3,s4))
                                                      pothesis for stroke s1; and A1, an arrow hypothesis involving
                                                      strokes s2, s3 and s4—and one constraint hypothesis—C1, in-
Figure 1: A single current source hypothesis (CS1) and asso- dicating that an ellipse ﬁt for stroke s1 contains strokes s2,
ciated lower-level hypotheses.                        s3, and s4. A1 is also compound and is further broken down
because we cannot predict a priori the number of strokes or into three line hypotheses (L1, L2 and L3) and six constraint
symbols the user will draw. For sketch recognition, as in re- hypotheses (C2,...,C7) according to the description of an ar-
lated dynamic tasks, models to reason about speciﬁc problem row. Thus, determining the strength of hypothesis CS1 can be
instances (e.g., a particular sketch) must be dynamically con- transformed into the problem of determining the strength of a
structed in response the input. This problem is known as the number of lower-level shape and constraint hypotheses.
task of knowledge-based model construction (KBMC).      The Bayes net used to evaluate CS1 is shown in Figure 2.
  Early approaches to KBMC focused on generating Bayes There is one node in the network for each hypothesis; each
nets from probabilistic knowledge bases [Glessner and node represents a boolean random variable that is true if the
Koller, 1995; Haddawy, 1994]. A recently proposed repre- corresponding hypothesis is correct. The probability of each
sentation uses generic template knowledge directly as Bayes hypothesis is inﬂuenced both through its children by stroke
net fragments that can be instantiated and linked together at data and through its parents by the context in which it appears,
run-time [Laskey and Mahoney, 1997]. Finally, Koller et allowing the system to handle noise in the drawing.
al. have developed a number of object-oriented frameworks The nodes labeled O1,...,O11 represent measurements of
[Koller and Pfeffer, 1997; Pfeffer et al., 1999; Getoor et al., the stroke data that correspond to the constraint or shape to
1999]. These models represent knowledge in terms of rela- which they are linked. The variables corresponding to these
tionships among objects and can be instantiated dynamically nodes have positive real numbered values that we discretize in
                                                                      1
in response to the number of objects in a particular situation. our implementation . For example, the variable O2 is a mea-
  Although these frameworks are powerful, they are not di- surement of the squared error between the stroke s1 and the
rectly suitable for sketch recognition. First, because of the best ﬁt ellipse to that stroke. Its raw value (later discretized)
size of the networks we encounter, it is sometimes desirable ranges from 0 to the maximum possible error between any
to generate only part of a complete network, or to prune nodes stroke and an ellipse ﬁt to that stroke. The boxes labeled
                                                        ,...,
from the network. In reasoning about nodes in the network, s1 s4 are not part of the Bayes net but serve to indicate
we must account for the fact that the network may not be the stroke or strokes from which each measurement, Oi,is
                                                                                       (    = |  )
fully generated or relevant information may have been pruned taken (e.g., O2 is measured from s1). P CS1 t ev (or sim-
                                                                   2
from it (see [Alvarado, 2004] for details). Second, these pre- ply P(CS1|ev)) , where ev is the evidence observed from the
vious models have been optimized for responding to speciﬁc user’s strokes, represents the probability that the hypothesis
queries about a single node in the network. In contrast, our CS1 is correct.
model must provide probabilities for a full set of possible in- The direction of the links may seem counterintuitive, but
terpretations of the user’s strokes.                  there are two important reasons why the links are directed
                                                      from higher-level shapes to lower-level shapes instead of the
3  Network Structure                                  opposite direction. First, whether a higher-level hypothe-
                                                      sis is true directly inﬂuences whether a lower-level hypoth-
Our Bayes net model is built around hierarchical descriptions
of shapes in a domain, described in a language called LAD- 1The BN software we used did not support continuous variables.
DER  [Hammond and Davis, 2003]. The basic unit in this   2Throughout this paper, t means true, and f means false.                                                                                 2
                                                                        1
                  CS1

                                                                              10
                                                                        8
                            A                                               9 11
   C1        E1              1
                                                                         12
                                                                                 5        7
                                                                             4       6
                   L1 L2 L3  C2 C3 C4  C5 C6 C7

                                                                         3
   O1        O2    O3 O4 O5  O6 O7 O8  O9 O10 O11
                                                      Figure 3: A partial sketch of a family tree. Quadrilaterals (Q)
                                                      represent males (M); ellipses (E) represent females (F), and
                                                      arrows (A) indicate a parent-child relationship.
            s1       s2       s3       s4

Figure 2: A Bayes net to verify a single current source hy- net model can be applied to recognize a shape in any size,
pothesis. Labels come from Figure 1.                  position and orientation. CS1 represents the hypothesis that
                                                      s1,...,s4 form a current source symbol, but the exact posi-
                                                      tion, orientation and size of that symbol is determined directly
esis is true. For example, if the arrow hypothesis A1 is from the stroke data 3.
true, then it is extremely likely that all three line hypotheses, Second, the system can model competing higher-level in-
  ,  ,
L1 L2 L3, are also true. Second, this representation allows us terpretations for lower-level shapes. For example, the system
to model lower-level hypothesis as conditionally independent may consider a stroke to be a line that is in turn part of ei-
given their parents, which reduces the complexity of the data ther an arrow or a quadrilateral. Because the line hypothesis
needed to construct the network.                      does not include any higher-level shape-speciﬁc constraint in-
  Each shape description constrains its subshapes only rela- formation, both an arrow hypothesis node and a quadrilateral
tive to one another. For example, an arrow may be made from hypothesis node can point to this same line hypothesis node.
any three lines that satisfy the necessary constraints. Based on These two hypotheses then become alternate, competing ex-
this observation, our representation models a symbol’s sub- planations for the line hypothesis. We further discuss how
shapes separately from the constraints between them. For ex- hypotheses are combined below.
ample, node L2 represents the hypothesis that stroke s3 is a
line. Its value will be true if the user intended for s3 to be a 4 Recognizing a Complete Sketch
line, any line regardless of its position, size or orientation. C4
                                                      To recognize a complete sketch, we create a Bayes net similar
separately represents the hypothesis that the line ﬁttos3 and
                                                      to the one above for each shape. We call each of these Bayes
the line ﬁttos4 are the same length.
  The conditional independence between shapes and con- nets a shape fragment because they can be combined to create
straints might seem a bit strange at ﬁrst. For example, a complete Bayes net for evaluating the whole sketch.
whether or not two lines are the same length seems to de- Given a set of hypotheses for the user’s strokes (hypothe-
                                                                              [                      ]
pend on the fact that they are lines. However, observation sis generation is described in Alvarado and Davis, 2004 and
                                                      [             ]
nodes for constraints are calculated in such a way that their Alvarado, 2004 ), the system instantiates the corresponding
value is not dependent on the true interpretation for a stroke. shape fragments and links them together to form a complete
For example, when calculating whether or not two lines are Bayes net, called the interpretation network. To illustrate this
the same length, we ﬁrst ﬁt lines to the strokes (regardless process, consider a piece of a network generated in response
of whether or not they actually look like lines), then measure to Strokes 6 and 7 in the example of Figure 3. Figure 4 shows
their length. How well these lines ﬁt the original strokes is the part of the Bayes net representing the hypotheses that the
not considered in this calculation.                   system generated for these strokes. Low level processing rec-
                                                      ognizes Strokes 6 and 7 as L-shaped polylines and breaks
  The fact that there is no edge between the constraint nodes each into two individual lines (L ...L ) that meet.
and the shapes they constrain has an important implication for                   1   4
using this model to perform recognition: There is no guaran- 4.1 Linking Shape Fragments
tee in this Bayes net that the constraints will be measured
                                                      When Bayes net fragments are linked during recognition,
between the correct subshapes because the model allows sub-                               ...
shapes and constraints to be detected independently. For ex- each node Hn may have several parents, S1 Sm, where each
                                                      parent represents a possible higher-level interpretation for Hn.
ample, we want C4 in Figure 2 to indicate that L2 and L3
(the two lines in the head of an arrow) are the same length, We use a noisy-OR function to combine the inﬂuences of all
                                                      the parents of Hn to produce the complete conditional proba-
not simply that any two lines are the same length. To satisfy             (  | ,...,  )
this requirement, the system must ensure that O is measured bility table (CPT) for P Hn S1 Sm . The noisy-OR func-
                                       8              tion models the assumption that each parent can indepen-
from the same strokes that O4 and O5 were measured from.
We use a separate mechanism to ensure that only legal bind- dently cause the child to be observed. For example, a single
ings are created between strokes and observation nodes.  3Orientation-dependent symbols (e.g., a downward-facing ar-
  The way we model shape and constraint information has row) can be recognized by including orientation-dependent con-
two important advantages for recognition. First, this Bayes straints in their descriptions (e.g., vertical, below).stroke might be part of a quadrilateral or an arrow, but both               M1
interpretations would favor that interpretation of the stroke
                 (   =  |  =  )=
as a line. We set P Hn f S j t    0 for all parents S j                     Q        A2
                                                                    A1       1
in which Hn is a required subshape or constraint, and we set
P(Hn = f |Sk = t)=0.5 for all parents Sk in which Hn is an
                                                             Constraints          Constraints Constraints
                                                                    L  L  L  L L         L
optional subshape or constraint. A consequence of these val-   for A1 5 1 2  3  4  for Q1 6  for A2
ues is that S j = t ⇒ P(Hn|S1,...,Sm)=1 for any S j in which
                                                                       O O  O O  Observations Observations
Hn is required, which is exactly what we intended.            Observations 1 2 3 4
  We experimented with a noisy-XOR  construct, imple-
mented using a “gate node” similar to that described in                Stroke6 Stroke7
[Boutilier et al., 1996], but found that noisy-OR semantics
were simpler and in fact produced better results. In effect, Figure 4: A portion of the interpretation network generated
a noisy-OR node in a Bayes net with low prior probabilities while recognizing the sketch in Figure 3.
behaves as a non-aggressive XOR. The fact that one parent
is true does not actively prohibit the other parents from being correct low-level hypotheses have not yet been proposed (be-
true, but it causes their probabilities to tend back to their prior cause of low-level recognition errors). A partial hypothesis
values because of the “explaining away” phenomenon.   with a high probability cues the system to examine the sketch
                                                      for possible missed low-level interpretations during the hy-
4.2  Signal Level Noise                               pothesis generation step.
The bottom layer of the network deals with signal level noise
by modeling the differences between the user’s intentions and 5 Implementation and Bayesian Inference
the strokes that she draws. For example, even if the user Our system updates the structure of the Bayes net in response
intends to draw L1, her stroke likely will not match L1 ex- to each stroke the user draws, using an off-the-shelf, open
actly, so the model must account for this variation. Consider source Bayes net package for Java called BNJ [bnj, 2004].
P(O1|L1 = t) (recall that O1 is a discretized continuous val- Generating and modifying the BNJ networks can be time
ued variable). If the user always drew perfect lines, this dis- consuming due to the exponential size of the CPTs between
tribution would be 1 when O1 = 0 (i.e., the error is 0), and 0 the nodes. We use two techniques to improve performance.
otherwise. However, most people do not draw perfect lines First, networks are generated only when the system needs to
(due to inaccurate pen and muscle movements), and this dis- evaluate the likelihood of a hypothesis. This on-demand con-
tribution allows for this error. It should be high when O1 is struction is more efﬁcient than continuously updating the net-
close to zero, and fall off as O1 gets larger. The wider the work, because batch construction of the CPTs is often more
distribution, the more error the system will tolerate, but the efﬁcient than incremental construction. Second, the system
less information a perfect line will provide.         modiﬁes only the portion of the network that has changed be-
  The other distribution needed is P(O1|L1 = f ) which is the tween strokes, rather than creating it from scratch every time.
probability distribution over line error given that the user did We experimented with several inference methods and
not intend to draw an line. This distribution should be close to found that loopy belief propagation (loopy BP) [Weiss, 1997]
uniform, with a dip around 0, indicating that if the user specif- was the most successful4. On our data, loopy BP almost al-
ically does not intend to draw an line, she might draw any ways converged (messages initialized to 1, run until node val-
other shape, but probably won’t draw anything that resem- ues stable within 0.001). We further limited the algorithm’s
bles an line. Details about how we determined the conditional running time in two ways. First, we terminated the algorithm
probability distributions between primitive shapes and con- after 60 seconds if it had not yet converged. Second, we al-
straints and their corresponding observation nodes are given lowed each node to have no more than 8 parents (i.e., only 8
elsewhere [Alvarado, 2004].                           higher-level hypotheses could be considered for a single hy-
                                                      pothesis), ensuring a limit on the complexity of the graphs
4.3  Missing Strokes                                  produced. These restrictions had little impact on recognition
                                                      performance in the family tree domain, but for complex do-
A is a partial hypothesis—it represents the hypothesis that L
 1                                                1   mains such as circuit diagrams, more efﬁcient inference algo-
and L (from Stroke 6) are part of an arrow whose other line
    2                                                 rithms or graph simpliﬁcation techniques are needed to im-
has not yet been drawn. Line nodes representing lines that
                                                      prove recognition results.
have not been drawn (e.g., L5) are not linked to observation
nodes because there is no stoke from which to measure these
observations. We refer to these nodes (and their correspond- 6 Application and Results
ing hypotheses) as virtual.                           We applied our implemented system  to two non-trivial
  The fact that partial hypotheses have probabilities allows domains—family trees and circuits—and found that it is ca-
the system to assess the likelihood of incomplete interpreta- pable of recognizing sketches in both domains without re-
tions based on the evidence it has seen so far. In fact, even programming. Qualitative and quantitative evaluation of our
virtual nodes have probabilities, corresponding to the proba-
bility that the user (eventually) intends to draw these shapes 4Junction Tree [Jensen et al., 1990] was often too slow and
but either has not yet drawn this part of the diagram or the Gibb’s Sampling did not produce meaningful results.                                                                                               Wire-3
                                                  1          Wire-1 Battery-1 Wire-2 Ground-1 Battery-2
               1                1
                                                2                                               Line-3
             2                2                          Line-1
                                                                              Line-2
           3                 3                3             Aligned-1                        Parallel-2
                                                                         Parallel-1 Aligned-2
                                                                                                  SqErr-3
                                                        SqErr-1 Smaller-1 NextTo-1
        (a)                                                                           Smaller-2 NextTo-2
                         (b)               (c)              Meas-1        Meas-4  Meas-5         Meas-8
                                                               Meas-2 Meas-3  SqErr-2 Meas-6 Meas-7
     Figure 5: Part of three possible ground symbols.
                                                      Figure 6: The Bayes net produced in response to the strokes
                                                      in each ground symbol in Figure 5. The shaded area shows
system illustrates its strengths over previous approaches and the network produced in response to the ﬁrst two strokes.
suggests some extensions.
  Applying our system to a particular domain involves two  Name              P[Shape|stroke data]
steps: specifying the structural descriptions for the shapes          after 2 strokes  after 3 strokes
in the domain, and specifying the prior probabilities for top-       (a)   (b)  (c)   (a)   (b)   (c)
level shapes and domain patterns, i.e., those that will not have Wire-1 0.4 0.41 0.41 0.4   0.4  0.42
parents in the Bayes net. For each domain, we wrote a de-  Wire-2    0.4  0.38  0.38  0.4   0.4  0.38
scription for each shape and pattern in that domain. We hand- Wire-3 N/A  N/A   N/A   0.4   0.4   0.1
estimated priors for each domain pattern and top level shape Battery-1 0.51 0.51 0.51 0.09  0.1  0.91
based on our intuition about the relative prevalence of each Battery-2 N/A N/A  N/A   0.09  0.1   0.0
shape. For example, in family-tree diagrams, we estimated  Ground-1 0.51  0.51  0.51  0.95 0.94  0.03
that marriages were much more likely than partnerships, and
                                                      Table 1: Posterior probabilities for part of the network in Fig-
set P[Mar]=0.1 and P[Part ]=0.001. These priors represent
                                                      ure 6 for each sketch in Figure 5.
the probability that a group of strokes chosen at random from
the page will represent the given shape and, in most cases,
should be relatively low. Although setting priors by hand can only wires, resistors, ground symbols and batteries. Figure 6
be tedious, we found through experimentation that the sys- shows the Bayes nets produced in response to the user’s ﬁrst
tem’s recognition performance was relatively insensitive to two and ﬁrst three strokes. After the ﬁrst two strokes, the net-
the exact values of these priors. For example, in the circuit work contains only the nodes in the shaded area; after three
diagrams, increasing all the priors by an order of magnitude strokes it contains all nodes shown. Continuous variables cor-
did not affect recognition performance; what matters instead responding to observation nodes were discretized into three
is the relative values of the prior probabilities.    values: 0 (low error: stroke data strongly supports the shape
  Our system is capable of recognizing simple sketches or constraint), 1 (medium error: stroke data weakly supports
nearly perfectly in both the family-tree and circuit domains. the shape or constraint), and 2 (high error: stroke data does
We also tested its performance on more complex, real-world not support the shape or constraint).
data. As there is no standard test corpus for sketch recog- Posterior probabilities are given in Table 1. For the rel-
nition, we collected our own sketches and have made them atively clean ground symbol (Figure 5(a)), the stroke data
available online to encourage others to compare their results strongly supports all the shapes and constraints, so all shaded
                     [                 ]
with those presented here Oltmans et al., 2004 . In total, we nodes in Figure 6 have value 0. After the ﬁrst two strokes,
tested our system on 10 family tree sketches and 80 circuit the battery symbol and the ground symbol have equal weight,
diagram sketches, with between 23 and 110 strokes each. which may seem counterintuitive: after two strokes the user
                                                      has drawn what appears to be a complete battery, but has
6.1  Qualitative Results                              drawn only a piece of the ground symbol. Recall, however,
Qualitative analysis reveals that the Bayes net mechanism that the Bayes net models not what has been drawn, but what
successfully aggregates information from stroke data and the user intends to draw. After two strokes, it is equally likely
context, resolves inherent ambiguities in the drawing, and that the user is in the process of drawing a ground symbol. Af-
updates its weighting of the various existing hypotheses as ter the third stroke, the ground symbol’s probability increases
new strokes are drawn. We show through an example that because there is more evidence to support it, while the bat-
the Bayes net scores hypotheses correctly in several respects: tery’s probability decreases. The fact that the ground symbol
it prefers to group subshapes into the fewest number of in- is preferred over the battery illustrates that the system prefers
terpretations, it allows competing interpretations to inﬂuence interpretations that result in fewer symbols (Okham’s razor).
one another, it updates interpretation strengths in response to Interpretations that involve more of the user’s strokes have
new stroke data, and it allows both stroke data and context to more support and get a higher score in the Bayes net. The fact
inﬂuence interpretation strength.                     that the battery symbol gets weaker as the ground symbol gets
  To illustrate the points above, we consider in detail how the stronger results from the “explaining away” behavior in this
system responds as the user draws the three sets of strokes Bayes net conﬁguration: each of the low-level components
in Figure 5 (a ground symbol). To simplify the example, (the lines and the constraints) are effectively “explained” by
we consider a reduced circuit domain in which users draw the existence of the ground symbol, so the battery is no longer