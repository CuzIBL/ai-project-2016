                An Adaptive Context-based Algorithm for Term Weighting
                            Application to Single-Word Question Answering
       Marco Ernandes, Giovanni Angelini, Marco Gori, Leonardo Rigutini, Franco Scarselli
                             Dipartimento di Ingegneria dell’Informazione
                                    Universit`a di Siena, Siena - Italy
                         {ernandes,angelini,marco,rigutini,franco}@dii.unisi.it

                    Abstract                          problems (e.g. [Debole and Sebastiani, 2003] ), to the best of
    Term weighting systems are of crucial importance  our knowledge there has been no attempt to compute TFIDF-
    in Information Extraction and Information Re-     like term weights that make use of relationships among terms.
    trieval applications. Common approaches to term   Latent Semantic Indexing (LSI) [Deerwester et al., 1990]
    weighting are based either on statistical or on nat- is a statistical approach that does not suffer from the word-
    ural language analysis. In this paper, we present a independence assumption. LSI tries to measure the principal
    new algorithm that capitalizes from the advantages associative patterns between sets of words and concepts using
    of both the strategies by adopting a machine learn- the Singular Value Decomposition technique, without taking
    ing approach. In the proposed method, the weights into account word order. Unfortunately LSI cannot be em-
    are computed by a parametric function, called Con- ployed in many practical problems. This is the case of QA,
    text Function, that models the semantic inﬂuence  because LSI projects document words into a non-linguistic
    exercised amongst the terms of the same context.  space, whereas QA requires answers in natural language.
    The Context Function is learned from examples,      On the other side, techniques that are more deeply inspired
    allowing the use of statistical and linguistic infor- by natural language theories and applications [Voorhees,
    mation at the same time. The novel algorithm was  1999], as morphological and syntax analysis, naturally ex-
    successfully tested on crossword clues, which rep- ploit the information provided by word contexts. But their
    resent a case of Single-Word Question Answering.  design is very difﬁcult and require a large human effort.
                                                        In this paper, we present a novel algorithm for term weight-
1  Introduction                                       ing that aims to combine the advantages of both statistical
Term weighting is an important task in many areas of Infor- and linguistic strategies by adopting a machine learning ap-
mation Retrieval (IR), including Question Answering (QA), proach. Our method exploits the relationships (i.e. order,
Information Extraction (IE) and Text Categorization. The distance, etc.) among the words of a document in order to
goal of term weighting is to assign to each term w found in a evaluate their semantic relevance to a given question. The
collection of text documents a speciﬁc score s(w) that mea- intuition behind the proposed model is that the relevance of
sures the importance, with respect to a certain goal, of the a term can be computed recursively as the combination of
information represented by the word. For instance, Passage its intrinsic relevance and the relevance of the terms that ap-
Retrieval systems weigh the words of a document in order to pear within the same context. The inﬂuence exercised by a
discover important portions of text and to discard irrelevant word on another one is computed using a parametric func-
ones. This is the case of applications as QA, Snippet Extrac- tion, called Context Function. This function is trained by
tion, Keyword Extraction, Automatic Summarization.    examples, thus it is well suited for using together statistical
  Common approaches to term weighting can be roughly di- (e.g. term frequency) and linguistic information (morpholog-
vided into two groups: statistical and linguistic techniques. ical, syntactical, lexical), requiring no pre-designed rules.
The methods in the former group are based on a statistical The main idea underlining the proposed approach is tightly
analysis that extracts, from a document collection, features related to the TextRank algorithm [Mihalcea and Tarau, 2004]
as word frequencies or information-theoretical measures. that has been succesfully applied to keyword extraction and
TFIDF [Salton and McGill, 1986] is a popular statistically- text summarization. More precisely, the context-based algo-
inspired method currently used in many IR systems to mea- rithm extends TextRank, by allowing a more general mod-
sure the importance of words. A major assumption in this elling of the word relationships and by providing a learning
approach is that the words of a document can be consid- algorithm to deﬁne the strength of those relationships.
ered as unordered and independent elements. Such assump- The Context-based algorithm has been evaluated on a spe-
tion is shared by many other measures as Information Gain, ciﬁc problem, i.e. Single-Word Question Answering, where
Gain Ratio, Best-Match (e.g. BM25) or the Chi-Square func- the goal is to ﬁnd the single correct word that answers a given
tion [Manning and Schtze, 1999]. Even if the literature question. Single-Word QA is particularly suited to evaluate a
presents several interesting adaptations of TFIDF for speciﬁc term weighting algorithm, since it directly exploits the rank

                                                IJCAI-07
                                                  2748produced by the weights. The proposed method has been ap- the Context Function from a set of examples and how to eval-
plied to improve the performances of a particular answering uate the performances of the system. These issues will be
system (WebCrow [Ernandes et al., 2005]), designed to ad- considered in the following sub–sections.
dress crossword clues. The results show that the approach is 2.1 Context Functions
viable for the Single-Word QA problem and suggest that it
could be extended to other ﬁelds of Text Processing.  A document set contains multiple instances of words and each
  Section 2 describes the Context-based term weighting al- instance (a word-occurrence) is affected by a different con-
gorithm. Section 3 presents the experimental environment text. Therefore, we distinguish between words (that we shall
                                                                   w
used to evaluate the proposed method. In Section 4 we report represent with ) and word occurrences (represented with a
                                                          wˆ                    c
the experimental results obtained by the system in the Single- hat, ). Here we assume that w,u can be computed as the
                                                                                               w     u
Word QA problem. Section 5 provides our conclusions.  sum of the contributions of all the occurrences of and
                                                                cw,u =                 Cp(ˆw, uˆ).    (2)
2  Context-based term weighting                                        wˆ∈occ(w) uˆ∈ict(ˆw,u)
The proposed method exploits the contexts of words. A word
context primarily consists of the text surrounding a given Here, occ(w) is the set of instances of word w,andict(ˆw, u)
word, but could also include other features, as document ti- is the set of the occurrences ofu that belong to the context
tles, hyper-linked documents, and so on. The idea is that, in of wˆ,i.e. ict(ˆw, u)=occ(u) ctxt(ˆw),wherectxt(ˆw) is
order to measure the relevance of a word with respect to a the context of wˆ. Moreover, given a set of parameters p,
certain goal (e.g. a query, a document category), the features Cp(ˆw, uˆ) is the parametric Context Function that establishes
of the context in which the term appears are important as well the strength of the inﬂuence between the instances wˆ (the
as the features of the word itself. Roughly speaking, a word word under evaluation) and uˆ (the context word). In our work
is important not just when this is statistically strong, but also this function is learned by examples1.
when the words that belong to its context are important too. The context of a word occurrence wˆ can be deﬁned as the
  This recursive deﬁnition recalls that of social net- set of words that are contained in the same document and
works [Seeley, 1949], where the status of a person depends within the surround of wˆ with dimensions kL + kR (left mar-
                                                                                “ ...uˆ   uˆ  ... uˆ wˆ uˆ
on the status of the persons that are somehow related to it. gin and right margin). Thus, if −kl−1 −kl −1 1
                                                      ... uˆ uˆ  ...
We assume that a text document can be represented by a so- kr kr+1   is a text passage of any document, then
                                                      ctxt(ˆw)={uˆ  , ..., uˆ , uˆ , ..., uˆ }
cial network, where the importance of the words can be com-      −kl     −1  1    +kr holds. In this formula-
puted on the basis of its neighbors. More precisely, the weight tion we introduce the strong assumption that direct semantic
s(w) of the word w is computed as                     relations between words can be registered only within their
                                                     surrounds. This assumption is plausible and widely adopted
        s(w)=(1−   λ)dw + λ       s(u)cw,u ,    (1)   (e.g. snippets in search engines). On the other hand, the ap-
                            u∈r(w)                    proach is easily extended to the cases when the context in-
where dw is the default score of w, r(w) is the set of words cludes other features as the document title, the section title,
that belong to the context of w, cw,u is a real number mea- the text of the anchors pointing to the document, and so on.
suring the inﬂuence exercised by u over w,andλ ∈ [0, 1] is The most delicate point of the system is the deﬁnition of
                                                                 C (ˆw, uˆ)
a damping factor. Thus, the score of a word will result from the function p . This establishes how word couples
                                                      can inﬂuence one another on the basis of features extracted
the combination of the default score dw and its context score
                                                      from the words and from the relationships between words.
  u∈r(w) s(u)cw,u.
                                                      Theoretically, the inﬂuence function can exploit any sort of
  The real numbers dw and cw,u can be calculated using in-
                                                      information related to wˆ and uˆ: information-theoretical (e.g.
formation about the occurrences and the context of the words
                                                      in text categorization: information gain, gain ratio), morpho-
(the frequency and the linguistic role of a word, the distance
                                                      logical (i.e. part-of-speech classes of w and u), syntactical
between two words, and so on). The functions used to com-
                                                      (i.e. syntax role of w and u) or lexical (e.g. WordNet dis-
pute these values can be either predeﬁned or learned by exam-
                                                      tance between w and u). The features that have been used
ples in a machine learning fashion. In our work, we assume
                                                      in our preliminary experiments are exclusively statistical (see
that dw is given by an initial scoring system (e.g. TFIDF).
                                                      Tab. 1). Interestingly, even with this reduced list of basic fea-
We then learn a parametric function for calculating the cw,u
                                                      tures, the system displays notable performances.
whose goal is to improve the initial values.
                                                        Moreover, how do we merge all this information into one
  Since the dw can be provided by external informative
                                                      single output? The most general approach consists of imple-
sources, the algorithm is well suited to exploit the weights  C (ˆw, uˆ)
produced by other algorithms in order to improve their per- menting p by a modeling tool that has the universal
                                                      approximation property, i.e. it allows to realize any function
formances. It is worth to notice that both dw and cw,u can be
computed exploiting statistical and linguistic features. From with any desired degree of precision. These tools include neu-
this point of view our method represents a mechanism to in- ral networks, polynomials, and rationales. While we plan to
tegrate both kinds of information.                       1The novelty of our approach with respect to TextRank [Mihal-
  In order to implement the proposed approach, the follow- cea and Tarau, 2004] consists of the introduction of the context func-
ing problems have to be addressed: how to deﬁne a Context tion and the learning algorithm. In fact, TextRank uses an equation
Function that computes the cw,u values, how to compute the similar to (1) to deﬁne the word scores, but the inﬂuence factors cw,u
term weights s(w) using Eq.(1), how to automatically learn are predeﬁned values computed using word co-occurrences.

                                                IJCAI-07
                                                  2749                                                                             δep
use neural networks in the future, for the introductory scope 1. compute the gradient δp of the error function with re-
of this paper we preferred to exploit a simpler approach both spect to the parameters;
for the implementation of Cp(ˆw, uˆ) and for the learning algo-
                                                        2. adapt the parameters p by resilient method [Riedmiller
rithm. Such a simpliﬁcation allows to evaluate the proposed
                                                          and Braun, 1993].
method on the term weighting problem, avoiding that the re-
sults are altered by the complexity of the system.    The resilient parameter adaptation is a technique that allows
  We deﬁned the inﬂuence function as                  to update the parameters using the signs of the partial deriva-
                       i=n                           tives of the error function. It has been experimentally proved
                                                      that resilient is often more efﬁcient than common gradient
            Cp(ˆw, uˆ)=   σ(αixi + βi) ,        (3)
                       i=0                            descent strategies [Riedmiller and Braun, 1993].
where xi  is the value associated with the i-th fea-    The gradient is approximated by a simple brute force al-
                                                                                           δep
ture, the αi,βi  are the  model parameters, p    =
                                                      gorithm. Each component of the gradient δpi is given by
[α1,...,αn,β1,...,βn],andσ is the logistic sigmoid func-
                   −x                                 the corresponding incremental ratio, which requires to calcu-
tion σ(x)=1/(1 +  e  ). Notice that the logistic sigmoid late the error function for n +1times at each training epoch.
is a monotone increasing function that approaches 0,when It is worth to mention that the proposed learning algorithm
x →−∞, and approaches   1,whenx   →∞. Thus, each      is a simpliﬁed version of the one described in [Gori et al.,
term σ(αixi + βi) of Eq. (3) is a sort of soft switch related 2005]. The latter procedure can be used efﬁciently even for
to the i-th feature and controlled by the parameters αi and βi. models having a large number of parameters. Such a fact is
When αi >  0, the switch is “ON” (i.e. close to 1), if xi is important, since it guarantees that our implementation of the
                                         0    x
large and positive and it is “OFF” (i.e. close to ), if i is context function Cp(ˆw, uˆ) can be efﬁciently replaced by more
large and negative. More precisely, the parameter αi controls complex and general models as, e.g. neural networks.
the steepness and the direction of the soft switch, while βi In our implementation we decided to learn, along with the
controls the point where the switch assumes a medium value set of parameters p, the damping factor λ (see Eq. (4)), which,
1/2. Finally, the whole function Cp(ˆw, uˆ) can be described intuitively, establishes the level of conﬁdence attributed by
as a simple boolean AND operation which is “ON” only if all the system to the information extracted from the context.
the switches are “ON”.                                With this incorporation, a couple of improvements are ex-
2.2  Computing and Learning Context Weights           pected. Firstly, the experiments that would be required to
                                                      establish the best λ are avoided. Secondly, the new weights
For a correct deﬁnition of the weights, System (1) must have
                                                      S are guaranteed not to be outperformed by the default scor-
a unique solution. Stacking all the variables, Eq. (1) can be
                                                      ing value D. In fact, if the Context Function turns out to be
written as
               S =(1−   λ)D + λCS  ,            (4)   deleterious for term ranking, λ can be set to 0 and hence the
                                                      weighting system replicates the ranking performances of D.
where {w1, ..., wn} is the set the words of the dictionary,
                                                     During the experiments this case has not been observed.
S =[s(w1), ..., s(wn)] is the vector of the weights, D =
[d  , ..., d ] C  = {c   }
 w1     wn  ,and       w,u  is a square matrix contain- 2.3 Evaluating term weighting performances
                         c
ing the word inﬂuence factors w,u.                    The criterion to evaluate a term weighting system depends
                                 λC  < 1
  It can be easily proved that, provided   holds for  on the speciﬁc task that is addressed by the application. In
any matrix norm ·, Eq. (4) has a unique solution which can
                                                 t    QA-like problems, as the one proposed in this paper, the
be computed iterating the system2 St+1 =(1−λ)D+λCS ,
                  t               0                   performance of the system depends on the position that the
i.e. S = limt→∞  S  for any initial S . In practice, the correct answer assumes in the candidate answer list. Two
computation can be stopped when the difference between the positional and cumulative measures can be used for QA
scores at iteration t +1and iteration t are below a speciﬁed
               t+1    t                               evaluation: the Mean Reciprocal Rank of correct answers
small value : ||S − S || <. The method is called Ja- (MRR), which is a standard evaluation measure in the QA
cobi algorithm [Golub and Loan, 1996] for the solution of community, and the Success Rate (SR) at N, in which we
linear systems. It is worth to mention that this technique is annotate the probability of ﬁnding a correct answer within
extremely efﬁcient and can be applied even on sparse matri- the ﬁrst N candidates. Formally, given a set of questions
ces containing billions of variables [Page et al., 1998]. Q = {quest(1), ..., quest(q), ..., quest(n)} and denoted by
  The learning procedure can be implemented by any op- pos(aq) the position of the correct answer aq for question
timization algorithm that minimizes an error function ep, quest(q)
                                                              ,wehave         q=n
where p =[p1,...,pn] denotes the set of parameters of the                    1      1
                                                                   (Q)=
Context Function Cp(ˆw, uˆ). The function ep measures the      MRR          n     pos(aq)             (5)
performance of the weighting system on a given dataset and                    q=1
it will be discussed in details in the following section. In our             1 q=n
                                                                (Q, N)=           Θ(N  − pos(aq)) ,
method the training algorithm repeats the following two steps SR            n                         (6)
for a predeﬁned number of times:                                              q=1
                   
  2      St =(1−λ)   t−1 λkCkD+λtCtS0                       Θ                           Θ(x)=1     x ≥ 0
   In fact,          k=0               holds by simple where  is Heaviside function deﬁned by    ,if    ,
algebra. Thus, by power series convergence theorems, if λC < 1 and Θ(x)=0,otherwise.
                    t                −1
is fulﬁlled, then limt→∞ S =(1− λ)(I − λC) D, which is the The above performance measures can be used both for
solution of Eq. (4), where I is the identity matrix.  evaluating the weighting system and for implementing the

                                                IJCAI-07
                                                  2750learning procedure. A drawback of using positional mea- Name       Feature Set
sures is that they are discrete functions and thus non differ- FS-A idf(w), idf(u), dist(ˆw, uˆ)
entiable. In fact the values of MRR and SR change abruptly FS-B    idf(w), idf(u), dist(ˆw, uˆ), dist(ˆu, Q)
when the positions of two words are exchanged. A possi- FS-B*      idf(w), idf(u), dist(ˆw, uˆ), dist(ˆu, Q), sw-list
ble approach could be to adopt a learning scheme robust to
                                                      Table 1: The set of features inserted in the Context Functions used
discrete environments, e.g. simulated annealing or genetic al-
                                                      for the experiments. dist(ˆw, uˆ) is the number of separating words
gorithms. However, a major problem of these algorithms is
                                                      between occurrence wˆ and uˆ. dist(ˆu, Q) is the number of separating
their stochastic nature that makes the convergence very slow.
                                                      words between uˆ and the closest occurrences of the words of query
  In order to adopt a differentiable error function, we deﬁned
                                                      Q. sw-list indicates that a stopword list is used in order to avoid the
a differentiable function soft pos that replaces the concept
                                                      presence of stopwords within the context window.
of position pos
                     
     soft pos(a)=          σ (γ(s(w) − s(a))) , (7)   useful documents using a search engine (currently Google)
                   w∈W, w=a                          and extracts the candidate answers. These words are then
                                                      ranked using statistical and morphological information.
where W is the set of words considered by the weighting al- In the experiments we extended WebCrow with the addi-
gorithm, a is the word whose position value has to be calcu- tion of the the Context-based algorithm. The results show
lated, and γ is a predeﬁned parameter. Moreover, σ is a sig- that the ranking of the correct answers is improved.
moid function, i.e. a monotone increasing differentiable func- Our dataset consisted of 525 Italian crossword clues3 ran-
tionsuchthatlimx→∞ σ(x)=1and   limx→−∞  σ(x)=0.       domly extracted from the archive in [Ernandes et al., 2005].
  Each term contained in the sum of Eq. (7) compares the The dataset has been divided into two subsets. On one side,
score of word a with the score of another word w.Ifγs(w) NE-questions (165 examples), whose answers are exclusively
is very larger than γs(a), then the term assumes a value close factoid Named Entities, e.g. ≺Real-life comic played in
to 1, and, in the converse case when γs(w) is very smaller ﬁlm by Dustin Hoffman: lennybruce. On the other side,
than γs(a), the term approaches 0. Thus, for large γ, each nonNE-questions (360 examples), whose answers are non-
terms approaches a Heaviside function and soft pos(a) ap- Named Entities (common nouns, adjectives, verbs, and so on,
proximates pos(a).                                    e.g. ≺Twenty four hours ago: yesterday).
  Based on Eq. (5) and (7) we deﬁned a differentiable evalu-
ation function which approximates MRR:                3.2  Weighting techniques under comparison
                   q=Q    i=|w|                       In order to assess the effectiveness of the Context-based al-
                 1       
  soft    (Q)=         1/     σ (γ(s(w ) − s(w )))    gorithm we compared its ranking performances with three
      MRR       |Q|                   i      a
                    q=0   i=0                         different weighting techniques. The ﬁrst is TFIDF,which
                                                (8)   gives a statistical evaluation of the importance of a word
Once the system is trained using this soft function, the evalu- to a document. The other two techniques, WebCrow-S and
ations can be done using the standard MRR (or SR).    WebCrow-SM, represent two ranking methods adopted by
                                                      WebCrow. WebCrow-S exploits only statistical information
3  Experimental Setup                                 of the words, the documents and the queries. This weighting
3.1  The Single-Word QA problem                       scheme is a modiﬁed version of the one presented in [Kwok
                                                      et al., 2001]. WebCrow-SM additionally uses a morpholog-
The problem on which the proposed approach was applied ical analysis of the documents joined with a morphological
is a special case of Question Answering. In Single-Word Answer Type classiﬁcation of the clues. For all the details
QA each question has to be answered with a single correct about WebCrow, see [Ernandes et al., 2005].
word, given a number of documents related to the question.
Since the target answer is a unique correct word the perfor- 4 Experimental Results
mance evaluations, unlike standard QA, do not require any
human refereeing. To the best of our knowledge no standard Train Set and Test Set. The two subsets (NE and nonNE-
dataset is available for the task of Single-Word QA, but a very questions) were randomly divided into a train set and a test
popular and challenging example is provided by crosswords. set, containing 40% and 60% of the examples respectively.
Since crossword clues are intrinsically ambiguous and open- The experiments were repeated 3 times for each conﬁgura-
                                                      tion. Each example of the data set contained: i) a ques-
domain, they represent an very challenging reference point.                       H
It is worth mentioning that Single-Word QA was used as a tion/answer couple; ii) a set of documents retrieved by
testbed since it is particularly suited to evaluate term weight- Google (the documents are selected in Google’s order); iii)
                                                      the vector of ranked terms W (the candidate answer list) ex-
ing. Actually, the answers of the QA system are directly de-        H                             W
pendent on the ranking produced by term weights.      tracted from the documents. In our experiments con-
  The aim of our experiments is to show that the Context- tains words of any length.
based algorithm improves the ranking quality of the candidate Features. Several Context Functions were trained, each one
answers provided by WebCrow [Ernandes et al., 2005].Web- with a different set of features. Table 1 presents the list of
Crow is a Web-based QA system designed to tackle cross- feature combinations adopted in the experiments. To fairly
word clues for crossword cracking. At the core of this system
there is a web search module, which, given a clue, retrieves 3The dataset is available on http://webcrow.dii.unisi.it.

                                                IJCAI-07
                                                  2751                  Learning Curves (MRR)                                   Success Rate at N
                                                               1
       0.36

                                                              0.8
       0.34

                                                              0.6
       0.32

                                                                                     TF−IDF
       MRR                                                    0.4
        0.3             WebCrow−S + context | Train−Set                              TF−IDF+context
                        WebCrow−S + context | Test−Set       Success  Rate           WebCrow−S
                                                              0.2
       0.28             Baseline (WebCrow−S) | Train−Set                             WebCrow−S + context
                        Baseline (WebCrow−S) | Test−Set                              WebCrow−SM + context
                                                               0
       0.26                                                    0   10 20 30  40 50 60  70 80 90 100
         0          50         100        150                             N (max lookdown position)
                        EPOCHS
 Figure 1: Learning curve of the Context Function on the NE- Figure 2: Success Rate at N for the NE-questions test set. The
 question problem with softMRR and the FS-B feature conﬁgu- curves depict the probability (X-axis) of ﬁnding the correct an-
 ration. WebCrow-S is outperformed after 34 epochs. After 60/70 swer to a question within the ﬁrst N candidate answers (Y-axis).
 epochs no sensible improvement can be observed.        For clarity, the curve of WebCrow-SM is not reported.

 Default TW             Feature Set NE     nonNE      fault scores provided by WebCrow-S, produced a 22.3% in-
 TFIDF                              0.216  0.071      crement of the MRR (from 0.29 to 0.355, row 5).
 TFIDF + context        FS-A        0.233  -            An insightful resume of the performance improvements
 WebCrow-S                          0.290  -          is provided by the Success Rate curve (Fig. 2), that shows
 WebCrow-S + context    FS-B        0.346  -          the probability of ﬁnding the correct answer within the ﬁrst
 WebCrow-S + context    FS-B*       0.355  -          N  words of W. Each weighting scheme under compar-
 WebCrow-SM                         0.293  0.104      ison appears consistently below the curve of its Context-
 WebCrow-SM + context   FS-B*       0.345  0.121      based reweighting. In particular, with N =50, the ob-
                                                      served success rate was: TFIDF, 60.4%; TFIDF+context,
Table 2: MRR performance.The results of the three baseline algo- 63.4%; WebCrow-S, 66.7%; WebCrow-S+context, 79.3%;
rithms are given in bold. Column Default TW denotes the algorithm WebCrow-SM, 75%; WebCrow-SM+context, 83.3%.
used for the default term weights dw, Feature Set speciﬁes the Fea-
ture Set of the Context Function. The last two columns contain the EXP 2: Answering nonNE-questions. Since the nonNE-
MRR values obtained by the system on the two subsets of the testset. questions represent a more difﬁcult task, all the Context Func-
“context” indicates that the context weighting algorithm is used. tions were trained and tested with a higher number of doc-
                                                      uments, H =10. The improvement (Tab. 2) of the rank-
compare the Context-based weighting with the other tech- ing quality provided by the Context-based algorithm was less
niques the features were restricted to those effectively used by evident than on the NE subset, but still clear. This phe-
each algorithm under comparison. For example, to reweight nomenon depends on the different nature of these questions,
TFIDF scores we adopted a feature set, called FS-A (Tab. 1, which make improbable a strong increment of the MRR using
ﬁrst row), that exploits no information about the question. In exclusively statistical features, as revealed by the poor perfor-
this work no morphological information was integrated in the mance of the TFIDF measure (ﬁrst row). The reweighting of
Context Functions. Nevertheless, these were able to improve the term scores provided by WebCrow-SM lead to a 16.3%
also WebCrow-SM’s performances, which adopts morpho-  increment of the MRR (from 0.104 to 0.121, row 6 and 7).
logical analysis. For simplicity we chose 20 as a ﬁx dimen-
                                                                                                      H
sion for the context window, with kL =10and kR =10.   EXP 3: Varying the number of the Web documents (  ).
                                                      An interesting feature to evaluate is the ability of the Context
Initialization and stop policy. The parameters of the Con- Function to work with a number of documents that differs
text Functions were initialized with small random weights, from that used in the learning phase. For this reason we tested
with the only exception of λ, that was initialized to 0.5. Each the ranking quality of the Context Functions trained over the
training session was stopped after 75 epochs. This stop policy NE subset, with dw given by WebCrow-SM (last two rows
was suggested by a prior observation of the variation curves of Table 2), varying H. The Context-based algorithm proved
over both the learning set and the test set (see Fig. 1). The to be robust to the changes of H, constantly outperforming
resilient learning scheme guaranteed a fast convergence, re- WebCrow-SM (Fig. 3). With H =20, the addition of the
quiring less than 60/70 epochs to reach good local minima. context increased the MRR by 15% (from 0.355 to 0.408).
EXP 1: Answering  NE-questions. For the NE-questions  An example.  An example can help to better illustrate these
problem the Context Functions were trained and tested using promising results. WebCrow-SM answered the question:
H =5as the number of documents used for extracting the ≺La paura nella folla: panico ( ≺Fear among the crowd:
word candidates. The results are presented in Table 2, which panic), with the correct answer in 16-th position. With the
shows the performances in terms of MRR. The ranking per- addition of the Context-based weighting, panic jumped to 2-
formances of the compared methods are given in bold at row nd position, and other related words appeared among the ﬁrst
1 (TFIDF), 3 (WebCrow-S) and 6 (WebCrow-SM).          20 candidates, as: collective, multitude, emergency
  The Context-based algorithm outperformed the compared and irrational. A passage extracted from one of the docu-
weighting methods. In particular, the reweighting of the de- ments may explain why the context information can result ef-

                                                IJCAI-07
                                                  2752