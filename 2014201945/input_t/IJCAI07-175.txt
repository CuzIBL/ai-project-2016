      Metric Properties of Structured Data Visualizations through Generative
                                      Probabilistic Modeling
                                Peter Tinoˇ        Nikolaos Gianniotis
                                       University of Birmingham
                                      School of Computer Science
                                       Birmingham B15 2TT, UK
                                        pxt,nxg@cs.bham.ac.uk

                    Abstract                          data items in the original data space. Otherwise, one would
                                                      not be able to detect and understand the underlying cluster
    Recently, generative probabilistic modeling princi- structure of data items. To that end, in the case of static vec-
    ples were extended to visualization of structured torial data of ﬁxed-dimensionality, Bishop et al. [1997] cal-
    data types, such as sequences. The models are for- culated magniﬁcation factors of the Generative Topographic
    mulated as constrained mixtures of sequence mod-  Mapping (GTM)  [Bishop et al., 1998b] using tools of differ-
    els - a generalization of density-based visualization ential geometry.
    methods previously developed for static data sets.
    In order to effectively explore visualization plots, However, in the case of structured data types (such as se-
    one needs to understand local directional magniﬁ- quences or trees) one has to be careful when dealing with the
    cation factors, i.e. the extend to which small posi- notion of a metric in the data space. The generative prob-
    tional changes on visualization plot lead to changes abilistic visualization models studied in this paper naturally
    in local noise models explaining the structured   induce a metric in the structured data space. Loosely speak-
    data. Magniﬁcation factors are useful for highlight- ing, two data items (sequences) are considered to be close (or
    ing boundaries between data clusters. In this paper similar) if both of them are well-explained by the same un-
    we present two techniques for estimating local met- derlying noise model (e.g. HMM) from the two-dimensional
    ric induced on the sequence space by the model for- manifold of noise models. Due to non-linear parametrization
    mulation. We ﬁrst verify our approach in two con- of the manifold, projections of two quite different data items
    trolled experiments involving artiﬁcially generated may end up being mapped close to each other. We emphasise,
    sequences. We then illustrate our methodology on  that in this framework, the distance between structured data
    sequences representing chorals by J.S. Bach.      items is implicitly deﬁned by the local noise models that drive
                                                      topographic map formation. If the noise model changes, the
                                                      perception of what kind of data items are considered similar
1  Introduction                                       changes as well. For example, if the noise models were 1st-
Topographic visualisation techniques have been an important order Markov chains, sequences would be naturally grouped
tool in multi-variate data analysis and data mining. Gener- with respect to their short-range subsequence structure. If,
ative probabilistic approaches [Bishop et al., 1998b; 1998a; on the other hand, the noise models were stochastic machines
Kab´an and Girolami, 2001; Nabney et al., 2005] demon- specializing at latching special events occurring within the
strated advantages over non-probabilistic alternatives in terms data sequences, the sequences would naturally group with re-
of a ﬂexible and technically sound framework that makes spect to the number of times the special event occurred, ir-
various extensions possible in a principled manner.Recently, respective of the length of temporal separation between the
Tino et al.[2004] introduced a general framework for visualiz- events.
ing sets of sequential data. Loosely speaking, a smooth two- In this paper, we quantify the extend to which small po-
dimensional manifold in the space of appropriate noise mod- sitional changes on manifold of local noise models explain-
els (e.g. Hidden Markov models (HMM) of a given topology) ing structured data (for visualization purposes the manifold
is constructed so that HMMs constrained on the manifold ex- is identiﬁed with visualization space) lead to changes in the
plain the observed sequences well. Each sequence (data item) distributions deﬁned by the noise models. It is important
is then projected on the manifold by visualizing the relevant to quantify the changes in a parametrization-free manner -
HMMs likely to be responsible for generating that sequence. we use approximations of Kullback-Leibler divergence. We
  It is possible in non-linear projections that visualizations present two techniques for estimating local metric induced
of two data items get close in the visualization space, even on the structured data space by the model formulation. We
though they may be separated by a large distance in the data ﬁrst verify our approach in two controlled experiments in-
space (and vice-versa). Therefore, to be of practical use, non- volving artiﬁcially generated sequences. We then illustrate
linear projection methods must be equipped with representa- our methodology on sequences representing chorals by J.S.
tions in the visualization space of metric relationships among Bach.

                                                IJCAI-07
                                                  10832  A latent trait model for sequential data           Maximum likelihood estimates of the free parameters can
Consider a set of sequences over the alphabet S of S sym- be obtained via Expectation-Maximization (EM) algorithm
bols, S = {1, 2, ..., S}.Then-th sequence is denoted by [Tino et al., 2004].
 (n)     (n)                                            Given a sequence s, some regions of the latent visualiza-
s           t=1:T ,where             and  n denotes
    =(st    )   n        n =1:N          T            tion space are better at explaining it than the others. A natural
its length. The aim is to represent each sequence using a                        s
                                      V     −    2    representation (visualization) of is the mean of the posterior
two-dimensional latent (visualization) space =[ 1, 1] . distribution over the latent space, given that sequence:
In order to exploit the visualization space fully, a maximum
entropy (uniform) prior distribution is imposed over the latent               C
                        x
space. With each latent point , we associate a generative dis-      Proj(s)=     xc p(xc|s).
tribution over sequences p(s|x). One possibility (considered                  c=1
in this paper) is to use HMM with K hidden states [Rabiner
and Juang, 1986]:                                     3   Quantifying metric properties of the
                    Tn            Tn
   (n)                                    (n)             visualization space
p(s  |x)=     p(h1|x)   p(ht|ht−1, x)  p(s  |ht, x),
                                          t           In this section, we present two approaches to quantifying
           h         t=2            t=1
                                                      metric properties of the visualization space V as sensitivi-
                                                (1)
                                                      ties (measured by Kullback-Leibler divergence) of local noise
where h is the set of all Tn-tuples over the K hidden states.
                                                      models (HMM) addressed by the points in V to small pertur-
  For tractability reasons, we discretize the latent space into
                                                      bations in V.
a regular grid of C points1 x1, ..., xC . In order to have the
                                            [
HMMs topologically organized, we (in the spirit of Bishop 3.1 Approximating Kullback-Leibler divergence
et al., 1998b; Kab´an and Girolami, 2001]) constrain the ﬂat
mixture of HMMs,                                           through observed Fisher information matrix
                                                                                          V      −      2
                        C                            Consider the visualization (latent) space =[1, +1]
                 s    1       s|x                     and the two-dimensional manifold M of local noise mod-
               p( )=        p(   c),                       ·|x
                      C c=1                           els p( ) (e.g. HMM  with 3 states, emissions over 7 sym-
                                                      bols) parametrized by the latent space through (1) and (2-4).
by requiring that the HMM parameters be generated through The manifold is embedded in manifold H of all noise models
a parameterised smooth nonlinear mapping from the latent of the same form (e.g. all HMMs with 3 states and emis-
     V
space  into the HMM parameter space:                  sions over 7 symbols). Consider a latent point x ∈V.Ifwe
                                   (π)                displace x by an inﬁnitesimally small perturbation dx,the
              p(h1 = k|x)=gk(A        φ(x)),    (2)
                                                      Kullback-Leibler divergence DKL(p(·|x)(p(·|x + dx)) be-
                                   (T l)
     p(ht = k|ht−1 = l, x)=gk(A       φ(x)),    (3)   tween the corresponding noise models p(·|x), (p(·|x+dx) ∈
                                                      M
                                   (Bk )                 can be determined via Fisher information matrix
        p(st = s|ht = k, x)=gs(A       φ(x)),   (4)
                                                                 F (x)   −        ∇2      |x
where indexes k, l run from 1 to K; index s ranges from 1 to           =   Ep(·|x)[  log p(. )]
S,and                                                 that acts like a metric tensor on the Riemannian manifold M
  • the function g(.) is the softmax function2 and gk(.) de- [Kullback, 1959]:
    notes the k-th component returned by the softmax, i.e.                               T
                                                              KL   ·|x   ·|x    x     x  F (x)  x
                            ak                            D    (p(  ) (p(  + d  )) = d        d .
                     T       e
      gk (a1,a2, ..., aq) = q     ,k=1,  2, ..., q,
                                 ai
                             i=1 e                    The situation is illustrated in ﬁgure 1.
                         T          2                   Local noise models used in this paper (HMMs) are latent
  • φ(.)=(φ1(.), ..., φM (.)) ,φm(.):R →Ris an or-    variable models and there is no closed-form formula for cal-
    dered set of M non-parametric nonlinear smooth basis culating the Fisher information matrix. However, Lystig and
    functions (typically RBFs),                       Hughes[2002] presented a framework for efﬁcient calculation

                 (π)     K×M     (T l)    K×M         of the observed Fisher information matrix of HMMs based on
  • the matrices A   ∈R       , A     ∈R       and
      B  )                                            forward calculations related to those used in maximum like-
    A(  k  ∈RS×M
                   are free parameters of the model.  lihood parameter estimation via EM algorithm. We adapt the
  Assuming the sequences s(n), n =1:N, were indepen-  framework to a special kind of parametrization of HMM used
dently generated, the model likelihood reads          in the latent trait model of section 2. Each HMM has two pa-
                                                      rameters – latent space coordinates x =(x1,x2) (these can
            N          N    C
                  (n)       1       (n)               be thought of as coordinates on the visualization screen).
       L =     p(s  )=           p(s   |xc).    (5)
                                                        Consider a set S(x) of N sequences independently gen-
            n=1         n=1 C c=1
                                                      erated by HMM p(·|x). All sequences have equal length T .
  1these sample (grid) points are analogous to the nodes of a Self Given the n-th sequence, we start recursion from the begin-
Organising Map                                        ning of the sequence:
  2
   which is the canonical inverse link function of multinomial dis- (n) (n)
tributions                                                  λ1 (k)=p(s1   |h1 = k, xc)p(h1 = k|x)

                                                IJCAI-07
                                                  1084                               x2
                       +1
                                     x+dx
                                                                          p(.x+dx)
                                    x                                  p(.x)

                                          x1
                                                         M

                          V
                        1               +1

                                                                H

Figure 1: Two-dimensional manifold M of local noise models p(·|x) parametrized by the latent space V through (1) and (2-4).
The manifold is embedded in manifold H of all noise models of the same form. Latent coordinates x are displaced to x + dx.
Kullback-Leibler divergence DKL(p(·|x)(p(·|x + dx) between the corresponding noise models p(·|x), (p(·|x + dx) ∈M
can be determined via Fisher information matrix F (x) that acts like a metric tensor on the Riemannian manifold M.

                                                           (n)         K     (n)
  Recursive step:                                     and Ωt  (xq,xr)=   j=1 ωt (j; xq,xr).
      (n)           (n)        (n)     (n)                                              (n)  (n)  (n)
     λt  (k)=p(st     ,ht = k|s1 ,...,st−1, x)          The calculations recursively employ ψt−1, ωt−1, λt−1 and
                                                        (n)
                 K                                   Λt−1. We also need both 1st- and 2nd-order derivatives of ini-
                      (n)     (n)
             =      [λt−1(i)p(st |ht = k, x)          tial state, state transition and state-conditional emission prob-
                 i=1                                  abilities with respect to latent coordinates. Again, we present
                                       (n) −1         only the equation for state transitions:
                    t    | t−1    x
                 p(h =  k h   = i, )](Λt−1)  ,                 2
                                                             ∂                           ∂      (T )
       (n)    K       (n)                                                 |        x           A   l φ x
                   t                                               p(ht = k ht−1 = l, )=    gk(       ( ))
where Λt  =   j=1 λ (j) .                                   ∂xq∂xr                      ∂xr
  Starting again at the beginning of the sequence, we recur-                                           
                                                                    φ x    K                      φ x
sively evaluate 1st-order derivatives with respect to latent co- (T l) ∂ ( )       (T l)      (T l) ∂ ( )
                                                             Ak          −    [gi(A    φ(x))Ai         ]
ordinates x1,x2. Letting q ∈{1, 2},wehave:
                                                                    ∂xq    i=1                     ∂xq
     (n)          ∂    (n)
                         |       x         |x                                                    (T l)
    ψ1  (k; xq)=    p(s1  h1 = k, )p(h1 = k  ),                                           +gk(A     φ(x))
                 ∂xq                                  
                                                               2φ x    K                         φ  x
                   ∂    (n)     (n)                      (T l) ∂ ( )        ∂     (T )       (T l) ∂ ( )
                                    t    |x             A            −           A   l φ x A
       (n)         ∂xq p(s1 ,... st ,h = k )             k                [   gi(       ( )) i         +
      ψt (k; xq)=                           ,                ∂xq∂xr        ∂xr                    ∂xq
                          (n)     (n) |x                               i=1                              
                       p(s1 ,...,st−1 )                                                           2
                                                                                 (T )       (T ) ∂ φ(x)
     (n)        K    (n)                                                        A   l φ x  A   l
         q                 q                                                  gi(      ( ))  i         ]
and Ψt (x )=    j=1 ψt (j; x ).                                                                  ∂xq∂xr
                                  (n)  (n)     (n)
  The calculations recursively employ ψt−1, λt−1 and Λt−1. Denoting
We also need 1st-order derivatives of initial state, state transi-   (n)           (n)     (n)
tion and state-conditional emission probabilities with respect Q(n) ΩT  (xq,xr) − ΨT  (xq)ΨT (xr)
                                                               q,r =     (n)            (n)      ,
to latent coordinates. We present only the equation for state          Λ              (Λ   )2
transitions, other formulas can be obtained in the same man-             T              T
ner:                                                  we calculate elements of the observed Fisher information ma-
                                                          Fˆ(x)                       S x
         ∂                            (T )            trix     , given the set of sequences ( ),as
                   |        x       A   l φ x
           p(ht = k ht−1 = l, )=gk(        ( ))                                    N
        ∂xq                                                                        
                                                                     ˆ          1       (n)
                                                                  F (x)   = −       Qq,r .
        φ x    K                      φ x                               q,r    N
   (T l) ∂ ( )         (T l)      (T l) ∂ ( )                                      n=1
 Ak          −    [gi(A    φ(x))Ai          ] ,
        ∂xq     i=1                    ∂xq              To illustrate metric properties of the visualization space,
                                                                                                   ˆ
       (.)                                      (.)   we calculate the observed Fisher information matrices F (xc)
where A  denotes the i-th row of the parameter matrix A .
       i                                              in all latent centers xc, c =1,  2, ..., C, and detect
  2nd-order derivatives are obtained in a recursive manner as               Fˆ(x )
well:                                                 through eigen-analysis of c  directions of dominant lo-
                    2                                 cal change in Kullback-Leibler divergence between the HMM
  (n)                     (n)
                   ∂                                                xc
 ω1  (k; xq,xr)=       p(s1 |h1 = k, x)p(h1 = k|x),   parametrized by  and its perturbed version parametrized by
                ∂xq∂xr                                xc + dx. In the visualization space, we signify magnitude of
                                                                                             ˆ
                    ∂2     (n)     (n)                the dominant change (dominant eigenvalue of F (xc))bylo-
                        p(s1  , ···,st ,ht = k|x)
    (n)            ∂xq∂xr                             cal brightness of the background as well as mark the direction
   ωt  (k; xq,xr)=         (n)     (n)
                         p(s1 , ···,st−1|x)           of the dominant change by a piece of line.

                                                IJCAI-07
                                                  10853.2  Direct recursive approximation of                4   Experiments
     Kullback-Leibler divergence                      In this section we demonstrate techniques introduced in sec-
Another alternative approach to probe metric properties of tions 3.1 and 3.2 to provide additional metric information
the visualization space is the estimate DKL(p(·|x)(p(·|x + when visualizing sequential data.
 x
Δ  )) directly. Do [2003] presented an efﬁcient algorithm for In all experiments the latent space centres xc were posi-
approximating Kullback-Leibler (K-L) divergence between tioned on a regular 10 × 10 square grid (C = 100)andthere
two HMM of the same topology. The approximation is based were M =16basis functions φi. The basis functions were
on backward calculations related to those used in maximum spherical Gaussian functions of the same width σ =1.0.The
likelihood parameter estimation via EM algorithm.     basis functions were centred on a regular 4×4 square grid, re-
  With each hidden state k ∈{1, 2, ..., K} we associate an ﬂecting uniform distribution of the latent classes. We account
auxiliary process βk(t). Given a sequence s =(st)t=1:T ,the for a bias term by using an additional constant basis function
           s|x
likelihood p( ) can be efﬁciently calculated by       φ17(x)=1.
  • starting at the end of the sequence:                Free parameters of the model were randomly initialized in
                                                                 −
                 βk(T ; x)=p(sT |k, x)                the interval [ 1, 1]. Training consisted of repeating EM iter-
                                                      ations [Tino et al., 2004]. Typically, the likelihood levelled
  • Recursive step:
                                                      up after 30-50 EM cycles.
                       K
                                                       When calculating metric properties of the visualization
     k   x      t|  x        t+1   | t    x   i    x
    β (t; )=p(s   k, )    p(h   = i h = k,  )β (t+1; ) space, each set S(xc) consisted of 100 generated sequences
                      i=1                             of length 40.
  • Final step:
                     K                               4.1  Toy data
            p(s|x)=     p(h1 = k|x)βk(1; x)           We ﬁrst generated a toy data set of 400 binary sequences
                     k=1                              of length 40 from four HMMs (K =2hidden states) with
  For u, v ∈ (0, 1],deﬁne                             identical emission structure, i.e. the HMMs differed only in
                               u                      transition probabilities. Each of the four HMMs generated
                 κ(u, v)=u  log .                                                      ·|x
                               v                      100 sequences. Local noise models p( ) were imposed as
The Kullback-Leibler (K-L) divergence between two HMMs HMMs with 2 hidden states. Visualization of the sequences
p(·|x) and p(·|x), can be approximated as follows:   is presented in ﬁgure 2(a). Sequences are marked with four
                                                      different markers, corresponding to the four different HMMs
  • Recursion is initiated at end of the sequence:
                                                      used to generate the data set. We stress that the model was
               x x          |  x      |  x
         Dk(T ; ,  )=κ(p(sT   k, ),p(sT k,  ))        trained in a completely unsupervised way. The markers are
  • Recursive step:                                   used for illustrative purposes only. Representations of in-
                                        
         Dk(t; x, x )=κ(p(st|k, x),p(st|k, x )) +     duced metric in the local noise model space based on Fisher
                |      x ||     |       x            Information matrix (section 3.1) and direct K-L divergence
    DKL(p(ht+1  ht = k, ) p(ht+1 ht = k,  )) +        estimations (section 3.2) can be seen in ﬁgures 2(b) and 2(c),
          K
                                                     respectively. Dark areas signify homogeneous regions of lo-
             p(ht+1 = i|ht = k, x)Di(t +1;x, x )      cal noise models and correspond to possible clusters in the
          i=1                                         data space. Light areas signify abrupt changes in local noise
  • Final step: the empirical K-L divergence, given the se- model distributions (as measured by K-L divergence) and cor-
    quence s, is bounded by                           respond to boundaries between data clusters. The visualiza-
                                     
       K(s, x, x )=DKL(p(h1|x)||p(h1|x )) +           tion plot reveals that the latent trait model essentially discov-
                                                      ered the organization of the data set and the user would be
                  K
                             |x      x  x            able to detect the four clusters, even without help of the mark-
                     p(h1 = k  )Dk(1;  ,  ).          ing scheme in ﬁgure 2(a). Of course, the latent trait model
                  k=1                                 beneﬁted from the fact that the distributions used to gener-
  Given the set of N sequences, S(x), generated inde- ate data were from the same model class as the local noise
pendently by the HMM  addressed by x, an estimate of  models. There were few atypical sequences generated by the
DKL(p(·|x)(p(·|x +Δx)) is calculated as              HMM marked with ’+’, and this is clearly reﬂected by their
                             N                       projections in the lower left corner.
                           1        (n)
Dˆ KL(p(·|x)(p(·|x +Δx)) =      K(s   , x, x +Δx).
                           N n=1                      4.2  Melodic lines of chorals by J.S. Bach
  Again, to illustrate metric properties of the visualization Next we subjected the latent trait model to a set of 100
space, we perturb latent centers xc in regular intervals around chorales by J.S. Bach from UCI repository of Machine Learn-
small circle centered at xc and for each perturbation Δx cal- ing Databases. We extracted the melodic lines – pitches are
culate DˆKL(p(·|x)(p(·|x +Δx)). As before, in the visual- represented in the space of one octave, i.e. the observation
ization space, we signify magnitude of the dominant change symbol space consists of 12 different pitch values. Local
by local brightness of the background as well as mark the di- noise models had K =3hidden states. Figure 3 shows choral
rection of the dominant change by a piece of line.    visualizations, while representations of induced metric in the

                                                IJCAI-07
                                                  1086             (a)(b)                                                                     (c)

Figure 2: (a) Visualization of 400 binary sequences of length 40 generated by four HMMs with 2 hidden states and with
identical emission structure. Sequences are marked with four different markers, corresponding to the four HMMs. Also shown
are representations of induced metric in the local noise model space based on Fisher Information matrix (b) and direct K-L
divergence estimations (c).


 flats            Latent space visualization
         1                                            5   Discussion and conclusion

        0.8
                                                      When faced with the problem of non-linear topographic data
        0.6

        0.4                                           visualization, one needs additional information putting rel-
                                     infrequentflats/sharps
        0.2                                           ative distances of data projections in proportion to local

         0                                            stretchings/contractions of the visualization manifold in the
  sharps
                                     gfgpatterns
        −0.2                                          data space. It is important to base representations of manifold
        −0.4                                          stitchings/contractions on the particular notion of distance or
        −0.6                                          similarity between data items that drives formation of the to-
        −0.8                                          pographic mapping. If one visualizes structured data, such

        −1
         −1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1   as sequences, this can be a highly non-trivial task, especially
                                                      for the family of neural-based topographic mappings of se-
   tritons   caddc
                     repeatedfnotesingfgpatterns      quences [Koskela et al., 1998; Horio and Yamakawa, 2001;
                          gfffg                       Voegtlin, 2002; Strickert and Hammer, 2003].However,
                                                      when the visualization model is formulated as a latent trait
Figure 3: Visualization of 100 melodic lines of chorales by model, such calculations follow naturally from the model
J.S. Bach.                                            formulation. Two sequences are considered to be similar if
                                                      both of them are well-explained by the same underlying noise
                                                      model (in our particular case - HMM). Noise models are or-
local noise model space based on Fisher Information matrix ganized on a smooth two-dimensional manifold and we can
and direct K-L divergence estimations can be seen in ﬁgures study changes of the local metric (based on Kullback-Leibler
4(a) and 4(b), respectively. The method discovered natural divergence) due to non-linear parametrization. We have pre-
topography of the key signatures, corroborated with similar- sented two approaches to metric analysis of the visualization
ities of melodic motives. The melodies can contain sharps space and experimentally veriﬁed that additional information
and ﬂats other than those included in their key signature due provided by metric representations in the visualization space
to both modulation and ornaments. The upper region con- can be useful for understanding of the overall organization of
tains melodic lines that utilise keys with ﬂats. Central part data. We stress. that the data organization revealed is always
of the visualisation space is taken by sharps (left) and almost with respect to the notion of similarity between data items
no sharps/ﬂats (center). The lower region of the plot con- imposed in the model construction.
tains chorals with tense patterns (e.g containing tritons) and The results can be easily generalized e.g. to tree-
is quite clearly strongly separated from other chorals. Again, structured data using, for example, Hidden Markov Tree
the overall clustering of chorals is well-matched by the met- models (HMTM) [Durand et al., 2004]). We stress that the
ric representations of ﬁgures 4(a) and 4(b). Flats, sharps and presented framework is general and can be applied to vi-
tense patterns are clearly separated, as are sharps and infre- sualizations through latent trait models of a wide variety of
quent sharps/ﬂats. There are more interesting features to this structured data, provided a suitable noise model is used. For
visualization (e.g. enharmony patterns) that cannot be ad- example, we are currently working on topographic organiza-
dressed due to space limitations.                     tions (and metric properties of such) of ﬂuxes form binary star

                                                IJCAI-07
                                                  1087