                          Learning     to Play   Like  the  Great   Pianists
                      Asmir  Tobudic                             Gerhard   Widmer
              Austrian  Research Institute for        Department  of Computational  Perception,
               Artiﬁcial Intelligence, Vienna             Johannes  Kepler University, Linz
                   asmir.tobudic@ofai.at                   Austrian  Research Institute for
                                                            Artiﬁcial Intelligence, Vienna
                                                               gerhard.widmer@jku.at

                    Abstract                          CD  recordings and the printed score of the music. Exper-
                                                      iments show that the system indeed captures some aspect of
    An application of relational instance-based learn- the pianists’ playing style: the machine’s performances of un-
    ing to the complex task of expressive music per-  seen pieces are substantially closer to the real performances
    formance is presented. We investigate to what ex- of the ‘training’ pianist than those of all other pianists in our
    tent a machine can automatically build ‘expres-   data set. An interesting by-product of the pianists’ ‘expres-
    sive proﬁles’ of famous pianists using only min-  sive models’ is demonstrated: the automatic identiﬁcation of
    imal performance information extracted from au-   pianists based on their style of playing. And ﬁnally, the ques-
    dio CD  recordings by pianists and the printed    tion of automatic style replication is brieﬂy discussed.
    score of the played music. It turns out that the    The rest of the paper is laid out as follows. After a short
    machine-generated expressive performances on un-  introduction to the notion of expressive music performance
    seen pieces are substantially closer to the real per- (Section 2), Section 3 describes the data and its representa-
    formances of the ‘trainer’ pianist than those of all tion in FOL. We also discuss how the complex task of learn-
    others. Two other interesting applications of the ing expressive curves can be decomposed into a well-deﬁned
    work are discussed: recognizing pianists from their instance-based learning task and shortly recapitulate the de-
    style of playing, and automatic style replication. tails of the relational instance-based learner DISTALL. Ex-
                                                      perimental results are presented in Section 4.
1  Introduction
Relational instance-based learning is a machine learning 2 Expressive Music  Performance
paradigm that tries to transfer the successful nearest-neighbor Expressive music performance is the art of shaping a musi-
or instance-based learning (IBL) framework to richer ﬁrst- cal piece by continuously varying important parameters like
order logic (FOL) representations [Emde and Wettschereck, tempo, loudness, etc. while playing [Widmer et al., 2003].
1996; Tobudic and Widmer, 2005]. As such it is a part of in- Human musicians do not play a piece of music mechanically,
ductive logic programming (ILP), the ﬁeld of research which with constant tempo or loudness, exactly as written in the
– after the euphoria in the nineties – suffered a certain ﬂat- printed music score. Rather, skilled performers speed up at
tening of interest in recent years, the main reason being the some places, slow down at others, stress certain notes or pas-
difﬁculties of effectively constraining the extremely large hy- sages etc. The most important parameter dimensions avail-
pothesis spaces. Nevertheless, some ILP systems have re- able to a performer are timing (tempo variations) and dynam-
cently been shown to achieve performance levels competi- ics (loudness variations). The way these parameters ‘should
tive to those of human experts in very complex domains (e.g. be’ varied is not speciﬁed precisely in the printed score; at the
[King et al., 2004]).                                 same time it is what makes a piece of music come alive, and
  A successful application of relational IBL to a real-world what distinguishes great artists from each other.
task from classical music has been presented in previous work Tempo and loudness variations can be represented as
([Tobudic and Widmer, 2005]). A system that predicts ex- curves that quantify these parameters throughout a piece rel-
pressive timing and dynamics patterns for new pieces by anal- ative to some reference value (e.g. the average loudness or
ogy to similar musical passages in a training set has been tempo of the same piece). Figure 1 shows a dynamics curve
shown to learn to play piano music ‘expressively’ with sub- of a small part of Mozart’s Piano Sonata K.280, 1st move-
stantial musical quality.                             ment, as played by ﬁve famous pianists. Each point gives the
  Here we investigate an even more interesting question: can relative loudness (relative to the average loudness of the piece
a relational learner learn models that are ‘personal’ enough by the same performer) at a given point in the piece. A purely
to capture some aspects of the playing styles of truly great pi- mechanical (unexpressive) rendition of the piece would cor-
anists? A system is presented that builds performance models respond to a ﬂat horizontal line at y = 1.0. Tempo variations
of six famous pianists using only crude information related to can be represented in an analogous way. In the next section
expressive timing and dynamics obtained from the pianist’s we discuss how predictive models of such curves can be auto-      D. Barenboim                                    Table 1: Mozart sonata sections selected for analysis. Sec-
 1.6  G. Gould
      M. J. Pires                                     tion ID should be read as <sonataName> : <movement> :
 1.4  A. Schiff
      M. Uchida                                       <section>. The total numbers of phrases are also shown.
 1.2

  1                                                             Section ID Tempo descr. #phrases
                                                                 kv279:1:1   fast 4/4    460

 rel.  dynamics 0.8
                                                                 kv279:1:2   fast 4/4    753
 0.6                                                             kv280:1:1   fast 3/4    467
 0.4                                                             kv280:1:2   fast 3/4    689
      32    34   36   38    40   42   44   46    48              kv280:2:1   slow 6/8    129
                          bars                                   kv280:2:2   slow 6/8    209
                                                                 kv280:3:1   fast 3/8    324
Figure 1: Dynamics curves of performances of ﬁve famous          kv280:3:2   fast 3/8    448
pianists for Mozart Sonata K.280, 1st mvt., mm. 31–48. Each      kv282:1:1   slow 4/4    199
point represents the relative loudness at the beat level (relative kv282:1:2 slow 4/4    254
to the average loudness of the piece by the same performer).     kv283:1:1   fast 3/4    455
                                                                 kv283:1:2   fast 3/4    519
                                                                 kv283:3:1   fast 3/8    408
matically learned from information extracted from audio CD       kv283:3:2   fast 3/8    683
recordings and the printed score of the music.                   kv332:2     slow 4/4    549

3  Data  and  Methodology
                                                                  Table 2: Pianists and recordings.
The data used in this work was obtained from commercial
recordings of famous concert pianists. We analyzed the per-
formances of 6 pianists across 15 different sections of piano ID Pianist name          Recording
sonatas by W.A.Mozart. The pieces selected for analysis are DB Daniel Barenboim EMI Classics CDZ 7 67295 2, 1984
                                                        RB     Roland Batik      Gramola 98701-705, 1990
complex, different in character, and represent different tempi GG Glenn Gould Sony Classical SM4K 52627, 1967
and time signatures. Tables 1 and 2 summarize the pieces, MP Maria Joao˜ Pires    DGG  431 761-2, 1991
pianists and recordings selected for analysis.          AS     Andras´ Schiff  ADD (Decca) 443 720-2, 1980
  For learning tempo and dynamics ‘proﬁles’ of the pianists MU Mitsuko Uchida Philips Classics 464 856-2, 1987
in our data set we extract time points from the audio record-
ings that correspond to beat 1 locations. From the (varying)
time intervals between these points, the beat-level tempo and 3.1 Phrase Representation in FOL
its changes can be computed. Beat-level dynamics is com-
puted from the audio signal as the overall loudness of the Phrases and relations between them can be naturally repre-
signal at the beat times as a very crude representation of the sented in ﬁrst-order logic. In our collection of pieces, phrases
dynamics applied by the pianists. Extracting such informa- are organized at three hierarchical levels, based on a man-
tion from the CD recordings was an extremely laborious task, ual phrase structure analysis. The musical content of each
even with the help of an intelligent interactive beat tracking phrase is encoded in the predicate phrCont/18. It has the
system [Dixon, 2001]. From these measurements, computing form phrCont(Id,A1,A2,...), where Id is the phrase identiﬁer
pianists’ dynamics and tempo performance curves as shown and A1,A2,... are attributes that describe very basic phrase
in Figure 1 – which are the raw material for our experiments properties. The ﬁrst seven of these are numeric: the length
– is rather straightforward.                          of a phrase, the relative position of the highest melodic point
                                                      (the ‘apex’), the melodic intervals between starting note and
  An examination of the dynamics curves in Figure 1 re-
                                                      apex, and between apex and ending note, metrical strengths
veals certain trends common for all pianists (e.g. up-down,
                                                      of starting note, apex, and ending note. The next three at-
crescendo-decrescendo tendencies). These trends reﬂect cer-
                                                      tributes are discrete: the harmonic progression between start,
tain aspects of the underlying structure of the piece: A piece
                                                      apex, and end, and two boolean attributes that state whether
of music commonly consists of phrases – segments that are
                                                      the phrase ends with a ‘cumulative rhythm’, and whether it
heard and interpreted as coherent units. Phrases are organized
                                                      ends with a cadential chord sequence. The remaining at-
hierarchically: smaller phrases are grouped into higher-level
                                                      tributes describe – in addition to some simple information
phrases, which are in turn grouped together, constituting a
                                                      about global tempo and the presence of trills – global char-
musical context at a higher level of abstraction etc. In Figure
                                                      acteristics of the phrases in statistical terms: mean and vari-
1, the hierarchical, three-level phrase structure of this passage
                                                      ance of the durations of the melody notes within the phrase
is indicated by three levels of brackets at the bottom. In this
                                                      (as rough indicators of the general ‘speed’ of events and of
work we aim at learning expressive patterns at different lev-
                                                      durational variability), and mean and variance of the sizes of
els of such a phrase structure, which roughly corresponds to
                                                      the melodic intervals between the melody notes (as measures
various levels of musical abstraction.
                                                      of the ‘jumpiness’ of the melodic line).
  1The beat is the time points where listeners would tap their foot This propositional representation ignores an essential as-
along with the music.                                 pect of the music: its temporal nature. The temporal re- 1.5                                                  levels of brackets at the bottom of the plot. The elementary
                                                      phrase shapes (at four levels of hierarchy) obtained after de-
                                                      composition are plotted in gray. We end up with a training
                                                      example for each phrase in the training set — a predicate
  1                                                   phrShape(Id, Coeﬀ ), where Coeﬀ = {a, b, c} are the coef-
                                                      ﬁcients of the polynomial ﬁtted to the part of the performance
rel.  dynamics
                                                      curve associated with the phrase.
                                                        Input to the learning algorithm are the (relational) repre-

 0.5                                                  sentation of the musical scores plus the training examples
  31    32    33    34    35    36   37    38    39
                      score position (bars)           (i.e. timing and dynamics polynomials), for each phrase in
                                                      the training set. Given a test piece the learner assigns the
Figure 2: Multilevel decomposition of dynamics curve of per- shape of the most similar phrase from the training set to each
formance of Mozart Sonata K.279:1:1, mm.31-38: original phrase in the test piece. In order to produce ﬁnal tempo and
dynamics curve plus the second-order polynomial shapes giv- dynamics curves, the shapes predicted for phrases at differ-
ing the best ﬁt at four levels of phrase structure.   ent levels must be combined. This is simply the inverse of the
                                                      curve decomposition problem: Given a new piece, the system
                                                      starts with an initial ‘ﬂat’ expression curve (i.e., a list of 1.0
lationships between successive phrases can be naturally ex- values) and then successively multiplies the current values by
pressed in FOL, as a relational predicate succeeds(Id2,Id1), the multi-level phrase predictions.
which simply states that the phrase Id2 succeeds the same-
level phrase Id1. Supplying the same information in a propo- 3.3 DISTALL, a Relational Instance-based
sitional representation would be very difﬁcult.            Learner
  What is still needed in order to learn are the training exam- We approach phrase-shape prediction with a straightforward
ples, i.e. for each phrase in the training set, we need to know nearest-neighbour (NN, IBL) method. Standard propositional
how it was played by a musician. This information is given k-NN is not applicable to our data representation discussed in
in the predicate phrShape(Id,Coeffs), where Coeffs encode in- section 3.1. Instead, we use DISTALL, an algorithm that gen-
formation about the way the phrase was played by a pianist. eralizes propositional k-NN to examples described in ﬁrst-
This is computed from the tempo and dynamics curves, as order logic [Tobudic and Widmer, 2005].
described in the following section.                     DISTALL  is a representative of the line of research ﬁrst
                                                      initiated in [Bisson, 1992], where a clustering algorithm to-
3.2  Deriving the Training Instances: Multilevel      gether with its similarity measure was presented. This work
     Decomposition  of Performance Curves             was later improved in [Emde and Wettschereck, 1996], in
Given a complex tempo or dynamics curve and the under- the context of the relational instance-based learning algorithm
lying phrase structure (see Figure 1), we need to calculate RIBL, which in turn can be regarded as DISTALL’s predeces-
the most likely contribution of each phrase to the overall ob- sor. We skip technical details here, but the main idea behind
served expression curve, i.e., we need to decompose the com- DISTALL’s similarity measure is that the similarity between
plex curve into basic expressive phrase ‘shapes’. As approxi- two objects depends not only on the similarities of their at-
mation functions to represent these shapes we decided to use tributes, but also on the similarities of the objects related to
the class of second-degree polynomials (i.e., functions of the them. The similarities of the related objects depend in turn
           2
form y = ax  + bx + c), because there is ample evidence on their attributes and related objects. For our music learning
from research in musicology that high-level tempo and dy- task it means that the ‘shaping’ of the current (test) phrase de-
namics are well characterized by quadratic or parabolic func- pends not only on its attributes, but also on the preceding and
tions [Todd, 1992]. Decomposing a given expression curve succeeding music (through the relation succeeds(Id1,Id2), see
is an iterative process, where each step deals with a speciﬁc section 3.1), which is – from a musical point of view – a rather
level of the phrase structure: for each phrase at a given level, intuitive idea. For a more detailed description of DISTALL
we compute the polynomial that best ﬁts the part of the curve see [Tobudic and Widmer, 2005].
that corresponds to this phrase, and ‘subtract’ the tempo or Experimental results with DISTALL on MIDI-like (i.e.,
dynamics deviations ‘explained’ by the approximation. The very detailed) performance data produced by a local pianist
curve that remains after this subtraction is then used in the are reported in [Tobudic and Widmer, 2005]. The new contri-
next level of the process. We start with the highest given level bution of the current paper is that we have laboriously mea-
of phrasing and move to the lowest. As tempo and dynamics sured audio recordings by truly famous artists and can show
curves are lists of multiplicative factors (relative to a default — for the ﬁrst time — that DISTALL actually succeeds in
tempo), ‘subtracting’ the effects predicted by a ﬁtted curve capturing something of personal artistic performance style.
from an existing curve simply means dividing the y values on
the curve by the respective values of the approximation curve. 4 Experiments
  Figure 2 illustrates the result of the decomposition process
on the last part (mm.31–38) of the Mozart Sonata K.279, 1st 4.1 Learning Predictive Performance Models
movement, 1st section. The four-level phrase structure our For each pianist, we conducted a systematic leave-one-piece-
music analyst assigned to the piece is indicated by the four out cross-validation experiment: each of 15 pieces was onceTable 3: Results of piecewise cross-validation experiment. Table 4: Detailed results (tempo dimension) of the cross-
The table cells list correlations between learned and real validation experiment with Mitsuko Uchida was the ‘training’
curves, where rows indicate the ‘training pianist’, and pianist. For each piece, the correlations between predicted
columns the pianist whose real performance curves are used curve and actual tempo curves from all pianists are given.
for comparison. The correlations are averaged over all pieces, The average over all pieces is given in the last row (repro-
weighted by the relative length of the piece. Each cell is fur- duced from the last row of Table 3)
ther divided into two rows corresponding to dynamics and      Piece   DB   RB   GG   MP    AS  MU
tempo correlations, respectively. The highest correlations in kv279:1:1 .36 .31 .22  .36   .29  .50
each row are printed in bold.                               kv279:1:2 .31  .35  .35  .28   .34  .47
                         compared with                      kv280:1:1 .40  .25  .26  .55   .42  .78
     learned from DB  RB   GG   MP   AS   MU                kv280:1:2 .42  .37  .42  .37   .47  .54
        DB       .44  .21  .26  .34  .38  .28               kv280:2:1 .53  .34  .52  .58   .59  .69
                 .44  .27  .26  .32  .31  .31               kv280:2:2 .44  .13  .39  .42   .50  .58
        RB       .21  .32  .09  .19  .19  .17               kv280:3:1 .72  .66  .62  .82   .68  .77
                 .28  .42  .20  .22  .30  .27               kv280:3:2 .51  .52  .44  .49   .43  .51
        GG       .25  .09  .36  .19  .21  .22               kv282:1:1 .38  .45  .36  .44   .44  .72
                 .25  .18  .32  .23  .29  .28               kv282:1:2 .23  .36  .33  .42   .46  .52
        MP       .33  .19  .19  .39  .33  .28               kv283:1:1 .21  .10  .12  .16   .22  .31
                 .31  .23  .27  .38  .28  .34               kv283:1:2 .17  .18  .21  .23   .22  .32
        AS       .36  .17  .20  .31  .40  .26               kv283:3:1 .30  .28  .42  .42   .42  .52
                 .32  .29  .28  .25  .41  .32               kv283:3:2 .19  .15  .29  .21   .32  .36
        MU       .27  .18  .21  .28  .26  .38                kv332:2  .32  .17  .14  .22   .16  .35
                 .34  .30  .32  .36  .37  .50                 Total   .34  .30  .32  .36   .37  .50

left aside as a test piece while the remaining 14 performances
(by the same pianist) were used for learning. DISTALL’s pa- music per artist). Moreover, the correlation estimates in Ta-
rameter for the number of nearest neighbors was set to 1, and ble 3 are somewhat unfair, since we compare the performance
the parameter for the depth of starting clauses (see [Tobudic curve produced by composing the polynomials predicted by
and Widmer, 2005]) to 4 (meaning that the distance between the learner, with the curve corresponding to the pianists’ ac-
two phrases can be inﬂuenced by at most 4 preceding and 4 tual performances. However, what DISTALL learned from
succeeding phrases).                                  was not the actual performance curves, but an approximation
  The expressive shapes for each phrase in a test piece are curve which is implied by the three levels of quadratic func-
predicted by DISTALL and then combined into a ﬁnal tempo tions that were used as training examples. Correctly predict-
and dynamics curve, as described in section 3.2. The result- ing these is the best the learner could hope to achieve.
ing curves are then compared to the real performance curves Table 4 shows a more detailed picture of the cross-
of all pianists (for the same test piece). If the curve learned validation experiment, where the training pianist was Mitsuko
from the performances of one pianist is more similar to the Uchida and the numbers refer to correlations in the tempo
real performance curve of the ‘teacher’ pianist than to those domain. In 13 out of 15 cases the learner produces tempo
of all other pianists, we could conclude that the learner suc- curves which are closer to Uchida’s playing than to any other
ceeded in capturing something of the pianist’s speciﬁc play- pianist, with correlations of .7 and better (e.g., for kv280:3:1
ing style. The described procedure is repeated for all pieces and kv280:1:1). The results are even more interesting if we
and all pianists in our data set.                     recall that the learner is given a very crude, beat-level repre-
  Correlation is chosen as a measure of how well the pre- sentation of the tempo and dynamics applied by the pianist,
dicted curve ‘follows’ the real one. The curves are ﬁrst nor- without any details about e.g. individual voices or timing de-
malized so that their autocorrelations are identically 1, giv- tails below the level of beat. On the other hand, the piecewise
ing a correlation estimate between curves as a number in the results revealed that some of the pianists (e.g. Gould or Pires)
range [-1,1]. The results of the cross-validation experiment seem to be less ‘predictable’ with our approach than Uchida
averaged over all pieces (weighted by the relative length of (not reported here for lack of space).
the pieces) are given in Table 3.                       Figure 3 shows an example of successful performance style
  Interestingly, the system succeeded in learning curves that learning. We see a passage from a Mozart piano sonata as
are substantially closer to the ‘trainer’ than all others, for all ‘played’ by the computer after learning from recordings of
pianists. Some of the pianists are better ‘predictable’ than other pieces by Daniel Barenboim (top) and Mitsuko Uchida
others, e.g. Daniel Barenboim and Mitsuko Uchida, which (bottom), respectively. Also shown are the performance
might indicate that they play Mozart in a more ‘consistent’ curves corresponding to these two pianists’ actual perfor-
way. While at ﬁrst sight the correlations may not seem im- mances of the test piece. In this case it is quite clearly visible
pressive, one should keep in mind that artistic performance that the curves predicted by the computer on the test piece are
is far from predictable, and the numbers in Table 3 are aver- much more similar to the curves by the respective ‘teacher’
ages over all pieces (about half an hour of concert-level piano than to those by the other pianist.     2
         D. Barenboim
         M. Uchida                                    Table 5: Confusion matrix of the pianist classiﬁcation exper-
    1.5  learned (Barenboim)
                                                      iment. Rows correspond to the test performances of each pi-
     1
                                                      anist (15 per row), columns to the classiﬁcations made by the
   rel.  dynamics 0.5                                 system. The rightmost column gives the accuracy achieved
     0
     120  125  130  135  140  145   150  155  160     for all performances of the respective. The baseline accuracy
                         bars                         in this 6-class problem is 16.67%.
    1.4
         D. Barenboim
         M. Uchida                                                         prediction
    1.2
         learned (Barenboim)                             pianist DB  RB   GG    MP   AS   MU   Acc.[%]
     1

   tempo                                                  DB     11   0    0     2    2    0     73.3
    0.8                                                   RB     1    12   1     0    0    1     80.0

    0.6                                                   GG     1    1    10    0    0    3     66.7
     120  125  130  135  140  145   150  155  160
                         bars                             MP     0    0    1    12    0    2     80.0
     2                                                    AS     1    0    2     0   10    2     66.7
         D. Barenboim
         M. Uchida                                        MU     0    0    1     0    0   14     93.3
    1.5  learned (Uchida)

     1                                                   Total   -    -    -     -    -    -     76.7


   rel.  dynamics 0.5

     0
     120  125  130  135  140  145   150  155  160     algorithms are applied to these. In [Saunders et al., 2004], the
                         bars

    1.4                                               sequential nature of music is addressed by representing per-
         D. Barenboim
         M. Uchida                                    formances as strings and using string kernels in conjunction
    1.2  learned (Uchida)                             with kernel partial least squares and support vector machines.
     1


   tempo                                              The string kernel approach is shown to achieve better perfor-
    0.8                                               mance than the best results obtained in [Widmer and Zanon,

    0.6                                               2004]. A clear result from both works is that identiﬁcation of
     120  125  130  135  140  145   150  155  160
                         bars                         pianists from their recordings is an extremely difﬁcult task.
                                                        The pianists studied in the present paper are identical
Figure 3: Dynamics and tempo curves produced by DISTALL to those in [Widmer and Zanon, 2004] and [Saunders et
on test piece (Sonata K.283, 3rd mvt., 2nd section, mm.120– al., 2004]; unfortunately, the sets of recordings differ con-
160) after learning from Daniel Barenboim (top) and Mitsuko siderably (because manual phrase structure analyses, which
Uchida (bottom), compared to the artists’ real curves as mea- are needed in our approach, were available only for certain
sured from the recordings.                            pieces), so a direct comparison of the results is impossible.
                                                      Still, to illustrate what can be achieved with a relational rep-
                                                      resentation and learning algorithm, we brieﬂy describe a clas-
  Admittedly, this is a carefully selected example, one of the siﬁcation experiment with DISTALL.
clearest cases of style replication we could ﬁnd in our data. Each of the 15 pieces is set aside once. The 6 performances
The purpose of this example is more to give an indication of of that piece (one by each pianist) are used as test instances.
the complexity of the curve prediction task and the difference A model of each pianist is built from his/her performances of
between different artists’ interpretations than to suggest that a the remaining 14 pieces. The result is two predicted curves
machine will always be able to achieve this level of prediction per pianist for the test piece (for tempo and dynamics), which
performance.                                          we call model curves. The ﬁnal classiﬁcation of a pianist,
                                                      represented by his/her tempo and dynamics curves tt and td
4.2  Identiﬁcation of Great Pianists                  on the test piece, is then determined as
The primary goal of our work is learning predictive mod-
els of pianists’ expressive performances.2 But the models
                                                                             corr(tt, mpt) + corr(td, mpd)
can also be used in a straightforward way for recognizing pi- c(t , t ) = argmax ∈ (                   )
                                                          t d           p P              2
anists. The problem of identifying famous pianists from in-                                           (1)
formation obtained from audio recordings of their playing has where P is set of all pianists and m and m are the pi-
                                [                                                     pt      pd
been addressed in the recent literature Saunders et al., 2004; anists’ model tempo and dynamics curves. In other words, the
                                               ]
Stamatatos and Widmer, 2002; Widmer and Zanon, 2004 . In performance is classiﬁed as belonging to the pianist whose
[                     ]
Widmer  and Zanon, 2004 , a number of low-level scalar fea- model curves exhibit the highest correlation (averaged over
tures related to expressive timing and dynamics are extracted tempo and dynamics) with the test curves. For each pianist,
from the audio CD recordings, and various machine learning DISTALL is tested on the 15 test pieces, which gives a to-
                                                      tal number of 90 test performances. The baseline accuracy
  2Note that learned tempo and dynamics curves as produced by
our system can be used to build truly machine generated expressive – the success rate of pure guessing – is 15, or 16.67%. The
performances: using the predicted tempo and dynamics curves (i.e. confusion matrix of the experiment is given in Table 5.
relative tempo and dynamics for each beat in the piece), it is straight- Again, it turns out that the artists are identiﬁable to varying
forward to calculate tempo and dynamics for each note in the piece degrees, but the recognition accuracies are all clearly above
(e.g. by interpolation).                              the baseline. In particular, note that the system correctly iden-