                           Qualitatively Faithful Quantitative Prediction 

                                   Dorian Sue, Daniel Vladusic, Ivan Bratko 
                    Faculty of Computer and Information Science, University of Ljubljana 
                                       Trzaska 25, 1000 Ljubljana, Slovenia 
                             {dorian.sue, ivan.bratko, daniel.vladusic}@fri.uni-lj.si 


                        Abstract                              tion has good chances to attain significant improvements over 
                                                              numerical system identification techniques, including tech•
     In this paper we describe a case study in which we       niques of numerical machine learning methods, such as re•
     applied an approach to qualitative machine learning      gression trees [Breiman et ai, 1984] and model trees [Quin-
     to induce, from system's behaviour data, a qual•         lan, 1992]. The potential improvements are in two respects: 
     itative model of a complex, industrially relevant        first, the predictions are qualitatively consistent with the prop•
     mechanical system (a car wheel suspension sys•           erties of the modelled system, and in addition they are also 
     tem). The induced qualitative model enables nice         numerically more accurate. 
     causal interpretation of the relations in the mod•
                                                                 This idea of combining qualitative and quantitative ma•
     elled system. Moreover, we also show that the 
                                                              chine learning for system identification is in this paper carried 
     qualitative model can be used to guide the quantita•
                                                              out in two stages. First, induce qualitative constraints from 
     tive modelling process leading to numerical predic•
                                                              system's behaviour data (training data) with program QUIN 
     tions that may be considerably more accurate than 
                                                              (overviewed in Section 3). Second, induce a numerical re•
     those obtained by state-of-the-art numerical mod•
                                                              gression function that both respects the qualitative constraints 
     elling methods. This idea of combining qualitative 
                                                              and fits well the training data numerically (called Qualitative 
     and quantitative machine learning for system iden•
                                                              to Quantitative transformation or Q2Q for short, described in 
     tification is in this paper carried out in two stages: 
                                                              Section 4). We call this approach Q  learning, which stands 
     (1) induction of qualitative constraints from sys•                                          2
                                                              for Qualitatively faithful Quantitative learning. To underline 
     tem's behaviour data, and (2) induction of a numer•
                                                              the importance of qualitative fidelity, we illustrate in Section 
     ical regression function that both respects the qual•
                                                              2 some problems that numerical learners typically have in re•
     itative constraints and fits the training data numer•
                                                              spect of qualitative consistency. In Section 5 we present the 
     ically. We call this approach Q  learning, which 
                                    2                         case study in applying Q  to the chosen problem of modelling 
     stands for Qualitatively faithful Quantitative learn•                           2
                                                              car suspension. 
     ing. 
                                                                 There are several approaches to learning qualitative models 
                                                              from numerical data that may support alternative approaches 
1 Introduction                                                to Q2 learning. These include the program QMN [Dzeroski 
                                                              and Todorovski, 1995], QSI [Say and Kuru, 1996], SQUID 
It is generally accepted that qualitative models are easier to 
                                                              [Kay et al, 2000], and QOPH [Coghill et al, 2002]. 
understand and reason about than quantitative models. Qual•
itative models thus provide a better basis for the explanation 
of phenomena in a modelled system than numerical models.      2 Qualitative difficulties of numerical learning 
In this paper we describe a case study in which we applied    Consider a simple container of cylindrical shape and a drain 
an approach to qualitative machine learning to induce, from   at the bottom. If we fill the container with water, the wa•
system's behaviour data, a qualitative model of a complex, in• ter drains out. Water level monotonically decreases, until it 
dustrially relevant mechanical system (a car wheel suspension reaches zero. Suppose we fill the container with water, and 
system). The induced qualitative model enables nice causal    measure initial outflow and the time behaviour 
interpretation of the relations among the variables in the sys• of water level . Since this is a rather simple behaviour, 
tem. This is precisely as one would expect from a qualitative one would naturally expect that machine learning techniques 
model. More surprisingly, however, we also show in this case  should be able to fairly well predict time behaviour of water 
study that the qualitative model can be used to guide the quan• level if enough learning examples are given. Quite surpris•
titative modelling process that may lead to numerical predic• ingly, even in such simple cases, the usual numerical pre•
tions that are considerably more accurate than those provided dictors can give strange and qualitatively unacceptable pre•
by state-of-the-art numerical modelling methods.              dictions. We illustrate the problems with a simple experi•
  Thus the main message of this paper is that a combination   ment, using well-known techniques for numerical prediction: 
of methods for qualitative and quantitative system identifica• model trees and locally weighted regression. 


1052                                                                                         QUALITATIVE REASONING                                                                Figure 2: A qualitative tree induced from a set of examples 
                                                               for the function Z = X2 - Y2. The rightmost leaf, applying 
                                                               when attributes A' and Y are positive, says that Z is strictly 
                                                               increasing in its dependence on A' and strictly decreasing in 
                                                               its dependence on}'. 
Figure 1: M5 predictions of water outflow: empty circles are 
M5 predictions of level on a test set with = 7.5. 
Other dots are the the learning examples. Note that M5 pre•    in advance or hidden in the data. Ignoring them results in 
dicts that water level increases at time                       clearly unrealistic predictions that a domain expert would not 
                                                               approve. The idea of this paper is to use such qualitative re•
                                                               lations, either given or induced from data, not only to avoid 
   In our experiment we used container outflow simulation 
                                                              qualitatively incorrect predictions, but also to improve the ac•
data to evaluate how different numerical predictors learn the 
                                                              curacy of numerical prediction. 
time behaviour of water level. The outflow from a container 
is where c is a parameter depending on the 
area ot the drain. For the simulation we used Euler integra•  3 Qualitative data mining with QUIN 
tion where seconds and                                        QU1N (Qualitative Induction) is a learning program that 
         We used six example sets, generated with different   looks for qualitative patterns in numerical data [Sue, 2001; 
initial water levels and initial outflows 6. Sue and Bratko, 2001; 2002]. Induction of the so-called qual•
An example set has 20 examples corresponding itative trees is similar to induction of decision trees. The dif•
to 19 seconds of simulation.                                   ference is that in decision trees the leaves are labelled with 
   We used the Weka [Witten and Frank, 2000J implemen•        class values, whereas in qualitative trees the leaves are la•
tations of locally weighted regression lAtkeson et al, 1997]  belled with what we call qualitatively constrained functions. 
(LWR, for short), and M5 regression and model trees [Quin-       Qualitatively constrained functions (QCFs for short) are a 
lan, 1992] to learn the time behaviour of level given the initial kind of monotonicity constraints that are widely used in the 
outflow, i.e. One example set was used as a                   field of qualitative reasoning. A simple example of QCF is: 
test set and other five sets (100 examples) for learning. When Y = A/+ (A"). This says that Y is a monotonically increasing 
the test set was the set with M5 with the default pa•         function of A. In general, QCFs can have more than one ar•
rameters induced a model tree with 9 leaves. Figure 1 shows   gument. For example, Z = M+,-(A, Y) says that Z mono•
the learning examples and the M5 prediction of level on tonically increases in A and decreases in Y. We say that 
a test set. Note that M5 predicts that water level increases at Z is positively related to X and negatively related to Y If 
time The same happens if we change the pruning pa•            both A and Y increase, then according to this constraint, Z 
rameter. This is of course qualitatively unacceptable as water may increase, decrease or stay unchanged. In such a case, a 
level can never increase. Also, there are no learning examples QCF cannot make an unambiguous prediction of the qualita•
where water level increases.                                  tive change in Z. 
   One might think that this is an isolated weird case or M5     QUIN takes as input a set of numerical examples and looks 
bug. But it is not. LWR makes a similar qualitative error     for qualitative patterns in the data. More precisely, QUIN 
on the test set with = 11.25, when it predicts that water     looks for regions in the data space where monotonicity con•
level increases at Of course, LWR predictions depend on       straints hold. Such a set of qualitative patterns are represented 
its parameter i.e. the number of neighbors used, but often    in terms of a qualitative tree. As in decision trees, the inter•
the appropriate that doesn't give qualitative errors on one   nal nodes in a qualitative tree specify conditions that split the 
container, gives qualitative errors when learning from similar attribute space into subspaces. In a qualitative tree, however, 
data but with different area of the drain. Often, the errors  each leaf specifies a QCF that holds among the input data that 
are even more obvious if we make predictions at the edges of  fall into that leaf. For example, consider a set of data points 
the space covered by learning examples, i.e. using as test set (Xy,Z) where Z = X2 - Y2 possibly with some noise 
    = 6.25 or = 12.5. As one would expect, regression         added. When QUIN is asked to find in these data qualitative 
trees make similar qualitative errors.                        constraints on Z as a function of X and Y, QUIN generates 
  We believe that other numerical predictors make similar     the qualitative tree shown in Figure 2. This tree partitions the 
qualitative errors, at least in more complex domains. This    data space into four regions that correspond to the four leaves 
might be quite acceptable in applications where we just want  of the tree. A different QCF applies in each of the leaves. The 
to minimize numerical prediction errors. But often it is also tree describes how Z qualitatively depends on A and Y. 
important to respect qualitative relations that are either given QUIN constructs a tree in the top-down greedy fashion, 


QUALITATIVE REASONING                                                                                              1053  similarly to decision tree induction algorithms. A more elab•
 orate description of the QUIN algorithm and its evaluation 
 on a set of artificial domains is given elsewhere [Sue, 2001; 
 Sue and Bratko, 2001 ]. QUIN has been applied to qualitative 
 reconstruction of human control strategies [Sue, 2001], and 
 to reverse engineer a complex industrial controller for gantry 
 cranes [Sue and Bratko, 2002]. 

 4 Q2Q transformation 

 In this section we describe the qualitative-to-quantitative 
 transformation (Q2Q for short). Given a set of numerical data Figure 3: Intec wheel model: wheel position is given by 
and a qualitative tree, Q2Q attempts to find a regression func•  and coordinates of the wheel center, and rotation angles 
 tion that fits the data well numerically, and also respects the about axes and called enforced wheel-spin angle 
qualitative constraints in the tree. We say that such a regres• camber and toe angle a, respectively. These are affected 
sion function is qualitatively consistent. In fact, Q2Q finds a by horizontal forces and elevation of the road R and 
qualitatively consistent regression function with good fit for rotational moment that act upon the tyre. 
each leaf in the tree separately. The overall regression func•
tion is then obtained by gluing together the regression func•
tions for the leaves.                                          5 Intec case study 
   The Q2Q procedure is as follows. First, we partition the    5.1 Intec wheel model 
learning examples according to the leaves of the qualitative 
                                                               In this section we present an application of Q2 learning to the 
tree. These subsets are then used for learning qualitatively   modelling of car wheel suspension system. This is a com•
consistent functions of the corresponding leaves. Let us focus plex mechanical system of industrial relevance. The model 
on learning a qualitatively consistent function for some par•  and simulation software used in this experiment were pro•
ticular leaf. Suppose we have a leaf with the qualitative con• vided by a German car simulation company Intec. The main 
straint We then have to find some monotonically                role of the application in this paper is to provide a controlled 
increasing function that fits the data well. One straightfor•
                                                               experiment to assess the potentials of Q2 learning on a mod•
ward solution, used in Q2Q, is to divide the range of variable elling problem of industrial complexity. However, although 
  with a number of equidistant points (i.e.                    the target model was already known and developing such a 
in which we learn from the given data the function values y.   model was not an issue of practical relevance, initially this 
The result is a set of pairs that defines                      case study was nevertheless motivated by a practical objec•
a piece-wise linear function which can be easily checked for   tive. Namely, the complexity of Intec's model is so high that 
compliance with the given qualitative constraint. This proce•  it cannot be simulated on the present simulation platform in 
dure can be generalized to qualitative constraints of any di•  real time. Therefore the practical objective of the application 
mension. 
                                                               of Q2 learning was to speed up the wheel simulation. The 
   In our implementation the function values yi were deter•    goal thus was to obtain a simplified wheel model that would 
mined with a standard version of locally weighted regression   still be sufficiently accurate and at the same time significantly 
(LWR) [Atkeson et ai, 1997], which used Gaussian weight•       simpler than the original model to allow real-time simulation. 
ing function. Therefore, the two parameters of the transfor•   Indeed, the simplified model obtained with Q2 is computa•
mation were the number of equidistant points per dimension     tionally trivial compared with the original model. 
                  and the kernel size of the Gaussian weight•    The Intec wheel model (shown in Figure 3) is a multi-body 
ing function All possible combina•                             model of a front wheel suspension built in compliance with 
tions of these two parameters (4*6=24) define the space of     the physical model assuming no car-body movement and no 
all candidate piece-wise linear functions for each leaf. These wheel-spin. In fact, the suspension system is modelled as if 
sets of candidate functions are exhaustively searched by Q2Q   the car-body is fixed. Wheel position is given by and 
to find functions that satisfy given qualitative constraints. For coordinates of the wheel center. Because the flexible joints in 
each leaf, Q2Q selects among these qualitatively consistent    multi-body suspension system that links the wheel to the car-
piece-wise linear functions one that has best fit with the data body allow several levels of displacements, rotation angles 
that fall into this leaf. Quality of the fit is measured with root about axes and are also measured. These are called 
mean squared error, RMSE for short. It is theoretically pos•   enforced wheel-spin angle camber and toe angle re•
sible that none of the candidate functions is qualitatively con• spectively. 
sistent with the QCF in the leaf. In such a case the Q2Q pro•    The multi-body simulation software Simpack [Intec, 2002] 
cedure would fail to find a qualitatively consistent regression was used to set up the model and to generate simulation 
function. Although in our experiments this never happened,     traces. During simulation, a number of elements are act•
we are working on an improved version of Q2Q that is guar•     ing upon the tyre: two horizontal forces and vertical 
anteed to find a qualitatively consistent regression function. movement (measured as elevation of the road R) and rota-


1054                                                                                          QUALITATIVE REASONING                                                                Figure 5: Induced qualitative tree for wheel center position. 

                                                               test traces have the same road profile as the traces used for 
                                                               learning, but different profiles of other three input variables, 
                                                               i.e. In the first trace all of the three input 
                                                               variables change similar as in the trace in Figure 4. This 
                                                               trace was recommended as the most critical test trace by the 
                                                               domain expert, who considered it far more difficult (all 4 in•
                                                               put variables change) than other 6 test traces where one or 
Figure 4: A typical simulation trace of the Intec wheel model: two input variables were always zero. 
the input variables are on the upper graph, the output variables 
                                                               5.3 Inducing a qualitative wheel model with QUIN 
(except that changes the same as road R) are on the lower 
graph, axis is time in steps dt=0.7 seconds. Note the com•     As described above, QUIN was used to induce a qualita•
plex behaviour of the output variables resulting from changes  tive tree for each of the six output variables, where the input 
                                                               variables were the attributes. All of the induced qualitative 
in Fx and road R. 
                                                               trees had over 99 % consistency on the learning set of ex•
                                                               amples. We say that a QCF is consistent with an example 
tional moment . For example, is acting upon the tyre           if the QCF's qualitative prediction of the dependent variable 
when braking, when driving through corners (centripetal        does not contradict the direction of change in the example. 
force) and rotational moment when parking the car.             The level of consistency of a qualitative tree with the exam•
   During the simulation, input and output variables are       ples is the percentage of the examples with which the tree 
logged to a file called simulation trace. We used traces of    is consistent. Consistency of 99% indicates that the induced 
wheel simulation with different trajectories of input vari•    qualitative model fits the data nearly perfectly. 
ables. Each trace lasted for 70 seconds, and was sampled         The simplest qualitative tree was induced for translation in 
with seconds. In this way a trace gives 100 examples,          the axis. This tree only has one leaf with QCF 
each example contains 10 values, corresponding to the values   This tree has a simple and obvious explanation. It says that 
of four input and six output variables at a given time. Figure changes in the direction of the road change. If road increases 
4 shows a typical simulation trace. It should be noted that all then z increases, i.e. the wheel center moves upwards. 
these traces correspond to very slow changes of input vari•      Qualitative trees for translations in and axes are a bit 
ables, and as a result the traces are illustrative mainly of the more complicated. Since they have similar explanations we 
kinematics of the mechanism, but not also of its dynamics.     will present just the qualitative tree for translations, given 
                                                               in Figure 5. Note that is measured in the opposite direction 
5.2 Details of experiments                                     as usual, i.e. positive means wheel center moving in the di•
The experiments reported in this paper were done using a       rection of car driving backwards. Both leaves of the tree have 
black-box approach. We did not use any knowledge of the        the same qualitative dependence on and but differ in 
model. The simulation traces were provided by our part•        qualitative dependence on road R. The qualitative tree says 
ners from Czech Technical University in the European project   that x is positively related to force that acts in the direc•
Clockwork.                                                     tion of Obviously, wheel center position changes (wheel 
   In all the experiments we used 7 traces for learning with the moves backward or forward) in the direction of force in 
same road profile as in the trace of Figure 4. In the first learn• direction. Second, is negatively related to force This 
ing trace all other three input variables were zero. In the next means that if we push the wheels together (we apply force in 
three traces two of the other three input variables were zero  the direction), the wheels will move forward position de•
and one other variable was changing. Figure                    creasing). This is not so obvious, but can be understood if we 
4 shows one such trace. The remaining three traces were sim•   consider the usual mechanics of wheel suspension. The qual•
ilar, but the trajectory of the changing variable was different, itative dependence on road R is a bit more complicated. The 
i.e. it first increased, stayed unchanged for 20 seconds, and  qualitative tree of Figure 5 says that is negatively related to 
than slowly decreased to zero. Each trace gives 100 examples,  R when . Otherwiseis positively related to R, 
giving altogether 700 learning examples with 10 variables.     When R increases from its minimum to its maximum, will 
   The task was to learn each of the six output variables as a first decrease and than increase, i.e. the wheel center will first 
function of input variables. In this way we have six learning  move forward and than backward. 
problems, where an output variable is the class and the input    Rotations about axes and are measured by enforced 
variables are the attributes. For example, angle a was learned wheel-spin camber , and toe angle respectively. For 
                                                               enforced wheel-spin QUIN induced a simple one-leaf tree 
   The prediction accuracy was tested on 7 test traces. All the that says Note that changes in the 


QUALITATIVE REASONING                                                                                                1055       Figure 6: Induced qualitative tree for toe angle 

 direction of the tyre rotation when driving forward. Consider 
 for example the dependence of on force that is positive 
                                                               Figure 7: LWR, LWR with optimized parameters, M5 and 
 during braking. Since is negatively related to increasing 
                                                               Q  predictions of on the most critical test trace. With each 
    causes to decrease, i.e. during braking enforced wheel       2
                                                               method, a at time step (on x-axis) was predicted according 
 spin angle changes in the direction of the tyre rotation. For 
                                                               to the values of input variables at time in the test trace. 
 camber angle QU1N induced a qualitative tree that is similar 
 to qualitative trees for and translations. 
   The toe angle i.e. the rotation about -axis is effected by 
 all input variables and is the most complicated. The induced 
 tree is given in Figure 6. We will omit explanation of this 
 qualitative tree, since it requires understanding of the flexi•
 ble nature of the multi-body suspension system that links the 
 wheel to the car-body. 
   These qualitative trees give a good explanation of wheel 
 suspension system behaviour. Moreover, they provide a qual•
 itative model of wheel suspension system that enables quali•  Figure 8: Comparing accuracy (measured with RMSE) of 
 tative simulation. In this way, they enable to predict all pos• Q2 and LWR with optimized parameters: the left graph com•
 sible qualitative changes of output variables over an arbitrary pares RMSE on the most difficult test trace and the right 
 time interval given qualitative changes of all or some input  graph shows RMSE on the remaining test traces. 
 variables. This qualitative model also enables to improve nu•
 merical predictions.                                          methods learned from 7 learning traces (also used for learn•
                                                               ing of qualitative trees) and were tested against 7 test traces 
 5.4 Qualitative correctness of numerical predictors 
                                                               described in Section 5.2. The test results are divided in two 
 In this section we illustrate why Q2 learning may have an ad• groups. The first group consists of results on a single test 
 vantage over the usual numerical predictors. Figure 7 shows   trace. This trace was recommended as the most critical test 
   predicted with M5 model tree, LWR and LWR with opti•        trace by the domain expert, who considered it far more diffi•
 mized parameters, on the most critical test trace, where all  cult (all 4 input variables are changing simultaneously) than 
 the input variables are changing simultaneously. The figure   the 6 test traces in the second group. 
 shows that both M5 and LWR sometimes make large errors.         Learning of qualitatively consistent functions in the leaves 
Moreover these errors are not only numerical, but also qual•   was performed as described in Section 4. The best fitting re•
itative. Consider for example the LWR predictions at the be•   gression functions were then taken and glued together into 
ginning of the trace. Here the predicted is decreasing, but    the overall induced numerical model. We compared the ac•
the correct is increasing. This error could be avoided by      curacy of our Q2 method with LWR and M5. The parame•
considering the induced qualitative tree for given in Fig•     ters of LWR were optimized for each output variable 
ure 6. Since at the beginning of the test trace road R is near ...) according to the RMSE criterion, through internal cross-
zero and increasing, and all other input variables are zero, the validation on the training set. When experimenting with M5, 
second leftmost leaf of the qualitative tree applies. Its QCF  we noticed that it was grossly inferior both in terms of qual•
                        requires increasing a since road R is  itative acceptability as well as numerical error. Attempts at 
increasing.                                                    optimizing M5's parameters did not help noticeably. 
   As can be observed in Figure 7, M5 and LWR often make         Figure 8 gives the prediction accuracy for variables 
qualitative errors. Q2 predictions are qualitatively correct.    and , The predictions of the remaining variables and 
The use of (induced) qualitative model enables Q2 to better    were not much affected by qualitative constraints. The results 
generalize in the areas sparsely covered by the training exam• on the most difficult test trace (left graph in Figure 8) show 
ples, resulting in better numerical accuracy.                  that even our simple Q2Q method improves the numerical 
                                                               prediction on all the variables (compared to LWR). Results 
5.5 Numerical accuracy of the induced models                   on the second test trace group are given on the right graph in 
Here we compare the numerical accuracy of the Weka imple•      Figure 8. As these traces were more similar to the learning 
mentations of LWR, M5 model trees and Q2 learning. All the     traces, the improvement of Q2 over LWR is smaller. 


1056                                                                                         QUALITATIVE REASONING 