           Mediating Between Qualitative and Quantitative Representations
                      for Task-Orientated Human-Robot Interaction
                     Michael Brenner                                Nick Hawes
                Albert-Ludwigs-University                   School of Computer Science
                    Freiburg, Germany                      University of Birmingham, UK
            brenner@informatik.uni-freiburg.de               N.A.Hawes@cs.bham.ac.uk

                      John Kelleher                                Jeremy Wyatt
              Dublin Institute of Technology                School of Computer Science
                      Dublin, Ireland                      University of Birmingham, UK
                John.Kelleher@comp.dit.ie                     J.L.Wyatt@cs.bham.ac.uk

                    Abstract                          as, “put the blue cube near the red cube” and “put the red
                                                      cubes and the green balls to the right of the blue ball”.
    In human-robot interaction (HRI) it is essential that
                                                        We are particularly interested in the consistent interpre-
    the robot interprets and reacts to a human’s utter-
                                                      tation and use of spatial relations throughout the modalities
    ances in a manner that reﬂects their intended mean-
                                                      available to a robot (e.g. vision, language, planning, manip-
    ing. In this paper we present a collection of novel
                                                      ulation). For their different purposes, these modalities use
    techniques that allow a robot to interpret and ex-
                                                      vastly different representations, and an integrated system
    ecute spoken commands describing manipulation
                                                      must be able to maintain consistent mappings between them.
    goals involving qualitative spatial constraints (e.g.
                                                      This is a hard problem because it means mediating between
    “put the red ball near the blue cube”). The result-
                                                      the quantitative information about objects available from
    ing implemented system integrates computer vi-
                                                      vision (e.g. where they are in the world), the qualitative
    sion, potential ﬁeld models of spatial relationships,
                                                      information available from language (e.g. descriptions
    and action planning to mediate between the contin-
                                                      of objects including spatial prepositions), the qualitative
    uous real world, and discrete, qualitative represen-
                                                      information that must be generated to reason about actions
    tations used for symbolic reasoning.
                                                      (e.g. hypothetical future conﬁgurations of objects), and
                                                      the quantitative information required by an action system
1  Introduction                                       in order manipulate objects. Additionally, when a robot
                                                      interacts with humans, mediation capabilities must extend
For a robot to be able to display intelligent behaviour when across system borders: the robot must be able to interpret
interacting with humans, it is important that it can reason the intended meaning of human input in terms of its own
qualitatively about the current state of the world and possible representational capabilities and react in a way that reﬂects
future states. Being an embodied cognitive system, a robot the human’s intentions. Our system makes the following
must also interact with the continuous real world and there- contributions in order to tackle these problems:
fore must link its qualitative representations to perceptions
and actions in continuous space. In this paper, we present an i) Planning-operator driven interpretation of commands:
implemented robot system that mediates between continuous we describe a generic method which uses formal planning
and qualitative representations of its perceptions and actions. operators to guide the interpretation of commands in natural
  To give an impression of the robot’s capabilities, consider a language and automatically generates formal planning goals.
hypothetical household service robot which is able to accept Referential expressions in the goal are kept for “lazy” reso-
an order to lay the dinner table such as “put the knives to the lution by the planner in the context of a given world state.
right of the plate and the forks to the left of the plate.” The This allows replanning to dynamically adapt behaviour with-
robot has to interpret this utterance and understand it as a goal out having to re-evaluate commands.
it must achieve. It has to analyse its camera input to ﬁnd the ii) Spatial model application: we use potential ﬁeld models
objects referred to in the owner’s command and it must also of spatial prepositions to generate qualitative representations
interpret the spatial expressions in the command in terms of of goals that satisfy a human command. This model accords
the camera input. Finally, it must plan appropriate actions with the ways that humans talk about spatial relations
to achieve the goal and execute that plan in the real world. [Costello and Kelleher, 2006]. This approach allows us to
In this paper, we present a system that is able to accomplish generate discrete solutions that ﬁt typical human descriptions
such tasks. In our domain we use cubes and balls in place of of continuous space.
cutlery as our robot’s manipulative abilities are limited (see
Figure 7). In this domain our system acts on commands such Although the individual techniques are still somewhat limited

                                                IJCAI-07
                                                  2072                                                      (:action put
                                                       :parameters
                                                          (?a - agent ?obj - movable ?wp - waypoint)
                                                       :precondition (and
                                                          (pos ?obj : ?a)
                                                          (not (exists (?obj2 - movable) (pos ?obj2 : ?wp))))
                                                       :effect
                                                          (pos ?obj : ?wp))

           Figure 1: The system architecture.
                                                           Figure 2: MAPL operator put for placing objects.

in scope, by combining them we provide each component in
the architecture access to more information than it would have seen in Figure 1. Our system is based on a combination of an
in isolation. Thus, the overall system is able to demonstrate iRobot B21r mobile robot and a table-mounted Katana 6M
intelligent behaviour greater than the sum of its parts. robotic arm. Mounted on the B21r is a pan-tilt unit support-
  In the following section we describe our robot platform and ing two parallel cameras which we use for visual input. From
the system architecture. We then expand on this in sections these cameras we create a 3D representation of the scene us-
§4 (planning domain), §5 (command interpretation), §6 (po- ing depth information from stereo to instantiate a collection
tential ﬁeld models for spatial relations) and §7 (qualitative of simple object models. To produce actions, information
spatial information from vision). Finally §8 presents an ex- from vision is fed into a workspace-based visual-servoing
ample of the functionality of the complete system.    system along with instructions about which object to grasp,
                                                      and where to put it. Actions are limited to pick and place.
2  Relation to Previous Work                          This sufﬁces for the current experimental scenarios.
The work presented in this paper is related to various sub-
ﬁelds of robotics and artiﬁcial intelligence. In particular it is
closely related to human-robot interaction and situated lan- 4 Planning Domain
guage understanding [Kruijff et al., 2006]. We do not fo-
cus solely on the process of understanding an utterance, but For the purpose of this paper, a simple ontology was designed
instead examine the steps necessary to mediate between the which consists mainly of agents and objects. Objects may be
various representations that can exist in systems that must act movable or not. They can have properties, e.g. colours, that
on the world as a result of a command in natural language. can be used to describe them or constrain subgroups of ob-
  In terms of gross functionality there are few directly com- jects in a scene. Positions of objects in a scene are described
parable systems, e.g. those presented in [Mavridis and Roy, by waypoints. Concrete instances of waypoints are generated
2006] and [Mcguire et al., 2002]. Whereas these systems on-the-ﬂy during the problem-solving process (cf. §7). Re-
specify complete architectures for following manipulation lations between waypoints include near, right of, and left of.
commands, we focus on a particular aspect of this behaviour. Despite being quite simple, this ontology allows us to repre-
As such our approach could be utilised by existing systems. sent complex situations and goals. Moreover, it is very easy
For example, it could be used in layer L3 of Mavridis and to extend to richer domains. For example, adding just one
Roy’s Grounded Situation Model [2006] to produced dis- new subtype of movable objects would enable the robot to
crete, categorical encodings of spatial relationships. distinguish between objects that are stackable.
  There are many plan-based dialogue systems that are used The ontology has been modelled as a planning domain in
(or potentially usable) for HRI (e.g., [Sidner et al., 2003; MAPL [Brenner, 2005], a planning language for multiagent
Allen et al., 2001]). Most such systems try to exploit the con- environments based on PDDL [Fox and Long, 2003]. MAPL
text of the current (dialogue) plan to interpret utterances. We is suitable for planning in HRI because it allows us to model
are not aware, however, of any system that, like ours, actu- the beliefs and mutual beliefs of agents, sensory actions, com-
ally uses the formal action representation from the planning municative actions, and different forms of concurrency. Al-
domain to resolve referential expressions in at least a semi- though these features make MAPL highly suitable for human-
formal way. Critically, the “guidance” provided by the plan- robot interaction, in this paper we mostly use the ADL sub-
ning domain leads to a logical representation of the command set of MAPL. Figure 2 shows the operator for placing ob-
that the planner can reason about. For example, the planner jects. Note that MAPL uses non-boolean state variables, e.g.
is able resolve referential expressions as part of the problem (pos obj), which are tested or changed with statements like
solving process. This can be signiﬁcant in dynamic environ- (pos obj : ?wp). Thus, in MAPL there is no need to state that
ments: if a situation changes then the planner can resolve the the robot no longer holds the object after putting it down (a
same referential expression differently.              statement which would be necessary in PDDL).
                                                        Currently, no planner is available that is speciﬁcally de-
3  Architecture                                       signed for MAPL. Instead, we use a compiler for transform-
To enable a robot to follow action commands such as those ing MAPL into PDDL and back. This enables us to use a
described in the introduction, we break the problem into a state-of-the-art planner in our system without losing the de-
number of processing steps. These steps are reﬂected by the scriptive power of MAPL; the planning system used currently
design of our overall processing architecture, which can be is FF [Hoffmann and Nebel, 2001].

                                                IJCAI-07
                                                  2073                                                      (forall (?obj - cube) (imply
5  Converting Linguistic Input to MAPL                  (and (initially (blue ?obj)))
                                                        (exists (?wp - waypoint)
In AI Planning, goals are typically formulated in (a subset of) (exists (?obj1 - ball ?wp1 - waypoint) (and
ﬁrst-order logic, i.e. as formulae that must hold in the state (initially (red ?obj1))
achieved by the plan (see, for example, the deﬁnition of goals (initially (pos ?obj1 : ?wp1))
                                                             (initially (left-of ?wp ?wp1))
in the ADL subset of PDDL [Fox and Long, 2003]). Humans,     (pos ?obj : ?wp))))))
however, usually use imperative commands, like “clear the ta-
ble”, when communicating goals. One reason for verbalising
an action command instead of a goal description could be that Figure 3: Automatically generated MAPL goal for “put the
the former provides a very compact representation of the lat- blue cubes to the left of the red ball”
ter by means of its postconditions, i.e. the immediate changes
to the world caused by the action. Speaking in AI Planning cubes be put?) and to the initial state (the reference con-
terms, if the action “clear table” has an ADL effect saying that straints determining the objects). It is crucial for the planning
after its execution “there exists no object that is on the table”, representation to be able to model this difference, otherwise
the action name plus its parameters is a much simpler means contradictory problems may be generated. For example, the
to convey that goal than the effect formula. What complicates command “put down the object that you are holding” pro-
the matter is that, in contrast to AI planners, humans usually vides two constraints on the object’s position: that it is held
do not use unique names for objects, but refer to them in ex- by the robot now, but is on the ground after plan execution.
pressions that constrain the possible referents (i.e. “the red Therefore, MAPL supports referring back to the initial state
ball” instead of object17). Altogether, the “human way” in the goal description as shown in Figure 3. The facts that
to describe goals can be described as goal = action + param- must hold after execution of the plan are described by the ef-
eters + reference constraints.                        fect of the put action. In our example, this effect describes
  Deliberative agents that have ADL-like action representa- the new position of the object in question.
tions can exploit this goal description scheme when trying It is important to realise that the goal descriptions gener-
to understand a natural language command: after matching ated by this process still contain the referential expressions
the verb phrase of a command with an appropriate planning from the original command, i.e. they are not compiled away
operator, this operator can be used to guide the further un- or resolved directly. Instead they will be resolved by the plan-
derstanding of the command, namely determining the action ner. We call this “lazy” reference resolution. It enables the
parameters and reference constraints.                 robot to dynamically re-evaluate its goals and plans in dy-
  We will illustrate this process with the command “put the namic situations. If, for example, another blue cube is added
blue cubes to the left of the red ball”. Our system parses to the scene, the planner will adapt to the changed situation
the command using a simple English grammar and a chart and move all of the blue blocks.
parser. The parse tree of the example command describes the
phrase as a verb, followed by a nominal phrase and a prepo- 6 Computational Models of Spatial Cognition
sitional phrase (V NP PP). When the system detects the verb
“put”, it is matched to the planning operator put (cf. Figure To act on the kinds of action commands we are interested in,
2). The subsequent interpretation procedure is speciﬁc to that the robot must be able to translate from the qualitative spatial
operator and aims at determining the constraints describing linguistic description of the location to place the object, to
the three parameters of the operator, ?a, ?obj and ?wp. This both a geometric description of the location that can be used
prior knowledge drives the interpretation of the phrase and by the manipulation system (i.e. a geometric waypoint posi-
simpliﬁes this process signiﬁcantly. In our example, the NP tioned in the robot’s world), and a logical description for the
is interpreted to describe the object ?obj that is to be moved planning domain (i.e. a symbolic representation of this way-
while the PP describes the target position ?wp. The follow- point and its relationships with other waypoints). This trans-
ing logical constraint on the parameters ?a, ?obj and ?wp is lation involves constructing computational geometric models
found (in which ?obj1 is the landmark1 object in relation to of the semantics of spatial terms.
which the goal position is described):                  Spatial reasoning is a complex activity that involves at least
 (blue ?obj) ∧(type ?obj : cube) ∧                    two levels of representation and reasoning: a geometric level
    ∃?wp1. ((left-of ?wp ?wp1) ∧∃?obj1. ((red ?obj1) ∧ where metric, topological, and projective properties are han-
       (type ?obj1 : ball) ∧ (pos ?obj1 : wp1)))      dled; and a functional level where the normal function of an
                                                      entity affects the spatial relationships attributed to it in a con-
Additionally, the interpretation states that all objects satisfy- text. In this paper we concentrate on the geometric level, al-
ing the constraints on ?obj must be moved. This quantiﬁca- though using functional spatial information would not require
tion becomes visible in the ﬁnal translation of the command any signiﬁcant changes to our overall system.
into a MAPL goal, shown in Figure 3 (where type constraints
                                                        Psycholinguistic research [Logan and Sadler, 1996; Regier
are transformed into the types of the quantiﬁed variables).
                                                      and Carlson, 2001; Costello and Kelleher, 2006] indicates
  One important aspect of the natural language command is
                                                      that people decide whether the spatial relation associated with
that it refers both to the goal state (where should the blue
                                                      a preposition holds between and landmark object and the re-
  1For the rest of the paper we will refer to the object or objects gions around it by overlaying a spatial template on the land-
that should be moved as the target, and the object or objects that are mark. A spatial template is a representation of the regions of
used to deﬁne the desired position of the target as the landmark. acceptability associated with a given preposition. It is centred

                                                IJCAI-07
                                                  2074on the landmark, and for each point in space it denotes the ac- for rel in goal relationships do
ceptability of the spatial relationship between it and the land- for wpl in waypoints do
                                                            initialise scene sc
mark. Figure 4 illustrates the spatial template for the prepo- add landmark wp to sc
sition “near” reported in [Logan and Sadler, 1996].                      l
                                                            for wpt to in waypoints − wpl do
                                                              add waypoints − wpl − wpt to s as distractors
                                                              compute ﬁeld pf for rel in s
            1.74 1.90 2.84 3.16 2.34 1.81 2.13                check value val of pf at wptarget
                                                              if val > 0 then
            2.61 3.84 4.66 4.97 4.90 3.56 3.26
                                                                add (rel wpt wpl) to state

            4.06 5.56 7.55 7.97 7.29 4.80 3.91
                                                       Figure 5: Algorithm for generating spatial relationship rel.
            4.47 5.91 8.52 O 7.90 6.13 4.46

            3.47 4.81 6.94 7.56 7.31 5.59 3.63        “left of”) for the planning domain. The second applies a ﬁeld

            3.25 4.03 4.50 4.78 4.41 3.47 3.10        to a single known waypoint to generate a new set of way-
                                                      points that all satisfy a predicate for the planning domain.
            1.84 2.23 2.03 3.06 2.53 2.13 2.00          The ﬁrst step in the process of interpreting and acting upon
                                                      a command is to translate the information directly obtain-
                                                      able from vision into our planning domain. This is done in
   Figure 4: Mean goodness ratings for the relation near. a straight-forward way. For each object in the world we gen-
                                                      erate a description in the language of the planning domain.
  If a computational model is going to accommodate the Each object is represented in the planning domain by an ID
gradation of a preposition’s spatial template it must de- which is stored to allow other process to index back into the
ﬁne the semantics of the preposition as some sort of con- geometric vision representation via the planning representa-
tinuum function. A potential ﬁeld model is one widely tion. Representing an object involves describing its colour
used form of continuum measure [Olivier and Tsujii, 1994; and type (information which is directly available from our vi-
Kelleher et al., 2006]. Using this approach, a spatial tem- sion system). To position the object in the world we must also
plate is built using a construction set of normalised equations place the object at a waypoint. To do this we add a waypoint
that for a given origin and point computes a value that repre- to the planning domain at the centre of the object’s bounding
sents the cost of accepting that point as the interpretation of box on the table. Waypoints are also represented by a stored
the preposition. Each equation used to construct the potential ID that can be used access its position in the real world, so
ﬁeld representation of a preposition’s spatial template models that later processes can use this information.
a different geometric constraint speciﬁed by the preposition’s
                                                        The second step in the process of generating an initial state
semantics. For example, for projective prepositions, such as
                                                      is to add information about the spatial relations of the way-
“to the right of”, an equation modelling the angular deviation
                                                      points to the planning problem. This allows the planner to
of a point from the idealised direction denoted by preposition
                                                      reason about how moving objects between waypoints changes
would be included in the construction set. The potential ﬁeld
                                                      their spatial relationships. Rather than add information about
is then built by assigning each point in the ﬁeld an overall po-
                                                      all of the spatial relationships that exist between all of the
tential by integrating the results computed for that by point by
                                                      waypoints, we focus only on the relationships included in the
each of the equations in the construction set. The point with
                                                      goal state because any additional information would be irrele-
the highest overall potential is then taken as the location that
                                                      vant to the current task. Thus our approach is explicitly task-
the object should be placed at to satisfy the relationship.
                                                      orientated. The algorithm we use is presented in Figure 5.
                                                      In this algorithm, distractors represent points in the potential
7  Qualitative Representations from Vision            ﬁeld that may inﬂuence the ﬁeld in some way (e.g reducing
The previous sections have discussed the representation we its value, or altering its extent).
use for planning, how we translate action commands into goal The ﬁnal step of the initial state generation process is to
states in this representation, and how we model spatial rela- add additional waypoints in order to give the planner enough
tionships. This section describes a process that produces an suitable object locations to produce a plan. This is neces-
initial state description for the a planning process by applying sary because the waypoints from the previous step are all ini-
these techniques to mediate between geometric visual infor- tially occupied, and may not satisfy the spatial constraints in
mation and the symbolic planning representation.      the goal description. To add waypoints, we ﬁrst ground the
  We break the task of generating a state description from target description (e.g. “the blue cubes” from “put the blue
vision and language into three steps: converting information cubes left of the red ball”) in the visual information to pro-
about visible objects into a symbolic representation, adding vide a count of the number of objects that must be moved.
information about speciﬁc spatial relationships to this repre- We then ﬁnd the waypoint for the landmark object (e.g. “the
sentation, and generating new information required by the red ball”), and generate the required number of waypoints in
planning process. These last two steps use potential ﬁeld the potential ﬁeld around the landmark for the given spatial
models in two different ways. The ﬁrst applies them to known relationship (e.g. “left of”). The algorithm we use to gen-
waypoints in the world to generate logical predicates (e.g. erate the new waypoints is presented in Figure 6. Because

                                                IJCAI-07
                                                  2075  initialise scene sc                                   (pos obj_d0 : wp_d1) (pos obj_d2 : wp_d3)
  add landmark wpl to sc                                (pos obj_d4 : wp_d5) (pos obj_d6 : wp_d7)
  add waypoints - targets - wpl to s as distractors     (pos obj_d8 : wp_d9)
  compute ﬁeld pf for rel in s                          (blue obj_d0) (blue obj_d2)
  for i =0to n do                                       (green obj_d4) (green obj_d6) (red obj_d8)
    get max of pf
            0                                           Next, the mapping process uses potential ﬁelds to generate
    if max > then                                     the spatial relationships between the waypoints for all of the
      add new waypoint at location of max
    else                                              visible objects. Only the relationships necessary to satisfy
      return failure                                  the goal state are considered, so in this case only the “left of”
                                                      relationship is considered. Part of this process is presented in
Figure 6: Algorithm for generating n new waypoints for Figure 8(b), which shows the “left of” ﬁeld for the top right
                                                      cube. In this picture the camera is positioned directly in front
targets at the spatial relationship rel around landmark wpl.
                                                      of the red ball (hence the ﬁeld being tilted). This results in
                                                      the following information being added to the initial state:

                                                      (left_of wp_d5 wp_d1) (left_of wp_d9 wp_d1)
                                                      (left_of wp_d5 wp_d3) (left_of wp_d7 wp_d3)
                                                      (left_of wp_d9 wp_d3) (left_of wp_d5 wp_d9)
                                                      (left_of wp_d7 wp_d9)
                                                        The next step is to generate new waypoints that can be used
                                                      to satisfy the goal state. This is done by grounding the land-
                                                      mark and target elements of the goal state in the information
                                                      from vision. The target group (“the blue cubes”) is grounded
     (a) The initial state. (b) The state after execution by counting how many of the visible objects match this de-
                                                      scription. Because there are two objects that match the colour
Figure 7: Images of the world before and after plan execution. and shape of the objects described by the human, two new
                                                      waypoints are generated at the speciﬁed spatial relationship
                                                      to the landmark group. The waypoint for the landmark object
this algorithm is greedy, it may fail to place the waypoints in is identiﬁed (in this case wp_d9), and then the new waypoints
the potential ﬁeld even if enough space is available. This is must be placed as dictated by the appropriate potential ﬁeld.
something that must be addressed in future work.      In this case a projective ﬁeld is generated around the red ball’s
                                                      waypoint, with the non-target objects (the green cubes) as dis-
8  Worked Example                                     tractors. This ﬁeld can be seen in Figure 8(c). The new way-
                                                      point positions are selected by picking the points in the ﬁeld
This section presents an example processing run from the im-
                                                      with the highest values (and inhibiting the area around the
plementation of our system. The initial scene for the exam-
                                                      points selected). This ﬁnal step is presented in Figure 8(d),
ple can be seen in Figure 7(a). A visualisation generated by
                                                      and results in the following extra information being added to
the system is presented in Figure 8(a). The scene contains
                                                      the mapping: wp_d10 (172,200), wp_d11 (156,200).
a red ball, two green cubes and two blue cubes. Processing
is started by the command “put the blue cubes to the left of To complete the planning problem, its initial state is ex-
the red ball”. This is passed into the linguistic processing tended with left_of propositions describing the spatial re-
component. This component processes the text as described lations of the newly generated empty waypoints to the already
in §5, which produces the MAPL goal state shown in Fig- occupied ones. Finally the FF planner is run, returning:
ure 3. The linguistic input triggers the current scene to be 0: pickup robot obj_d0 wp_d1
pulled from vision. This returns a scene with a red ball cen- 1: put robot obj_d0 wp_d11
      (200 200)             (150 150)    (150 250)      2: pickup robot obj_d2 wp_d3
tred at   ,   , green cubes at  ,     and    ,    ,     3: put robot obj_d2 wp_d10
and blue cubes at (250, 250) and (250, 150) (these numbers
have been adjusted for a simpler presentation).       Although the plan looks simple in this example, note that the
  The goal and visual information is then used as input into referential constraints in the goal description (cf. Figure 3) are
the discrete-continuous mediation process. As described in correctly resolved: the two blue blocks are picked up. Note
§7, this process assigns IDs for each object and a waypoint further that even this problem contains non-trivial causal con-
for each object position. This results in the following map- straints between actions to which the planner automatically
ping for the scene (the brackets contain information that is adheres: neither does it try to pick up several objects at once,
accessible from vision via the IDs):                  nor does it place several objects on the same waypoint.
                                                        Before plan execution, the plan must be updated to include
 obj_d0 (blue cube) at wp_d1 (250,250)
 obj_d2 (blue cube) at wp_d3 (250,150)                information about the current scene. This is done by query-
 obj_d4 (green cube) at wp_d5 (150,250)               ing the mediation process to determine the objects from vi-
 obj_d6 (green cube) at wp_d7 (150,150)               sion referred to by the object IDs. Using this information the
 obj_d8 (red ball) at wp_d9 (200,200)
                                                      manipulation system acts out the human’s command by pick-
  The qualitative part of this is transformed into a MAPL ing up each blue cube in turn and placing them at the points
expression to form part of the initial state for planning: indicated in Figure 8(d), resulting in the scene in Figure 7(b).

                                                IJCAI-07
                                                  2076