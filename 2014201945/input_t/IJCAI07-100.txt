                   A Convergent Solution to Tensor Subspace Learning

              Huan Wang1             Shuicheng Yan2,Thomas Huang2              Xiaoou Tang   1,3
          1 IE, Chinese University       2 ECE, University of Illinois     3 Microsoft Research Asia
          of Hong Kong, Hong Kong         at Urbana Champaign, USA             Beijing, China
          hwang5@ie.cuhk.edu.hk          {scyan,huang}@ifp.uiuc.edu           xitang@microsoft.com


                                                                      T             T
                                                                     k   p k       k   k k  1
                    Abstract                          arg maxU k Tr(U  Sk U )/T r(U  S  U )   to the Ratio
                                                                                kT  k  k −1  kT  p  k
    Recently, substantial efforts have been devoted to Trace form arg maxU k Tr[(U S U  )  (U   Sk U )]in
    the subspace learning techniques based on tensor  order to obtain a closed-form solution for each iteration. Con-
    representation, such as 2DLDA [Ye et al., 2004],  sequently, the derived projection matrices are unnecessary to
    DATER  [Yan et al., 2005] and Tensor Subspace     converge, which greatly limits the application of these algo-
    Analysis (TSA) [He et al., 2005]. In this context, rithms since it is unclear how to select the iteration number
    a vital yet unsolved problem is that the computa- and the solution is not optimal even in the local sense.
    tional convergency of these iterative algorithms is In this work, by following the graph embedding formula-
    not guaranteed. In this work, we present a novel so- tion for general dimensionality reduction proposed by [Yan
    lution procedure for general tensor-based subspace et al., 2007], we present a new solution procedure for sub-
    learning, followed by a detailed convergency proof space learning based on tensor representation. In each it-
    of the solution projection matrices and the objec- eration, instead of transforming the objective function into
    tive function value. Extensive experiments on real- the ratio trace form, we transform the trace ratio optimiza-
    world databases verify the high convergence speed tion problem into a trace difference optimization problem
                                                                 kT   p    k   k
    of the proposed procedure, as well as its superiority maxU k Tr[U (Sk −λS )U ] where λ is the objective func-
    in classiﬁcation capability over traditional solution                                 k n
                                                      tion value computed from the solution (U |k=1) of the pre-
    procedures.                                       vious iteration. Then, each iteration is efﬁciently solved with
                                                      the eigenvalue decomposition method [Fukunaga, 1991].A
1  Introduction                                       detailed proof is presented to justify that λ, namely the value
                                                      of the objective function, will increase monotonously, and
Subspace learning algorithms [Brand, 2003] such as Princi-                                k
                                                      also we prove that the projection matrix U will converge to
                            [                     ]
pal Component Analysis (PCA) Turk and Pentland, 1991  a ﬁxed point based on the point-to-set map theories [Hogan,
                                   [
and Linear Discriminant Analysis (LDA) Belhumeur et al., 1973].
1997] traditionally express the input data as vectors and of-
                                                        It is worthwhile to highlight some aspects of our solution
ten in a high-dimensional feature space. In real applications,
                                                      procedure to general subspace learning based on tensor rep-
the extracted features are usually in the form of a multidi-
                                                      resentation here:
mensional union, i.e. a tensor, and the vectorization process
destroys this intrinsic structure of the original tensor form. 1. The value of the objective function is guaranteed to
Another drawback brought by the vectorization process is the monotonously increase; and the multiple projection ma-
curse of dimensionality which may greatly degrade the algo- trices are proved to converge. These two properties en-
rithmic learnability especially in the small sample size cases. sure the algorithmic effectiveness and applicability.
  Recently substantial efforts have been devoted to the em-
ployment of tensor representation for improving algorith- 2. Only eigenvalue decomposition method is applied for
mic learnability [Vasilescu and Terzopoulos, 2003]. Among iterative optimization, which makes the algorithm ex-
them, 2DLDA  [Ye et al., 2004] and DATER [Yan et al.,     tremely efﬁcient; and the whole algorithm does not suf-
2005] are tensorized from the popular vector-based LDA    fer from the singularity problem that is often encoun-
algorithm. Although the initial objectives of these algo- tered by the traditional generalized eigenvalue decompo-
rithms are different, they all end up with solving a higher- sition method used to solve the ratio trace optimization
order optimization problem, and commonly iterative pro-   problem.
cedures were used to search for the solution. A collec- 3. The consequent advantage brought by the sound theo-
tive problem encountered by their solution procedures is  retical foundation is the enhanced potential classiﬁcation
that the iterative procedures are not guaranteed to con-
                                                         1        p     k
verge, since in each iteration, the optimization problem Matrices Sk and S are both positive semideﬁnite and more
is approximately simpliﬁed from the Trace Ratio form  detailed deﬁnitions are described afterward.

                                                IJCAI-07
                                                   629                                                           n
    capability of the derived low-dimensional representation f(Uk|k=1) may have two kinds of deﬁnitions. One is for scale
    from the subspace learning algorithms.            normalization, that is,
  The rest of this paper is organized as follows. Section II              N
                                                                   |n         X  ×   k|n  2
reviews the general subspace learning based on tensor rep-    f(Uk  k=1)=       i  k U  k=1  Bii,     (5)
resentation, and then we introduce our new solution proce-                i=1
dure along with the theoretical convergency proof in section where B is a diagonal matrix with non-negative elements.
III. By taking the Marginal Fisher Analysis (MFA) algorithm The other is a more general constraint which relies on a new
proposed in [Yan et al., 2007] as an example, we verify the graph, referred to as penalty graph with similarity matrix Sp,
convergency properties of the new proposed solution pro-
                                                      and is deﬁned as 
cedure and the classiﬁcation capability of the derived low-    k|n         X  − X   ×    k|n  2 p
dimensional representation is examined with a set of experi- f(U k=1)=     (  i    j)  k U k=1   Sij . (6)
ments on the real-world databases in Section IV.                       i=j
                                                        Without losing generality, we assume that the constraint
2  Subspace Learning with Tensor Data                 function is deﬁned with penalty matrix for simplicity; and for
                                                      scale normalization constraint, we can easily have the similar
In this section, we present a general subspace learning frame- deduction for our new solution procedure. Then, the gen-
work by encoding data as tensors of arbitrary order, extended eral formulation of the tensor-based subspace learning is ex-
from the one proposed by [Yan et al., 2007] and taking the pressed as
                                                                   
data inputs as vectors. The concepts of tensor inner produc-                            k n    2  p
                                                                         (Xi − Xj) ×k U |     S
tion, mode-k production with matrix, and mode-k unfolding          i=j                  k=1     ij
                                                           arg max                      k n    2   .  (7)
are referred to the work of [Yan et al., 2005].             U k |n        Xi − Xj  ×k   |       ij
                                                               k=1   i=j (       )    U  k=1   S
2.1  Graph Embedding with Tensor Representation         Recent studies [Shashua and Levin, 2001][Ye, 2005][Ye
                                                      et al., 2004][Yan et al., 2005] have shown that dimensional-
Denote the sample set as X = [X1, X2,...,XN ], Xi ∈
 m  ×m ×...×m                                         ity reduction algorithms with data encoded as high-order ten-
R  1  2      n ,i=1,...N, with N as the total number of
                                                      sors usually outperform those with data represented as vec-
samples. Let G = {X, S} be an undirected similarity graph,
                                                      tors, especially when the number of training samples is small.
called an intrinsic graph, with vertex set X and similarity ma-
         N×N                                          Representing images as 2D matrices instead of vectors allows
trix S ∈ R   . The corresponding diagonal matrix D and
                                                      correlations between both rows and columns to be exploited
the Laplacian matrix L of the graph G are deﬁned as
                                                     for subspace learning.
                                                        Generally, no closed-form solution exists for (7). Previ-
           L = D − S, Dii  =    Sij ∀ i.        (1)
                                                      ous works [Ye et al., 2004][Yan et al., 2005] utilized iter-
                             j=i
                                                      ative procedures to search for approximate solutions. First,
                                                                           1      n
The task of graph embedding is to determine a low-    the projection matrices U ,...,U are initialized arbitrarily;
                                     X                then each projection matrix U k is reﬁned by ﬁxing the other
dimensional representation of the vertex set that preserves              1      k−1   k+1      n
the similarities between pairs of data in the original high- projection matrices U ,...,U ,U ,...,U and solv-
dimensional feature space. Denote the low-dimensional em- ing the optimization problem:
                                                                       
bedding of the vertices as Y =[Y1, Y2,...,YN ],where                          kT  k −  kT  k2 p
       m ×m ×...×m                                      k∗            i=j U  Yi   U   Yj   Sij
Y  ∈    1   2     n                            X                       
  i  R              is the embedding for the vertex i,   U   =argmax            T        T            (8)
                                                                   U k        k   k −  k   k2
with the assumption that Yi is the mode-k production of Xi               i=j U  Yi   U   Yj   Sij
                                      k    m  ×m
with a series of column orthogonal matrices U ∈ R k k ,                    kT  p  k
                                                                      Tr(U    Sk U )
                                                            =argmax         T                         (9)
              1     2        n     kT  k                           U k     k   k  k
 Yi    Xi ×1    ×2      ×n                 m                         Tr(U    S U  )
    =        U     U  ...   U ,U     U  = I  k , (2)
                                                             k                                      ˜
                                                    where Yi is the mode-k unfolding matrix of the tensor Yi =
where Im is an mk-by-mk identity matrix. To maintain sim-    1          k−1       k+1         n     k
        k                                             Xi ×1 U ...×k−1 U     ×k+1 U    ...×n U and S  =
ilarities among vertex pairs according to the graph preserving   k    k    k    k T   p           p  k
                                                        i=j Sij (Yi − Yj )(Yi − Yj ) ,Sk =  i=j Sij (Yi −
criterion [Yan et al., 2007],wehave                     k   k    k T
                                                     Yj )(Yi − Yj ) .
                                        2
                              Yi − Yj  Sij            The optimization problem in (9) is still intractable, and tra-
       k ∗|n              i=j
    (U  ) k=1 =argmin            k n            (3)   ditionally its solution is approximated by transforming the
                 U k|n       f(U  |k=1)
                  k=1                                objective function in (9) into a more tractable approximate
                                 k n   2
             i=j  (Xi − Xj) ×k U |k=1  Sij         form, namely, Ratio Trace form,
  =argmin                 k n               ,   (4)         ∗                 T           T  p
     U k |n            f(U |k=1)                           k                 k  k  k −1  k      k
       k=1                                               U   =argmaxTr((U      S U  )  (U   Sk U ))  (10)
                                                                   U k
         k n
where f(U |k=1) is a function that poses extra constraint for which can be directly solved with the generalized eigenvalue
                                        k n
the graph similarity preserving criterion. Here U |k=1 means decomposition method. However, this distortion of the objec-
the sequence U 1, U 2 to U n and so for the other similar rep- tive function leads to the computational issues as detailed in
resentations in the following parts of this work. Commonly, the following subsection.

                                                IJCAI-07
                                                   6302.2  Computational Issues                             Algorithm 1 . Procedure to Tensor-based Subspace Learning
                                                                                1  2      n
As the objective function in each iteration is changed from the 1: Initialization. Initialize U0 ,U0 ,...,U0 as arbitrary col-
trace ratio form (9) to the ratio trace form (10), the deduced umn orthogonal matrices.
solution can satisfy neither of the two aspects: 1) the objective 2: Iterative optimization.
function value in (7) can monotonously increase; and 2) the For t=1, 2,...,Tmax,Do
solution (U 1,U2,...,Un) can converge to a ﬁxed point. In  For k=1, 2,...,n,Do
                                                                     
this work, we present a convergent solution procedure to the                         o k     o  n  2 p
                                                                          (Xi−Xj )×oU | -1 ×oU |   S
                                                                       i=j          t          o=k  ij
optimization problem deﬁned in (7).                        1. Set λ =                 o=1   t-1       .
                                                                          (X −X )× U o|k-1 × U o |n 2S
                                                                             i  j  o t     o    o k  ij
                                                                       i=j            o=1   t-1 =
3  Solution Procedure and Convergency Proof                            k     p
                                                           2. Compute S  and Sk as in (9) based on the projection
                                                                       1      k−1      k+1      n
In this section, we ﬁrst introduce our new solution procedure matrices Ut ,...,Ut and Ut−1 ,...,Ut−1.
to the tensor-based subspace learning problems, and then give 3. Conduct Eigenvalue Decomposition:
the convergency proof to the two aspects mentioned above.
                                                                     p     k                      
  As described above, there does not exist closed-form solu-       (Sk − λS )vj = λj vj,j=1,...,mk,
tion for the optimization problem (7), and we solve the op-
timization problem also in an iterative manner. For each it-  where vj is the eigenvector corresponding to the j-
eration, we reﬁne one projection matrix by ﬁxing the others   th largest eigenvalue λj .
and an efﬁcient method is proposed for this reﬁnement. In- 4. Reshape the projection directions for the sake of or-
stead of solving a ratio trace optimization problem (10) for an thogonal transformation invariance:
approximate solution, we transform the trace ratio optimiza-
                                                                           1  2      m
tion problem (9) into a trace difference optimization problem  (a) Set V =[v ,v ,...,v k ];
                                                                                  
deﬁned as                                                              v        T      k  kT     T
                                                              (b) Let S   =  VV  (  i Xi Xi  )VV  ,where
         k∗               kT   p     k  k                           k                                X
       U    =argmaxTr(U     (Sk − λS  )U ),    (11)               Xi is the mode-k unfolding of the tensor i;
                  U k
                                                               (c) Conduct Eigenvalue Decomposition as
where λ is the value of objective function (7) computed from                     v
the projection matrices of the previous iteration.                             S  ui = γi ui.        (12)
  Though the iterative procedure may converge to a local op-                                k
                                                           5. Set the column vectors of matrix Ut as the leading
timum for the optimization problem (7), it can monotonously                       k
                                                              eigenvectors, namely, Ut =[u1,u2,...,um ].
increase the objective function value as proved later, which                                       k
directly leads to its superiority over the ratio trace based opti- End     
                                                               k    k             
mization procedure, since the step-wise solution of the latter If Ut − Ut−1 < mkmk ε, k =1, 2,...,n(ε is set
is unnecessarily optimal for (9).                          to 10−4 in this work), then break.
  We iteratively reﬁne the projection matrices, and the de-
tailed solution procedure to solve the tensor-based general End
                                                                                       k   k
subspace learning problem is listed in Algorithm 1.    3: Output the projection matrices U =Ut ,k=1, 2,...,n.
3.1  Analysis of Monotonous Increase Property
                                                                     T
                                                      Moreover, from U U = Im , it is easy to prove that
Rewrite the objective function of (7) as                                     k
                                                                                 m
                                  k n   2 p                                       k
                  (Xi − Xj) ×k U  |    Sij
                                   k=1                                 sup g(U)=     λj .
       k|n     i=j
   G(U  k=1)=                     k n   2   ,  (13)                               j=1
                  (Xi − Xj) ×k U  |    Sij
                                   k=1                                                    
               i=j                                                                k      mk
                                                        From Algorithm 1, we have g(Ut )= j=1 λj , and hence
and then we have the theory as below:                                    k       k
                                                                      g(Ut ) ≥ g(Ut−1)=0.
  Theorem-1. By following the terms in Algorithm-1 and
Eqn. (13), we have                                                kT   p     k   k                k
                                                        Then, Tr(Ut  (Sk − λS )Ut ) ≥ 0.AsmatrixS  is pos-
     1      k−1   k    k+1      n                     itive semideﬁnite 2,wehave
 G(Ut ,...,Ut  ,Ut−1,Ut−1 ...,Ut−1) ≤
                                                                            kT p  k
                1       k−1  k   k+1      n                            Tr(Ut  S Ut )
            G(Ut ,...,Ut   ,Ut ,Ut−1 ...,Ut−1). (14)                           k     ≥ λ,
                                                                            kT k  k
                              p                                        Tr(Ut  S Ut )
  Proof. Denote g(U)=Tr(U T (S − λSk)U)  where
                              k                       that is,
              1      k−1   k    k+1      n                     o k−1  o  n          o k    o  n
      λ = G(Ut ,...,Ut  ,Ut−1,Ut−1 ...,Ut−1),              G(Ut |o=1 ,Ut−1|o=k) ≤ G(Ut |o=1,Ut−1|o=k+1)

                                                                                             T
                                                         2       k                          k  k  k
then we have                                             Though S may have zero eigenvalues, Tr(Ut S Ut ) will be
                       k                                           
                   g(Ut−1)=0.                         positive when mk is larger than the number of the zero eigenvalues.

                                                IJCAI-07
                                                   631           1.4                          9.5                          13
           1.3
                                         9                           12
           1.2
                                                                     11
                                        8.5
           1.1       Ours                        Ours                         Ours
                                                                     10       Traditional
            1        Traditional         8       Traditional
                                                                      9
           0.9
                                        7.5
           0.8                                                        8
                                         7
           0.7                                                        7
           Objective  Function Value
                                        Objective  Function Value
                                        6.5                          Objective  Function Value
            0  5  10 15 20 25 30 35 40   0  5  10 15 20 25 30 35 40   60 5  10 15 20 25 30 35 40
                  Iteration Number             Iteration Number             Iteration Number
                      (a)                          (b)                          (c)

Figure 1: The value of the objective function (7) vs. iteration number. (a) USPS database (b) ORL database, and (c) CMU PIE
database. Here the traditional method means the solution procedure based ratio trace optimization.

  From theorem-1, we can conclude that the value of the ob- hence the second condition is also satisﬁed and the Algo-
jective function monotonously increases.              rithm 1 is strictly monotonic.
                                                        Theorem-3  [Meyer, 1976]. Assume that the algorithm Ω
3.2  Proof of Convergency                             is strictly monotonic with respect to J and it generates a se-
To prove the  convergency of the projection matrices  quence {xt} which lies in a compact set. If χ is normed, then
U 1,U2,...,Un, we need the concept of point-to-set map. xt − xt−1→0.
The power set ℘(χ) of a set χ is the collection of all sub- From theorem-3, we can have the conclusion that the ob-
                                                              k|n
sets of χ.Apoint-to-set map Ω is a function: χ → ℘(χ). tained (Ut k=1) will converge to a local optimum, since the
In our solution procedure to tensor-based subspace learning, χ is compact and with norm deﬁnition.
              k  n         k n
the map from (Ut−1|k=1)to(Ut |k=1) can be considered as
                           k
a point-to-set map, since each Ut is invariant under any or- 4 Experiments
thogonal transformation.
  Strict Monotony. An algorithm is a point-to-set map In this section, we systematically examine the convergency
                                                      properties of our proposed solution procedure to tensor-based
Ω:χ →  ℘(χ). Given an initial point x0, an algorithm gen-
                                                      subspace learning. We take the Marginal Fisher Analysis
erates a sequence of points via the rule that xt ∈ Ω(xt−1).
                                                      (MFA) as an instance of general subspace learning, since
Suppose J : χ → R+ is continuous, non-negative function,
an algorithm is called strict monotony if 1) y ∈ Ω(x) implies MFA has shown to be superior to many traditional subspace
that J(y) ≥ J(x), and 2) y ∈ Ω(x) and J(y)=J(x) imply learning algorithms such as Linear Discriminant Analysis
                                                                                                     [
that y = x.                                           (LDA); more details on the MFA algorithm is referred to Yan
                                                               ]
  Let set χ be the direct sum of the orthogonal ma-   et al., 2007 . Then, we evaluate the classiﬁcation capability of
            m  ×m                                    the derived low-dimensional representation from our solution
trix space O  k   k , that is, the data space χ  =
                                                procedure compared with the traditional procedure proposed
 m1×m       m2×m            mn×m
O     1   O      2   ...  O      n , then the Algo-   in [Ye et al., 2004] and [Yan et al., 2005]. For tensor-based
rithm 1 produces a point-to-set algorithm with respect to algorithm, the image matrix, 2rd tensor, is used as input, and
           k|n
J(x)=G(U    k=1), and it can be proved to be strictly mono- the image matrix is transformed into the corresponding vector
tonic as follows.                                     as the input of vector-based algorithms.
  Theorem-2.  The point-to-set map from Algorithm 1 is
strictly monotonic.
                                        k  n          4.1  Data Sets
  Proof.  From  theorem-1, we have G(Ut−1|k=1)   ≤
    k|n                                               Three real-world data sets are used. One is the USPS hand-
G(Ut k=1), and hence the ﬁrst condition for strict monotony        4
                                            1         written dataset of 16-by-16 images of handwritten digits
is satisﬁed. For the second condition, we take U as an
                                                      with pixel values ranging between -1 and 1. The other two
example to prove that this condition is also satisﬁed. If                                           5
    k  n          k n                                 are the benchmark face databases, ORL and CMU PIE .For
G(U   |   )=G(U    |   ), then from the proof of theorem-
    t−1 k=1       t k=1                               the face databases, afﬁne transform is performed on all the
              1           1                k  |n
1, we have g(Ut−1)=g(Ut    ) with λ =  G(Ut−1 k=1)    samples to ﬁx the positions of the two eyes and the mouth
     k  p                 k  |n
and S ,Sk computed from (Ut−1 k=1). From the proof of center. The ORL database contains 400 images of 40 per-
theorem-1, we can have that there only exists one orthogo- sons, where each image is normalized to the size of 56-by-46
               3          1       1
nal transformation between Ut−1 and Ut . As shown in Al- pixels. The CMU PIE (Pose, Illumination, and Expression)
gorithm 1, this kind of orthogonal transformation has been
                                           1     1    database contains more than 40,000 facial images of 68 peo-
normalized by the reshaping step, hence we have Ut−1=Ut . ple. In our experiment, a subset of ﬁve near frontal poses
                         k     k
Similarly, we can prove that Ut = Ut−1 for k =1, 2,...,n,
                                                         4Available at: http://www-stat-class.stanford.edu/ tibs/ElemStat-
  3This claim is based on the assumption that there do not exist Learn/data.html
duplicated eigenvalues in (11).                          5Available at http://www.face-rec.org/databases/.

                                                IJCAI-07
                                                   632         6                           25                            9
               Ours                                                8
         5                           20
               Traditional                                         7
         4                                                         6
                                     15
                                                                   5
         3                                    Ours
                                                                   4
                                     10       Traditional
         2                                                         3
                                                                              Ours
                                      5                            2
         1                                                                    Traditional
                                     Projection  Difference
        Projection  Difference
                                                                  Projection  Difference 1

         00    5    10    15   20     00    5    10    15   20     00    5    10    15    20
               Iteration Number             Iteration Number             Iteration Number
                  1    1                        1   1                        1    1
             (a) Ut − Ut−1              (b) Ut − Ut−1               (c) Ut − Ut−1

         7                            10                           12
                                      9                                     Ours
         6
                                      8                            10       Traditional
         5
                                      7                            8
         4                            6
                                      5                            6
         3                            4       Ours
                                              Traditional          4
         2       Ours                 3
                                      2
         1       Traditional                                       2
                                                                   Projection  Difference
                                     Projection  Difference
        Projection  Difference        1
                                      0                            0
         00    5    10    15    20     0    5     10   15    20     0    5     10   15    20
               Iteration Number              Iteration Number             Iteration Number
                   2   2                        2    2                       2    2
             (d) Ut − Ut−1               (e) Ut − Ut−1              (f) Ut − Ut−1

Figure 2: The step difference of the projection matrices vs. iteration number. (a,d) USPS database, (b,e) ORL database, and
(c,f) CMU PIE database.

(C27, C05, C29, C09 and C07) and illuminations indexed as
08 and 11 are used and normalized to the size of 32-by-32. Table 1: Recognition error rates (%) on the ORL database.
                                                                  Method    G3P7   G4P6  G5P5
4.2  Monotony of Objective Function Value                         w/o DR.   28.57  24.17  21.5
                                                                   LDA      17.86  17.08 11.00
In this subsection, we examine the monotony property of the       MFA RT    17.50  16.25 10.50
objective function value from our solution procedure com-         MFA TR    13.93  10.00  6.50
pared with the optimization procedure that step-wisely trans-    TMFA  RT   12.14  11.67  5.00
forms the objective function into the ratio trace form. The      TMFA  TR   11.07  6.67   4.00
USPS, ORL and PIE databases are used for this evalua-
tion. The detailed results are shown in Figure 1. It is ob-
served that the traditional ratio trace based procedure does Table 2: Recognition error rates (%) on the PIE database.
not converge, while our new solution procedure guarantees
the monotonous increase of the objective function value and       Method    G3P7   G4P6  G5P5
commonly our new procedure will converge after about 4-10         w/o DR.   49.89  31.75 30.16
iterations. Moreover, the ﬁnal converged value of the objec-       LDA      18.82  19.84 18.10
tive function from our new procedure is much larger than the      MFA RT    16.55  15.61 13.65
value of the objective function for any iteration of the ratio    MFA TR    14.97  13.49  9.52
trace based procedure.                                           TMFA  RT   14.74  14.29  3.81
                                                                 TMFA  TR   13.61  12.17  9.52
4.3  Convergency of the Projection Matrices
To evaluate the solution convergency property compared with 4.4 Face Recognition
the traditional ratio trace based optimization procedure, we In this subsection, we conduct classiﬁcation experiments on
calculate the difference norm of the projection matrices from the benchmark face databases. The Tensor Marginal Fisher
two successive iterations and the detailed results are dis- Analysis algorithm based on our new solution procedure
played in Figure 2. It demonstrates that the projection matri- (TMFA TR) is compared with the traditional ratio trace based
ces converge after 4-10 iterations for our new solution pro- Tensor Marginal Fisher Analysis (TMFA RT), LDA, Ratio
cedure; while for the traditional procedure, heavy oscilla- Trace based MFA (MFA RT) and Trace Ratio based MFA
tions exist and the solution does not converge. As shown in (MFA TR), where MFA TR means to conduct tensor-based
Figure 3, the recognition rate is sensitive to the oscillations MFA by assuming n=1. To speed up model training, PCA is
caused by the unconvergent projection matrices and the clas- conducted as a preprocess step for vector-based algorithms.
siﬁcation accuracy is degraded dramatically.          The PCA dimension is set as N-Nc (N is the sample number

                                                IJCAI-07
                                                   633