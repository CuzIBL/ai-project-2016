                             Learning from Partial Observations∗

                                            Loizos Michael
                             Division of Engineering and Applied Sciences
                          Harvard University, Cambridge, MA 02138, U.S.A.
                                        loizos@eecs.harvard.edu


                    Abstract                            In this work we employ learning as a means of identifying
                                                      structure in a domain of interest, given access to certain ob-
    We present a general machine learning framework   servations. We subsequently utilize such identiﬁed structure
    for modelling the phenomenon of missing informa-  to recover missing information in new observations coming
    tion in data. We propose a masking process model  from the same domain. Note that the manner in which ac-
    to capture the stochastic nature of information loss. quired knowledge may be utilized to draw conclusions is not
    Learning in this context is employed as a means to necessarily a single step process (see, e.g., [Valiant, 2000]).
    recover as much of the missing information as is re- Nonetheless, we focus here on examining whether even in-
    coverable. We extend the Probably Approximately   dividual learned rules can be meaningfully applied on partial
    Correct semantics to the case of learning from par- observations, given that such rules are learned from obser-
    tial observations with arbitrarily hidden attributes. vations that are partial themselves. Studying how multiple
    We establish that simply requiring learned hypothe- learned rules can be chained and reasoned with to draw richer
    ses to be consistent with observed values sufﬁces to conclusions presents further challenges, and necessitates a so-
    guarantee that hidden values are recoverable to a lution to the more fundamental problem examined herein.
    certain accuracy; we also show that, in some sense,
                                                        We present a general machine learning framework within
    this is an optimal strategy for achieving accurate re-
                                                      which the problem of dealing with missing information can
    covery. We then establish that a number of natural
                                                      be understood. We formulate the notion of masked attributes,
    concept classes, including all the classes of mono-
                                                      whose values in learning examples (e.g., patient records) are
    tone formulas that are PAC learnable by monotone
                                                      not made known to an agent. Such masked attributes account
    formulas, and the classes of conjunctions, disjunc-
                                                      for missing information both in the given target that the agent
    tions, k-CNF, k-DNF, and linear thresholds, are
                                                      attempts to learn (e.g., the presence of a particular disease),
    consistently learnable from partial observations.
                                                      as well as in the learning features over which the agent’s hy-
    We ﬁnally show that the concept classes of parities
                                                      potheses are formed (e.g., various pieces of information from
    and monotone term 1-decision lists are not properly
                                                      a patient’s medical history). Masked attributes are determined
    consistently learnable from partial observations, if
                                                      by an arbitrary stochastic process that induces for each ex-
    RP = NP. This implies a separation of what is con-
                                                      ample a possibly different but ﬁxed distribution over partial
    sistently learnable from partial observations versus
                                                      observations to which the example is mapped (see also [Schu-
    what is learnable in the complete or noisy setting.
                                                      urmans and Greiner, 1994]); this is intended to capture situa-
                                                      tions such as the probabilistic failure or inability of an agent’s
1  Introduction                                       sensors to provide readings. We extend the Probably Approx-
                                                      imately Correct learning semantics [Valiant, 1984] to apply
Consider the task of predicting missing entries in a medical to the described situation. A salient feature of the extension
database, given the information that is already available. How we propose is the lack of need for specially prepared learn-
does one go about making such predictions, and what kind of ing materials; the agent simply utilizes whatever information
guarantees might one provide on the accuracy of these pre- is made available through the masking process. We call this
dictions? The problem with which one is faced here is that of type of learning autodidactic to emphasize that although the
missing information in data, an arguably universal and multi- agent might still employ supervised learning techniques, this
                                           [
disciplinary problem. Standard statistical techniques Schafer is done without the presence of a teacher explicitly providing
               ]
and Graham, 2002 fail to provide a formal treatment for the the agent with “labelled instances” during the learning phase.
general case of this problem, where the fact that information
is missing might be arbitrarily correlated with the actual value We propose consistency as an intuitive measure of success
of the missing information (e.g., patients exhibiting a certain of the learning process. An agent faced with partial observa-
symptom might be less inclined to disclose this fact). tions needs to produce hypotheses that do not contradict what
                                                      is actually observed; the values of masked attributes need not
  ∗This work was supported by grant NSF-CCF-04-27129. be predicted correctly. In addition, hypotheses that do not as-

                                                IJCAI-07
                                                   968sume a deﬁnite value due to masked attributes need not make Deﬁnition 2.1 (Examples and Observations) Consider any
a prediction. We allow, thus, the possibility of “don’t know” non-empty ﬁnite set A of attributes.Anexample over A is a
predictions, but restrict such predictions in a natural manner, vector exm ∈{0, 1}|A|.Anobservation over A is a vector
providing a notion of completeness of the prediction process. obs ∈{0, 1, ∗}|A|. An observation obs masks an exam-
  Following the presentation of our framework, we discuss ple exm if obs[i] ∈{exm[i], ∗} for every attribute xi ∈A.
accuracy as an alternative measure of success, whereby an An attribute xi ∈Ais masked in an observation obs if
agent is expected to correctly predict the values of masked at- obs[i]=∗.Amasking process is a (stochastic) function
tributes. We show that the success of an agent in this stricter mask : {0, 1}|A| →{0, 1, ∗}|A| that maps each example
setting might be completely impaired, depending on how con- exm to some observation obs that masks exm.
cealing the masking process is (i.e., how adversarially infor-
mation is hidden from the agent). On the positive side, we Examples deﬁne the “truth” about the environment. Such
show that to the degree allowed by the masking process, an examples are drawn from some underlying ﬁxed probability
                                                                D
agent can perform optimally in making accurate predictions, distribution that is unknown to the agent. Unlike standard
by simply making consistent predictions. This surprising re- PAC learning, the agent does not directly observe such exam-
lation between the two measures of success allows an agent to ples, but only masked versions of the examples. We denote by
                                                      mask(D)
focus on the more natural task of learning consistently, while the induced distribution over these observations.
not losing anything with respect to predicting accurately. The stochastic masking process can be understood in two
  We then examine consistent learnability more closely. We ways. If attributes correspond to an agent’s sensors, masking
deﬁne a notion of reduction between learning tasks, and es- corresponds to the stochastic failure of these sensors to pro-
tablish that any concept class of monotone formulas that is vide readings. If attributes correspond to properties of the en-
PAC learnable by some hypothesis class of monotone formu- vironment, masking corresponds to an agent’s inability to si-
                                                      multaneously sense all properties. In either case, masking in-
las is also consistently learnable from partial observations;             exm
the result is obtained by reducing the learning task to one over duces for each example a possibly different but ﬁxed dis-
                                                      tribution mask(exm) over observations; the induced distribu-
complete observations. Through a second reduction we show                                     mask(D)
that the concept classes of conjunctions, disjunctions, k-CNF, tions remain unknown to the agent, and so does .
k-DNF, and linear thresholds over literals, are all properly Masked attributes have their values hidden, without any
consistently learnable from partial observations.     connotations. In particular, a masked attribute is not to be
  On the negative side, we show that the set of consistently understood as “non-deducible” from the rest of the attributes.
                                                      The goal of an agent is not to deduce that a masked attribute
learnable concept classes is a subset of the PAC learnable con-         ∗
cept classes. We continue to prove that the concept classes of is assigned the value , but rather to deduce the truth-value of
parities and monotone term 1-decision lists are not properly the masked attribute according to the underlying masked ex-
consistently learnable from partial observations, given that ample. This is a rather non-trivial, and sometimes impossible,
the widely held complexity assumption RP = NP is true. The task, depending on the masking process being considered.
                                          1                                              (         )
intractability of properly learning monotone term -decision Deﬁnition 2.2 (Formulas) A formula f xi1 ,...,xik over
lists from partial observations provides a partial answer to a A is a function f : {0, 1}k →{0, 1} whose arguments
question posed by Rivest [1987]. Our intractability results es-                                ∈A
                                                      are associated with the attributes xi1 ,...,xik .The
tablish separations between our model of consistent learnabil- (          )               exm
                                                      value of f xi1 ,...,xik given an example is deﬁned to
ity from partial observations, and the existing models of PAC val( (     ) | exm)   (exm[  ]    exm[  ])
                                                      be     f xi1 ,...,xik         f     i1 ,...,   ik .
learnability [Valiant, 1984] and learnability in the presence of   (         )                   obs
                                                      The value of f xi1 ,...,xik given an observation ,de-
random classiﬁcation noise [Angluin and Laird, 1987].          val( (          ) | obs)
                                                      noted by     f xi1 ,...,xik     , is deﬁned to be the
  We assume the reader is familiar with basic PAC learning common value of the formula given all examples masked by
terminology (see, e.g., [Kearns and Vazirani, 1994]). Proofs obs, in case such a common value exists, or ∗ otherwise.
are only brieﬂy discussed in this paper due to lack of space.
                                                        An agent’s task of identifying structure in its environment
                                                      can be made precise as the problem of learning how a certain
2  The Learning Framework                             target attribute in A can be expressed as a formula over other
In the PAC learning model [Valiant, 1984], a set of boolean attributes in A, as these are perceived through the agent’s sen-
                                                          1
variables {x1,x2,...,xn} represents the attributes of the en- sors. To study learnability, one usually assumes that the tar-
vironment. A concept c is a boolean formula over the boolean get attribute is indeed expressible as such a formula, called
variables. An example for the concept c is a truth-assignment the target concept, and that the target attribute always as-
to the boolean variables, drawn from an underlying probabil- sumes a truth-value according to the target concept. The de-
ity distribution D, paired with the induced truth-value of c. scribed setting is captured by the following deﬁnitions.
  Such a treatment distinguishes the target attribute from the Deﬁnition 2.3 (Formula Equivalence) Formulas ϕ1 and ϕ2
attributes acting as learning features for that target. As a more over A are equivalent w.r.t. a probability distribution D if
natural and better suited approach for autodidactic learning, Pr(val(ϕ1 | exm)=val(ϕ2 | exm) | exm ←D)=1.
where target attributes are not externally “labelled”, we con-
sider examples that treat all attributes equally as properties of 1More generally, one can consider how a formula over attributes
the environment. The attribute acting as a learning target need can be expressed as a formula over other attributes. The approach is
only be deﬁned as part of the learning task one undertakes. similar, and our deﬁnitions and results apply largely unchanged.

                                                IJCAI-07
                                                   969Deﬁnition 2.4 (Supported Concepts) A concept class over hypotheses will correctly predict the values of non-masked at-
A is a set C of formulas over A. A probability distribution tributes that are artiﬁcially (for the purposes of analysis) “ob-
D supports C for an attribute xt if xt is equivalent to some scured” in an observation, after the observation is drawn. In
formula c ∈Cw.r.t. D; c is the target concept for xt under D. some sense this is the best one can hope for. If an agent never
  Supported concept classes essentially encode a known or gets to observe parts of its environment, then it can only form
assumed bias on the probability distribution from which ex- hypotheses that in the best case are consistent with its obser-
amples are drawn. This imposes constraints on the examples, vations, although they might not agree with the underlying
in what is perhaps the simplest possible manner that still facil- masked examples. This is reminiscent of developing physical
itates learnability. Assuming such a bias, the goal of an agent theories by ﬁnding laws that are consistent with what we ob-
is then to identify a formula from some hypothesis class, that serve, without this implying that our current, past, or future
is consistent with the target attribute with high probability. physical theories are actually the “correct” ones. Hypotheses
                                                      developed in this manner are, of course, used to make predic-
Deﬁnition 2.5 (Learning Tasks) A learning task over A is a tions on masked attributes of the world. Humans go into great
triple xt, C, H,wherext is an attribute in A, C is a concept lengths to subsequently obtain the values of such masked at-
class over A, and H a hypothesis class of formulas over A. tributes, so as to experimentally validate a physical theory.
  We omit writing the set of attributes A over which a learn- In the context of this work we study whether developed the-
ing task is deﬁned, when this does not introduce ambiguities. ories, or hypotheses, that are consistent with the partial obser-
             (1− )                                    vations of an agent, would actually make accurate predictions
Deﬁnition 2.6 ( ε -consistency) A hypothesis h conﬂicts on a hypothetical validation experiment. That is, given an ob-
with a target attribute xt ∈Aw.r.t. an observation obs if     obs                   exm
{val(  | obs) val(  | obs)} = {0 1}                   servation    masking an example   , and an attribute xt
     h      ,     xt             ,  . A hypothesis h  that is masked in obs, we wish to examine whether it is pos-
is (1 − ε)-consistent with a target attribute xt ∈Aunder a         exm[ ]
                    D                    mask         sible to predict  t , and thus accurately (and not simply
probability distribution and a masking process if     consistently) “ﬁll-in” the missing information in obs.
   ({val(  | obs) val(  | obs)} = {0 1}|
Pr       h      ,     xt           ,                               (1 −  )                        (1 − )
                  exm  ←D;  obs ←  mask(exm))  ≤ ε.   Deﬁnition 3.1 (   ε -accuracy) A hypothesis h is ε -
                                                      accurate w.r.t. a target attribute xt ∈Aunder a probability
  Recall that formulas might evaluate to ∗ given an observa- distribution D and a masking process mask if
tion. We interpret this value as a “don’t know” prediction, and
                                                      Pr({val(h  | obs), val(xt | exm)} = {0, 1}|
such a prediction is always consistent with a target attribute.
                                                                         exm ←D;  obs  ← mask(exm))  ≤ ε.
Similarly, a value of ∗ for an attribute is interpreted as a “don’t
know” sensor reading, and every prediction is consistent with Hypotheses might still evaluate to ∗ given an observation.
such a sensor reading. That is to say, as long as the prediction Thus, the accuracy requirement amounts to asking that when-
coming through a hypothesis and the sensor reading do not ever a hypothesis predicts a {0, 1} value, the value should
directly conﬂict by producing different {0, 1} values, there is be in accordance with the actual (rather than the observed)
no inconsistency at the observational level.          value of the target attribute. Identifying the conditions under
  It is important to note that the ability to make “don’t know” which one can form accurate hypotheses is essential, in that
predictions cannot be abused by an agent. Every hypothesis is an agent’s actions yield utility based not on what the agent
necessarily a formula, which assumes a deﬁnite {0, 1} value observes, but based on what actually holds in the agent’s en-
whenever sufﬁciently many of its arguments are speciﬁed. It vironment. The more informed the agent is about the actual
only evaluates to ∗ given an observation, when its value on state of its environment (either through observations or accu-
the actual underlying example that was masked to obtain the rate predictions), the better decisions the agent might reach.
observation cannot be determined. Thus, our framework ac- Clearly, predictions that are accurate are necessarily con-
counts for an implicit notion of completeness, by imposing a sistent (since it holds that obs[t] ∈{exm[t], ∗}). The other
natural restriction on the “don’t know” predictions.  direction, however, does not hold in general. Indeed, predic-
                                                      tions on masked target attributes are always consistent, while
Deﬁnition 2.7 (Consistent Learnability) An algorithm L is
                                                      there is no evident reason why they should also be accurate.
a consistent learner for a learning task xt, C, H over A if
for every probability distribution D supporting C for xt, every Theorem 3.1 (Indistinguishability in Adversarial Settings)
masking process mask, every real number δ :0<δ≤ 1, and Consider a target attribute xt, and a concept class C over
every real number ε :0<ε≤ 1, the algorithm runs in time A\{xt}, and let ϕ1,ϕ2 ∈Cbe such that ϕ1 = ϕ2.There
polynomial in 1/δ, 1/ε, |A|, and the size of the target concept exist probability distributions D1, D2 such that: (i) ϕ1,ϕ2
for xt under D, and with probability 1−δ returns a hypothesis are equivalent w.r.t. neither D1 nor D2, (ii) ϕ1,xt are
h ∈Hthat is (1−ε)-consistent with xt under mask(D).The equivalent w.r.t. D1, and (iii) ϕ2,xt are equivalent w.r.t.
concept class C over A is consistently learnable on xt by H D2. There also exists a masking process mask such that
if there exists a consistent learner for xt, C, H over A. mask(D1)=mask(D2), and no attribute in A\{xt} is
                                                      masked in any drawn observation.
3  Consistent Learners vs. Accurate Predictors          Theorem 3.1 shows that examples might be masked in such
We have taken the approach that learned hypotheses are ex- a way so that two non-equivalent concepts are indistinguish-
pected to be consistent with observations, a natural general- able given a set of observations. In fact, it sufﬁces to only
ization of the respective requirements of PAC learning. Such mask the target attribute in a few (but adversarially selected)

                                                IJCAI-07
                                                   970cases for the result to go through. The non-masked attributes Deﬁnition 3.3 (Accurate Predictability) An algorithm L is
are also adversarially selected so that observations will imply an accurate predictor for a learning task xt, C, H over A if
a {0, 1} value for all formulas over A\{xt}, excluding the for every probability distribution D supporting C for xt, every
possibility of a “don’t know” prediction. Clearly, an agent has real number η :0<η≤ 1, every masking process mask
no means of identifying which of the probability distributions that is (1 − η)-concealing for xt, C, H, every real number
D1, D2 examples are drawn from, or equivalently, which of δ :0<δ≤ 1, and every real number ε :0<ε≤  1,the
the formulas ϕ1,ϕ2 is the target concept for xt. Thus, it is im- algorithm runs in time polynomial in 1/η, 1/δ, 1/ε, |A|, and
possible for the agent to conﬁdently return a hypothesis that the size of the target concept for xt under D, and with proba-
is highly accurate w.r.t. xt under mask(D1)=mask(D2); bility 1−δ returns a hypothesis h ∈Hthat is (1−ε)-accurate
either conﬁdence or accuracy is necessarily compromised. w.r.t. xt under mask(D). The concept class C over A is ac-
  We note that the indistinguishability result imposes very curately predictable on xt by H if there exists an accurate
mild restrictions on the probability distributions D1, D2,and predictor for xt, C, H over A.
              C
the concept class , which implies that an adversarial choice It is now straightforward to show the following.
of the masking process mask can “almost always” prove dis-
astrous for an algorithm attempting to make accurate predic- Theorem 3.3 (Consistent Learners / Accurate Predictors)
tions, even if the algorithm is computationally unbounded, Consider a learning task xt, C, H, and a masking process
the known bias on the probability distribution is as strong as mask that is (1 − η)-concealing for xt, C, H. If algorithm
possible (i.e., the concept class is of cardinality two), and the L is a consistent learner for xt, C, H, then algorithm L
hypothesis class comprises of all formulas over A\{xt}. given η as extra input and allowed running time that grows
  The established impossibility result suggests that having an polynomially in 1/η, is an accurate predictor for xt, C, H.
infrequently masked target attribute does not sufﬁce to learn We have thus established not only that consistent learning
accurately; it is important to have an infrequently masked tar- implies accurate predicting, but that the same algorithm can
get attribute in the right context. We formalize this next. be used, with the only provision that the algorithm will be
Deﬁnition 3.2 ((1 − η)-concealment) A masking process allowed more running time to achieve the same precision as
mask  is (1 − η)-concealing for a learning task xt, C, H determined by ε. The running time dependence on η can be
if η is the maximum value such that for every example exm, eliminated if the following are true: (i) the consistent learner
and every hypothesis h ∈H                             is such that it only uses the observations that do not mask the
                                                      target attribute, and (ii) the induced predictor has access to an
Pr(val(xt | obs) = ∗|obs ← mask(exm);
                                                      oracle that returns observations from distribution mask(D),
          {val(h | obs), val(xt | exm)} = {0, 1}) ≥ η.
                                                      conditioned, however, on the observations not masking the
  Roughly speaking, Deﬁnition 3.2 asks that whenever a hy- target attribute. The use of such an oracle exempliﬁes the fact
pothesis is inaccurate, the agent will observe evidence of this that a predictor does not require more computation to produce
fact with some probability. This generalizes the case of PAC an accurate hypothesis, but rather more observations in order
learning, where an inaccurate hypothesis is always observed to obtain enough “labelled instances” of the target concept.
to conﬂict with the target attribute (which is never masked). A rather intriguing implication of our results is that a con-
We note that the masking process mask whose existence is sistent learner is, without any knowledge of the concealment
guaranteed by Theorem 3.1 is necessarily 1-concealing for degree of the masking process, also able to predict accurately,
every learning task xt, C, H with a non-trivial concept class. albeit with a “discounted” accuracy factor. In fact, as condi-
                                                      tion (ii) of Theorem 3.2 suggests, a consistent learner is, in
Theorem 3.2 (The Relation of Consistency and Accuracy)
                                                      some sense, as accurate a predictor as possible. Given this
Consider a learning task xt, C, H, and a masking process
                                                      result, it sufﬁces to restrict our attention to consistent learning
mask  that is (1 − η)-concealing for xt, C, H. Then, (i)
                                                      for the rest of our study on learning from partial observations.
for every probability distribution D and hypothesis h ∈H,
h is (1 − ε/η)-accurate w.r.t. xt under mask(D) if h is
(1 − ε)-consistent with xt under mask(D), and (ii) there 4 Consistently Learnable Concept Classes
exists a probability distribution D0 and a hypothesis h0 ∈H The stronger learnability requirements we impose compared
such that h0 is (1 − ε/η)-accurate w.r.t. xt under mask(D0) to PAC learning do not render learnability impossible. It is
only if h0 is (1 − ε)-consistent with xt under mask(D0). an easy exercise to show that the typical algorithm for PAC
                                                      learning conjunctions [Valiant, 1984] and its analysis can be
  Assuming that our physical world does not adversarially
hide information from us, one can interpret the above result applied essentially unmodiﬁed on partial observations.
as a partial explanation of how it is possible for humans to Theorem 4.1 The concept class C of conjunctions of literals
learn rules, and construct physical theories, that make accu- over A\{xt} is properly consistently learnable on xt.
rate predictions in situations where nothing is known, despite
the fact that learning takes place and is evaluated mostly on 4.1 One-To-Many Reductions
observations with inherently missing information.     Reductions between learning tasks are often used to establish
  Similarly to the case of constructing learners for noisy ex- that certain concept classes are or are not learnable. Standard
amples [Kearns, 1998], we assume that an algorithm is given reductions map examples from one learning task to examples
a bound on the concealment degree of the masking process of a different learning task. In our case such reductions map,
and allowed time that depends on this bound during learning. in general, partial observations to partial observations.

                                                IJCAI-07
                                                   971Deﬁnition 4.1 (Reductions) The learning task xt, C, H k-CNF formulas are deﬁned (since exactly one substitution is
                             j  j  j       j r−1
over A is reducible to the set {xt , C , H  over A }j=0 of required for each of the polynomially many possible clauses).
learning tasks, where r ∈ N is polynomially-bounded by |A|, Theorem 4.3 (Reduction to Monotone Classes) The learn-
if there exists an efﬁciently computable hypothesis mapping ing task xt, C, H over A is reducible to the learning task
    0         r−1                                                  
g : H × ...×H     →H, and an efﬁciently computable in- xt, C , H  over A if there exists a set M of substitutions
                                         j
stance mapping f j : {0, 1, ∗}|A| →{0, 1, ∗}|A | for every such that: (i) M is computable in time polynomial in |A|,
 ∈{0       − 1}                                       (ii) every sub-formula substituted under M can be evaluated
j    ,...,r     , such that the following conditions hold:                                            
                                                      given an observation in time polynomial in |A|, and (iii) xt=
                   ∈H0  ×    ×Hr−1                                                       
 (i) for every tuple h    ...       and every obser-  basis(xt |M), C =basis(C|M), and  H =basis(H|M).
    vation obs ∈{0, 1, ∗}|A|, it holds that g(h) conﬂicts
               obs                  ∈{0       − 1}      An immediate corollary of Theorems 4.1 and 4.3 is that
    with xt w.r.t. only if there exists j ,...,r      the concept class of k-CNF formulas is properly consistently
             j =  [ ]            j      j(obs)
    such that h  h j conﬂicts with xt w.r.t. f ;      learnable for every constant k ∈ N.
(ii) the probability distribution mask(D) from which obs is
                                                      4.3  Learning Monotone Formulas
    drawn is such that D supports C for xt only if for every
    j ∈{0,...,r − 1} there exists an induced probability We employ reductions to establish certain learnability results.
    distribution maskj(Dj ) from which f j(obs) is drawn Theorem 4.4 (Total Self-Reduction) The learning task
                             j
            Dj        Cj                              xt, C, H over A is total reducible to the learning task
    such that  supports  for xt , and the size of the target                                 
               j     Dj                        |A|    xt, C , H  over A such that A = A, xt = xt, C = C,
    concept for xt under is polynomially-boundedby    H =  H       (·)                       C     H
    and the size of the target concept for xt under D.        , and g  is the identity mapping, if and are
                                                      classes of monotone formulas over A\{xt}, and  ∈C.2
  Roughly, the two conditions guarantee that (i) learned hy-
                                                      Proof Idea: By monotonicity, any formula in {xt}∪C∪H
potheses can be meaningfully employed in the original task,
                                                      that assumes a {0, 1} value given obs retains its value when
and that (ii) observations in the resulting tasks can be obtained
                                                      masked attributes are mapped to val(xt | obs) ∈{0, 1}. 
by masking examples drawn from appropriate distributions.
                                                        Theorem 4.4 establishes a rather surprising fact for mono-
Theorem 4.2 (Learning through Reductions) Consider a
                                                      tone formulas: consistently learning from partial observations
learning task xt, C, H over A that is reducible to the set
                                                      reduces to consistently learning the same concept class from
               { j Cj Hj     Aj }r−1
of learning tasks xt , ,   over    j=0. The concept   complete observations. Equally intriguing is the fact that hy-
     C                                H
class  is consistently learnable on xt by if for every potheses learned (from complete observations) for the result-
j ∈{0,...,r−1}, the concept class Cj is consistently learn-
        j    j                                        ing task, apply unmodiﬁed for making predictions (on partial
able on xt by H .                                     observations) in the original task. The preceding facts nicely
  The following special case is of particular interest, in that complement Theorem 3.2, which establishes that consistently
observations in the resulting learning tasks are complete. learned hypotheses are also as accurate as possible. Together,
                                                      Theorems 3.2 and 4.4 imply that a concrete strategy to predict
Deﬁnition 4.2 (Total Reductions) A reduction is total if for
                                              j       accurately on partial observations is to simply assign appro-
every j ∈{0,...,r− 1}, f j : {0, 1, ∗}|A| →{0, 1}|A |. priate default truth-values to masked attributes, consistently
                                                      learn from the resulting complete observations, and then em-
4.2  Shallow-Monotone Formulas                        ploy the learned hypothesis unchanged to make predictions.
We establish a reduction between certain classes of formulas. A technical point worth discussing here is the encoding of
                                                      the value of the target attribute in certain attributes of the re-
Deﬁnition 4.3 (Shallow-Monotonicity) Aformulaϕ   is   sulting task. Observe that an agent learning in the resulting
shallow-monotone w.r.t. a set M of substitutions if the pro-
                                                     task, although agnostic to this fact, uses the “label” of the ex-
cess of substituting an attribute xi(ψ) for every sub-formula ψ ample in a much more involved manner than its standard use
             
of ϕ such that xi(ψ)/ψ ∈M, produces a monotone formula; as a means to test the predictions of a hypothesis. What makes
denote by basis(ϕ |M) the resulting (monotone) formula. A the result established by the reduction non-trivial, is the fact
set F of formulas is shallow-monotone w.r.t. a set M of sub- that the hypothesis does not depend on the target attribute in
stitutions if every formula ϕ ∈Fis shallow-monotone w.r.t. the context of the original task. In some sense, we allow an
M; we deﬁne basis(F|M)    {basis(ϕ |M)  | ϕ ∈F}.     agent to use an example’s “label” in an involved manner when
                                                      learning, but require that the hypothesis eventually employed
  We implicitly assume that the substitution process replaces for making predictions does not depend on the target attribute.
distinct new attributes for distinct sub-formulas of ϕ,andthat The next corollary follows from Theorems 4.3 and 4.4, and
the resulting formula is entirely over these new attributes. the PAC learnability of certain monotone concept classes.
                 F
  Clearly, every set of formulas is shallow-monotone w.r.t.                         C∈{
some set M of substitutions. The emphasis of Deﬁnition 4.3 Corollary 4.5 Each concept class conjunctions, dis-
               M                            F         junctions, k-CNF, k-DNF, linear thresholds} of literals over
is on the choice of , and the corresponding basis of w.r.t. A\{ }
M. Note, for instance, that the class of k-CNF formulas for xt is properly consistently learnable on xt.
some constant k ∈ N has a basis that comprises of conjunc- 2Assuming  ∈Cis without loss of generality, since sampling
tions, and this basis is w.r.t. a set of substitutions that is only can be used to determine w.h.p. whether the target concept is a tau-
polynomially large in the number of attributes over which the tology, and the reduction can be used only when this is not the case.

                                                IJCAI-07
                                                   972