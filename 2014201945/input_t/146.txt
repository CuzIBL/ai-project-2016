                                      Covariant Policy Search 

                                 J. Andrew Bagnell and Jeff Schneider 
                                            Robotics Institute 
                                       Carnegie-Mellon University 
                                           Pittsburgh, PA 15213 
                                     { dbagnell, schneide } @ ri. emu. edu 

                     Abstract                            Inspired by the work of [Amari and Nagaoka, 2000], 
                                                       Kakade proposed a "natural gradient" algorithm. In particu•
     We investigate the problem of non-covariant behav• lar, [Kakade, 2002] proposed a scheme for generating a met•
     ior of policy gradient reinforcement learning algo• ric on parameter space that has interesting theoretical prop•
     rithms. The policy gradient approach is amenable  erties. Most convincingly, Kakade showed strong empiri•
     to analysis by information geometric methods. This cal evidence for the algorithm's usefulness. In particular, 
     leads us to propose a natural metric on controller [Kakade, 2002] applies the algorithm to the Tetris problem 
     parameterization that results from considering the of [Bertsekas and Tsitsiklis, 1996]. This problem is partic•
     manifold of probability distributions over paths in• ularly interesting as value function methods (as for example 
     duced by a stochastic controller. Investigation   described in [Bertsekas and Tsitsiklis, 1996] ) demonstrate 
     of this approach leads to a covariant gradient as• non-monotone performance; first policy iteration improves 
     cent rule. Interesting properties of this rule are the policy dramatically, but after a small number of iterations 
     discussed, including its relation with actor-critic the policy gets much worse. Normal gradient methods, in•
     style reinforcement learning algorithms. The al•  cluding second-order and conjugate methods also prove very 
     gorithms discussed here are computationally quite ineffective on this problem; even after a tremendous number 
     efficient and on some interesting problems lead   of rounds they only mildly increase the performance of the 
     to dramatic performance improvement over non-     game-player. The method presented in [Kakade, 2002] shows 
     covariant rules.                                  rapid performance improvement and achieves a significantly 
                                                       better policy than value-function methods (at their peak) in 
 1 Introduction                                        comparable time. 
 Much recent work in reinforcement learning and stochastic However, despite recognizing an interesting defect in the 
 optimal control has focused on algorithms that search directly general approach and intuiting an apparently powerful algo•
 through a space of policies rather than building approximate rithm, Kakade concludes that his method also fails to be co-
 value functions. Policy search has numerous advantages: do• variant leaving the problem open. We present here what we 
 main knowledge may be easily encoded in a policy, the policy believe to be an appropriate solution. 
 may require less representational power than a value-function In Kakade's work, there is no proper probability manifold, 
 approximation, there are simple extensions to the multi-agent but rather a collection of such (one for each state) based on 
 domain, and convergent algorithms are known. Furthermore, the policy. As such, Kakade must rely on an ad-hoc method 
 policy search approaches have recently scored some encour• for generating a metric. Here instead we take a proper mani•
 aging successes [Bagnell and Schneider, 2001 ] [Baxter et al, fold, the distribution over paths induced by the controller, and 
 19991 including helicopter control and game-playing.  compute a metric based on that. In the special case appropri•
   One interesting aspect of existing gradient search algo• ate to the average reward RL formulation, Kakade's scheme 
 rithms, however, is that they are non-covariant; that is, a is shown to give rise to a bona-fide natural metric, despite the 
 simple re-parameterization of the policy typically leads to a observation in the paper that the learning was non-covariant. 
 different gradient direction. (Not that found by applying the We believe this is an artifact- perhaps of step-length. Fur•
 Jacobian of the re-parameterization to the gradient). This ther, we note that parametric invariance does not require 
 is a odd result; it is intuitively difficult to justify the gradi• this metric- there are numerous covariant metrics. Rather, 
 ent computation as actually indicating the direction of steep• a stronger probabilistically natural invariance demands the 
est descent. This problem is well recognized in the pattern- metric used. We describe this result to motivate our method. 
recognition and statistics communities and is a very active Importantly, the notion of the metric on the path-
area of research. [Kakade, 2002] was, to the best of our distribution allows us to provide very simple and natural ex•
knowledge the first to identify this problem in reinforcement tensions to Kakade's algorithm that cover the finite horizon, 
 learning and to suggest that techniques from information ge• discounted start-state, and partially-observed reinforcement 
ometry may prove valuable in its solution.             learning problems as well as the average reward one. Fi-

 PROBABILISTIC PLANNING                                                                              1019 nally, for completeness, we describe the result discovered by 
Kakade relating the invariant metric and compatible actor-
critic methods [Sutton et aL, 1999]. 

1.1 Problem Setup and Notation                      Figure 1: A two-state MDP. Each state has two controls one which 
A stochastic control problem consists of paths (also called self transitions (and earns the reward that labels the state) and an•
system trajectories) in a space a distribution over path- other that simply transitions to the other state. Rewards occur only 
space, that is a function of a sequence of (finite) controls on the transitions between different states. 
            indexed by time, from a space A. Throughout 
this paper we will be considering Partially Observed Markov 
Decision Problems, in which there are state-variables of 
the system that compose a path and render the past and fu•
ture independent. In a POMDP, is defined by an initial 
state and next state transition probabilities 
We also typically have a sequence of outputs (observations) 
           measurable with respect to . A controller (or 
policy), usually parameterized by is a feedback-type 
strategy that maps the history of observations to 
a distribution over controls Most of the derivations will 
be given in terms of memoryless stochastic controllers that 
map the current state to a distribution over actions, although Figure 2: Log odds of actions for policies logp 
they can be easily extended to finite window or recurrent rep• 1 on the two-state MDP (horizontal axis corresponds to state 0). 
resentations that operate on only the observations. The goal Different curves correspond to varying Notice the non-covariant 
in a control problem is to maximize the expected reinforce• policy behavior. 
ment with respect to The sum is 
always assumed to exist for all controllers. The reward func•
tion on paths in a POMDP is additive in time (or in the infi• state 1. In state 1, action 0 returns to state 1 and gets reward 
nite time case, discounted or averaged), and a function R 2. Action 1 transits to state 0 and achieves no reward. 
of state, although the algorithms discussed here are not neces• Consider parameterized probabilistic policies of the form 
sarily predicated on that assumption. Reinforcement learning                         is an arbitrary scale 
is the adaptive version of the control problem where we at- parameter that we use to demonstrate that even mildly differ•
tept to maximize the expected reinforcment by sampling tra• ent parameterization lead to dramatically different behavior. 
jectories and then improving our policy. We use the notation Below we plot the resulting track through the space of poli•
  throughout to denote where the parameter should be cies using the log ratio of probabilities of the actions for each 
                                                    of the possible states. We start from a policy that makes it 
clear from context. For vectors and we use the notation 
                                                    somewhat more likely to choose action 0 in state 0 and action 
       to denote the inner product with respect to metric 
                                                    1 in state 1. We then scale to 1, .5, and 2 and plot the log 
M. The notation indicates the expected value of 
                                                    odds of the policy from state 1 (Figure 2). 
the function / with respect to the distribution 
                                                      The non-covariant behavior is quite clear in this graph. 
2 Covariance and Riemannian manifolds               Further, we note that it is very difficult to achieve a good 
                                                    policy using this algorithm from this starting point as the 
2.1 Meaning of steepest ascent                      wrong action becomes overwhelmingly likely to be chosen 
For direct policy search methods, we are interested in finding from state 0. If sampling were used to compute the gradient, 
the direction of steepest ascent with respect to our current pa• we would nearly never get samples from state 1- which we 
rameters. That is, we wish to maximize the reward function need to improve the policy. 
         subject to an infinitesimal. If we 
reparameterize our controller in terms of, e.g. and express 2.3 Path distribution manifolds 
the same effective infinitesimal policy change in terms of The control problem is essentially a coupled one of optimiza•
these parameters, will of course remain the same. How• tion and integration over the space of possible paths, This 
ever, if we measure lengths using the naive dot product, the motivates the idea that instead of considering the (arbitrary) 
same effective change will not have the same size. It is this distance in terms of parameterization of the policy, we may 
problem that leads to the odd behavior of "steepest descent" consider distance in terms of the change in distribution over 
rules.                                              paths resulting from the policy change. We may view the 
                                                    distribution over paths as a parameterized manifold 
2.2 Non-covariant learning rule                     (nominally embedded in of dimension This takes 
To demonstrate the problem we first consider a simple two some work to visualize; as an example consider a space of 
state, two action MDP described in [Kakade02j. (Figure 1) three possible paths. All possible distributions over these 
In state 0 executing action 0 returns to the same state and gets three paths can be smoothly represented with two parameters. 
a reward of 1. Executing action 1 get no reward but transits to In Figure 3 (left) we see one visualization with the embedding 

1020                                                                          PROBABILISTIC PLANNING                                                                                                        (3) 
                                                        and take derivatives with respect to each , then set to zero 
                                                      to solve for the optimal direction: 


Figure 3: An example probability manifold over paths. Each This implies that (since G is positive definite and hence 
axis represents the log probability of one of the three possible invertible), giving us: 
paths. The manifold is formed as we vary the parameters 
defining it throughout their domain. On the right we attach 
the tangent space at a particular point in parameter space.                                            (4) 
                                                        That is, the direction of steepest descent is simply the nor•
                                                      mal gradient times the inverse of the metric evaluated at the 
                                                      point of tangency. We call this the "natural gradient" for the 
  With parameters, assuming no redundancy, we generate 
                                                      Reimannian manifold. [Amari and Nagaoka, 20001 
an dimensional manifold. In the case pictured, it is the set 
of all distributions on 3 paths, but in general, the dimension•
ality of the path probability manifold will be tremendously 3 Invariance and Chentsov's Theorem 
less than the number of possible paths. It is important to un• Some confusion seems to surround the choice of metric on 
derstand the manifold under consideration is that of the prob• probability manifolds. There is unique answer for this 
ability distribution over paths and not of paths themselves. metric, even under the supposition of parametric invariance 
  The study of parameterized manifolds like that pictured as suggested by some authors. The issue is rather more sub•
above is the domain of differential geometry. Here we are tle. Any metric that transforms under parameter change ac•
interested in establishing a Riemannian structure on the man• cording to the Jacobian of the function connecting parameters 
ifold of paths. By that we mean we wish to establish a metric meets this requirement. This type of parametric covariance is 
on the tangent space (the local linear approximation to the a minimum requirement for us. It is natural to suggest a met•
manifold at a point so we can measure small parameter ric that preserves essential probabilistic aspects of the mani•
changes. We do this with the metric on the tan•       fold. Consider, for example, functions q (Markov mappings, 
gent space (which is spanned by the partials with respect to congruent cmbeddings, or sufficient statistics, depending on 
each parameter) as where G is a positive              your viewpoint) that carry one distribution over paths to an•
definite matrix. This is a very natural thing to do- instead other in a natural and recoverable way. For example, con•
of just the standard dot product, we have a dot product that sider the mapping between the two distributions P(paths) and 
allows us to represent rotations and scalings (exactly what P(paths')given by congruent embedding q depicted below: 
a positive definite matrix can represent) and that can vary 
throughout the manifold. In Figure 3 (right) we depict the 
tangent space at a point in our parameterization as the local 
linear approximation at that point. 

2.4 Steepest ascent on Riemannian manifold 
There are two questions that then naturally arise- what is 
steepest descent on a function defined over a Riemannian 
                                                      Paths' 
space, and is there a Riemannian metric on the manifold of 
the paths that is in some sense natural. We quickly answer The mapping q above interchanges the role of two paths 
the first question, and pursue the second in the next two sec• and splits one path into two (with probability 1/2 for each 
tions. A Lagrange multiplier argument (given schematically split). In a probabilistically fundamental way, the mani•
here) makes it easy to see the form of the steepest descent folds P(path) and P(path') (where we imagine P(path) is 
direction.                                            a smoothly parameterized set of distributions) are similar 
                                                      to each other. For each path' we can uniquely recover the 
                                                      original probability. In this way, a parameterized distribu•
                                                (1)   tion over paths can be embedded into a different path space. 
                                                (2)   (This equivalence can actually be phrased in a category the•
                                                      oretic way where the morphisms are these congruent embed-
  Form the Lagrangian:                                dings.) iChentsov, 1980] General congruent embeddings can 

PROBABILISTIC PLANNING                                                                               1021 be thought of as simply generalizations of the example de•  - Compute zj = i—^z + jdjlog7r(a\x;9) 
picted above to allow arbitrary permutations and arbitrary 
probabilities for different paths stemming from a single path, 
as well as compositions of these two. In the control problem Return G 
this could arise by simple permutation of the state variables We use the Markov property and the invariance of the tran•
or change of co-ordinates to ones with redundant information. sition probabilities (given the actions) in the algorithm above 
It is natural to require that with our metric the congruent em• to compute 9jogp(£;0) simply. These details can be ex•
bedding is an isometry on the tangent space, (i.e. preserves 
                                                      tracted from the proofs below, as well as other, potentially 
the length of the vectors). That is, if we make a change of 
                                                      better ways to sample. 
size (under the metric to the distribution (paths)then 
                                                        To compute the natural gradient, we simply invert this ma•
carrying that change through we should measure the same 
                                                      trix and multiply it with the gradient computed using standard 
change c using . Adding the requirement of invariance 
                                                      policy search methods. 
with respect to congruent embeddings leads to a unique (up to 
scale) metric on the manifold. [Chentsov, 1980] This metric 4.2 Limiting metric for infinite horizon problems 
is well-known in statistical inference as the Fisher-Rao metric 
and it can be written as the Fisher information matrix [DeG- A well-known result in statistics [DeGroot, 1970] gives a dif•
root, 19701:                                          ferent form for the Fisher metric under appropriate regularity 
                                                      conditions. We quickly derive that result, and then show how 
                                                      this gives us a simple form for the path probability metric as 

  Another way to derive the metric is to think about "dis•
tances" on probability spaces. The KL-divergencc (or rel•
ative entropy) between two distributions is a natural diver•
gence on changes in a distribution. It is also manifestly in•
variant to re-parameterization. If we think about derivatives 
as small changes in parameters (differentials) then we dis•
cover that we also get a unique metric as the second-order 
Taylor expansion of the KL-divergence. This too agrees with 
the Fisher information (up to scale). We note that the di•
rection of the KL-divergence is irrelevant as to second-order 
                  fAmari and Nagaoka, 2000]. 
4 Fisher-Rao Metric on the Path-space 
                                                                                                      (5) 
   Manifold 
The issue now becomes how to derive the Fisher metric on The third line follows from the second by integrating by 
the space of path distributions. It turns out in the case of parts and the fifth follows by observing that the total proba•
processes with underlying Markovian state to be rather easy, bility is constant. 
and involves only computations we already make in the likeli• Now we show that in the limit this leads to a simple metric 
hood ratio approach standard in gradient-based reinforcement for the infinite horizon case. To get a convergent metric, we 
learning.                                             must normalize the metric, in particular by the total length 
                                                      of the path denoted t. Since the metric is defined only up to 
4.1 Derivation of finite-time path metric             scale, this is perfectly justified. 
The Fisher information metric involves computing      Theorem 1 (Infinite-Horizon Metric) For an ergodic 
                          . Fortunately, this is easy to Markov process the Fisher information matrix limits to the 
do. The essential algorithm within gradient methods like expected Fisher information of the policy for each state and 
REINFORCE and GPOMDP is a clever method of computing  control under stationary distribution of states and actions. 
                 the expected score function. Thus, while Proof: We use to indicate the t-step finite-horizon metric. 
the expected score is the gradient, the correlation of the 
score is the Fisher matrix. The following simple algorithm 
is unbiased and converges almost surely (under the same 
regularity conditions as in [Baxter et al, 1999] as the number 
                                                                                                      (6) 
of sample paths goes to infinity: 
Algorithm 1 (Finite-horizon metric computation) For i in 
                                                        For a Markov process we can write the likelihood ratio 
1 to m: 


1022                                                                            PROBABILISTIC PLANNING  Using the chain rule we continue the derivation with    where is the limiting distri•
 indicating the stationary distribution:               bution of states. Thus the infinite horizon (undiscounted) and 
                                                       start-state metrics give essentially the same metric, with only 
                                                       the effective weighting by states differing. 
                                                       4.3 Metrics for Partially Observed Problems 
                                                       For policies that map the observation space of a partially-
                                                       observed markov decision process into distributions over ac•
                                                       tions, it is just as easy to derive appropriate metric using our 
                                                       approach. The tuple is also a markov chain, and 
                                                       with only subtle changes to the arguments above we end up 
                                                       with the same metric except using the limiting distribution of 
                                                       observations instead of states. 
                                                       5 Relation to Compatible Value Function 
                                                           Actor Critic 
                                                       Kakade noticed a fascinating connection between the limit•
                                                       ing metric given in Theorem 1 and the improvement direction 
                                                       computed by a class of actor-critic methods that use a special 
   where the second equality follows by noting that the like• compatible function approximation technique.[Sutton et al., 
 lihood ratio is independent of transition probability matrix- 1999; Konda and Tsitsiklis, 20021 Following Kakade we let 
 given the action probability, and by application of the ergodic             and let the compatible function ap•
 theorem. The last line follows (with i the Fisher informa• proximator be linear in 
 tion for the policy at a given state) as the second term in line 
 3 vanishes since the total probability is constant with respect 
 to 
   It is also interesting to consider what happens in the start- This type of value function approximator was initially sug•
 state, discounted formulation of the problem. In this case we gested as it may be used to compute the true gradient. In prac•
 would like our metric, using the general definition over path tice, it has not been clear what advantage this brings over the 
 distributions given above, to naturally weigh the start start gradient estimation routines of [Baxter et al, 1999]. How•
 more than it necessarily is in the infinite horizon average case. ever, "folk-wisdom" has it that performing an that infinitesi•
 It is well-known that a discount factor is equivalent to an mal policy iteration (moving in the direction of the best policy 
 undiscounted problem where each trajectory terminates with according to using this approximation has very 
 probability 1 - at each step. We can use this fact to derive good properties and often significantly outperforms standard 
 a metric more appropriate for the discounted formalism. gradient methods. The natural gradient provides insight into 
                                                       this behavior. Let minimize the squared value-function er•
 Theorem 2 (Start-State Metric) For a discounted Markov ror: 
 process the Fisher information matrix equals the Fisher in•
formation of the policy for each state and control under the 
 limiting distribution of states and actions. Proof: The 
 proof is very similiar to the infinite horizon case and so we 
 simply sketch it:                                     where is the exact advantage function. [Sutton et 
                                                       al, 1999J It is easy to check that ([Kakade, 2002], Theo•
                                                       rem 1 That is, the direction of maximum 
                                                       policy improvement is exactly the natural gradient direction. 
                                                       This can be seen by simply differentiating to minimize 
                                                       it with respect to and noting the result of [Sutton et al, 
                                                       1999] that 

                                                       6 Demonstration 
                                                       As a consequence of the results of section 5 and iKakade, 
                                                       2002], experimental results already exist demonstrating the 
                                                       effectiveness of the natural gradient method. That is, all re•
                                                       sults using policy improvement with compatible function ap•
                                                       proximators are implicitly computing this result. As a demon•
                                                       stration, we computed analytically the natural gradient for the 

 PROBABILISTIC PLANNING                                                                               1023 