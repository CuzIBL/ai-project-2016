                  A Distributed Architecture for Symbolic Data Fusion

                  Fulvio Mastrogiovanni, Antonio     Sgorbissa and Renato Zaccaria
                             Laboratorium DIST, University of Genoa, Italy
                  {fulvio.mastrogiovanni, antonio.sgorbissa, renato.zaccaria}@unige.it


                    Abstract                          by prior knowledge guiding data interpretation, and it must
                                                      coexist with a reliable architecture providing data acquisition,
    This paper presents a distributed knowledge rep-  ﬁltering, etc.
    resentation and data fusion system designed for     Several data fusion models have been designed and in part
    highly integrated Ambient Intelligence applica-   successfully used in different applications reported in liter-
    tions. The architecture, based on the idea of     ature. In particular, the extended JDL (Joint Directors of
    an ecosystem of interacting artiﬁcial entities, is a Laboratories) model [White, 1998] received great attention:
    framework for collaborating agents to perform an  hypothesizing ﬁve layers occurring in a typical information
    intelligent multi-sensor data fusion. In particular, acquisition process, JDL characterizes heterogeneous pro-
    we focus on the cognitive layers leading the over- cesses using the same functional framework. Present archi-
    all data acquisition process. The approach has been tectures for AmI mainly address data fusion at the numer-
    thoroughly tested in simulation, and part of it has ical level, usually exploiting sound and well-deﬁned tech-
    been already exploited in successful applications. niques like bayesian inference (such as the well known
                                                      Kalman Filter [Dhaher and MacKesy, 2004]), parameter es-
1  Introduction                                       timation [Kong et al., 2005], Fuzzy Logic [Chen et al.,
                                                          ][                 ]
The Ambient Intelligence (AmI) paradigm highlights the 2003 Hagras et al., 2003 or other methods, disregarding
need for autonomous and distributed systems that are able data fusion at the symbolic level.
to “understand” users’ intents using sensor data collected by In this paper we propose a multi-agent cognitive architec-
a network of heterogeneous devices and to exhibit an intel- ture aimed at designing integrated AmI applications. While
ligent behavior to provide users with context-based services, in Section 2 we introduce the main concepts related to the ar-
possibly taking decisions on their behalf.            chitectural approach, in Section 3 we focus on its data fusion
  During the last few years, several approaches have been capabilities distributed throughout the system detailing the
proposed to develop the idea of a Smart Space, i.e., a func- representation at the symbolic level. Next, actual implemen-
tional interface towards services that are aimed at improv- tation and experimental results are presented and discussed.
ing the users’ quality of life. This concept arises from the Conclusion follows.
mutual role played by different disciplines, encompassing
topics ranging from health care monitoring [Barger et al., 2 An Ecosystem of Artiﬁcial Entities
2005] to mobile robotics [Broxvall et al., 2006] , from in- The presented architecture is designed to support numerical
telligent surveillance applications [Kong et al., 2005][Chen as well as symbolic data fusion. In particular, data fusion
et al., 2003] to knowledge representation [Augusto and Nu- is considered as a decentralized process managed by several
gent, 2005]. The complexity of AmI systems requires an cognitive-oriented tasks. An ecosystem, according to [Odum,
efﬁcient management of the information ﬂow exchanged by 1959], is “an area of nature that includes living organisms and
the involved services: pervasive and distributed systems can non-living substances that interact to produce an exchange of
be considered truly “intelligent” only if a close interaction materials between the living and non-living parts”. In AmI
among the several architectural layers (and their subsystems) this deﬁnition can be extended to support the intuition that the
is achieved. Moreover, the related increased complexity of Smart Space is an ecosystem whose artiﬁcial entities interact
the networks connecting sensors and actuators requires the for the fulﬁllment of their goals: one of them is the “well
combination of possibly ambiguous information. As a conse- being” of the users.
quence, reliable data fusion techniques must be used as well. The basic building block of our “ecological space” is the
  The aim of a data fusion process is to maximize the use- concept of “agent”. Therefore, a Smart Space can be mod-
ful information content acquired by heterogeneous sources in eled as a collection of agents {α} attending different tasks.
order to infer relevant situations and events related to the ob- Agents can be further classiﬁed into two disjoint sets: whilst
served environment. In order to be effective, data fusion does device-related {αd} agents manage sensors for data acquisi-
not operate in isolation: on the contrary, it must be supported tion or actuators, cognitive {αc} agents perform data fusion

                                                IJCAI-07
                                                  2153and apply control rules to guide the system behavior. 3   The Ecosystem as an Integrated Model for
  More speciﬁcally, in the context of an AmI system, {αd} Data Fusion
agents interact with hardware devices, thus acting as the
bridge between the physical world and the cognitive layers The JDL extended model is a ﬁve-layers architecture specif-
of the architecture. While dealing with device control, they ically designed to deal with heterogeneous information pro-
can also perform preliminary data analysis and react to emer- vided by multiple sources. In particular, whilst base layers
gency situations using simple decision making procedures deal with data segmentation, feature extraction and symbol
(e.g., checking if a numerical value is over a given thresh- grounding, advanced layers are designed to operate on sym-
old), or by providing inputs to cognitive agents. {αc} agents bolic information and metadata: e.g., ﬁnding relationships
are aimed at applying given control rules to incoming data in between symbols, adaptating to user needs through learning,
order to issue low level references back to {αd} agents. They etc. In the following, the data fusion capabilities distributed
are organized in specialized networks performing tasks re- throughout the proposed architecture will be discussed with
lated to data ﬁltering, feature extraction, symbolic inferences, respect to the ﬁve layers of the JDL model.
knowledge representation, data fusion and planning.
  Regardless of the category, each agent αj can be formally 3.1 Level 0: Sub-object Assessment
                              =
deﬁned as a 4-element vector, αj  θ, γ, ι, ω ,whereθ  Level 0 involves acquiring, ﬁltering and processing raw data
speciﬁes the agent capabilities, γ is a set of goals that αj can in order to extract feature based information. Since the goal
contribute to achieve, and ι and ω are operators returning, re- of this level is to provide Level 1 with manageable informa-
spectively, the set of data needed by αj to perform its tasks tion, no semantic meaning is assigned to the features. In our
and the set of data produced by αj. Agents can aggregate architecture, Level 0 capabilities are achieved through a tight
in more complex niches, characterized by a common goal g coupling between device and cognitive agents. In particular,
to fulﬁll. According to the above deﬁnition of Smart Space suitable niches are designed according to the goals of Level
based on an ecological approach, we deﬁne a goal g as the re- 1, i.e., arranging incoming data in predeﬁned formats. Raw
                                          =   (  )
sult of the cooperation of n agents. In particular, g ω Ag , data coming from {αd} are usually preprocessed and then, if
         = {   :  =1      }
where Ag    αj  j    , ..., n is the niche of the involved necessary, sent to the proper cognitive agent for feature ex-
agents, and ω is the previously introduced operator returning traction. Usually, data related to environmental sensors (e.g.,
the relevant output data of Ag.                       temperature or smoke sensors) do not need further process-
  Niches are the ecological counterpart of the concept of ing, and can be directly exploited by numeric or even sym-
“subsystem”. From a broader perspective, niches themselves bolic data fusion techniques. On the other hand, other kinds
can be modeled as agents and, as such, they are possibly of data (e.g. provided by laser rangeﬁnders or cameras) need
part of larger niches. Consider, for example, a surveillance feature extraction techniques in order to be useful for the in-
system based on several sensors: laser rangeﬁnders, cam- formation acquisition process (i.e., manageable in real time).
eras, etc. According to our approach, it can be viewed as Recall the previously introduced example of a surveillance
a niche composed by a collection of simpler cooperating system based on laser and camera data. Level 0 could involve
agents, some responsible for providing feature-based infor- the fulﬁllment of the goals gl and gbb, respectively, to obtain
mation, others for data fusion, etc. These considerations lead lines from range data and bounding boxes from images (as-
us to use indifferently the generic term “agent” in the follow- sumed that lines and bounding boxes are used as features).
ing discussion, and to adopt a recursive deﬁnition of agent, The ﬁrst goal is solved by instantiating the following niche,
αn+1 = Ag = {αj : j =1, ..., n}. An exact formalization is         =       = {  d  c }
                                                      or agent, αlfe  Alfe    αld,αle ,wherelfe stands for
outside the scope of the paper.                                             d                     c
                                                      “line feature extractor”, αld is a laser device and αle is an
  It is worth noticing that the given deﬁnitions do not assume agent implementing a line extraction algorithm. Analogous
any speciﬁc restriction about agent situatedness or inter-agent
                                                      considerations hold for a bounding box extractor agent αbbe.
communication. In particular, it seems reasonable to assume
    { d}
that α   agents should be considered embedded in the de- 3.2 Level 1: Object Assessment
vices they are dealing with. For example, an agent managing
analogical data originating from a temperature sensor will re- The main objectives of Level 1 are the combination of data
side on the board which performs signal acquisition. No as- acquired by Level 0 and the maintaining of coherent models
sumption is made about embodiment of {αc} agents. Again, for data and symbols. In particular, this level addresses topics
a Kalman Filter or an entire localization system used for peo- related to numerical data fusion and data association, and then
ple tracking could be scheduled – in principle – on a worksta- the symbol grounding problem [Harnad, 1990].
tion which is not physically located within the Smart Space. Numerical data fusion techniques and data association do
Nonetheless, if cooperating agents can be distributed in a net- not deserve further investigation in this paper, being well
work according to speciﬁc needs, we must ensure that infor- studied in literature. It sufﬁces to note that, with respect to
mation among them can be shared in some way. Thus, in the problem of tracking the user metric position through laser
the following, we assume that, for each pair (i, j) of commu- and/or cameras, at this level it is possible to combine in a new
nicating agents, there exists a communication channel ci,j , niche a Level 0 niche with a suited collection of agents (e.g.,
         ∀     ∃   (     )                                                c                          c
(formally, αi,αj ci,j αi,αj ), where αi and αj are coop- a Kalman Filter agent αkf and a motion model agent αmm).
erating agents such that (a subset of) ι(αi) is provided by (a The symbol grounding problem is deﬁned as “the process of
subset of) ω(αj) or viceversa.                        creating and maintaining a correspondence between symbols

                                                IJCAI-07
                                                  2154and sensor data that refer to the same physical object rep- characterized by a corresponding speciﬁcation of Data.As
resented within an artiﬁcial system” [Harnad, 1990]. Still an example, consider a line based feature, as provided by αlfe
unresolved in its general formulation, the symbol grounding (see Section 3.1): in AL syntax it is modeled as a child con-
problem arises whenever a symbolic component is added to cept of Data, characterized by speciﬁc information related to
an artiﬁcial system.                                  the feature itself (e.g., distance, direction, etc.) through the
  Our architecture deals with symbolic knowledge represen- use of roles. Finally, the User concept models human users
                                                 c
tation and data fusion by introducing a cognitive agent αkb monitored by the system. Moreover, Action is used to model
managing a knowledge base described using a Description user interactions with the environment, e.g. Go, Cook,etc.
Logic. Description Logics usually consist of a Terminology In this layer, the ABox contains instances of the concepts
Box (TBox), representing concepts, descriptions and their re- Device, Agent, Area, Data, etc. Sensory data are mapped
lationships (roles), and an Assertional Box (ABox), describ- to instances of Data, thus updating speciﬁc roles of Device
ing the actual scenario in terms of the involved concepts and instances. Therefore, they are not really given a semantic
descriptions. TBox and ABox must be carefully designed for meaning. For these reasons, this layer does not suffer from
the problem at hand. Therefore, a brief description of the sce- the symbol grounding problem, because association between
nario we consider is necessary. We focus on the problem of sensor data and symbols is designed apriori.
active monitoring of aging and disabled people: in particu-
lar, we want to design a pervasive system supervising their A Symbolic Data Fusion Structure
activities and reacting to emergency situations.      Symbolic data fusion is achieved through the well known
  The TBox allows Level 1 symbolic data fusion by imple- mechanism of subsumption, which is the main reasoning
menting two layers: a representation of the Smart Space, i.e. scheme of Description Logic-based languages. Given two
“objects” of the physical world and devices, which is updated concepts, namely C1 and C2, we say that C1 is subsumed by
by sensors, and the data fusion structure, responsible for cre- C2 (and, using AL syntax, we write C1 C2)ifC2 is more
ating meaningful instances of concepts from individual sym- general than or equivalent to C1. Speciﬁcally, the subsump-
bols originating from sensor data. Corresponding instances tion operates on the descriptions of the given concepts. In the
are created within the ABox. In order to update the ABox following, given a concept C, we will denote its description
in real time, we assume the availability of a comprehensive D by appropriately deﬁning an operator δ such that D = δC,
                       c                              and its instances I by an operator ξ such that I = ξC.
niche of agents providing αkb with heterogeneous data. The
       c                                                Let’s start by an example, considering again the problem
aim of αkb is to perform fusion of simple information pro-
vided by a redundant network of devices. Describing in de- of user position tracking. Suppose we are not interested in
tail all the concepts used in the TBox is impossible within metric information; instead, the knowledge of the areas vis-
the scope of this paper. Therefore, we only brieﬂy discuss ited by the user is sufﬁcient. At our disposal we have three
concepts which will be useful in later paragraphs.    sensors: one camera and two PIR (Passive Infra Red) detec-
                                                      tors. In our architecture, a niche could be arranged as fol-
                                                                               d     d      d
Modeling the Smart Space                              lows. Three device agents, αcam, αp1 and αp2, are present.
                                                       d                           c                  c
In order to arrange sensor data in semantic structures, the αcam provides a cognitive agent αbbe with raw images. αbbe
TBox ﬁrst represents the entire Smart Space using a topo- extracts bounding boxes from the moving objects which are
                                                                               c
logical representation. It is divided in places (by means of then passed to another agent αblob, able to extract color blobs
the Place concept), and then each place in areas. Spe- from the bounding boxes. These data are used to associate
ciﬁc deﬁnitions of Area,e.g. ToiletteArea or BedArea, a speciﬁc bounding box with a user, whose dress colors are
are deﬁned. For each physical device type (e.g., lasers or known. No spatial information is provided: if a user is within
cameras, but windows or doors as well), a corresponding the scope of the camera, only the place in which he or she is
Device concept is available in the TBox. A Device is de- can be inferred, not a speciﬁc area. The details of the process
ﬁned as an Object with a Position in the environment  are not really relevant: it could be possible as well to use in-
and an actual State, updated regularly by the proper niche telligent wearable technology and RFID tags to perform this
through an established communication channel. Devices are association. The symbol grounding problem which should
classiﬁed in SensorDevice, characterized by an inﬂuence arise is thus simpliﬁed by the redundancy of the sensor net-
                                                                           d      d
area, (i.e., their scope in the environment modeled as a col- work. On the contrary, αp1 and αp2 are not able to infer user
lection of areas: in AL syntax, ∀infArea.Area), and in identity, but they can provide boolean information about the
ActuatorDevice. For each sensor or actuator, a corre- presence of someone in a speciﬁc area (according to the sen-
sponding child of Device is introduced, e.g. LaserDevice sor range).
or WindowDevice.                                        In the ABox, cameraDev  is a CameraDevice, while
  In our architecture, each device is managed by a spe- pirDev1 and pirDev2 are instances of PIRDevice.The
                                      AL                                                   c       c
ciﬁc agent. An Agent is deﬁned using an   deﬁnition   niche of cognitive agents composed by αbbe and αblob is
corresponding to the one introduced in Section 2, using abstracted by functionality in userIdAgent, controlling
roles linking to the concepts Capability, Goal,andData. cameraDev. userIdAgent provides data about user iden-
Next, a DeviceAgent is modeled as a child of Agent with tity, i.e., instances of the CameraData concept. pirDev1
an additional role specifying its controlled Device.For and pirDev2 are managed by pirAgent1 and pirAgent2,
each Device concept (e.g., LaserDevice), a correspond- respectively. Moreover, a User is characterized by a role id
ing DeviceAgent is introduced (e.g., LaserDeviceAgent), specifying its identity, such that ∀id.CameraData, and by a

                                                IJCAI-07
                                                  2155Position i.e., a collection of areas.                 are characterized by the role in restricted to be, respectively,
  The data fusion process is then managed by checking the ToiletteArea, BedArea, StoveArea etc. Of course, this
result of subsumption of the infArea role of each Device approach can be iterated: the situation NearStove is special-
(see Algorithm 1). The CameraData concept is used to iden- ized in Cooking, CleaningStove, etc., e.g. on the basis of
tify the instance of User whose position is to be computed. data coming from a smoke sensor placed on top of the stove.
Moreover, the user position is initialized to the description Despite its simplicity, this tree-like layer proves to be ef-
of the cameraDev.infArea role. Assuming that the cam- fective in simulation experiments. Moreover it is character-
era is able to observe three areas, namely area1, area2 and ized by a number of advantages compared to other more com-
area3 in the ABox, the description d is inizialited to area1 plex models: (i) it is easily manageable and extensible: new
area2  area3. Then, for each PIRDevice such that some- Situation branches can be added by creating new concepts,
thing was detected, the role infArea is checked. Again, as- given that the system is able to distinguish among different
suming that dp1 = δpirDev1.infArea = area2  area3    situations through (a combination of) sensor data; (ii) its cre-
and dp2 =  δpirDev2.infArea  =  area2,wehaved        ation can be automated: actual work is focused on creating
dp1   dp2. As a consequence, dp2 = area2 is the new  decision trees from data sets obtained in simulation whose
user position.                                        nodes are concepts conveniently deﬁned; (iii) using the sub-
                                                      sumption, a system exploiting the tree for managing an active
Algorithm 1 Compute User Area                         monitoring system can be easily implemented.
Require: id = ξCameraData; p1, p2 = ξPIRData            Within the ABox, we have a generic situation s such that
Ensure: user area                                     s =  ξSituation, corresponding to u = ξUser. Assume
 1: for all u such that u = ξUser do                  now that we want to track the user activities in the kitchen.
 2:  if id  δUser.id then                            The following example, although incomplete, explain how
 3:    d = δcameraDev.infArea                         the data fusion process is carried out at this Level. Suppose
 4:    for all p such that p = ξPIRDevice do          that we inferred the user position to be StoveArea,usingAl-
 5:      if d  δp.infArea then                       gorithm 1, implemented by a cognitive agent αc .Inother
             =                                                                                cua
 6:         d  δp.infArea                             words, an operator Kcua was ﬁred, which updated u.
 7:      end if                                         Basically, for each epistemic operator K relative to a user u
 8:    end for                                        it is possible to derive a new description to be added to its sit-
 9:  end if                                           uation s. Obviously, this description (which we deﬁne as δK)
10:  δuser.pos  = d                                   is operator dependent. This implies that, for each branching
11: end for                                           concept of our Situation tree, we must implement an epis-
                                                      temic operator K providing the necessary δK to further detail
                                                                                        c
  Generalizing, symbolic data fusion processes can be de- the classiﬁcation, i.e., a corresponding αK is to be istantiated,
veloped by adding proper cognitive agents cooperating with conveying the required information to perform data fusion.
 c
αkb, implementing the required algorithms. For example, Al-
gorithm 1, computing an user position, can be managed by
                                                      Algorithm 2 Classify Situations
a cognitive agent αc , operating on the knowledge base.
                 cua                                           K;
Moreover, because for each cognitive agent there is a sym- Require: id
bolic counterpart within the knowledge base, each data fusion Ensure: classiﬁcation or alarm
                                                           K
process can be thought of as an epistemic operator K operat- 1: if is ﬁred then
                                                             =             :   =
ing on concepts described by the input and output data of the 2: s ξSituation id δs.id
                                                              =      K
corresponding αc when a particular conﬁguration of sensor 3: δs  δs  δ
                                                                ⊥
data is updated within the knowledge base.             4:   if δs   then
                                                       5:     unexpected classiﬁcation
3.3  Level 2: Situation Assessment                     6:   end if
The main goal of this Level is to ﬁnd relationships among 7: end if
Level 1 objects. Again, this Level is to be modeled on the ba-
sis of the Level 1 entities, according to the speciﬁc scenario.
                                                        Using the information provided by Kcua, it is inferred that
As previously pointed out, we are interested in tracking (se- 
quence of) user situations in order to detect and react to aris- s NearStove. According to the Situation tree, this
                                                      concept can be specialized in Cooking or CleaningStove.
ing anomalies. For these reasons, we provide the TBox with                   K
another layer modeling the situations we want to monitor. Be- A new epistemic operator, ss, is thus introduced. It corre-
cause we are using a Descripion Logic-based language, we sponds to a niche of agents communicating information about
implicitly obtain a hierarchical representation.      the detection of smoke by a speciﬁc sensor located over the
                                                      stove. This operator provides the system with a description
Modeling the Situations                               δKss used, e.g., to classify the user situation s as Cooking.
We start by deﬁning the Situation concept, related to a The overall process can be modeled at the metalevel using
User. Next, we further detail this concept on the basis of the a new epistemic operator Kc,wherec stands for “classiﬁ-
user position (i.e., Area). Thus, we can deﬁne situations like cation”, implementing Algoritm 2. If δs is inconsistent, an
NearToilette,  InBed, NearStove  etc. These concepts  alarm can be raised.

                                                IJCAI-07
                                                  2156Detecting Sequences of Situations
As an example of how  to use the symbolic data fusion
mechanism presented so far, we focus in this Section on
the problem of detecting classes or sequences of situations.
This task can be accomplished by adding new concepts to
the knowledge base and new corresponding operators. Se-
quences of situations are modeled using the SitSeq concept,
which is a collection of situations managed by a role madeOf
such that ∀madeOf.Situation. A particular SitSeq is
MonitoredSitSeq, specifying the interesting Situation
to be monitored through the role interesting.
  When  Kc  is ﬁred, the corresponding Situation in-
stance is added to the madeOf role. In order to detect Figure 1: (Left) The simulation environment; (Right) Map of
sequences, several approaches are equally legitimate. For an on going experimental set-up.
example, the frequency at which situations occur is an
important indicator of periodic user behaviors. Consider
the problem of monitoring user health status by check- through the use of mobile robots. In our perspective, mo-
ing its visits to the toilette. This can be achieved by in- bile robots are mobile extensions to the Smart Space. They
stantiating a new concept, ToiletteSeq, characterized by are provided with local device or cognitive agents communi-
∀interesting.NearToilette.    Each time the madeOf    cating with other distributed niches. Given the high level of
role is updated, its deﬁnition is compared to δinteresting. system integration, cognitive agents running on ﬁxed work-
                        K
By applying a new operator a implementing an alarm con- stations can cooperate with device agents on board of mobile
dition (based, e.g., on the frequency of the occurrences of the robots. As a consequence, the operator Kp can be used to in-
interesting Situation in δmadeOf), it is thus possible to ﬁre stantiate a new Problem whose corresponding solution will
speciﬁc alarm behaviors. For example, if the user is living in be used to move the robot toward the speciﬁed Area, thus
a private apartment within an assisted-living facility, a remote implementing the reconﬁguration.
human supervisor in the nursery can be notiﬁed about the user Inter level information ﬂow management deals with tech-
problems.                                             niques able to guide data acquisition. In our architecture, this
                                                      can be accomplished by a new operator, Kac, performing an
3.4  Level 3: Impact Assessment                       active classiﬁcation procedure over the Situation concepts.
The Level 3 of the JDL model is responsible for predicting Recall the Situation hierarchy introduced in Section 3.3.
the effects of the system behavior over the entities (devices For each branching level, we assume the availability of a cor-
and users) modeled by the system itself. Each Device is responding epistemic operator K able to provide a description
augmented by a description of its behavior modeled as a state δK useful for the classiﬁcation. Whenever a Situation con-
machine. The purpose of this representation is twofold: (i) cept has a non null set of child concepts, and no information
modeling each device as a ﬂuent, i.e., an entity whose state is available from the corresponding operator, the system can
changes in time, in order to reason at the temporal level; (ii) decide by purpose to query the speciﬁed cognitive agent or
determining expected state values given the actual state and to instantiate an alternate method to obtain the same informa-
inputs, thus being able to detect device faults and react ac- tion, i.e., by requiring a network reconﬁguration through the
cordingly. This design choice can be extended to involve a use of mobile robots.
planning system, embedded in the representation itself, able
to predict the sequence of actions (and, above all, their sup-
posed effects on the environment and users) necessary for the 4 Implementation and Experimental Results
fulﬁllment of a given goal. The system can be implemented in In our actual implementation, {αd} agents exploit the Eche-
practice by deﬁning a new operator Kp to perform the plan- lon LonWorks Fieldbus for distributed control and program-
ning process. Our approach is very similar to the one de- ming. {αc} agents are implemented using ETHNOS, a multi
scribed in [Mastrogiovanni et al., 2004], thus not deserving agent framework developed for distributed Robotics and AmI
further details.                                      applications [Piaggio et al., 2000]. The knowledge base is de-
                                                      veloped embedding in ETHNOS Classic, a Description Logic
3.5  Level 4: Process Reﬁnement                       which guarantees sound and complete subsumption infer-
Level 4 deals with topics ranging from learning to input re- ences. Planning capabilities required by Level 3 are achieved
ﬁnement or inter level information ﬂow management. As pre- using a PDDL compatible planner.
viously discussed in Section 3.3, learning is the focus of ac- The cognitive agents of our framework have been tested in
tual work, so it is not longer discussed here. Input reﬁne- a simulation environment built using the architecture itself. It
ment is managed by our architecture as follows. A simple consists of a niche composed by agents implementing simu-
mechanism, introduced in Section 3.4, is used to infer pos- lated devices and user behaviors, visual interfaces (see Fig.1
sible device faults. If it is the case, or if a particular device on the left), etc. Simulated experimental results are carried
is needed within a particular area (e.g. a gas sensor to detect out by adding speciﬁc agents implementing instances of pat-
gas leaks), the device network can be physically reconﬁgured terns of user activities, and providing sensors with data sets

                                                IJCAI-07
                                                  2157