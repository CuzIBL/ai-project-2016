 AUC: a Statistically Consistent and more Discriminating Measure than Accuracy 

              Charles X. Ling Jin Huang                                             Harry Zhang 
              Department of Computer Science                              Faculty of Computer Science 
             The University of Western Ontario                            University of New Brunswick 
             London, Ontario, Canada N6A 5B7                            Fredericton, NB, Canada E3B 5A3 
                  {ling,jhuang}@csd.uwo.ca                                       hzhang@unb.ca 


                        Abstract                               in training and testing sets, are there better methods than ac•
                                                               curacy to evaluate classifiers that also produce rankings? 
     Predictive accuracy has been used as the main and           The ROC (Receiver Operating Characteristics) curve has 
     often only evaluation criterion for the predictive        been recently introduced to evaluate machine learning algo•
     performance of classification learning algorithms.        rithms [Provost and Fawcett, 1997; Provost et al, 1998]. It 
     In recent years, the area under the ROC (Receiver         compares the classifiers' performance across the entire range 
     Operating Characteristics) curve, or simply AUC,          of class distributions and error costs. However, often there is 
     has been proposed as an alternative single-number         no clear dominating relation between two ROC curves in the 
     measure for evaluating learning algorithms. In this       entire range; in those situations, the area under the ROC curve, 
     paper, we prove that AUC is a better measure than         or simply AUC, provides a single-number "summary" for the 
     accuracy. More specifically, we present rigourous         performance of the learning algorithms. Bradley [Bradley, 
     definitions on consistency and discriminancy in           1997] has compared popular machine learning algorithms us•
     comparing two evaluation measures for learning al•        ing AUC, and found that AUC exhibits several desirable prop•
     gorithms. We then present empirical evaluations           erties compared to accuracy. For example, AUC has increased 
     and a formal proof to establish that AUC is indeed        sensitivity in Analysis of Variance (ANOVA) tests, is inde•
     statistically consistent and more discriminating than     pendent to the decision threshold, and is invariant to a pri•
     accuracy. Our result is quite significant since we        ori class probability distributions [Bradley, 1997]. However, 
     formally prove that, for the first time, AUC is a bet•    no formal arguments or criteria have been established. How 
     ter measure than accuracy in the evaluation of learn•     can we compare two evaluation measures for learning algo•
     ing algorithms.                                           rithms? How can we establish that one measure is better than 
                                                               another? In this paper, we give formal definitions on the con•
                                                               sistency and discriminancy for comparing two measures. We 
1 Introduction                                                 show, empirically and formally, that AUC is indeed a statisti•
The predictive ability of the classification algorithm is typi• cally consistent and more discriminating measure than accu•
cally measured by its predictive accuracy (or error rate, which racy. 
is 1 minus the accuracy) on the testing examples. However,       One might ask why we need to care about anything more 
most classifiers (including C4.5 and Naive Bayes) can also     than accuracy, since by definition, classifiers only classify ex•
produce probability estimations or "confidence" of the class   amples (and do not care about ranking and probability). We 
prediction. Unfortunately, this information is completely ig•  answer this question from two aspects. First, as we discussed 
nored in accuracy. This is often taken for granted since the   earlier, even with labelled training and testing examples, most 
true probability is unknown for the testing examples anyway.   classifiers do produce probability estimations that can rank 
   In many applications, however, accuracy is not enough. For  training/testing examples. Ranking is very important in most 
example, in direct marketing, we often need to promote the     real-world applications. As we establish that AUC is a better 
top X% of customers during gradual roll-out, or we often de•   measure than accuracy, we can choose classifiers with better 
ploy different promotion strategies to customers with differ•  AUC, thus producing better ranking. Second, and more im•
ent likelihood of buying some products. To accomplish these    portantly, if we build classifiers that optimize AUC (instead 
tasks, we need more than a mere classification of buyers and   of accuracy), it has been shown [Ling and Zhang, 2002] that 
non-buyers. We need (at least) a ranking of customers in terms such classifiers produce not only better AUC (a natural con•
of their likelihoods of buying. If we want to achieve a more   sequence), but also better accuracy (a surprising result), com•
accurate ranking from a classifier, one might naturally expect pared to classifiers that optimize the accuracy. To make an 
that we must need the true ranking in the training examples    analogy, when we train workers on a more complex task, they 
[Cohen et a/., 1999]. In most scenarios, however, that is not  will do better on a simple task than workers who are trained 
possible. Instead, what we are given is a dataset of examples  only on the simple task. 
with class labels only. Thus, given only classification labels   Our work is quite significant for several reasons. First, 


LEARNING                                                                                                              519  we establish rigourously, for the first time, that even given 
 only labelled examples, AUC is a better measure (defined in 
 Section 2.2) than accuracy. Second, our result suggests that 
 AUC should replace accuracy in comparing learning algo•
 rithms in the future. Third, our results prompt and allow us       Table 1: An example for calculating AUC with rz 
 to re-evaluate well-established results in machine learning. 
 For example, extensive experiments have been conducted and 
published on comparing, in terms of accuracy, decision tree 
classifiers to Naive Bayes classifiers. A well-established and 
accepted conclusion in the machine learning community is       Table 2: An Example in which two classifiers have the same 
that those learning algorithms are very similar as measured    classification accuracy, but different AUC values 
by accuracy [Kononenko, 1990;Langley et al, 1992;Domin-
gos and Pazzani, 1996]. Since we have established that AUC 
is a better measure, are those learning algorithms still very  the other 5 as negative. If we rank the testing examples ac•
similar as measured by AUC? We have shown [Ling et a/.,        cording to increasing probability of being + (positive), we get 
2003] that, surprisingly, this is not true: Naive Bayes is sig• the two ranked lists as in Table 2. 
nificantly better than decision tree in terms of AUC. These      Clearly, both classifiers produce an error rate of 20% (one 
kinds of new conclusions are very useful to the machine learn• false positive and one false negative), and thus the two classi•
ing community, as well as to machine learning applications    fiers are equivalent in terms of error rate. However, intuition 
(e.g., data mining). Fourth, as a new measure (such as AUC)   tells us that Classifier 1 is better than Classifier 2, since overall 
is discovered and proved to be better than a previous mea•    positive examples are ranked higher in Classifier 1 than 2. If 
sure (such as accuracy), we can re-design most learning al•   we calculate AUC according to Equation 1, we obtain that the 
gorithms to optimize the new measure [Ferri et al, 2002; 
                                                              AUC of Classifier 1 is (as seen in Table 1), and the AUC 
Ling and Zhang, 2002]. This would produce classifiers that 
                                                              of Classifier 2 is . Clearly, AUC does tell us that Classifier 
not only perform well in the new measure, but also in the pre•
                                                               1 is indeed better than Classifier 2. 
vious measure, compared to the classifiers that optimize the 
                                                                 Unfortunately, "counter examples" do exist, as shown in 
previous measure, as shown in [Ling and Zhang, 2002]. This 
                                                              Table 3 on two other classifiers: Classifier 3 and Classifier 4. 
would further improve the performance of our learning algo•
                                                              It is easy to obtain that the AUC of Classifier 3 is , and the 
rithms. 
                                                              AUC of Classifier 4 is . However, the error rate of Classi•
2 Criteria for Comparing Evaluation                           fier 3 is 40%, while the error rate of Classifier 4 is only 20% 
                                                              (again we assume that the threshold for accuracy is set at the 
    Measures for Classifiers                                  middle so that 5 examples are predicted as positive and 5 as 
We start with some intuitions in comparing AUC and accu•      negative). Therefore, a larger AUC does not always imply a 
racy, and then we present formal definitions in comparing     lower error rate. 
evaluation measures for learning algorithms. 

2.1 AUC vs Accuracy 
Hand and Till [Hand and Till, 2001] present a simple ap•
proach to calculating the AUC of a classifier G below.        Table 3: A counter example in which one classifier has higher 
                                                              AUC but lower classification accuracy 
                                                       (1) 
                                                                 Another intuitive argument for why AUC is better than ac•
where n0 and n1 are the numbers of positive and negative ex•  curacy is that AUC is more discriminating than accuracy since 
amples respectively, and where is the rank                    it has more possible values. More specifically, given a dataset 
of ith positive example in the ranked list. Table 1 shows an  with n examples, there is a total of only n + 1 different clas•
example of how to calculate AUC from a ranked list with 5     sification accuracies . On the other hand, 
positive examples and 5 negative examples. The AUC of the 
                                                              assuming there are no positive examples and n1 negative ex-
ranked list in Table 1 is which is 24/25.                                                             different AUC val-
It is clear that AUC obtained by Equation 1 is a measure for the                                    , generally more than 
quality of ranking, as the more positive examples are ranked  n + 1. However, counterexamples also exist in this regard. 
higher (to the right of the list), the larger the term . AUC  Table 4 illustrates that classifiers with the same AUC can have 
is shown to be equivalent to the Wilcoxon statistic rank test different accuracies. Here, we see that Classifier 5 and Clas•
[Bradley, 1997].                                              sifier 6 have the same but different accuracies (60% 
  Intuitively, we can see why AUC is a better measure than    and 40% respectively). In general, a measure with more val•
accuracy from the following example. Let us consider two      ues is not necessarily more discriminating. For example, the 
classifiers, Classifier 1 and Classifier 2, both producing prob• weight of a person (having infinitely many possible values) 
ability estimates for a set of 10 testing examples. Assume that has nothing to do with the number of siblings (having only a 
both classifiers classify 5 of the 10 examples as positive, and small number of integer values) he or she has. 


520                                                                                                          LEARNING Table 4: A counter example in which two classifiers have 
same AUC but different classification accuracies 

   How do we compare different evaluation measures for 
learning algorithms? Some general criteria must be estab•
lished. 
2.2 (Strict) Consistency and Discriminaney of Two 
       Measures 
Intuitively speaking, when we discuss two different measures 
/ and g on evaluating two learning algorithms A and B, we                Figure 1: Illustrations of two measures / and g. A link be•
want at least that / and g be consistent with each other. That           tween two points indicates that the function values are the 
is, when / stipulates that algorithm A is (strictly) better than         same on domain . In (a), / is strictly consistent and more 
B, then g will not say B is better than A. Further, if/ is more          discriminating than g. In (b), / is not strictly consistent or 
discriminating than g, we would expect to see cases where /              more discriminating than g. Counter examples on consistency 
can tell the difference between algorithms A and B but g can•            (denoted by X in the figure) and discriminaney (denoted by Y) 
not.                                                                     exist here 
   This intuitive meaning of consistency and discriminaney 
can be made precise as the following definitions. 
Definition 1 (Consistency) For two measures f, g on do•
main , /, g are (strictly) consistent if there exist no a, b 
such that f (a) > f(b) and g(a) < g(b). 
Definition 2 (Discriminaney) For two measures f, g on do•
main , f is (strictly) more discriminating than g if there exist 
a, b such that f (a) > f(b) and g(a) - g(b), and there 
exist no a, b such that g(a) > g(b) and f(a) — f(b). 
   As an example, let us think about numerical marks and let•
ter marks that evaluate university students. A numerical mark 
gives 100, 99, 98 1, or 0 to students, while a letter mark-
gives A, B, C, D, or F to students. Obviously, we regard A >                There are clear and important implications of these defini•
B > C > D > F. Clearly, numerical marks are consistent with              tions of measures / and g in evaluating two machine learning 
letter marks (and vice versa). In addition, numerical marks              algorithms, say A and B. If / and g are consistent to degree C, 
are more discriminating than letter marks, since two students            then when / stipulates that A is better than B, there is a prob•
who receive 91 and 93 respectively receive different numer•              ability C that g will agree (stipulating A is better than B). If 
ical marks but the same letter mark, but it is not possible to           / is D times more discriminating than g, then it is D times 
have students with different letter marks (such as A and B) but          more likely that / can tell the difference between A and B but 
with the same numerical marks. This ideal example of a mea•              g cannot than that g can tell the difference between A and B 
sure / (numerical marks) being strictly consistent and more              but / cannot. Clearly, we require that C > 0.5 and D > 1 if 
discriminating than another g (letter marks) can be shown in             we want to conclude a measure / is "better" than a measure 
the figure 1(a).                                                         g. This leads to the following definition: 

2.3 Statistical Consistency and Discriminaney of                         Definition 5 The measure f is statistically consistent and 
       Two Measures                                                      more discriminating than g if and only if C > 0.5 and D > 1. 
As we have already seen in Section 2.1, counter examples on              In this case, we say, intuitively, that f is a better measure than 
consistency and discriminaney do exist for AUC and accu•                 9> 
racy. Therefore, it is impossible to prove the consistency and              The statistical consistency and discriminaney is a special 
discriminaney on AUC and accuracy based on Definitions 1                 case of the strict consistency and more discriminaney. For the 
and 2. Figure 1(b) illustrates a situation where one measure             example of numerical and letter marks in the student evalua•
/ is not completely consistent with g, and is not strictly more          tion discussed in Section 2.2, we can obtain that C = 1.0 and 
discriminating than g. In this case, we must consider the prob•          D — oc, as the former is strictly consistent and more discrim•
ability of being consistent and degree of being more discrimi•           inating than the latter. 
nating. What we will define and prove is the probabilistic ver•
sion of the two definitions on strict consistency and discrim•               'it is easy to prove that this definition is symmetric; that is, the 
inaney. That is, we extend the previous definitions to degree            degree of consistency of/ and g is same as the degree of consistency 
of consistency and degree of discriminaney, as follows:                  of g and /. 


LEARNING                                                                                                                                 521    To prove AUC is statistically consistent and more discrim­
inating than accuracy, we substitute / by AUC and g by accu­
racy in the definition above. To simplify our notation, we will 
use AUC to represent AUC values, and ace for accuracy. The 
domain Ω is the ranked lists of testing examples (with n0 pos­
itive and n1 negative examples). Since we require C > 0.5 
and D > 1 we will essentially need to prove: 


                                                              Table 5: Experimental results for verifying statistical consis­
                                                              tency between AUC and accuracy for the balanced dataset 


3 Empirical Verification on AUC and 
    Accuracy 
Before we present a formal proof that AUC is statistically con­
sistent and more discriminating than accuracy, we first present 
an empirical verification. To simplify our notation, we will 
use AUC to represent AUC values, and ace for accuracy. 
  We conduct experiments with (artificial) testing sets to    Table 6: Experimental results for verifying AUC is statis­
verify the statistical consistency and discriminancy between  tically more discriminating than accuracy for the balanced 
AUC and accuracy. The datasets are balanced with equal        dataset 
numbers of positive and negative examples. We test datasets 
with 4, 6, 8, 10, 12, 14, and 16 testing examples. For each 
number of examples, we enumerate all possible ranked lists    measure than accuracy for the balanced datasets, as a measure 
of positive and negative examples. For the dataset with 2n ex­ for learning algorithms. 
amples, there are (2n) such ranked lists. 
  We exhaustively compare all pairs of ranked lists to see how 
they satisfy the consistency and discriminating propositions  4 Formal Proof 
probabilistically. To obtain degree of consistency, we count 
the number of pairs which satisfy "AUC (a) > AUC(b)           In our following discussion, we assume that the domain ^ is 
and acc(a) > acc(b) and the number of pairs which sat­        the set of all the ranked lists with no positive and n1 nega­
isfy "AUC(a) > AUC(b) and ace(a) < acc(b)". We then           tive examples. In this paper, we only study the cases where 
calculate the percentage of those cases; that is, the degree of a ranked list contains an equal number of positive and nega­
consistency. To obtain degree of discriminancy, we count the  tive examples, and we assume that the cutoff for classification 
number of pairs which satisfy "AUC(a) > AUC(b) and            is at the exact middle of the ranked list (each classifier classi­
acc(a) = acc(by\ and the number of pairs which satisfy        fies exactly half examples into the positive class and the other 
"AUC(a) = AUC(b) and aec{a) > orc(6)".                        half into the negative class). That is, n0 = n1, and we use n 
                                                              instead of n  and n . 
  Tables 5 and 6 show the experiment results for the balanced            0       1
dataset. For consistency, we can see (Table 5) that for vari­   Lemma 1 gives the maximum and minimum AUC values 
ous numbers of balanced testing examples, given AUC (a) >     for a ranked list with a fixed accuracy rate. 
AUC(b), the number (and percentage) of cases that satisfy 
aee(a) > ace(b) is much greater than those that satisfy 
ace(a) < aee(b). When n increases, the degree of consis­
tency (C) seems to approach 0.94, much larger than the re­
quired 0.5. For discriminancy, we can see clearly from Table 6 

that the number of cases that satisfy. and Proof: Assume that there are np positive examples correctly 
acc(a) = acc(b) is much more (from 15.5 to 18.9 times more)   classified. Then there are n—np positive examples in negative 
than the number of cases that satisfy ace(a) > acc(b) and     section. According to Equation 1, AUC value is determined 
AUC(a) = AUC(b). When n increases, the degree of dis­         by 5o, which is the sum of indexes of all positive examples. 
criminancy (D) seems to approach 19, much larger than the     So when all n—np positive examples are put on the highest po­
required threshold 1.                                         sitions in negative section (their indexes are n, n -1, • • •, np + 
  These empirical experiments verify empirically that AUC     1), and np positive examples are put on the highest positions in 
is indeed a statistically consistent and more discriminating  positive section (their indexes are 


522                                                                                                          LEARNING AUC reaches the maximum value. So 


Similarly when all positive examples are put on the lowest po­
sitions in both positive and negative sections, AUC reaches 
the minimum value. Thus 
                                                                 Now let us explore the discri mi nancy of AUC and accuracy. 
                                                              Lemma 4 to Lemma 5 are proposed as a basis to prove Theo­
                                                              rem 2. In the following lemmas, stands for the number of 
                                                              examples in ranked list r, and stands for the number of 
                                                              positive examples in r. 


                                                              Proof: For any ranked list r E R1, we switch the examples 
                                                              on position i and n — i, to obtain another unique ranked list 
                                                              s. Next we replace all positive examples in .s by negative ex­
                                                              amples and all negative examples in s by positive examples to 
                                                              obtain ranked list s'. It is obvious that. 
                                                              Since. Thus,. Similarly 
                                                              we can prove Therefore . □ 
Proof: (a) It is straightforward to prove it from Lemma 1. 
(b) For any ranked list r £ R1 we can convert it to another 
unique ranked list v' G R2 by exchanging the positions of 
examples. In the negative section, for any positive example 
whose position is r7, we exchange it with the example in the 
position of n + 1 — ri In the positive section, for any posi­
tive example in position rt, we exchange it with the example 
in the position of n — (r, — n) + 1 + n. It's easy to obtain that 
AUC(r') = AUCmax + AUCmin. Sor' E R2. Thus, we 
have that \Ri\ < |R2|. Similarly, we can prove \r2\ < lR1l-
Therefore 1R1| = |R2|- ° 


LEARNING                                                                                                             523 