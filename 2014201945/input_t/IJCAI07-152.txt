            Automatic Gait Optimization with Gaussian Process Regression

                  Daniel Lizotte, Tao Wang, Michael Bowling, Dale Schuurmans
                                   Department of Computing Science
                                          University of Alberta
                    {dlizotte,trysi,bowling,dale}@cs.ualberta.ca


                    Abstract                          of friction, softness, and height variation (e.g., compare con-
                                                      crete, linoleum, carpet, and grass). Robot platforms them-
    Gait optimization is a basic yet challenging prob- selves also vary due to manufacturing imperfections and gen-
    lem for both quadrupedal and bipedal robots. Al-  eral joint and motor wear. Lastly, even the criterion for de-
    though techniques for automating the process ex-  termining an effective gait is likely to be situation speciﬁc.
    ist, most involve local function optimization pro- Although velocity may seem like the obvious choice, relative
    cedures that suffer from three key drawbacks. Lo- stability of the robot’s sensor hardware can also be important.
    cal optimization techniques are naturally plagued Although tailoring a robot’s gait to the environmental, robot,
    by local optima, make no use of the expensive gait and task speciﬁc circumstances would be ideal, constant man-
    evaluations once a local step is taken, and do not ual re-tuning is impractical. Automatic gait optimization is an
    explicitly model noise in gait evaluation. These  attractive alternative to this laborious manual process.
    drawbacks increase the need for a large number      Although walk learning is not a new idea, existing tech-
    of gait evaluations, making optimization slow, data niques are based on the common approach of local func-
    inefﬁcient, and manually intensive. We present    tion optimization. Hence, existing algorithms also share key
    a Bayesian approach based on Gaussian process     drawbacks including local minima and inefﬁcient use of gait
    regression that addresses all three drawbacks. It evaluations. These drawbacks combine to increase the num-
    uses a global search strategy based on a poste-   ber of gait evaluations required to ﬁnd effective parameters.
    rior model inferred from all of the individual noisy In this work, we propose a different approach based on Gaus-
    evaluations. We demonstrate the technique on a    sian process regression. Not only does the approach not suf-
    quadruped robot, using it to optimize two different fer drawbacks inherent in local methods, but it has the ad-
    criteria: speed and smoothness. We show in both   ditional advantages of providing conﬁdence estimates on the
    cases our technique requires dramatically fewer   gait’s performance, which is useful for exploration, and al-
    gait evaluations than state-of-the-art local gradient lows for a natural inclusion of prior knowledge. We demon-
    approaches.                                       strate the effectiveness of the approach on the Sony AIBO
                                                      quadruped robot. We show that under two separate criteria
                                                      the Gaussian process regression not only ﬁnds effective gait
1  Introduction                                       parameters, but also does so with an order of magnitude fewer
Legged robot platforms offer many advantages over tradi- evaluations than a local gradient competitor.
tional wheeled robots. In addition to their ability to traverse We begin in Section 2 by examining some of the recent
a wide variety of terrain, walking robots are basically a re- approaches to gait optimization. We note the common draw-
quirement for performing useful tasks in our human-centric backs in these approaches as motivation for our proposed
world. Despite these advantages, walking is also one of the technique. In Section 3 we present relevant background ma-
fundamental challenges for legged robots.             terial on Gaussian process regression. We then, in Section 4
  Optimizing a robot’s gait is not a simple control problem. describe how we apply Gaussian processes to the problem
An open loop gait consists of a sequence of joint values for of gait optimization. In Section 5 we show results of our
an already high degree of freedom system of leg joints. Sim- technique applied to two different gait optimization scenar-
pliﬁed parametric representations of leg trajectories can re- ios, demonstrating dramatically more efﬁcient learning than
sult in a manageable number of parameters. However, these both simple and state-of-the-art gait optimization techniques.
parameters will likely have complicated interactions making Finally, we discuss some future extensions of our approach.
manual tuning of gait parameters time-consuming for simple
robots and nearly impossible for the increasing complexity of 2 Motivation
humanoid platforms.                                   The Sony AIBO, a commercially available quadruped robot,
  Even worse, no single gait will be effective in all circum- has spurred recent interest in gait optimization. Its use in
stances. The walking surface is critical and can vary in terms robot soccer within the RoboCup Legged League gives an

                                                IJCAI-07
                                                   944immediate application where gait velocity and uncertain com- competitive walks using only 4,000 evaluations and a total
petition conditions reward well tuned walks. Since the AIBO running time of approximately ﬁve hours distributed across
is our evaluation platform as well, we will examine a number four robots for a total of twenty robot-hours.
of recent approaches pioneered on this robot.           As evaluations are noisy, both approaches must deal with
  A common foundation for all of these approaches, includ- the possibility that an inaccurate evaluation will cause poor
ing ours as well, is the notion of a “walk engine”, or param- gait parameters to incorrectly remain in the population. Both
eterized gait generation system. Since the number of degrees used targeted reevaluation to reduce this possibility, reevalu-
of freedom in legged robots is large, optimizing the angle of ating either the parameters that remained for multiple gener-
each joint at a ﬁnely discretized time scale would involve ations or the ones that performed disproportionately well.
searching over a thousand parameters. The walk engine re-
duces the number of parameters by focusing on leg trajecto- 2.2 Function Optimization
ries that are both physically possible and intuitively plausible. The second family of approaches that has been explored
These parameters usually deﬁne properties of each leg’s tra- involves adapting techniques for multidimensional function
jectory such as the “distance the foot is lifted off the ground” optimization to the gait optimization problem. Kim and
and “the period of the walk”. The space of parameterized Uther [2003] used Powell’s method [Press et al., 1992],
walks obviously has a large impact on the ﬁnal quality of any which performs line search along a chosen search direction
automatic gait optimization, but the optimization problem it- based on the effectiveness of previously chosen search direc-
self is the same regardless of the walk engine. Optimization tions. Kohl and Stone [2004] used a hill climbing approach.
in all cases requires ﬁnding a point in parameter space, rang- Although the gradient is not known, a set of random pertur-
ing from eight to ﬁfty parameters, that results in an effective bations are evaluated empirically, and these evaluations are
gait. All of the cited work below are based on different walk used to approximate the gradient. The parameters are then
engines. Hence, because of the variation in walk engine, sur- adjusted by a ﬁxed step size in the direction of this estimated
faces, and robots themselves, reported walk speeds are largely gradient. Kohl and Stone reported the fastest learning re-
            1
incomparable.                                         sult in the literature, requiring only three hours distributed
  A second common feature of the approaches is the exper- across three robots for a total of nine robot-hours. It is im-
imental setup. All of the approaches involve evaluating spe- portant to note that the reported experiments for both tech-
ciﬁc parameter settings by having the AIBO walk in a struc- niques involved initializing the optimization with a known set
tured arena using the parameters to be evaluated. Local sen- of reasonable parameters. This differs from evolutionary ap-
sor readings on the AIBO–either camera images of visual proaches, which begin with a random initial population.
landmarks or the IR sensor readings of walls–are then used
to compute a noisy estimate of the gait’s average speed. The 2.3 Drawbacks
procedure may be replicated and each estimate averaged to All of the previous approaches share three key drawbacks.
compute a more accurate evaluation.                   First, they can get stuck at local optima. Kim and Uther re-
                                                      ported actual experiences of local optima and Kohl and Stone
2.1  Evolutionary Approaches                          noted the importance of starting from good initial parame-
An evolutionary approach was the ﬁrst proposed method ters, having found considerably poorer performance under
for gait optimization on the AIBO. Hornby et al. [1999; different starting conditions. There are techniques to deal
2000] used a fairly standard evolutionary search on an early with local optima, such as random restarts for local func-
prototype AIBO. A population of parameters were main- tion optimization approaches and radiation for evolutionary
tained at each evolutionary generation. A new population was approaches. However, both involve a considerable increase
formed through mutation and crossover of individuals from in the required number of gait evaluations. Furthermore, the
the previous generation, replacing parameters in the popula- approaches forget previously evaluated gaits once they either
tion that evaluated poorly. Their procedure showed slow gait die out of the population or after the gradient step is taken.
improvement over 500 generations, requiring approximately Not only is this an inefﬁcient use of expensive gait evalu-
twenty-ﬁve robot-hours of learning.                   ations, but certain parameter settings may be unnecessarily
  The evolutionary approach was revisited by Chernova and reevaluated. Finally, none of the approaches explicitly model
Veloso [Chernova and Veloso, 2004]. They used similar mu- the noise in the evaluation process. Hence they all involve
tation and crossover operations to generate candidate gaits. long evaluations or even repeated evaluations that are aver-
Unlike the work of Hornby, their parametric space of walks aged to compute a less noisy estimate. In summary, if the
included a measurement of the possibility that the AIBO goal is to reduce the total number of robot-hours these draw-
could physically perform the gait. This allowed them to throw backs severely hurt the usefulness of the approaches. Our
out poor gait parameters without requiring an empirical eval- approach addresses these disadvantages and consequently re-
uation. In addition, they used a “radiation” procedure to dis- quires considerably fewer gait evaluations.
perse clusters of similar parameters in the population, forcing
further search. They demonstrated that the technique learned 3 Background
  1Despite the lack of basis for comparison, the three most recent At the core of our proposed method is a Gaussian process re-
techniques discussed below all achieve a similar walk speed in the gression model that describes our knowledge about the func-
range of 0.27–0.29 m/s.                               tion we are optimizing. Before discussing its use for gait opti-

                                                IJCAI-07
                                                   945mization we give a brief overview of Gaussian processes and 3.3 Observation Selection
Gaussian process regression.                          Given a Gaussian process model and a set of observations we
                                                      have shown how to incorporate the observations to create the
3.1  Gaussian Processes                               function posterior and then ﬁnd the expected optimum. The
A Gaussian process (GP)  [Williams, 1999; Rasmussen,  remaining challenge, then, is to decide for which points xi
2004; Rasmussen and Williams, 2006] is a collection F of to receive observations. This is, in fact, a sequential decision

random variables Fx1 ,Fx2 , ... for which any ﬁnite subset of making task, as each observation gives additional information
the variables has a joint multivariate Gaussian distribution. that can be used when selecting future points.
The variables are indexed by elements x of a set X .Foranyﬁ-
                                        T
nite length vector of indices x =[x1,x2, ..., xn] ,wehavea Optimal Selection. Assume we have a ﬁxed horizon, i.e.,
                  F     F  ,F  , ..., F T
corresponding vector x =[ x1 x2     xn ] of variables a limited number of remaining observations, the optimal ac-
that has a multivariate Gaussian (or normal) distribution, tion given our model can theoretically be computed. This re-
              F  ∼N{μ    x ,k x, x } ,                quires considering all possible points in the domain, all possi-
               x        ( )  (    )             (1)   ble real-valued outcomes of this function evaluation, and then
where the elements of μ(x) are given by a prior mean func- taking expectations and maximizations for all possible future
tion μ(xi),andk is the kernel function. The kernel takes two observation sequences out to the the horizon. We then make
indices xi and xj , and gives the covariance between their cor- the next observation at the point which maximizes the even-
                                                      tual posterior maximum assuming future optimal decisions.
responding variables Fxi and Fxj . Given vectors of indices
xi and xj, k returns the matrix of covariances between all This enumeration for exploration is discussed in our previ-
                                               F      ous work [Wang et al., 2005], but in domains with signiﬁcant
pairs of variables where the ﬁrst in the pair comes from xi
                  F                 F                 size, it is impractical. Instead, we will focus on myopic search
and the second from xj . Note that each xi is marginally
Gaussian, with mean μ(xi) and variance k(xi,xi).      strategies.

3.2  Gaussian Process Regression                      Most Probable Improvement.   Consider our proposal to
Suppose we have a function f(x) that we would like to op- return the domain point with the largest observed value. If we
timize. Further, suppose that we cannot observe f directly, want to maximize the largest observed value, one approach
                                                      would be to search for points that are likely to be better than
but that we can observe a random variable Fx that is indexed
                                                                                   F +
by the same domain as f and whose expected value is f, i.e., our current best observation. Let x be the maximum ob-
                                                      served value. Choose xmpi to maximize the posterior proba-
∀x ∈X,E[Fx]=f(x). In particular, we assume that our
                                                           P  F mpi >F+
prior belief about the function f conforms to a Gaussian pro- bility ( x x ). We call this the most probable im-
cess with prior mean μ and kernel k, as described above. Sup- provement (MPI) criterion.
                                                        Computing the posterior probability of improvement at a
pose that Fx is an observation of f(x) that has been corrupted
                                                      point xˆ in the domain is a simple computation since the pos-
by zero-mean, i.i.d. Gaussian noise, i.e., Fx = f(x)+,
      ∼N    ,σ2        f x                           terior distribution of Fxˆ is Gaussian as given in Equations 2
where      (0   ). Hence, ( ) is a hidden variable whose                  xmpi
posterior distribution we can infer after observing samples of and 3. Finding the point then requires a search through
                                                      the domain to maximize the probability of improvement. This
Fx at various locations in the domain. The resulting inference
is called Gaussian process regression.                is equivalent to maximizing,
  Let x be the set of observations points and Fx be the result- φ(ˆx)=σ(ˆx|x)/(Fx+    − μ(ˆx|x)).     (4)
ing real-valued observations. We want to compute the poste-
                                                      This criterion for deciding where to evaluate next was sug-
rior distribution of some new point x ∈X. The distribution
                              ˆ                       gested by Mockus [1989]. Fortunately, the posterior mean
will be Gaussian with mean and variance,
                                                      and standard deviation in this ratio are continuous and dif-
   μ x|x       μ x   k x, x k x, x −1 F  − μ x        ferentiable functions, so we can apply function optimization
    (ˆ  )=      (ˆ)+   (ˆ  ) (   )   ( x    ( )) (2)                             mpi
   2                                −1                techniques in the model to ﬁnd x .
  σ (ˆx|x)=k(ˆx,   xˆ) − k(ˆx, x)k(x, x) k(x, xˆ). (3)
Note that the inverse applies to the kernel matrix of observed 4 Gait Optimization
domain points, and so can be computed once and used to eval- Our proposed approach to gait optimization is basically the
uate the posterior at many points in the domain.      application of the above described Gaussian process opti-
  Since the problem is to ﬁnd an optimum of the unknown mization procedure. Like previous approaches, we assume
function, the ﬁnal step is to compute the optimum of the re- that we already have some parameterized walk engine with k
                     R
sulting posterior mean x =argmaxxˆ∈X μ(ˆx|x).This,in  parameters. In addition, we can empirically take noisy evalu-
general, cannot be computed in closed form, and requires the ations of a gait’s velocity (or other feature of the gait). Unlike
application of some technique for function optimization. Al- previous approaches, we model the stochastic velocity func-
though not optimal, another technique is to simply return the tion, f : Rk → R, which maps walk parameters to velocity,

xi from x with the largest observed value Fxi .Thishasthe as a Gaussian process. The exact mean and kernel function
practical side-effect that the technique will not return a point of the Gaussian process are chosen based on prior domain
that has never been evaluated, adding some protection from knowledge. We use the most probable improvement selec-
an incorrect prior.                                   tion rule to decide which parameters to empirically evaluate

                                                IJCAI-07
                                                   946based on the previous observations. After some number of observed velocity to be 0.15 meters per second and within
gait evaluations, the parameters that generated the fastest ob- about 0.2 meters per second 99% of the time.” For observa-
served walk are returned.                             tion noise, our domain expert was not familiar with our par-
  In place of choosing mutation and crossover rates or initial ticular experimental setup. Instead, we computed the sample
parameters and step sizes, our approach requires the speciﬁ- variance of a small number of observations of one particu-
cation of the mean and kernel of the Gaussian process. This lar setting of the walk parameters. This gave us a value of
                                                       2
provides a very natural way to encode domain knowledge, if σ =0.01, which seemed to work well in practice.
it exists, or one can just use “generic” priors. We ﬁrst describe As an interesting test of the stability of our method with
our walk engine and then detail exactly how priors were cho- respect to the model parameters, we also ran the velocity op-
                                                                                             2
sen for our AIBO experiments.                         timization with a prior variance parameter of σf =0.66 (ten
                                                      times larger).
4.1  Walk Engine
As discussed earlier, a complete joint trajectory for a gait
could have thousands of parameters. All gait optimization Prior for Smoothness. Since smoothness had not been
techniques parameterize this walk space using a walk engine. evaluated before, we had no domain expert to consult. In-
For this work, we have used Carnegie Mellon University’s stead, we took a small number of samples (about 30) and used
Tekkotsu2 software to control the AIBO, which includes a sample means and variances to estimate the parameters of the
                                                           μ     −   σ2         σ2    .
walk parameterization (as of release 2.4.1) originating from prior f = 30, f = 100,  =225. As we will show
the Carnegie Mellon CMPack 2002 robot soccer team. This in the next section, even this simple uninformed method for
is an early version of the CMWalk engine used in the work specifying the Gaussian process model can be very effective.
by Chernova et al. [2004]. In consultation with a domain
expert, we identiﬁed 15 walk parameters along with reason- 4.3 Implementation Details
able bounds on each parameter to deﬁne our domain, i.e., In order to compute the most probable improvement point as
X ⊂ R15
        .                                             described in Section 3.3, we used the generic constrained hill
4.2  Priors                                           climber in MATLAB (fmincon) supplied with the function
                                                      and gradient of φ(ˆx) from Equation 4. We used as default
Deﬁning the prior for the Gaussian process means deﬁning a starting points the two best parameters found so far, and 13
            μ  X  → R                  k  X × X →
mean function :       and kernel function :           drawn uniformly random within the bounded domain for a
R                                σ2
 . We also need to specify the variance  of the noise that is total of 15 starting points. In addition, we forced the ﬁrst
                                F
believed to be added to the observation x. The priors that we gait evaluation selection by choosing the center point of the
use have a very simple form. For the mean, we use a constant domain. Since the Gaussian process model starts with a uni-
       μ x    μ
function ( )=  f , which means we have no a priori belief form belief over the domain, all function points are equally
about any speciﬁc parameters’ velocities. For the covariance good to the most expected probable rule.
function, it is hypothesized that, in general, parameter vec-
tors that are close in terms of Euclidean distance are likely
to have similar walk velocities, and therefore a large posi- 5 Results
tive covariance. Furthermore, we expect that some parame- We have applied our Gaussian process approach to two gait
ters may have wide-reaching consequences, and so parame- optimization tasks on the Sony AIBO ERS-7. We ﬁrst look
ters far apart should still have some small positive correlation. at the standard problem of maximizing walk velocity, and we
We therefore chose to use the radial basis function kernel, also examine the problem of optimizing a gait’s smoothness.
                          1     T
                     2  − (xi−xj ) S(xi−xj )          The goal of any gait optimization technique is to ﬁnd a near-
         k(xi,xj )=σf · e 2              ,      (5)
                                                      optimal gait in as few evaluations as possible. Therefore,
where S is a scaling matrix that has along its diagonal the in- we’ll want to compare techniques using this criterion.
verse of the range of each dimension, i.e., the distance from Since previous gait learning has not involved the same walk
one end to the other of one dimension’s range is scaled to
                                                 2    engine, experimental setup, or robots, comparing directly
equal one. We now need to simply choose the constants σ ,
        2                                             with previously reported results can be somewhat problem-
μf ,andσf . Setting these to appropriate values depends on atic. For a more direct comparison, we have implemented the
the feature of the gait to be optimized. In Section 5, we hill climbing method of Kohl and Stone [2004] as described
present results both for optimizing the gait’s velocity and its in Section 2.2 and applied it in identical circumstances as our
smoothness. So we examine each of these cases in turn. approach. The Kohl and Stone algorithm is the most data efﬁ-
                                                      cient technique for the AIBO from the literature, demonstrat-
Prior for Velocity. Gait velocity has been studied relatively ing effective walks with only nine robot-hours of training. We
extensively both on the AIBO and with our particular walk replicated their experimental setup, using 15 random evalua-
engine. In consulting with our domain expert, we easily tions to approximate the gradient at each “iteration” and using
solicited useful prior information. In particular, we chose a step size of 2. The empirical epsilon used in estimating the
               2
μf =0.15  and σf  =0.066, which correspond to the in- gradient was 1/15 of the parameters’ range, which seemed to
tuition, “For some random gait parameters, we expect the be an adequate change in performance. As with the Gaussian
                                                      process model, we started the hill climber from the point in
  2http://www.tekkotsu.org                            the center of the space.

                                                IJCAI-07
                                                   9475.1  Physical Setup                                     Although the results of the hill climbing approach were not
                                                      poor, we were somewhat surprised that the performance was
To evaluate each parameter choice, we had the robot walk be-
                                                      not better. The method tended to take very poor steps once
tween two visual landmarks while focusing the head on the
                                                      it reached a value of about 0.230 m/s. There are two natu-
center of the landmark. The robot determined its distance
                                                      ral explanations. One may simply be local optima, which is
from a landmark as it walked toward it based on the land-
                                                      in line with Kohl and Stone’s noted importance of the initial
mark’s apparent width in the camera’s ﬁeld of view, and that
                                                      parameter vector. Alternatively, based on the frequency of
change is used to estimate velocity. To determine a gait’s
                                                      taking poor gradient steps, the step size may have been too
smoothness, we measured the time-averaged distance from
                                                      large along certain dimensions. Random restarts and a vari-
the center of the landmark to the center of the robot’s ﬁeld
                                                      able step size for each dimension could mitigate these unim-
of view, and negated this. Unstable walks that result in a
                                                      pressive results.
large amount of head movement yield negative smoothness
values, since it is difﬁcult for the robot to keep the head and 5.3 Gait Smoothness
the camera aimed at the target. More ﬂuid walks allow the
robot to aim the camera more directly at the target, resulting A similar graph for gait smoothness is shown in Figure 1(b).
in a smoothness much nearer to zero.                  The Gaussian process optimization found the smoothest walk
  Each observed measurement is the result of three “traver- of all three methods and did so within the ﬁrst 20 observa-
sals” from one landmark to the other. The average time for tions, or only twenty minutes of robot time. Later improve-
three traversals, including time to turn around, was approx- ment was only incremental. In a post-mortem analysis, the
imately one minute. This was chosen to be consistent with gait smoothness task is apparently much simpler. The impact
Kohl and Stone’s hill climbing experiments. We could have of parameters on our measure of smoothness ended up be-
easily only used a single traversal and compensated by in- ing quite simple as a wide range of walks with a short period
creasing the observation variance used in the model.  (below about 310 ms) and a moderate requested walk speed
                                                      (between 210 and 240 m/s) resulted in a smooth gait mea-
5.2  Gait Velocity                                    surement. Scores of such walks were typically greater than
                                                      -10. Due to the independence of this pair of parameters from
A graph showing the result of 321 observations is shown in the rest, it is not unlikely that even random search would ﬁnd
Figure 1(a). We chose this number of observations a priori a smooth gait as choosing parameters in this range will occur
to allow the hill climber 20 “steps” or iterations with 15 test on average every 250 gait evaluations. Our random search
points for each. Both our Gaussian process technique and the trial did happen to ﬁnd one such point, giving it a moderate
Kohl and Stone hill climbing technique are shown, as well as win over the hill climbing technique. Again, we suspect the
the simple baseline of choosing gaits uniformly at random. unimpressive hill climbing result to be due to initial condi-
The solid lines represent the maximum achieved walk speed tions and local maxima.
over the accumulating observations, and the corresponding
isolated markers show the maximum velocity achieved over
the most recent 15 observations.                      6Conclusion
  We found that both the Gaussian process and hill climbing We proposed a new approach to gait learning based on Gaus-
methods performed appreciably better than random evalua- sian process optimization. The approach overcomes many
tions. The best walk velocity found was 0.285 m/s, which drawbacks of previous techniques, effectively avoiding local
was found by the Gaussian process with the over-estimated optima, efﬁciently using all gait evaluations, and explicitly
              2
prior variance, σf =0.66, although the walk found by the modelling noisy observations. Even with all of the caveats
Gaussian process with the “sensibly” initialized variance is associated with comparing gait velocities and training time,
nearly as fast. Despite having already warned of the difﬁcul- we have demonstrated that the approach is not only effec-
ties in comparing walk speeds, note that this speed is com- tive in our high dimensional, noisy, non-convex, optimization
parable to other learned gaits. More impressively, though, is problem, but also requires drastically fewer evaluations. This
the fact this speed was attained after only 120 observations, was also accomplished with a minimum of parameter tun-
which took approximately two robot-hours. This is nearly a ing, demonstrating effective performance when using prior
ﬁve-fold improvement in the required number of robot-hours3 settings from a domain expert, incorrect settings, and even
  It is interesting that the best walks found by the two Gaus- data derived settings.
sian process models are in widely separated parts of the space. There are three main directions we still feel should be ex-
                                     2
The walk found by the sensible setting of σf =0.066 has a plored from this point. First, our model is quite simple in
low period and shorter stride, with a parameter vector far from comparison to Gaussian process models in the recent ma-
the center of the space. On the other hand, the experiment at chine learning literature. Many interesting innovations in-
 2
σf =0.66 found a similarly fast walk near the center of the volving kernel optimization, hyper-parameter models, and di-
space with longer, slower strides that cover a similar distance. mensionality reduction seem well suited to this problem and
                                                      are worth investigating. It would also be compelling to build
  3Although the work of Kohl and Stone used multiple AIBOs in models that incorporate knowledge from previous gait op-
parallel to reduce their total time to three hours, our approach could timization runs with similar robots or surfaces. Second, it
also easily beneﬁt by evaluating multiple sample points simultane- would be interesting to explore how knowledge of an imposed
ously.                                                budget on the number of samples or compute time could be

                                                IJCAI-07
                                                   948