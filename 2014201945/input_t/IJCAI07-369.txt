                              From Sampling to Model Counting
          Carla P. Gomes†       Joerg Hoffmann‡        Ashish Sabharwal†        Bart Selman†

        †Department of Computer Science, Cornell University, Ithaca, NY 14853-7501, U.S.A.∗
                            {gomes,sabhar,selman}@cs.cornell.edu
                 ‡University of Innsbruck, Technikerstraße 21a, 6020 Innsbruck, Austria
                                     joerg.hoffmann@deri.org

                    Abstract                          sample uniformly from the set of solutions, then one can use
                                                      samples to obtain a highly accurate count of the total num-
    We introduce a new technique for counting models  ber of solutions.2 We will explain this strategy below. This
    of Boolean satisﬁability problems. Our approach   approach was exploited in the work on ApproxCount [Wei
    incorporates information obtained from sampling   and Selman, 2005], where a model sampling procedure called
    the solution space. Unlike previous approaches, our SampleSat [Wei et al., 2004] wasusedtoprovidesamplesof
    method does not require uniform or near-uniform   satisfying assignments of the underlying SAT instance. In this
    samples. It instead converts local search sampling setting, the counting is only approximate — with no guaran-
    without any guarantees into very good bounds on   tees on the accuracy — because SampleSat does in general
    the model count with guarantees. We give a formal not sample purely uniformly; it uses Markov Chain Monte
    analysis and provide experimental results showing Carlo (MCMC) methods  [Madras, 2002; Metropolis et al.,
    the effectiveness of our approach.                1953; Kirkpatrick et al., 1983], which often have exponential
                                                      (and thus impractical) mixing times. In fact, the main draw-
1  Introduction                                       back of Jerrum et al.’s counting strategy is that for it to work
                                                      well one needs (near-)uniform sampling, which is a very hard
Boolean satisﬁability (SAT) solvers have been successfully problem in itself. Moreover, biased sampling can lead to ar-
applied in a range of domains, most prominently, in AI plan- bitrarily bad under- or over-estimates of the true count.
ning and hardware and software veriﬁcation. In these appli- The key new insight of this paper is that, somewhat sur-
cations, the basic task is to decide whether a SAT encoding prisingly, using sampling with a modiﬁed strategy, one can
of the underlying problem domain is satisﬁable or not. Given get very good lower-bounds on the total model count, with
the tremendous progress in state of the art SAT solvers and high conﬁdence guarantees, without any requirement on the
their applications, researchers have become interested in pur- quality of the sampling. We will provide both a formal anal-
suing other questions concerning SAT encodings to further ysis of our approach and experimental results demonstrating
extend the reach of SAT technology. For example, can one its practical effectiveness.
randomly sample from the set of satisfying assignments? Or Our strategy, SampleCount, provides provably (proba-
can one count the total number of satisfying assignments? bilistic) guaranteed lower-bounds on the model counts of
In both a formal complexity theoretic sense as well as in Boolean formulas using solution sampling. Interestingly, the
practice, these computational questions are much harder than correctness of the obtained bounds holds even when the sam-
“just” determining satisﬁability versus unsatisﬁability of a pling method used is arbitrarily bad; only the quality of the
formula. Formally, counting the number of solutions is a bounds may go down (i.e., the bound may get farther away
#P-complete problem, making it complete for a complexity from the true count on the lower side). Thus, our strategy re-
class at least as hard as the polynomial-time hierarchy (PH) mains sound even when a heuristic-based practical solution
[         ] 1
Toda, 1989 .  On the positive side, efﬁcient methods for sampling method is used instead of a true sampler.
sampling and counting would open up a wide range of new
                                                        SampleCount  can also be viewed as a way of extend-
applications, e.g., involving various forms of probabilistic
                                                      ing the reach of exact model counting procedures, such as
reasoning [Darwiche, 2005; Roth, 1996; Littman et al., 2001;
                                                      Relsat  [Bayardo Jr. and Pehoushek, 2000] and Cachet
Park, 2002; Sang et al., 2005].
                                                      [Sang et al., 2004].3 More speciﬁcally, SampleCount ﬁrst
  Uniform sampling and model counting are closely related
                                                      uses sampling to select a set of variables of the formula to
tasks. In particular, Jerrum, Valiant, and Vazirani [1986]
                                                      ﬁx. Once a sufﬁcient number of variables have been set,
showed that for many combinatorial problems, if one can

  ∗Research supported by Intelligent Information Systems Insti- 2Conversely, one can also design a uniform solution sampler us-
tute (IISI), Cornell University (AFOSR grant F49620-01-1-0076) ing a solution counter.
and DARPA (REAL grant FA8750-04-2-0216).                 3These exact counters also provide a lower-bound on the true
  1The class NP is the ﬁrst of an inﬁnite sequence of levels in PH. model count if they are aborted before terminating.

                                                IJCAI-07
                                                  2293the remaining formula can be counted using an exact model rate counts in expectation. The key strength of SampleCount
counter. From the exact residual model count and the num- is that it works no matter how biased the model sampling is,
ber of ﬁxed variables, we can then obtain a lower-bound on thereby circumventing the main drawback of approach (a).
the total number of models. As our experiments will show, Instead of using the sampler to select the variable setting and
these bounds can be surprisingly close to optimal. Moreover, compute a multiplier, we use the sampler only as a heuristic
on many problem classes our method scales very well. To to determine in what order to set the variables. In particu-
mention just one example, we consider a circuit synthesis for- lar, we use the sampler to select a variable whose positive
mula from the literature, called 3bitadd 32.cnf. This for- and negative setting occurs most balanced in our set of sam-
mula can be solved quite easily with a local search style SAT ples (ties are broken randomly). Note that such a variable
solver, such as Walksat [McAllester et al., 1997].However, will have the highest possible multiplier (closest to 2) in the
it is out of reach for all but the most recent DPLL style solvers SampleCount setting discussed above. Informally, setting
(MiniSat [E´en and S¨orensson, 2005] takes almost two hours the most balanced variable will divide the solution space most
to ﬁnd a single solution). Consequently, it is completely out evenly (compared to setting one of the other variables). Of
of reach of exact model counters based on DPLL. The origi- course, as we noted, our sampler may be heavily biased and
nal formula has nearly 9,000 variables. SampleCount sets we therefore cannot really rely on the observed ratio between
around 3,000 variables in around 30 minutes, with the re- positive and negative settings of a variable. Interestingly, we
maining formula solved by Cachet in under two minutes. can simply set the variable to a randomly selected truth value
The overall lower-bound on the model count — with 99% and use the multiplier 2. This strategy will still give — in ex-
conﬁdence — thus obtained is an astonishing 101339 models. pectation — the true model count. A simple example shows
We see that the formula has a remarkable number of truth as- why this is so. Consider our formula above and assume x1 oc-
signments, yet exact counters cannot ﬁnd any models after curs most balanced in our sample. Let the model count of F+
over 12 hours of run time.                            be 2M/3 and of F− be M/3. If we select with probability 1/2
                                                      to set x1 to True, we obtain a total model count of 2 × 2M/3,
1.1  Main Idea                                        i.e., too high; but, with probability 1/2, we will set the vari-
To provide the reader with an intuitive understanding of our able to False, obtaining a total count of 2×M/3, i.e., too low.
approach, we ﬁrst give a high-level description of how sam- Overall, our expected (average) count will be exactly M.
pling can be used to count. We will then discuss how this can Technically, the expected total model count is correct be-
be made to work well in practice.                     cause of the linearity of expectation. However, we also
  (a) From Sampling to Counting [Jerrum et al., 1986]. Con- see that we may have signiﬁcant variance between speciﬁc
sider a Boolean formula F with M satisfying assignments. counts, especially since we are setting a series of variables
Assuming we could sample these satisfying assignments uni- in a row (obtaining a sequence of multipliers of 2), until we
formly at random, we can measure the fraction of all models have a simpliﬁed formula that can be counted exactly. In fact,
                      +
that have x1 set to True, M , by taking the ratio of the num- in practice, the total counts distributions (over different runs)
ber of assignments in the sample that have x1 set to True over are often heavy-tailed [Kilby et al., 2006]. To mitigate the
the sample size. This fraction will converge with increasing ﬂuctuations between runs, we use our samples to select the
sample size to the true fraction of models with x1 set posi- best variables to set next. Clearly, a good heuristic would be
tively, γ = M+/M. (For now, assume that γ > 0.) It follows to set such “balanced” variables ﬁrst. We use SampleSat
immediately that M = (1/γ)M+. We will call 1/γ the “mul- to get guidance on ﬁnding such balanced variables. The ran-
tiplier” (> 0). We have thus reduced the problem of counting dom value setting of the selected variable leads to an expected
the models of F to counting the models of a simpler formula, model count that is equal to the actual model count of the
F+. We can recursively repeat the process, leading to a se- formula. We show how this property can be exploited us-
ries of multipliers, until all variables are assigned or until we ing Markov’s inequality to obtain lower-bounds on the total
can count the number of models of the remaining formulas model count with predeﬁned conﬁdence guarantees. These
with an exact counter. For robustness, one usually sets se- guarantees can be made arbitrarily strong by repeated itera-
lected variable to the truth value that occurs more often in tions of the process.
the sample. This also avoids the problem of having γ = 0and We further boost the effectiveness of our approach by using
therefore an inﬁnite multiplier. (Note that the more frequently variable “equivalence” when no single variable appears suf-
occurring truth value gives a multiplier of at most 2.) ﬁciently balanced in the sampled solutions. For instance, if
  (b) ApproxCount    [Wei  and  Selman,  2005].In     variables x1 and x2 occur with the same polarity (either both
ApproxCount, the above strategy is made practical by  positive or both negative) in nearly half the sampled solutions
using a SAT solution sampling method called SampleSat and with a different polarity in the remaining, we randomly
[Wei et al., 2004]. Unfortunately, there are no guarantees replace x2 with either x1 or x1, and simplify. This turns out to
on the uniformity of the samples from SampleSat.Al-   have the same positive effect as setting a single variable, but is
though the counts obtained can be surprisingly close to the more advantageous when no single variable is well balanced.
true model counts, one can also observe cases where the Our experimental results demonstrate that in practice,
method signiﬁcantly over-estimates or under-estimates. Our SampleCount provides surprisingly good lower-bounds —
experimental results will conﬁrm this behavior.       with high conﬁdence and within minutes — on the model
  (c) SampleCount. Our approach presented here uses a counts of many problems which are completely out of reach
probabilistic modiﬁcation of strategy (a) to obtain true accu- of current exact counting methods.

                                                IJCAI-07
                                                  22942  Preliminaries                                        Params:integerst,z; real α > 0
                                                        Input :ACNFformulaF  over n variables
Let V be a set of propositional (Boolean) variables that take Output : A lower-bound on the model count of F
value in the set {0,1}. We think of 1 as True and 0 as False. begin
Let F be a propositional formula over V. A solution or model minCount ← 2n
of F (also referred to as a satisfying assignment for F)isa for iteration ← 1 to t do
                                                               G ← F
0-1 assignment to all variables in V such that F evaluates to   ←
1. Propositional Satisﬁability or SAT is the decision problem  s  0
                                                               until G becomes feasible for ExactModelCount do
of determining whether F has any models. This is the canon-       s ← s + 1
ical NP-complete problem. In practice, one is also interested     S ← SampleSolutions(G,z)
in ﬁnding a model, if there exists one. Propositional Model       u ← GetMostBalancedVar(S)
Counting is the problem of computing the number of models         (v,w) ← GetMostBalancedVarPair(S)
for F. This is the canonical #P-complete problem. It is the-      r ← a random value chosen uniformly from {0,1}
oretically believed to be signiﬁcantly harder than SAT, and       if balance(u) ≥ balance(v,w) then
also turns out to be so in practice.                                 set u to r in G
  The correctness guarantee for our algorithm relies on ba-       else
                                                                     if r = 0 then replace w with v in G
sic concepts from probability theory. Let X and Y be two             else replace w with ¬v in G
discrete random variables. The expected value of X, denoted
E[ ]             [ =  ]                                           Simplify(G)
  X , equals ∑x xPr X x .Theconditional expectation of                 −α
                 E[  |  ]                                      count ← 2s · ExactModelCount(G)
X given Y , denoted X Y , is a function of Y deﬁned as               <
       E[  |  =  ]=∑      [ =   |  =  ]                        if count minCount then
follows: X  Y   y     x xPr X  x Y   y . We will need             minCount ← count
the following two results whose proof may be found in stan-
                                                           return Lower-bound: minCount
dard texts on probability theory.                       end
Proposition 1 (The Law of Total Expectation). For any dis-        Algorithm 1: SampleCount
crete random variables X and Y, E[X]=E[E[X | Y]].
                                                                         ( , )
Proposition 2 (Markov’s inequality). For any random vari- selects a variable pair v w whose same and different occur-
able X, Pr[X > k E[X]] < 1/k.                         rences are as balanced as possible. Here “same occurrence”
                                                      in a solution means that either both v and w appear positively
                                                      or they both appear negatively. If u is at least as balanced
3  Model Counting Using Solution Sampling             as (v,w), SampleCount uniformly randomly sets u to0or
This section describes our sample-based model counting al- 1inG. Otherwise, it uniformly randomly replaces w with
                                                                ¬
gorithm, SampleCount (see Algorithm 1). The algorithm either v or v, forcing a literal equivalence. Now G is sim-
has three parameters: t, the number of iterations; z, the num- pliﬁed by unit propagating this variable restriction, it is tested
ber of samples for each variable setting; and α, the “slack” again for the possibility of exact counting, and, if necessary,
factor, which is a positive real number. The input is a formula the sampling and simpliﬁcation process is repeated. Once G
F over n variables. The output is a lower-bound on the model comes within the range of the exact counting subroutine after
                                                      s simpliﬁcation steps, the number of remaining models in G
count of F. SampleCount is a randomized algorithm with                              s−α
a high probability of success. We will shortly formalize its is computed. This, multiplied with 2 , is the ﬁnal count for
success probability quantitatively as Theorem 1.      this iteration. After t iterations, the minimum of these counts
  SampleCount  is formally stated as Algorithm 1 and de- is reported as the lower-bound.
scribed below. SampleCount performs t iterations on F   By setting variables and equivalences, the original formula
and ﬁnally reports the minimum of the counts obtained in is reduced in size. By doing this repeatedly, we eventually
these iterations. Within an iteration, it makes a copy G reach a formula that can be counted exactly. SampleCount
of F and, as long as G is too hard to be solved by an provides a practically effective way of selecting the variables
exact model counting subroutine,4 does the following. It to set or the equivalence to assert. Each time it looks for the
calls SampleSolutions(G,z) to sample up to z solutions most evenly way to divide the set of remaining solutions. By
of G; if the sampling subroutine times out, it may re- picking one of the two subsets randomly, we obtain real guar-
turn fewer than z samples.5 Call this set of solutions S. antees (see Section 3.1) and the formula is eventually sufﬁ-
GetMostBalancedVar(S)   selects a variable u of G whose ciently simpliﬁed to enable an exact count. The sampling is
positive and negative occurrences in S are as balanced as pos- used to attempt an as evenly as possible division of the solu-
sible, i.e., as close to |S|/2 each as possible. Ties are bro- tion space. The better this works, the better our lower-bound.
ken randomly. Similarly, GetMostBalancedVarPair(S)    But we don’t need any guarantees on the sampling quality;
                                                      we will still obtain a valid, non-trivial lower-bound.
  4
   This is decided based on the number of remaining variables 3.1 Correctness Analysis of SampleCount
or other natural heuristics. A common strategy is to run an exact
counter for a pre-deﬁned amount of time [Wei and Selman, 2005]. We now analyze SampleCount as a randomized algorithm
  5Even when no samples are found, the procedure can continue and show that the probability that it provides an incorrect
by selecting a random variable. Of course, the quality of the bound lower-bound on an input formula decreases exponentially to
obtained will suffer.                                 zero with the number of iterations, t, and the slack factor, α.

                                                IJCAI-07
                                                  2295Somewhat surprisingly, as we discuss below, the bound on This implies that the conditional expectation of count given
                                                                          s−α            s−α
the error probability we obtain is independent of the number s is E[count | s]=E[2 ∑σ Yσ | s]=2 ∑σ E[Yσ | s]=
                                                       s−α                  s−α     −s   −α
z of samples used and the quality of the samples (i.e., how 2 ∑σ Pr[Yσ = 1 | s]=2 ∑σ 2 = 2  ∑σ 1. Since F
                                                           ∗                                ∗
uniformly they are distributed in the solution space). has 2s solutions, we have E[count | s]=2s −α . Applying
                          ( , ,α)                     the law of total expectation, E[count]=E[E[count | s]] =
Theorem  1. For parameters t z  , the lower-bound re-     ∗−α     ∗−α
turned by SampleCount is correct with probability at least E 2s = 2s .
    −α                                                                                              ∗ 
1 − 2 t independent of the number and quality of solution Finally, using Markov’s inequality, Pr count > 2s <
                                                                 ∗
samples.                                              E[count]/2s = 2−α . This proves that the error probability in
                                                                                 −α
           ∗                                          any single iteration is at most 2 (in fact, strictly less than
          s , ∗ > ,                                    −α
Proof. Let 2 s   0 be the true model count of F. Sup- 2   ). From our argument at the beginning of this proof, the
pose SampleCount  returns an incorrect lower-bound, i.e., overall probability of error after t iterations is less than 2−αt.
            s∗                        s∗
minCount > 2 . This implies that count > 2 in all of the Since we did not use any property of the number or quality of
t iterations. We will show that this happens in any single iter- samples, the error bound holds independent of these.
ation with probability at most 2−α . By probabilistic indepen-
dence of the t iterations, the overall error probability would We end with a discussion of the effect of the number of
then be at most 2−αt, proving the theorem.            samples, z,onSampleCount. It is natural to expect more
  Fix any iteration of SampleCount. When executing the samples to lead to a “better” bound at the cost of a higher
algorithm, as variables of G are ﬁxed or replaced with an- runtime. Note, however, that z does not factor into our formal
other variable within the inner (until-do) loop, G is repeat- result above. This is, in fact, one of the key points of this
edly simpliﬁed and the number of variables in it is reduced. paper, that we provide guaranteed bounds without making
For the analysis, it is simpler to consider a closely related for- any assumptions whatsoever on the quality of the sampling
mula ρ(G) over all n variables. At the start of the iteration, process or the structure of the formula. Without any such
ρ(G)=G   = F. However, within the inner loop, instead of assumptions, there is no reason for more samples to guide
ﬁxing a variable u to 0 or 1 or replacing w with v or ¬v and SampleCount towards a better lower-bound. In the worst
simplifying (as we did for G), we add additional constraints case, a highly biased sampler could output the same small set
u = 0, u = 1, w = v,orw = ¬v, respectively, to ρ(G). Clearly, of solutions over and over again, making more samples futile.
at any point during the execution of SampleCount,thesolu- However, in practice, we do gain from any sampling pro-
tions of ρ(G) are isomorphic to the solutions of G;everyso- cess that is not totally biased. It guides us towards balanced
lution of G uniquely extends to a solution of ρ(G) and every variables whose true multipliers are close to 2, which reduces
solution of ρ(G) restricted to the variables of G is a solution probabilistic ﬂuctuations arising from randomly ﬁxing the se-
of G. In particular, the number of solutions of G is always the lected variables. Indeed, under weak uniformity-related as-
same as that of ρ(G). Further, every solution of ρ(G) is also sumptions on the sampler and assuming the formula has a
a solution of F.                                      mix of balanced and imbalanced variables, a higher number
  Let G denote the ﬁnal formula G on which the subroutine of samples will reduce the variation in the lower-bound re-
ExactModelCount(G)   is applied after s variable restric- ported by SampleCount over several runs.
tions. Note that s itself is a random variable whose value is
determined by which variables are restricted to which random 4 Experimental Results
value and how that propagates to simplify G so that its resid-
ual models may be counted exactly. Consider the variant of We conducted experiments on a cluster of 3.8 GHz Intel
                      ρ( ) ρ( )                    Xeon machines with 2GB memory per node running Linux.
G deﬁned on all n variables, G . G is a random formula The model counters used were SampleCount, Relsat ver-
determined by F and the random bits used in the iteration. Fi- sion 2.00 with counting, Cachet version 1.2 extended to re-
nally, the variable count for this iteration is a random variable port partial counts, and ApproxCount version 1.2. Both
              s−α                      ρ( )
whose value is 2 times the model count of G .Weare    SampleCount  and ApproxCount internally use SampleSat
interested in the behavior of count as a random variable.
                                                     for obtaining solution samples. SampleCount was created
  Recall that every solution of ρ(G) is also a solution of F. by modifying ApproxCount to ignore its sample-based mul-
For every solution σ of F,letYσ be an indicator random tipliers, ﬁx the most balanced variables at random, and ana-
variable which is 1 iff σ is a solution of G. Then, count = lyze equivalences, and by creating a wrapper to perform mul-
 s−α
2   ∑σ Yσ . We will compute the expected value of count us- tiple iterations (t) with the speciﬁed slack factor α.
ing the law of total expectation: E[count]=E[E[count | s]]. In all our experiments with SampleCount, α and t were
  Fix s. For each σ,Pr[Yσ = 1 | s] equals the probability that set so that αt = 7, giving a correctness conﬁdence of 1 −
each of the s variable restrictions within the inner loop is con- 2−7 = 99% (see Theorem 1). t ranged from 1 to 7 so as to
sistent with σ, i.e., if σ has u = 0thenu is not restricted to 1, keep the runtimes of SampleCount well below two hours,
if σ has v = w then w is not replaced with ¬v, etc. Because while the other model counters were allowed a full 12 hours.
of the uniformly random value r used in the restrictions, this The number of samples per variable setting, z, was typically
                            1
happens with probability exactly /2 in each restriction. Note chosen to be 20. Our results demonstrate that SampleCount
that the s restrictions set or replace s different variables, and is quite robust even with so few samples. Of course, it can
are therefore probabilistically independent with respect to be- be made to produce even better results with more samples
                                                −
ing consistent with σ. Consequently, Pr[Yσ = 1 | s]=2 s. of better quality, or by using a “buckets” strategy that we will

                                                IJCAI-07
                                                  2296 Table 1: Performance of SampleCount compared with exact counters and with an approximate counter without guarantees.
                          SampleCount                    Exact Counters                ApproxCount
                          (99% conﬁdence)         Relsat             Cachet          (without guarantees)
   Instance True Count
                          Models     Time      Models     Time    Models     Time     Models      Time
    CIRCUIT SYNTH.
  2bitmax 62.1× 1029   ≥ 2.4× 1028   29 sec   2.1× 1029  66 sec   2.1× 1029  2sec   ≈ 5.6× 1028    8sec
  3bitadd 32  —        ≥ 5.9× 101339 32 min   —12hrs—12hrs≈                           7.3× 10941 43 min
    RANDOM   k-CNF
  wff-3-3.5 1.4× 1014  ≥ 1.6× 1013   4min     1.4× 1014   2hrs    1.4× 1014  7min   ≈ 8.4× 1013   11 sec
  wff-3-1.5 1.8× 1021  ≥ 1.6× 1020   4min    ≥ 4.0× 1017 12 hrs   1.8× 1021  3hrs   ≈ 9.3× 1018    8sec
  wff-4-5.0   —        ≥ 8.0× 1015   2min    ≥ 1.8× 1012 12 hrs ≥ 1.0× 1014  12 hrs ≈ 4.2× 1015   11 sec
     LATIN SQUARE
   ls8-norm 5.4× 1011  ≥ 3.1× 1010   19 min  ≥ 1.7× 108  12 hrs ≥ 1.9× 107   12 hrs ≈ 2.7× 1012    5sec
   ls9-norm 3.8× 1017  ≥ 1.4× 1015   32 min  ≥ 7.0× 107  12 hrs ≥ 1.7× 107   12 hrs ≈ 9.5× 1017   11 sec
  ls10-norm 7.6× 1024  ≥ 2.7× 1021   49 min  ≥ 6.1× 107  12 hrs ≥ 2.4× 107   12 hrs ≈ 2.1× 1027   22 sec
  ls11-norm 5.4× 1033  ≥ 1.2× 1030   69 min  ≥ 4.7× 107  12 hrs ≥ 1.2× 107   12 hrs ≈ 5.1× 1040   1min
  ls12-norm   —        ≥ 6.9× 1037   50 min  ≥ 4.6× 107  12 hrs ≥ 1.5× 107   12 hrs ≈ 1.8× 1051   8min
  ls13-norm   —        ≥ 3.0× 1049   67 min  ≥ 2.1× 107  12 hrs ≥ 2.0× 107   12 hrs ≈ 4.1× 1064  12 min
  ls14-norm   —        ≥ 9.0× 1060   44 min  ≥ 2.6× 107  12 hrs ≥ 1.5× 107   12 hrs ≈ 2.3× 1089  18 min
  ls15-norm   —        ≥ 1.1× 1073   56 min   —12hrs≥             9.1× 106   12 hrs ≈ 5.6× 10115   2hrs
  ls16-norm   —        ≥ 6.0× 1085   68 min   —12hrs≥             1.0× 107   12 hrs ≈ 5.4× 10123  2.5hrs
   LANGFORD PROBS.
  lang-2-12 1.0× 105   ≥ 4.3× 103    32 min   1.0× 105   15 min   1.0× 105   4hrs   ≈ 3.7× 105   1.5min
  lang-2-15 3.0× 107   ≥ 1.0× 106    60 min  ≥ 1.8× 105  12 hrs ≥ 1.1× 105   12 hrs ≈ 7.4× 1010  23 min
  lang-2-16 3.2× 108   ≥ 1.0× 106    65 min  ≥ 1.8× 105  12 hrs ≥ 1.0× 105   12 hrs ≈ 6.3× 1010   6min
  lang-2-19 2.1× 1011  ≥ 3.3× 109    62 min  ≥ 2.4× 105  12 hrs ≥ 1.1× 105   12 hrs ≈ 1.2× 1014  12 min
  lang-2-20 2.6× 1012  ≥ 5.8× 109    54 min  ≥ 1.5× 105  12 hrs ≥ 1.0× 105   12 hrs ≈ 9.9× 1015  24 min
  lang-2-23 3.7× 1015  ≥ 1.6× 1011   85 min  ≥ 1.2× 105  12 hrs ≥ 8.4× 104   12 hrs ≈ 1.3× 1020  75 min
  lang-2-24   —        ≥ 4.1× 1013   80 min  ≥ 4.1× 105  12 hrs   —12hrs≈             1.3× 1022   1.5hrs
  lang-2-27   —        ≥ 5.2× 1014  111 min  ≥ 1.1× 104  12 hrs   —12hrs≈             1.7× 1033    3hrs
  lang-2-28   —        ≥ 4.0× 1014  117 min  ≥ 1.1× 104  12 hrs   —12hrs≈             2.3× 1026    2hrs

brieﬂy outline in Section 5. On the other hand, ApproxCount on ApproxCount even though the comparison is not really
often signiﬁcantly under- or over-estimated the number of meaningful as ApproxCount does not provide any correct-
solutions with 20 samples. We therefore allowed it around ness guarantees. Indeed, it, for example, under-estimates the
100 samples per variable setting in all our runs, except for count by at least 10398 on 3bitadd 32, and over-estimates
the very easy instances (circuit synthesis and random for- by 107 (with an increasing trend) on the Latin square for-
mulas) where it used 1000 samples. Other parameters of mulas. (Of course, there are also classes of formulas where
ApproxCount  were set so as to obtain the desired number of ApproxCount appears to count quite accurately when given
samples in a reasonable amount of time from SampleSat.A good quality samples.) We discuss the results in detail below.
local search “cutoff” between 2,000 and 20,000 was sufﬁcient The circuit synthesis formulas are for ﬁnding minimal size
for most problems, while the Langford instances required a circuits for a given Boolean function. These are known to
cutoff of 100,000 to obtain enough samples. Finally, both quickly become very difﬁcult for DPLL style procedures.
SampleCount  and ApproxCount  were set to call Cachet The instance 2bitmax 6 is still easy for exact model count-
when typically between 50 and 350 variables remained unset. ing procedures, and SampleCount also gets a very good
  Table 1 summarizes our main results, where we evaluate lower-bound quickly. 3bitadd 32, on the other hand, was
our approach on formulas from four domains: circuit syn- only recently solved for a single solution using MiniSat in
thesis, random k-CNF, Latin square, and Langford problems. about 6,000 seconds. 3bitadd 32 is certainly far beyond
We see that SampleCount scales well with problem size the reach of exact counting. The total solution count for
and provides good high-conﬁdence lower-bounds close to the this formula is astonishing; SampleCount reports a lower-
true counts, in most cases within an hour. It clearly out- bound of 5.9 × 101339. Note that the formula has close to
performs exact counters, which almost always time out after 9,000 variables and therefore the solution set is still an expo-
12 hours, providing counts that are several orders of magni- nentially small fraction of the total number of assignments.
tude below those of SampleCount. We also report results SampleCount sets around 3,000 variables in around 30 min-

                                                IJCAI-07
                                                  2297