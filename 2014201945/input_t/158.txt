           Definition and Complexity of Some Basic Metareasoning Problems* 

                                   Vincent Conitzer and Tuomas Sandholm 
                                            Carnegie Mellon University 
                                           Computer Science Department 
                                                 5000 Forbes Avenue 
                                             Pittsburgh, PA 15213, USA 
                                         {conitzer,sandholm} @cs.cmu.edu 

                        Abstract                              Because limited time (or other resources) prevent the agent 
                                                              from performing all potentially useful deliberation (or infor•
     In most real-world settings, due to limited time or 
                                                              mation gathering) actions, it has to select among such actions. 
     other resources, an agent cannot perform all poten•
                                                              Reasoning about which deliberation actions to take is called 
     tially useful deliberation and information gathering 
                                                              metareasoning. Decision theory [7,10] provides a norma•
     actions. This leads to the metareasoning problem 
                                                              tive basis for metareasoning under uncertainty, and decision-
     of selecting such actions. Decision-theoretic meth•
                                                              theoretic deliberation control has been widely studied in AI 
     ods for metareasoning have been studied in AI, but 
                                                              (e.g., [2,4-6,8,9,12-15,18-20]). 
     there are few theoretical results on the complexity 
                                                                 However, the approach of using metareasoning to control 
     of metareasoning. We derive hardness results for 
                                                              reasoning is impractical if the metareasoning problem itself 
     three settings which most real metareasoning sys•
                                                              is prohibitively complex. While this issue is widely acknowl•
     tems would have to encompass as special cases. 
                                                              edged (e.g., [8,12-14]), there are few theoretical results on 
     In the first, the agent has to decide how to allo•
                                                              the complexity of metareasoning. 
     cate its deliberation time across anytime algorithms 
                                                                 We derive hardness results for three central metareason•
     running on different problem instances. We show 
                                                              ing problems. In the first (Section 2), the agent has to de•
     this to be ATP-complete. In the second, the agent 
                                                              cide how to allocate its deliberation time across anytime al•
     has to (dynamically) allocate its deliberation or in•
                                                              gorithms running on different problem instances. We show 
     formation gathering resources across multiple ac•
                                                               this to be NP-complcte. In the second metareasoning prob•
     tions that it has to choose among. We show this 
                                                               lem (Section 3), the agent has to (dynamically) allocate its de•
     to be AfP-hard even when evaluating each individ•
                                                               liberation or information gathering resources across multiple 
     ual action is extremely simple. In the third, the 
                                                              actions that it has to choose among. We show this to be MV-
     agent has to (dynamically) choose a limited num•
                                                              hard even when evaluating each individual action is extremely 
     ber of deliberation or information gathering actions 
                                                              simple. In the third metareasoning problem (Section 4), the 
     to disambiguate the state of the world. We show 
                                                              agent has to (dynamically) choose a limited number of delib•
     that this is AfP-hard under a natural restriction, and 
                                                              eration or information gathering actions to disambiguate the 
                hard in general. 
                                                              state of the world. We show that this is NP-hard under a 
                                                              natural restriction, and PSPACE-hard in general. 
1 Introduction                                                   These results have general applicability in that most metar•
In most real-world settings, due to limited time, an agent can• easoning systems must somehow deal with one or more of 
not perform all potentially useful deliberation actions. As   these problems (in addition to dealing with other issues). We 
a result it will generally be unable to act rationally in the also believe that these results give a good basic overview of 
world. This phenomenon, known as bounded rationality, has     the space of high-complexity issues in metareasoning. 
been a long-standing research topic (e.g., [3,17]). Most of 
that research has been descriptive: the goal has been to char• 2 Allocating anytime algorithm time across 
acterize how agents—in particular, humans—deal with this           problems 
constraint. Another strand of bounded rationality research    In this section we study the setting where an agent has to 
has the normative (prescriptive) goal of characterizing how   allocate its deliberation time across different problems—each 
agents should deal with this constraint. This is particularly of which the agent can solve using an anytime algorithm. We 
important when building artificial agents.                    show that this is hard even if the agent can perfectly predict 
  Characterizing how an agent should deal with bounded ra•    the performance of the anytime algorithms. 
tionality entails determining how the agent should deliberate. 
                                                              2.1 Motivating example 
   *The material in this paper is based upon work supported by the 
National Science Foundation under CAREER Award IRI-9703122,   Consider a newspaper company that has, by midnight, re•
Grant IIS-9800994, ITR IIS-0081246, and ITR nS-0121678.       ceived the next day's orders from newspaper stands in the 3 


RESOURCE-BOUNDED REASONING                                                                                          1099  cities where the newpaper is read. The company owns a fleet             problem instances to get a total performance of at least K; 
 of delivery trucks in each of the cities. Each fleet needs its           that is, whether there exists a vector with 
 vehicle routing solution by 5am. The company has a default 
 routing solution for each fleet, but can save costs by improv•           i 
 ing (tailoring to the day's particular orders) the routing solu•
                                                                            A reasonable approach to representing the performance 
 tion of any individual fleet using an anytime algorithm. In this 
                                                                          profiles is to use piecewise linear performance profiles. They 
 setting, the "solution quality" that the anytime algorithm pro•
                                                                          can model any performance profile arbitrarily closely, and 
 vides on a fleet's problem instance is the amount of savings 
 compared to the default routing solution.                                have been used in the resource-bounded reasoning litera•
                                                                          ture to characterize the performance of anytime algorithms 
    We assume that the company can perfectly predict the sav•
 ings made on a given fleet's problem instance as a function of           (e.g. [2]). We now show that the metareasoning problem is 
 deliberation time spent on it (we will prove hardness of metar-         AfP-complete even under this restriction. We will reduce 
 easoning even in this deterministic variant). Such functions             from the KNAPSACK problem.2 
 are called (deterministic) performance profiles [2,6,8,9,20]. 
 Each fleet's problem instance has its own performance pro•

 file.1 Suppose the performance profiles are as shown in Fig. 1. 


                                                                         Theorem 1 PERFORMANCE-PROFILES is NP-complete 
                                                                         even if each performance profile is continuous and piecewise 

                                                                         linear.3. 

                                                                         Proof: The problem is in MV because we can nondctermin-
                                                                         istically generate the in polynomial time (since we do not 
                                                                         need to bother trying numbers greater than N), and given the 
                                                                              we can verify if the target value is reached in polyno•
                                                                         mial time. To show NP-hardness, we reduce an arbitrary 
                                                                         KNAPSACK instance to the following PERFORMANCE-
   Figure 1: Performance profiles for the routing problems. 

 Then the maximum savings we can obtain with 5 hours of 
 deliberation time is 2.5, for instance by spending 3 hours on 
 instance 1 and 2 on instance 2. On the other hand, if we had 
 until 6am to deliberate (6 hours), we could obtain a savings 
of 4 by spending 6 hours on instance 3. 

2.2 Definitions and results 
 We now define the metareasoning problem of allocating de•
 liberation across problems according to performance profiles. 

Definition 1 (PERFORMANCE-PROFILES) We are 

given a list of performance profiles (fi, f2, • • •, fm) (where 
each f, is a nondecreasing function of deliberation time, 
mapping to nonnegative real numbers), a number of de•
liberation steps N, and a target value K. We are asked 
whether we can distribute the deliberation steps across the 

    'Because the anytime algorithm's performance differs across in•          2This only demonstrates weak NP-completencss, as KNAP•
stances, each instance has its own performance profile (in the set•      SACK is weakly NP-complete; thus, perhaps pseudopolynomial 
ting of deterministic performance profiles). In reality, an anytime      time algorithms exist. 

algorithm's performance on an instance cannot be predicted per•              3If one additionally assumes that each performance profile is 
fectly. Rather, usually statistical performance profiles are kept that   concave, then the metareasoning problem is solvable in polynomial 
aggregate across instances. In that light one might question the as•     time [2]. While returns to deliberation indeed tend to be diminish•
sumption that different instances have different performance pro•        ing, usually this is not the case throughout the performance profile. 
files. However, sophisticated deliberation control systems can con•      Algorithms often have a setup phase in the beginning during which 
dition the performance prediction on features of the instance—and        there is no improvement. Also, iterative improvement algorithms 
this is necessary if the deliberation control is to be fully normative.  can switch to using different local search operators once progress has 
(Research has already been conducted on conditioning performance         ceased using one operator (for example, once 2-swap has reached a 
profiles on instance features [8,9,15] or results of deliberation on     local optimum in TSP, one can switch to 3-swap and obtain gains 
the instance so far [4,8,9,15,18-20].)                                   from deliberation again) [16]. 


1100                                                                                             RESOURCE-BOUNDED REASONING                            So we have found a solution to      is no silver, the test will be positive with probability 0. This 
                                                               test takes 3 units of time. (3) Test for copper at C. If there 
the PERFORMANCE-PROFILES instance. On the other                is copper, the test will be positive with probability l;if there 
hand, suppose there is a solution to the PERFORMANCE-          is no copper, the test will be positive with probability 0. This 
                                                              test takes 2 units of time. 
                                                                 Given the probabilities of the tests turning out positive un•
                                                              der various circumstances, one can use Bayes' rule to com•
                                                              pute the expected utility of each digging option given any 
                                                              (lack of) test result. For instance, letting be the event that 


                                                              utility is Doing a similar analysis everywhere, we can rep•
                                                              resent the problem by trees shown in Fig. 2. In these trees, be-

   The PERFORMANCE-PROFILES problem occurs natu•
rally as a subproblem within many metareasoning problems, 
and thus its complexity leads to significant difficulties for 
metareasoning. This is the case even under the (unrealistic) 
assumption of perfect predictability of the efficacy of deliber•
ation. On the other hand, in the remaining two metareasoning 
problems that we analyze, the complexity stems from uncer•
tainty about the results that deliberation will provide. 

3 Dynamically allocating evaluation effort 
                                                              Figure 2: Tree representation of the action evaluation in•
    across options (actions)                                  stance. 
In this section we study the setting where an agent is faced 
with multiple options (actions) from which it eventually has  ing at the root represents not having done a test yet, whereas 
to choose one. The agent can use deliberation (or information being at a left (right) leaf represents the test having turned 
gathering) to evaluate each action. Given limited time, it has out positive (negative); the value at each node is the expected 
to decide which ones to evaluate. We show that this is hard   value of digging at this site given the information correspond•
even in very restricted cases.                                ing to that node. The values on the edges are the probabilities 
                                                              of the test turning out positive or negative. We can subse•
3.1 Motivating example 
                                                              quently use these trees for analyzing how we should gather 
Consider an autonomous robot looking for precious metals. It  information. For instance, if we have 5 units of time, the op•
can choose between three sites for digging (it can dig at most timal information gathering policy is to test at B first; if the 
one site). At site A it may find gold; at site B, silver; at site result is positive, test at A; otherwise test at C. (We omit the 
C, copper. If the robot chooses not to dig anywhere, it gets  proof because of space constraint.) 
utility 1 (for saving digging costs). If the robot chooses to dig 
somewhere, the utility of finding nothing is 0; finding gold, 3.2 Definitions 
5; finding silver, 3; finding copper, 2. The prior probability of In the example, there were four actions that we could eval•
there being gold at site A is 1/8, that of finding silver at site B uate: digging for a precious metal at one of three locations, 
is 1/2, and that of finding copper at site C is 1/2.          or not digging at all. Given the results of all the tests that 
  In general, the robot could perform deliberation or infor•  we might undertake on a given action, executing it has some 
mation gathering actions to evaluate the alternative (digging) expected value. If, on the other hand, we do not (yet) know 
actions. The metareasoning problem would be the same for      all the results of these tests, we can still associate an expected 
both, so for simplicity of exposition, we will focus on in•   value with the action by taking an additional expectation over 
formation gathering only. Specifically, the robot can perform the outcomes of the tests. In what follows, we will drop the 
tests to better evaluate the likelihood of there being a precious word "expected" in its former meaning (that is, when talk•
metal at each site, but it has only limited time for such tests. ing about the expected value given the outcomes of all the 
The tests are the following: (1) Test for gold at A. If there tests), because the probabilistic process regarding this expec•
is gold, the test will be positive with probability if there  tation has no relevance to how the agent should choose to test. 
is no gold, the test will be positive with probability 1/15. This Hence, all expectations are over the outcomes of the tests. 
test takes 2 units of time. (2) Test for silver at B. If there   While we have presented this as a model for information 
is silver, the test will be positive with probability 1; if there gathering planning, we can use this as a model for planning 


RESOURCE-BOUNDED REASONING                                                                                          1101  (computational) deliberation over multiple actions as well. In takes its first evaluation step on action J, and gives maximal 
 this case, we regard the tests as computational steps that the expected utility among online evaluation control policies that 

 agent can take toward evaluating an action.4                  spend at most N units of effort. (If at the end of the deliber•
   To proceed, we need a formal model of how evaluation        ation process, we are at node for tree then our utility is 
 effort (information gathering or deliberation) invested on a  max because we will choose the action with the 
 given action changes the agent's beliefs about that action.   highest expected value.) 
 For this model, we generalize the above example to the case 
 where we can take multiple evaluation steps on a certain ac•  3.3 Results 
 tion (although we will later show hardness even when we can 
                                                               We now show that even a severely restricted version of this 
 take at most one evaluation step per action). 
                                                               problem is NP-hard.5 
 Definition 3 An action evaluation tree is a tree with 
                                                               Theorem 2 ACTION-EVALUATION is NP-hard, even when 
   • A root r, representing the start of the evaluation;       all trees have depth either 0 or I, branching factor 2, and all 

   • For each nonleafnode w, a cost kwfor investing another    leaf values are -I, 0, or I. 
     step of evaluation effort at this point; 
   • For each edge e between parent node p and child node 
     c, a probability of transitioning from p to c 
     upon taking a step of evaluation effort at p; 
   • For each leaf node value 
   According to this definition, at each point in the evalua•
tion of a single action, the agent's onlv choice is whether to 
invest further evaluation effort, but not how to continue the 
evaluation. This is a reasonable model when the agent does 
evaluation through deliberation and has one algorithm at its 
disposal. However, in general the agent may have different 
information gathering actions to choose from at a given point 
in the evaluation, or may be able to choose from among sev•
eral deliberation actions (e.g., via search control [1,141). In 
Section 4, we will discuss how being able to choose between 
tests may introduce drastic complexity even when evaluating 
a single thing. In this section, however, our focus is on the  vations about the constructed ACTION-EVALUATION in•
complexities introduced by having to choose between differ•    stance. First, once we determine the value of a action to 
ent actions on which to invest evaluation effort next.         be 1, choosing this action is certainly optimal regardless of 
                                                               the rest of the deliberation process. Second, if at the end of 
   The agent can determine its expected value of an action, 
                                                               the deliberation process we have not discovered the value of 
given its evaluation so far, using the subtree of the action 
                                                               any action to be 1, then for any of the trees of depth 1, ei•
evaluation tree that is rooted at the node where evaluation has 
                                                               ther we have discovered the corresponding action's value to 
brought us so far. This value can be determined in that sub•
                                                               be -1, or we have done no deliberation on it at all. In the 
tree by propagating upwards from the leafs: for parent p with 
                                                               latter case, the expected value of the action is always below 
a set of children C, we have 
                                                               0 is carefully set to achieve this). Hence, we will pick 
   We now present the metareasoning problem. In general,       action 3 for value 0. It follows that an optimal deliberation 
the agent could use an online evaluation control policy where  policy is one that maximizes the probability of discovering 
the choices of how to invest future evaluation effort can de•  that a action has value 1. Now, consider the test set of a pol•
pend on evaluation results obtained so far. However, to avoid  icy, which is the set of actions that the policy would evaluate 
trivial complexity issues introduced by the fact that such a   if no action turned out to have value 1. Then, the probabil•
contingency strategy for evaluation can be exponential in size, ity of discovering that a action has value 1 is simply equal 
we merely ask what action the agent should invest its first    to the probability that at least one of the actions in this set 
evaluation step on.                                            has value 1. So, in this case, the quality of a policy is de•
                                                               termined by its test set. Now we observe that any optimal 
Definition 4 (ACTION-EVALUATION) We are given I ac•            action is either the one that only evaluates action 2 (and then 
tion evaluation trees, indexed 1 through I, corresponding to   runs out of deliberation time), or one that has action 1 in its 
I different actions. (The transition processes of the trees are 
independent.) Additionally, we are given an integer N. We         5ACTION-EVALUATION is trivial for / = 1: the answer is 
are asked whether, among the online evaluation control poli•   "yes" if it is possible to take a step of evaluation. The same is true if 
cies that spend at most N units of effort, there exists one that there is no uncertainty with regard to the value of any action; in that 
                                                               case any evaluation is irrelevant. 

   4For this to be a useful model, it is necessary that updating be• 6Note that using m in the exponent does not make the reduction 
liefs about the value of an action (after taking a deliberation step) is exponential in size, because the length of the binary representation 
computationally easy relative to the evaluation problem itself. of numbers with in the exponent is linear in 


1102                                                                                RESOURCE-BOUNDED REASONING test set. (For consider any other policy; since evaluating ac• over a hole, or simply walk away. If the gap turns out to be 
tion 1 has minimal cost, and gives strictly higher probability a staircase and the robot descends down it, this gives utility 
of discovering a action with value 1 than evaluating on any    2. If it turns out to be a hole and the robot jumps over it, this 
other action besides 2, simply replacing any other action in   gives utility 1 (discovering new floors is more interesting). If 
the test set with action 1 is possible and improves the pol•   the robot walks away, this gives utility 0 no matter what the 
icy.) Now suppose there is a solution to the KNAPSACK          gap was. Unfortunately, attempting to jump over a staircase 
instance, that is, a set S such that and or canyon, or trying to descend into a hole or canyon, has the 
                                                               disastrous consequence of destroying the robot (utility ). 
              Then we can construct a policy which has as      It follows that if the agent cannot determine with certainty 
                                                               what the gap is, it should walk away. 
test set (Evaluating all these 
actions costs at most deliberation units.) The proba•            In order to determine the nature of the gap, the robot can 
bility of at least one of these actions having value 1 is at least conduct various tests (or queries). The tests can determine 
the probability that exactly one of them has value 1, which is the answers to the following questions: (1) Am I inside a 
                                                               building? A yes answer is consistent only with S\ a no answer 
                                                               is consistent with 5, H, C. (2) If I drop a small item into the 
                                                               gap, do I hear it hit the ground? A yes answer is consistent 
                         Using our previous observation we 
                                                               with S, H\ a no answer is consistent with H, C. (3) Can 1 
can conclude that there is an optimal action that has action 1 
                                                               walk around the gap? A yes answer is consistent with S, H; 
in its test set, and since the order in which we evaluate ac•
                                                               a no answer is consistent with S, H, C. 
tions in the test set does not matter, there is an optimal policy 
which evaluates action 1 first. On the other hand, suppose       Assume that if multiple answers to a query are consistent 
                                                               with the true state of the gap, the distributions over such an•
there is no solution to the KNAPSACK instance. Consider a 
                                                               swers are uniform and independent. Note that after a few 
policy which has 1 in its test set, that is, the test set can be 
                                                               queries, the set of states consistent with all the answers is 
expressed as for some set Then 
we must have and since there is no solution to                 the intersection of the sets consistent with the individual an•
                                                               swers; once this set has been reduced to one element, the 
the KNAPSACK instance, it follows that But robot knows the state of the gap. 
                                                                 Suppose the agent only has time to run one test. Then, 
the probability that at least one of the actions in the test set 
                                                               to maximize expected utility, the robot should run test 1, be•
has value 1 is at most                                         cause the other tests give it no chance of learning the state of 
                                                               the gap for certain. Now suppose that the agent has time for 
             On the other hand, 
                                                               two tests. Then the optimal test policy is as follows: run test 
                                     If we now observe that 
                                                               2 first; if the answer is yes, run test 1 second; otherwise, run 
                                                 it follows 
                                                               test 3 second. (If the true state is 5, this is discovered with 
                                                               probability if it is H, this is discovered with probability 
that the policy of just evaluating action 2 is strictly better. So, 
                                                               so total expected utility is Starting with test 1 or test 3 can 
there is no optimal policy which evaluates action 1 first. 
                                                               only give expected utility 
   We have no proof that the general problem is in It 
is an interesting open question whether stronger hardness re•
sults can be obtained for it. For instance, perhaps the general 4.2 Definitions 
problem is -complete.                                          We now define the metareasoning problem of how the agent 
                                                               should dynamically choose queries to ask (deliberation or in•
4 Dynamically choosing how to disambiguate                     formation gathering actions to take) so as to disambiguate the 
    state                                                      state of the world. While the illustrative example above was 
                                                               for information gathering actions, the same model applies to 
We now move to the setting where the agent has only one       deliberation actions for state disambiguation (such as image 
thing to evaluate, but can choose the order of deliberation (or processing, auditory scene analysis, sensor fusing, etc.). 
information gathering) actions for doing so. In other words, 
the agent has to decide how to disambiguate its state. We     Definition 5 (STATE-DISAMBIGUATION) We are given 
show that this is hard. (We consider this to be the most sig•
                                                                                             of possible world states;
nificant result in the paper.)                                                                                      1 
                                                                   A probability function p over 
4.1 Motivating example 
Consider an autonomous robot that has discovered it is on the    7If there are two situations that are equivalent from the agent's 
                                                              point of view (the agent's optimal course of action is the same and 
edge of the floor; there is a gap in front of it. It knows this gap 
                                                              the utility is the same), then we consider those situations to be one 
can only be one of three things: a staircase (5), a hole (i/), or state. Note that two such situations may lead to different answers 
a canyon (C) (assume a uniform prior distribution over these). to the queries. For example, one situation may be that the gap is 
The robot would like to continue its exploration beyond the   an indoor staircase, and another situation may be that the gap is an 
gap. There are three courses of physical action available to  outdoor staircase. These situations are considered to be the same 
the robot: attempt a descent down a staircase, attempt to jump state, but will give different answers to the query "Am I inside?". 


RESOURCE-BOUNDED REASONING                                                                                          1103 