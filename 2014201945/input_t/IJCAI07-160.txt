                             Neighborhood MinMax Projections               ∗

                        Feiping Nie1, Shiming Xiang2, and Changshui Zhang2
                      State Key Laboratory of Intelligent Technology and Systems,
                Department of Automation, Tsinghua University, Beijing 100080, China
                   1nfp03@mails.tsinghua.edu.cn;   2{xsm, zcs}@mail.tsinghua.edu.cn


                    Abstract                          Chen et al., 2000; Yu and Yang, 2001]. However, these vari-
                                                      ants of LDA discard a subspace and thus some important dis-
    A new algorithm, Neighborhood MinMax Projec-      criminative information may be lost. Another drawback in
    tions (NMMP), is proposed for supervised dimen-   LDA is its distribution assumption. LDA is optimal in the
    sionality reduction in this paper. The algorithm  case that the data distribution of each class is Gaussian, which
    aims at learning a linear transformation, and fo- can not always be satisﬁed in real world applications. When
    cuses only on the pairwise points where the two   the class distribution is more complex than Gaussian, LDA
    points are neighbors of each other. After the trans- may fail to ﬁnd the optimal discriminative directions. More-
    formation, the considered pairwise points within  over, the number of available projection directions in LDA is
    the same class are as close as possible, while those smaller than the class number [Duda. et al., 2000],butitmay
    between different classes are as far as possible. We be insufﬁcient for many complex problems, especially when
    formulate this problem as a constrained optimiza- the number of class is small.
    tion problem, in which the global optimum can be
    effectively and efﬁciently obtained. Compared with
                                                        For the distance metric based classiﬁcation methods, such
    the popular supervised method, Linear Discrimi-
                                                      as the nearest neighbor classiﬁer, learning an appropriate
    nant Analysis (LDA), our method has three signif-
                                                      distance metric plays a vital role. Recently, a number of
    icant advantages. First, it is able to extract more
                                                      methods have been proposed to learn a Mahalanobis dis-
    discriminative features. Second, it can deal with
                                                      tance metric [Xing et al., 2003; Goldberger et al., 2005;
    the case where the class distributions are more com-
                                                      Weinberger et al., 2006]. Linear dimensionality reduction
    plex than Gaussian. Third, the singularity problem
                                                      can be viewed as a special case of learning a Mahalanobis dis-
    existing in LDA does not occur naturally. The per-
                                                      tance metric(see section 5). This viewpoint can give a reason-
    formance on several data sets demonstrates the ef-
                                                      able interpretation for the fact that the performance of nearest
    fectiveness of the proposed method.
                                                      neighbor classiﬁer can always be improved after performing
                                                      linear dimensionality reduction.
1  Introduction
                                                        In this paper, we propose a new supervised linear di-
Linear dimensionality reduction is an important method when
                                                      mensionality reduction method, Neighborhood MinMax Pro-
facing with high-dimensional data. Many algorithms have
                                                      jections (NMMP). The method is largely inspired by the
been proposed during the past years. Among these algo-
                                                      classical supervised linear dimensionality reduction method,
rithms, Principal Component Analysis (PCA) [Jolliffe, 2002]
                                                      i.e., LDA, and the recent proposed distance metric learning
and Linear Discriminant Analysis (LDA) [Fukunaga, 1990]
                                                      method, large margin nearest neighbor (LMNN) classiﬁca-
are two of the most widely used methods. PCA is an unsu-
                                                      tion [Weinberger et al., 2006]. In our method, we focus
pervised method, which does not take the class information
                                                      only on the pairwise points where the two points are neigh-
into account. LDA is one of the most popular supervised di-
                                                      bors of each other. After the transformation, we try to pull
mensionality reduction techniques for classiﬁcation. How-
                                                      the considered pairwise points within the same class as close
ever, there exist several drawbacks in it. One drawback is that
                                                      as possible, and take those between different classes apart.
it often suffers from the Small Sample Size problem when
                                                      This goal can be achieved by formulating the task as a con-
dealing with high dimensional data. In this case, the within-
                                                      strained optimization problem, in which the global optimum
class scatter matrix Sw may become singular, which makes
                                                      can be effectively and efﬁciently obtained. Compared with
LDA difﬁcult to be performed. Many approaches have been
                                                      LDA, our method avoids the three drawbacks in LDA dis-
proposed to address this problem [Belhumeur et al., 1997;
                                                      cussed in above. Compared with the LMNN method, our
  ∗This paper is Funded by Basic Research Foundation of Ts- method is computationally much more efﬁcient. The perfor-
inghua National Laboratory for Information Science and Technol- mance on several data sets demonstrates the effectiveness of
ogy (TNList).                                         our method.

                                                IJCAI-07
                                                   993                                                      where
                                                                      
     N (i)                       N (i)
      w                           b                     ˜                                          T
                N (i)                       N (j)       Sb                         xi − xj xi − xj
                 w                           b             =                      (       )(      )   (5)
                                                             i,j:xi∈Nb(Cj )&xj ∈Nb(Ci)
         A    B                     A     B
                                                                         ˜
                                                        Here, Ci = Cj,andSb is positive semi-deﬁnite too.
                                                        To achieve our goal, we should maximize sb while mini-
          (a)                         (b)             mize sw. The following two function can be used as objec-
                                                      tive:                                  
                                                                M   W     tr WT   S˜ − λS˜ W
Figure 1: In the left ﬁgure, point A and B belong to the same     1(  )=         ( b     w)           (6)
class i, and the two circles denote the within-class neighbor-
                                                                                    T ˜
hood of A and B respectively. A is B’s within-class neighbor-                  tr(W  SbW)
                                                                    M2  W
hood and B is A’s within-class neighborhood. After the trans-          (   )=      T ˜                (7)
                                                                              tr(W   SwW)
formation, we try to pull the two points as close as possible;
In the right ﬁgure, point A belongs to class i, point B be- As it is difﬁcult to determine a suitable weight λ for the for-
longs to class j, and the two circles denote the between-class mer objective function, we select the latter as our objective to
neighborhood of A and B respectively. A is B’s between-class optimization. In fact, as we will see, the latter is just a spe-
neighborhood and B is A’s between-class neighborhood. Af- cial case of the former, where the weight λ is automatically
ter the transformation, we try to push the two points as far as determined.
possible.                                               Therefore, we formulate the problem as a constrained op-
                                                      timization problem:
2  Problem Formulation                                                            tr WT S˜ W
                                             d                   W∗                 (     b  )
Given the data matrix X  x1, x2, ..., xn , xi ∈ R , our              =arg   max                       (8)
                      =[             ]                                    WT W=I  tr WT S˜ W
goal is to learn a linear transformationW : Rd → Rm,where                          (     w   )
       d×m       T
W  ∈ R     and W   W  = I. I is m × m identity matrix. Fortunately, the globally optimal solution of this problem can
Then the original high-dimensional data x is transformed into be efﬁciently calculated. In the next section, we will describe
a low-dimensional vector:                             the details for solving this constrained optimization problem.
                           T
                    y = W   x                   (1)
                                                      3   The Constrained Optimization Problem
  Let each data point of class i have two kinds of neighbor-
                                                      We address the above optimization problem in a more general
hood: within-class neighborhood Nw(i) and between-class
                                                      form which is described as follows:
neighborhood Nb(i),whereNw(i)  is the set of the data’s
kw(i) nearest neighbors in the same class i and Nb(i) is the The constrained optimization problem: Given the real sym-
                                                      metric matrix A ∈ Rd×d and the positive semi-deﬁnite ma-
set of the data’s kb(i) nearest neighbors in the class other than d×d
i            ≤ k  i  ≤ n −         ≤ k  i ≤ n − n     trix B ∈  R    , rank(B)=r      ≤  d.  Find a matrix
. Obviously, 1  w( )    i  1,and1     b( )       i,          d×m
     n                        i                       W  ∈  R    that maximize the following objective function
where  i is the data number of class .                                  WT  W    I
  Here, we focus only on the pairwise points where the two with the constraint of = :
points are neighbors of each other. After the transformation,                     tr WT  AW
                                                                 W∗                 (        )
we hope that the distance of the considered pairwise points          =arg   max        T              (9)
                                                                           WT W=I tr(W   BW)
within the same class will be minimized, while the distance
of those between different classes will be maximized. (see At ﬁrst, we propose Lemma 1, which shows that when
Figure 1).                                            WT  W  = I and m>d−    r,thevalueoftr(WT  BW)   will
  After the transformation W, the sum of the Euclidean dis- not be equal to zero.
tances of the pairwise points within the same class can be Lemma1. Suppose W ∈ Rd×m, WT W = I, B ∈ Rd×d is
formulated as:                                        a positive semi-deﬁnite matrix, and rank(B)=r ≤ d, m >
                          T ˜                                                  T
                sw = tr(W   SwW)                (2)   d − r, then it holds that tr(W BW) > 0.
     tr ·                                               proof. According to the result of Rayleigh quotient[Golub
where  ( ) denotes the trace operator of matrix, and                                  T            m
                                                      and van Loan, 1996],  min  tr(W   BW)=          βi,
                                                                           T                      i=1
 S˜                          x  − x   x − x  T                            W  W=I
  w =                       (  i   j)( i    j)  (3)   where β1,β2,...,βm  are the ﬁrst m smallest eigenvalues
       i,j:xi∈Nw(Cj )&xj ∈Nw (Ci)                     of B.AsB    is positive semi-deﬁnite, rank B r,and
                                                                    m                      (  )=
                                                      m>d−    r,then  i=1 βi > 0. Therefore, with the constraint
  Here, Ci denote the class label of xi,andCj denote the   T            T                 T
                                      ˜               of W  W  = I, tr(W  BW)  ≥  min tr(W  BW)  > 0.
class label of xj. Obviously, Ci = Cj,andSw is positive
                                                        Thus we discuss this optimization problem in two cases.
semi-deﬁnite.
                                                        Case 1: m>d−   r,
  Similarly, the sum of the Euclidean distances of the pair-
                                                        Lemma 1 ensures that the optimal value is ﬁnite in this case.
wise points between different classes is:                                       ∗
                                                      Suppose the optimal value is λ ,Guo[2003] has derived that
                                                                  T       ∗
                 s   tr WT  S˜ W                        max  tr(W   (A − λ B)W)=0.
                  b =  (     b  )               (4)   WT W=I

                                                IJCAI-07
                                                   994  Note that tr(WT BW)   >  0, so we can easy to see,    Now, we obtain an iterative algorithm for obtaining the op-
 max   tr(WT (A  − λB)W)    <  0  ⇒  λ>λ∗,and         timal solution, which is described in Table 1. From the al-
  T
W  W=I                                                gorithm we can see, only a few iterative steps are needed to
 max   tr(WT (A  − λB)W)  > 0 ⇒  λ<λ∗.
WT W=I                                                obtain a precise solution. Note that the algorithm need not
  On the other hand, max  tr(WT (A  − λB)W)=γ,        calculate the inverse of B, and thus the singularity problem
                   WT W=I
     γ                   m                    A  −    does not exist in it naturally.
where  is the sum of the ﬁrst largest eigenvalues of    Case 2: m ≤ d − r,
λB. Given a value λ,ifγ =0,then λ is just the optimal value,              W
        γ>           λ                                  In this case, when    lies in the null space of ma-
otherwise    0 implies is smaller than the optimal value trix B,thentr(WT BW)=0, the value of the objec-
and vice versa. Thus the global optimal value of the problem tive function becomes inﬁnite. Therefore, we can rea-
can be obtained by an iterative algorithm. Subsequently, in sonably replace the optimization problem with V∗ =
order to give a suitable value λ, we need to determine the          T   T                    (d−r)×m
                                                      arg max   tr(V (Z  AZ)V),whereV     ∈ R       ,and
possible bound of the optimal value. Theorem 1 is proposed VT V=I
to solve this problem.                                Z =[z1, z2, ..., zd−r] are the eigenvectors corresponding to
  Theorem 1. Given the real symmetric matrix A ∈ Rd×d d − r zero eigenvalues of B.
                                    d×d                                   ∗
and the positive semi-deﬁnite matrix B ∈ R , rank(B)=   We   know  that V      =[μ1,   μ2, ..., μm],where
                 d×m1         d×m2
r ≤ d.IfW1   ∈ R     , W2  ∈ R     and m1  >m2   >    μ1, μ2, ..., μm are the ﬁrst m largest eigenvectors of
                  tr(WT AW )           tr(WT AW )       T                                     ∗       ∗
                      1   1                2    2     Z  AZ. So, in this case, the ﬁnal solution is W = Z · V
d − r,then max        T      ≤   max       T     .
           T      tr(W1 BW1)     T     tr(W2 BW2)
          W1 W1=I              W2 W2=I
  The proof of Theorem 1 is based on the following lemma: Input:
                                a1   a2         ak
  Lemma 2.If∀i, ai ≥ 0,bi > 0 and  ≤    ≤ ··· ≤   ,                                   d×d
                                b1   b2         bk        The real symmetric matrix A ∈ R and the positive
    a1+a2+···+ak ≤ ak                                                          d×d
then b1+b2+···+bk bk .                                  semi-deﬁnite matrix B ∈ R , rank(B)=r  ≤  d.The
            ak
                 q    ∀i, ai ≥ ,bi >          ai ≤                  
  Proof.Letbk =   .So         0      0,wehave           error constant .
            a1+a2+···+ak  ak
qbi                     ≤
  . Therefore b1+b2+···+bk bk                           Output:
  Now we give the proof of Theorem 1 in the following.                    W∗         W∗    ∈  Rd×m
                               ∗                          Projection matrix  ,where                 and
                           W        w , w , ..., w        ∗T   ∗
  Proof of theorem 1. Suppose 1  =[  1   2     m1 ]     W   W   =  I.
                        tr(WT AW )
    W∗                      1   1       Cm2      h
and   1  =arg    max       T      .Let    m1  =   ,                  m>d−    r
                 T     tr(W1 BW1)                       Inthecaseof :         .
               W1 W1=I
                                      T                           tr(A)        α1+α2+···+αm       λ1+λ2
                                  tr(W   AW p(1))         1.λ1 ←      , λ2 ←              , λ ←        ,
                                      p(1)       ≤                tr(B)        β1+β2+···+βm         2
without loss of generality, we suppose tr(WT BW )
                                      p(1)  p(1)        where α1,α2, ..., αm are the ﬁrst m largest eigenvalues
    T                     T
tr(W   AW p(2))       tr(W   AW p(h))                     A  β ,β , ..., β          m
    p(2)      ≤···≤       p(h)                          of  , 1  2     m are the ﬁrst smallest eigenvalues
tr(WT  BW    )        tr(WT  BW    )
    p(2)  p(2)            p(h)  p(h)                    of B.
        W      ∈   Rd×m2         i
  where    p(i)           is the -th combination of       2.While λ2 − λ1 >,do
w , w , ..., w     m                  m   >m
  1  2     m1 with   2 elements(note that 1   2), so                    γ       γ                    m
the number of combinations is h.                            a) Calculate ,where   is the sum of the ﬁrst
       m2−1                                             largest eigenvalues of A − λB.
  Let Cm −1 = l, note that each of wj(1 ≤ j ≤ m1) occurs
        1                                                       γ>         λ ←  λ     λ  ← λ
l times in {Wp(1), Wp(2), ..., Wp(h)}. According to Lemma   b) If   0,then  1    ,else 2    .
2,wehave                                                    c) λ ← λ1+λ2 .
              T                      ∗T    ∗                         2
          tr(W1 AW 1)          l·tr(W1 AW1  )
    max   tr(WT BW  )    =     l·tr(W ∗T BW ∗)   =          End while.
  WT W  =I    1    1                1     1
    1  1                                                    ∗
    T              T                T                     W      ν1, ν2, ..., νm ,whereν1, ν2, ..., νm are the
tr(Wp(1)AW p(1))+tr(Wp(2)AW p(2))+···+tr(Wp(h)AW p(h) )       =[             ]
tr(WT  BW    )+tr(WT BW    )+···+tr(WT BW    )             m                     A − λB
    p(1)  p(1)     p(2)  p(2)       p(h)  p(h)          ﬁrst  largest eigenvectors of   .
      T                        T
  tr(W   AW p(h))          tr(W AW  )                                m ≤ d − r
≤     p(h)       ≤             2    2                   Inthecaseof :         .
  tr(WT  BW    )     max   tr(WT BW  )
      p(h)  p(h)   WT W  =I    2    2                       ∗
                     2  2                                 W   = Z · [μ1, μ2, ..., μm],whereμ1, μ2, ..., μm are
  According to Theorem 1 we know, with the reduced      the ﬁrst m largest eigenvectors of ZT AZ,andZ =
dimension m  increases, the optimal value is decreased  [z1, z2, ..., zd−r] are the eigenvectors corresponding to
monotonously. When m =  d, the optimal value is equal to d − r zero eigenvalues of B.
                    T
tr(A)           tr(W AW ) ≥ tr(A)
tr(B) .So max   tr(WT BW)   tr(B) .
        WT W=I                         
                              T          m                Table 1: The algorithm for the optimization problem
  On the other hand, max tr(W  AW  )=    i=1 αi,and
                  WT W=I
            T            m
  min  tr(W  BW)=        i=1 βi,whereα1,α2,...,αm
WT W=I
are the ﬁrst m largest eigenvalues of A,andβ1,β2,...,βm 4 Neighborhood MinMax Projections
are the ﬁrst m smallest eigenvalues of B. Therefore,
           T                                          The method of Neighborhood MinMax Projections(NMMP)
       tr(W  AW )   α1+α2+···+αm
 max   tr(WT BW) ≤  β +β +···+β .                     is described in Table 2 . In order to speed up, PCA can be
WT W=I               1  2     m
  As a result, the bound of the optimal value is given by used as a preprocessing step before performing NMMP.
                   T                                                                      S
tr(A)          tr(W AW )   α +α +···+α                  Denote the covariance matrix of data by t, and denote the
     ≤                   ≤  1  2     m                                                                 ⊥
tr(B)    max   tr(WT BW)   β +β +···+β                           S    φ                          φ    φ
       WT W=I               1  2     m                null space of t by , the orthogonal complement of by .

                                                IJCAI-07
                                                   995                                                                                   1.5

                                                               1
 0. Preprocessing: eliminate the null space of the co-                             1
 variance matrix of data, and obtain new data X =             0.5                  0.5
                  d×n                                                              0
  x , x , ..., x ∈ R        rank X     d                       0
 [ 1  2     n]       ,where     (  )=                                              −0.5

                                                              −0.5                 −1

 1. Input:                                                                         −1.5
                                                              −1

                                                                                   −2
                         d×n                                   −1.5 −1 −0.5 0 0.5 1 1.5 −2 −1.5 −1 −0.5 0 0.5 1 1.5 2
    X =[x1, x2, ..., xn] ∈ R , kw(i), kb(i), m
           ˜      ˜                                            (a) Original data       (b) PCA
 2. calculate Sw and Sb according to Eq.(3) and Eq.(5)

                                                              1.5
           W                                                                       1
 3. calculate using the algorithm described in Table 1         1
                                                                                   0.5
                                                              0.5

 4. Output:                                                    0                   0

          T               d×m       T                         −0.5
    y = W   x,whereW   ∈ R    and W   W  = I.                                      −0.5
                                                              −1
                                                                                   −1
                                                              −1.5
                                                                −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 −1.5 −1 −0.5 0 0.5 1 1.5
             Table 2: Algorithm of NMMP                           (c) LDA             (d) NMMP

It is well known that the null space of St can be eliminated Figure 2: (a) is the ﬁrst two dimensions of the original ten-
without lose of any information. In fact, it can be easy to dimensional data set; (b),(c),(d) are the two-dimensional sub-
prove that the null space of St comprises the null space of space found by PCA, LDA and NMMP, respectively. It illus-
˜                     ˜                               trates that NMMP can ﬁnd a low-dimensional transformation
Sw and the null space of Sb deﬁned in Section 2. Suppose
w ∈ φ⊥ and ξ ∈ φ,then                                 preserving manifold structure with more discriminability.
                  T ˜             T ˜
           (w + ξ) Sb(w +  ξ)   w  Sbw                  Distance metric learning is an important problem for
                  T ˜         =   T ˜          (10)
           (w + ξ) Sw(w  + ξ)   w  Sww                the distance based classiﬁcation method. Learning a Ma-
                                                      halanobis distance metric is to learn a positive semideﬁ-
  Eq.(10) demonstrates that eliminating the null space of the
                                                      nite matrix M, and using the Mahalanobis distance met-
covariance matrix of data will not affect the result of the pro-   T
                                                      ric (xi − xj)  M(xi  −  xj) to replace the Euclidean
posed method. Thus we use PCA to eliminate the null space                      T
                                                      distance metric (xi − xj) (xi −  xj),whereM      ∈
of the covariance matrix of data.                       d×d           d
                                                      R    , xi, xj ∈ R . Note that M is positive semideﬁnite,
                                                                                           T
5  Discussion                                         with the eigen-decomposition, M = VV  ,whereV    =
                                                      [σ1v1,σ2v2, ..., σdvd], σ and v are eigenvalues and eigen-
Our method is closely connected with LDA. Both of them are
                                                      vectors of M. Therefore, the Mahalanobis distance metric
supervised dimensionality reduction methods, and the goals                T       T   T   T       T
                                                      can be formulated as (V xi − V xj) (V xi − V xj).In
are also similar. They both try to maximize the scatter be-
                                                      this form, we can see that Learning a Mahalanobis distance
tween different classes, and minimize the scatter within the
                    ˜                    ˜            metric is to learn a weighted orthogonal linear transformation.
same class. The matrix Sb deﬁned in Eq.(5) and Sw deﬁned
                                                      NMMP learns a linear transformation W with the constraint
in Eq.(3) are parallel to the between-class scatter matrix Sb T
                                                      of W  W  = I. So it can be viewed as a special case of learn-
and within-class scatter matrix Sw in LDA respectively. In
                                                      ing a Mahalanobis distance metric, where the weight value
fact, when the number of neighbors reaches the number of the
                                                      σi is either 0 or 1. Note that directly learning the matrix M
total available neighbors(kw(i)=ni −1,andkb(i)=n−ni,
                                                      is a very difﬁcult problem and it is usually formulated as a
where ni is the data number of class i,andn is the number
                   ˜    ˜      2                      semideﬁnite programming (SDP) problem, where the com-
of total data), we have Sb + Sw = n St, which is similar to
                                                      putation burden is extremely heavy. However, if we learn the
Sb + Sw = St in LDA.
                                                      transformation V instead of learning the matrix M, the prob-
  However, in comparison with LDA, we do not impose the
                                                      lem will become much easier to solve.
faraway pairwise points within the same class to be close to
each other, which makes us focus more on the improvement
of the discriminability of local structure. This property is 6 Experimental Results
especially useful when the distribution of class data is more We evaluated the proposed NMMP algorithm on several data
complex than Gaussian. We give a toy example to illustrate sets, and compared it with LDA and LMNN method. The data
it(Figure 2).                                         sets we used belong to different ﬁelds, a brief description of
  The toy data set consists of three classes(shown by dif- these data sets is list on Table 3.
ferent shapes). In the ﬁrst two dimensions, the classes are We use PCA as the preprocessing step to eliminate the null
distributed in concentric circles, while the other eight dimen- space of data covariance matrix St. For LDA, due to the sin-
sions are all Gaussian noise with large variance. Figure 2 gularity problem existing in it, we further reduce the dimen-
shows the two-dimensional subspace learned by PCA, LDA sion of data such that the within-class scatter matrix Sw is
and NMMP, respectively. It illustrates that NMMP can ﬁnd nonsingular.
a low-dimensional transformation preserving manifold struc- In each experiment, we randomly select several samples
ture with more discriminability.                      per class for training and the remaining samples for testing.
  Moreover, compared with LDA, our method is able to ex- the average results and standard deviations are reported over
tract more discriminative features and the singularity problem 50 random splits. The classiﬁcation is based on k-nearest
existing in LDA will not occur naturally.             neighbor classiﬁer(k =3in these experiments).

                                                IJCAI-07
                                                   996                                             Iris Bal   Faces  Objects  USPS   News
                    class                    3     3     40      20       4      4
                    training number          60   60    200      120     80     120
                    testing number           90   565   200     1320    3794    3850
                    input dimensionality     4     4   10304     256     256    8014
                    dimensionality after PCA 4     4    199      119     79     119

                                  Table 3: A brief description of the data sets.

           data set  method   Projection number Accuracy(%)   Std. Dev.(%) Training time(per run)
           Iris      baseline        4              95.4          1.8               –
                      LDA            2              96.6          1.6              0s
                     LMNN            4              96.2          1.5             4.91s
                     NMMP            3              96.5          1.6             0.02s
           Bal       baseline        4              61.6          2.8               –
                      LDA            2              74.9          3.2              0s
                     LMNN            4              70.1          3.7             3.37s
                     NMMP            2              72.9          4.2             0.02s
           Faces     baseline       199             86.9          2.1               –
                      LDA            39             92.2          1.8             0.15s
                     LMNN           199             95.9          1.6            399.03s
                     NMMP            60             96.6          1.6             2.84s
           Objects   baseline       119             76.8          1.8               –
                      LDA            19             78.2          2.0             0.04s
                     LMNN           119             84.1          1.8            221.29s
                     NMMP            60             86.5          1.6             0.66s
           USPS      baseline        79             93.2          1.1               –
                      LDA            3              84.2          2.6             0.01s
                     LMNN            79             86.2          2.2             70.83s
                     NMMP            60             94.5          0.9             0.12s
           News      baseline       119             30.9          2.8               –
                      LDA            3              46.9          5.7             0.05s
                     LMNN           119             62.1          7.3             73.38s
                     NMMP            60             58.5          5.0             0.40s

                                 Table 4: Experimental results in each data set.

  It is worth noting that the parameters in our method are not and have variations [Samaria and Harter, 1994] including ex-
sensitive. In fact, in each experiment, we simply set kb(i) to pression and facial details. Each image in the database is of
10, and set kw(i) to ni/2+2for each class i,whereni is the size 112 × 92 and with 256 gray-levels.
training number of class i.                             In this experiment, no other preprocessings are performed
  The experimental results are reported in Table 4. We use except the PCA preprocessing step. The result of our method
the recognition result directly performed after the preprocess- is much better than those of LDA and the baseline. LMNN
ing by PCA as the baseline.                           have a good performance too, but the computation burden is
  In the following we describe the details of each experiment. extremely heavy.
  The UCI data sets                                     We also perform the experiments on many other face
  In this experiment, we perform on two small data sets, Iris databases, and obtain the similar results, say, our method
and Balance, taken from the UCI Machine Learning Reposi- demonstrates the much better performances uniformly.
   1
tory . As the class distributions of this two data sets are not Object recognition
very complex, LDA works well, and our method also demon-
                                                        The COIL-20 database [Nene et al., 1996] consists of im-
strates the competitive performance.                  ages of 20 objects viewed from varying angles at the interval
  Face recognition                                    of ﬁve degrees, resulting in 72 images per object.
  The AT&T face database (formerly the ORL database) in-
                                                        In this experiment, each image is down-sampled to the size
cludes 40 distinct individuals and each individual has 10 dif- ×
ferent images. Some images were taken at different times, of 16 16 for saving the computation time.
                                                        Similar to the face recognition experiments, the results of
  1Available at http://www.ics.uci.edu/ mlearn/MLRepository.html our method and LMNN are much better than those of LDA

                                                IJCAI-07
                                                   997