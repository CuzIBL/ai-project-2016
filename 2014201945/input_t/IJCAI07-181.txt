                             A Primitive Based Generative Model
           to Infer Timing Information in Unpartitioned Handwriting Data

                        Ben H Williams, Marc Toussaint and Amos J Storkey
                                          Edinburgh University
                                         School of Informatics
                                         ben.williams@ed.ac.uk

                    Abstract                          switched off. Rather, to quickly modify a movement trajec-
                                                      tory, the movement primitives are superimposed [Kargo and
    Biological movement control and planning is based Giszter, 2000].
    upon motor primitives. In our approach, we pre-     A primitive in the handwriting domain will be a short time
    sume that each motor primitive takes responsibil- extended block of pen motion, brought about by a corre-
    ity for controlling a small sub-block of motion,  sponding block of muscle activations. These blocks are su-
    containing coherent muscle activation outputs. A  perimposed upon each other to create coherent movement -
    central timing controller cues these subroutines of in this case, a character.
    movement, creating complete movement strategies     To reliably model natural handwriting, we introduce a fully
    that are built up by overlaying primitives, thus cre- generative model, allowing proper modelling of handwriting
    ating synergies of muscle activation. This partition- variation and adaptation irrespective of the character drawn.
    ing allows the movement to be deﬁned by a sparse  If the handwriting output is made up of primitive type sub-
    code representing the timing of primitive activa- blocks then the generative model must represent these primi-
    tions. This paper shows that it is possible to use tives, to allow it to efﬁciently model the internal handwriting
    a factorial hidden Markov model to infer primitives encoding.
    in handwriting data. The variation in the handwrit- In section 2, we introduce our generative handwriting
    ing data can to a large extent be explained by timing model based on primitives in terms of a factorial hidden
    variation in the triggering of the primitives. Once Markov model. Section 3 covers the generalisation over the
    an appropriate set of primitives has been inferred, timing of primitives to create a timing model. In section 4
    the characters can be represented as a set of tim- we present some typical samples, and the results of using this
    ings of primitive activations, along with variances, model, and ﬁnally the discussion is in section 5.
    giving a very compact representation of the charac-
    ter. The model is naturally partitioned into a low 2  Model
    level primitive output stage, and a top-down primi-
    tive timing stage. This partitioning gives us an in- A generative handwriting model must be capable of reproduc-
    sight into behaviours such as scribbling, and what ing the class of characters upon which it has been trained. As-
    is learnt in order to write a new character.      suming that all motor commands are made up of motor primi-
                                                      tives, handwriting must therefore contain projections of these
                                                      primitives. Assuming also that motor primitives are ﬁxed, or
1  Introduction                                       adaptable over long time scales, any short term adaptability
                                                      and learning must come from the timing and selection of dif-
Biological systems are strongly superior to current robotic ferent primitives.
movement control systems, despite having very noisy sen- Assuming the individual primitives are independent of
sors, and unpredictable muscles. Therefore, the amount and each other, and are linearly superimposed, a controller to se-
nature of pre-planning in biological movement is extremely lect primitive onset timing is necessary, similar in nature to a
interesting. Strong evidence exists to suggest that biological Piano Model, where key pressing controls the onset of time
motor control systems are modularised, with motor primitives extended clips of sound that the listener hears as a superim-
ﬁrst being conclusively found in frogs [Bizzi et al., 1995; posed waveform.
d’Avella and Bizzi, 2005; d’Avella et al., 2003], where stim-
ulation of a single spinal motor afferent triggered a complete 2.1 A Deterministic Piano Model
sweeping movement of the frog’s leg. For a review of mod- To formalise this model in a generative way, the output of the
ularisation of motor control in the spine, see [Bizzi et al., system Y at time t is deﬁned as
2002].                                                                   
  Evidence suggests that once a particular subsection of          Y (t)=    αmnWm(t   − τmn) ,        (1)
movement has commenced, it cannot be unexpectedly                        m,n

                                                IJCAI-07
                                                  1119                                                      factor is in state i and zero otherwise. This allows us to write
                                                                                       m
                                                      expectations of the hidden states as St , which is also the
                                                                                                 m
                                                      probability distribution over the individual states St . Each
                                                      latent factor is a Markov chain deﬁned by the state transition
                                                      probabilities and the initial state distribution as
                                                            m        m          m      m          m
                                                        P (S1 =i)=πi   ,P(St      =i | St−1 =j)=Pi,j , (4)
                                                      where πm  is a K-dimensional parameter vector giving the
                                                      initial hidden state distribution, and P m is a K×K parameter
                                                      matrix denoting the state transition probabilities. As can be
                                                      seen in Figure 1, each factor is independent. This means that
                                                      the joint probability distribution can be factorised as
Figure 1: Graphical representation of a Factorial Hidden Markov
Model, showing the independence of the separate Markov chains.                     T
Although the observable output is dependent upon the state of the P ({Yt,St})=P (S1)P (Y1|S1) P (St|St−1)P (Yt|St)
entire system, the internal states evolve with no interdependencies.              t=2
Sm
 t denotes the hidden state vector at time t, in factor m.                                            (5)
                                                                     M             T M
where Wm(t) are the primitives, and τmn represents the time       =     πmP  (Y |S )       P mP (Y |S ) .
      th                                                                       1  1               t t
of the n activation of primitive m, and αmn deﬁnes the ac-           m=1            t=2 m=1
tivation strengths of the primitives. In this deﬁnition, m enu-                                       (6)
merates the primitives, whilst n enumerates the occurrence of
the primitive within the sample window, at time τmn.    Given the fully parameterised modelling framework, learn-
  Similar models have been used for modelling real piano ing of the parameters can be done using an Expectation-
operation such as [Cemgil et al., 2005], where the transfer Maximisation (EM) method. The structured variational ap-
from piano roll to sound uses note onset, and damping on- proximation was chosen for the E-step inference. For more
set as key timing information for the reproduction of sound. details on the various arguments for and against this choice,
Also [Karklin and Lewicki, 2005] present a generative model refer to [Ghahramani and Jordan, 1997], which provides de-
of speech waveforms, where their ‘Sparse Shiftable Kernel tails about the fHMM model, and the learning procedure.
Representation’ is similar in nature to the Piano Model pre- The EM method is an iterative algorithm, in which the E-
sented here.                                          step infers the expectations of the hidden states given a set of
  The data samples of handwritten characters are not seg- parameters, then the M-step updates the parameters to their
mented or keyed to a common start point, apart from the pen maximum-likelihood values, given the inferred hidden state
touching the paper. As this is not a reliable keying time for distributions. In our case, the E-step ﬁts the primitives to the
primitive activation, a ﬂexible model must be used to infer the data, inferring the primitive onset timings for each sample.
primitives, which will not only infer the shape of the primi- The M-step infers the shapes of the primitives. Some con-
tives, but their timing onsets. We take the idea of the Piano straints were imposed upon the parameters, so that the primi-
Model as a basis, but model it probabilistically using a facto- tives progressed monotonically along their states, and that the
rial hidden Markov model (fHMM).                      rest state for all primitives, gave a zero output contribution.
                                                        The fHMM can reconstruct the data by using a set of maxi-
2.2  Factorial Hidden Markov Model                    mally statistically independent primitives, and the appropriate
A graphical model of the fHMM can be seen in Figure 1. At hidden state values. Due to the constraints imposed upon the
each time step, the observable output Yt, a vector of dimen- hidden state transitions, these state values can be reduced to
                                      (1)   m         a set of primitive activation timings, or spikes. Without this
sion D, is dependent on M hidden variables S , .., S . The
                                      t     t         spike timing information, the primitive model can still be run
output is a multivariate Gaussian, such that
                                                      separately, as can be seen in Figure 4, which can be thought
                  Yt ∼N(μt,C)  ,                (2)   of as primitive babbling. To reconstruct a character, the prim-
     C     D ×  D                                     itives need to be coordinated, and activated at the appropriate
where   is a      parameter matrix of output covariance, times. This is achieved by introducing a separate part of the
and
                      M                              model, the centralised timing controller.
                            m  m
                 μt =     W   St                (3)
                      m=1                             3   Timing Model
is the D-dimensional output mean at time t. W m is a D × K The centralised timing controller must be capable of repro-
parameter matrix giving the output means for each factor m, ducing spiking characteristics that in some areas of the char-
such that the output mean μt is a linear combination of its acter are variable, and others less so, both in time of spike,
columns weighted with the hidden state activations.   and existence of spike. In other words, some primitives are
  Each of the M hidden variables can be in K different necessary, occurring in every character sample in roughly, but
states. In equation (3) this is encoded in the K-dimensional not exactly the same place. Others occur occasionally in a re-
           m                       m
state vector St using a 1-in-K code, i.e., St,i =1if the m-th production, but not in every case. Crucially, on a short time

                                                IJCAI-07
                                                  1120                                                                    m
scale, there is heavy dependency between spikes, whereas on ﬁt the average It matrix that is constructed by taking linear
the long term, they are simply dependent upon the character interpolations.
                                                                    
being drawn.                                                            τ m
                                                                m     n  kn,n
  We have chosen a stochastic Integrate and Fire (IF) model    It =          ,kn   =(t + δtn)δln.    (10)
for the generation of spikes, as this model has a local tempo-         N
ral dependency characteristic, and also allows variance in the More precisely, we optimize the temporal offset and strecht-
total number of spikes in a sample.                   ing by iteratively ﬁnding δtn and δln via gradient ascent that
                                                      maximize the objective function
                                                                          
Integrate and Fire The Integrate and Fire (IF) model orig-                     m   m
                                                                              τkn,nIt .              (11)
inates from simpliﬁed models of biological neurons. It treats            n,m,t
the neuron as a leaky capacitor, upon which a charge is built
                                                                     m
up by the inputs, over time. Once the voltage across the ca- This ﬁnds an It matrix that best reﬂects an average prim-
pacitor reaches a noisy threshold level, the neuron ﬁres, pro- itive onset probability matrix, where t has a separate linear
ducing a spike at its output, and discharging the capacitor. shifting and stretching factor associated with it for each char-
This means that, due to the leak term, over a long time scale, acter sample, n. This is used to parameterise the IF model,
the inputs at different times are independent, however, on a which generates the spike timing information needed to run
short time scale, they are not, as it is the short-term running the fHMM generatively.
sum of the inputs that causes the neuron to ﬁre. This is desir-
able for the primitive model, because the timing of a neces- 3.1 Implementation
sary primitive can be variable in the character samples, how- Handwriting data were gathered using an INTUOS 3
ever, the IF neuron will still ﬁre as long as it receives enough WACOM digitisation tablet http://www.wacom.com/
inputs during its temporal memory window.             productinfo/9x12.cfm. This provided 5 dimensional
  The most straight forward model using IF neurons is to data at 200Hz. The dimensions of the data were x-position,
attribute one IF neuron to one primitive. The inputs to the y-position, pen tip pressure, pen tilt angle, and pen orienta-
                                                                ◦
neurons will then determine the timing of the primitives and tion (0-360 ). The normalised ﬁrst differential of the data
thus the character that is written. For a particular primitive, was used, so that the data mean was close to zero, providing
                                    m
m, the probability of a spike at time t, P (λt ) is given by: the requirements for the zero state assumption in the model
           m  m             m      m     m            constraints (see section 2.2). Only 3 dimensions of the data
       P (λt |λt−1 =0)=P  (λt−1)+It  − Lt ,     (7)   were used, x-position, y-position, and pressure, as the sig-
           m  m          m     m
       P (λt |λt−1 =1)=It  −  Lt ,              (8)   nal to noise ratio of the other two was too low to provide
                   Lm = νP(λm   ),                    usful data. The data collected were separated into samples,
                    t        t−1                (9)   or characters, for processing purposes, and then the parame-
      m                           m                   ters were ﬁtted to the data using our algorithm. We generated
where It are the input excitations, and L is a leak term pro-
portional to the accumulated probability. Therefore, given a datasets of the characters ‘g’, ‘m’, ‘a’, ‘b’, ‘c’ and ‘d’. The ‘g’
common set of primitives, a character is deﬁned by its tempo- character set was the largest character set, and can be found
                   m                                  at http://homepages.inf.ed.ac.uk/s0349619/
ral excitation matrix, It , which parameterises the IF model.
This matrix is learnt from the spiking statistics of the charac- data/mixoutG.mat. The character sets ‘g’ and ‘m’ were
ter training set, as seen below.                      chosen to explore the differences in primitives inferred from
  During the E-step for the fHMM, the hidden state dis- two contrasting characters, see Figure 4. The characters ‘a’,
            m                                         ‘b’, ‘c’ and ‘d’ were chosen to explore the sharing of prim-
tribution P (St ) is inferred. As the transition matrix is
constrained so that the primitives progress monotonically itives between different characters, and the ability to create
                                     m                distinct reconstructions, as seen in Figure 7.
through their states, the information in P (St ) can be sum-
marised as the onset probabilities of the different primitives, Data set Size
    m                           m                             ‘g’  1292
P (St,1 =1)which are the rows of P (St ) for state 1, the ﬁrst
state in each primitive. For a set of primitives that ﬁt the data ‘m’ 125
well, these probabilities are close to zero or one, and form ‘abcd’ 296
a very sparse matrix containing spikes representing primitive
activation appropriate to reconstruct a single character sample 4 Results
from the data set. It is effectively an average over the samples A typical subset of primitives learnt via the fHMM from data
of these spiking matrices that is needed to parameterise the IF of g-characters are shown in Figure 2 demonstrating the vari-
         m                       m       m
model, as It . For ease of notation, let τt,n = P (St,1 =1)be ation in shape and length on the paper of the primitives when
the posterior onset probability at time t of the mth primitive reconstructed on their own. In this example, 20 primitives of
for the n data sample.                                length 40 were used to model a data set of 1292 characters, of
  To allow for differences in the start point time, and aver- average length 103.8. The average error of the reconstruction
age speed of each character sample, two parameters are as- in velocity space for the data was [0.0617 0.0601 0.0295].
sociated with each data sample, a temporal offset δtn and a Using these primitives, the fHMM model can reconstruct the
linear temporal stretching factor δln. These parameters are original character as shown in Figure 3 using the posterior
                    m
optimised so that the τt,n matrices for the nth sample best timings St that are inferred from the data in the E-step of

                                                IJCAI-07
                                                  1121                                                                         Primitive Timing Data  Pen Space Reconstruction
               Primitive Timing Data  Pen Space Reconstruction

                                                                   1                       0.3
          2                                          ←←2
                                                ←2   ←1913
                                                  ←20 ←18
                                  0
                                                    ←6←13          2
          4                                                                                0.2                 ←6
                                                                                                              ←5
                                                     ←
          6                      −0.1                 9            3
                                                     ←7                                    0.1
                                                     ←4
          8                                                        4
                                                                                                       ←2
                                 −0.2                                                              ←←296
                                          ←17                                               0           ←4

         10                                                        5
                                                                                                   ←9
                                                    ←                                     −0.1
                                 −0.3                20           Primitive  no.                          ←
         Primitive  no.                                                                                    4
         12                                                                                         ←7
                                                                   6                                ←9  ←←298
                                                                                                ←3
         14                                                                               −0.2
                                 −0.4
                                                                   7
                                                 ←14
         16                                                                                 ←
                                               ←10                                        −0.3 ←147
                                 −0.5                              8
         18
                                          ←6
                                        ←11                                               −0.4
                                                                   9
         20                      −0.6
              20 40  60  80 100    −0.3 −0.2 −0.1 0                   20 40 60 80 100 120    −0.1  0  0.1  0.2
                Time /sample no.                                         Time /sample no.
                              (a)                                                       (b)

Figure 3: Two examples of character reconstructions, using timing information derived from the E-step inference. In both (a) and (b), the
timing information is shown on the left, and the reproduction of the sample on the right, with the onset of each primitive marked with an
arrow. In (a), 20 primitives of length 40 time steps were used to model the ‘g’ dataset, of over 1000 characters. In (b), 9 primitives of length
30 were used to model the ‘m’ dataset, of over 100 characters.


        0.5                                                       0.4


         0
                                                                  0.2

        −0.5
                                                                   0

        −1
                                                                 −0.2

        −1.5

                                                                 −0.4
        −2

                                                                 −0.6
        −2.5

                                                                 −0.8
        −3


        −3.5                                                      −1
         −2.5   −2      −1.5   −1     −0.5     0      0.5         −0.3 −0.2 −0.1 0   0.1 0.2  0.3 0.4 0.5  0.6 0.7
                              (a)                                                       (b)

Figure 4:  Two samples generated using primitives without speciﬁc timing information (neither the posterior timing nor a learnt timing
model). (a) was generated using primitives inferred from the ‘g’ dataset, (b) was generated using primitives from the ‘m’ dataset. Starting
point of both reconstructions is (0, 0).


                                                        IJCAI-07
                                                          1122                      −3
                     x 10
                    0
     0.02                        0.02
                   −5            0.01
      0
                   −10            0

                                −0.01
    −0.02          −15

      0 0.02 0.04 0.06 −0.01 0 0.01  −0.04 −0.02 0

                    0             0

                  −0.02         −0.01
     0.04
                  −0.04         −0.02
     0.02                                                  5
                  −0.06         −0.03

      0           −0.08         −0.04
         0 0.02 0.04 −0.08−0.06−0.04−0.02 0 −0.04 −0.02 0

                                                           10
     0.06                         0
                   0.04
                                −0.01
     0.04
                                −0.02
                   0.02
     0.02                       −0.03                      15
                                −0.04
      0             0
      0 0.02 0.04 0.06 0.08 −0.02 0 0.02 −0.06 −0.04 −0.02 0

                                                           20
Figure 2: A sample of 9 primitives used for the reconstruction in
Figure 3(a). The axis values refer to some normalised distance on

paper, as the primitives are shown as a reconstruction in pen-space, 25
rather than the raw data, which is in 3-dimensional velocity space.
As the data was normalised ignoring the physical size of the pixels, 20 40 60 80 100 120 140 160 180 200
it would be meaningless to provide units for the axes. Thickness
corresponds to pen pressure. Refer to section 3.1 for an explanation. Figure 5: A graphical representation of the distribution of spikes as
                                                              m
                                                      given by It for a ‘g’ character set.

the primitive extraction stage. The timing information, seen
on the left of the Figures is a representation of the poste-
rior probabilities of onset of the primitives whilst reconstruct-
ing the data set. These probabilities are inferred during the
E-step, and reveal the expected hidden state values of the
fHMM. Furthermore, given a sufﬁcient set of common prim-
itives to model the data reliably, these probabilities can be
represented as spike timings, which provide a very compact
encoding of the character. This timing information can also
be modelled using a timing model, as investigated in Section

3. Here, no timing model was used or learnt. Without such a        0.1                          0.2

                                                                   0                            0.1
timing model and without the hidden state posterior extracted 5                      5
                                                                  −0.1                          0

from the data, one can still run the primitive model gener-       −0.2                          −0.1
                                                       10                            10
atively, ignoring any precise timing information, and sam-        −0.3                          −0.2
                                                                  −0.4                          −0.3
                                                       15                            15
pling the model states from the priors, without calculating       −0.5                          −0.4

                                                                  −0.6                          −0.5
the posterior distributions. The result is a random superposi- 20                    20
                                                                  −0.7                          −0.6

tion of primitives producing a scribbling form of output such 25  −0.8               25         −0.7

                                                                  −0.9                          −0.8
as shown in Figure 4. Here it can be seen that the primitives, 50 100 150 200 −0.6 −0.4 −0.2 0 0.2 50 100 150 200 −0.6 −0.4 −0.2 0 0.2 0.4
even without precise timing information controlling their ac-      0.4                          0.4
                                                                   0.2                          0.2
tivation, still generate samples that capture an aspect of the 5                     5
                                                                   0                            0
training set from which they were extracted. Allowing the 10                         10
temporal shifting and stretching as described in Section 3,       −0.2                          −0.2
                                                       15                            15
                                        m                         −0.4                          −0.4
produces a distribution over spiking patterns, It as can be
seen in Figure 5 for the ‘g’-dataset. Sampling from this dis- 20  −0.6               20         −0.6
                                                                  −0.8                          −0.8
tribution using our integrate and ﬁre approach as described 25                       25
                                                                   −1                           −1
above, produces samples that reliably model the variation of 50 100 150 200 −1 −0.5 0 0.5 50 100 150 200 −0.6 −0.4 −0.2 0 0.2 0.4
the data set, as can be seen in Figure 6. Clearly, these are
                                                      Figure 6: Four samples generated from the full generative model
all examples of a character ‘g’, however, the pen trajectory is as given by the learnt primitives (the fHMM) and the timing model
                                                                                     m
vastly different in each example, reﬂecting the variance in the parameterized by the spike distribution It (shown in Figure 5).
original data set. Inferring primitives from a range of char-
acters is also possible. Figure 7 shows 4 different characters
all drawn using the same primitives. Despite the variation in
the characters of a particular alphabet, the actual hand move-
ments when controlling the pen are very similar, so it should
come as no surprise that it is possible to use the same primi-

                                                IJCAI-07
                                                  1123